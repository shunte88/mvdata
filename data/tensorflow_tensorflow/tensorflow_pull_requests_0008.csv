id,type,state,state_reason,title,body,author,created_at,assignees,updated_at,closed_at,url,labels,comments_list,comment_thread
2778824537,pull_request,open,,Fix missing BUILD dependency in compiler/xla/tests:collective_pipeline_parallelism_test.,"Fix missing BUILD dependency in compiler/xla/tests:collective_pipeline_parallelism_test.
",copybara-service[bot],2025-01-09 22:00:44+00:00,[],2025-01-09 22:00:44+00:00,,https://github.com/tensorflow/tensorflow/pull/84504,[],[],
2778823910,pull_request,closed,,Remove unused data members from PluggableDeviceProcessState.,"Remove unused data members from PluggableDeviceProcessState.
",copybara-service[bot],2025-01-09 22:00:15+00:00,[],2025-01-10 20:33:19+00:00,2025-01-10 20:33:18+00:00,https://github.com/tensorflow/tensorflow/pull/84503,[],[],
2778817678,pull_request,closed,,Cleanup. Sort the declarations in spmd_partitioner.,"Cleanup. Sort the declarations in spmd_partitioner.
",copybara-service[bot],2025-01-09 21:55:19+00:00,[],2025-01-09 22:29:29+00:00,2025-01-09 22:29:28+00:00,https://github.com/tensorflow/tensorflow/pull/84502,[],[],
2778796491,pull_request,closed,,[xla:python] Add method to get python callback capsule without requiring operand or result shapes / returning capsule descriptor.,"[xla:python] Add method to get python callback capsule without requiring operand or result shapes / returning capsule descriptor.
",copybara-service[bot],2025-01-09 21:38:36+00:00,[],2025-02-05 19:15:01+00:00,2025-02-05 19:15:01+00:00,https://github.com/tensorflow/tensorflow/pull/84501,[],[],
2778789869,pull_request,closed,,Add tests to make sure DenseResourceElementsAttr are handled/supported by flatbuffer_export.,"Add tests to make sure DenseResourceElementsAttr are handled/supported by flatbuffer_export.

Note that, flatbuffer_import will remain unchanged and it will not create DenseResourceElementsAttr, now.
",copybara-service[bot],2025-01-09 21:33:36+00:00,['vamsimanchala'],2025-01-10 06:34:25+00:00,2025-01-10 06:34:24+00:00,https://github.com/tensorflow/tensorflow/pull/84500,[],[],
2778786297,pull_request,closed,,Speed-up DepthwiseInputCopyOp,"Speed-up DepthwiseInputCopyOp

Optimize this by replacing multiplication
with advancing the pointer every iteration. Also avoid reloading depth/etc.
from args every time.

Fixing benchmark for depthwise conv and running them I get a lot of noise,
but it seems positive overall.

name                                                                                   old cpu/op   new cpu/op   delta
BM_ConvFloatDepthwiseFwdCPU1_conv0_float/real_time  [32_112_112_3_8_24_3_3_1_2_cpu1 ]  33.4µs ±16%  34.7µs ±28%     ~     (p=0.284 n=38+39)
BM_ConvFloatDepthwiseFwdCPU4_conv0_float/real_time  [32_112_112_3_8_24_3_3_1_2_cpu4 ]  27.3µs ±57%  26.6µs ±52%     ~     (p=0.556 n=40+40)
BM_ConvFloatDepthwiseFwdCPU1_conv1_float/real_time  [32_112_112_64_1_64_3_3_1_2_cpu1]  35.6µs ±24%  36.3µs ±27%     ~     (p=0.283 n=35+40)
BM_ConvFloatDepthwiseFwdCPU4_conv1_float/real_time  [32_112_112_64_1_64_3_3_1_2_cpu4]  30.0µs ±27%  31.1µs ±33%     ~     (p=0.377 n=36+34)
BM_ConvFloatDepthwiseFwdCPU1_conv2_float/real_time  [32_56_56_128_1_128_3_3_1_2_cpu1]  32.8µs ±14%  33.1µs ±18%     ~     (p=0.761 n=33+38)
BM_ConvFloatDepthwiseFwdCPU4_conv2_float/real_time  [32_56_56_128_1_128_3_3_1_2_cpu4]  25.7µs ±57%  26.4µs ±55%     ~     (p=0.609 n=40+40)
BM_ConvFloatDepthwiseFwdCPU1_conv3_float/real_time  [32_56_56_128_1_128_3_3_2_2_cpu1]  32.2µs ±17%  31.7µs ±12%     ~     (p=0.204 n=37+35)
BM_ConvFloatDepthwiseFwdCPU4_conv3_float/real_time  [32_56_56_128_1_128_3_3_2_2_cpu4]  27.8µs ±32%  27.0µs ±24%     ~     (p=0.341 n=34+39)
BM_ConvFloatDepthwiseFwdCPU1_conv4_float/real_time  [32_28_28_128_1_128_3_3_1_2_cpu1]  32.1µs ±13%  31.9µs ±12%     ~     (p=0.470 n=39+36)
BM_ConvFloatDepthwiseFwdCPU4_conv4_float/real_time  [32_28_28_128_1_128_3_3_1_2_cpu4]  26.2µs ±30%  25.5µs ±44%     ~     (p=0.677 n=38+37)
BM_ConvFloatDepthwiseFwdCPU1_conv5_float/real_time  [32_14_14_512_1_512_3_3_1_2_cpu1]  31.5µs ±18%  31.7µs ±17%     ~     (p=0.742 n=38+39)
BM_ConvFloatDepthwiseFwdCPU4_conv5_float/real_time  [32_14_14_512_1_512_3_3_1_2_cpu4]  28.5µs ±28%  27.3µs ±29%     ~     (p=0.208 n=35+37)
BM_ConvFloatDepthwiseFwdCPU1_conv6_float/real_time  [32_7_7_1024_1_1024_3_3_1_2_cpu1]  29.3µs ±16%  28.9µs ±21%     ~     (p=0.334 n=39+31)
BM_ConvFloatDepthwiseFwdCPU4_conv6_float/real_time  [32_7_7_1024_1_1024_3_3_1_2_cpu4]  8.35µs ±62%  7.08µs ±46%  -15.24%  (p=0.026 n=40+37)
BM_ConvFloatDepthwiseFwdCPU1_conv7_float/real_time  [32_112_112_3_8_24_3_3_2_2_cpu1 ]  31.2µs ±17%  31.4µs ±22%     ~     (p=0.987 n=35+38)
BM_ConvFloatDepthwiseFwdCPU4_conv7_float/real_time  [32_112_112_3_8_24_3_3_2_2_cpu4 ]  25.9µs ±45%  26.5µs ±32%     ~     (p=0.859 n=39+38)
BM_ConvFloatDepthwiseFwdCPU1_conv8_float/real_time  [32_112_112_3_8_24_3_3_2_1_cpu1 ]  30.0µs ±16%  30.5µs ±18%     ~     (p=0.228 n=34+33)
BM_ConvFloatDepthwiseFwdCPU4_conv8_float/real_time  [32_112_112_3_8_24_3_3_2_1_cpu4 ]  26.2µs ±41%  24.4µs ±53%     ~     (p=0.288 n=36+40)
BM_ConvFloatDepthwiseFwdCPU1_conv9_float/real_time  [1_100_100_72_1_72_3_3_1_2_cpu1 ]  26.5µs ±16%  25.6µs ±15%     ~     (p=0.051 n=34+37)
BM_ConvFloatDepthwiseFwdCPU4_conv9_float/real_time  [1_100_100_72_1_72_3_3_1_2_cpu4 ]  6.33µs ±37%  5.60µs ±36%  -11.46%  (p=0.011 n=40+35)
BM_ConvFloatDepthwiseFwdCPU1_conv10_float/real_time [1_100_100_72_1_72_5_5_1_2_cpu1 ]  26.4µs ±13%  27.8µs ±20%     ~     (p=0.140 n=33+40)
BM_ConvFloatDepthwiseFwdCPU4_conv10_float/real_time [1_100_100_72_1_72_5_5_1_2_cpu4 ]  14.6µs ±79%   9.2µs ±90%  -36.75%  (p=0.000 n=40+40)
",copybara-service[bot],2025-01-09 21:30:56+00:00,[],2025-01-13 21:44:56+00:00,2025-01-13 21:44:56+00:00,https://github.com/tensorflow/tensorflow/pull/84499,[],[],
2778740577,pull_request,closed,,AHWB must outlive tensor buffer,"AHWB must outlive tensor buffer
",copybara-service[bot],2025-01-09 20:57:52+00:00,[],2025-01-14 15:23:05+00:00,2025-01-14 15:23:05+00:00,https://github.com/tensorflow/tensorflow/pull/84498,[],[],
2778736843,pull_request,closed,,Replacement PR for #76210 Add support for quint8 type for uniform_quantize and uniform_ dequantize ops,This PR is a replacement for https://github.com/tensorflow/tensorflow/pull/76210,mahmoud-abuzaina,2025-01-09 20:55:07+00:00,['gbaned'],2025-02-07 00:05:49+00:00,2025-02-07 00:05:49+00:00,https://github.com/tensorflow/tensorflow/pull/84497,"[('awaiting review', 'Pull request awaiting review'), ('ready to pull', 'PR ready for merge process'), ('size:L', 'CL Change Size: Large')]","[{'comment_id': 2588545191, 'issue_id': 2778736843, 'author': 'sdasgup3', 'body': '@mahmoud-abuzaina Can you please rebase and try again.  The internal error fixed on sync.', 'created_at': datetime.datetime(2025, 1, 14, 1, 11, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2590619822, 'issue_id': 2778736843, 'author': 'mahmoud-abuzaina', 'body': '@sdasgup3 I just rebased it. Thanks!', 'created_at': datetime.datetime(2025, 1, 14, 17, 19, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641105398, 'issue_id': 2778736843, 'author': 'sdasgup3', 'body': '>> [Presubmit - Linux x86 CPU - Py+CPP Test Suite](https://storage.cloud.google.com/tensorflow-public-build-artifacts/prod/tensorflow/official/presubmit/github/linux_x86_cpu/69559/20250206-122225/github/tensorflow/build_output/index.html)\r\n\r\n`//tensorflow/python/kernel_tests:collective_ops_multi_worker_test` time-out. Looking into it.', 'created_at': datetime.datetime(2025, 2, 6, 21, 36, 3, tzinfo=datetime.timezone.utc)}]","sdasgup3 on (2025-01-14 01:11:02 UTC): @mahmoud-abuzaina Can you please rebase and try again.  The internal error fixed on sync.

mahmoud-abuzaina (Issue Creator) on (2025-01-14 17:19:36 UTC): @sdasgup3 I just rebased it. Thanks!

sdasgup3 on (2025-02-06 21:36:03 UTC): `//tensorflow/python/kernel_tests:collective_ops_multi_worker_test` time-out. Looking into it.

"
2778709368,pull_request,closed,,Add support for default memory space descriptions;,"Add support for default memory space descriptions;
",copybara-service[bot],2025-01-09 20:36:24+00:00,[],2025-01-10 01:06:08+00:00,2025-01-10 01:06:07+00:00,https://github.com/tensorflow/tensorflow/pull/84496,[],[],
2778703647,pull_request,closed,,Move convert_async_collectives_to_sync to collectives directory,"Move convert_async_collectives_to_sync to collectives directory
",copybara-service[bot],2025-01-09 20:32:27+00:00,['frgossen'],2025-01-14 00:52:50+00:00,2025-01-14 00:52:48+00:00,https://github.com/tensorflow/tensorflow/pull/84495,[],[],
2778662634,pull_request,open,,[XLA] Make HloParser idempotent wrt independent parses,"[XLA] Make HloParser idempotent wrt independent parses

Using a static local variable means that parsing the same text twice would result in different modules.
",copybara-service[bot],2025-01-09 20:04:30+00:00,['majnemer'],2025-02-05 23:07:44+00:00,,https://github.com/tensorflow/tensorflow/pull/84494,[],[],
2778590318,pull_request,closed,,Add source line and stack_frame functionality in hlo_module_map and utils,"Add source line and stack_frame functionality in hlo_module_map and utils
",copybara-service[bot],2025-01-09 19:19:34+00:00,['zzzaries'],2025-01-11 02:12:08+00:00,2025-01-11 02:12:07+00:00,https://github.com/tensorflow/tensorflow/pull/84493,[],[],
2778555499,pull_request,open,,Remove `SKIP_TEST_IF_NUM_DEVICES_LESS_THAN` macro.,"Remove `SKIP_TEST_IF_NUM_DEVICES_LESS_THAN` macro.

Macros should be [avoided whenever
possible](https://google.github.io/styleguide/cppguide.html#Preprocessor_Macros).
The `SKIP_TEST_IF_NUM_DEVICES_LESS_THAN` macro does two things. It inserts a new
field `num_devices`, polluting the scope of the rest of the test. It also adds
an implicit/non-obvious dependency on the runner.

This patch removes the macro and switches any remaining uses to use
`HloRunnerInterface::device_count` with an explicit message instead.
",copybara-service[bot],2025-01-09 18:57:22+00:00,[],2025-01-09 18:57:22+00:00,,https://github.com/tensorflow/tensorflow/pull/84492,[],[],
2778529357,pull_request,open,,Add div/truediv support for bool-val tensor.,"To achieve compliance of tensorflow divide/truediv operator on boolean tensors with numpy, the solution is to cast tensors to int32.

Numpy divide behavior on boolean values
<img width=""836"" alt=""Screenshot 2025-01-09 at 10 29 34 AM"" src=""https://github.com/user-attachments/assets/5d32fca1-549f-48f2-98c1-d713027689bd"" />
Tensorflow divide/truediv behavior on boolean tensors
<img width=""1268"" alt=""Screenshot 2025-01-09 at 10 32 24 AM"" src=""https://github.com/user-attachments/assets/29c5ad07-0b66-4ed4-8ff0-e4f47d5fde49"" />
<img width=""1192"" alt=""Screenshot 2025-01-09 at 10 31 39 AM"" src=""https://github.com/user-attachments/assets/78f49e23-d8dd-4c13-a232-db19164a5bdf"" />
<img width=""1234"" alt=""Screenshot 2025-01-09 at 10 31 03 AM"" src=""https://github.com/user-attachments/assets/d7e6d77a-f938-4c02-872b-e1da7faf9513"" />
",codinglover222,2025-01-09 18:41:25+00:00,['gbaned'],2025-02-05 17:29:36+00:00,,https://github.com/tensorflow/tensorflow/pull/84491,"[('comp:ops', 'OPs related issues'), ('size:M', 'CL Change Size: Medium')]","[{'comment_id': 2581089345, 'issue_id': 2778529357, 'author': 'mihaimaruseac', 'body': 'It depends. If we want to keep the same type then a table would be better\r\n\r\n```\r\nT T T\r\nT F ?\r\nF F ?\r\nF T F\r\n```\r\n\r\nWhere `?` should probably be nans?\r\n\r\nIf we change type (convert to float) then conversion is useful.', 'created_at': datetime.datetime(2025, 1, 9, 19, 26, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2581095029, 'issue_id': 2778529357, 'author': 'cantonios', 'body': ""I question the value of this.  We don't necessarily want to blindly follow what numpy does.  What would a user actually need a truediv with booleans for?"", 'created_at': datetime.datetime(2025, 1, 9, 19, 30, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2581555341, 'issue_id': 2778529357, 'author': 'codinglover222', 'body': 'Since tensorflow tensors supports operations with numpy arrays, it is good to have numpy-like features in tensorflow. This may make tensorflow math more consistent with users that are familiar with numpy. Just my thoughts.', 'created_at': datetime.datetime(2025, 1, 10, 1, 27, 46, tzinfo=datetime.timezone.utc)}]","mihaimaruseac on (2025-01-09 19:26:33 UTC): It depends. If we want to keep the same type then a table would be better

```
T T T
T F ?
F F ?
F T F
```

Where `?` should probably be nans?

If we change type (convert to float) then conversion is useful.

cantonios on (2025-01-09 19:30:04 UTC): I question the value of this.  We don't necessarily want to blindly follow what numpy does.  What would a user actually need a truediv with booleans for?

codinglover222 (Issue Creator) on (2025-01-10 01:27:46 UTC): Since tensorflow tensors supports operations with numpy arrays, it is good to have numpy-like features in tensorflow. This may make tensorflow math more consistent with users that are familiar with numpy. Just my thoughts.

"
2778516813,pull_request,closed,,[xla:gpu] Rename gpu_clique_locking to gpu_cliques for consistency with XLA:CPU,"[xla:gpu] Rename gpu_clique_locking to gpu_cliques for consistency with XLA:CPU
",copybara-service[bot],2025-01-09 18:33:43+00:00,['ezhulenev'],2025-01-09 19:12:51+00:00,2025-01-09 19:12:50+00:00,https://github.com/tensorflow/tensorflow/pull/84490,[],[],
2778487211,pull_request,closed,,[xla:cpu] Migrate AllReduce to RendezvousSingle API,"[xla:cpu] Migrate AllReduce to RendezvousSingle API
",copybara-service[bot],2025-01-09 18:16:21+00:00,['ezhulenev'],2025-01-09 23:56:43+00:00,2025-01-09 23:56:43+00:00,https://github.com/tensorflow/tensorflow/pull/84489,[],[],
2778486334,pull_request,closed,,[xla:cpu] Migrate AllGather to RendezvousSingle API,"[xla:cpu] Migrate AllGather to RendezvousSingle API
",copybara-service[bot],2025-01-09 18:15:50+00:00,['ezhulenev'],2025-01-09 22:40:38+00:00,2025-01-09 22:40:36+00:00,https://github.com/tensorflow/tensorflow/pull/84488,[],[],
2778476428,pull_request,closed,,[XLA:GPU] Replace genrule by LLVM archive parser to load fatbin in tests,"[XLA:GPU] Replace genrule by LLVM archive parser to load fatbin in tests

generated .a library has a different name depending on cuda/rocm build.
",copybara-service[bot],2025-01-09 18:09:48+00:00,[],2025-01-13 08:34:43+00:00,2025-01-13 08:34:43+00:00,https://github.com/tensorflow/tensorflow/pull/84487,[],[],
2778436760,pull_request,closed,,[XLA:TPU] Disable memory space assignment pass via `exec_time_optimization_effort`.,"[XLA:TPU] Disable memory space assignment pass via `exec_time_optimization_effort`.
",copybara-service[bot],2025-01-09 17:51:52+00:00,[],2025-01-14 14:36:43+00:00,2025-01-14 14:36:42+00:00,https://github.com/tensorflow/tensorflow/pull/84486,[],[],
2778393203,pull_request,open,,Add more traces for custom calls.,"Add more traces for custom calls.
",copybara-service[bot],2025-01-09 17:33:04+00:00,[],2025-01-09 17:33:04+00:00,,https://github.com/tensorflow/tensorflow/pull/84485,[],[],
2778390898,pull_request,open,,Plumb memory space descriptions through IFRT.,"Plumb memory space descriptions through IFRT.
",copybara-service[bot],2025-01-09 17:31:50+00:00,[],2025-01-09 18:25:36+00:00,,https://github.com/tensorflow/tensorflow/pull/84484,[],[],
2778383867,pull_request,closed,,Add LiteRT accelerator API implementation.,"Add LiteRT accelerator API implementation.
",copybara-service[bot],2025-01-09 17:28:03+00:00,['qukhan'],2025-01-21 22:06:15+00:00,2025-01-21 22:06:14+00:00,https://github.com/tensorflow/tensorflow/pull/84483,[],[],
2778358234,pull_request,closed,,Integrate LLVM at llvm/llvm-project@644de6ad1c75,"Integrate LLVM at llvm/llvm-project@644de6ad1c75

Updates LLVM usage to match
[644de6ad1c75](https://github.com/llvm/llvm-project/commit/644de6ad1c75)
",copybara-service[bot],2025-01-09 17:14:14+00:00,['d0k'],2025-01-09 19:47:11+00:00,2025-01-09 19:47:09+00:00,https://github.com/tensorflow/tensorflow/pull/84482,[],[],
2778346712,pull_request,open,,Updated rules_python patch to get 3.13.1 python,cc @hawkinsp ,vfdev-5,2025-01-09 17:07:50+00:00,['gbaned'],2025-02-06 06:37:47+00:00,,https://github.com/tensorflow/tensorflow/pull/84481,"[('ready to pull', 'PR ready for merge process'), ('comp:xla', 'XLA'), ('size:M', 'CL Change Size: Medium')]",[],
2778341501,pull_request,closed,,Update users of moved TSL headers to use new location in XLA for `activity_watcher`,"Update users of moved TSL headers to use new location in XLA for `activity_watcher`
",copybara-service[bot],2025-01-09 17:05:01+00:00,['ddunl'],2025-01-09 18:25:55+00:00,2025-01-09 18:25:54+00:00,https://github.com/tensorflow/tensorflow/pull/84480,[],[],
2778279181,pull_request,closed,,PR #20911: [XLA:GPU] Update cudnn frontend version to 1.9,"PR #20911: [XLA:GPU] Update cudnn frontend version to 1.9

Imported from GitHub PR https://github.com/openxla/xla/pull/20911

cudnn frontend 1.9 is released, there are some new features that cudnn flash attention will incorporate, hence this PR.
* flex attention with arbitrary pointwise operations after softmax in cudnn flash attention graph.
* [sequence packing](https://github.com/openxla/xla/pull/20861) enhancement with reduced workspace size.

Release note: https://github.com/NVIDIA/cudnn-frontend/releases/tag/v1.9.0
Copybara import of the project:

--
07a0d7a6cdff107b3c69dd2a66fc2b247de056e7 by cjkkkk <ske@nvidia.com>:

update

Merging this change closes #20911

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20911 from Cjkkkk:update_cudnn_fe_1.9 07a0d7a6cdff107b3c69dd2a66fc2b247de056e7
",copybara-service[bot],2025-01-09 16:34:13+00:00,[],2025-01-11 02:39:20+00:00,2025-01-11 02:39:19+00:00,https://github.com/tensorflow/tensorflow/pull/84479,[],[],
2778148089,pull_request,closed,,[XLA:CPU] Remove unused stream_executor host code.,"[XLA:CPU] Remove unused stream_executor host code.
",copybara-service[bot],2025-01-09 15:34:32+00:00,[],2025-01-14 10:40:57+00:00,2025-01-14 10:40:57+00:00,https://github.com/tensorflow/tensorflow/pull/84478,[],[],
2778138949,pull_request,closed,,[XLA:GPU] Migrate collective bytes transfered to `BytesTransferred`.,"[XLA:GPU] Migrate collective bytes transfered to `BytesTransferred`.

`output_bytes_accessed` is not necessarily transferred by the network (for example reduce scatter breaks the rule)
",copybara-service[bot],2025-01-09 15:30:53+00:00,[],2025-01-15 15:31:31+00:00,2025-01-15 15:31:31+00:00,https://github.com/tensorflow/tensorflow/pull/84477,[],[],
2778126225,pull_request,closed,,[XLA:GPU] Fix reduce scatter transfered bytes.,"[XLA:GPU] Fix reduce scatter transfered bytes.
",copybara-service[bot],2025-01-09 15:25:51+00:00,[],2025-01-10 09:44:32+00:00,2025-01-10 09:44:31+00:00,https://github.com/tensorflow/tensorflow/pull/84476,[],[],
2778053398,pull_request,closed,,Update README.md,"This code provides the implementation of a custom version of the Adam optimizer called NonFusedAdam for TensorFlow. It inherits from its base optimizer class and implements the Adam algorithm with or without AMSGrad and other hyperparameters including learning rate, betas, and epsilon.

To use this optimizer:
1. Make sure TensorFlow is installed.
2. Save the optimizer code in a Python file (like custom_adam.py).
3. Import the optimizer in your script.
4. Build a model, compile it using NonFusedAdam, and train it with your dataset (such as MNIST).
5. Run the script to train the model and check its performance.",dewa-ai,2025-01-09 14:54:44+00:00,['gbaned'],2025-01-09 15:30:46+00:00,2025-01-09 15:30:46+00:00,https://github.com/tensorflow/tensorflow/pull/84475,"[('size:M', 'CL Change Size: Medium')]","[{'comment_id': 2580482282, 'issue_id': 2778053398, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/84475/checks?check_run_id=35377723208) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2025, 1, 9, 14, 54, 49, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2025-01-09 14:54:49 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/84475/checks?check_run_id=35377723208) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2778035469,pull_request,closed,,[XLA:GPU] move TransposeFolding after simplifier pipeline,"[XLA:GPU] move TransposeFolding after simplifier pipeline

Combination of DotDecompose, AlgebraicSimplifier, and TransposeFolding might never reach a fixed point and stuck in a rewrite loop.

Moving TransposeFolding in simplification-2 pipeline should result in a similar output as we keep the relative order.

Also:

- Added a few tests to detect / record cases when we run into rewrite loop

- Log warning when HloPassFix reaches iteraction limit as this is likely a similar bug
",copybara-service[bot],2025-01-09 14:47:19+00:00,['metaflow'],2025-01-14 09:36:46+00:00,2025-01-14 09:36:45+00:00,https://github.com/tensorflow/tensorflow/pull/84474,[],[],
2777831803,pull_request,closed,,Update to match upstream API change (NFC).,"Update to match upstream API change (NFC).

This method was renamed but staging function kept, switch to renamed variant.
",copybara-service[bot],2025-01-09 13:21:44+00:00,['jpienaar'],2025-01-15 18:52:21+00:00,2025-01-15 18:52:20+00:00,https://github.com/tensorflow/tensorflow/pull/84473,[],[],
2777821473,pull_request,closed,,Update to match upstream API change (NFC).,"Update to match upstream API change (NFC).

This method was renamed but staging function kept, switch to renamed variant.
",copybara-service[bot],2025-01-09 13:17:38+00:00,['jpienaar'],2025-01-09 20:25:54+00:00,2025-01-09 20:25:53+00:00,https://github.com/tensorflow/tensorflow/pull/84472,[],[],
2777803187,pull_request,closed,,Moving test from Triton patch file internally. The associated fix was obsolete when reorderValues function was removed. We still want to keep the test and remove the patch.,"Moving test from Triton patch file internally. The associated fix was obsolete when reorderValues function was removed. We still want to keep the test and remove the patch.
",copybara-service[bot],2025-01-09 13:10:32+00:00,[],2025-01-09 13:45:54+00:00,2025-01-09 13:45:54+00:00,https://github.com/tensorflow/tensorflow/pull/84471,[],[],
2777794932,pull_request,closed,,Update to match upstream API change (NFC).,"Update to match upstream API change (NFC).

This method was renamed but staging function kept, switch to renamed variant.
",copybara-service[bot],2025-01-09 13:07:11+00:00,['jpienaar'],2025-01-16 06:19:21+00:00,2025-01-16 06:19:20+00:00,https://github.com/tensorflow/tensorflow/pull/84470,[],[],
2777726894,pull_request,closed,,Adding vectorization support for atomic_rmw.,"Adding vectorization support for atomic_rmw.

Currently only supports f32 vectors of size 2 or 4. There is a bug in LLVM when lowering to PTX that lowers the vectorized atomic RMW incorrectly. For now, we scalarize, so effectively this is disabled. This should be followed-up with a direct lowering to PTX as a work-around.
",copybara-service[bot],2025-01-09 12:39:41+00:00,[],2025-01-14 17:08:12+00:00,2025-01-14 17:08:11+00:00,https://github.com/tensorflow/tensorflow/pull/84469,[],[],
2777714826,pull_request,closed,,Move xla::gpu::mlir_converter namespace to xla::emitters namespace.,"Move xla::gpu::mlir_converter namespace to xla::emitters namespace.

The code is not gpu specific. Also move the code to a corresponding directory.
",copybara-service[bot],2025-01-09 12:34:44+00:00,['akuegel'],2025-01-13 06:59:48+00:00,2025-01-13 06:59:47+00:00,https://github.com/tensorflow/tensorflow/pull/84468,[],[],
2777714340,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-09 12:34:35+00:00,[],2025-01-09 12:34:35+00:00,,https://github.com/tensorflow/tensorflow/pull/84467,[],[],
2777431018,pull_request,closed,,[xla:cpu:benchmarks] Add scripts to run Gemma2 Keras model.,"[xla:cpu:benchmarks] Add scripts to run Gemma2 Keras model.
",copybara-service[bot],2025-01-09 10:29:14+00:00,['penpornk'],2025-01-09 16:14:17+00:00,2025-01-09 16:14:17+00:00,https://github.com/tensorflow/tensorflow/pull/84465,[],[],
2777390817,pull_request,closed,,[XLA:GPU] NFC: a few small polishing touches.,"[XLA:GPU] NFC: a few small polishing touches.
",copybara-service[bot],2025-01-09 10:10:22+00:00,['chsigg'],2025-01-13 13:00:16+00:00,2025-01-13 13:00:15+00:00,https://github.com/tensorflow/tensorflow/pull/84464,[],[],
2777385202,pull_request,closed,,Fix typos in multiple documentation strings,"Hi, Team
I observed few typos in the documentation strings and I have fixed those typos so please do the needful. Thank you.",Venkat6871,2025-01-09 10:07:53+00:00,['gbaned'],2025-01-15 01:21:16+00:00,2025-01-09 21:01:51+00:00,https://github.com/tensorflow/tensorflow/pull/84463,"[('ready to pull', 'PR ready for merge process'), ('comp:dist-strat', 'Distribution Strategy related issues'), ('size:S', 'CL Change Size: Small')]",[],
2777290464,pull_request,open,,Update GraphDef version to 2102.,"Update GraphDef version to 2102.
",copybara-service[bot],2025-01-09 09:30:10+00:00,[],2025-01-09 09:30:10+00:00,,https://github.com/tensorflow/tensorflow/pull/84462,[],[],
2777283358,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-09 09:27:30+00:00,[],2025-01-09 13:10:40+00:00,2025-01-09 13:10:39+00:00,https://github.com/tensorflow/tensorflow/pull/84461,[],[],
2777274015,pull_request,closed,,-,,Nana16794625829,2025-01-09 09:23:28+00:00,['gbaned'],2025-01-09 09:25:03+00:00,2025-01-09 09:24:05+00:00,https://github.com/tensorflow/tensorflow/pull/84459,"[('size:M', 'CL Change Size: Medium')]","[{'comment_id': 2579561086, 'issue_id': 2777274015, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/84459/checks?check_run_id=35360600278) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2025, 1, 9, 9, 23, 32, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2025-01-09 09:23:32 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/84459/checks?check_run_id=35360600278) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2777264258,pull_request,closed,,PR #21163: [GPU] Redefine the flag xla_gpu_cudnn_gemm_fusion_level.,"PR #21163: [GPU] Redefine the flag xla_gpu_cudnn_gemm_fusion_level.

Imported from GitHub PR https://github.com/openxla/xla/pull/21163

The levels defined so far were used for testing/benchmarking. The new definitions will help the architecture-targeted deployment of the feature.

This change also lets the relevant tests run manually on Ampere+ GPUs - previously they were skipped before Hopper.
Copybara import of the project:

--
6bcca3cead59f584a6f7d69e2b56aeda94e97414 by Ilia Sergachev <isergachev@nvidia.com>:

[GPU] Redefine the flag xla_gpu_cudnn_gemm_fusion_level.

The levels defined so far were used for testing/benchmarking. The new
definitions will help the architecture-targeted deployment of the
feature.

This change also lets the relevant tests run manually on Ampere+ GPUs -
previously they were skipped before Hopper.

--
91ea9520de83a75b2c9d32f513d0d3d6044ca8dd by Ilia Sergachev <isergachev@nvidia.com>:

add missing build dependency

Merging this change closes #21163

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21163 from openxla:cudnn_gemm_redefine_levels 91ea9520de83a75b2c9d32f513d0d3d6044ca8dd
",copybara-service[bot],2025-01-09 09:18:42+00:00,[],2025-01-11 02:47:31+00:00,2025-01-11 02:47:31+00:00,https://github.com/tensorflow/tensorflow/pull/84458,[],[],
2777263125,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-09 09:18:08+00:00,[],2025-01-09 12:57:46+00:00,2025-01-09 12:57:45+00:00,https://github.com/tensorflow/tensorflow/pull/84457,[],[],
2777259302,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-09 09:16:11+00:00,[],2025-01-10 09:11:54+00:00,2025-01-10 09:11:53+00:00,https://github.com/tensorflow/tensorflow/pull/84456,[],[],
2777255088,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-09 09:14:26+00:00,[],2025-01-10 10:16:32+00:00,2025-01-10 10:16:32+00:00,https://github.com/tensorflow/tensorflow/pull/84455,[],[],
2777254455,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-09 09:14:16+00:00,[],2025-01-09 13:37:16+00:00,2025-01-09 13:37:16+00:00,https://github.com/tensorflow/tensorflow/pull/84454,[],[],
2777253854,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-09 09:13:59+00:00,[],2025-01-09 09:13:59+00:00,,https://github.com/tensorflow/tensorflow/pull/84453,[],[],
2777251250,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-09 09:12:38+00:00,[],2025-01-09 13:23:03+00:00,2025-01-09 13:23:02+00:00,https://github.com/tensorflow/tensorflow/pull/84452,[],[],
2777249812,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-09 09:11:54+00:00,[],2025-01-09 13:08:18+00:00,,https://github.com/tensorflow/tensorflow/pull/84451,[],[],
2777214240,pull_request,closed,,PR #21166: [DOC] Fix a link in the documentation.,"PR #21166: [DOC] Fix a link in the documentation.

Imported from GitHub PR https://github.com/openxla/xla/pull/21166


Copybara import of the project:

--
b939d5aea471e4b267a806b19102b6d56a7abe0a by Ilia Sergachev <isergachev@nvidia.com>:

[DOC] Fix a link in the documentation.

Merging this change closes #21166

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21166 from openxla:fix_doc b939d5aea471e4b267a806b19102b6d56a7abe0a
",copybara-service[bot],2025-01-09 08:56:31+00:00,[],2025-01-09 10:15:08+00:00,2025-01-09 10:15:07+00:00,https://github.com/tensorflow/tensorflow/pull/84450,[],[],
2777192938,pull_request,closed,,PR #21175: [DOC] Fix a mistype.,"PR #21175: [DOC] Fix a mistype.

Imported from GitHub PR https://github.com/openxla/xla/pull/21175


Copybara import of the project:

--
caaf17448ae8dade929d728852093ec82384337b by Ilia Sergachev <isergachev@nvidia.com>:

[DOC] Fix a mistype.

Merging this change closes #21175

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21175 from openxla:fix_mistype caaf17448ae8dade929d728852093ec82384337b
",copybara-service[bot],2025-01-09 08:46:23+00:00,[],2025-01-09 10:25:58+00:00,2025-01-09 10:25:57+00:00,https://github.com/tensorflow/tensorflow/pull/84449,[],[],
2777189113,pull_request,closed,,PR #21123: Disable cuDNN fusions explicitly in tests that are testing the Triton path,"PR #21123: Disable cuDNN fusions explicitly in tests that are testing the Triton path

Imported from GitHub PR https://github.com/openxla/xla/pull/21123

cuDNN fusions are OFF by default, and some tests that are testing the Triton codegen path implicitly rely on this. It is best to turn off cuDNN fusions explicitly in these tests, e.g., NVIDIA has internal builds that turn on cuDNN fusions and these tests suddenly start to fail in CI.
Copybara import of the project:

--
ab9827658a7bfbc68620b920525360e5df9dfaf2 by Dimitris Vardoulakis <dvardoulakis@nvidia.com>:

Disable cuDNN fusions explicitly in tests that are testing the Triton path.

Merging this change closes #21123

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21123 from dimvar:disable-cudnn-in-triton-tests ab9827658a7bfbc68620b920525360e5df9dfaf2
",copybara-service[bot],2025-01-09 08:44:39+00:00,[],2025-01-11 03:00:14+00:00,2025-01-11 03:00:14+00:00,https://github.com/tensorflow/tensorflow/pull/84448,[],[],
2777057320,pull_request,closed,,Generalize `GetFirstMergeableDimForSortOperand` and rename it as `GetFirstTargetDimToMoveShardingTiles`.,"Generalize `GetFirstMergeableDimForSortOperand` and rename it as `GetFirstTargetDimToMoveShardingTiles`.

`GetFirstTargetDimToMoveShardingTiles` can be used for moving the sharding tiles from a source dimension to a target dimension when the source dimension and target dimension are different and the size of target dimension is divisible by the merged tile size. This util function will be used in the dimensions that need replication in the partitioner.

This cl has no behavior change. We will use this util function to support
1. Concat dimension in concat operations
2. Slice dimensions in dynamic-slice operations
",copybara-service[bot],2025-01-09 07:39:10+00:00,[],2025-01-09 10:03:18+00:00,2025-01-09 10:03:17+00:00,https://github.com/tensorflow/tensorflow/pull/84447,[],[],
2777010930,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-09 07:15:26+00:00,[],2025-01-09 12:30:44+00:00,2025-01-09 12:30:42+00:00,https://github.com/tensorflow/tensorflow/pull/84446,[],[],
2777006295,pull_request,open,,PR #20340: Fix missing template value,"PR #20340: Fix missing template value

Imported from GitHub PR https://github.com/openxla/xla/pull/20340

Fixes a bug introduced in this change: https://github.com/google/tsl/pull/2944

The change makes use of a template variable `%{compiler}`, that is not defined for this file. This causes the `-fno-canonical-system-headers` option to be set for Clang builds, and Clang will fail with an error about that command line flag not being defined.
Copybara import of the project:

--
75a3d3fbcf2ead55df3872aa80ff21ac3dd9336c by Charles Hofer <Charles.Hofer@amd.com>:

Fix missing template value

--
e08537b09200b0037db7a05780dea0d525399376 by Charles Hofer <Charles.Hofer@amd.com>:

Change flag to compiler_is_clang

--
373f359cbd8d02ee850d98fed92a7bbca4a09c1b by Charles Hofer <Charles.Hofer@amd.com>:

Fix typo

--
2be3c309d05f93a48dd9fdd06af8159108920516 by Harsha HS <Harsha.HavanurShamsundara@amd.com>:

[ROCm] Add cuda-only tags for nvidia profiler test

Merging this change closes #20340

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20340 from ROCm:fix-missing-template-value 2be3c309d05f93a48dd9fdd06af8159108920516
",copybara-service[bot],2025-01-09 07:13:00+00:00,[],2025-01-09 07:13:00+00:00,,https://github.com/tensorflow/tensorflow/pull/84445,[],[],
2776998117,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-09 07:08:44+00:00,[],2025-01-10 07:07:59+00:00,2025-01-10 07:07:58+00:00,https://github.com/tensorflow/tensorflow/pull/84444,[],[],
2776956980,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-09 06:42:30+00:00,[],2025-01-09 06:42:30+00:00,,https://github.com/tensorflow/tensorflow/pull/84442,[],[],
2776944015,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-09 06:33:04+00:00,[],2025-01-09 06:33:04+00:00,,https://github.com/tensorflow/tensorflow/pull/84441,[],[],
2776942696,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-09 06:32:07+00:00,[],2025-01-10 11:14:04+00:00,2025-01-10 11:14:03+00:00,https://github.com/tensorflow/tensorflow/pull/84440,[],[],
2776929658,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-09 06:21:39+00:00,[],2025-01-13 13:24:37+00:00,2025-01-13 13:24:36+00:00,https://github.com/tensorflow/tensorflow/pull/84439,[],[],
2776929296,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-09 06:21:22+00:00,[],2025-01-14 09:27:29+00:00,,https://github.com/tensorflow/tensorflow/pull/84438,[],[],
2776929154,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-09 06:21:15+00:00,[],2025-01-09 06:21:15+00:00,,https://github.com/tensorflow/tensorflow/pull/84437,[],[],
2776927906,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-09 06:20:30+00:00,[],2025-01-09 06:20:30+00:00,,https://github.com/tensorflow/tensorflow/pull/84436,[],[],
2776927232,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-09 06:19:55+00:00,[],2025-01-10 07:27:12+00:00,2025-01-10 07:27:11+00:00,https://github.com/tensorflow/tensorflow/pull/84435,[],[],
2776926159,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-09 06:19:04+00:00,[],2025-01-10 06:54:25+00:00,2025-01-10 06:54:24+00:00,https://github.com/tensorflow/tensorflow/pull/84434,[],[],
2776922238,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-09 06:15:47+00:00,[],2025-01-10 06:49:11+00:00,2025-01-10 06:49:10+00:00,https://github.com/tensorflow/tensorflow/pull/84433,[],[],
2776921170,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-09 06:14:57+00:00,[],2025-01-10 10:42:08+00:00,2025-01-10 10:42:07+00:00,https://github.com/tensorflow/tensorflow/pull/84432,[],[],
2776910717,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-09 06:06:32+00:00,[],2025-01-09 06:06:32+00:00,,https://github.com/tensorflow/tensorflow/pull/84431,[],[],
2776884682,pull_request,closed,,[xla] Delete unused Rendezvous implementation,"[xla] Delete unused Rendezvous implementation
",copybara-service[bot],2025-01-09 05:43:44+00:00,['ezhulenev'],2025-01-10 05:19:49+00:00,2025-01-10 05:19:43+00:00,https://github.com/tensorflow/tensorflow/pull/84430,[],[],
2776876085,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-09 05:36:07+00:00,[],2025-01-09 05:36:07+00:00,,https://github.com/tensorflow/tensorflow/pull/84429,[],[],
2776730552,pull_request,closed,,Internal change only,"Internal change only
",copybara-service[bot],2025-01-09 03:02:00+00:00,[],2025-01-09 17:38:28+00:00,2025-01-09 17:38:26+00:00,https://github.com/tensorflow/tensorflow/pull/84428,[],[],
2776712039,pull_request,closed,,Add original cp name prefix to the send/receives instructions for better readability.,"Add original cp name prefix to the send/receives instructions for better readability.
",copybara-service[bot],2025-01-09 02:41:23+00:00,[],2025-01-11 00:27:09+00:00,2025-01-11 00:27:08+00:00,https://github.com/tensorflow/tensorflow/pull/84427,[],[],
2776681768,pull_request,closed,,Minor code simplification.,"Minor code simplification.

There is only one call to `TfLiteDelegateCopyFromBufferHandleInternal`,
which passes in `t` for the `tensor` parameter and `t->delegate` for the `delegate` parameter, so inside this function, `tensor->delegate` and `delegate` are equivalent expressions that evaluate to the same value.  But referencing `delegate` rather
than `tensor->delegate` is simpler and more readable here, and makes the nullness check match the dereference on the following line, and is more consistent with the other functions in this file.  So this change modifies the code to use `delegate`
rather than `tensor->delegate`.
",copybara-service[bot],2025-01-09 02:11:06+00:00,[],2025-01-09 05:52:24+00:00,2025-01-09 05:52:24+00:00,https://github.com/tensorflow/tensorflow/pull/84426,[],[],
2776680695,pull_request,closed,,Make PJRTArray::Create validate the create-request for addressable devices only.,"Make PJRTArray::Create validate the create-request for addressable devices only.
",copybara-service[bot],2025-01-09 02:09:58+00:00,[],2025-01-09 04:33:18+00:00,2025-01-09 04:33:17+00:00,https://github.com/tensorflow/tensorflow/pull/84425,[],[],
2776655722,pull_request,closed,,Increase wheel limit size up to 270M for a temporary nightlies fix.,"Increase wheel limit size up to 270M for a temporary nightlies fix.
",copybara-service[bot],2025-01-09 01:16:42+00:00,['rtg0795'],2025-01-09 19:25:30+00:00,2025-01-09 19:25:29+00:00,https://github.com/tensorflow/tensorflow/pull/84424,[],[],
2776651137,pull_request,closed,,Internal change. Need to write more words to keep the presubmit happy.,"Internal change. Need to write more words to keep the presubmit happy.
",copybara-service[bot],2025-01-09 01:11:59+00:00,['jimlinntu'],2025-01-13 22:58:53+00:00,2025-01-13 22:58:52+00:00,https://github.com/tensorflow/tensorflow/pull/84423,[],[],
2776650219,pull_request,closed,,Remove unused free_visitors from DeviceMemAllocator.,"Remove unused free_visitors from DeviceMemAllocator.
",copybara-service[bot],2025-01-09 01:10:54+00:00,[],2025-01-09 20:48:30+00:00,2025-01-09 20:48:29+00:00,https://github.com/tensorflow/tensorflow/pull/84422,[],[],
2776648953,pull_request,closed,,Remove unused MemoryTypeString function.,"Remove unused MemoryTypeString function.
",copybara-service[bot],2025-01-09 01:09:21+00:00,[],2025-01-09 19:01:55+00:00,2025-01-09 19:01:54+00:00,https://github.com/tensorflow/tensorflow/pull/84421,[],[],
2776629901,pull_request,closed,,Extend MTK dispatch API to support DMA-BUF buffers,"Extend MTK dispatch API to support DMA-BUF buffers
",copybara-service[bot],2025-01-09 00:48:34+00:00,[],2025-01-09 16:36:46+00:00,2025-01-09 16:36:45+00:00,https://github.com/tensorflow/tensorflow/pull/84420,[],[],
2776615571,pull_request,closed,,Port array_elementwise_ops_test to derive from `HloTestBase`.,"Port array_elementwise_ops_test to derive from `HloTestBase`.
",copybara-service[bot],2025-01-09 00:38:40+00:00,[],2025-01-16 20:34:33+00:00,2025-01-16 20:34:32+00:00,https://github.com/tensorflow/tensorflow/pull/84419,[],[],
2776611612,pull_request,closed,,Port convert_test to derive from `HloTestBase`.,"Port convert_test to derive from `HloTestBase`.
",copybara-service[bot],2025-01-09 00:36:01+00:00,[],2025-01-16 22:52:31+00:00,2025-01-16 22:52:30+00:00,https://github.com/tensorflow/tensorflow/pull/84418,[],[],
2776610569,pull_request,closed,,Port convolution_test to derive from `HloTestBase`.,"Port convolution_test to derive from `HloTestBase`.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/18838 from terryysun:terryysun/grouped_cp e5ad9d9b601ada1c3984f34967c98574061bd043
",copybara-service[bot],2025-01-09 00:35:24+00:00,[],2025-01-16 21:19:16+00:00,2025-01-16 21:19:15+00:00,https://github.com/tensorflow/tensorflow/pull/84417,[],[],
2776610008,pull_request,open,,Port pad_test to derive from `HloTestBase`.,"Port pad_test to derive from `HloTestBase`.
",copybara-service[bot],2025-01-09 00:35:05+00:00,[],2025-01-09 00:35:05+00:00,,https://github.com/tensorflow/tensorflow/pull/84416,[],[],
2776606976,pull_request,closed,,Port custom_call_test to derive from `HloTestBase`.,"Port custom_call_test to derive from `HloTestBase`.
",copybara-service[bot],2025-01-09 00:33:17+00:00,[],2025-01-16 03:20:47+00:00,2025-01-16 03:20:46+00:00,https://github.com/tensorflow/tensorflow/pull/84415,[],[],
2776605451,pull_request,closed,,Add `ClientLibraryTestRunnerMixin`.,"Add `ClientLibraryTestRunnerMixin`.

`ClientLibraryTestRunnerMixin` is a sort-of replacement for
`ClientLibraryTestBase` to run tests on top of `HloTestBase` and friends (e.g.
`HloRunnerAgnosticTestBase`).  This is to enable a future migration to PjRt and
TFRT.

Due to `ClientLibraryTestBase` containing many client-specific calls, moving
tests is not as trivial as simply dropping in a new base class. The idea with
this class is just to make that migration simpler and to reduce (but not
eliminate) the amount of code changes required in tests.

Migration timeline for `ClientLibraryTestBase` tests:

1. `class XYZ: ClientLibraryTestBase` (starting point)
2. `class XYZ: ClientLibraryTestRunnerMixin<HloTestBase>` (intermediate state)
3. `class XYZ: ClientLibraryTestRunnerMixin<HloPjRtReferenceMixin<HloPjRtTestBase>>` (end state)
",copybara-service[bot],2025-01-09 00:32:15+00:00,[],2025-01-13 19:37:16+00:00,2025-01-13 19:37:15+00:00,https://github.com/tensorflow/tensorflow/pull/84414,[],[],
2776561854,pull_request,closed,,Improve comment in ShapeUtil,"Improve comment in ShapeUtil
",copybara-service[bot],2025-01-08 23:58:46+00:00,['SandSnip3r'],2025-01-09 00:39:52+00:00,2025-01-09 00:39:52+00:00,https://github.com/tensorflow/tensorflow/pull/84411,[],[],
2776549859,pull_request,closed,,Use const reference to context instead of universal reference.,"Use const reference to context instead of universal reference.
",copybara-service[bot],2025-01-08 23:45:45+00:00,[],2025-01-09 19:39:35+00:00,2025-01-09 19:39:34+00:00,https://github.com/tensorflow/tensorflow/pull/84410,[],[],
2776526612,pull_request,open,,Integrate LLVM at llvm/llvm-project@ac08f0dfef27,"Integrate LLVM at llvm/llvm-project@ac08f0dfef27

Updates LLVM usage to match
[ac08f0dfef27](https://github.com/llvm/llvm-project/commit/ac08f0dfef27)
",copybara-service[bot],2025-01-08 23:24:00+00:00,[],2025-01-08 23:24:00+00:00,,https://github.com/tensorflow/tensorflow/pull/84409,[],[],
2776489382,pull_request,closed,,[xla:cpu] Migrate AllToAll to RendezvousSingle API,"[xla:cpu] Migrate AllToAll to RendezvousSingle API
",copybara-service[bot],2025-01-08 22:47:52+00:00,['ezhulenev'],2025-01-09 21:47:14+00:00,2025-01-09 21:47:14+00:00,https://github.com/tensorflow/tensorflow/pull/84408,[],[],
2776456594,pull_request,open,,[HLO Componentization] Populate hlo/testlib sub-component (Phase II).,"[HLO Componentization] Populate hlo/testlib sub-component (Phase II).

This CL takes care of
1. Migrating external projects dependencies from

```
tensorflow/compiler/xla:test
tensorflow/compiler/xla:test_helpers
tensorflow/compiler/xla/service:pattern_matcher_gmock
```

to `tensorflow/compiler/xla/hlo/testlib:*`
",copybara-service[bot],2025-01-08 22:21:16+00:00,['sdasgup3'],2025-01-10 18:24:23+00:00,,https://github.com/tensorflow/tensorflow/pull/84407,[],[],
2776439076,pull_request,closed,,"#tf-data For an empty `from_list`, update the error message to suggest a workaround.","#tf-data For an empty `from_list`, update the error message to suggest a workaround.
",copybara-service[bot],2025-01-08 22:06:50+00:00,['mpcallanan'],2025-01-08 22:36:51+00:00,2025-01-08 22:36:50+00:00,https://github.com/tensorflow/tensorflow/pull/84406,[],[],
2776437221,pull_request,closed,,[xla:cpu] Migrate CollectivePermute to RendezvousSingle API,"[xla:cpu] Migrate CollectivePermute to RendezvousSingle API

Migrate from deprecated rendezvous APIs to the new one.
",copybara-service[bot],2025-01-08 22:05:35+00:00,['ezhulenev'],2025-01-09 20:09:58+00:00,2025-01-09 20:09:58+00:00,https://github.com/tensorflow/tensorflow/pull/84405,[],[],
2776399896,pull_request,closed,,Reverts changelist 546034127,"Reverts changelist 546034127
",copybara-service[bot],2025-01-08 21:39:22+00:00,[],2025-01-09 00:58:52+00:00,2025-01-09 00:58:51+00:00,https://github.com/tensorflow/tensorflow/pull/84404,[],[],
2776361244,pull_request,closed,,[xla:collectives] Remove redundant nranks argument from collectives API,"[xla:collectives] Remove redundant nranks argument from collectives API
",copybara-service[bot],2025-01-08 21:13:58+00:00,['ezhulenev'],2025-01-09 17:58:09+00:00,2025-01-09 17:58:08+00:00,https://github.com/tensorflow/tensorflow/pull/84403,[],[],
2776347841,pull_request,closed,,[xla:cpu] Consolidate all XLA:CPU collectives under backends/cpu/collectives,"[xla:cpu] Consolidate all XLA:CPU collectives under backends/cpu/collectives
",copybara-service[bot],2025-01-08 21:04:57+00:00,['ezhulenev'],2025-01-09 16:47:33+00:00,2025-01-09 16:47:30+00:00,https://github.com/tensorflow/tensorflow/pull/84402,[],[],
2776315127,pull_request,open,,PR #20808: [GSPMD] Partitions collective permute instructions in manual sharding group.,"PR #20808: [GSPMD] Partitions collective permute instructions in manual sharding group.

Imported from GitHub PR https://github.com/openxla/xla/pull/20808

This is a small fix in GSPMD partitioning for partitioning collective permutes instructions added in manual sharding group.

In JAX, we can add `ppermute` instruction in shard_map. In cases where we have shard_map with auto axes specified, collective permuting an operand even with the same sharding will end up with an `all-gather` and then collective permute, which leads to inefficient collectives. The correct and efficient way is to partition the collective permute as an element-wise op.

The unit test added provides a repro. Also, the JAX unit test in https://github.com/jax-ml/jax/blob/fa9c7edf736516052df6eab22947bc627d0deca3/tests/shard_map_test.py#L2167 gives a real-world JAX example.
Copybara import of the project:

--
8ee6ecd51f6e4aae8e3d92a6a439a60f53ab02ae by Yunlong Liu <yunlongl@x.ai>:

A hacky fix on partitioning collective permute.

--
e50e87696defb290f7561a7808ee42ebbc11e144 by Yunlong Liu <yunlongl@x.ai>:

Local change.

--
84eb38597c783a4488774823c2c464296a8c54c7 by Yunlong Liu <yunlongl@x.ai>:

Simplifies sharding in tests.

Merging this change closes #20808

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20808 from yliu120:cp_sharding_2 84eb38597c783a4488774823c2c464296a8c54c7
",copybara-service[bot],2025-01-08 20:45:24+00:00,[],2025-01-08 20:45:24+00:00,,https://github.com/tensorflow/tensorflow/pull/84401,[],[],
2776270834,pull_request,closed,,PR #19066: [XLA:CPU][oneDNN] Handle oneDNN scalar,"PR #19066: [XLA:CPU][oneDNN] Handle oneDNN scalar

Imported from GitHub PR https://github.com/openxla/xla/pull/19066

This PR makes sure oneDNN handles the scalar properly.
Copybara import of the project:

--
2fb157a16c0ea3ff29a39363ef83510edabf3a13 by Mahmoud Abuzaina <mahmoud.abuzaina@intel.com>:

Handle oneDNN scalar

--
77a39b6c047a797bb7db1ed6361440e8e6a6345a by Mahmoud Abuzaina <mahmoud.abuzaina@intel.com>:

Addressed review comments

--
32b5aba9ee009b3fc025825e705fa0dae49af9d6 by Mahmoud Abuzaina <mahmoud.abuzaina@intel.com>:

Return output instead of having parameter

--
576e244530ce0698de0b7137d8e93965fef9d528 by Mahmoud Abuzaina <mahmoud.abuzaina@intel.com>:

Unpack the pair return

Merging this change closes #19066

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19066 from Intel-tensorflow:mabuzain/handle-onednn-scalar 576e244530ce0698de0b7137d8e93965fef9d528
",copybara-service[bot],2025-01-08 20:22:59+00:00,[],2025-01-11 03:19:42+00:00,2025-01-11 03:19:41+00:00,https://github.com/tensorflow/tensorflow/pull/84400,[],[],
2776238202,pull_request,closed,,[xla:cpu] Replace xla::cpu::CollectivesInterface with xla::cpu::CpuCollectives,"[xla:cpu] Replace xla::cpu::CollectivesInterface with xla::cpu::CpuCollectives
",copybara-service[bot],2025-01-08 20:07:27+00:00,['ezhulenev'],2025-01-09 15:21:25+00:00,2025-01-09 15:21:24+00:00,https://github.com/tensorflow/tensorflow/pull/84399,[],[],
2776211475,pull_request,closed,,#tf-data Remove obsolete todo.,"#tf-data Remove obsolete todo.
",copybara-service[bot],2025-01-08 19:57:41+00:00,['mpcallanan'],2025-01-08 20:30:04+00:00,2025-01-08 20:30:03+00:00,https://github.com/tensorflow/tensorflow/pull/84398,[],[],
2776199503,pull_request,closed,,#tf-data Remove obsolete todo.,"#tf-data Remove obsolete todo.
",copybara-service[bot],2025-01-08 19:51:57+00:00,['mpcallanan'],2025-01-08 21:33:11+00:00,2025-01-08 21:33:10+00:00,https://github.com/tensorflow/tensorflow/pull/84397,[],[],
2776181976,pull_request,closed,,#tf-data-service Remove obsolete todo.,"#tf-data-service Remove obsolete todo.
",copybara-service[bot],2025-01-08 19:41:24+00:00,['mpcallanan'],2025-01-08 20:20:04+00:00,2025-01-08 20:20:02+00:00,https://github.com/tensorflow/tensorflow/pull/84396,[],[],
2776120453,pull_request,open,,Handle negative begin/end/end-mask values in StridedSlice.,"Handle negative begin/end/end-mask values in StridedSlice.
",copybara-service[bot],2025-01-08 19:07:35+00:00,[],2025-01-14 00:34:25+00:00,,https://github.com/tensorflow/tensorflow/pull/84395,[],[],
2776104806,pull_request,closed,,#tf-data-service Remove obsolete todo.,"#tf-data-service Remove obsolete todo.
",copybara-service[bot],2025-01-08 18:57:58+00:00,['mpcallanan'],2025-01-08 19:30:27+00:00,2025-01-08 19:30:26+00:00,https://github.com/tensorflow/tensorflow/pull/84394,[],[],
2776103607,pull_request,closed,,Rewrite `Reshard(HloSharding::Replicate())` as `Replicate()` for `PartitionedHlo`.,"Rewrite `Reshard(HloSharding::Replicate())` as `Replicate()` for `PartitionedHlo`.
",copybara-service[bot],2025-01-08 18:57:23+00:00,[],2025-01-09 16:58:49+00:00,2025-01-09 16:58:48+00:00,https://github.com/tensorflow/tensorflow/pull/84393,[],[],
2775983866,pull_request,open,,Integrate LLVM at llvm/llvm-project@f69585235ec8,"Integrate LLVM at llvm/llvm-project@f69585235ec8

Updates LLVM usage to match
[f69585235ec8](https://github.com/llvm/llvm-project/commit/f69585235ec8)
",copybara-service[bot],2025-01-08 17:52:10+00:00,['d0k'],2025-01-08 17:52:11+00:00,,https://github.com/tensorflow/tensorflow/pull/84392,[],[],
2775962471,pull_request,open,,[XLA:Test] Attach default device assignment in base test classes.,"[XLA:Test] Attach default device assignment in base test classes.
",copybara-service[bot],2025-01-08 17:41:50+00:00,['Tongfei-Guo'],2025-01-31 00:48:04+00:00,,https://github.com/tensorflow/tensorflow/pull/84391,[],[],
2775948007,pull_request,closed,,Split `RunAndCompare` with reference backend functionality into a mixin.,"Split `RunAndCompare` with reference backend functionality into a mixin.

Many users don't require `RunAndCompare` functionality, but are forced to select
and initialize a reference backend anyway.

With this change, users can opt to extend their specific
`HloRunnerAgnosticTestBase` implementation to add `RunAndCompare` functionality.
The mixin acts as a wrapper around any `HloRunnerAgnosticTestBase`
implementation, allowing a high degree of customization.
",copybara-service[bot],2025-01-08 17:34:31+00:00,[],2025-01-10 18:11:15+00:00,2025-01-10 18:11:14+00:00,https://github.com/tensorflow/tensorflow/pull/84390,[],[],
2775916992,pull_request,closed,,[XLA:GPU] Make dnn_compiled_graphs as bytes. This can fix parsing errors from invalid utf-8 data.,"[XLA:GPU] Make dnn_compiled_graphs as bytes. This can fix parsing errors from invalid utf-8 data.
",copybara-service[bot],2025-01-08 17:18:26+00:00,[],2025-01-09 00:49:36+00:00,2025-01-09 00:49:35+00:00,https://github.com/tensorflow/tensorflow/pull/84389,[],[],
2775911999,pull_request,closed,,[XLA:GPU] Model output_bytes_accessed for collectives.,"[XLA:GPU] Model output_bytes_accessed for collectives.
",copybara-service[bot],2025-01-08 17:15:41+00:00,[],2025-01-09 11:45:11+00:00,2025-01-09 11:45:11+00:00,https://github.com/tensorflow/tensorflow/pull/84388,[],[],
2775909228,pull_request,closed,,[XLA:GPU] Use output_bytes_accessed in SoL latency estimator.,"[XLA:GPU] Use output_bytes_accessed in SoL latency estimator.
",copybara-service[bot],2025-01-08 17:14:16+00:00,[],2025-01-09 12:41:17+00:00,2025-01-09 12:41:16+00:00,https://github.com/tensorflow/tensorflow/pull/84387,[],[],
2775896831,pull_request,closed,,[xla:gpu] Only run XLA Triton passes on XLA fusions.,"[xla:gpu] Only run XLA Triton passes on XLA fusions.
",copybara-service[bot],2025-01-08 17:07:23+00:00,[],2025-01-09 11:33:28+00:00,2025-01-09 11:33:28+00:00,https://github.com/tensorflow/tensorflow/pull/84386,[],[],
2775780783,pull_request,closed,,[XLA:GPU][Emitters] Fix a typo in vectorize_loads_stores.mlir,"[XLA:GPU][Emitters] Fix a typo in vectorize_loads_stores.mlir
",copybara-service[bot],2025-01-08 16:11:25+00:00,['pifon2a'],2025-01-08 17:42:00+00:00,2025-01-08 17:41:59+00:00,https://github.com/tensorflow/tensorflow/pull/84385,[],[],
2775766757,pull_request,open,,"[XLA] Add support for sharing computations to XlaBuilder, and use it in MHLO to HLO conversion.","[XLA] Add support for sharing computations to XlaBuilder, and use it in MHLO to HLO conversion.

Currently XlaBuilder, by construction, always creates a copy of any subcomputations that are passed to, say, Call. If the same computation is passed to multiple Calls, then it is duplicated each time.

This change changes XlaBuilder to allow creating a subcomputation once and reusing it multiple times, and changes the MHLO to HLO conversion code to use it.

To achieve reuse, we need to be able to refer to a computation embedded in an XlaBuilder multiple times. The current XlaComputation type passed into builder methods contains a copy of the computation; instead we need something more like a reference. This change introduces a new handle class XlaComputationId that refers to a computation embedded in an XlaBuilder or its parents. A inner computation can be created once and used multiple times by passing the resulting XlaComputationId where previously an XlaComputation was expected.

To maintain backwards compatibility with existing XlaBuilder users, we keep overloads that accept `XlaComputation` as well, deferring any migration of those users to a possible future change.

We add two ways to build an XlaComputationId:
* a method `XlaBuilder::AddSubComputation`, which adds an `XlaComputation` to a builder and returns its ID. This method primarily exists for backwards compatibility.
* a method `XlaBuilder::BuildSubComputation`, which adds the contents of a sub-builder to its parent as an an embedded computation.

as well as a helper that turns out to be helpful for the MHLO->HLO conversion case where we want to treat the entry function the same way as other functions;
* a new overload `XlaBuilder::Build(XlaComputationId)` that can be used to convert an embedded computation into an `XlaComputation`.
",copybara-service[bot],2025-01-08 16:05:00+00:00,[],2025-01-15 16:29:46+00:00,,https://github.com/tensorflow/tensorflow/pull/84384,[],[],
2775702338,pull_request,closed,,Passing device information to Vectorization pass. This will be needed when adding vectorization for AtomicRMW which will only be available for Hopper.,"Passing device information to Vectorization pass. This will be needed when adding vectorization for AtomicRMW which will only be available for Hopper.
",copybara-service[bot],2025-01-08 15:36:52+00:00,[],2025-01-08 16:37:01+00:00,2025-01-08 16:37:00+00:00,https://github.com/tensorflow/tensorflow/pull/84383,[],[],
2775599420,pull_request,closed,,[XLA:CPU] Remove no thunks tests for exhaustive_binary_test,"[XLA:CPU] Remove no thunks tests for exhaustive_binary_test
",copybara-service[bot],2025-01-08 14:53:33+00:00,[],2025-01-08 15:45:21+00:00,2025-01-08 15:45:21+00:00,https://github.com/tensorflow/tensorflow/pull/84382,[],[],
2775577123,pull_request,closed,,Moving AtomicRMW utilities out of lower_tensors. These are going to also be used in vectorizing AtomicRMW in follow-up changes.,"Moving AtomicRMW utilities out of lower_tensors. These are going to also be used in vectorizing AtomicRMW in follow-up changes.
",copybara-service[bot],2025-01-08 14:44:07+00:00,[],2025-01-08 15:34:17+00:00,2025-01-08 15:34:16+00:00,https://github.com/tensorflow/tensorflow/pull/84381,[],[],
2775529937,pull_request,closed,,[xla:gpu] fix bug in counting good autotuner configs,"[xla:gpu] fix bug in counting good autotuner configs

Move comparison of executable != nullptr _before_ calling std::move(executable).

This is really only used for logging, but definitely adds confusion to the logs when it's always 0 :).
",copybara-service[bot],2025-01-08 14:24:04+00:00,[],2025-01-08 14:44:03+00:00,2025-01-08 14:44:02+00:00,https://github.com/tensorflow/tensorflow/pull/84380,[],[],
2775427830,pull_request,open,,Replace outdated select() on --cpu in lite/kernels/internal/BUILD with platform API equivalent.,"Replace outdated select() on --cpu in lite/kernels/internal/BUILD with platform API equivalent.
",copybara-service[bot],2025-01-08 13:41:20+00:00,[],2025-01-09 12:35:58+00:00,,https://github.com/tensorflow/tensorflow/pull/84379,[],[],
2775398351,pull_request,closed,,Replace outdated select() on --cpu in lite/kernels/BUILD with platform API equivalent.,"Replace outdated select() on --cpu in lite/kernels/BUILD with platform API equivalent.
",copybara-service[bot],2025-01-08 13:27:41+00:00,[],2025-01-10 19:16:47+00:00,2025-01-10 19:16:45+00:00,https://github.com/tensorflow/tensorflow/pull/84378,[],[],
2775323370,pull_request,closed,,Adds InferenceCalculatorLiteRt backend,"Adds InferenceCalculatorLiteRt backend
",copybara-service[bot],2025-01-08 12:52:13+00:00,[],2025-01-22 10:22:22+00:00,2025-01-22 10:22:20+00:00,https://github.com/tensorflow/tensorflow/pull/84377,[],[],
2775280854,pull_request,closed,,[pjrt] Removed unused prefer_to_retain_reference argument from RecordUsage,"[pjrt] Removed unused prefer_to_retain_reference argument from RecordUsage

It was always set to false by the callers.
",copybara-service[bot],2025-01-08 12:31:10+00:00,['superbobry'],2025-01-08 15:18:58+00:00,2025-01-08 15:18:57+00:00,https://github.com/tensorflow/tensorflow/pull/84376,[],[],
2775244309,pull_request,closed,,Update .bazelignore,"changes is done
",shreya13052002,2025-01-08 12:13:15+00:00,['gbaned'],2025-01-08 12:13:27+00:00,2025-01-08 12:13:27+00:00,https://github.com/tensorflow/tensorflow/pull/84375,"[('size:XS', 'CL Change Size: Extra Small')]","[{'comment_id': 2577525416, 'issue_id': 2775244309, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/84375/checks?check_run_id=35309734939) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2025, 1, 8, 12, 13, 20, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2025-01-08 12:13:20 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/84375/checks?check_run_id=35309734939) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2775182934,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 11:43:28+00:00,[],2025-01-08 11:43:28+00:00,,https://github.com/tensorflow/tensorflow/pull/84374,[],[],
2775178167,pull_request,closed,,[XLA:GPU] Fix sorted scatter with imperfectly tiled indices.,"[XLA:GPU] Fix sorted scatter with imperfectly tiled indices.

The algorithm was checking whether to write to the output or not by comparing the current slice index with the number of indices per warp. It works only when we have perfectly tiled indices, e.g. 50 indices per warp with a total of 2000 indices. As soon as we have 2001 indices, the last warp processes 1 update slice, but never writes it down.

Also simplified the logic for the update loop that accumulates elements in registers. Instead of having scf.if inside of xla.loop, now we have two different xla.loops in different cases of scf.if, that either overwrite the accumulator or combine it with the new data.
",copybara-service[bot],2025-01-08 11:40:53+00:00,['pifon2a'],2025-01-08 16:25:05+00:00,2025-01-08 16:25:03+00:00,https://github.com/tensorflow/tensorflow/pull/84373,[],[],
2775142085,pull_request,closed,,Remove unused constructor parameter (NFC).,"Remove unused constructor parameter (NFC).

This was forgotten to be removed during an earlier refactoring.
",copybara-service[bot],2025-01-08 11:23:45+00:00,['akuegel'],2025-01-08 12:28:09+00:00,2025-01-08 12:28:08+00:00,https://github.com/tensorflow/tensorflow/pull/84372,[],[],
2775115820,pull_request,closed,,"[pjrt] Removed unused CreateDeviceToHostChannelHandle, CreateChannelHandle and SupportsSendRecvCallbacks","[pjrt] Removed unused CreateDeviceToHostChannelHandle, CreateChannelHandle and SupportsSendRecvCallbacks
",copybara-service[bot],2025-01-08 11:10:18+00:00,['superbobry'],2025-01-08 15:08:20+00:00,2025-01-08 15:08:19+00:00,https://github.com/tensorflow/tensorflow/pull/84371,[],[],
2775086645,pull_request,closed,,Silence Dlopen log messages when probing for Neuron library,"Silence Dlopen log messages when probing for Neuron library
",copybara-service[bot],2025-01-08 10:56:47+00:00,[],2025-01-08 23:12:32+00:00,2025-01-08 23:12:31+00:00,https://github.com/tensorflow/tensorflow/pull/84370,[],[],
2774999959,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 10:27:49+00:00,[],2025-01-09 08:58:18+00:00,2025-01-09 08:58:17+00:00,https://github.com/tensorflow/tensorflow/pull/84369,[],[],
2774999767,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 10:27:44+00:00,[],2025-01-08 10:27:44+00:00,,https://github.com/tensorflow/tensorflow/pull/84368,[],[],
2774916581,pull_request,closed,,[NFC] Polish ScalarOrTensor a little.,"[NFC] Polish ScalarOrTensor a little.

- Remove std::variant, MLIR's run-time type information already provides the same.
- Change  `ScalarOrTensor::UnwrapTensor` to return `TypedValue`.
- Use `getType()` instead of `Type()` to align the naming.
",copybara-service[bot],2025-01-08 10:01:04+00:00,['chsigg'],2025-01-08 11:12:15+00:00,2025-01-08 11:12:14+00:00,https://github.com/tensorflow/tensorflow/pull/84367,[],[],
2774871901,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:46:56+00:00,[],2025-01-08 09:46:56+00:00,,https://github.com/tensorflow/tensorflow/pull/84366,[],[],
2774859474,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:43:04+00:00,[],2025-01-08 09:43:04+00:00,,https://github.com/tensorflow/tensorflow/pull/84365,[],[],
2774811460,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:27:31+00:00,[],2025-01-08 10:58:07+00:00,,https://github.com/tensorflow/tensorflow/pull/84363,[],[],
2774808589,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:26:41+00:00,[],2025-01-08 09:26:41+00:00,,https://github.com/tensorflow/tensorflow/pull/84362,[],[],
2774802642,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:24:33+00:00,[],2025-01-10 08:37:17+00:00,2025-01-10 08:37:16+00:00,https://github.com/tensorflow/tensorflow/pull/84361,[],[],
2774798713,pull_request,open,,compat: Update forward compatibility horizon to 2025-01-08,"compat: Update forward compatibility horizon to 2025-01-08
",copybara-service[bot],2025-01-08 09:23:13+00:00,[],2025-01-08 09:23:13+00:00,,https://github.com/tensorflow/tensorflow/pull/84360,[],[],
2774796512,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:22:26+00:00,[],2025-01-09 10:38:45+00:00,2025-01-09 10:38:44+00:00,https://github.com/tensorflow/tensorflow/pull/84359,[],[],
2774792650,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:21:18+00:00,[],2025-01-08 12:18:15+00:00,2025-01-08 12:18:15+00:00,https://github.com/tensorflow/tensorflow/pull/84358,[],[],
2774791521,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:20:56+00:00,[],2025-01-09 07:19:24+00:00,2025-01-09 07:19:23+00:00,https://github.com/tensorflow/tensorflow/pull/84357,[],[],
2774790763,pull_request,open,,Remove unused constructor parameter (NFC).,"Remove unused constructor parameter (NFC).

This was forgotten to be removed during an earlier refactoring.
",copybara-service[bot],2025-01-08 09:20:32+00:00,[],2025-01-08 12:18:40+00:00,,https://github.com/tensorflow/tensorflow/pull/84356,[],[],
2774790755,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:20:32+00:00,[],2025-01-08 09:20:32+00:00,,https://github.com/tensorflow/tensorflow/pull/84355,[],[],
2774790748,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:20:32+00:00,[],2025-01-10 05:41:18+00:00,2025-01-10 05:41:16+00:00,https://github.com/tensorflow/tensorflow/pull/84354,[],[],
2774789018,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:19:56+00:00,[],2025-01-14 08:16:34+00:00,2025-01-14 08:16:33+00:00,https://github.com/tensorflow/tensorflow/pull/84353,[],[],
2774788424,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:19:48+00:00,[],2025-01-08 09:19:48+00:00,,https://github.com/tensorflow/tensorflow/pull/84352,[],[],
2774788343,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:19:46+00:00,[],2025-01-09 08:09:13+00:00,,https://github.com/tensorflow/tensorflow/pull/84351,[],[],
2774784372,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:18:14+00:00,[],2025-01-10 06:09:46+00:00,2025-01-10 06:09:45+00:00,https://github.com/tensorflow/tensorflow/pull/84350,[],[],
2774782553,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:17:40+00:00,[],2025-01-08 09:17:40+00:00,,https://github.com/tensorflow/tensorflow/pull/84349,[],[],
2774782398,pull_request,closed,,PR #20557: [ds-fusion] Add HandleReducePrecision to algebraic simplifier,"PR #20557: [ds-fusion] Add HandleReducePrecision to algebraic simplifier

Imported from GitHub PR https://github.com/openxla/xla/pull/20557

When the mantissa and exponent of the reduce-precision instruction are the same as the mantissa and exponent of the primitive type of the operand, then the reduce-precision operation is a no-op.
Copybara import of the project:

--
8b9852bb24ea6dbbc2a6d6dd6cf68c41efde8b30 by Shraiysh Vaishay <svaishay@nvidia.com>:

Add HandleReducePrecision to algebraic simplifier

When the mantissa and exponent of the reduce-precision instruction are
the same as the mantissa and exponent of the primitive type of the
operant, then the reduce-precision operation is a no-op.

--
f54f2d35f2d85913e3d5febdbb12c38468d4e1ea by Shraiysh Vaishay <svaishay@nvidia.com>:

Addressed comments

--
39c4be640db7a3b8a60483cea7f8f47154c1e691 by Shraiysh Vaishay <svaishay@nvidia.com>:

Move the pass after the last pass that causes precision changes

The last pass to cause precision changes is SimplifyFPConversions. Moved
the handling of reduce-precision after that.

--
f82bc5c034922ba39c301ee0e173f86917d08da4 by Shraiysh Vaishay <svaishay@nvidia.com>:

addressed comments

--
34ee3317c45d48fc3904d11db2ad296e90b6f51a by Shraiysh Vaishay <svaishay@nvidia.com>:

Handle clang-format failure.

Merging this change closes #20557

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20557 from shraiysh:handle_reduce_precision 34ee3317c45d48fc3904d11db2ad296e90b6f51a
",copybara-service[bot],2025-01-08 09:17:36+00:00,[],2025-01-08 11:02:25+00:00,2025-01-08 11:02:24+00:00,https://github.com/tensorflow/tensorflow/pull/84348,[],[],
2774780103,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:16:37+00:00,[],2025-01-09 08:45:39+00:00,2025-01-09 08:45:38+00:00,https://github.com/tensorflow/tensorflow/pull/84347,[],[],
2774779346,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:16:14+00:00,[],2025-01-08 09:16:14+00:00,,https://github.com/tensorflow/tensorflow/pull/84346,[],[],
2774779177,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:16:08+00:00,[],2025-01-10 07:39:47+00:00,2025-01-10 07:39:46+00:00,https://github.com/tensorflow/tensorflow/pull/84345,[],[],
2774778414,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:15:44+00:00,[],2025-01-08 09:15:44+00:00,,https://github.com/tensorflow/tensorflow/pull/84344,[],[],
2774778191,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:15:37+00:00,[],2025-01-09 10:49:11+00:00,2025-01-09 10:49:02+00:00,https://github.com/tensorflow/tensorflow/pull/84343,[],[],
2774778052,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:15:33+00:00,[],2025-01-08 12:04:50+00:00,,https://github.com/tensorflow/tensorflow/pull/84342,[],[],
2774777685,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:15:22+00:00,[],2025-01-10 06:43:31+00:00,2025-01-10 06:43:31+00:00,https://github.com/tensorflow/tensorflow/pull/84341,[],[],
2774776912,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:15:04+00:00,[],2025-01-11 08:27:11+00:00,2025-01-11 08:27:11+00:00,https://github.com/tensorflow/tensorflow/pull/84340,[],[],
2774774475,pull_request,closed,,PR #19067: [XLA:CPU][oneDNN] Move simplification pass before oneDNN pass,"PR #19067: [XLA:CPU][oneDNN] Move simplification pass before oneDNN pass

Imported from GitHub PR https://github.com/openxla/xla/pull/19067

This PR moves the simplification pass before oneDNN rewriter pass which simplifies the pattern matching for quantization support by getting rid of redundant copy ops.
Copybara import of the project:

--
57f2f3b3e5a850ff264450af5a8bc796062cc8c6 by Mahmoud Abuzaina <mahmoud.abuzaina@intel.com>:

Move simplification pass before oneDNN pass

--
5248e332594414e71533154a63ea03145f533e4a by Mahmoud Abuzaina <mahmoud.abuzaina@intel.com>:

Added a unit test

Merging this change closes #19067

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/19067 from Intel-tensorflow:mabuzain/reorder-passes 5248e332594414e71533154a63ea03145f533e4a
",copybara-service[bot],2025-01-08 09:14:05+00:00,[],2025-01-10 09:49:49+00:00,2025-01-10 09:49:48+00:00,https://github.com/tensorflow/tensorflow/pull/84339,[],[],
2774772814,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:13:32+00:00,[],2025-01-08 12:14:45+00:00,,https://github.com/tensorflow/tensorflow/pull/84338,[],[],
2774772600,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:13:26+00:00,[],2025-01-10 08:00:22+00:00,2025-01-10 08:00:20+00:00,https://github.com/tensorflow/tensorflow/pull/84337,[],[],
2774769754,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 09:12:32+00:00,[],2025-01-08 09:12:32+00:00,,https://github.com/tensorflow/tensorflow/pull/84336,[],[],
2774720680,pull_request,open,,PR #18838: [NVIDIA GPU] Support multi-operand collective-permute,"PR #18838: [NVIDIA GPU] Support multi-operand collective-permute

Imported from GitHub PR https://github.com/openxla/xla/pull/18838

For collective-permutes with small message sizes, it is beneficial to combine them into a single collective because
1. it gets rid of some kernel launch overhead, and allows NCCL to do some message fusion;
2. fewer collectives make it easier for LHS to make better decision.

In order to support combining collective-permutes, we need to support multi-operand collective-permute first, a.k.a. the combined collective-permute. This PR extends the existing CP interface by overloading it, so that a CP can have multiple operands.
Copybara import of the project:

--
5e10aba5b8f6ae66d1071a1894a87987b6a5bceb by Terry Sun <tesun@nvidia.com>:

support multi-operand cp

--
170fead3de942f5e14f4936df1d76bf7e5e319d4 by Terry Sun <tesun@nvidia.com>:

minor refactoring

--
0d85070baee3f26075f0b3660c4674d7b414c861 by Terry Sun <tesun@nvidia.com>:

update python interface

--
9812a104822ea479d29fef0531b9e10d5c2a831d by Terry Sun <tesun@nvidia.com>:

polish python interface

--
3a1552cbcd2e26f814373e0e01adbe8eceb3be9f by Terry Sun <tesun@nvidia.com>:

formatting

--
d3657f81ac57dc1de86561b3449d051d178e0f75 by Terry Sun <tesun@nvidia.com>:

formatting

--
9caacb4e84ac3bb580443afc76e048a6e264094a by Terry Sun <tesun@nvidia.com>:

refactor overloading

--
0aff5e0a372af9e4a859b54681acf0501adca096 by Terry Sun <tesun@nvidia.com>:

minor refactor

--
20a0e3d7dd57a7d70cffe20a1b35fb4b4c1e5c8a by Terry Sun <tesun@nvidia.com>:

add parser test

--
e5ad9d9b601ada1c3984f34967c98574061bd043 by Terry Sun <tesun@nvidia.com>:

fix merge issue

Merging this change closes #18838

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/18838 from terryysun:terryysun/grouped_cp e5ad9d9b601ada1c3984f34967c98574061bd043
",copybara-service[bot],2025-01-08 08:51:32+00:00,[],2025-01-09 09:35:18+00:00,,https://github.com/tensorflow/tensorflow/pull/84335,[],[],
2774695935,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 08:42:23+00:00,[],2025-01-08 10:53:30+00:00,,https://github.com/tensorflow/tensorflow/pull/84334,[],[],
2774576352,pull_request,closed,,Update to match upstream API change (NFC).,"Update to match upstream API change (NFC).

This method was renamed but staging function kept, switch to renamed variant.
",copybara-service[bot],2025-01-08 07:56:28+00:00,['jpienaar'],2025-01-14 04:16:14+00:00,2025-01-14 04:16:13+00:00,https://github.com/tensorflow/tensorflow/pull/84333,[],[],
2774554276,pull_request,closed,,IFRT proxy asan fix: Do not call `promise.Set()` twice in error-handling path.,"IFRT proxy asan fix: Do not call `promise.Set()` twice in error-handling path.
",copybara-service[bot],2025-01-08 07:47:04+00:00,[],2025-01-08 18:06:24+00:00,2025-01-08 18:06:24+00:00,https://github.com/tensorflow/tensorflow/pull/84332,[],[],
2774541053,pull_request,closed,,Remove unused alias rules,"Remove unused alias rules

The last internal users have been migrated.
",copybara-service[bot],2025-01-08 07:41:16+00:00,['akuegel'],2025-01-08 08:11:10+00:00,2025-01-08 08:11:10+00:00,https://github.com/tensorflow/tensorflow/pull/84331,[],[],
2774531881,pull_request,open,,[tflite/kernels] Default to ruy for all non-x86 platforms,"[tflite/kernels] Default to ruy for all non-x86 platforms

This may cause small numerical differences due to different kernels being used.
",copybara-service[bot],2025-01-08 07:37:59+00:00,[],2025-01-14 19:21:05+00:00,,https://github.com/tensorflow/tensorflow/pull/84330,[],[],
2774502600,pull_request,closed,,Update to match upstream API change (NFC).,"Update to match upstream API change (NFC).

This method was renamed but staging function kept, switch to renamed variant.
",copybara-service[bot],2025-01-08 07:23:51+00:00,['jpienaar'],2025-01-10 07:33:05+00:00,2025-01-10 07:33:04+00:00,https://github.com/tensorflow/tensorflow/pull/84329,[],[],
2774354105,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 05:47:00+00:00,[],2025-01-08 09:28:17+00:00,2025-01-08 09:28:16+00:00,https://github.com/tensorflow/tensorflow/pull/84328,[],[],
2774340085,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 05:33:48+00:00,[],2025-01-08 05:33:48+00:00,,https://github.com/tensorflow/tensorflow/pull/84327,[],[],
2774331403,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 05:25:59+00:00,[],2025-01-08 08:38:46+00:00,2025-01-08 08:38:45+00:00,https://github.com/tensorflow/tensorflow/pull/84326,[],[],
2774280353,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-08 04:43:45+00:00,[],2025-01-08 07:54:39+00:00,2025-01-08 07:54:39+00:00,https://github.com/tensorflow/tensorflow/pull/84325,[],[],
2774261705,pull_request,closed,,Allow composite op odml.quantize_and_dequantize to be converted to custom op.,"Allow composite op odml.quantize_and_dequantize to be converted to custom op.

Attributes will determine behavior
",copybara-service[bot],2025-01-08 04:25:33+00:00,['daverim'],2025-01-14 01:40:20+00:00,2025-01-14 01:40:19+00:00,https://github.com/tensorflow/tensorflow/pull/84324,[],[],
2774092576,pull_request,open,,[JAX] Add a new jax_num_cpu_devices flag that allows the user to specify the number of CPU directly.,"[JAX] Add a new jax_num_cpu_devices flag that allows the user to specify the number of CPU directly.

This subsumes (and ultimately will deprecate) overriding the number of CPU devices via XLA_FLAGS.

In addition, replace the test utility jtu.set_host_platform_device_count with jtu.request_cpu_devices(...), which sets or increases the flag's value. This both removes the need for an overly complicated context stack, and prepares for removing remaining uses of setUpModule as part of work parallelizing the test suite with threads.
",copybara-service[bot],2025-01-08 02:25:46+00:00,[],2025-01-08 02:45:13+00:00,,https://github.com/tensorflow/tensorflow/pull/84323,[],[],
2774091454,pull_request,closed,,[XLA:Python] Add an optional argument to the CPU client factory method that specifies the number of CPU devices.,"[XLA:Python] Add an optional argument to the CPU client factory method that specifies the number of CPU devices.

This is more ergonomic than overriding the CPU device count via XLA_FLAGS.
",copybara-service[bot],2025-01-08 02:24:37+00:00,[],2025-01-08 03:04:06+00:00,2025-01-08 03:04:06+00:00,https://github.com/tensorflow/tensorflow/pull/84322,[],[],
2774068087,pull_request,closed,,Remove `RunAndCompare` functionality from `HloRunnerAgnosticTestBase`.,"Remove `RunAndCompare` functionality from `HloRunnerAgnosticTestBase`.

This functionality is now fully contained in `HloRunnerAgnosticReferenceMixin`
and therefore is no longer needed in `HloRunnerAgnosticTestBase`.

This change temporarily adds the mixin to `HloPjRtTestBase`. Next, we'll go
through all tests that extend these base classes and will move the uses of the
mixins to the leaves.
",copybara-service[bot],2025-01-08 02:01:34+00:00,[],2025-01-13 17:47:34+00:00,2025-01-13 17:47:33+00:00,https://github.com/tensorflow/tensorflow/pull/84321,[],[],
2774065430,pull_request,closed,,Add `HloPjRtInterpreterReferenceMixin` wrapper around `HloRunnerAgnosticReferenceMixin`.,"Add `HloPjRtInterpreterReferenceMixin` wrapper around `HloRunnerAgnosticReferenceMixin`.

This mixin provides a default way to run comparison tests against an interpreter
reference via the PjRt-based interpreter.
",copybara-service[bot],2025-01-08 01:58:40+00:00,[],2025-01-10 19:10:36+00:00,2025-01-10 19:10:35+00:00,https://github.com/tensorflow/tensorflow/pull/84320,[],[],
2774065314,pull_request,closed,,Remove mixin from `HloPjRtTestBase`.,"Remove mixin from `HloPjRtTestBase`.

Mixin uses should be specified at the test level.
",copybara-service[bot],2025-01-08 01:58:32+00:00,[],2025-01-13 19:11:45+00:00,2025-01-13 19:11:44+00:00,https://github.com/tensorflow/tensorflow/pull/84319,[],[],
2774063642,pull_request,open,,Split `RunAndCompare` with reference backend functionality into a mixin.,"Split `RunAndCompare` with reference backend functionality into a mixin.

Many users don't require `RunAndCompare` functionality, but are forced to select
and initialize a reference backend anyway.

With this change, users can opt to extend their specific
`HloRunnerAgnosticTestBase` implementation to add `RunAndCompare` functionality.
The mixin acts as a wrapper around any `HloRunnerAgnosticTestBase`
implementation, allowing a high degree of customization.
",copybara-service[bot],2025-01-08 01:56:58+00:00,[],2025-01-08 01:56:58+00:00,,https://github.com/tensorflow/tensorflow/pull/84318,[],[],
2774063193,pull_request,open,,PR #21022: [XLA:CPU][oneDNN] Modify addend shape for small Convolution + Bias contraction,"PR #21022: [XLA:CPU][oneDNN] Modify addend shape for small Convolution + Bias contraction

Imported from GitHub PR https://github.com/openxla/xla/pull/21022

For small constant biases, XLA may perform constant folding, which can alter the shape of the bias passed to oneDNN. This PR removes any extraneous trivial dimensions from the bias that is passed to the oneDNN library. It also adds a test in all applicable dtypes to test the functionality.
Copybara import of the project:

--
5ce5ba690d5acd1ebedca1484483639b2f3b0be1 by Akhil Goel <akhil.goel@intel.com>:

Modify conv bias shape

Merging this change closes #21022

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21022 from Intel-tensorflow:akhil/conv_bias_fix 5ce5ba690d5acd1ebedca1484483639b2f3b0be1
",copybara-service[bot],2025-01-08 01:56:24+00:00,[],2025-01-08 01:56:31+00:00,,https://github.com/tensorflow/tensorflow/pull/84317,[],"[{'comment_id': 2576552763, 'issue_id': 2774063193, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/84317/checks?check_run_id=35287836002) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2025, 1, 8, 1, 56, 30, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2025-01-08 01:56:30 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/84317/checks?check_run_id=35287836002) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2774043316,pull_request,open,,PR #20557: [ds-fusion] Add HandleReducePrecision to algebraic simplifier,"PR #20557: [ds-fusion] Add HandleReducePrecision to algebraic simplifier

Imported from GitHub PR https://github.com/openxla/xla/pull/20557

When the mantissa and exponent of the reduce-precision instruction are the same as the mantissa and exponent of the primitive type of the operand, then the reduce-precision operation is a no-op.
Copybara import of the project:

--
8b9852bb24ea6dbbc2a6d6dd6cf68c41efde8b30 by Shraiysh Vaishay <svaishay@nvidia.com>:

Add HandleReducePrecision to algebraic simplifier

When the mantissa and exponent of the reduce-precision instruction are
the same as the mantissa and exponent of the primitive type of the
operant, then the reduce-precision operation is a no-op.

--
f54f2d35f2d85913e3d5febdbb12c38468d4e1ea by Shraiysh Vaishay <svaishay@nvidia.com>:

Addressed comments

--
39c4be640db7a3b8a60483cea7f8f47154c1e691 by Shraiysh Vaishay <svaishay@nvidia.com>:

Move the pass after the last pass that causes precision changes

The last pass to cause precision changes is SimplifyFPConversions. Moved
the handling of reduce-precision after that.

--
f82bc5c034922ba39c301ee0e173f86917d08da4 by Shraiysh Vaishay <svaishay@nvidia.com>:

addressed comments

--
34ee3317c45d48fc3904d11db2ad296e90b6f51a by Shraiysh Vaishay <svaishay@nvidia.com>:

Handle clang-format failure.

Merging this change closes #20557

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20557 from shraiysh:handle_reduce_precision 34ee3317c45d48fc3904d11db2ad296e90b6f51a
",copybara-service[bot],2025-01-08 01:36:52+00:00,[],2025-01-08 01:36:52+00:00,,https://github.com/tensorflow/tensorflow/pull/84316,[],[],
2774035554,pull_request,closed,,Qualify unqualified calls to llvm::cast.,"Qualify unqualified calls to llvm::cast.
",copybara-service[bot],2025-01-08 01:27:47+00:00,['gharibian'],2025-01-08 02:42:52+00:00,2025-01-08 02:42:52+00:00,https://github.com/tensorflow/tensorflow/pull/84315,[],[],
2774030398,pull_request,closed,,Load all available dialects in `xla::ifrt::support::RegisterMlirDialects`,"Load all available dialects in `xla::ifrt::support::RegisterMlirDialects`

This avoids lazily loading dialects in a potentially multi-threaded context, which results in the following crash: `LLVM ERROR: Loading a dialect (chlo) while in a multi-threaded execution context (maybe the PassManager): this can indicate a missing `dependentDialects` in a pass for example.`.
",copybara-service[bot],2025-01-08 01:21:30+00:00,[],2025-01-08 08:20:48+00:00,2025-01-08 08:20:46+00:00,https://github.com/tensorflow/tensorflow/pull/84314,[],[],
2774005262,pull_request,closed,,Add an HLO parsing option to enable/disable initialization of short form constants (dots) to random values.,"Add an HLO parsing option to enable/disable initialization of short form constants (dots) to random values.
",copybara-service[bot],2025-01-08 00:51:30+00:00,[],2025-01-09 21:11:47+00:00,2025-01-09 21:11:46+00:00,https://github.com/tensorflow/tensorflow/pull/84313,[],[],
2773983723,pull_request,closed,,Remove obsolete target.,"Remove obsolete target.
",copybara-service[bot],2025-01-08 00:27:00+00:00,[],2025-01-08 18:16:38+00:00,2025-01-08 18:16:38+00:00,https://github.com/tensorflow/tensorflow/pull/84312,[],[],
2773982629,pull_request,closed,,OpenCL tensor buffer for litert,"OpenCL tensor buffer for litert
Support Lock and Unlock, instantiate MLD cl environment as singleton instance.
Added CompileModel CPU test with OpenCL Tensorbuffers as inputs and outputs.
",copybara-service[bot],2025-01-08 00:25:44+00:00,[],2025-01-15 19:03:10+00:00,2025-01-15 19:03:09+00:00,https://github.com/tensorflow/tensorflow/pull/84311,[],[],
2773975234,pull_request,closed,,Add a using to make referencing environment option overrides as a parameter later easier.,"Add a using to make referencing environment option overrides as a parameter later easier.
",copybara-service[bot],2025-01-08 00:17:13+00:00,[],2025-01-08 01:13:28+00:00,2025-01-08 01:13:28+00:00,https://github.com/tensorflow/tensorflow/pull/84310,[],[],
2773958582,pull_request,closed,,Move most of kernel Launch processing from Stream to the Kernel classes.,"Move most of kernel Launch processing from Stream to the Kernel classes.
",copybara-service[bot],2025-01-07 23:59:14+00:00,[],2025-01-08 19:02:15+00:00,2025-01-08 19:02:14+00:00,https://github.com/tensorflow/tensorflow/pull/84309,[],[],
2773940589,pull_request,open,,Integrate LLVM at llvm/llvm-project@478648e2c0ad,"Integrate LLVM at llvm/llvm-project@478648e2c0ad

Updates LLVM usage to match
[478648e2c0ad](https://github.com/llvm/llvm-project/commit/478648e2c0ad)
",copybara-service[bot],2025-01-07 23:37:48+00:00,[],2025-01-07 23:37:48+00:00,,https://github.com/tensorflow/tensorflow/pull/84308,[],[],
2773923927,pull_request,closed,,[xla:cpu] Add CpuClique to XLA:CPU collectives and use generic collectives APIs to acquire communicator in CollectiveThunk,"[xla:cpu] Add CpuClique to XLA:CPU collectives and use generic collectives APIs to acquire communicator in CollectiveThunk

Implement Cliques support for XLA:CPU collectives for consistency with XLA:GPU. Further unification will be in followup CLs.
",copybara-service[bot],2025-01-07 23:18:59+00:00,['ezhulenev'],2025-01-08 16:54:04+00:00,2025-01-08 16:54:03+00:00,https://github.com/tensorflow/tensorflow/pull/84307,[],[],
2773902837,pull_request,open,,[pfor] Handle 64-bit shapes. ,"[pfor] Handle 64-bit shapes. 

Previously, if 64-bit shapes were enabled by default, this would lead to type mismatches during pfor op conversion, because certain internal values are assumed to have type `tf.int32`.
",copybara-service[bot],2025-01-07 22:57:47+00:00,[],2025-01-07 22:57:47+00:00,,https://github.com/tensorflow/tensorflow/pull/84306,[],[],
2773898013,pull_request,open,,In progress experimention. Add StringDType to JAX's supported types.,"In progress experimention. Add StringDType to JAX's supported types.
",copybara-service[bot],2025-01-07 22:54:48+00:00,[],2025-01-09 22:14:14+00:00,,https://github.com/tensorflow/tensorflow/pull/84305,[],[],
2773896122,pull_request,closed,,Fixed a bug where slice Op legalization constructing QNN param with slicing size instead of end index.,"Fixed a bug where slice Op legalization constructing QNN param with slicing size instead of end index.
",copybara-service[bot],2025-01-07 22:53:06+00:00,[],2025-01-09 00:20:33+00:00,2025-01-09 00:20:32+00:00,https://github.com/tensorflow/tensorflow/pull/84304,[],[],
2773891592,pull_request,closed,,Add counter for graph conversion in V1 compat pipeline.,"Add counter for graph conversion in V1 compat pipeline.
",copybara-service[bot],2025-01-07 22:48:36+00:00,[],2025-01-08 00:00:38+00:00,2025-01-08 00:00:38+00:00,https://github.com/tensorflow/tensorflow/pull/84303,[],[],
2773890619,pull_request,open,,Retain metric that counts graph conversion in MLIR bridge for v1 compat pipeline,"Retain metric that counts graph conversion in MLIR bridge for v1 compat pipeline
",copybara-service[bot],2025-01-07 22:47:33+00:00,[],2025-01-07 22:47:33+00:00,,https://github.com/tensorflow/tensorflow/pull/84302,[],[],
2773808581,pull_request,closed,,[PJRT:C] Implement PJRT_AsyncHostToDeviceTransferManager class. Introduce more of its member function to C Api.,"[PJRT:C] Implement PJRT_AsyncHostToDeviceTransferManager class. Introduce more of its member function to C Api.
",copybara-service[bot],2025-01-07 21:48:30+00:00,[],2025-01-13 23:08:47+00:00,2025-01-13 23:08:46+00:00,https://github.com/tensorflow/tensorflow/pull/84301,[],[],
2773791738,pull_request,closed,,Adds InferenceRunnerLiteRt class,"Adds InferenceRunnerLiteRt class
",copybara-service[bot],2025-01-07 21:39:19+00:00,[],2025-01-14 15:47:19+00:00,2025-01-14 15:47:17+00:00,https://github.com/tensorflow/tensorflow/pull/84300,[],[],
2773790004,pull_request,closed,,Update CompiledModel.Run(),"Update CompiledModel.Run()

Changed to use signature_key for the Run() method for input / output maps
since it aligns with other parameters.
",copybara-service[bot],2025-01-07 21:38:00+00:00,['terryheo'],2025-01-08 17:54:34+00:00,2025-01-08 17:54:34+00:00,https://github.com/tensorflow/tensorflow/pull/84299,[],[],
2773759680,pull_request,open,,[XLA] No longer support mpmd model parallelism in the verifier.,"[XLA] No longer support mpmd model parallelism in the verifier.
",copybara-service[bot],2025-01-07 21:22:04+00:00,['blakehechtman'],2025-01-07 21:22:05+00:00,,https://github.com/tensorflow/tensorflow/pull/84298,[],[],
2773754098,pull_request,closed,,Remove redundant string conversions.,"Remove redundant string conversions.
",copybara-service[bot],2025-01-07 21:18:13+00:00,[],2025-01-08 23:02:34+00:00,2025-01-08 23:02:33+00:00,https://github.com/tensorflow/tensorflow/pull/84297,[],[],
2773737695,pull_request,closed,,[XLA:Python] Fix three concurrency problems.,"[XLA:Python] Fix three concurrency problems.

These problems can be reproduced even with the GIL enabled, they are not no-GIL bugs.

In pmap_lib.cc, defend against a use after free in the following scenario:
* thread A misses in the compilation cache and calls `cache_miss()` to populate the cache, relying on the new entry in executables_ remaining alive.
* thread B calls `cache_clear()`, which erases the contents of `executables_`
Use a std::shared_ptr to keep the entry alive.

In pjit.cc, refactor PjitFunctionStore to use a doubly-linked list of PjitFunctionObject entries. When consuming the list of functions in the store, take strong references to them. This prevents a use-after-free if the cache is cleared concurrently multiple times.

In pjit.cc, do not add functions to the PjitFunctionStore until executables_ is populated. This avoids a null pointer dereference from a concurrent call to `cache_clear`.

Problems found with some upcoming test infrastructure that runs JAX test cases in parallel.
",copybara-service[bot],2025-01-07 21:07:38+00:00,[],2025-01-08 00:49:38+00:00,2025-01-08 00:49:38+00:00,https://github.com/tensorflow/tensorflow/pull/84296,[],[],
2773688183,pull_request,closed,,[xla:cpu] FFI: Add support for token arguments and results,"[xla:cpu] FFI: Add support for token arguments and results

Fix for https://github.com/jax-ml/jax/issues/25756
",copybara-service[bot],2025-01-07 20:34:53+00:00,['ezhulenev'],2025-01-07 21:15:42+00:00,2025-01-07 21:15:42+00:00,https://github.com/tensorflow/tensorflow/pull/84295,[],[],
2773681112,pull_request,open,,Integrate LLVM at llvm/llvm-project@478648e2c0ad,"Integrate LLVM at llvm/llvm-project@478648e2c0ad

Updates LLVM usage to match
[478648e2c0ad](https://github.com/llvm/llvm-project/commit/478648e2c0ad)
",copybara-service[bot],2025-01-07 20:30:42+00:00,[],2025-01-07 20:30:42+00:00,,https://github.com/tensorflow/tensorflow/pull/84294,[],[],
2773603394,pull_request,open,,Record device time measurements in PJRT stream executor client. Set device type to the platform that the client is running on.,"Record device time measurements in PJRT stream executor client. Set device type to the platform that the client is running on.
",copybara-service[bot],2025-01-07 19:43:14+00:00,[],2025-02-06 19:41:26+00:00,,https://github.com/tensorflow/tensorflow/pull/84293,[],[],
2773466928,pull_request,closed,,Fix undefined behavior of mismatch in coordination service.,"Fix undefined behavior of mismatch in coordination service.

`std::mismatch` should be called with an end iterator as the second argument if there is no guarantee on element count in the second range.
",copybara-service[bot],2025-01-07 18:19:19+00:00,[],2025-01-08 14:14:00+00:00,2025-01-08 14:13:59+00:00,https://github.com/tensorflow/tensorflow/pull/84292,[],[],
2773466876,pull_request,closed,,Replace outdated select() on --cpu in lite/delegates/gpu/BUILD with platform API equivalent.,"Replace outdated select() on --cpu in lite/delegates/gpu/BUILD with platform API equivalent.
",copybara-service[bot],2025-01-07 18:19:17+00:00,[],2025-01-10 17:34:03+00:00,2025-01-10 17:34:03+00:00,https://github.com/tensorflow/tensorflow/pull/84291,[],[],
2773386086,pull_request,closed,,Integrate LLVM at llvm/llvm-project@faa3f7528969,"Integrate LLVM at llvm/llvm-project@faa3f7528969

Updates LLVM usage to match
[faa3f7528969](https://github.com/llvm/llvm-project/commit/faa3f7528969)
",copybara-service[bot],2025-01-07 17:30:32+00:00,['d0k'],2025-01-07 18:53:18+00:00,2025-01-07 18:53:16+00:00,https://github.com/tensorflow/tensorflow/pull/84290,[],[],
2773368814,pull_request,closed,,Create option to allow tensorflow::Tensor objects to be imported as DenseResourceElementsAttr during TF V1/V2 saved models import to MLIR Module.,"Create option to allow tensorflow::Tensor objects to be imported as DenseResourceElementsAttr during TF V1/V2 saved models import to MLIR Module.
",copybara-service[bot],2025-01-07 17:20:39+00:00,['vamsimanchala'],2025-01-09 02:52:23+00:00,2025-01-09 02:52:22+00:00,https://github.com/tensorflow/tensorflow/pull/84289,[],[],
2773360828,pull_request,closed,,Refactor collective_permute decomposer. Extract general purpose collective permute related methods to a cp_utils.,"Refactor collective_permute decomposer. Extract general purpose collective permute related methods to a cp_utils.
",copybara-service[bot],2025-01-07 17:16:14+00:00,[],2025-01-09 03:03:33+00:00,2025-01-09 03:03:32+00:00,https://github.com/tensorflow/tensorflow/pull/84288,[],[],
2773337219,pull_request,closed,,[XLA:GPU] Inline a call to `ScheduleGpuModuleWithMemoryScheduler`.,"[XLA:GPU] Inline a call to `ScheduleGpuModuleWithMemoryScheduler`.
",copybara-service[bot],2025-01-07 17:04:01+00:00,[],2025-01-08 09:38:09+00:00,2025-01-08 09:38:08+00:00,https://github.com/tensorflow/tensorflow/pull/84287,[],[],
2773309000,pull_request,closed,,Hook up memory descriptions extension for TPU.,"Hook up memory descriptions extension for TPU.
",copybara-service[bot],2025-01-07 16:50:29+00:00,[],2025-01-07 19:33:30+00:00,2025-01-07 19:33:30+00:00,https://github.com/tensorflow/tensorflow/pull/84286,[],[],
2773303112,pull_request,closed,,Adds CreateFromAhwb method,"Adds CreateFromAhwb method
",copybara-service[bot],2025-01-07 16:47:28+00:00,[],2025-01-08 21:22:21+00:00,2025-01-08 21:22:21+00:00,https://github.com/tensorflow/tensorflow/pull/84285,[],[],
2773299170,pull_request,closed,,[xla:python] Removed unused `*Executable.compile_options`,"[xla:python] Removed unused `*Executable.compile_options`

This change also drops the relevant C++ plumbing.
",copybara-service[bot],2025-01-07 16:45:30+00:00,['superbobry'],2025-01-08 22:19:18+00:00,2025-01-08 22:19:17+00:00,https://github.com/tensorflow/tensorflow/pull/84284,[],[],
2773185834,pull_request,closed,,[XLA][Emitters] Fold constant dimensions in loop op.,"[XLA][Emitters] Fold constant dimensions in loop op.

If we know that indexing dimension is a constant we can safely fold it to
value.

GetRange function got moved to xla_ops to avoid dependency cycle.
",copybara-service[bot],2025-01-07 15:57:42+00:00,['metaflow'],2025-01-07 18:31:00+00:00,2025-01-07 18:31:00+00:00,https://github.com/tensorflow/tensorflow/pull/84283,[],[],
2773049647,pull_request,closed,,Remove outdated and no longer used mips cpu config_setting in lite/BUILD.,"Remove outdated and no longer used mips cpu config_setting in lite/BUILD.
",copybara-service[bot],2025-01-07 14:57:53+00:00,[],2025-01-10 09:55:04+00:00,2025-01-10 09:55:03+00:00,https://github.com/tensorflow/tensorflow/pull/84282,[],[],
2772973449,pull_request,closed,,Fixes typo: buffer_is_cpu_compatible,"Fixes typo: buffer_is_cpu_compatible
",copybara-service[bot],2025-01-07 14:24:04+00:00,[],2025-01-07 17:49:10+00:00,2025-01-07 17:49:09+00:00,https://github.com/tensorflow/tensorflow/pull/84281,[],[],
2772927420,pull_request,closed,,Remove unused alias target,"Remove unused alias target
",copybara-service[bot],2025-01-07 14:02:31+00:00,['akuegel'],2025-01-07 16:19:09+00:00,2025-01-07 16:19:07+00:00,https://github.com/tensorflow/tensorflow/pull/84280,[],[],
2772815018,pull_request,closed,,PR #20861: [XLA:GPU] add cudnn flash attention sequence packing support,"PR #20861: [XLA:GPU] add cudnn flash attention sequence packing support

Imported from GitHub PR https://github.com/openxla/xla/pull/20861

cudnn flash attention has support for sequence packing, which means multiple batches(segments) could be packed into one batch. It could help save memories and speed up both training and inference workloads.

This PR makes following changes:
* added 2 extra tensors to cudnn custom call, **q_offsets** and **kv_offsets** which specify the starting position of each segment in one batch and one extra element for ending of last segment. For example, 3 segments of size 80 is packed into one batch with maximum sequence 256, the q_offsets will be [0, 80, 160, 256]. **q_offsets** and **kv_offsets** will be used to indicate the layout of Q, K, V, O, dO, dQ, dK, dV.
* added one **max_segment_per_batch** option in backend config which specify the maximum number of segments each batch has, since XLA has static memory allocation and the number of segments can change at runtime, we use this option to compile one cudnn graph and allocate static size for **softmax_stat** tensors.
* added one test case. This sequence packing feature essentially has the same effect as using a segment mask. Comparing this feature against passing segment mask as bias to cudnn.
Copybara import of the project:

--
ae2c14a7c2391f1b343c3721d739a1588360841f by cjkkkk <ske@nvidia.com>:

add cudnn sequence packing support

Merging this change closes #20861

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20861 from Cjkkkk:segment_id ae2c14a7c2391f1b343c3721d739a1588360841f
",copybara-service[bot],2025-01-07 13:10:28+00:00,[],2025-01-07 21:56:16+00:00,2025-01-07 21:56:15+00:00,https://github.com/tensorflow/tensorflow/pull/84279,[],[],
2772775081,pull_request,closed,,Remove unused alias target,"Remove unused alias target
",copybara-service[bot],2025-01-07 12:52:06+00:00,['akuegel'],2025-01-07 14:11:16+00:00,2025-01-07 14:11:15+00:00,https://github.com/tensorflow/tensorflow/pull/84277,[],[],
2772735439,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-07 12:33:18+00:00,[],2025-01-07 12:33:18+00:00,,https://github.com/tensorflow/tensorflow/pull/84276,[],[],
2772703880,pull_request,closed,,Reverts 9c37878c473c92b7a4918acdfb6f1390c6c6ff0d,"Reverts 9c37878c473c92b7a4918acdfb6f1390c6c6ff0d
",copybara-service[bot],2025-01-07 12:17:49+00:00,[],2025-01-07 20:04:26+00:00,2025-01-07 20:04:25+00:00,https://github.com/tensorflow/tensorflow/pull/84275,[],[],
2772668962,pull_request,closed,,Integrate LLVM at llvm/llvm-project@743aee4951d4,"Integrate LLVM at llvm/llvm-project@743aee4951d4

Updates LLVM usage to match
[743aee4951d4](https://github.com/llvm/llvm-project/commit/743aee4951d4)
",copybara-service[bot],2025-01-07 11:59:46+00:00,['d0k'],2025-01-07 13:36:02+00:00,2025-01-07 13:36:01+00:00,https://github.com/tensorflow/tensorflow/pull/84274,[],[],
2772666914,pull_request,closed,,[XLA][Emitters] Reuse emitters_opt for XLA:CPU as well.,"[XLA][Emitters] Reuse emitters_opt for XLA:CPU as well.
",copybara-service[bot],2025-01-07 11:58:39+00:00,['pifon2a'],2025-01-07 12:39:45+00:00,2025-01-07 12:39:45+00:00,https://github.com/tensorflow/tensorflow/pull/84273,[],[],
2772554931,pull_request,closed,,[XLA:CPU] Decouple object loading from JIT compiler.,"[XLA:CPU] Decouple object loading from JIT compiler.
",copybara-service[bot],2025-01-07 11:05:58+00:00,[],2025-01-10 10:22:15+00:00,2025-01-10 10:22:14+00:00,https://github.com/tensorflow/tensorflow/pull/84272,[],[],
2772534853,pull_request,closed,,[XLA:GPU] Issue a warning when autotuning fails with OOM.,"[XLA:GPU] Issue a warning when autotuning fails with OOM.

We suggest to disable autotuning correctness checking (i.e. `--xla_gpu_autotune_level=3`) to reduce memory usage. Correctness checking requires holding a reference buffer in memory for the duration of the profiling phase.
",copybara-service[bot],2025-01-07 10:56:28+00:00,[],2025-01-07 15:07:16+00:00,2025-01-07 15:07:15+00:00,https://github.com/tensorflow/tensorflow/pull/84271,[],[],
2772532636,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-07 10:55:27+00:00,[],2025-01-07 10:55:27+00:00,,https://github.com/tensorflow/tensorflow/pull/84270,[],[],
2772523130,pull_request,open,,Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.,"Replace outdated select() on --cpu in compiler/xla/tsl/BUILD with platform API equivalent.
",copybara-service[bot],2025-01-07 10:51:05+00:00,[],2025-01-14 09:28:24+00:00,,https://github.com/tensorflow/tensorflow/pull/84269,[],[],
2772483593,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-07 10:33:37+00:00,[],2025-01-08 13:11:30+00:00,2025-01-08 13:11:29+00:00,https://github.com/tensorflow/tensorflow/pull/84267,[],[],
2772475070,pull_request,closed,,NFC: Improve comments for IndexingMap members.,"NFC: Improve comments for IndexingMap members.

Also change GetDimVars to GetDimVar for naming consistency.
",copybara-service[bot],2025-01-07 10:30:27+00:00,['chsigg'],2025-01-08 08:48:51+00:00,2025-01-08 08:48:49+00:00,https://github.com/tensorflow/tensorflow/pull/84266,[],[],
2772436795,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-07 10:12:49+00:00,[],2025-01-07 10:12:49+00:00,,https://github.com/tensorflow/tensorflow/pull/84265,[],[],
2772412287,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-07 10:02:34+00:00,[],2025-01-07 10:02:34+00:00,,https://github.com/tensorflow/tensorflow/pull/84264,[],[],
2772246570,pull_request,closed,,Reverts 126b347377519119d0d35e7a73e64f7986f0ebb8,"Reverts 126b347377519119d0d35e7a73e64f7986f0ebb8
",copybara-service[bot],2025-01-07 08:49:57+00:00,[],2025-01-07 12:04:40+00:00,2025-01-07 12:04:39+00:00,https://github.com/tensorflow/tensorflow/pull/84263,[],[],
2772097876,pull_request,closed,,"NFC: Escape positional identifiers which should be replaced in a separate absl::Substitute step, instead of replacing them with itself.","NFC: Escape positional identifiers which should be replaced in a separate absl::Substitute step, instead of replacing them with itself.
",copybara-service[bot],2025-01-07 07:32:22+00:00,['chsigg'],2025-01-07 08:59:55+00:00,2025-01-07 08:59:55+00:00,https://github.com/tensorflow/tensorflow/pull/84262,[],[],
2772070972,pull_request,closed,,Update to match upstream API change (NFC).,"Update to match upstream API change (NFC).

This method was renamed but staging function kept, switch to renamed variant.
",copybara-service[bot],2025-01-07 07:14:30+00:00,['jpienaar'],2025-01-08 12:38:36+00:00,2025-01-08 12:38:35+00:00,https://github.com/tensorflow/tensorflow/pull/84261,[],[],
2772028789,pull_request,closed,,PR #20604: hlo_instruction_utils had no tests. Adding them.,"PR #20604: hlo_instruction_utils had no tests. Adding them.

Imported from GitHub PR https://github.com/openxla/xla/pull/20604

See title.
Copybara import of the project:

--
7bc8052999822b879173448ddc79c949cca10339 by Shraiysh Vaishay <svaishay@nvidia.com>:

hlo_instruction_utils had no tests. Adding them.

--
318444c8b9cc20301b5584c3b9a926d012a8878e by Shraiysh Vaishay <svaishay@nvidia.com>:

Addressed comments

Merging this change closes #20604

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20604 from shraiysh:add_tests_for_instruction_utils 318444c8b9cc20301b5584c3b9a926d012a8878e
",copybara-service[bot],2025-01-07 06:46:19+00:00,[],2025-01-07 22:17:54+00:00,2025-01-07 22:17:53+00:00,https://github.com/tensorflow/tensorflow/pull/84260,[],[],
2772017485,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-07 06:37:48+00:00,[],2025-01-07 06:37:48+00:00,,https://github.com/tensorflow/tensorflow/pull/84259,[],[],
2772016465,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-07 06:37:10+00:00,[],2025-01-07 06:37:10+00:00,,https://github.com/tensorflow/tensorflow/pull/84258,[],[],
2771925527,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-07 05:23:31+00:00,[],2025-01-07 05:23:31+00:00,,https://github.com/tensorflow/tensorflow/pull/84257,[],[],
2771916504,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-07 05:14:01+00:00,[],2025-01-09 11:00:17+00:00,2025-01-09 11:00:16+00:00,https://github.com/tensorflow/tensorflow/pull/84256,[],[],
2771896823,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-07 04:54:07+00:00,[],2025-01-08 06:27:30+00:00,2025-01-08 06:27:29+00:00,https://github.com/tensorflow/tensorflow/pull/84255,[],[],
2771894225,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-07 04:52:13+00:00,[],2025-01-07 06:49:24+00:00,,https://github.com/tensorflow/tensorflow/pull/84254,[],[],
2771890568,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-07 04:48:07+00:00,[],2025-01-07 04:48:07+00:00,,https://github.com/tensorflow/tensorflow/pull/84253,[],[],
2771883549,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-07 04:41:11+00:00,[],2025-01-08 05:56:55+00:00,2025-01-08 05:56:54+00:00,https://github.com/tensorflow/tensorflow/pull/84252,[],[],
2771881727,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-07 04:39:05+00:00,[],2025-01-14 04:50:17+00:00,,https://github.com/tensorflow/tensorflow/pull/84251,[],[],
2771878292,pull_request,open,,Automated Code Change,"Automated Code Change

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20996 from openxla:fix_mistype f6f4a3f81f0cd893e6fcc9c99ab03732a32c1af7
",copybara-service[bot],2025-01-07 04:36:40+00:00,[],2025-01-07 04:36:40+00:00,,https://github.com/tensorflow/tensorflow/pull/84250,[],[],
2771874636,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-07 04:32:53+00:00,[],2025-01-08 10:53:43+00:00,2025-01-08 10:53:43+00:00,https://github.com/tensorflow/tensorflow/pull/84249,[],[],
2771873331,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-07 04:31:22+00:00,[],2025-01-07 07:41:00+00:00,2025-01-07 07:41:00+00:00,https://github.com/tensorflow/tensorflow/pull/84248,[],[],
2771870092,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-07 04:28:20+00:00,[],2025-01-07 04:28:20+00:00,,https://github.com/tensorflow/tensorflow/pull/84247,[],[],
2771863733,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-07 04:21:11+00:00,[],2025-01-07 06:47:23+00:00,2025-01-07 06:47:23+00:00,https://github.com/tensorflow/tensorflow/pull/84246,[],[],
2771845782,pull_request,closed,,[xla:cpu] Move MpiCommunicator to backends/cpu/collectives,"[xla:cpu] Move MpiCommunicator to backends/cpu/collectives
",copybara-service[bot],2025-01-07 04:00:19+00:00,['ezhulenev'],2025-01-08 05:22:16+00:00,2025-01-08 05:22:16+00:00,https://github.com/tensorflow/tensorflow/pull/84245,[],[],
2771845664,pull_request,closed,,[xla:cpu] Move GlooCommunicator to backends/cpu/collectives,"[xla:cpu] Move GlooCommunicator to backends/cpu/collectives
",copybara-service[bot],2025-01-07 04:00:13+00:00,['ezhulenev'],2025-01-08 04:09:06+00:00,2025-01-08 04:09:05+00:00,https://github.com/tensorflow/tensorflow/pull/84244,[],[],
2771843677,pull_request,closed,,[xla:cpu] Move InProcessCommunicator to backends/cpu/collectives,"[xla:cpu] Move InProcessCommunicator to backends/cpu/collectives
",copybara-service[bot],2025-01-07 03:57:44+00:00,['ezhulenev'],2025-01-08 02:12:34+00:00,2025-01-08 02:12:33+00:00,https://github.com/tensorflow/tensorflow/pull/84243,[],[],
2771840284,pull_request,closed,,Fold `xla::PjRtXlaLayout` into `xla::PjRtLayout` for simplification,"Fold `xla::PjRtXlaLayout` into `xla::PjRtLayout` for simplification

`xla::PjRtLayout` was designed as an abstract class so that it leaves options to represent layouts without depending on `xla::Layout`. In reality, `xla::PjRtXlaLayout` is the only concrete layout representation that will exist in the foreseeable future, and the lack of a proper type-erased layout creation interface forces everyone to use unsafe downcast to access the underlying layout. This causes an unnecessary code bloat without much extensibility because too many downcasts practically prevent new layout representations from being easily introduced.

This CL folds `xla::PjRtXlaLayout` into `xla::PjRtLayout` and make `xla::PjRtLayout` a non-abstract class. Like `xla::Shape` that is used pervasively in PjRt, this CL makes layouts a concrete type based on `xla::Layout`. The benefit is that it simplifies many callers that use PjRt layouts: `xla::GetXlaLayoutUnsafe()` is now replaced with the `pjrt_layout->xla_layout()` accessor, no more `down_cast`/`dynamic_cast` to access `xla::PjRtXlaLayout`, etc.

`xla::ifrt::BasicStringArrayLayout` was the only other implementation of `xla::PjRtLayout` and this is now removed. Since string arrays are supported only in IFRT and not in PjRt, its layout representation should also live only in IFRT. Since no one depends on string array layouts, this CL simply removes its implementation so that we can add a proper one once a proper IFRT layout type is added.
",copybara-service[bot],2025-01-07 03:53:35+00:00,[],2025-01-09 04:49:16+00:00,2025-01-09 04:49:14+00:00,https://github.com/tensorflow/tensorflow/pull/84242,[],[],
2771816258,pull_request,closed,,Update to match upstream API change (NFC).,"Update to match upstream API change (NFC).

This method was renamed but staging function kept, switch to renamed variant.
",copybara-service[bot],2025-01-07 03:26:04+00:00,['jpienaar'],2025-01-07 05:43:16+00:00,2025-01-07 05:43:15+00:00,https://github.com/tensorflow/tensorflow/pull/84241,[],[],
2771771026,pull_request,closed,,PR #20976: [GPU][NFC] Add missing override specifier,"PR #20976: [GPU][NFC] Add missing override specifier

Imported from GitHub PR https://github.com/openxla/xla/pull/20976


Copybara import of the project:

--
9bfc792476528d411438eebf781042e02dd7af22 by Ilia Sergachev <isergachev@nvidia.com>:

[GPU][NFC] Add missing override specifier

Merging this change closes #20976

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20976 from openxla:override_specifier 9bfc792476528d411438eebf781042e02dd7af22
",copybara-service[bot],2025-01-07 02:35:27+00:00,[],2025-01-07 03:27:48+00:00,2025-01-07 03:27:47+00:00,https://github.com/tensorflow/tensorflow/pull/84240,[],[],
2771739257,pull_request,closed,,Optimize the partitioner for dynamic-slice operations.,"Optimize the partitioner for dynamic-slice operations.

1. Replicate along the slice dims to get temp_sharding.
2. Reshard the input to temp_sharding.
3. Apply dynamic slice with temp_sharding.
4. Reshard the output from temp_sharding to the final sharding.

Before this change, we will replicate the input if there exists a sharded slice dim, which is sub-optimal. Taking the added test target `DynamicSlicePartitionedBothDimensions` as an example,

```
ENTRY entry {
  %input = s32[128,64] parameter(0), sharding={devices=[2,2]<=[4]}
  %index = s32[] parameter(1)
  %trivial_index = s32[] parameter(2)
  ROOT %dynamic-slice = s32[128,16] dynamic-slice(%input, %trivial_index, %index), dynamic_slice_sizes={128,16}, sharding={devices=[2,2]<=[4]}
}
```

Previous, the partitioner generated the following result
```
ENTRY %entry_spmd (param: s32[64,32], param.2: s32[], param.1: s32[]) -> s32[64,8] {
  %param = s32[64,32]{1,0} parameter(0), sharding={devices=[2,2]<=[4]}
  %all-gather = s32[64,64]{1,0} all-gather(s32[64,32]{1,0} %param), channel_id=1, replica_groups=[2,2]<=[4], dimensions={1}, use_global_device_ids=true
  %all-gather.1 = s32[128,64]{1,0} all-gather(s32[64,64]{1,0} %all-gather), channel_id=2, replica_groups=[2,2]<=[2,2]T(1,0), dimensions={0}, use_global_device_ids=true
  %param.1 = s32[] parameter(2), sharding={replicated}
  %param.2 = s32[] parameter(1), sharding={replicated}
  %dynamic-slice.1 = s32[128,16]{1,0} dynamic-slice(s32[128,64]{1,0} %all-gather.1, s32[] %param.1, s32[] %param.2), dynamic_slice_sizes={128,16}
  %constant = s32[4]{0} constant({0, 0, 64, 64})
  %partition-id = u32[] partition-id()
  %dynamic-slice.2 = s32[1]{0} dynamic-slice(s32[4]{0} %constant, u32[] %partition-id), dynamic_slice_sizes={1}
  %reshape = s32[] reshape(s32[1]{0} %dynamic-slice.2)
  %constant.1 = s32[4]{0} constant({0, 8, 0, 8})
  %dynamic-slice.3 = s32[1]{0} dynamic-slice(s32[4]{0} %constant.1, u32[] %partition-id), dynamic_slice_sizes={1}
  %reshape.1 = s32[] reshape(s32[1]{0} %dynamic-slice.3)
  ROOT %dynamic-slice.4 = s32[64,8]{1,0} dynamic-slice(s32[128,16]{1,0} %dynamic-slice.1, s32[] %reshape, s32[] %reshape.1), dynamic_slice_sizes={64,8}
}
```

With this change, the result is
```
ENTRY %entry_spmd (param: s32[64,32], param.2: s32[], param.1: s32[]) -> s32[64,8] {
  %param.1 = s32[] parameter(2), sharding={replicated}
  %param = s32[64,32]{1,0} parameter(0), sharding={devices=[2,2]<=[4]}
  %all-gather = s32[64,64]{1,0} all-gather(s32[64,32]{1,0} %param), channel_id=1, replica_groups=[2,2]<=[4], dimensions={1}, use_global_device_ids=true
  %constant.3 = s32[] constant(0)
  %param.2 = s32[] parameter(1), sharding={replicated}
  %dynamic-slice.4 = s32[64,16]{1,0} dynamic-slice(s32[64,64]{1,0} %all-gather, s32[] %constant.3, s32[] %param.2), dynamic_slice_sizes={64,16}
  %constant.7 = s32[4]{0} constant({0, 0, 64, 64})
  %partition-id = u32[] partition-id()
  %dynamic-slice.6 = s32[1]{0} dynamic-slice(s32[4]{0} %constant.7, u32[] %partition-id), dynamic_slice_sizes={1}
  %reshape.4 = s32[] reshape(s32[1]{0} %dynamic-slice.6)
  %subtract = s32[] subtract(s32[] %reshape.4, s32[] %reshape.4)
  %constant.8 = s32[4]{0} constant({0, 8, 0, 8})
  %dynamic-slice.7 = s32[1]{0} dynamic-slice(s32[4]{0} %constant.8, u32[] %partition-id), dynamic_slice_sizes={1}
  %reshape.5 = s32[] reshape(s32[1]{0} %dynamic-slice.7)
  %subtract.1 = s32[] subtract(s32[] %reshape.5, s32[] %constant.3)
  ROOT %dynamic-slice.9 = s32[64,8]{1,0} dynamic-slice(s32[64,16]{1,0} %dynamic-slice.4, s32[] %subtract, s32[] %subtract.1), dynamic_slice_sizes={64,8}
}
```
",copybara-service[bot],2025-01-07 01:56:13+00:00,[],2025-01-07 04:59:49+00:00,2025-01-07 04:59:49+00:00,https://github.com/tensorflow/tensorflow/pull/84239,[],[],
2771733455,pull_request,closed,,Add DutyCycleTracker to open source.,"Add DutyCycleTracker to open source.
",copybara-service[bot],2025-01-07 01:48:50+00:00,[],2025-01-10 19:04:32+00:00,2025-01-10 19:04:32+00:00,https://github.com/tensorflow/tensorflow/pull/84238,[],[],
2771719271,pull_request,closed,,Switch to using bytes field for CoreDetails instead of string.,"Switch to using bytes field for CoreDetails instead of string.
",copybara-service[bot],2025-01-07 01:31:56+00:00,[],2025-01-07 19:22:58+00:00,2025-01-07 19:22:57+00:00,https://github.com/tensorflow/tensorflow/pull/84237,[],[],
2771716097,pull_request,closed,,PR #21037: Typo Fix,"PR #21037: Typo Fix

Imported from GitHub PR https://github.com/openxla/xla/pull/21037


Copybara import of the project:

--
588990f2fee70a9237faeff6e1ed17161c770163 by flyingcat <1004815462@qq.com>:

Typo Fix

Merging this change closes #21037

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21037 from knightXun:knightXun-patch-1 588990f2fee70a9237faeff6e1ed17161c770163
",copybara-service[bot],2025-01-07 01:28:17+00:00,[],2025-01-07 02:42:03+00:00,2025-01-07 02:42:03+00:00,https://github.com/tensorflow/tensorflow/pull/84236,[],[],
2771711879,pull_request,open,,Refactor OverrideCompileOptionFromOptionOverrides to put into common config.,"Refactor OverrideCompileOptionFromOptionOverrides to put into common config.

This will allow some consolidation of tpu compilation environment work in the future.
",copybara-service[bot],2025-01-07 01:23:00+00:00,[],2025-01-07 18:08:51+00:00,,https://github.com/tensorflow/tensorflow/pull/84235,[],[],
2771675661,pull_request,closed,,Handle INT64 shapes correctly for resource_variable_ops. Fix other parts of,"Handle INT64 shapes correctly for resource_variable_ops. Fix other parts of
TF graph generation code where INT64 shapes were not handled correctly.
",copybara-service[bot],2025-01-07 00:37:51+00:00,[],2025-01-11 03:27:18+00:00,2025-01-11 03:27:16+00:00,https://github.com/tensorflow/tensorflow/pull/84234,[],[],
2771673751,pull_request,open,,Integrate LLVM at llvm/llvm-project@c1ea05eaf0fb,"Integrate LLVM at llvm/llvm-project@c1ea05eaf0fb

Updates LLVM usage to match
[c1ea05eaf0fb](https://github.com/llvm/llvm-project/commit/c1ea05eaf0fb)
",copybara-service[bot],2025-01-07 00:35:32+00:00,[],2025-01-07 00:35:32+00:00,,https://github.com/tensorflow/tensorflow/pull/84233,[],[],
2771670077,pull_request,open,,Integrate LLVM at llvm/llvm-project@743aee4951d4,"Integrate LLVM at llvm/llvm-project@743aee4951d4

Updates LLVM usage to match
[743aee4951d4](https://github.com/llvm/llvm-project/commit/743aee4951d4)
",copybara-service[bot],2025-01-07 00:31:18+00:00,[],2025-01-07 00:31:18+00:00,,https://github.com/tensorflow/tensorflow/pull/84232,[],[],
2771644222,pull_request,closed,,Implement `ConcreteSharding::GetShardShape()` for cases where all per-shard shapes are the same,"Implement `ConcreteSharding::GetShardShape()` for cases where all per-shard shapes are the same

Ideally, this should've used `ConcreteEvenSharding`, but there are many existing places that unconditionally instantiate `ConcreteSharding` from a list of per-shard shapes without checking for identical per-shard shapes. This CL avoids callers from having to special case `ConcreteSharding` when the callsites require identical per-shard shapes.
",copybara-service[bot],2025-01-06 23:59:48+00:00,[],2025-01-07 03:49:53+00:00,2025-01-07 03:49:53+00:00,https://github.com/tensorflow/tensorflow/pull/84231,[],[],
2771631108,pull_request,open,,Remove special handling for Reduce in HostOffloader.,"Remove special handling for Reduce in HostOffloader.
",copybara-service[bot],2025-01-06 23:44:39+00:00,['SandSnip3r'],2025-01-06 23:44:40+00:00,,https://github.com/tensorflow/tensorflow/pull/84230,[],[],
2771608631,pull_request,closed,,Reverts 9055c056336ab90f4c54e24dc9a77ce7afd85166,"Reverts 9055c056336ab90f4c54e24dc9a77ce7afd85166
",copybara-service[bot],2025-01-06 23:22:33+00:00,[],2025-01-07 02:54:06+00:00,2025-01-07 02:54:05+00:00,https://github.com/tensorflow/tensorflow/pull/84229,[],[],
2771584732,pull_request,closed,,[MultiHostHloRunner] Add `XSpaceProfilerInterface` that supports returning XSpace proto in program after profiling,"[MultiHostHloRunner] Add `XSpaceProfilerInterface` that supports returning XSpace proto in program after profiling
",copybara-service[bot],2025-01-06 22:57:13+00:00,[],2025-01-24 02:24:19+00:00,2025-01-24 02:24:18+00:00,https://github.com/tensorflow/tensorflow/pull/84228,[],[],
2771574557,pull_request,closed,,- Fix op_profile deduplicated grouping by including the root dedup node whose deduplicated op name is empty string,"- Fix op_profile deduplicated grouping by including the root dedup node whose deduplicated op name is empty string
- Fixed op limit control on op_profile UI
",copybara-service[bot],2025-01-06 22:48:24+00:00,['zzzaries'],2025-01-07 02:19:04+00:00,2025-01-07 02:19:04+00:00,https://github.com/tensorflow/tensorflow/pull/84227,[],[],
2771562572,pull_request,closed,,Enabling folding of transpose ops with per-axis quant inputs.,"Enabling folding of transpose ops with per-axis quant inputs.
",copybara-service[bot],2025-01-06 22:37:19+00:00,['majiddadashi'],2025-01-07 02:06:25+00:00,2025-01-07 02:06:24+00:00,https://github.com/tensorflow/tensorflow/pull/84226,[],[],
2771559353,pull_request,closed,,Fix `HloRunnerAgnosticTestBase` includes.,"Fix `HloRunnerAgnosticTestBase` includes.

Many of the tests that extend `HloTestBase` rely on symbols included
transitively.  The main ones are:

- `PlatformUtil`
- `LiteralUtil`
- `LiteralTestUtil`

This patch adds includes for these explicitly.
",copybara-service[bot],2025-01-06 22:34:28+00:00,[],2025-01-08 00:37:53+00:00,2025-01-08 00:37:52+00:00,https://github.com/tensorflow/tensorflow/pull/84225,[],[],
2771523661,pull_request,closed,,Add new class xla::ifrt::PjRtMemoryDescription.,"Add new class xla::ifrt::PjRtMemoryDescription.

(This only adds the class, in preparation of plumbing memory descriptions through IFRT. No functional changes yet.)
",copybara-service[bot],2025-01-06 22:03:44+00:00,[],2025-01-07 21:25:57+00:00,2025-01-07 21:25:57+00:00,https://github.com/tensorflow/tensorflow/pull/84224,[],[],
2771509600,pull_request,closed,,[XLA:LatencyHidingScheduler] Do not assume the operand of a recv-done (or send-done) is always a recv (or send). This code change fixes the use_of_uninitialized_value runtime error that was caused by calling is_host_transfer on a failed casting operation in the `OutOfOrderStartAndDone` test (due to the operand of recv-done not being a recv op).,"[XLA:LatencyHidingScheduler] Do not assume the operand of a recv-done (or send-done) is always a recv (or send). This code change fixes the use_of_uninitialized_value runtime error that was caused by calling is_host_transfer on a failed casting operation in the `OutOfOrderStartAndDone` test (due to the operand of recv-done not being a recv op).
",copybara-service[bot],2025-01-06 21:52:44+00:00,['seherellis'],2025-01-06 23:26:57+00:00,2025-01-06 23:26:56+00:00,https://github.com/tensorflow/tensorflow/pull/84223,[],[],
2771478788,pull_request,closed,,Add missing split markers to test,"Add missing split markers to test
",copybara-service[bot],2025-01-06 21:29:54+00:00,['ghpvnist'],2025-01-06 22:21:42+00:00,2025-01-06 22:21:41+00:00,https://github.com/tensorflow/tensorflow/pull/84222,[],[],
2771432751,pull_request,open,,Save TF wheel version and suffix in repository rule.,"Save TF wheel version and suffix in repository rule.
",copybara-service[bot],2025-01-06 20:56:35+00:00,[],2025-01-14 00:47:34+00:00,,https://github.com/tensorflow/tensorflow/pull/84221,[],[],
2771427579,pull_request,closed,,"[Coordination Service]Allow restartable tasks to connect back to cluster, as long as they have the same local topology as before.","[Coordination Service]Allow restartable tasks to connect back to cluster, as long as they have the same local topology as before.
",copybara-service[bot],2025-01-06 20:52:51+00:00,['ishark'],2025-01-11 00:59:24+00:00,2025-01-11 00:59:23+00:00,https://github.com/tensorflow/tensorflow/pull/84220,[],[],
2771406461,pull_request,closed,,Reverts 04dd53811eb0b694a41cdd91767f6f452605387a,"Reverts 04dd53811eb0b694a41cdd91767f6f452605387a
",copybara-service[bot],2025-01-06 20:37:47+00:00,['ishark'],2025-01-07 20:17:51+00:00,2025-01-07 20:17:49+00:00,https://github.com/tensorflow/tensorflow/pull/84219,[],[],
2771365458,pull_request,closed,,Add `device_count` accessor to `HloRunnerInterface`.,"Add `device_count` accessor to `HloRunnerInterface`.

Also fixes hlo_runner_interface includes.
",copybara-service[bot],2025-01-06 20:11:04+00:00,[],2025-01-09 04:01:36+00:00,2025-01-09 04:01:35+00:00,https://github.com/tensorflow/tensorflow/pull/84218,[],[],
2771363778,pull_request,closed,,Remove `SKIP_TEST_IF_NUM_DEVICES_LESS_THAN` macro.,"Remove `SKIP_TEST_IF_NUM_DEVICES_LESS_THAN` macro.

Macros should be [avoided whenever
possible](https://google.github.io/styleguide/cppguide.html#Preprocessor_Macros).
The `SKIP_TEST_IF_NUM_DEVICES_LESS_THAN` macro does two things. It inserts a new
field `num_devices`, polluting the scope of the rest of the test. It also adds
an implicit/non-obvious dependency on the runner.

This patch removes the macro and switches any remaining uses to use
`HloRunnerInterface::device_count` with an explicit message instead.
",copybara-service[bot],2025-01-06 20:09:55+00:00,[],2025-01-09 18:51:04+00:00,2025-01-09 18:51:03+00:00,https://github.com/tensorflow/tensorflow/pull/84217,[],[],
2771359166,pull_request,closed,,Migrate replicated_io_feed_test to always use PjRt for its test backend.,"Migrate replicated_io_feed_test to always use PjRt for its test backend.
",copybara-service[bot],2025-01-06 20:06:38+00:00,[],2025-01-09 20:35:38+00:00,2025-01-09 20:35:37+00:00,https://github.com/tensorflow/tensorflow/pull/84216,[],[],
2771340556,pull_request,closed,,Remove unnecessary 4D operand checks for dynamic update slice. We have 4D operand support in the shader.,"Remove unnecessary 4D operand checks for dynamic update slice. We have 4D operand support in the shader.
",copybara-service[bot],2025-01-06 19:55:57+00:00,[],2025-01-06 21:49:21+00:00,2025-01-06 21:49:20+00:00,https://github.com/tensorflow/tensorflow/pull/84215,[],[],
2771319001,pull_request,closed,,Fix invalid memory access.,"Fix invalid memory access.
",copybara-service[bot],2025-01-06 19:41:58+00:00,[],2025-01-06 22:55:12+00:00,2025-01-06 22:55:11+00:00,https://github.com/tensorflow/tensorflow/pull/84214,[],[],
2771296495,pull_request,closed,,Add a flag protected pass to lower fake_quant annotation.,"Add a flag protected pass to lower fake_quant annotation.

LowerQuantAnnotationsPass is added which lowers quant.fake_quant composites to a pair of tfl.Quantize-tfl.Dequantize ops which are later consumed by the converter quantization passes.
",copybara-service[bot],2025-01-06 19:26:38+00:00,['majiddadashi'],2025-01-06 23:40:48+00:00,2025-01-06 23:40:48+00:00,https://github.com/tensorflow/tensorflow/pull/84213,[],[],
2771267804,pull_request,closed,,Remove #ifdef TENSORFLOW_USE_ROCM from array_elementwise_ops_test.cc.,"Remove #ifdef TENSORFLOW_USE_ROCM from array_elementwise_ops_test.cc.
",copybara-service[bot],2025-01-06 19:06:49+00:00,[],2025-01-07 17:37:11+00:00,2025-01-07 17:37:09+00:00,https://github.com/tensorflow/tensorflow/pull/84212,[],[],
2771234569,pull_request,closed,,Simplify the test targets in SPMD partitioner for reverse operations.,"Simplify the test targets in SPMD partitioner for reverse operations.
",copybara-service[bot],2025-01-06 18:44:54+00:00,[],2025-01-07 02:27:23+00:00,2025-01-07 02:27:22+00:00,https://github.com/tensorflow/tensorflow/pull/84211,[],[],
2771186170,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-06 18:13:43+00:00,[],2025-01-09 20:41:52+00:00,2025-01-09 20:41:50+00:00,https://github.com/tensorflow/tensorflow/pull/84210,[],[],
2771150150,pull_request,closed,,Forward `use_spmd_partitioning` in HloRunnerPjRt.,"Forward `use_spmd_partitioning` in HloRunnerPjRt.

This patch also removes an unused and redundant invocation of
`GenerateDefaultCompileOptions`.
",copybara-service[bot],2025-01-06 17:50:23+00:00,[],2025-01-07 23:29:43+00:00,2025-01-07 23:29:42+00:00,https://github.com/tensorflow/tensorflow/pull/84209,[],[],
2771058762,pull_request,closed,,[xla:cpu] Extend Dot benchmark test to support BF16 data type.,"[xla:cpu] Extend Dot benchmark test to support BF16 data type.

+ Add a variant of `LiteralUtil::CreateRandomLiteral` that takes `mean` and `stddev` of type double.
",copybara-service[bot],2025-01-06 16:56:34+00:00,['penpornk'],2025-01-07 18:09:22+00:00,2025-01-07 18:09:21+00:00,https://github.com/tensorflow/tensorflow/pull/84208,[],[],
2770849293,pull_request,closed,,[XLA:TPU] Disable optimization passes based on effort flag,"[XLA:TPU] Disable optimization passes based on effort flag
",copybara-service[bot],2025-01-06 15:02:53+00:00,[],2025-01-16 21:31:51+00:00,2025-01-16 21:31:50+00:00,https://github.com/tensorflow/tensorflow/pull/84207,[],[],
2770627953,pull_request,open,,PR #21022: [XLA:CPU][oneDNN] Modify addend shape for small Convolution + Bias contraction,"PR #21022: [XLA:CPU][oneDNN] Modify addend shape for small Convolution + Bias contraction

Imported from GitHub PR https://github.com/openxla/xla/pull/21022

For small constant biases, XLA may perform constant folding, which can alter the shape of the bias passed to oneDNN. This PR removes any extraneous trivial dimensions from the bias that is passed to the oneDNN library. It also adds a test in all applicable dtypes to test the functionality.
Copybara import of the project:

--
5ce5ba690d5acd1ebedca1484483639b2f3b0be1 by Akhil Goel <akhil.goel@intel.com>:

Modify conv bias shape

Merging this change closes #21022

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/21022 from Intel-tensorflow:akhil/conv_bias_fix 5ce5ba690d5acd1ebedca1484483639b2f3b0be1
",copybara-service[bot],2025-01-06 13:08:44+00:00,[],2025-01-11 01:38:49+00:00,,https://github.com/tensorflow/tensorflow/pull/84206,[],"[{'comment_id': 2573078668, 'issue_id': 2770627953, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/84206/checks?check_run_id=35197997641) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2025, 1, 6, 13, 8, 50, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2025-01-06 13:08:50 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/84206/checks?check_run_id=35197997641) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2770431374,pull_request,closed,,Drop shard barrier custom calls in sharding-remover HLO pass.,"Drop shard barrier custom calls in sharding-remover HLO pass.

This enables them to be no-ops for SingleDeviceSharding.
",copybara-service[bot],2025-01-06 11:14:56+00:00,[],2025-01-09 22:02:55+00:00,2025-01-09 22:02:54+00:00,https://github.com/tensorflow/tensorflow/pull/84204,[],[],
2770351993,pull_request,closed,,Change the name of the ir dumps to have the suffix before the -ir-(with/no)-opt,"Change the name of the ir dumps to have the suffix before the -ir-(with/no)-opt
",copybara-service[bot],2025-01-06 10:30:33+00:00,[],2025-01-07 10:32:54+00:00,2025-01-07 10:32:54+00:00,https://github.com/tensorflow/tensorflow/pull/84202,[],[],
2770248156,pull_request,closed,,PR #20744: [NVIDIA GPU] Add a flag to control a2a collective matmul rewrite,"PR #20744: [NVIDIA GPU] Add a flag to control a2a collective matmul rewrite

Imported from GitHub PR https://github.com/openxla/xla/pull/20744

This is address the revert in https://github.com/openxla/xla/pull/19451 where customers see MFU when enabling collective matmul by default. The a2a collective matmul kicks in by default on some small gemms and lead to inefficient transformation.
Adding a flag to disable it by default since it's experimental.
Copybara import of the project:

--
f3d320881ba0de6cd07429dc00176231fd2a1d9a by TJ Xu <tjx@nvidia.com>:

Add a flag to control a2a collective matmul rewrite

--
0068abc2dba6865debcd71b80b235b268f048e6c by TJ Xu <tjx@nvidia.com>:

added more comment for the new flag

--
9f88fe9a7feba2945aafe087dcdd639348581422 by TJ Xu <tjx@nvidia.com>:

add flag to debug options

Merging this change closes #20744

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/20744 from Tixxx:tixxx/add_flag_a2a_gemm 9f88fe9a7feba2945aafe087dcdd639348581422
",copybara-service[bot],2025-01-06 09:34:32+00:00,[],2025-01-10 10:52:32+00:00,2025-01-10 10:52:31+00:00,https://github.com/tensorflow/tensorflow/pull/84201,[],[],
2770235719,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-06 09:28:56+00:00,[],2025-01-06 09:28:56+00:00,,https://github.com/tensorflow/tensorflow/pull/84200,[],[],
2770205148,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-06 09:14:52+00:00,[],2025-01-06 09:14:52+00:00,,https://github.com/tensorflow/tensorflow/pull/84199,[],[],
2770184215,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-06 09:02:41+00:00,[],2025-01-06 09:02:41+00:00,,https://github.com/tensorflow/tensorflow/pull/84198,[],[],
2770171831,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-06 08:55:20+00:00,[],2025-01-06 08:55:20+00:00,,https://github.com/tensorflow/tensorflow/pull/84197,[],[],
2770163353,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-06 08:51:16+00:00,[],2025-01-06 08:51:16+00:00,,https://github.com/tensorflow/tensorflow/pull/84196,[],[],
2770160083,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2025-01-06 08:49:14+00:00,[],2025-01-06 08:49:14+00:00,,https://github.com/tensorflow/tensorflow/pull/84195,[],[],
