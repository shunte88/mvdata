id,type,state,state_reason,title,body,author,created_at,assignees,updated_at,closed_at,url,labels,comments_list,comment_thread
2433057465,pull_request,open,,Integrate LLVM at llvm/llvm-project@67ad23fe17a5,"Integrate LLVM at llvm/llvm-project@67ad23fe17a5

Updates LLVM usage to match
[67ad23fe17a5](https://github.com/llvm/llvm-project/commit/67ad23fe17a5)
",copybara-service[bot],2024-07-26 22:30:16+00:00,[],2024-07-27 05:53:37+00:00,,https://github.com/tensorflow/tensorflow/pull/72602,[],[],
2433040791,pull_request,open,,This is a test .... Remove lite/flex:delegate from pywrap_tensorflow_internal,"This is a test .... Remove lite/flex:delegate from pywrap_tensorflow_internal
",copybara-service[bot],2024-07-26 22:10:51+00:00,['ecalubaquib'],2024-07-29 19:16:44+00:00,,https://github.com/tensorflow/tensorflow/pull/72601,[],"[{'comment_id': 2253575481, 'issue_id': 2433040791, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72601/checks?check_run_id=27984705660) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 7, 26, 22, 10, 57, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-07-26 22:10:57 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72601/checks?check_run_id=27984705660) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2433011122,pull_request,closed,,Reverts cc5982a5f3ad7dfde136615c991221ae8541ae83,"Reverts cc5982a5f3ad7dfde136615c991221ae8541ae83
",copybara-service[bot],2024-07-26 21:37:26+00:00,[],2024-07-27 05:00:37+00:00,2024-07-27 05:00:36+00:00,https://github.com/tensorflow/tensorflow/pull/72600,[],[],
2433000067,pull_request,open,,[xla:cpu] Update expected target_loss for xla:cpu thunks,"[xla:cpu] Update expected target_loss for xla:cpu thunks
",copybara-service[bot],2024-07-26 21:27:31+00:00,['ezhulenev'],2024-07-26 21:27:32+00:00,,https://github.com/tensorflow/tensorflow/pull/72599,[],[],
2432963545,pull_request,closed,,[xla:cpu] Add support for exporting executables without jit-compiled kernels,"[xla:cpu] Add support for exporting executables without jit-compiled kernels
",copybara-service[bot],2024-07-26 20:52:39+00:00,['ezhulenev'],2024-07-26 21:55:12+00:00,2024-07-26 21:55:11+00:00,https://github.com/tensorflow/tensorflow/pull/72598,[],[],
2432962019,pull_request,closed,,Remove unnecessary dependency on core/c/common.h from stderr_reporter.h.,"Remove unnecessary dependency on core/c/common.h from stderr_reporter.h.
",copybara-service[bot],2024-07-26 20:51:34+00:00,[],2024-08-02 22:44:00+00:00,2024-08-02 22:43:59+00:00,https://github.com/tensorflow/tensorflow/pull/72597,[],[],
2432957979,pull_request,closed,,"Move initialization of embedding tables from TPU device to CPU host during variable creation, to avoid exceeding Trillium's HBM capacity.","Move initialization of embedding tables from TPU device to CPU host during variable creation, to avoid exceeding Trillium's HBM capacity.
",copybara-service[bot],2024-07-26 20:48:20+00:00,[],2024-07-26 23:12:02+00:00,2024-07-26 23:12:02+00:00,https://github.com/tensorflow/tensorflow/pull/72596,[],[],
2432952518,pull_request,closed,,[xla:cpu] Add support for sorting inputs with non-descending layouts,"[xla:cpu] Add support for sorting inputs with non-descending layouts
",copybara-service[bot],2024-07-26 20:43:41+00:00,['ezhulenev'],2024-07-26 22:10:29+00:00,2024-07-26 22:10:29+00:00,https://github.com/tensorflow/tensorflow/pull/72595,[],[],
2432914446,pull_request,closed,,"r2.17 cherry-pick: 3735c15ed47 ""Change the naming of the installer wheel envs""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/3735c15ed472c2a7cea28d138665b8545e456401,tensorflow-jenkins,2024-07-26 20:12:55+00:00,[],2024-07-26 20:18:05+00:00,2024-07-26 20:18:03+00:00,https://github.com/tensorflow/tensorflow/pull/72594,[],[],
2432903401,pull_request,closed,,Seen performance regression.,"Seen performance regression.

Reverts 95714d7d1bd5864675dcebbbf3340bdccbe79ff6
",copybara-service[bot],2024-07-26 20:06:41+00:00,[],2024-07-26 22:03:18+00:00,2024-07-26 22:03:17+00:00,https://github.com/tensorflow/tensorflow/pull/72593,[],[],
2432879245,pull_request,closed,,"r2.17 cherry-pick: 8beec28b91c ""Move installer wheels scripts to OSS""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/8beec28b91ce5363c40171cfecd3e9a5c6d4b9c3,tensorflow-jenkins,2024-07-26 19:55:00+00:00,[],2024-07-26 20:08:16+00:00,2024-07-26 20:08:14+00:00,https://github.com/tensorflow/tensorflow/pull/72592,[],[],
2432855844,pull_request,closed,,PR #15369: [ROCm] Fix build break due to 7cad716,"PR #15369: [ROCm] Fix build break due to 7cad716

Imported from GitHub PR https://github.com/openxla/xla/pull/15369


Copybara import of the project:

--
a1f189a0ff7ef5f88ed45d1e8738b22a4d0029a5 by Harsha HS <Harsha.HavanurShamsundara@amd.com>:

[ROCm] Fix build break due to 7cad716

Merging this change closes #15369

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15369 from ROCm:ci_fix_build_break_20240726 a1f189a0ff7ef5f88ed45d1e8738b22a4d0029a5
",copybara-service[bot],2024-07-26 19:34:39+00:00,[],2024-07-26 21:07:34+00:00,2024-07-26 21:07:34+00:00,https://github.com/tensorflow/tensorflow/pull/72591,[],[],
2432842739,pull_request,closed,,"Improve how HostOffloadLegalize moves copies out of host-memory-only offloading. When a copy is moved over a shape changing bitcast or a dynamic-slice/slice, the shape of the copy also needs to change.","Improve how HostOffloadLegalize moves copies out of host-memory-only offloading. When a copy is moved over a shape changing bitcast or a dynamic-slice/slice, the shape of the copy also needs to change.
",copybara-service[bot],2024-07-26 19:23:38+00:00,['SandSnip3r'],2024-07-26 22:49:16+00:00,2024-07-26 22:49:16+00:00,https://github.com/tensorflow/tensorflow/pull/72590,[],[],
2432841066,pull_request,closed,,Change MakeTensorFromArray to be async,"Change MakeTensorFromArray to be async
",copybara-service[bot],2024-07-26 19:22:21+00:00,[],2024-07-30 17:07:35+00:00,2024-07-30 17:07:34+00:00,https://github.com/tensorflow/tensorflow/pull/72589,[],[],
2432821665,pull_request,closed,,Remove dependency on tflite::FlatBufferModel from flatbuffer_import.cc,"Remove dependency on tflite::FlatBufferModel from flatbuffer_import.cc
",copybara-service[bot],2024-07-26 19:06:44+00:00,[],2024-08-06 20:07:53+00:00,2024-08-06 20:07:52+00:00,https://github.com/tensorflow/tensorflow/pull/72588,[],[],
2432778125,pull_request,closed,,Remove KernelFactory in favor of just calling StreamExecutor::LoadKernel directly.,"Remove KernelFactory in favor of just calling StreamExecutor::LoadKernel directly.
",copybara-service[bot],2024-07-26 18:31:27+00:00,[],2024-07-29 19:06:38+00:00,2024-07-29 19:06:37+00:00,https://github.com/tensorflow/tensorflow/pull/72587,[],[],
2432776727,pull_request,closed,,Combine StreamExecutor::GetKernel and ::CreateKernel into a single new method ::LoadKernel.,"Combine StreamExecutor::GetKernel and ::CreateKernel into a single new method ::LoadKernel.
",copybara-service[bot],2024-07-26 18:30:25+00:00,[],2024-07-29 17:54:06+00:00,2024-07-29 17:54:05+00:00,https://github.com/tensorflow/tensorflow/pull/72586,[],[],
2432721011,pull_request,closed,,Rolling back due to breakages.,"Rolling back due to breakages.

Reverts 91df7e393aa1361a215e7bc6e70480aff6111b41
",copybara-service[bot],2024-07-26 17:58:03+00:00,[],2024-07-26 19:18:53+00:00,2024-07-26 19:18:52+00:00,https://github.com/tensorflow/tensorflow/pull/72584,[],[],
2432691685,pull_request,closed,,Remove cuda_library rule that's missing srcs.,"Remove cuda_library rule that's missing srcs.
",copybara-service[bot],2024-07-26 17:44:37+00:00,[],2024-07-26 21:34:31+00:00,2024-07-26 21:34:30+00:00,https://github.com/tensorflow/tensorflow/pull/72583,[],[],
2432665126,pull_request,closed,,Directly invoke ifrt MakeArrayFromHostBuffer for fully replicated sharding,"Directly invoke ifrt MakeArrayFromHostBuffer for fully replicated sharding
",copybara-service[bot],2024-07-26 17:34:37+00:00,[],2024-07-26 18:26:43+00:00,2024-07-26 18:26:42+00:00,https://github.com/tensorflow/tensorflow/pull/72582,[],[],
2432609823,pull_request,closed,,Fix data race in coordination_service_test.cc,"Fix data race in coordination_service_test.cc
",copybara-service[bot],2024-07-26 16:55:00+00:00,[],2024-07-26 17:27:36+00:00,2024-07-26 17:27:36+00:00,https://github.com/tensorflow/tensorflow/pull/72581,[],[],
2432600867,pull_request,open,,"XNNPACK supports dynamic shapes, allow the delegate to handle them.","XNNPACK supports dynamic shapes, allow the delegate to handle them.
",copybara-service[bot],2024-07-26 16:48:42+00:00,[],2024-07-26 16:48:42+00:00,,https://github.com/tensorflow/tensorflow/pull/72580,[],[],
2432575107,pull_request,closed,,[XLA:GPU] Add a test for triton to tritongpu with sparse dot,"[XLA:GPU] Add a test for triton to tritongpu with sparse dot
",copybara-service[bot],2024-07-26 16:30:55+00:00,[],2024-08-02 16:12:57+00:00,2024-08-02 16:12:57+00:00,https://github.com/tensorflow/tensorflow/pull/72579,[],[],
2432549463,pull_request,open,,PR #15216: Make GpuExecutor::HostMemoryAllocate NUMA aware,"PR #15216: Make GpuExecutor::HostMemoryAllocate NUMA aware

Imported from GitHub PR https://github.com/openxla/xla/pull/15216

This improves the achieved throughput of D2H transfers, for example when checkpointing. For example, there is a ~2x improvement in throughput of overlapped D2H copies from 8xH100 on a DGX node.

Notes:
- `TENSORFLOW_USE_NUMA` is set unconditionally instead of being hidden behind an option; it's not clear from OSS-world if this is an important handle for Google internally.
- `stream_executor::StreamExecutor::HostMemoryDeallocate` now takes the allocation size; all call sites updated. This is required by the `tsl::port::NUMAFree` API.

Copybara import of the project:

--
c8bc494a46a5bc192689c0754428c83d3d951bf3 by Olli Lupton <olupton@nvidia.com>:

stream_executor::StreamExecutor::HostMemoryDeallocate: pass size

--
f50c9acce27aae4931c41ea1a3c1e9fb866c2d14 by Olli Lupton <olupton@nvidia.com>:

GpuExecutor::HostMemory[De]Allocate: NUMA-aware

In the CUDA executor allocate host memory that is close to the device.

--
b8d9927e16ddb249fe35e5ef19e48464726c23a5 by Olli Lupton <olupton@nvidia.com>:

bazel: enable numa-aware by default (FIXME?)

--
d07182fc45647ad50f3480587ea0a70f8895e423 by Olli Lupton <olupton@nvidia.com>:

GpuExecutor::HostMemory[De]Allocate: improve error handling

--
42f930bdc807a5947cf928afb15a44dea9b81f3f by Olli Lupton <olupton@nvidia.com>:

Add unit test for NUMA-aware allocation

--
dc6c68e1252fb98cb4d8b4be4dc3833a6d78494c by Olli Lupton <olupton@nvidia.com>:

workaround failure on platforms that cannot detect numa domains

Merging this change closes #15216

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15216 from olupton:numa-pinning dc6c68e1252fb98cb4d8b4be4dc3833a6d78494c
",copybara-service[bot],2024-07-26 16:13:12+00:00,[],2024-07-29 10:37:00+00:00,,https://github.com/tensorflow/tensorflow/pull/72578,[],[],
2432521756,pull_request,closed,,[XLA:GPU] Enable tests.,"[XLA:GPU] Enable tests.
",copybara-service[bot],2024-07-26 15:56:09+00:00,['frgossen'],2024-07-30 17:37:33+00:00,2024-07-30 17:37:31+00:00,https://github.com/tensorflow/tensorflow/pull/72577,[],[],
2432518102,pull_request,closed,,[XLA:GPU] Fix layout normalization for clamp and select,"[XLA:GPU] Fix layout normalization for clamp and select

The operations allow for limited broadcasting of scalars.
",copybara-service[bot],2024-07-26 15:53:47+00:00,['frgossen'],2024-07-30 15:19:33+00:00,2024-07-30 15:19:33+00:00,https://github.com/tensorflow/tensorflow/pull/72576,[],[],
2432468360,pull_request,closed,,[XLA:GPU] Share common HLO computations between more of the pipeline tests,"[XLA:GPU] Share common HLO computations between more of the pipeline tests
",copybara-service[bot],2024-07-26 15:24:13+00:00,['frgossen'],2024-08-06 22:29:56+00:00,2024-08-06 22:29:56+00:00,https://github.com/tensorflow/tensorflow/pull/72575,[],[],
2432466748,pull_request,closed,,[XLA:GPU] Share common HLO computations between some of the pipeline tests,"[XLA:GPU] Share common HLO computations between some of the pipeline tests
",copybara-service[bot],2024-07-26 15:23:14+00:00,['frgossen'],2024-08-01 18:16:51+00:00,2024-08-01 18:16:50+00:00,https://github.com/tensorflow/tensorflow/pull/72574,[],[],
2432466477,pull_request,closed,,[XLA:GPU] Fix test names DFS -> BFS,"[XLA:GPU] Fix test names DFS -> BFS
",copybara-service[bot],2024-07-26 15:23:04+00:00,['frgossen'],2024-08-08 18:22:25+00:00,2024-08-08 18:22:23+00:00,https://github.com/tensorflow/tensorflow/pull/72573,[],[],
2432466308,pull_request,closed,,[XLA:GPU] Share common HLO computations between all of the pipeline tests,"[XLA:GPU] Share common HLO computations between all of the pipeline tests
",copybara-service[bot],2024-07-26 15:22:58+00:00,['frgossen'],2024-08-07 22:30:09+00:00,2024-08-07 22:30:09+00:00,https://github.com/tensorflow/tensorflow/pull/72572,[],[],
2432462216,pull_request,closed,,[XLA:GPU] Add forgotten template arg,"[XLA:GPU] Add forgotten template arg
",copybara-service[bot],2024-07-26 15:20:38+00:00,['frgossen'],2024-08-01 17:15:51+00:00,2024-08-01 17:15:50+00:00,https://github.com/tensorflow/tensorflow/pull/72571,[],[],
2432298067,pull_request,open,,Integrate LLVM at llvm/llvm-project@67ad23fe17a5,"Integrate LLVM at llvm/llvm-project@67ad23fe17a5

Updates LLVM usage to match
[67ad23fe17a5](https://github.com/llvm/llvm-project/commit/67ad23fe17a5)
",copybara-service[bot],2024-07-26 13:53:53+00:00,[],2024-07-26 13:53:53+00:00,,https://github.com/tensorflow/tensorflow/pull/72570,[],[],
2432264503,pull_request,closed,,Have a version of the shardings as mhlo.shardings during SDY round-tripping.,"Have a version of the shardings as mhlo.shardings during SDY round-tripping.
",copybara-service[bot],2024-07-26 13:38:58+00:00,[],2024-07-30 12:53:18+00:00,2024-07-30 12:53:17+00:00,https://github.com/tensorflow/tensorflow/pull/72569,[],[],
2432165152,pull_request,closed,,Create new builder & verifier for IndexingMapAttr,"Create new builder & verifier for IndexingMapAttr
",copybara-service[bot],2024-07-26 12:44:57+00:00,[],2024-07-29 11:31:53+00:00,2024-07-29 11:31:52+00:00,https://github.com/tensorflow/tensorflow/pull/72568,[],[],
2432135395,pull_request,closed,,[XLA:GPU][MLIR-based emitters] Move XlaGpuDialect to xla_gpu_dialect.cc.,"[XLA:GPU][MLIR-based emitters] Move XlaGpuDialect to xla_gpu_dialect.cc.
",copybara-service[bot],2024-07-26 12:27:34+00:00,['pifon2a'],2024-07-26 13:25:42+00:00,2024-07-26 13:25:41+00:00,https://github.com/tensorflow/tensorflow/pull/72567,[],[],
2432061696,pull_request,closed,,IndexingMapAttr: use aliases to print it always at the top & with a new line.,"IndexingMapAttr: use aliases to print it always at the top & with a new line.
",copybara-service[bot],2024-07-26 11:46:41+00:00,[],2024-07-26 12:32:40+00:00,2024-07-26 12:32:40+00:00,https://github.com/tensorflow/tensorflow/pull/72566,[],[],
2431899635,pull_request,closed,,Add missing patch after latest LLVM integrate,"Add missing patch after latest LLVM integrate
",copybara-service[bot],2024-07-26 10:09:20+00:00,['gflegar'],2024-07-26 12:05:02+00:00,2024-07-26 12:05:01+00:00,https://github.com/tensorflow/tensorflow/pull/72564,[],[],
2431889072,pull_request,closed,,[XLA:GPU] Add Custom Kernel Fusion Autotuner HLO pass.,"[XLA:GPU] Add Custom Kernel Fusion Autotuner HLO pass.
",copybara-service[bot],2024-07-26 10:03:25+00:00,[],2024-07-26 13:47:54+00:00,2024-07-26 13:47:53+00:00,https://github.com/tensorflow/tensorflow/pull/72563,[],[],
2431876809,pull_request,closed,,[XLA:CPU] Align getting device ordinal with current runtime.,"[XLA:CPU] Align getting device ordinal with current runtime.
",copybara-service[bot],2024-07-26 09:58:01+00:00,[],2024-07-30 17:12:27+00:00,2024-07-30 17:12:26+00:00,https://github.com/tensorflow/tensorflow/pull/72562,[],"[{'comment_id': 2252399315, 'issue_id': 2431876809, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72562/checks?check_run_id=27957223747) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 7, 26, 9, 58, 6, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-07-26 09:58:06 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72562/checks?check_run_id=27957223747) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2431675315,pull_request,closed,,[XLA:GPU] Keep outer custom call name in the profiler.,"[XLA:GPU] Keep outer custom call name in the profiler.
",copybara-service[bot],2024-07-26 08:09:17+00:00,[],2024-07-26 17:59:54+00:00,2024-07-26 17:59:54+00:00,https://github.com/tensorflow/tensorflow/pull/72560,[],[],
2431669408,pull_request,closed,,PR #15328: [GPU][NFC] Remove duplicate validation of cuDNN graphs.,"PR #15328: [GPU][NFC] Remove duplicate validation of cuDNN graphs.

Imported from GitHub PR https://github.com/openxla/xla/pull/15328

The other time it happens for these graphs in CudnnGraph::Prepare(): https://github.com/openxla/xla/blob/45dca1a0a1d87f3d3c93fa4175e1df971acddb10/xla/stream_executor/cuda/cuda_dnn.cc#L8359

Copybara import of the project:

--
2b979036bfa52c05a0ce541ab8a89e1f2e6834ee by Ilia Sergachev <isergachev@nvidia.com>:

[GPU][NFC] Remove duplicate validation of cuDNN graphs.

The other time it happens for these graphs in CudnnGraph::Prepare().

Merging this change closes #15328

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15328 from openxla:remove_extra_validation 2b979036bfa52c05a0ce541ab8a89e1f2e6834ee
",copybara-service[bot],2024-07-26 08:05:42+00:00,[],2024-07-26 09:10:22+00:00,2024-07-26 09:10:21+00:00,https://github.com/tensorflow/tensorflow/pull/72559,[],[],
2431655145,pull_request,open,,Automated Code Change,"Automated Code Change

Reverts 0af72d018f00cfefd7829bebc02482882ebea74a
",copybara-service[bot],2024-07-26 07:57:17+00:00,[],2024-07-26 07:57:17+00:00,,https://github.com/tensorflow/tensorflow/pull/72558,[],[],
2431598041,pull_request,closed,,Abhinav rough,"-Adds bf16,f16 for tflite min max operations
-Adds bf16,f16 min max unit tests",abhinavph21,2024-07-26 07:21:29+00:00,['gbaned'],2024-07-26 07:22:13+00:00,2024-07-26 07:22:12+00:00,https://github.com/tensorflow/tensorflow/pull/72557,"[('size:XS', 'CL Change Size: Extra Small')]","[{'comment_id': 2252131595, 'issue_id': 2431598041, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72557/checks?check_run_id=27951189507) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 7, 26, 7, 21, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2252132548, 'issue_id': 2431598041, 'author': 'abhinavph21', 'body': 'opened in different repo', 'created_at': datetime.datetime(2024, 7, 26, 7, 22, 13, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-07-26 07:21:33 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72557/checks?check_run_id=27951189507) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

abhinavph21 (Issue Creator) on (2024-07-26 07:22:13 UTC): opened in different repo

"
2431585897,pull_request,closed,,Reverts 0af72d018f00cfefd7829bebc02482882ebea74a,"Reverts 0af72d018f00cfefd7829bebc02482882ebea74a
",copybara-service[bot],2024-07-26 07:13:41+00:00,[],2024-07-26 07:51:55+00:00,2024-07-26 07:51:54+00:00,https://github.com/tensorflow/tensorflow/pull/72556,[],[],
2431572177,pull_request,closed,,Integrate LLVM at llvm/llvm-project@51d4980a133d,"Integrate LLVM at llvm/llvm-project@51d4980a133d

Updates LLVM usage to match
[51d4980a133d](https://github.com/llvm/llvm-project/commit/51d4980a133d)
",copybara-service[bot],2024-07-26 07:04:50+00:00,[],2024-07-26 11:45:08+00:00,2024-07-26 11:45:07+00:00,https://github.com/tensorflow/tensorflow/pull/72555,[],[],
2431528479,pull_request,closed,,[XLA:GPU][NFC] Force `triton_support_test.cc` to run on GPU.,"[XLA:GPU][NFC] Force `triton_support_test.cc` to run on GPU.

This is a temporary measure to get OSS coverage while the test fails on CPU.
The alternative is completely disabling it in OSS, which is not ideal.
",copybara-service[bot],2024-07-26 06:34:28+00:00,['akuegel'],2024-07-26 07:44:54+00:00,2024-07-26 07:44:54+00:00,https://github.com/tensorflow/tensorflow/pull/72554,[],[],
2431293015,pull_request,closed,,[xla:cpu] Add support for sorting 25 inputs,"[xla:cpu] Add support for sorting 25 inputs
",copybara-service[bot],2024-07-26 02:31:06+00:00,['ezhulenev'],2024-07-26 03:33:06+00:00,2024-07-26 03:33:06+00:00,https://github.com/tensorflow/tensorflow/pull/72553,[],[],
2431290236,pull_request,closed,,[IFRT] Add memory support to ShardingTest and generalize it into DeviceTest,"[IFRT] Add memory support to ShardingTest and generalize it into DeviceTest

`ShardingTest` in `sharding_test_util` is now used at several IFRT tests when
the tests uses IFRT devices without using real clients. This change extends it
to support a ""host"" memory kind to enable memory-related tests, and generalize
it as `DeviceTest` to match its current use cases beyond sharding tests.
",copybara-service[bot],2024-07-26 02:27:55+00:00,[],2024-07-26 20:23:38+00:00,2024-07-26 20:23:37+00:00,https://github.com/tensorflow/tensorflow/pull/72552,[],[],
2431284934,pull_request,closed,,[XLA:UNSTACKER] Change existing unstacker patterns to support more cases.,"[XLA:UNSTACKER] Change existing unstacker patterns to support more cases.

Previously, DSFusion and NestedDSFusion patterns would match only when the entire stacked operand was being read. With this change, we now support cases where the stacked operand is dynamically sliced where the dynamic-slice is effectively static at compile time.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/69756 from tensorflow:LakshmiKalaKadali-patch-7 df06d3ad03c532d51f00eed33138fefa58649ebe
",copybara-service[bot],2024-07-26 02:20:45+00:00,[],2024-07-26 04:18:52+00:00,2024-07-26 04:18:52+00:00,https://github.com/tensorflow/tensorflow/pull/72551,[],[],
2431272522,pull_request,closed,,[xla:cpu] Don't forget to commit donation transaction on thunk execution error,"[xla:cpu] Don't forget to commit donation transaction on thunk execution error
",copybara-service[bot],2024-07-26 02:04:59+00:00,['ezhulenev'],2024-07-26 03:12:13+00:00,2024-07-26 03:12:13+00:00,https://github.com/tensorflow/tensorflow/pull/72550,[],[],
2431268501,pull_request,closed,,Add utility function for determining collectives that are not inside custom fusions.,"Add utility function for determining collectives that are not inside custom fusions.
",copybara-service[bot],2024-07-26 01:59:29+00:00,[],2024-08-14 05:59:40+00:00,2024-08-14 05:59:40+00:00,https://github.com/tensorflow/tensorflow/pull/72549,[],[],
2431253538,pull_request,closed,,[XLA:UNSTACKER] Changes to unstacker to improve performance.,"[XLA:UNSTACKER] Changes to unstacker to improve performance.

For DSFusionPattern and NestedDSFusionPattern:
1) Bring out the dynamic-slice inside the unstacking fusion computation.
2) Convert dynamic-slice instructions to slices (this is done to allow MSA to convert then to async-slice later)
3) Keep the bitcast fusion right next to the user inside the loop
",copybara-service[bot],2024-07-26 01:49:50+00:00,[],2024-07-26 02:27:32+00:00,2024-07-26 02:27:30+00:00,https://github.com/tensorflow/tensorflow/pull/72548,[],[],
2431220322,pull_request,open,,Integrate LLVM at llvm/llvm-project@51d4980a133d,"Integrate LLVM at llvm/llvm-project@51d4980a133d

Updates LLVM usage to match
[51d4980a133d](https://github.com/llvm/llvm-project/commit/51d4980a133d)

Reverts 27ca5e057e4d66e84c4abdc27449fb318410588b
",copybara-service[bot],2024-07-26 01:12:06+00:00,[],2024-07-26 01:12:06+00:00,,https://github.com/tensorflow/tensorflow/pull/72547,[],[],
2431205107,pull_request,closed,,Add IsSubnormal utility function for exhaustive tests,"Add IsSubnormal utility function for exhaustive tests
",copybara-service[bot],2024-07-26 00:52:22+00:00,[],2024-07-26 02:20:23+00:00,2024-07-26 02:20:22+00:00,https://github.com/tensorflow/tensorflow/pull/72546,[],[],
2431201165,pull_request,closed,,Add long polling as a new way to propagate error in coordination service.,"Add long polling as a new way to propagate error in coordination service.
",copybara-service[bot],2024-07-26 00:47:01+00:00,[],2024-07-26 04:26:29+00:00,2024-07-26 04:26:28+00:00,https://github.com/tensorflow/tensorflow/pull/72545,[],[],
2431177397,pull_request,closed,,Remove unnecessary parentheses to fix build breakage from cl/656106821.,"Remove unnecessary parentheses to fix build breakage from cl/656106821.

Reverts 27ca5e057e4d66e84c4abdc27449fb318410588b
",copybara-service[bot],2024-07-26 00:30:14+00:00,[],2024-07-26 02:00:28+00:00,2024-07-26 02:00:27+00:00,https://github.com/tensorflow/tensorflow/pull/72544,[],[],
2431168109,pull_request,open,,[XLA:SPMD] Do the element-wise operation and then reshard to output sharding when the operand has more tiles than the output.,"[XLA:SPMD] Do the element-wise operation and then reshard to output sharding when the operand has more tiles than the output.

Given the following pattern
```
A is a tensor with sharding S1
B = foo(A), foo is an element-wise operation and B has sharding S2
```

Before this cl, we always convert it into
```
A with S2 = reshard(A with S1)
B with S2 = foo(A with S2)
```

This cl will convert it into the following pattern if S1 has more tiles. Namely, A has a smaller local shape.
```
B with S1 = foo(A with S1)
B with S2 = reshard(B with S1)
```

For example, if A and B has shape [16], A is sharded in 4 ways, B is sharded in 2 ways, we can do the computation and then reshard. Thus, the computation on each device is on a smaller shape.
",copybara-service[bot],2024-07-26 00:23:26+00:00,[],2024-07-26 00:23:26+00:00,,https://github.com/tensorflow/tensorflow/pull/72543,[],[],
2431163757,pull_request,closed,,Change the naming of the installer wheel envs,"Change the naming of the installer wheel envs
",copybara-service[bot],2024-07-26 00:18:38+00:00,['nitins17'],2024-07-26 14:26:26+00:00,2024-07-26 14:26:26+00:00,https://github.com/tensorflow/tensorflow/pull/72542,[],[],
2431135426,pull_request,open,,[IFRT] Introduce Client::AllocateDevices() and DeviceAllocation,"[IFRT] Introduce Client::AllocateDevices() and DeviceAllocation

`xla::ifrt::Client::AllocateDevices()` is a new API that processes a user
request for getting an ordered set of devices that satisfies constraints
specified in the request. It returns a new runtime object `DeviceAllocation`,
which represents the allocated devices as a collection, which will be used
where `DeviceList` is used today.

Main use cases of the new API will include:

* The user can get a set of devices in a platform-specific manner (e.g., TPU
user code typically uses a logical ""mesh"" that consists of devices ordered in
a certain way to make collectives run efficiently).

* The user can get a set of devices in relation to other set of devices
(finding CPU devices colocated with accelerator devices).

* The user can request allocating a new set of devices that do not overlap with
in-use device to run computations in parallel.

This initial change implements the interface of the API and object type, and a
simple implementation `BasicDeviceAllocation` that wraps around a `DeviceList`
to help transition from `DeviceList` to `DeviceAllocation` in the future.
",copybara-service[bot],2024-07-25 23:44:59+00:00,[],2024-08-08 18:16:21+00:00,,https://github.com/tensorflow/tensorflow/pull/72541,[],[],
2431134208,pull_request,closed,,[xla:gpu] NFC: Do not modify command buffer flag in gemm fusion autotuner,"[xla:gpu] NFC: Do not modify command buffer flag in gemm fusion autotuner
",copybara-service[bot],2024-07-25 23:43:19+00:00,['anlunx'],2024-07-26 00:58:19+00:00,2024-07-26 00:58:17+00:00,https://github.com/tensorflow/tensorflow/pull/72540,[],[],
2431130304,pull_request,closed,,[XLA] Change attribute name skip-simplify-while-loops/trip-count-one to skip-simplify-while-loops_trip-count-one because / is not valid for attribute names.,"[XLA] Change attribute name skip-simplify-while-loops/trip-count-one to skip-simplify-while-loops_trip-count-one because / is not valid for attribute names.
",copybara-service[bot],2024-07-25 23:37:54+00:00,['carlos-guia'],2024-07-30 22:14:20+00:00,2024-07-30 22:14:19+00:00,https://github.com/tensorflow/tensorflow/pull/72539,[],[],
2431118603,pull_request,closed,,[xla:gpu] Add experimental flag to enable command buffers while profiling is active.,"[xla:gpu] Add experimental flag to enable command buffers while profiling is active.
",copybara-service[bot],2024-07-25 23:22:19+00:00,[],2024-07-26 21:27:07+00:00,2024-07-26 21:27:07+00:00,https://github.com/tensorflow/tensorflow/pull/72538,[],[],
2431118260,pull_request,open,,Reverts 27ca5e057e4d66e84c4abdc27449fb318410588b,"Reverts 27ca5e057e4d66e84c4abdc27449fb318410588b
",copybara-service[bot],2024-07-25 23:21:49+00:00,['ezhulenev'],2024-07-25 23:21:50+00:00,,https://github.com/tensorflow/tensorflow/pull/72537,[],[],
2431106432,pull_request,closed,,[XLA:MSA] Add comments to MsaAlgorithm class indicating its relationship with HeapSimulator class and how buffer_intervals_ are populated.,"[XLA:MSA] Add comments to MsaAlgorithm class indicating its relationship with HeapSimulator class and how buffer_intervals_ are populated.

Reverts 27ca5e057e4d66e84c4abdc27449fb318410588b
",copybara-service[bot],2024-07-25 23:07:08+00:00,['subhankarshah'],2024-07-26 01:29:13+00:00,2024-07-26 01:29:12+00:00,https://github.com/tensorflow/tensorflow/pull/72536,[],[],
2431077696,pull_request,closed,,Add mutable_debug_options() in HloModuleConfig.,"Add mutable_debug_options() in HloModuleConfig.
",copybara-service[bot],2024-07-25 22:33:29+00:00,[],2024-07-26 18:19:57+00:00,2024-07-26 18:19:56+00:00,https://github.com/tensorflow/tensorflow/pull/72535,[],[],
2431076833,pull_request,closed,,Remove deleted patch file from `third_party/triton/llvm_integration/series.bzl`,"Remove deleted patch file from `third_party/triton/llvm_integration/series.bzl`
",copybara-service[bot],2024-07-25 22:32:36+00:00,['ddunl'],2024-07-25 23:59:31+00:00,2024-07-25 23:59:30+00:00,https://github.com/tensorflow/tensorflow/pull/72534,[],[],
2431068316,pull_request,closed,,Add some additional args for select_and_scatter_test for certain backends. Also clean up includes for the select_and_scatter_test.,"Add some additional args for select_and_scatter_test for certain backends. Also clean up includes for the select_and_scatter_test.
",copybara-service[bot],2024-07-25 22:23:20+00:00,[],2024-08-22 17:41:37+00:00,2024-08-22 17:41:36+00:00,https://github.com/tensorflow/tensorflow/pull/72533,[],[],
2431055423,pull_request,closed,,Remove tf/lite/kernels/shim:tf_headers from tf/core:framework ,"Remove tf/lite/kernels/shim:tf_headers from tf/core:framework 

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15671 from ROCm:rocm_mlir b955a9219d3602b0ac82c762f1fb93d6e2cd511d
",copybara-service[bot],2024-07-25 22:09:37+00:00,['pak-laura'],2024-08-27 23:56:39+00:00,2024-08-27 23:56:39+00:00,https://github.com/tensorflow/tensorflow/pull/72532,[],[],
2431036617,pull_request,closed,,[XLA:GPU] Annotate instructions with their scheduling names.,"[XLA:GPU] Annotate instructions with their scheduling names.
",copybara-service[bot],2024-07-25 21:52:34+00:00,[],2024-07-26 14:46:47+00:00,2024-07-26 14:46:47+00:00,https://github.com/tensorflow/tensorflow/pull/72531,[],[],
2431035034,pull_request,closed,,Copy part of TFL quantization_utils to MLIR.,"Copy part of TFL quantization_utils to MLIR.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/tensorflow/tensorflow/pull/65125 from tensorflow:LakshmiKalaKadali-patch-6 ea8f04fe660119f19a8477d4c14fd19aba2c657e
",copybara-service[bot],2024-07-25 21:51:05+00:00,['pak-laura'],2024-07-31 18:35:29+00:00,2024-07-31 18:35:29+00:00,https://github.com/tensorflow/tensorflow/pull/72530,[],[],
2430936978,pull_request,closed,,[xla:cpu] Use DCHECK on a hot path in thunk_executor,"[xla:cpu] Use DCHECK on a hot path in thunk_executor
",copybara-service[bot],2024-07-25 20:35:54+00:00,['ezhulenev'],2024-07-26 02:34:32+00:00,2024-07-26 02:34:32+00:00,https://github.com/tensorflow/tensorflow/pull/72529,[],[],
2430887742,pull_request,closed,,Integrate StableHLO at openxla/stablehlo@8555db77,"Integrate StableHLO at openxla/stablehlo@8555db77
",copybara-service[bot],2024-07-25 20:00:08+00:00,['ghpvnist'],2024-07-25 23:13:29+00:00,2024-07-25 23:13:27+00:00,https://github.com/tensorflow/tensorflow/pull/72528,[],[],
2430814972,pull_request,closed,,[XLA:GPU] Add variant to pipeline parallelism tests that breaks the direct data dependency,"[XLA:GPU] Add variant to pipeline parallelism tests that breaks the direct data dependency

This variant breaks the direct data dependency between the previous iteration's compute and the collective permute.
",copybara-service[bot],2024-07-25 19:10:55+00:00,['frgossen'],2024-08-01 00:37:21+00:00,2024-08-01 00:37:20+00:00,https://github.com/tensorflow/tensorflow/pull/72526,[],[],
2430814943,pull_request,closed,,Integrate LLVM at llvm/llvm-project@58fb51492d96,"Integrate LLVM at llvm/llvm-project@58fb51492d96

Updates LLVM usage to match
[58fb51492d96](https://github.com/llvm/llvm-project/commit/58fb51492d96)
",copybara-service[bot],2024-07-25 19:10:54+00:00,[],2024-07-25 22:30:10+00:00,2024-07-25 22:30:10+00:00,https://github.com/tensorflow/tensorflow/pull/72525,[],[],
2430775351,pull_request,closed,,Handle non-trivial padding for direct standard conv legalizations.,"Handle non-trivial padding for direct standard conv legalizations.
This is done by pulling out *all* non-trivial padding before mhlo->tfl (but after convs are re-layoutted). Then, SAME padding is fused in later in tfl dialect.

This approach has the following benefits:
* Only need to handle negative slices in padding in one location
* Keeps legalizations simple and 1-1

Also turn off prepare patterns on 1d convs. These are not needed since we will rewrite 1d convs to 2d convs in the future.

Reverts 9dc0a3eb6c18f37e9c83a642641bede1a37f1dd2
",copybara-service[bot],2024-07-25 18:50:22+00:00,['LukeBoyer'],2024-07-29 22:55:43+00:00,2024-07-29 22:55:42+00:00,https://github.com/tensorflow/tensorflow/pull/72524,[],[],
2430772294,pull_request,closed,,Reenable clang_format.yml,"Reenable clang_format.yml

This became possible after https://github.com/openxla/xla/commit/8ed81337532dec2f4ee3e63dc44d327980763449 removed comments which previously confused clang-format.

FORCE_TEST_ACTIONS
",copybara-service[bot],2024-07-25 18:48:43+00:00,['ddunl'],2024-07-25 20:48:52+00:00,2024-07-25 20:48:52+00:00,https://github.com/tensorflow/tensorflow/pull/72523,[],[],
2430736152,pull_request,closed,,[tsl] Remove platform:types from mutex dependencies,"[tsl] Remove platform:types from mutex dependencies
",copybara-service[bot],2024-07-25 18:30:10+00:00,['ezhulenev'],2024-07-25 23:01:46+00:00,2024-07-25 23:01:45+00:00,https://github.com/tensorflow/tensorflow/pull/72522,[],[],
2430730858,pull_request,closed,,[XLA] Moved existing fuzzy matcher out of flash_attention to eventually use it more widely. Also added a test that it ignore converts,"[XLA] Moved existing fuzzy matcher out of flash_attention to eventually use it more widely. Also added a test that it ignore converts
",copybara-service[bot],2024-07-25 18:27:23+00:00,[],2024-07-30 18:11:51+00:00,2024-07-30 18:11:50+00:00,https://github.com/tensorflow/tensorflow/pull/72521,[],[],
2430684917,pull_request,closed,,hlo_evaluator: Don't dereference a disengaged optional.,"hlo_evaluator: Don't dereference a disengaged optional.

We can't look at error_detail if there's no value there.
",copybara-service[bot],2024-07-25 18:06:30+00:00,[],2024-07-26 17:19:59+00:00,2024-07-26 17:19:59+00:00,https://github.com/tensorflow/tensorflow/pull/72519,[],[],
2430668441,pull_request,closed,,Update docs to make use of new API for adding a TfLiteRegistrationExternal to a MutableOpResolver.,"Update docs to make use of new API for adding a TfLiteRegistrationExternal to a MutableOpResolver.
",copybara-service[bot],2024-07-25 17:56:44+00:00,[],2024-11-20 16:59:34+00:00,2024-11-20 16:59:32+00:00,https://github.com/tensorflow/tensorflow/pull/72518,[],[],
2430603547,pull_request,closed,,[numpy] Fix users of NumPy APIs that are removed in NumPy 2.0.,"[numpy] Fix users of NumPy APIs that are removed in NumPy 2.0.

This change migrates users of APIs removed in NumPy 2.0 to their recommended replacements (https://numpy.org/devdocs/numpy_2_0_migration_guide.html).
",copybara-service[bot],2024-07-25 17:17:22+00:00,[],2024-07-25 19:05:01+00:00,2024-07-25 19:05:00+00:00,https://github.com/tensorflow/tensorflow/pull/72517,[],[],
2430593186,pull_request,closed,,Delete mesh.Loop now that xmap has been deleted,"Delete mesh.Loop now that xmap has been deleted
",copybara-service[bot],2024-07-25 17:11:45+00:00,['yashk2810'],2024-07-25 21:26:11+00:00,2024-07-25 21:26:10+00:00,https://github.com/tensorflow/tensorflow/pull/72516,[],[],
2430580553,pull_request,closed,,[xla:cpu] Correctly resolve device ordinal from parent stream,"[xla:cpu] Correctly resolve device ordinal from parent stream

For consistency with current XLA:CPU, always use parent stream to resolve device ordinal.
",copybara-service[bot],2024-07-25 17:04:48+00:00,['ezhulenev'],2024-07-25 18:07:40+00:00,2024-07-25 18:07:40+00:00,https://github.com/tensorflow/tensorflow/pull/72515,[],[],
2430461001,pull_request,closed,,[xla:cpu] Create intra-op thread pool with same number of threads as PjrtClient thread pool,"[xla:cpu] Create intra-op thread pool with same number of threads as PjrtClient thread pool

This prevents possible deadlocks.
",copybara-service[bot],2024-07-25 16:15:55+00:00,['ezhulenev'],2024-07-25 17:35:01+00:00,2024-07-25 17:35:00+00:00,https://github.com/tensorflow/tensorflow/pull/72514,[],[],
2430293164,pull_request,closed,,Conditionally use Shardy in XLA CPU pipeline.,"Conditionally use Shardy in XLA CPU pipeline.
",copybara-service[bot],2024-07-25 15:22:31+00:00,[],2024-07-26 11:14:41+00:00,2024-07-26 11:14:40+00:00,https://github.com/tensorflow/tensorflow/pull/72513,[],[],
2430278179,pull_request,closed,,[XLA:GPU] Classify PartitionId as a noop.,"[XLA:GPU] Classify PartitionId as a noop.
",copybara-service[bot],2024-07-25 15:16:57+00:00,[],2024-07-25 21:52:56+00:00,2024-07-25 21:52:56+00:00,https://github.com/tensorflow/tensorflow/pull/72512,[],[],
2430084100,pull_request,closed,,[xla:cpu] Add PadThunk implementation,"[xla:cpu] Add PadThunk implementation

Reuse all the code from IrEmitter.
",copybara-service[bot],2024-07-25 14:13:23+00:00,[],2024-07-25 17:53:55+00:00,2024-07-25 17:53:54+00:00,https://github.com/tensorflow/tensorflow/pull/72511,[],"[{'comment_id': 2250430374, 'issue_id': 2430084100, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72511/checks?check_run_id=27916680729) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 7, 25, 14, 13, 29, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-07-25 14:13:29 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72511/checks?check_run_id=27916680729) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2430074190,pull_request,closed,,[XLA:CPU] Turn on `local_client_execute_test` for thunks runtime.,"[XLA:CPU] Turn on `local_client_execute_test` for thunks runtime.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14202 from shraiysh:rs_ds_fusion be1152712acc9ddcc5e32da97ad2d2e959f08b25
",copybara-service[bot],2024-07-25 14:09:27+00:00,[],2024-08-02 14:34:42+00:00,2024-08-02 14:34:42+00:00,https://github.com/tensorflow/tensorflow/pull/72510,[],"[{'comment_id': 2250420270, 'issue_id': 2430074190, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72510/checks?check_run_id=27916462900) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 7, 25, 14, 9, 33, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-07-25 14:09:33 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72510/checks?check_run_id=27916462900) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2430050866,pull_request,closed,,[Triton] Add a test for BF16 to FP8 conversion in Triton fusion emitter.,"[Triton] Add a test for BF16 to FP8 conversion in Triton fusion emitter.
",copybara-service[bot],2024-07-25 13:59:50+00:00,[],2024-07-25 14:54:55+00:00,2024-07-25 14:54:54+00:00,https://github.com/tensorflow/tensorflow/pull/72509,[],[],
2430050752,pull_request,closed,,Move scattered implementation of TFLite arrays into one place.,"Move scattered implementation of TFLite arrays into one place.
",copybara-service[bot],2024-07-25 13:59:47+00:00,['qukhan'],2024-07-27 09:21:31+00:00,2024-07-27 09:21:31+00:00,https://github.com/tensorflow/tensorflow/pull/72508,[],[],
2429942491,pull_request,closed,,[XLA:GPU][Triton] Properly emit float to int conversions in ir_emitter_triton.cc,"[XLA:GPU][Triton] Properly emit float to int conversions in ir_emitter_triton.cc
",copybara-service[bot],2024-07-25 13:16:36+00:00,[],2024-07-26 11:06:37+00:00,2024-07-25 13:59:25+00:00,https://github.com/tensorflow/tensorflow/pull/72507,[],[],
2429917423,pull_request,open,,Integrate LLVM at llvm/llvm-project@ecaacd14c359,"Integrate LLVM at llvm/llvm-project@ecaacd14c359

Updates LLVM usage to match
[ecaacd14c359](https://github.com/llvm/llvm-project/commit/ecaacd14c359)
",copybara-service[bot],2024-07-25 13:05:08+00:00,[],2024-07-25 13:05:08+00:00,,https://github.com/tensorflow/tensorflow/pull/72506,[],[],
2429857774,pull_request,closed,,Fix inconsistencies between the C and C++ variants of XNNPack Delegate Plugin.,"Fix inconsistencies between the C and C++ variants of XNNPack Delegate Plugin.

In particular:

  - In the C++ XNNPack Delegate Plugin, support `weight_cache_file_path`;
    previously it was only supported in the C XNNPack Delegate Plugin.

  - In C++ XNNPack Delegate Plugin, use the default XNNPack delegate flags
    when the `flags` field of `XNNPackSettings` is zero or unset;
    previously we were doing this only in the C XNNPack Delegate Plugin.

Also, improve the documentation of the treatment of XNNPack flag values in XNNPackSettings.

Also, some include-what-you-use fixes to #includes.
",copybara-service[bot],2024-07-25 12:36:54+00:00,[],2024-07-25 17:09:26+00:00,2024-07-25 17:09:25+00:00,https://github.com/tensorflow/tensorflow/pull/72505,[],[],
2429802019,pull_request,closed,,[XLA:CPU] Fix LLVM compiler test for thunks runtime.,"[XLA:CPU] Fix LLVM compiler test for thunks runtime.
",copybara-service[bot],2024-07-25 12:09:17+00:00,[],2024-07-25 19:54:39+00:00,2024-07-25 19:54:38+00:00,https://github.com/tensorflow/tensorflow/pull/72504,[],"[{'comment_id': 2250171166, 'issue_id': 2429802019, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72504/checks?check_run_id=27910319487) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 7, 25, 12, 9, 22, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-07-25 12:09:22 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72504/checks?check_run_id=27910319487) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2429705786,pull_request,closed,,[XLA:GPU] Reserve elements in hash set.,"[XLA:GPU] Reserve elements in hash set.
",copybara-service[bot],2024-07-25 11:18:00+00:00,[],2024-07-25 11:38:41+00:00,2024-07-25 11:38:40+00:00,https://github.com/tensorflow/tensorflow/pull/72503,[],[],
2429695314,pull_request,closed,,Fix floordiv simplification bug.,"Fix floordiv simplification bug.

The inner divisor can't be reused after new_dividend was
changed, since some ad-hoc simplifications may have been
applied.
",copybara-service[bot],2024-07-25 11:12:11+00:00,[],2024-07-25 12:26:45+00:00,2024-07-25 12:26:44+00:00,https://github.com/tensorflow/tensorflow/pull/72502,[],[],
2429666942,pull_request,closed,,[XLA:GPU] Print Interval as a closed interval.,"[XLA:GPU] Print Interval as a closed interval.

Replaced by
%s/in \[\(\d\+\), \(\d\+\))/\='in ['.(submatch(1)).', '.(submatch(2)-1).']'/g
",copybara-service[bot],2024-07-25 10:58:20+00:00,['pifon2a'],2024-07-25 11:19:22+00:00,2024-07-25 11:19:20+00:00,https://github.com/tensorflow/tensorflow/pull/72501,[],[],
2429606056,pull_request,open,,[XLA:GPU][MLIR-based emitters] Vectorize concats in concatenate emitter.,"[XLA:GPU][MLIR-based emitters] Vectorize concats in concatenate emitter.
",copybara-service[bot],2024-07-25 10:26:27+00:00,['pifon2a'],2024-07-25 10:26:28+00:00,,https://github.com/tensorflow/tensorflow/pull/72500,[],[],
2429547914,pull_request,closed,,[XLA:GPU] Comment properly sparse-local-load-to-llvm pass,"[XLA:GPU] Comment properly sparse-local-load-to-llvm pass
",copybara-service[bot],2024-07-25 09:58:47+00:00,[],2024-07-25 13:03:47+00:00,2024-07-25 13:03:46+00:00,https://github.com/tensorflow/tensorflow/pull/72499,[],[],
2429403187,pull_request,closed,,Use kernel_index from backend_config to select custom fusion kernel.,"Use kernel_index from backend_config to select custom fusion kernel.
",copybara-service[bot],2024-07-25 08:49:51+00:00,[],2024-07-25 12:34:00+00:00,2024-07-25 12:33:59+00:00,https://github.com/tensorflow/tensorflow/pull/72498,[],[],
2429389169,pull_request,open,,Integrate LLVM at llvm/llvm-project@58fb51492d96,"Integrate LLVM at llvm/llvm-project@58fb51492d96

Updates LLVM usage to match
[58fb51492d96](https://github.com/llvm/llvm-project/commit/58fb51492d96)
",copybara-service[bot],2024-07-25 08:42:55+00:00,[],2024-07-25 15:01:57+00:00,,https://github.com/tensorflow/tensorflow/pull/72496,[],[],
2429341233,pull_request,closed,,Add indexing support to the assert_cardinality op.,"Add indexing support to the assert_cardinality op.
",copybara-service[bot],2024-07-25 08:20:24+00:00,[],2024-07-25 19:10:22+00:00,2024-07-25 19:10:22+00:00,https://github.com/tensorflow/tensorflow/pull/72495,[],[],
2429286544,pull_request,closed,,PR #15292: [NVIDIA GPU] Disable post layout assignment collective pipeliner by default,"PR #15292: [NVIDIA GPU] Disable post layout assignment collective pipeliner by default

Imported from GitHub PR https://github.com/openxla/xla/pull/15292

This is to address the reduce-scatter pipelining issue after enabling post layout assignment collective pipeline by default.

Copybara import of the project:

--
cd654152be1a3e4b08c5c8d8a5a460d06d0c1113 by TJ Xu <tjx@nvidia.com>:

Disable post layout assignment collective pipeliner by default

Merging this change closes #15292

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15292 from Tixxx:tixxx/disable_post_layout_pipeliner cd654152be1a3e4b08c5c8d8a5a460d06d0c1113
",copybara-service[bot],2024-07-25 07:52:36+00:00,[],2024-07-25 15:06:01+00:00,2024-07-25 15:05:59+00:00,https://github.com/tensorflow/tensorflow/pull/72494,[],[],
2429271977,pull_request,open,,PR #14202: [fusion] Add RS->DUS dynamic slice fusion,"PR #14202: [fusion] Add RS->DUS dynamic slice fusion

Imported from GitHub PR https://github.com/openxla/xla/pull/14202

This patch adds execution support and fusion rewriting support for reduce-scatter -> dynamic-update-slice pattern.
Copybara import of the project:

--
04ac09da6ae7e18fb426b7d8b2eb5c3550c9c855 by Shraiysh Vaishay <svaishay@nvidia.com>:

[fusion] Add RS->DUS dynamic slice fusion

This patch adds execution support and fusion rewriting support for reduce-scatter -> dynamic-update-slice pattern.

Merging this change closes #14202

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14202 from shraiysh:rs_ds_fusion 04ac09da6ae7e18fb426b7d8b2eb5c3550c9c855
",copybara-service[bot],2024-07-25 07:44:49+00:00,[],2024-07-25 08:50:25+00:00,,https://github.com/tensorflow/tensorflow/pull/72493,[],[],
2429216702,pull_request,closed,,Return vector of cutlass kernels instead of single kernel.,"Return vector of cutlass kernels instead of single kernel.
This is in preparation to support selecting the best kernel from multiple kernels via autotuning.
",copybara-service[bot],2024-07-25 07:16:10+00:00,[],2024-07-25 10:53:16+00:00,2024-07-25 10:53:16+00:00,https://github.com/tensorflow/tensorflow/pull/72492,[],[],
2429203431,pull_request,closed,,TfLite pad missing datatype support,,abhinav-mcw,2024-07-25 07:09:16+00:00,['gbaned'],2024-07-25 07:17:17+00:00,2024-07-25 07:09:40+00:00,https://github.com/tensorflow/tensorflow/pull/72491,"[('size:M', 'CL Change Size: Medium')]","[{'comment_id': 2249613419, 'issue_id': 2429203431, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72491/checks?check_run_id=27897727652) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 7, 25, 7, 9, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2249613977, 'issue_id': 2429203431, 'author': 'abhinav-mcw', 'body': 'opened pr for wrong repo', 'created_at': datetime.datetime(2024, 7, 25, 7, 9, 41, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-07-25 07:09:21 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72491/checks?check_run_id=27897727652) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

abhinav-mcw (Issue Creator) on (2024-07-25 07:09:41 UTC): opened pr for wrong repo

"
2429185915,pull_request,closed,,TfLite concat missing datatype support ,,abhinav-mcw,2024-07-25 07:00:19+00:00,['gbaned'],2024-07-26 05:35:29+00:00,2024-07-25 07:03:48+00:00,https://github.com/tensorflow/tensorflow/pull/72490,"[('size:M', 'CL Change Size: Medium')]","[{'comment_id': 2249599614, 'issue_id': 2429185915, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72490/checks?check_run_id=27897397352) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 7, 25, 7, 0, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2249604668, 'issue_id': 2429185915, 'author': 'abhinav-mcw', 'body': 'opened pr for wrong repo', 'created_at': datetime.datetime(2024, 7, 25, 7, 3, 48, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-07-25 07:00:23 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72490/checks?check_run_id=27897397352) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

abhinav-mcw (Issue Creator) on (2024-07-25 07:03:48 UTC): opened pr for wrong repo

"
2428952823,pull_request,closed,,[xla:cpu] Consistently use NotFound for not found symbols and custom calls,"[xla:cpu] Consistently use NotFound for not found symbols and custom calls
",copybara-service[bot],2024-07-25 04:16:35+00:00,['ezhulenev'],2024-07-25 15:24:08+00:00,2024-07-25 15:24:08+00:00,https://github.com/tensorflow/tensorflow/pull/72489,[],[],
2428928134,pull_request,closed,,[xla:cpu] Move oneDNN tests to the CPU folder since they are specific to CPU.,"[xla:cpu] Move oneDNN tests to the CPU folder since they are specific to CPU.

+ Add missing header include and build dependencies. 
+ Temporarily disable onednn_matmul_test.
",copybara-service[bot],2024-07-25 03:58:23+00:00,['penpornk'],2024-07-25 07:50:47+00:00,2024-07-25 07:50:47+00:00,https://github.com/tensorflow/tensorflow/pull/72488,[],[],
2428897505,pull_request,open,,Integrate LLVM at llvm/llvm-project@58fb51492d96,"Integrate LLVM at llvm/llvm-project@58fb51492d96

Updates LLVM usage to match
[58fb51492d96](https://github.com/llvm/llvm-project/commit/58fb51492d96)
",copybara-service[bot],2024-07-25 03:26:59+00:00,[],2024-07-25 03:26:59+00:00,,https://github.com/tensorflow/tensorflow/pull/72487,[],[],
2428886770,pull_request,closed,,[xla:cpu] Do not test while loop temp aliasing with thunks runtime,"[xla:cpu] Do not test while loop temp aliasing with thunks runtime
",copybara-service[bot],2024-07-25 03:14:39+00:00,['ezhulenev'],2024-07-25 04:49:01+00:00,2024-07-25 04:49:00+00:00,https://github.com/tensorflow/tensorflow/pull/72486,[],[],
2428806066,pull_request,closed,,Make self._hyperparameters of type tuple so that it's hashable.,"Make self._hyperparameters of type tuple so that it's hashable.

The _Optimizer base class has override __hash__() and we need to make
sure that all member variables are hashable.
",copybara-service[bot],2024-07-25 02:14:55+00:00,[],2024-07-25 22:00:09+00:00,2024-07-25 22:00:09+00:00,https://github.com/tensorflow/tensorflow/pull/72485,[],[],
2428793378,pull_request,open,,Add support for EXP operator in XNNPACK delegate.,"Add support for EXP operator in XNNPACK delegate.
",copybara-service[bot],2024-07-25 02:00:55+00:00,[],2024-09-19 19:49:06+00:00,,https://github.com/tensorflow/tensorflow/pull/72484,[],[],
2428758652,pull_request,closed,,[xla:cpu] Fix tsan error in Thunk::ExecuteState,"[xla:cpu] Fix tsan error in Thunk::ExecuteState
",copybara-service[bot],2024-07-25 01:19:46+00:00,['ezhulenev'],2024-07-25 02:51:47+00:00,2024-07-25 02:51:47+00:00,https://github.com/tensorflow/tensorflow/pull/72483,[],[],
2428736930,pull_request,closed,,Add composite call to HLO and MHLO <=> HLO converter,"Add composite call to HLO and MHLO <=> HLO converter

Added support for frontend attributes map to additionally take json-like (dictionary) string. This is useful to preserve MLIR dictionary attribute for MHLO <=> HLO roundtrip.
",copybara-service[bot],2024-07-25 01:05:03+00:00,['ghpvnist'],2024-08-05 22:12:51+00:00,2024-08-05 22:12:50+00:00,https://github.com/tensorflow/tensorflow/pull/72482,[],[],
2428706464,pull_request,closed,,"Put requirements.in file back, which was accidentally deleted in https://github.com/tensorflow/tensorflow/commit/ec90bea08dcd9e8674bbd122f0cbf40f0b8d81a6","Put requirements.in file back, which was accidentally deleted in https://github.com/tensorflow/tensorflow/commit/ec90bea08dcd9e8674bbd122f0cbf40f0b8d81a6
",copybara-service[bot],2024-07-25 00:39:08+00:00,['vam-google'],2024-07-25 01:40:03+00:00,2024-07-25 01:40:02+00:00,https://github.com/tensorflow/tensorflow/pull/72481,[],[],
2428699116,pull_request,closed,,[XLA:CPU] Turn off thunks runtime test for conv_depthwise_test.,"[XLA:CPU] Turn off thunks runtime test for conv_depthwise_test.
",copybara-service[bot],2024-07-25 00:29:36+00:00,[],2024-07-25 01:19:05+00:00,2024-07-25 01:19:04+00:00,https://github.com/tensorflow/tensorflow/pull/72478,[],[],
2428641776,pull_request,open,,Add a custom `ParallelLookupDataset` that uses the `UnboundedThreadPool` to read a data source in parallel.,"Add a custom `ParallelLookupDataset` that uses the `UnboundedThreadPool` to read a data source in parallel.
",copybara-service[bot],2024-07-24 23:40:45+00:00,[],2024-07-24 23:40:45+00:00,,https://github.com/tensorflow/tensorflow/pull/72476,[],[],
2428620777,pull_request,closed,,Remove undefined `noaws` and `nohdfs` configs in build_pip_package_with_bazel.sh,"Remove undefined `noaws` and `nohdfs` configs in build_pip_package_with_bazel.sh
",copybara-service[bot],2024-07-24 23:18:26+00:00,[],2024-07-25 00:19:16+00:00,2024-07-25 00:19:15+00:00,https://github.com/tensorflow/tensorflow/pull/72475,[],[],
2428614778,pull_request,closed,,Correct mhlo -> tfl legalization to correctly differentiated between standard and depthwise conv when input and kernel features are both one.,"Correct mhlo -> tfl legalization to correctly differentiated between standard and depthwise conv when input and kernel features are both one.
",copybara-service[bot],2024-07-24 23:11:20+00:00,['LukeBoyer'],2024-07-25 00:26:00+00:00,2024-07-25 00:26:00+00:00,https://github.com/tensorflow/tensorflow/pull/72474,[],[],
2428610014,pull_request,closed,,[XLA][HostOffloader] Remove redundant copies to and from host for host offloaded computation outputs,"[XLA][HostOffloader] Remove redundant copies to and from host for host offloaded computation outputs

The simple algorithm tracks usages of all outputs of each host offloaded computation. For each:
- If they are ONLY used on the host and they are outputs of the entry computation, it sets the memory space to Host.
- If they are ONLY used on the host, but are temporaries, no changes are made.
- For cases replaced, if a MoveToHost is found (NOTE: that the algorithm does not explicitly check that any exist nor that all paths
lead to a MoveToHost) for an output that is only used on the host, we simply replace the usage.
",copybara-service[bot],2024-07-24 23:06:04+00:00,[],2024-07-26 16:40:40+00:00,2024-07-26 16:40:39+00:00,https://github.com/tensorflow/tensorflow/pull/72473,[],[],
2428583706,pull_request,closed,,PR #15285: Ensure only one device is visible in pjrt_c_api_gpu_test,"PR #15285: Ensure only one device is visible in pjrt_c_api_gpu_test

Imported from GitHub PR https://github.com/openxla/xla/pull/15285

The test fails when the number of available devices is more than 1. This patch fixes that by ensuring that only one device is visible to the test.
Copybara import of the project:

--
587bebe70c7d298008eff0c65dfcfa901e1fe21a by Shraiysh Vaishay <svaishay@nvidia.com>:

Ensure only one device is visible in pjrt_c_api_gpu_test

The test fails when the number of available devices is more than 1.
This patch fixes that by ensuring that only one device is visible
to the test.

Merging this change closes #15285

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15285 from shraiysh:fix_pjrt_c_api_gpu_test_gpu 587bebe70c7d298008eff0c65dfcfa901e1fe21a
",copybara-service[bot],2024-07-24 22:41:20+00:00,[],2024-07-25 08:41:18+00:00,2024-07-25 08:41:16+00:00,https://github.com/tensorflow/tensorflow/pull/72472,[],[],
2428566751,pull_request,closed,,[IFRT] Harden XLA executable compilation in xla_executable_impl_test_lib,"[IFRT] Harden XLA executable compilation in xla_executable_impl_test_lib

xla_executable_impl_test_lib `CompileOnDevices` missed a few steps to compile
an XLA computation in a portable way. This changes fixes them:

* num_replicas, num_partitions, use_spmd_partitioning are set correctly.
* device_assignment is set correctly for SPMD.
* device_assignment uses device IDs correctly instead of device indices;
there is no guarante that device ids and indices match.
",copybara-service[bot],2024-07-24 22:22:24+00:00,[],2024-07-25 00:12:16+00:00,2024-07-25 00:12:16+00:00,https://github.com/tensorflow/tensorflow/pull/72471,[],[],
2428563163,pull_request,closed,,Create cuda::ToStatus helper function to translate CUresult codes into absl::Status objects.,"Create cuda::ToStatus helper function to translate CUresult codes into absl::Status objects.

This creates  common handling of CUresults, and the elimination of RETURN_IF_CUDA_ERROR-style macros in favor of using the common TF_CHECK_OK/TF_RETURN_IF_ERROR ones.
",copybara-service[bot],2024-07-24 22:18:48+00:00,[],2024-07-25 23:19:02+00:00,2024-07-25 23:19:02+00:00,https://github.com/tensorflow/tensorflow/pull/72470,[],[],
2428534585,pull_request,closed,,[XLA:GPU] Move test helper functions to literal utils.,"[XLA:GPU] Move test helper functions to literal utils.

Move helper functions to literal utils: CreateFull, MakeScalarMatrixR2, CreateFingerprintMatixR2.
",copybara-service[bot],2024-07-24 21:51:20+00:00,['frgossen'],2024-07-26 17:00:24+00:00,2024-07-26 17:00:23+00:00,https://github.com/tensorflow/tensorflow/pull/72469,[],[],
2428533021,pull_request,closed,,[XLA:GPU] Add pipeline parallelism tests with circular repeat,"[XLA:GPU] Add pipeline parallelism tests with circular repeat
",copybara-service[bot],2024-07-24 21:49:52+00:00,['frgossen'],2024-07-29 15:52:45+00:00,2024-07-29 15:52:44+00:00,https://github.com/tensorflow/tensorflow/pull/72468,[],[],
2428528381,pull_request,open,,Update ortools to version 9.10,"Update ortools to version 9.10
",copybara-service[bot],2024-07-24 21:45:38+00:00,['d0k'],2024-07-24 22:05:37+00:00,,https://github.com/tensorflow/tensorflow/pull/72467,[],[],
2428515700,pull_request,open,,Integrate LLVM at llvm/llvm-project@ecaacd14c359,"Integrate LLVM at llvm/llvm-project@ecaacd14c359

Updates LLVM usage to match
[ecaacd14c359](https://github.com/llvm/llvm-project/commit/ecaacd14c359)
",copybara-service[bot],2024-07-24 21:35:09+00:00,[],2024-07-24 22:30:48+00:00,,https://github.com/tensorflow/tensorflow/pull/72466,[],[],
2428502292,pull_request,closed,,[XLA:GPU] Return flops cost of common instruction directly.,"[XLA:GPU] Return flops cost of common instruction directly.

Calling `GpuHloCostAnalysis` to get FLOPs is too expensive. 
For instructions that only do indexing, the cost is always 0. Elementwise instruction can be extracted from hlo profile data.

For more complicated and rare instructions, still fall back to GpuHloCostAnalysis.
",copybara-service[bot],2024-07-24 21:25:34+00:00,[],2024-07-25 08:13:02+00:00,2024-07-25 08:13:02+00:00,https://github.com/tensorflow/tensorflow/pull/72465,[],[],
2428475374,pull_request,closed,,[XLA:GPU] Add HLO-based pipeline parallelism test for #microbatches > #devices,"[XLA:GPU] Add HLO-based pipeline parallelism test for #microbatches > #devices
",copybara-service[bot],2024-07-24 21:09:12+00:00,['frgossen'],2024-07-26 15:39:05+00:00,2024-07-26 15:39:04+00:00,https://github.com/tensorflow/tensorflow/pull/72464,[],[],
2428432427,pull_request,closed,,Use `remap_paths` instead of `strip_prefix` for `//tensorflow/tools/lib_package:cheaders`,"Use `remap_paths` instead of `strip_prefix` for `//tensorflow/tools/lib_package:cheaders`

Tested with:
```
ddunleavy@ddunleavy:/tmp/tensorflow$ bazelisk build //tensorflow/tools/lib_package:cheaders && tar -tf bazel-bin/tensorflow/tools/lib_package/cheaders.tar
INFO: Analyzed target //tensorflow/tools/lib_package:cheaders (1 packages loaded, 1 target configured).
INFO: Found 1 target...
Target //tensorflow/tools/lib_package:cheaders up-to-date:
  bazel-bin/tensorflow/tools/lib_package/cheaders.tar
INFO: Elapsed time: 0.325s, Critical Path: 0.13s
INFO: 3 processes: 2 internal, 1 local.
INFO: Build completed successfully, 3 total actions
include/
include/tensorflow/
include/tensorflow/c/
include/tensorflow/c/c_api.h
include/tensorflow/c/c_api_experimental.h
include/tensorflow/c/c_api_macros.h
include/tensorflow/c/tensor_interface.h
include/tensorflow/c/tf_attrtype.h
include/tensorflow/c/tf_buffer.h
include/tensorflow/c/tf_datatype.h
include/tensorflow/c/tf_file_statistics.h
include/tensorflow/c/tf_status.h
include/tensorflow/c/tf_tensor.h
include/tensorflow/c/tf_tensor_helper.h
include/tensorflow/c/tf_tstring.h
include/tensorflow/core/
include/tensorflow/core/platform/
include/tensorflow/core/platform/ctstring.h
include/tensorflow/core/platform/ctstring_internal.h
include/tsl/
include/tsl/platform/
include/tsl/platform/ctstring.h
include/tsl/platform/ctstring_internal.h
include/xla/
include/xla/tsl/
include/xla/tsl/c/
include/xla/tsl/c/tsl_status.h
```
",copybara-service[bot],2024-07-24 20:39:33+00:00,['ddunl'],2024-07-24 23:53:39+00:00,2024-07-24 23:53:39+00:00,https://github.com/tensorflow/tensorflow/pull/72463,[],[],
2428419844,pull_request,closed,,Refactor the heartbeat lambda function in coordination service agent into a private member function.,"Refactor the heartbeat lambda function in coordination service agent into a private member function.
",copybara-service[bot],2024-07-24 20:30:44+00:00,[],2024-07-25 16:07:50+00:00,2024-07-25 16:07:50+00:00,https://github.com/tensorflow/tensorflow/pull/72462,[],[],
2428417992,pull_request,closed,,"[XLA:FFI] Add API version to XLA_FFI_Api struct, and check at runtime.","[XLA:FFI] Add API version to XLA_FFI_Api struct, and check at runtime.

This exposes the FFI API version as part of the API struct and adds a check at runtime to make sure that the major versions match. Eventually we will want to include logic to check the minor version like in Pjrt.

It would probably be a somewhat better user experience to error at registration time rather than call time, but we don't currently have enough metadata at registration time. It would be probably be possible to update the handler bundle provided when registering to make this work, but this was a much less invasive change, and since we already check the struct size at runtime here it seemed reasonable to include the version check in the same place.

I will update with benchmark info once I get a chance to run it.
",copybara-service[bot],2024-07-24 20:29:31+00:00,[],2024-07-26 15:06:38+00:00,2024-07-26 15:06:38+00:00,https://github.com/tensorflow/tensorflow/pull/72461,[],[],
2428389312,pull_request,closed,,PR #15267: [ROCm] Fixed compilation issues.,"PR #15267: [ROCm] Fixed compilation issues.

Imported from GitHub PR https://github.com/openxla/xla/pull/15267

Issues were caused by following commits:
https://github.com/openxla/xla/commit/d17181b49de71b0fb0ff6236745d43d630c39401
https://github.com/openxla/xla/commit/429da0c5cca821b75a75f95e1c4256f883b0bae5

Copybara import of the project:

--
66fbafe91986ebb02b8236175f27d7ebf8a23989 by Zoran Jovanovic <zjovanov@amd.com>:

[ROCm] Fixed compilation issues.

Merging this change closes #15267

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15267 from ROCm:ci_hotfix_20240724 66fbafe91986ebb02b8236175f27d7ebf8a23989
",copybara-service[bot],2024-07-24 20:12:12+00:00,[],2024-07-24 21:31:32+00:00,2024-07-24 21:31:31+00:00,https://github.com/tensorflow/tensorflow/pull/72460,[],[],
2428387327,pull_request,closed,,[JAX] Do not skip array copies whenever source or destination memory is default,"[JAX] Do not skip array copies whenever source or destination memory is default

If `jax.device_put()` copies arrays from/to non-default memory, we should not
skip the array copy just because destination/source memory (respectively) is a
default memory. By canonicalizing memory kinds when doing this filtering, we
can skip array copies only when it is fine.
",copybara-service[bot],2024-07-24 20:10:48+00:00,[],2024-07-25 18:47:47+00:00,2024-07-25 18:47:46+00:00,https://github.com/tensorflow/tensorflow/pull/72459,[],[],
2428347789,pull_request,closed,,[xla:cpu] Embed LLVM IR into executable when running thunks,"[xla:cpu] Embed LLVM IR into executable when running thunks
",copybara-service[bot],2024-07-24 19:44:58+00:00,['ezhulenev'],2024-07-25 05:08:07+00:00,2024-07-25 05:08:06+00:00,https://github.com/tensorflow/tensorflow/pull/72458,[],[],
2428253005,pull_request,closed,,Reverts 190932b47657829a9ec58d5c3f30de12aa7a2a56,"Reverts 190932b47657829a9ec58d5c3f30de12aa7a2a56
",copybara-service[bot],2024-07-24 18:48:02+00:00,[],2024-07-24 19:10:55+00:00,2024-07-24 19:10:54+00:00,https://github.com/tensorflow/tensorflow/pull/72457,[],[],
2428250096,pull_request,closed,,Support Int16 Activation Int4 Weight quantization in fully connected,"Support Int16 Activation Int4 Weight quantization in fully connected

The 4bit weights are unpacked on the fly to 8bits for computation.
",copybara-service[bot],2024-07-24 18:46:14+00:00,[],2024-07-26 05:10:30+00:00,2024-07-26 05:10:29+00:00,https://github.com/tensorflow/tensorflow/pull/72456,[],[],
2428242473,pull_request,closed,,"Make both ROCm and CUDA GpuContexts operate in terms of device_ordinals rather than using ""next_id_"" sometimes.","Make both ROCm and CUDA GpuContexts operate in terms of device_ordinals rather than using ""next_id_"" sometimes.
",copybara-service[bot],2024-07-24 18:41:19+00:00,[],2024-07-24 22:51:53+00:00,2024-07-24 22:51:52+00:00,https://github.com/tensorflow/tensorflow/pull/72455,[],[],
2428220362,pull_request,closed,,Add fusion pattern to fuse redundent slice and pack.,"Add fusion pattern to fuse redundent slice and pack.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15267 from ROCm:ci_hotfix_20240724 66fbafe91986ebb02b8236175f27d7ebf8a23989
",copybara-service[bot],2024-07-24 18:27:31+00:00,['sirakiin'],2024-07-24 22:09:06+00:00,2024-07-24 22:09:05+00:00,https://github.com/tensorflow/tensorflow/pull/72454,[],[],
2428170968,pull_request,open,,"Bump absl to latest LTS 20240722, Release Candidate 1","Bump absl to latest LTS 20240722, Release Candidate 1

This lets us use VLOG and status macros.
",copybara-service[bot],2024-07-24 17:59:53+00:00,['d0k'],2024-07-24 19:24:37+00:00,,https://github.com/tensorflow/tensorflow/pull/72453,[],[],
2428151565,pull_request,closed,,"Simplify and generalize the strategy generation code for convolution ops. Rather than explicitly generating strategies corresponding to different sets of dimensions being sharded, we now generate strategies in a more principled and general manner.","Simplify and generalize the strategy generation code for convolution ops. Rather than explicitly generating strategies corresponding to different sets of dimensions being sharded, we now generate strategies in a more principled and general manner.
",copybara-service[bot],2024-07-24 17:47:43+00:00,[],2024-07-26 23:23:10+00:00,2024-07-26 23:23:09+00:00,https://github.com/tensorflow/tensorflow/pull/72452,[],[],
2428151109,pull_request,open,,remove tflite from output_init_files_test target,"remove tflite from output_init_files_test target
",copybara-service[bot],2024-07-24 17:47:25+00:00,[],2024-07-29 18:20:14+00:00,,https://github.com/tensorflow/tensorflow/pull/72451,[],[],
2428132295,pull_request,closed,,Move model_builder to TFL compiler/converter.,"Move model_builder to TFL compiler/converter.
",copybara-service[bot],2024-07-24 17:37:55+00:00,[],2024-07-26 19:11:37+00:00,2024-07-26 19:11:36+00:00,https://github.com/tensorflow/tensorflow/pull/72450,[],[],
2428118648,pull_request,closed,,[IFRT] Include diagnostic info in the error message when parsing IFRT IR module fails.,"[IFRT] Include diagnostic info in the error message when parsing IFRT IR module fails.
",copybara-service[bot],2024-07-24 17:29:55+00:00,[],2024-07-24 20:52:38+00:00,2024-07-24 20:52:38+00:00,https://github.com/tensorflow/tensorflow/pull/72449,[],[],
2428051609,pull_request,closed,,Update `@rules_python` to 0.34.0 after LLVM integrate,"Update `@rules_python` to 0.34.0 after LLVM integrate

Should fix XLA/JAX CI
",copybara-service[bot],2024-07-24 16:50:25+00:00,['ddunl'],2024-07-24 18:04:26+00:00,2024-07-24 18:04:25+00:00,https://github.com/tensorflow/tensorflow/pull/72448,[],[],
2427948179,pull_request,open,,PR #15267: [ROCm] Fixed compilation issues.,"PR #15267: [ROCm] Fixed compilation issues.

Imported from GitHub PR https://github.com/openxla/xla/pull/15267

Issues were caused by following commits:
https://github.com/openxla/xla/commit/d17181b49de71b0fb0ff6236745d43d630c39401
https://github.com/openxla/xla/commit/429da0c5cca821b75a75f95e1c4256f883b0bae5

Copybara import of the project:

--
66fbafe91986ebb02b8236175f27d7ebf8a23989 by Zoran Jovanovic <zjovanov@amd.com>:

[ROCm] Fixed compilation issues.

Merging this change closes #15267

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15267 from ROCm:ci_hotfix_20240724 66fbafe91986ebb02b8236175f27d7ebf8a23989
",copybara-service[bot],2024-07-24 15:54:55+00:00,[],2024-07-24 15:54:55+00:00,,https://github.com/tensorflow/tensorflow/pull/72447,[],[],
2427859378,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-24 15:16:56+00:00,[],2024-11-28 16:10:29+00:00,,https://github.com/tensorflow/tensorflow/pull/72446,[],[],
2427846121,pull_request,closed,,Update XNNPack version and add it's new KleidiAI dependency.,"Update XNNPack version and add it's new KleidiAI dependency.

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15444 from eaplatanios:u/eaplatanios/cpp-17-fixes 73f3cd7e0135ec05c97595f795ec318fb635bd32
",copybara-service[bot],2024-07-24 15:10:52+00:00,['qukhan'],2024-07-31 14:05:50+00:00,2024-07-31 14:05:49+00:00,https://github.com/tensorflow/tensorflow/pull/72445,[],[],
2427840205,pull_request,open,,Update Shardy to latest commit.,"Update Shardy to latest commit.
",copybara-service[bot],2024-07-24 15:08:39+00:00,[],2024-07-24 15:08:39+00:00,,https://github.com/tensorflow/tensorflow/pull/72444,[],[],
2427789334,pull_request,closed,,Move the LLVM integration patch to sparsity patches,"Move the LLVM integration patch to sparsity patches

It is only impacting sparsity, and not Triton itself, so we should not be upstreaming it to Triton. Rather, we should just figure out how to properly fix it in our sparsity extension.
",copybara-service[bot],2024-07-24 14:46:44+00:00,['gflegar'],2024-07-24 16:31:11+00:00,2024-07-24 16:31:10+00:00,https://github.com/tensorflow/tensorflow/pull/72443,[],[],
2427763669,pull_request,closed,,Move installer wheels scripts to OSS,"Move installer wheels scripts to OSS
",copybara-service[bot],2024-07-24 14:35:41+00:00,['nitins17'],2024-07-25 17:28:09+00:00,2024-07-25 17:28:07+00:00,https://github.com/tensorflow/tensorflow/pull/72442,[],[],
2427657786,pull_request,closed,,[tsl] Remove dependency on platform:types from platform:mutex,"[tsl] Remove dependency on platform:types from platform:mutex
",copybara-service[bot],2024-07-24 13:53:05+00:00,['ezhulenev'],2024-07-25 02:32:03+00:00,2024-07-25 02:32:03+00:00,https://github.com/tensorflow/tensorflow/pull/72440,[],[],
2427634608,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-24 13:42:54+00:00,[],2024-08-09 00:46:22+00:00,2024-08-09 00:46:21+00:00,https://github.com/tensorflow/tensorflow/pull/72439,[],[],
2427631188,pull_request,closed,,Fix `llvm_compiler_test` debug compilation.,"Fix `llvm_compiler_test` debug compilation.


When trying to build this test for LLDB debugging purposes, there is a linker error:
`ld: error: duplicate symbol: main`. This error is only observed with `--dynamic_mode=off` bazel flag.

Removing gunit from dependencies fixes that issue.
",copybara-service[bot],2024-07-24 13:41:32+00:00,[],2024-07-25 13:10:23+00:00,2024-07-25 13:10:22+00:00,https://github.com/tensorflow/tensorflow/pull/72438,[],"[{'comment_id': 2247969722, 'issue_id': 2427631188, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72438/checks?check_run_id=27861208534) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 7, 24, 13, 41, 38, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-07-24 13:41:38 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72438/checks?check_run_id=27861208534) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2427573469,pull_request,closed,,[XLA:GPU] Fix error message in profile guided latency estimator.,"[XLA:GPU] Fix error message in profile guided latency estimator.

Print actually found instructions, not missing instructions twice.
",copybara-service[bot],2024-07-24 13:17:51+00:00,[],2024-07-25 16:41:59+00:00,2024-07-25 16:41:58+00:00,https://github.com/tensorflow/tensorflow/pull/72437,[],[],
2427356373,pull_request,closed,,[XLA:GPU] Make FusionInfoCache thread-safe and use in Priority Fusion.,"[XLA:GPU] Make FusionInfoCache thread-safe and use in Priority Fusion.

The cache helps to reduce compile time of big modules.
",copybara-service[bot],2024-07-24 11:36:32+00:00,[],2024-07-24 12:50:59+00:00,2024-07-24 12:50:59+00:00,https://github.com/tensorflow/tensorflow/pull/72436,[],[],
2427307679,pull_request,closed,,Add an additional guard for ReduceOfBatchDot simplification.,"Add an additional guard for ReduceOfBatchDot simplification.

This simplification creates non-canonical dots (more than one contracting
dimension). So guard it behind supports_non_canonical_dots option.
",copybara-service[bot],2024-07-24 11:12:52+00:00,['akuegel'],2024-07-24 12:45:24+00:00,2024-07-24 12:45:24+00:00,https://github.com/tensorflow/tensorflow/pull/72435,[],[],
2427200175,pull_request,closed,,[XLA:CPU] Turn on another batch of tests for thunks runtime.,"[XLA:CPU] Turn on another batch of tests for thunks runtime.
",copybara-service[bot],2024-07-24 10:31:17+00:00,[],2024-07-24 14:03:02+00:00,2024-07-24 14:03:00+00:00,https://github.com/tensorflow/tensorflow/pull/72432,[],"[{'comment_id': 2247548081, 'issue_id': 2427200175, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72432/checks?check_run_id=27852368755) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 7, 24, 10, 31, 22, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-07-24 10:31:22 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72432/checks?check_run_id=27852368755) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2427058826,pull_request,closed,,Return the TfDeviceId as the device ID of GPU.,"Return the TfDeviceId as the device ID of GPU.

This is to support virtual GPUs.
",copybara-service[bot],2024-07-24 09:30:34+00:00,['changhuilin'],2024-08-02 01:28:59+00:00,2024-08-02 01:28:59+00:00,https://github.com/tensorflow/tensorflow/pull/72431,[],[],
2427027916,pull_request,closed,,[XLA:GPU][NFC] Verify no changes to the instruction name in HloVerifier post-scheduling.,"[XLA:GPU][NFC] Verify no changes to the instruction name in HloVerifier post-scheduling.

This PR:
0. Adds scheduling_name to OpMetadata. This field is set in a separate pass in the subsequent CL.
1. Adds parsing of the new scheduling_name field to hlo_parser.
2. Adds verification logic to HloVerifier. (disabled by default)

Currently this is NFC as nothing sets scheduling_name.
",copybara-service[bot],2024-07-24 09:18:59+00:00,[],2024-07-25 14:06:35+00:00,2024-07-25 14:06:33+00:00,https://github.com/tensorflow/tensorflow/pull/72430,[],[],
2427022434,pull_request,closed,,[XLA:GPU] Clean up post-scheduling passes.,"[XLA:GPU] Clean up post-scheduling passes.

Create a single pipeline and move relevant pipelines into subpipelines.
",copybara-service[bot],2024-07-24 09:16:25+00:00,[],2024-07-24 20:59:57+00:00,2024-07-24 20:59:56+00:00,https://github.com/tensorflow/tensorflow/pull/72429,[],[],
2426995406,pull_request,closed,,Integrate Triton up to fd691c67 (https://github.com/openai/triton/commits/fd691c67ac20958a67693358186d877790f5f48f),"Integrate Triton up to fd691c67 (https://github.com/openai/triton/commits/fd691c67ac20958a67693358186d877790f5f48f)
",copybara-service[bot],2024-07-24 09:04:30+00:00,[],2024-07-29 12:29:06+00:00,2024-07-29 12:29:05+00:00,https://github.com/tensorflow/tensorflow/pull/72428,[],[],
2426961778,pull_request,closed,,[XLA:GPU] Skip GemmFusionIsNoOpWhenGemmFusionAutotunerFallsBackToCublas test.,"[XLA:GPU] Skip GemmFusionIsNoOpWhenGemmFusionAutotunerFallsBackToCublas test.
",copybara-service[bot],2024-07-24 08:49:03+00:00,[],2024-07-24 09:49:22+00:00,2024-07-24 09:49:20+00:00,https://github.com/tensorflow/tensorflow/pull/72427,[],[],
2426946850,pull_request,open,,PR #15203: Fix Translate hlo_to_mhlo tests on Windows,"PR #15203: Fix Translate hlo_to_mhlo tests on Windows

Imported from GitHub PR https://github.com/openxla/xla/pull/15203

This pull request addresses the failing hlo_to_mhlo tests, which are designed to verify the translation between MLIR-HLO and HLO dialects

Error:
hlo_to_mhlo tests were failing on the Windows platform with an error shown below:

INFO: From Testing //xla/translate/hlo_to_mhlo/tests:while.hlo.test

==================== Test output for //xla/translate/hlo_to_mhlo/tests:while.hlo.test:
D:/e7rjstdl/external/python_x86_64-pc-windows-msvc/python.exe: can't open file 'D:\e7rjstdl\execroot\xla\bazel-out\x64_windows-opt\bin\xla\translate\hlo_to_mhlo\tests\while.hlo.test.zip': [Errno 2] No such file or directory

Solution:

Changed the build file to perform testing on the Windows platform using the standard LIT test configuration from LLVM
Tweaked the PATH to correctly find the tools required for executing LIT tests on the Windows platform such as FileCheck.
Copybara import of the project:

--
d22d23116e7835659db4fbb9cc7f8b54992b13f1 by Raunak <mayank.kumar.raunak@intel.com>:

Fix hlo_to_mhlo tests on Windows

--
51410aa9c965062505027e6299f4c42173cd962f by mraunak <83710963+mraunak@users.noreply.github.com>:

Update test_cases.bzl

Fix run buildifier
--
e94346ed5838c366e9c8a5bd146247b5b4f99c8c by mraunak <83710963+mraunak@users.noreply.github.com>:

Update BUILD

Fix run buildifier
--
83e39c1808f5a40da48b33f14c0d1445a16a6d2a by mraunak <83710963+mraunak@users.noreply.github.com>:

Update BUILD

Fix run buildifier
--
d7e05946556bc39b65a0c017684b608338728e1e by mraunak <83710963+mraunak@users.noreply.github.com>:

Update BUILD
--
abd1cbedcd1c6a7e9d49d5c9af039f1c79d27d83 by mraunak <83710963+mraunak@users.noreply.github.com>:

Update test_cases.bzl

Fix run buildifier
--
8f75ec31ce29529a127abcc1653962a4f324170c by Raunak <mayank.kumar.raunak@intel.com>:

fix run buildifier

--
dd599365a798d22c1de5edbbdbadb6cffce01acd by mraunak <83710963+mraunak@users.noreply.github.com>:

Fix run buildifier
--
11f778fdcc70d3371b38557266ce9cc590645cee by mraunak <83710963+mraunak@users.noreply.github.com>:

Update test_cases.bzl
--
aa7673cc90944457aad39bdbf8e57208217cfe53 by mraunak <83710963+mraunak@users.noreply.github.com>:

copybara fix

Merging this change closes #15203

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15203 from Intel-tensorflow:mraunak/hlo_mhlo aa7673cc90944457aad39bdbf8e57208217cfe53
",copybara-service[bot],2024-07-24 08:41:41+00:00,[],2024-07-31 09:43:08+00:00,,https://github.com/tensorflow/tensorflow/pull/72426,[],"[{'comment_id': 2247243643, 'issue_id': 2426946850, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72426/checks?check_run_id=27847408376) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 7, 24, 8, 41, 46, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-07-24 08:41:46 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72426/checks?check_run_id=27847408376) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2426945438,pull_request,closed,,PR #14310: [GPU] Cleanup handling of determinism settings.,"PR #14310: [GPU] Cleanup handling of determinism settings.

Imported from GitHub PR https://github.com/openxla/xla/pull/14310

TF_CUDNN_DETERMINISTIC is TF-specific and will be handled there.

Merge all checks of determinism flags in RequireDeterminism().
Set NumericOptions.require_determinism using it.
Copybara import of the project:

--
1351c0fe1784d538977590a1bc6ace4056a8bd0d by Ilia Sergachev <isergachev@nvidia.com>:

[GPU] Remove TF_CUDNN_DETERMINISTIC; cleanup handling of determinism settings.

TF_CUDNN_DETERMINISTIC is TF-specific and will be handled there.

Merge all checks of determinism flags in RequireDeterminism().
Set NumericOptions.require_determinism using it.

Merging this change closes #14310

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14310 from openxla:cleanup_determinism_handling 1351c0fe1784d538977590a1bc6ace4056a8bd0d
",copybara-service[bot],2024-07-24 08:40:59+00:00,[],2024-07-24 13:23:43+00:00,2024-07-24 13:23:42+00:00,https://github.com/tensorflow/tensorflow/pull/72425,[],[],
2426922677,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-24 08:30:13+00:00,[],2024-07-24 08:30:13+00:00,,https://github.com/tensorflow/tensorflow/pull/72424,[],[],
2426900207,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-24 08:19:13+00:00,[],2024-07-25 12:15:12+00:00,2024-07-25 12:15:11+00:00,https://github.com/tensorflow/tensorflow/pull/72423,[],[],
2426806799,pull_request,closed,,Integrate LLVM at llvm/llvm-project@84658fb82b67,"Integrate LLVM at llvm/llvm-project@84658fb82b67

Updates LLVM usage to match
[84658fb82b67](https://github.com/llvm/llvm-project/commit/84658fb82b67)
",copybara-service[bot],2024-07-24 07:29:17+00:00,[],2024-07-24 15:36:52+00:00,2024-07-24 15:36:51+00:00,https://github.com/tensorflow/tensorflow/pull/72421,[],[],
2426628150,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-24 05:26:42+00:00,[],2024-07-24 05:26:42+00:00,,https://github.com/tensorflow/tensorflow/pull/72419,[],[],
2426595437,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-24 04:57:12+00:00,[],2024-07-24 04:57:12+00:00,,https://github.com/tensorflow/tensorflow/pull/72418,[],[],
2426592071,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-24 04:53:42+00:00,[],2024-07-24 04:53:42+00:00,,https://github.com/tensorflow/tensorflow/pull/72417,[],[],
2426586079,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-24 04:47:29+00:00,[],2024-07-24 04:47:29+00:00,,https://github.com/tensorflow/tensorflow/pull/72416,[],[],
2426584777,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-24 04:46:08+00:00,[],2024-07-24 04:46:08+00:00,,https://github.com/tensorflow/tensorflow/pull/72415,[],[],
2426577873,pull_request,closed,,Add support for mhlo.conv -> tfl.depthwise_conv.,"Add support for mhlo.conv -> tfl.depthwise_conv.
",copybara-service[bot],2024-07-24 04:38:53+00:00,['LukeBoyer'],2024-07-30 01:21:23+00:00,2024-07-30 01:21:22+00:00,https://github.com/tensorflow/tensorflow/pull/72414,[],[],
2426556304,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-24 04:14:36+00:00,[],2024-07-26 09:32:06+00:00,,https://github.com/tensorflow/tensorflow/pull/72413,[],[],
2426527131,pull_request,closed,,Add support for depthwise convs in the hlo prepare phase. ,"Add support for depthwise convs in the hlo prepare phase. 

Handles paddings and transposing layouts. Legalizations to come in future CL. Also remove a redundant test.

Update the conv util lib to have clear helper functions for determining what ""type"" of tfl conv some mhlo conv maps to.
",copybara-service[bot],2024-07-24 03:38:35+00:00,['LukeBoyer'],2024-07-30 00:40:53+00:00,2024-07-30 00:40:52+00:00,https://github.com/tensorflow/tensorflow/pull/72412,[],[],
2426431837,pull_request,closed,,Shoutout,To the homies,eddie-santos,2024-07-24 02:01:34+00:00,['gbaned'],2024-07-24 02:05:51+00:00,2024-07-24 02:05:46+00:00,https://github.com/tensorflow/tensorflow/pull/72410,"[('size:XS', 'CL Change Size: Extra Small')]",[],
2426422259,pull_request,closed,,"r2.17 cherry-pick: f0f52aab97b ""Fix a typo in the GCS URI.""",Refer to the original commit: https://github.com/tensorflow/tensorflow/commit/f0f52aab97bd82b53c51c86a1c03b1fe623b99d3,tensorflow-jenkins,2024-07-24 01:51:14+00:00,[],2024-07-24 04:10:06+00:00,2024-07-24 04:10:04+00:00,https://github.com/tensorflow/tensorflow/pull/72409,[],[],
2426406115,pull_request,open,,"HostOffloading ConvertMemoryPlacementToInternalAnnotations: Avoid inserting ""MoveToHost"" custom calls that are only used for a host computation.","HostOffloading ConvertMemoryPlacementToInternalAnnotations: Avoid inserting ""MoveToHost"" custom calls that are only used for a host computation.
",copybara-service[bot],2024-07-24 01:35:05+00:00,[],2024-07-24 16:41:21+00:00,,https://github.com/tensorflow/tensorflow/pull/72408,[],[],
2426367509,pull_request,closed,,"Simplify and generalize the strategy generation code for dot ops. Rather than explicitly generating strategies corresponding to different sets of dimensions being sharded, we now generate strategies in a more principled and general manner.","Simplify and generalize the strategy generation code for dot ops. Rather than explicitly generating strategies corresponding to different sets of dimensions being sharded, we now generate strategies in a more principled and general manner.
",copybara-service[bot],2024-07-24 00:50:13+00:00,[],2024-07-25 17:02:22+00:00,2024-07-25 17:02:21+00:00,https://github.com/tensorflow/tensorflow/pull/72407,[],[],
2426345489,pull_request,closed,,Replace uses of gtl::linked_hash_{map/set} to absl::btree_{map/set} in third_party ,"Replace uses of gtl::linked_hash_{map/set} to absl::btree_{map/set} in third_party 
auto-sharding code with absl::btree_{map/set}.
",copybara-service[bot],2024-07-24 00:24:07+00:00,[],2024-07-24 19:49:42+00:00,2024-07-24 19:49:42+00:00,https://github.com/tensorflow/tensorflow/pull/72406,[],[],
2426332215,pull_request,open,,PR #14310: [GPU] Cleanup handling of determinism settings.,"PR #14310: [GPU] Cleanup handling of determinism settings.

Imported from GitHub PR https://github.com/openxla/xla/pull/14310

TF_CUDNN_DETERMINISTIC is TF-specific and will be handled there.

Merge all checks of determinism flags in RequireDeterminism().
Set NumericOptions.require_determinism using it.
Copybara import of the project:

--
1351c0fe1784d538977590a1bc6ace4056a8bd0d by Ilia Sergachev <isergachev@nvidia.com>:

[GPU] Remove TF_CUDNN_DETERMINISTIC; cleanup handling of determinism settings.

TF_CUDNN_DETERMINISTIC is TF-specific and will be handled there.

Merge all checks of determinism flags in RequireDeterminism().
Set NumericOptions.require_determinism using it.

Merging this change closes #14310

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14310 from openxla:cleanup_determinism_handling 1351c0fe1784d538977590a1bc6ace4056a8bd0d
",copybara-service[bot],2024-07-24 00:09:55+00:00,[],2024-07-24 01:03:48+00:00,,https://github.com/tensorflow/tensorflow/pull/72405,[],[],
2426286996,pull_request,open,,"[SPMD:Bug-Fix] Add some additional IsManual checks + logic in SPMD Reshard, ReshardNoCache, and Replicate functions.","[SPMD:Bug-Fix] Add some additional IsManual checks + logic in SPMD Reshard, ReshardNoCache, and Replicate functions.
",copybara-service[bot],2024-07-23 23:33:27+00:00,[],2024-07-24 17:04:58+00:00,,https://github.com/tensorflow/tensorflow/pull/72404,[],[],
2426281334,pull_request,closed,,[XLA] Fixing pattern_matcher include header warnings,"[XLA] Fixing pattern_matcher include header warnings
",copybara-service[bot],2024-07-23 23:28:20+00:00,[],2024-07-29 19:14:03+00:00,2024-07-29 19:14:02+00:00,https://github.com/tensorflow/tensorflow/pull/72403,[],[],
2426280850,pull_request,closed,,Add Gloo support for MacOS.,"Add Gloo support for MacOS.

This is an alternative to #7726.

Gloo supports MacOS, but requires using libuv as the transport mechanism.

This closes https://github.com/openxla/xla/pull/15027
",copybara-service[bot],2024-07-23 23:27:52+00:00,[],2024-07-30 00:18:11+00:00,2024-07-30 00:18:11+00:00,https://github.com/tensorflow/tensorflow/pull/72402,[],[],
2426277777,pull_request,open,,[XLA] Add condition for slice(dot) to dot(slice) rewrite to eliminate regression. Dimension numbers of SparseDotMoveSliceToOperands updated to satisfy this new condition.,"[XLA] Add condition for slice(dot) to dot(slice) rewrite to eliminate regression. Dimension numbers of SparseDotMoveSliceToOperands updated to satisfy this new condition.
",copybara-service[bot],2024-07-23 23:25:05+00:00,[],2024-08-04 23:23:51+00:00,,https://github.com/tensorflow/tensorflow/pull/72401,[],[],
2426270483,pull_request,closed,,"[XLA] Rewrite max(f(x), f(y)) as f(max(x,y)) where f is any of cbrt, erf, logistic, and tanh. This is possible because these are nondecreasing functions and asymptotically satisfy |f(x)| <= |x|, so there will not be overflow.","[XLA] Rewrite max(f(x), f(y)) as f(max(x,y)) where f is any of cbrt, erf, logistic, and tanh. This is possible because these are nondecreasing functions and asymptotically satisfy |f(x)| <= |x|, so there will not be overflow.
",copybara-service[bot],2024-07-23 23:19:19+00:00,[],2024-08-01 20:03:33+00:00,2024-08-01 20:03:33+00:00,https://github.com/tensorflow/tensorflow/pull/72400,[],[],
2426227005,pull_request,closed,,Remove lite/flex:delegate from pywrap_tensorflow_internal,"Remove lite/flex:delegate from pywrap_tensorflow_internal

Reverts 7f0ef5c661fb38add631912694ba6742d0e29ce6
",copybara-service[bot],2024-07-23 22:30:00+00:00,['ecalubaquib'],2024-09-03 21:16:08+00:00,2024-09-03 21:16:07+00:00,https://github.com/tensorflow/tensorflow/pull/72399,[],"[{'comment_id': 2246417285, 'issue_id': 2426227005, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72399/checks?check_run_id=27830476114) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 7, 23, 22, 30, 6, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-07-23 22:30:06 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72399/checks?check_run_id=27830476114) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2426221597,pull_request,closed,,Remove GOOGLE_CUDA block in py_array.cc as JAX has migrated to use CUDA plugin.,"Remove GOOGLE_CUDA block in py_array.cc as JAX has migrated to use CUDA plugin.
",copybara-service[bot],2024-07-23 22:24:07+00:00,['jyingl3'],2024-08-13 22:29:24+00:00,2024-08-13 22:29:23+00:00,https://github.com/tensorflow/tensorflow/pull/72398,[],[],
2426127257,pull_request,closed,,[XLA:LHS] Improve logging for kShareable resource occupiers.,"[XLA:LHS] Improve logging for kShareable resource occupiers.
",copybara-service[bot],2024-07-23 21:04:21+00:00,['seherellis'],2024-07-24 00:13:16+00:00,2024-07-24 00:13:15+00:00,https://github.com/tensorflow/tensorflow/pull/72397,[],[],
2425944108,pull_request,closed,,[xla] Replace debug option xla_use_shardy with execution option,"[xla] Replace debug option xla_use_shardy with execution option
use_shardy_partitioner.

Replace the use of xla_use_shardy with use_shardy_partitioner and remove
xla_use_shardy.
",copybara-service[bot],2024-07-23 19:19:50+00:00,['bixia1'],2024-07-29 23:42:02+00:00,2024-07-29 23:42:01+00:00,https://github.com/tensorflow/tensorflow/pull/72389,[],[],
2425902972,pull_request,closed,,[XLA:CPU] Support `reshape` op in thunks runtime.,"[XLA:CPU] Support `reshape` op in thunks runtime.


In most cases `reshape` op is rewritten by `ReshapeDecomposer` pass. When it is not, current runtime defaults to elemental IR emitter. This CL aligns thunks runtime behavior with the current one.

Also added a test case for reshape that runs without HLO passes.
",copybara-service[bot],2024-07-23 19:00:26+00:00,[],2024-07-25 11:57:12+00:00,2024-07-25 11:57:11+00:00,https://github.com/tensorflow/tensorflow/pull/72387,[],"[{'comment_id': 2246038383, 'issue_id': 2425902972, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72387/checks?check_run_id=27822446546) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 7, 23, 19, 0, 31, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-07-23 19:00:31 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72387/checks?check_run_id=27822446546) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2425896785,pull_request,closed,,Hide the SDY dialect right before MLIR->HLO conversion in the XLA pipeline.,"Hide the SDY dialect right before MLIR->HLO conversion in the XLA pipeline.

Since Shardy is inside the middle of the XLA pipeline, after converting down to HLO, we need to run the Shardy export pipeline to preserve the SDY ops and sharding attributes for when we come back from HLO to MLIR when Shardy propagation is run.
",copybara-service[bot],2024-07-23 18:57:54+00:00,[],2024-07-31 17:33:34+00:00,2024-07-31 17:33:33+00:00,https://github.com/tensorflow/tensorflow/pull/72386,[],[],
2425880430,pull_request,open,,Integrate LLVM at llvm/llvm-project@84658fb82b67,"Integrate LLVM at llvm/llvm-project@84658fb82b67

Updates LLVM usage to match
[84658fb82b67](https://github.com/llvm/llvm-project/commit/84658fb82b67)
",copybara-service[bot],2024-07-23 18:48:29+00:00,[],2024-07-23 18:48:29+00:00,,https://github.com/tensorflow/tensorflow/pull/72385,[],[],
2425843744,pull_request,closed,,[xla:cpu] Make WhileThunk non-blocking,"[xla:cpu] Make WhileThunk non-blocking

BlockUntilReady inside Thunk:Execute is illegal and leads to deadlocks. Run while loop asynchronously relying on AndThen callbacks.
",copybara-service[bot],2024-07-23 18:28:00+00:00,['ezhulenev'],2024-07-24 14:47:42+00:00,2024-07-24 14:47:40+00:00,https://github.com/tensorflow/tensorflow/pull/72384,[],[],
2425834590,pull_request,closed,,[XLA:CPU] Test `dynamic-reshape` op in thunks runtime.,"[XLA:CPU] Test `dynamic-reshape` op in thunks runtime.

`dynamic-reshape` op is already supported in thunks runtime, because it is rewritten as other ops. This CL adds tests covering dynamic reshape basic functionality, and turns on related tests.
",copybara-service[bot],2024-07-23 18:22:10+00:00,[],2024-07-26 11:04:55+00:00,2024-07-25 14:24:38+00:00,https://github.com/tensorflow/tensorflow/pull/72383,[],"[{'comment_id': 2245947565, 'issue_id': 2425834590, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72383/checks?check_run_id=27820825642) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 7, 23, 18, 22, 15, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-07-23 18:22:15 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72383/checks?check_run_id=27820825642) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2425826026,pull_request,closed,,Rollback to fix test failures,"Rollback to fix test failures

Reverts c93e8564e3398a8b86095437b49936e7eba289a9
",copybara-service[bot],2024-07-23 18:16:36+00:00,['JW1992'],2024-07-23 21:06:26+00:00,2024-07-23 21:06:25+00:00,https://github.com/tensorflow/tensorflow/pull/72382,[],[],
2425810406,pull_request,closed,,Remove dependency on tf_op_shim from tensorflow_python.so.,"Remove dependency on tf_op_shim from tensorflow_python.so.
",copybara-service[bot],2024-07-23 18:06:53+00:00,[],2024-08-07 22:21:12+00:00,2024-08-07 22:21:11+00:00,https://github.com/tensorflow/tensorflow/pull/72381,[],[],
2425779116,pull_request,closed,,Test BasicFlatBufferModel.TestBufferAlignment only for ARM devices,"Test BasicFlatBufferModel.TestBufferAlignment only for ARM devices

Test the buffer alignment only for ARM since the test may crash on x86_64
with certain compiler option `-fsanitize=alignment`.
",copybara-service[bot],2024-07-23 17:52:13+00:00,['terryheo'],2024-07-24 16:49:57+00:00,2024-07-24 16:49:56+00:00,https://github.com/tensorflow/tensorflow/pull/72380,[],[],
2425751708,pull_request,open,,Integrate LLVM at llvm/llvm-project@98ebdd0ca9a7,"Integrate LLVM at llvm/llvm-project@98ebdd0ca9a7

Updates LLVM usage to match
[98ebdd0ca9a7](https://github.com/llvm/llvm-project/commit/98ebdd0ca9a7)
",copybara-service[bot],2024-07-23 17:40:39+00:00,[],2024-07-23 17:40:39+00:00,,https://github.com/tensorflow/tensorflow/pull/72379,[],[],
2425737182,pull_request,closed,,Update TensorFlow release notes about TensorFlow Lite breaking changes,"Update TensorFlow release notes about TensorFlow Lite breaking changes
",copybara-service[bot],2024-07-23 17:31:50+00:00,[],2024-07-24 21:38:23+00:00,2024-07-24 21:38:22+00:00,https://github.com/tensorflow/tensorflow/pull/72378,[],[],
2425723164,pull_request,open,,Fix Triton patch for LLVM integration at llvm/llvm-project@acc159aea1e6,"Fix Triton patch for LLVM integration at llvm/llvm-project@acc159aea1e6
",copybara-service[bot],2024-07-23 17:23:13+00:00,[],2024-07-23 17:23:13+00:00,,https://github.com/tensorflow/tensorflow/pull/72377,[],[],
2425722024,pull_request,closed,,[xla] Add use_shardy_partitioner as a field in ExecutableOptions.,"[xla] Add use_shardy_partitioner as a field in ExecutableOptions.

Add a test.
",copybara-service[bot],2024-07-23 17:22:32+00:00,['bixia1'],2024-07-24 18:50:22+00:00,2024-07-24 18:50:21+00:00,https://github.com/tensorflow/tensorflow/pull/72376,[],[],
2425713919,pull_request,closed,,Add missing patch file from LLVM integration,"Add missing patch file from LLVM integration
",copybara-service[bot],2024-07-23 17:18:28+00:00,['gflegar'],2024-07-23 18:16:04+00:00,2024-07-23 18:16:04+00:00,https://github.com/tensorflow/tensorflow/pull/72375,[],[],
2425705676,pull_request,open,,Integrate LLVM at llvm/llvm-project@73799b46072c,"Integrate LLVM at llvm/llvm-project@73799b46072c

Updates LLVM usage to match
[73799b46072c](https://github.com/llvm/llvm-project/commit/73799b46072c)
",copybara-service[bot],2024-07-23 17:13:12+00:00,[],2024-07-23 17:13:12+00:00,,https://github.com/tensorflow/tensorflow/pull/72374,[],[],
2425579950,pull_request,open,,Integrate LLVM at llvm/llvm-project@94ed08d6b2a8,"Integrate LLVM at llvm/llvm-project@94ed08d6b2a8

Updates LLVM usage to match
[94ed08d6b2a8](https://github.com/llvm/llvm-project/commit/94ed08d6b2a8)
",copybara-service[bot],2024-07-23 16:06:37+00:00,[],2024-07-23 16:06:37+00:00,,https://github.com/tensorflow/tensorflow/pull/72373,[],[],
2425573544,pull_request,closed,,[xla:cpu] Make cpu_external_constants_test more robust to IR names,"[xla:cpu] Make cpu_external_constants_test more robust to IR names
",copybara-service[bot],2024-07-23 16:03:17+00:00,['ezhulenev'],2024-07-23 19:19:09+00:00,2024-07-23 19:19:08+00:00,https://github.com/tensorflow/tensorflow/pull/72372,[],[],
2425571380,pull_request,closed,,"Move delay kernel handling for GpuTimer into cuda_executor.cc, as it's only supported for CUDA, not ROCm.","Move delay kernel handling for GpuTimer into cuda_executor.cc, as it's only supported for CUDA, not ROCm.
",copybara-service[bot],2024-07-23 16:02:10+00:00,[],2024-07-23 22:03:29+00:00,2024-07-23 22:03:29+00:00,https://github.com/tensorflow/tensorflow/pull/72371,[],[],
2425516415,pull_request,open,,[Tosa] Add support for legalizing scatter_nd,"This commit legalizes tf(l).scatter_nd to tosa.scatter in a similar approach to the existing gather_nd support. Specifically, inputs are rewritten to the expected formats by tosa.scatter and finally the output of tosa.scatter is reshaped to the expected output of scatter_nd.

tosa.scatter does not support duplicated indices while TF does. Therefore, we restrict legalization of scatter_nd to a constant indices tensor only and check that the provided indices are unique.",lhutton1,2024-07-23 15:35:21+00:00,['gbaned'],2025-02-05 15:39:44+00:00,,https://github.com/tensorflow/tensorflow/pull/72370,"[('awaiting review', 'Pull request awaiting review'), ('size:L', 'CL Change Size: Large'), ('comp:lite-tosa', 'TFLite TOSA conversion issues')]","[{'comment_id': 2257552492, 'issue_id': 2425516415, 'author': 'keerthanakadiri', 'body': 'Hi @rdzhabarov , Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 7, 30, 6, 17, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2283457980, 'issue_id': 2425516415, 'author': 'keerthanakadiri', 'body': 'Hi @rdzhabarov , Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 8, 12, 9, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2298011416, 'issue_id': 2425516415, 'author': 'keerthanakadiri', 'body': 'Hi @rdzhabarov , Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 8, 20, 5, 42, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311693405, 'issue_id': 2425516415, 'author': 'keerthanakadiri', 'body': 'Hi @rdzhabarov , Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 8, 27, 6, 41, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2324017878, 'issue_id': 2425516415, 'author': 'keerthanakadiri', 'body': 'Hi @rdzhabarov , Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 9, 2, 7, 32, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2354469175, 'issue_id': 2425516415, 'author': 'keerthanakadiri', 'body': 'Hi @rdzhabarov , Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 9, 17, 4, 14, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2367699078, 'issue_id': 2425516415, 'author': 'keerthanakadiri', 'body': 'Hi @rdzhabarov , Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 9, 23, 9, 38, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2384877446, 'issue_id': 2425516415, 'author': 'keerthanakadiri', 'body': 'Hi @rdzhabarov , Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 10, 1, 6, 7, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401388256, 'issue_id': 2425516415, 'author': 'keerthanakadiri', 'body': 'Hi @rdzhabarov , Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 10, 9, 6, 4, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2431514712, 'issue_id': 2425516415, 'author': 'keerthanakadiri', 'body': 'Hi @rdzhabarov , Can you please review this PR? Thank you !', 'created_at': datetime.datetime(2024, 10, 23, 9, 40, 9, tzinfo=datetime.timezone.utc)}]","keerthanakadiri on (2024-07-30 06:17:29 UTC): Hi @rdzhabarov , Can you please review this PR? Thank you !

keerthanakadiri on (2024-08-12 09:09:00 UTC): Hi @rdzhabarov , Can you please review this PR? Thank you !

keerthanakadiri on (2024-08-20 05:42:35 UTC): Hi @rdzhabarov , Can you please review this PR? Thank you !

keerthanakadiri on (2024-08-27 06:41:07 UTC): Hi @rdzhabarov , Can you please review this PR? Thank you !

keerthanakadiri on (2024-09-02 07:32:47 UTC): Hi @rdzhabarov , Can you please review this PR? Thank you !

keerthanakadiri on (2024-09-17 04:14:08 UTC): Hi @rdzhabarov , Can you please review this PR? Thank you !

keerthanakadiri on (2024-09-23 09:38:51 UTC): Hi @rdzhabarov , Can you please review this PR? Thank you !

keerthanakadiri on (2024-10-01 06:07:13 UTC): Hi @rdzhabarov , Can you please review this PR? Thank you !

keerthanakadiri on (2024-10-09 06:04:45 UTC): Hi @rdzhabarov , Can you please review this PR? Thank you !

keerthanakadiri on (2024-10-23 09:40:09 UTC): Hi @rdzhabarov , Can you please review this PR? Thank you !

"
2425498641,pull_request,closed,,[XLA:GPU] Add a version of HloAnyOf for HloFusionAdaptor that iterates over instruction.,"[XLA:GPU] Add a version of HloAnyOf for HloFusionAdaptor that iterates over instruction.

We don't need to run BFS to iterate over all the instruction is a fusion adaptor. Performing a linear scan is much more efficient is cases when the order doesn't matter and we want to look through the all the instruction in the fusion.

There is a use-case for BFS when we start not from the root or the order matters.

Added 'Bfs' to the name of existing 'HloFindIf' and 'HloFindAny' function to indicate the difference.
",copybara-service[bot],2024-07-23 15:27:23+00:00,[],2024-07-24 14:21:15+00:00,2024-07-24 14:21:12+00:00,https://github.com/tensorflow/tensorflow/pull/72368,[],[],
2425277546,pull_request,closed,,[XLA:CPU] Add runtime check for whether `batch-norm-training` is rewritten,"[XLA:CPU] Add runtime check for whether `batch-norm-training` is rewritten

In the current runtime, emitting for op `batch-norm-training` is not supported by design.
The op is expected to be rewritten by another HLO pass before ever reaching the emit phase.
This CL adds a runtime check  for whether this op was actually rewritten and returns an explicit message if it wasn't.
Also includes a new unit test covering existing and new functionality: batch_norm_training_test.cc.
",copybara-service[bot],2024-07-23 13:51:03+00:00,[],2024-07-24 12:26:25+00:00,2024-07-24 12:26:25+00:00,https://github.com/tensorflow/tensorflow/pull/72367,[],"[{'comment_id': 2245318351, 'issue_id': 2425277546, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72367/checks?check_run_id=27807359135) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 7, 23, 13, 51, 8, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-07-23 13:51:08 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72367/checks?check_run_id=27807359135) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2425157198,pull_request,closed,,PR #15222: [GPU] Fix cuDNN workspace test condition.,"PR #15222: [GPU] Fix cuDNN workspace test condition.

Imported from GitHub PR https://github.com/openxla/xla/pull/15222


Copybara import of the project:

--
e57258f605e6df6bf61ac55632e939b5badd6c22 by Ilia Sergachev <isergachev@nvidia.com>:

[GPU] Fix cuDNN workspace test condition.

Merging this change closes #15222

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15222 from openxla:fix_cudnn_test e57258f605e6df6bf61ac55632e939b5badd6c22
",copybara-service[bot],2024-07-23 12:59:16+00:00,[],2024-07-23 14:14:35+00:00,2024-07-23 14:14:34+00:00,https://github.com/tensorflow/tensorflow/pull/72363,[],[],
2425127495,pull_request,closed,,[XLA:GPU] Don't upcast supported fp8 dot operands when is are inside Triton fusion.,"[XLA:GPU] Don't upcast supported fp8 dot operands when is are inside Triton fusion.

Keep normalizing fp8 outside of Triton, but in the Triton fused computations, certain operand type combinations are fine.
",copybara-service[bot],2024-07-23 12:45:06+00:00,[],2024-07-25 10:01:17+00:00,2024-07-25 10:01:16+00:00,https://github.com/tensorflow/tensorflow/pull/72361,[],[],
2425015724,pull_request,closed,,PR #15210: Require Matching Types in Layer Norm Fusion,"PR #15210: Require Matching Types in Layer Norm Fusion

Imported from GitHub PR https://github.com/openxla/xla/pull/15210

Disables the fusion of layer norm patterns when the input and output types are not the same.
Copybara import of the project:

--
4872f4413abbbc5e330a36f1c2ce5a5264a61fd7 by Philipp Hack <phack@nvidia.com>:

Disables layer norm fusion when the input and output types differ.

Merging this change closes #15210

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15210 from philipphack:u_layer_type_mismatch_xla 4872f4413abbbc5e330a36f1c2ce5a5264a61fd7
",copybara-service[bot],2024-07-23 11:53:31+00:00,[],2024-07-23 12:36:09+00:00,2024-07-23 12:36:09+00:00,https://github.com/tensorflow/tensorflow/pull/72360,[],[],
2424937950,pull_request,closed,,[XLA:CPU] Add runtime check for whether `batch-norm-grad` is rewritten,"[XLA:CPU] Add runtime check for whether `batch-norm-grad` is rewritten

In the current runtime, emitting for op `batch-norm-grad` is not supported by design.
The op is expected to be rewritten by another HLO pass before ever reaching the emit phase.
This CL adds a runtime check  for whether this op was actually rewritten and returns an explicit message if it wasn't.
Also includes a new unit test covering existing and new functionality: batch_norm_grad_test.cc.
",copybara-service[bot],2024-07-23 11:16:32+00:00,[],2024-07-24 10:09:54+00:00,2024-07-24 10:09:53+00:00,https://github.com/tensorflow/tensorflow/pull/72359,[],"[{'comment_id': 2244961347, 'issue_id': 2424937950, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72359/checks?check_run_id=27799829992) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 7, 23, 11, 16, 37, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-07-23 11:16:37 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72359/checks?check_run_id=27799829992) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2424826348,pull_request,closed,,[xla:cpu] Rename thunks execute event to avoid confusion,"[xla:cpu] Rename thunks execute event to avoid confusion

We already have another execute_event in a scope.
",copybara-service[bot],2024-07-23 10:22:13+00:00,['ezhulenev'],2024-07-23 19:59:01+00:00,2024-07-23 19:59:00+00:00,https://github.com/tensorflow/tensorflow/pull/72358,[],[],
2424820169,pull_request,closed,,[xla:cpu] Disable HLO execution profile test on CPU backend,"[xla:cpu] Disable HLO execution profile test on CPU backend

Similar to GPU backend we can't generate profiling data that is anywhere close to reality, so we prefer to disable the test for now.
",copybara-service[bot],2024-07-23 10:19:05+00:00,['ezhulenev'],2024-07-23 11:01:43+00:00,2024-07-23 11:01:43+00:00,https://github.com/tensorflow/tensorflow/pull/72357,[],[],
2424811891,pull_request,closed,,[xla:cpu] Disable thunks support in xla_jit_compiled_cpu_function,"[xla:cpu] Disable thunks support in xla_jit_compiled_cpu_function
",copybara-service[bot],2024-07-23 10:15:15+00:00,['ezhulenev'],2024-07-23 10:41:13+00:00,2024-07-23 10:41:13+00:00,https://github.com/tensorflow/tensorflow/pull/72356,[],[],
2424794909,pull_request,open,,Reverts e61124b6b9933cc48a9bf0cbf35a9384599a5761,"Reverts e61124b6b9933cc48a9bf0cbf35a9384599a5761
",copybara-service[bot],2024-07-23 10:06:41+00:00,['akuegel'],2024-07-24 12:52:37+00:00,,https://github.com/tensorflow/tensorflow/pull/72355,[],[],
2424757980,pull_request,closed,,[XLA_GPU][MLIR-based emitters] Add a pass to flatten tensors.,"[XLA_GPU][MLIR-based emitters] Add a pass to flatten tensors.

Right now it only supports tensor.extract, tensor.insert, xla_gpu.atomic_rmw, func.func and func.return, scf.for, scf.if and scf.yield.
",copybara-service[bot],2024-07-23 09:48:51+00:00,['pifon2a'],2024-07-25 15:00:35+00:00,2024-07-25 15:00:33+00:00,https://github.com/tensorflow/tensorflow/pull/72354,[],[],
2424718993,pull_request,closed,,Add support for Two-GPU tests in `xla_test`s.,"Add support for Two-GPU tests in `xla_test`s.
",copybara-service[bot],2024-07-23 09:30:28+00:00,['thomasjoerg'],2024-07-24 08:00:11+00:00,2024-07-24 08:00:10+00:00,https://github.com/tensorflow/tensorflow/pull/72353,[],[],
2424625318,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-23 08:46:44+00:00,[],2024-07-23 09:48:34+00:00,,https://github.com/tensorflow/tensorflow/pull/72351,[],[],
2424592610,pull_request,closed,,PR #15149: [XLA:GPU] print cudnn frontend check_support error message,"PR #15149: [XLA:GPU] print cudnn frontend check_support error message

Imported from GitHub PR https://github.com/openxla/xla/pull/15149


Copybara import of the project:

--
0e5c2fa7e539fe97f7c5b8aadfc3fedd38ad4232 by cjkkkk <ske@nvidia.com>:

init

--
8565f356e82549e05b9d46e8a92f3e49a4370f3b by cjkkkk <ske@nvidia.com>:

update function signature

Merging this change closes #15149

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15149 from Cjkkkk:print_cudnn_check_support_message 8565f356e82549e05b9d46e8a92f3e49a4370f3b
",copybara-service[bot],2024-07-23 08:30:23+00:00,[],2024-07-23 12:08:37+00:00,2024-07-23 12:08:36+00:00,https://github.com/tensorflow/tensorflow/pull/72350,[],[],
2424590981,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-23 08:29:33+00:00,[],2024-07-23 08:29:33+00:00,,https://github.com/tensorflow/tensorflow/pull/72349,[],[],
2424580730,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-23 08:24:31+00:00,[],2024-07-23 08:24:31+00:00,,https://github.com/tensorflow/tensorflow/pull/72348,[],[],
2424579526,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-23 08:23:55+00:00,[],2024-07-23 08:23:55+00:00,,https://github.com/tensorflow/tensorflow/pull/72347,[],[],
2424576753,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-23 08:22:35+00:00,[],2024-07-23 08:22:35+00:00,,https://github.com/tensorflow/tensorflow/pull/72346,[],[],
2424575691,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-23 08:22:04+00:00,[],2024-07-23 08:22:04+00:00,,https://github.com/tensorflow/tensorflow/pull/72345,[],[],
2424571210,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-23 08:19:47+00:00,[],2024-07-23 08:19:47+00:00,,https://github.com/tensorflow/tensorflow/pull/72344,[],[],
2424560774,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-23 08:14:27+00:00,[],2024-07-23 08:14:27+00:00,,https://github.com/tensorflow/tensorflow/pull/72343,[],[],
2424556989,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-23 08:12:27+00:00,[],2024-07-23 08:12:27+00:00,,https://github.com/tensorflow/tensorflow/pull/72342,[],[],
2424554420,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-23 08:11:05+00:00,[],2024-07-23 08:11:05+00:00,,https://github.com/tensorflow/tensorflow/pull/72341,[],[],
2424493013,pull_request,closed,,Handle non-trivial padding for direct standard conv legalizations.,"Handle non-trivial padding for direct standard conv legalizations.
This is done by pulling out *all* non-trivial padding before mhlo->tfl (but after convs are re-layoutted). Then, SAME padding is fused in later in tfl dialect.

This approach has the following benefits:
* Only need to handle negative slices in padding in one location
* Keeps legalizations simple and 1-1

Also turn off prepare patterns on 1d convs. These are not needed since we will rewrite 1d convs to 2d convs in the future.
",copybara-service[bot],2024-07-23 07:39:30+00:00,['LukeBoyer'],2024-07-23 19:39:11+00:00,2024-07-23 19:39:10+00:00,https://github.com/tensorflow/tensorflow/pull/72340,[],[],
2424483287,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-23 07:34:03+00:00,[],2024-07-23 07:34:03+00:00,,https://github.com/tensorflow/tensorflow/pull/72339,[],[],
2424466108,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-23 07:24:24+00:00,[],2024-07-23 07:24:24+00:00,,https://github.com/tensorflow/tensorflow/pull/72338,[],[],
2424276623,pull_request,closed,,Add support for mhlo.reduce_window -> tfl.maxpool,"Add support for mhlo.reduce_window -> tfl.maxpool
",copybara-service[bot],2024-07-23 05:16:38+00:00,['LukeBoyer'],2024-08-02 18:49:52+00:00,2024-08-02 18:49:51+00:00,https://github.com/tensorflow/tensorflow/pull/72337,[],[],
2424269574,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-23 05:09:32+00:00,[],2024-07-23 05:09:32+00:00,,https://github.com/tensorflow/tensorflow/pull/72336,[],[],
2424262752,pull_request,open,,Add support for conv 3d (no padding) in mhlo->tfl,"Add support for conv 3d (no padding) in mhlo->tfl
",copybara-service[bot],2024-07-23 05:02:36+00:00,['LukeBoyer'],2024-07-23 05:02:37+00:00,,https://github.com/tensorflow/tensorflow/pull/72335,[],[],
2424262706,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-23 05:02:34+00:00,[],2024-07-24 04:11:53+00:00,2024-07-24 04:11:52+00:00,https://github.com/tensorflow/tensorflow/pull/72334,[],[],
2424175841,pull_request,open,,Propagate shardings between operands of custom-call instructions.,"Propagate shardings between operands of custom-call instructions.
",copybara-service[bot],2024-07-23 03:44:39+00:00,[],2024-07-23 03:44:39+00:00,,https://github.com/tensorflow/tensorflow/pull/72333,[],[],
2423963574,pull_request,closed,,[XLA:GPU] Rename the debug flag xla_gpu_enable_address_computation_fusion to xla_gpu_enable_dynamic_slice_fusion for consistency.,"[XLA:GPU] Rename the debug flag xla_gpu_enable_address_computation_fusion to xla_gpu_enable_dynamic_slice_fusion for consistency.

""AddressComputation"" is confusing, it simply fuses dynamic slice (and dynamic update slice) into other thunks via buffer assignment tricks
",copybara-service[bot],2024-07-22 23:53:41+00:00,[],2024-07-29 20:37:11+00:00,2024-07-29 20:37:11+00:00,https://github.com/tensorflow/tensorflow/pull/72331,[],[],
2423960574,pull_request,open,,Internal Copybara config change,"Internal Copybara config change
",copybara-service[bot],2024-07-22 23:51:40+00:00,['ddunl'],2024-07-23 20:02:06+00:00,,https://github.com/tensorflow/tensorflow/pull/72330,[],[],
2423928971,pull_request,closed,,internal copybara refactor change,"internal copybara refactor change
",copybara-service[bot],2024-07-22 23:27:36+00:00,['ddunl'],2024-07-23 20:13:34+00:00,2024-07-23 20:13:33+00:00,https://github.com/tensorflow/tensorflow/pull/72329,[],[],
2423917541,pull_request,closed,,Add original_value field to HloInstruction,"Add original_value field to HloInstruction

Add the field to track a value in an optimized graph to its corresponding value in the unoptimized graph.
",copybara-service[bot],2024-07-22 23:15:05+00:00,['jcai19'],2024-07-30 19:14:21+00:00,2024-07-30 19:14:21+00:00,https://github.com/tensorflow/tensorflow/pull/72328,[],[],
2423889268,pull_request,closed,,[xla:cpu] Use InProcess collectives if run options do not provide an override,"[xla:cpu] Use InProcess collectives if run options do not provide an override

Make thunk implementation consistent with current XLA
",copybara-service[bot],2024-07-22 22:45:34+00:00,['ezhulenev'],2024-07-23 00:46:11+00:00,2024-07-23 00:46:11+00:00,https://github.com/tensorflow/tensorflow/pull/72327,[],[],
2423880927,pull_request,open,,Interal copybara config refactor,"Interal copybara config refactor
",copybara-service[bot],2024-07-22 22:37:29+00:00,['ddunl'],2024-07-22 22:37:29+00:00,,https://github.com/tensorflow/tensorflow/pull/72326,[],[],
2423863524,pull_request,open,,Internal diff tool for high level XLA IR.,"Internal diff tool for high level XLA IR.
",copybara-service[bot],2024-07-22 22:21:01+00:00,[],2024-07-22 22:21:01+00:00,,https://github.com/tensorflow/tensorflow/pull/72325,[],[],
2423856737,pull_request,open,,Integrate StableHLO at openxla/stablehlo@840c41ce,"Integrate StableHLO at openxla/stablehlo@840c41ce
",copybara-service[bot],2024-07-22 22:14:49+00:00,['ghpvnist'],2024-07-22 22:14:50+00:00,,https://github.com/tensorflow/tensorflow/pull/72324,[],[],
2423849482,pull_request,closed,,[XLA:GPU] Add HLO-based test for naive implementation of pipeline parallelism,"[XLA:GPU] Add HLO-based test for naive implementation of pipeline parallelism

- 4 devices
- 4 microbatches
- no circular repeat
- no disabled collectives
- no collective pipelining
",copybara-service[bot],2024-07-22 22:08:30+00:00,['frgossen'],2024-07-25 21:45:33+00:00,2024-07-25 21:45:33+00:00,https://github.com/tensorflow/tensorflow/pull/72323,[],[],
2423813764,pull_request,open,,[XLA:GPU] Uniquify only command buffer created instructions.,"[XLA:GPU] Uniquify only command buffer created instructions.
",copybara-service[bot],2024-07-22 21:38:33+00:00,[],2024-07-22 21:38:33+00:00,,https://github.com/tensorflow/tensorflow/pull/72322,[],[],
2423813319,pull_request,closed,,[XLA:GPU] Add convenience test function for `ExecuteReplicated`,"[XLA:GPU] Add convenience test function for `ExecuteReplicated`

Add convenience function to pass different inputs to different replicas of the same program.
",copybara-service[bot],2024-07-22 21:38:11+00:00,['frgossen'],2024-07-25 18:00:53+00:00,2024-07-25 18:00:53+00:00,https://github.com/tensorflow/tensorflow/pull/72321,[],[],
2423811251,pull_request,open,,Integrate LLVM at llvm/llvm-project@84658fb82b67,"Integrate LLVM at llvm/llvm-project@84658fb82b67

Updates LLVM usage to match
[84658fb82b67](https://github.com/llvm/llvm-project/commit/84658fb82b67)
",copybara-service[bot],2024-07-22 21:36:27+00:00,[],2024-07-22 23:13:45+00:00,,https://github.com/tensorflow/tensorflow/pull/72320,[],[],
2423811083,pull_request,closed,,[XLA:GPU] Move pipeline parallelism tests to separate test file.,"[XLA:GPU] Move pipeline parallelism tests to separate test file.

Reverts c93e8564e3398a8b86095437b49936e7eba289a9
",copybara-service[bot],2024-07-22 21:36:19+00:00,['frgossen'],2024-07-23 23:52:03+00:00,2024-07-23 23:52:02+00:00,https://github.com/tensorflow/tensorflow/pull/72319,[],[],
2423806866,pull_request,closed,,Part of the initial commit for the diff tool. Needed a unique fingerprint to distinguish between XLA Expressions in the immediate future.,"Part of the initial commit for the diff tool. Needed a unique fingerprint to distinguish between XLA Expressions in the immediate future.
",copybara-service[bot],2024-07-22 21:32:58+00:00,[],2024-07-22 23:44:23+00:00,2024-07-22 23:44:22+00:00,https://github.com/tensorflow/tensorflow/pull/72318,[],[],
2423800215,pull_request,closed,,[xla:cpu] Switch XLA:CPU runtime to thunks interpreter,"[xla:cpu] Switch XLA:CPU runtime to thunks interpreter

With this change XLA:CPU instead of compiling one LLVM function for the whole HLO module compiles separate functions for different fusions and runs them via the interpreter-like runtime.

This can change numerics because of slightly different LLVM IR and missed cross-fusion optimizations. If this breaks your tests, they likely have to relax numerical error tolerance.

Another potential issue is performance regressions for while loops with large number of iterations and small computation, as instead of compiling, we run such loops in interpreter. We plan to fix it in the future.

To disable thunks runtime set env variable: XLA_FLAGS=--xla_cpu_use_thunk_runtime=false.
",copybara-service[bot],2024-07-22 21:28:00+00:00,['ezhulenev'],2024-07-31 22:05:52+00:00,2024-07-31 22:05:51+00:00,https://github.com/tensorflow/tensorflow/pull/72317,[],[],
2423792366,pull_request,closed,,Remove unused cuda_stream.h and associated unused functions.,"Remove unused cuda_stream.h and associated unused functions.
",copybara-service[bot],2024-07-22 21:22:04+00:00,[],2024-07-23 01:45:48+00:00,2024-07-23 01:45:48+00:00,https://github.com/tensorflow/tensorflow/pull/72316,[],[],
2423766553,pull_request,closed,,Simplifies the handling of edge strategy variables.,"Simplifies the handling of edge strategy variables.
",copybara-service[bot],2024-07-22 21:03:08+00:00,[],2024-07-23 00:14:12+00:00,2024-07-23 00:14:11+00:00,https://github.com/tensorflow/tensorflow/pull/72315,[],[],
2423763677,pull_request,closed,,Removes 'e_val' from the AutoShardingSolverOutput class (these values can be determined from 's_val').,"Removes 'e_val' from the AutoShardingSolverOutput class (these values can be determined from 's_val').
",copybara-service[bot],2024-07-22 21:01:07+00:00,[],2024-07-22 22:39:34+00:00,2024-07-22 22:39:33+00:00,https://github.com/tensorflow/tensorflow/pull/72314,[],[],
2423716597,pull_request,closed,,Integrate StableHLO at openxla/stablehlo@840c41ce,"Integrate StableHLO at openxla/stablehlo@840c41ce
",copybara-service[bot],2024-07-22 20:28:57+00:00,['ghpvnist'],2024-07-22 22:47:46+00:00,2024-07-22 22:47:44+00:00,https://github.com/tensorflow/tensorflow/pull/72313,[],[],
2423709708,pull_request,closed,,[xla:cpu] Support run time pointer sizes for sorted elements,"[xla:cpu] Support run time pointer sizes for sorted elements
",copybara-service[bot],2024-07-22 20:24:20+00:00,['ezhulenev'],2024-07-22 21:03:15+00:00,2024-07-22 21:03:14+00:00,https://github.com/tensorflow/tensorflow/pull/72312,[],[],
2423696648,pull_request,closed,,"Use post order when searching for instructions to move in collective_pipeliner, such that","Use post order when searching for instructions to move in collective_pipeliner, such that
those instructions are also cloned in post order to avoid an assert on
last_cloned != nullptr.
",copybara-service[bot],2024-07-22 20:16:51+00:00,[],2024-07-27 17:30:09+00:00,2024-07-27 17:30:09+00:00,https://github.com/tensorflow/tensorflow/pull/72311,[],[],
2423696376,pull_request,closed,,[xla:cpu] Support for up to 16 sorted inputs,"[xla:cpu] Support for up to 16 sorted inputs

+ enable more jax/lax tests for XLA CPU thunks
",copybara-service[bot],2024-07-22 20:16:39+00:00,['ezhulenev'],2024-07-23 19:00:40+00:00,2024-07-23 19:00:39+00:00,https://github.com/tensorflow/tensorflow/pull/72310,[],[],
2423640217,pull_request,open,,Reverts 14725f749c49e4fdf540a712cee765d90f6f7805,"Reverts 14725f749c49e4fdf540a712cee765d90f6f7805
",copybara-service[bot],2024-07-22 19:42:49+00:00,[],2024-07-22 20:30:05+00:00,,https://github.com/tensorflow/tensorflow/pull/72309,[],[],
2423540142,pull_request,open,,Remove //third_party/tensorflow/lite/kernels/shim:tf_kernel_shim from pywrap_tensorflow_internal,"Remove //third_party/tensorflow/lite/kernels/shim:tf_kernel_shim from pywrap_tensorflow_internal
",copybara-service[bot],2024-07-22 18:41:38+00:00,['pak-laura'],2024-08-29 19:16:23+00:00,,https://github.com/tensorflow/tensorflow/pull/72308,[],[],
2423535283,pull_request,closed,,Use GpuContext in GpuEvent and derived classes rather than GpuExecutor.,"Use GpuContext in GpuEvent and derived classes rather than GpuExecutor.

The only thing GpuExecutor was used for was to get the GpuContext.
",copybara-service[bot],2024-07-22 18:39:02+00:00,[],2024-07-23 00:39:24+00:00,2024-07-23 00:39:22+00:00,https://github.com/tensorflow/tensorflow/pull/72307,[],[],
2423524269,pull_request,closed,,Use GpuEvent class instead of reimplementing portions of it in GpuTimer.,"Use GpuEvent class instead of reimplementing portions of it in GpuTimer.
",copybara-service[bot],2024-07-22 18:33:30+00:00,[],2024-07-22 21:48:05+00:00,2024-07-22 21:48:05+00:00,https://github.com/tensorflow/tensorflow/pull/72306,[],[],
2423520249,pull_request,closed,,update collective_permute_cycle_decomposer_test to include matmul operation,"update collective_permute_cycle_decomposer_test to include matmul operation
",copybara-service[bot],2024-07-22 18:30:53+00:00,[],2024-07-22 22:10:14+00:00,2024-07-22 22:10:14+00:00,https://github.com/tensorflow/tensorflow/pull/72305,[],[],
2423431068,pull_request,closed,,PR #15163: [NVIDIA GPU] Skip processing for trip count 1 in loop double buffer unrolling,"PR #15163: [NVIDIA GPU] Skip processing for trip count 1 in loop double buffer unrolling

Imported from GitHub PR https://github.com/openxla/xla/pull/15163

A minor improvement to loop double buffer unrolling pass to skip processing for loops with trip count =1 
Copybara import of the project:

--
f6bccd4612dde10cd020141f804523a75d9c84a2 by TJ Xu <tjx@nvidia.com>:

Skip processing for trip count 1

--
19514326f83ee91f712ffbd74cec90b19db77df7 by TJ Xu <tjx@nvidia.com>:

added a test case

Merging this change closes #15163

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15163 from Tixxx:tixxx/double_buffer_skip-1 19514326f83ee91f712ffbd74cec90b19db77df7
",copybara-service[bot],2024-07-22 17:52:13+00:00,[],2024-07-23 11:38:24+00:00,2024-07-23 11:38:23+00:00,https://github.com/tensorflow/tensorflow/pull/72304,[],[],
2423386708,pull_request,closed,,"Pass GpuContext to GpuTimer creation instead of GpuExecutor, as that's the only thing from GpuExecutor that's needed.","Pass GpuContext to GpuTimer creation instead of GpuExecutor, as that's the only thing from GpuExecutor that's needed.
",copybara-service[bot],2024-07-22 17:26:43+00:00,[],2024-07-22 21:42:12+00:00,2024-07-22 21:42:12+00:00,https://github.com/tensorflow/tensorflow/pull/72303,[],[],
2423377036,pull_request,closed,,Integrate LLVM at llvm/llvm-project@acc159aea1e6,"Integrate LLVM at llvm/llvm-project@acc159aea1e6

Updates LLVM usage to match
[acc159aea1e6](https://github.com/llvm/llvm-project/commit/acc159aea1e6)
",copybara-service[bot],2024-07-22 17:21:00+00:00,[],2024-07-23 14:23:07+00:00,2024-07-23 14:23:06+00:00,https://github.com/tensorflow/tensorflow/pull/72302,[],[],
2423304812,pull_request,closed,,Remove now unused GpuTimer::Create functions.,"Remove now unused GpuTimer::Create functions.
",copybara-service[bot],2024-07-22 16:39:58+00:00,[],2024-07-22 18:59:21+00:00,2024-07-22 18:59:21+00:00,https://github.com/tensorflow/tensorflow/pull/72301,[],[],
2423235109,pull_request,closed,,Update Shardy commit hash,"Update Shardy commit hash
",copybara-service[bot],2024-07-22 16:03:49+00:00,[],2024-07-22 20:03:34+00:00,2024-07-22 20:03:33+00:00,https://github.com/tensorflow/tensorflow/pull/72300,[],[],
2423224370,pull_request,closed,,Fix log macro redefinition when building for aarch64 android.,"Fix log macro redefinition when building for aarch64 android.
",copybara-service[bot],2024-07-22 15:58:09+00:00,['qukhan'],2024-07-24 11:59:42+00:00,2024-07-24 11:59:41+00:00,https://github.com/tensorflow/tensorflow/pull/72299,[],[],
2423199510,pull_request,closed,,[GPUUtil] Add a TraceMe for when buffers are staged to pinned memory before H2D memcpy.,"[GPUUtil] Add a TraceMe for when buffers are staged to pinned memory before H2D memcpy.
",copybara-service[bot],2024-07-22 15:46:48+00:00,[],2024-07-22 16:22:56+00:00,2024-07-22 16:22:56+00:00,https://github.com/tensorflow/tensorflow/pull/72298,[],[],
2423120886,pull_request,closed,,Fill in parsers & printers for indexing_map attribute,"Fill in parsers & printers for indexing_map attribute

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15285 from shraiysh:fix_pjrt_c_api_gpu_test_gpu 587bebe70c7d298008eff0c65dfcfa901e1fe21a
",copybara-service[bot],2024-07-22 15:08:26+00:00,[],2024-07-25 09:13:51+00:00,2024-07-25 09:13:50+00:00,https://github.com/tensorflow/tensorflow/pull/72297,[],[],
2422981068,pull_request,closed,,[XLA:GPU] Uniquify only command buffer created instructions.,"[XLA:GPU] Uniquify only command buffer created instructions.
",copybara-service[bot],2024-07-22 14:09:48+00:00,[],2024-07-22 22:19:09+00:00,2024-07-22 22:19:09+00:00,https://github.com/tensorflow/tensorflow/pull/72296,[],[],
2422896053,pull_request,closed,,[XLA:CPU] Support `add-dependency` in thunks runtime.,"[XLA:CPU] Support `add-dependency` in thunks runtime.
",copybara-service[bot],2024-07-22 13:32:56+00:00,[],2024-07-23 09:33:47+00:00,2024-07-23 09:33:47+00:00,https://github.com/tensorflow/tensorflow/pull/72294,[],"[{'comment_id': 2242974658, 'issue_id': 2422896053, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72294/checks?check_run_id=27750923814) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 7, 22, 13, 33, 2, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-07-22 13:33:02 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72294/checks?check_run_id=27750923814) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2422825380,pull_request,open,,Integrate LLVM at llvm/llvm-project@2ee5586ac7d8,"Integrate LLVM at llvm/llvm-project@2ee5586ac7d8

Updates LLVM usage to match
[2ee5586ac7d8](https://github.com/llvm/llvm-project/commit/2ee5586ac7d8)
",copybara-service[bot],2024-07-22 13:01:31+00:00,[],2024-07-22 14:59:35+00:00,,https://github.com/tensorflow/tensorflow/pull/72293,[],[],
2422696778,pull_request,closed,,[XLA:CPU] Add runtime check if `stochastic-convert` was decomposed.,"[XLA:CPU] Add runtime check if `stochastic-convert` was decomposed.

In the current runtime, `stochastic-convert` op emitting is not supported by design, normally it is rewritten by `StochasticConvertDecomposer` HLO pass and never reaches the emit phase. This CL adds a runtime check if this op was actually rewritten and returns an explicit message if it wasn't (instead of cryptic message that `kStochasticConvert` opcode was not handled in elemental IR emitter).

Also added fundamental tests for stochastic convert op for both runtimes - current and thunks.
",copybara-service[bot],2024-07-22 12:00:08+00:00,[],2024-07-24 10:27:25+00:00,2024-07-24 10:27:24+00:00,https://github.com/tensorflow/tensorflow/pull/72292,[],"[{'comment_id': 2242783890, 'issue_id': 2422696778, 'author': 'google-cla[bot]', 'body': ""Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\nView this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72292/checks?check_run_id=27746208067) of the CLA check for more information.\n\nFor the most up to date status, view the checks section at the bottom of the pull request."", 'created_at': datetime.datetime(2024, 7, 22, 12, 0, 14, tzinfo=datetime.timezone.utc)}]","google-cla[bot] on (2024-07-22 12:00:14 UTC): Thanks for your pull request! It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).

View this [failed invocation](https://github.com/tensorflow/tensorflow/pull/72292/checks?check_run_id=27746208067) of the CLA check for more information.

For the most up to date status, view the checks section at the bottom of the pull request.

"
2422691197,pull_request,closed,,PR #15176: [GPU] Fix use of DfsHloRewriteVisitor by cuDNN fusion compiler.,"PR #15176: [GPU] Fix use of DfsHloRewriteVisitor by cuDNN fusion compiler.

Imported from GitHub PR https://github.com/openxla/xla/pull/15176

DfsHloVisitor visits replaced instructions again. This used to happen here on addition of workspace, HandleFusion() was called again. SetVisited() prevents that.
Copybara import of the project:

--
25b0a8bd982bd47bb87c9baab79382f09127f2fa by Ilia Sergachev <isergachev@nvidia.com>:

[GPU] Fix use of DfsHloRewriteVisitor by cuDNN fusion compiler.

DfsHloVisitor visits replaced instructions again. This used to happen
here on addition of workspace, HandleFusion() was called again.
SetVisited() prevents that.

Merging this change closes #15176

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15176 from openxla:fix_cudnn_compiler 25b0a8bd982bd47bb87c9baab79382f09127f2fa
",copybara-service[bot],2024-07-22 11:57:16+00:00,[],2024-07-22 12:50:24+00:00,2024-07-22 12:50:24+00:00,https://github.com/tensorflow/tensorflow/pull/72291,[],[],
2422685444,pull_request,closed,,Prepare tests for MLIR reduction emitter launch.,"Prepare tests for MLIR reduction emitter launch.

- hlo tests just hard code the emitter level to 0. We can adjust
  these when we remove the flag (or remove them / move them to
  cc tests)
- tests that depend on MOF were adjusted to the new IR, except for
  one that tests something that does not occur in real pipelines.
  I'm not really sure what that test is trying to test - probably
  side outputs, which are covered in the unit test.
- one test that verified a failure to vectorize was disabled
",copybara-service[bot],2024-07-22 11:54:17+00:00,[],2024-07-22 13:09:13+00:00,2024-07-22 13:09:13+00:00,https://github.com/tensorflow/tensorflow/pull/72290,[],[],
2422625275,pull_request,open,,rolling back Handle bad_indices_policy for GatherNd and ScatterNd,"rolling back Handle bad_indices_policy for GatherNd and ScatterNd
",copybara-service[bot],2024-07-22 11:23:13+00:00,[],2024-07-22 11:23:13+00:00,,https://github.com/tensorflow/tensorflow/pull/72289,[],[],
2422606613,pull_request,open,,Handle bad_indices_policy for GatherNd and ScatterNd,"Handle bad_indices_policy for GatherNd and ScatterNd
",copybara-service[bot],2024-07-22 11:13:29+00:00,[],2024-07-22 11:13:29+00:00,,https://github.com/tensorflow/tensorflow/pull/72288,[],[],
2422601162,pull_request,closed,,Fix vectorization of tiny multi-row reductions.,"Fix vectorization of tiny multi-row reductions.

For these we can attempt to use a vectorization factor greater
than the row length, which is not something we currently support
in codegen.
",copybara-service[bot],2024-07-22 11:10:32+00:00,[],2024-07-22 12:11:21+00:00,2024-07-22 12:11:21+00:00,https://github.com/tensorflow/tensorflow/pull/72287,[],[],
2422429446,pull_request,closed,,Don't skip materialization of indices for some selects.,"Don't skip materialization of indices for some selects.

If the select is not really elementwise, we just materialize
the indices. This is very rare, so keeping the code reasonably
simple is more important than saving all possible materializations.
",copybara-service[bot],2024-07-22 09:44:39+00:00,[],2024-07-22 12:31:11+00:00,2024-07-22 12:31:10+00:00,https://github.com/tensorflow/tensorflow/pull/72286,[],[],
2422406710,pull_request,closed,,Fix indexing maps for broadcasting elementwise.,"Fix indexing maps for broadcasting elementwise.
",copybara-service[bot],2024-07-22 09:34:06+00:00,[],2024-07-22 10:26:59+00:00,2024-07-22 10:26:59+00:00,https://github.com/tensorflow/tensorflow/pull/72285,[],[],
2422318709,pull_request,closed,,Fix multi-output reductions.,"Fix multi-output reductions.

In the existing test, the variadic reduction was the
last one, so the output index and the root index were
the same. If the variadic reduction is the first one,
or there is more than one, the current logic is broken.
",copybara-service[bot],2024-07-22 08:52:49+00:00,[],2024-07-22 10:03:15+00:00,2024-07-22 10:03:14+00:00,https://github.com/tensorflow/tensorflow/pull/72283,[],[],
2422298760,pull_request,closed,,PR #15153: [XLA:CPU][oneDNN] Revert PR 13527 causing accuracy issue,"PR #15153: [XLA:CPU][oneDNN] Revert PR 13527 causing accuracy issue

Imported from GitHub PR https://github.com/openxla/xla/pull/15153

Revert ""PR #13527: [XLA:CPU][oneDNN] Enable mm-bias-add fusion""
This reverts commit 4ac9fdaae77256df4531eca38683d70777abf434 as we are seeing accuracy issues with some workloads.
Copybara import of the project:

--
003d71c185e8b0e2f3f486f80839a2c20a7410dd by Kanvi Khanna <kanvi.khanna@intel.com>:

Revert ""PR #13527: [XLA:CPU][oneDNN] Enable mm-bias-add fusion""

This reverts commit 4ac9fdaae77256df4531eca38683d70777abf434.

Merging this change closes #15153

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15153 from Intel-tensorflow:kanvi/accuracy-fix 003d71c185e8b0e2f3f486f80839a2c20a7410dd
",copybara-service[bot],2024-07-22 08:43:39+00:00,[],2024-07-22 11:12:15+00:00,2024-07-22 11:12:14+00:00,https://github.com/tensorflow/tensorflow/pull/72280,[],[],
2422274930,pull_request,closed,,PR #15029: Skip emitting Triton kernel when deserializing from cache,"PR #15029: Skip emitting Triton kernel when deserializing from cache

Imported from GitHub PR https://github.com/openxla/xla/pull/15029

This change avoids running final part of the Triton kernel emission when deserializing form cache. This can make a 0.5-1s difference on larger Pallas kernels (we see ~600ms/2x improvement in deserialization time of a step/update function with Pallas attention kernel).
Copybara import of the project:

--
700c1704ff124042185a5b3e8dba82b5eca6bc34 by Jaroslav Sevcik <jsevcik@nvidia.com>:

Skip emitting Triton kernel when deserializing

--
69505da4cd81a35c390f6301f4a474eb2d0c0c67 by Jaroslav Sevcik <jsevcik@nvidia.com>:

Address reviewer comments

Merging this change closes #15029

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15029 from jaro-sevcik:avoid-triton-compilation-on-deserialization 69505da4cd81a35c390f6301f4a474eb2d0c0c67
",copybara-service[bot],2024-07-22 08:31:37+00:00,[],2024-07-22 11:43:54+00:00,2024-07-22 11:43:54+00:00,https://github.com/tensorflow/tensorflow/pull/72278,[],[],
2422270763,pull_request,closed,,Legalize mhlo.reduce window to tfl,"Legalize mhlo.reduce window to tfl

* Add op view for reduce window
* Move some of the conv view logic into common file to be shared
* Add new pattern that makes all reduce_window NHWC in prepare
* Migrate average pool lowering to mhlo->tfl, use new op view, lots of cleanups 

The pattern to transpose reduce windows needs to go at the same time as the avg pool legalizations. This is because inserting the transposes will disrupt the mhlo->tf path, but that logic is covered in the new legalization.
",copybara-service[bot],2024-07-22 08:29:30+00:00,['LukeBoyer'],2024-08-02 18:20:13+00:00,2024-08-02 18:20:12+00:00,https://github.com/tensorflow/tensorflow/pull/72277,[],[],
2422246824,pull_request,closed,,Add physical device ordinal to the run options.,"Add physical device ordinal to the run options.
",copybara-service[bot],2024-07-22 08:17:23+00:00,['changhuilin'],2024-07-22 20:31:50+00:00,2024-07-22 20:31:49+00:00,https://github.com/tensorflow/tensorflow/pull/72276,[],[],
2422099743,pull_request,closed,,PR #15081: Adds a convenient python binding for obtaining instruction costs.,"PR #15081: Adds a convenient python binding for obtaining instruction costs.

Imported from GitHub PR https://github.com/openxla/xla/pull/15081

The python binding being added in this PR can help quick analysis on the cost of the HLO instructions with easy python code.

The binding is tested with a real tensorboard dir:

```
import jax.lib.xla_bridge as xb
client = xb.xla_client
costs = client.profiler.get_instructions_profile(tensorboard_dir)
for name, cost in costs:
  print(name, cost)
```

works.
Copybara import of the project:

--
e890d5ca56eaa212d63e2f578140758ffe230923 by Yunlong Liu <yunlongl@x.ai>:

in memory python representation of profiled instructions

Merging this change closes #15081

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/15081 from yliu120:main e890d5ca56eaa212d63e2f578140758ffe230923
",copybara-service[bot],2024-07-22 06:58:46+00:00,[],2024-07-22 10:21:13+00:00,2024-07-22 10:21:13+00:00,https://github.com/tensorflow/tensorflow/pull/72275,[],[],
2422024088,pull_request,open,,PR #14202: [fusion] Add RS->DUS dynamic slice fusion,"PR #14202: [fusion] Add RS->DUS dynamic slice fusion

Imported from GitHub PR https://github.com/openxla/xla/pull/14202

This patch adds execution support and fusion rewriting support for reduce-scatter -> dynamic-update-slice pattern.
Copybara import of the project:

--
04ac09da6ae7e18fb426b7d8b2eb5c3550c9c855 by Shraiysh Vaishay <svaishay@nvidia.com>:

[fusion] Add RS->DUS dynamic slice fusion

This patch adds execution support and fusion rewriting support for reduce-scatter -> dynamic-update-slice pattern.

Merging this change closes #14202

FUTURE_COPYBARA_INTEGRATE_REVIEW=https://github.com/openxla/xla/pull/14202 from shraiysh:rs_ds_fusion 04ac09da6ae7e18fb426b7d8b2eb5c3550c9c855
",copybara-service[bot],2024-07-22 06:08:53+00:00,[],2024-07-22 16:01:03+00:00,,https://github.com/tensorflow/tensorflow/pull/72274,[],[],
2421952356,pull_request,closed,,Export CustomOptimizer as tpu.experimental.embedding.CustomOptimizer.,"Export CustomOptimizer as tpu.experimental.embedding.CustomOptimizer.
",copybara-service[bot],2024-07-22 05:12:24+00:00,[],2024-07-22 16:30:14+00:00,2024-07-22 16:30:13+00:00,https://github.com/tensorflow/tensorflow/pull/72273,[],[],
2421856640,pull_request,closed,,[xla:cpu] Add sort operation with non-aliasing args and results buffers,"[xla:cpu] Add sort operation with non-aliasing args and results buffers
",copybara-service[bot],2024-07-22 03:55:04+00:00,['ezhulenev'],2024-07-22 05:13:51+00:00,2024-07-22 05:13:51+00:00,https://github.com/tensorflow/tensorflow/pull/72271,[],[],
2421845082,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-22 03:41:06+00:00,[],2024-07-24 05:27:28+00:00,2024-07-24 05:27:27+00:00,https://github.com/tensorflow/tensorflow/pull/72270,[],[],
2421844520,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-22 03:40:43+00:00,[],2024-07-22 03:40:43+00:00,,https://github.com/tensorflow/tensorflow/pull/72269,[],[],
2421842626,pull_request,open,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-22 03:38:22+00:00,[],2024-07-22 03:38:22+00:00,,https://github.com/tensorflow/tensorflow/pull/72268,[],[],
2421840896,pull_request,closed,,Automated Code Change,"Automated Code Change
",copybara-service[bot],2024-07-22 03:36:19+00:00,[],2024-07-23 05:41:47+00:00,2024-07-23 05:41:47+00:00,https://github.com/tensorflow/tensorflow/pull/72267,[],[],
