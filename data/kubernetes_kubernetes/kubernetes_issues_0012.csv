id,type,state,state_reason,title,body,author,created_at,assignees,updated_at,closed_at,url,labels,comments_list,comment_thread
1530587336,issue,closed,completed,Support labeled events in EventRecorder interface,"### What would you like to be added?

It would be nice support labeled Events in `EventRecorder` interface.

PoC of the required functionality available here:

https://github.com/odubajDT/client-go/commit/44fc5c38a817d0585a4692e15413ddc1f6291a8d

### Why is this needed?

Labels are the default filter objects for filtering in lists of k8s resources. It would be sufficient to have the option to set labels in Events, so users are able to filter them effectively.",odubajDT,2023-01-12 11:57:34+00:00,[],2023-01-27 09:04:48+00:00,2023-01-27 09:04:48+00:00,https://github.com/kubernetes/kubernetes/issues/115009,"[('priority/awaiting-more-evidence', 'Lowest priority. Possibly useful, but not yet enough support to actually get it done.'), ('sig/api-machinery', 'Categorizes an issue or PR as relevant to SIG API Machinery.'), ('kind/feature', 'Categorizes issue or PR as related to a new feature.'), ('sig/instrumentation', 'Categorizes an issue or PR as relevant to SIG Instrumentation.'), ('triage/accepted', 'Indicates an issue or PR is ready to be actively worked on.')]","[{'comment_id': 1381213737, 'issue_id': 1530587336, 'author': 'pacoxu', 'body': '/sig api-machinery', 'created_at': datetime.datetime(2023, 1, 13, 1, 56, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 1381702870, 'issue_id': 1530587336, 'author': 'odubajDT', 'body': 'PR fixing the issue: https://github.com/kubernetes/kubernetes/pull/115058', 'created_at': datetime.datetime(2023, 1, 13, 11, 17, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 1386076219, 'issue_id': 1530587336, 'author': 'cici37', 'body': '/sig instrumentation', 'created_at': datetime.datetime(2023, 1, 17, 21, 30, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 1386154015, 'issue_id': 1530587336, 'author': 'lavalamp', 'body': ""/priority awaiting-more-evidence\r\n\r\nLike I said on the PR, this could encourage people to do extremely non-performant things; we shouldn't add it without some concrete use case(s)."", 'created_at': datetime.datetime(2023, 1, 17, 22, 14, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 1397377223, 'issue_id': 1530587336, 'author': 'cici37', 'body': '/triage accepted', 'created_at': datetime.datetime(2023, 1, 19, 17, 48, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 1405565026, 'issue_id': 1530587336, 'author': 'dgrisonnet', 'body': '/cc', 'created_at': datetime.datetime(2023, 1, 26, 19, 59, tzinfo=datetime.timezone.utc)}]","pacoxu on (2023-01-13 01:56:33 UTC): /sig api-machinery

odubajDT (Issue Creator) on (2023-01-13 11:17:55 UTC): PR fixing the issue: https://github.com/kubernetes/kubernetes/pull/115058

cici37 on (2023-01-17 21:30:15 UTC): /sig instrumentation

lavalamp on (2023-01-17 22:14:35 UTC): /priority awaiting-more-evidence

Like I said on the PR, this could encourage people to do extremely non-performant things; we shouldn't add it without some concrete use case(s).

cici37 on (2023-01-19 17:48:46 UTC): /triage accepted

dgrisonnet on (2023-01-26 19:59:00 UTC): /cc

"
1530523945,issue,closed,completed,kubelet/cadvisor metrics are only getting bigger over time,"### What happened?

Metrics collected from http://node:10255/metrics/cadvisor endpoints include information on non-existent pods, making them larger and larger.

### What did you expect to happen?

Metrics data collected from endpoints http://node:10255/metrics/cadvisor should only include information on existing/running pods.

### How can we reproduce it (as minimally and precisely as possible)?

```console
$ cat test-pod.yml
---
apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  containers:
    - name: test
      image: almalinux:8
      command: [""/bin/sleep"", ""100""]
$ kubectl apply -f test-pod.yml
pod/test created
$ crictl ps | grep test
7084cfed5adf2       340c79d7a8cd9c23c7169e0455a2c16e5775c0b4a0590c0a5c522fedd9c123e2       3 seconds ago       Running       test       0       772d9f998e8b4
$ kubectl delete -f test-pod.yml
pod ""test"" deleted
$ curl -s http://localhost:10255/metrics/cadvisor | grep 7084cfed5adf2 | head -n1
$ curl -s http://localhost:10255/metrics/cadvisor | grep 772d9f998e8b4 | head -n1
container_cpu_load_average_10s{container=""POD"",id=""/pids/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod9e2b2641_45ed_49ff_b2ef_e7ec19797804.slice/crio-772d9f998e8b44622bfdeb0ef0e662c51115d3b49712c9a3d016ce8d252b7be9.scope"",image="""",name=""k8s_POD_test_default_9e2b2641-45ed-49ff-b2ef-e7ec19797804_0"",namespace=""default"",pod=""test""} 0 1673520901759
```

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""23"", GitVersion:""v1.23.12"", GitCommit:""c6939792865ef0f70f92006081690d77411c8ed5"", GitTreeState:""clean"", BuildDate:""2022-09-21T12:20:29Z"", GoVersion:""go1.17.13"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""23"", GitVersion:""v1.23.12"", GitCommit:""c6939792865ef0f70f92006081690d77411c8ed5"", GitTreeState:""clean"", BuildDate:""2022-09-21T12:13:07Z"", GoVersion:""go1.17.13"", Compiler:""gc"", Platform:""linux/amd64""}
```

</details>


### Cloud provider

<details>
Bare metal
</details>


### OS version

<details>

```console
$ cat /etc/os-release | hean -n2
NAME=""AlmaLinux""
VERSION=""8.7 (Stone Smilodon)""
$ uname -a
Linux node-1.k8s 4.18.0-425.3.1.el8.x86_64 #1 SMP Tue Nov 8 14:08:25 EST 2022 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


### Install tools

<details>

```console
$ runc -v
runc version 1.1.4
spec: 1.0.2-dev
go: go1.18.4
libseccomp: 2.5.2
```

</details>


### Container runtime (CRI) and version (if applicable)

<details>

```console
$ crio -v
crio version 1.23.4
Version:          1.23.4
GitCommit:        a6a1e6ebf4c4baca7ca7d57f069a4e0cd89056cd
GitTreeState:     clean
BuildDate:        2022-11-30T13:47:23Z
GoVersion:        go1.17.5
Compiler:         gc
Platform:         linux/amd64
Linkmode:         dynamic
BuildTags:        exclude_graphdriver_devicemapper, seccomp
SeccompEnabled:   true
AppArmorEnabled:  false
```

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",tomglaza,2023-01-12 11:07:21+00:00,['n4j'],2023-02-11 08:27:37+00:00,2023-02-11 08:27:36+00:00,https://github.com/kubernetes/kubernetes/issues/115008,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('sig/node', 'Categorizes an issue or PR as relevant to SIG Node.'), ('triage/accepted', 'Indicates an issue or PR is ready to be actively worked on.')]","[{'comment_id': 1381214018, 'issue_id': 1530523945, 'author': 'pacoxu', 'body': '/sig node\r\n/area cadvisor', 'created_at': datetime.datetime(2023, 1, 13, 1, 56, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 1381301769, 'issue_id': 1530523945, 'author': 'n4j', 'body': '@tomglaza In the statement `curl -s http://localhost:10255/metrics/cadvisor | grep 772d9f998e8b4 | head -n1`  how did you figure pod id `772d9f998e8b4`?', 'created_at': datetime.datetime(2023, 1, 13, 4, 26, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 1381415680, 'issue_id': 1530523945, 'author': 'tomglaza', 'body': ""@n4j It is in the result of the `crictl ps | grep test` command at the end of the line (you need to scroll to the right).\r\n\r\nHowever, perhaps this is not a problem with `kubernetes/cadvisor` as I initially assumed, because I found an older cluster (same version of kubernetes /1.23.12/, but lower version of cri-o /1.21.7/) and there the problem does not occur.\r\nBut it's strange, because it is recommended to have cri-o in the same version as kubernetes.\r\n\r\nEdit: I found a very similar looking issue in the cri-o project [1.24.3 leaks pidns with cgroup v2](https://github.com/cri-o/cri-o/issues/6349) but I don't know anymore whether the problem is on the side of cri-o or kubelet."", 'created_at': datetime.datetime(2023, 1, 13, 7, 23, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 1387325109, 'issue_id': 1530523945, 'author': 'n4j', 'body': ""/assign\r\nLet me figure what's going on here"", 'created_at': datetime.datetime(2023, 1, 18, 16, 11, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 1387590630, 'issue_id': 1530523945, 'author': 'SergeyKanzhelev', 'body': '/triage accepted', 'created_at': datetime.datetime(2023, 1, 18, 18, 55, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 1387713093, 'issue_id': 1530523945, 'author': 'tomglaza', 'body': 'A small update:\r\n- update cri-o to 1.23.5 with no changes\r\n- downgrade cri-o to 1.22.5 no change\r\n- after downgrading cri-o to 1.21.7 the problem goes away\r\n- using the https://github.com/kubernetes/kubernetes/issues/106957#issuecomment-1123465945 script on one of the machines reduced the size of the reported metrics from 59.2M to 7541k', 'created_at': datetime.datetime(2023, 1, 18, 20, 10, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 1396062605, 'issue_id': 1530523945, 'author': 'dims', 'body': '@tomglaza are you suspecting that this is only with cri-o?\r\n\r\n( cc @mrunalp @haircommander )', 'created_at': datetime.datetime(2023, 1, 18, 20, 44, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 1396630089, 'issue_id': 1530523945, 'author': 'tomglaza', 'body': ""@dims It's hard to say because I only have clusters with cri-o and docker. On docker I haven't seen this problem, and containerd I haven't tried."", 'created_at': datetime.datetime(2023, 1, 19, 8, 49, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 1396851098, 'issue_id': 1530523945, 'author': 'dims', 'body': '@tomglaza docker underneath uses containerd, so probably safe to rule that out.', 'created_at': datetime.datetime(2023, 1, 19, 11, 49, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 1398841733, 'issue_id': 1530523945, 'author': 'haircommander', 'body': ""@n4j thanks for volunteering to work on this! let me know if there's anything I can do to help :)"", 'created_at': datetime.datetime(2023, 1, 20, 19, 33, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 1399176045, 'issue_id': 1530523945, 'author': 'n4j', 'body': ""> @n4j thanks for volunteering to work on this! let me know if there's anything I can do to help :)\r\n\r\nThanks @haircommander, I'll let you know"", 'created_at': datetime.datetime(2023, 1, 21, 4, 34, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 1407312482, 'issue_id': 1530523945, 'author': 'n4j', 'body': 'Update my analysis on the root cause of the issue in this [comment](https://github.com/cri-o/cri-o/issues/6349#issuecomment-1407293012)', 'created_at': datetime.datetime(2023, 1, 28, 6, 51, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 1407312620, 'issue_id': 1530523945, 'author': 'n4j', 'body': '/remove-area cadvisor', 'created_at': datetime.datetime(2023, 1, 28, 6, 52, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 1426660489, 'issue_id': 1530523945, 'author': 'n4j', 'body': 'This issue is caused by `cri-o` it would be tracked via https://github.com/cri-o/cri-o/issues/6349\r\n\r\n/close', 'created_at': datetime.datetime(2023, 2, 11, 8, 27, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 1426660506, 'issue_id': 1530523945, 'author': 'k8s-ci-robot', 'body': '@n4j: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/115008#issuecomment-1426660489):\n\n>This issue is caused by `cri-o` it would be tracked via https://github.com/cri-o/cri-o/issues/6349\r\n>\r\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 2, 11, 8, 27, 36, tzinfo=datetime.timezone.utc)}]","pacoxu on (2023-01-13 01:56:59 UTC): /sig node
/area cadvisor

n4j (Assginee) on (2023-01-13 04:26:31 UTC): @tomglaza In the statement `curl -s http://localhost:10255/metrics/cadvisor | grep 772d9f998e8b4 | head -n1`  how did you figure pod id `772d9f998e8b4`?

tomglaza (Issue Creator) on (2023-01-13 07:23:33 UTC): @n4j It is in the result of the `crictl ps | grep test` command at the end of the line (you need to scroll to the right).

However, perhaps this is not a problem with `kubernetes/cadvisor` as I initially assumed, because I found an older cluster (same version of kubernetes /1.23.12/, but lower version of cri-o /1.21.7/) and there the problem does not occur.
But it's strange, because it is recommended to have cri-o in the same version as kubernetes.

Edit: I found a very similar looking issue in the cri-o project [1.24.3 leaks pidns with cgroup v2](https://github.com/cri-o/cri-o/issues/6349) but I don't know anymore whether the problem is on the side of cri-o or kubelet.

n4j (Assginee) on (2023-01-18 16:11:09 UTC): /assign
Let me figure what's going on here

SergeyKanzhelev on (2023-01-18 18:55:16 UTC): /triage accepted

tomglaza (Issue Creator) on (2023-01-18 20:10:19 UTC): A small update:
- update cri-o to 1.23.5 with no changes
- downgrade cri-o to 1.22.5 no change
- after downgrading cri-o to 1.21.7 the problem goes away
- using the https://github.com/kubernetes/kubernetes/issues/106957#issuecomment-1123465945 script on one of the machines reduced the size of the reported metrics from 59.2M to 7541k

dims on (2023-01-18 20:44:28 UTC): @tomglaza are you suspecting that this is only with cri-o?

( cc @mrunalp @haircommander )

tomglaza (Issue Creator) on (2023-01-19 08:49:30 UTC): @dims It's hard to say because I only have clusters with cri-o and docker. On docker I haven't seen this problem, and containerd I haven't tried.

dims on (2023-01-19 11:49:03 UTC): @tomglaza docker underneath uses containerd, so probably safe to rule that out.

haircommander on (2023-01-20 19:33:22 UTC): @n4j thanks for volunteering to work on this! let me know if there's anything I can do to help :)

n4j (Assginee) on (2023-01-21 04:34:28 UTC): Thanks @haircommander, I'll let you know

n4j (Assginee) on (2023-01-28 06:51:42 UTC): Update my analysis on the root cause of the issue in this [comment](https://github.com/cri-o/cri-o/issues/6349#issuecomment-1407293012)

n4j (Assginee) on (2023-01-28 06:52:36 UTC): /remove-area cadvisor

n4j (Assginee) on (2023-02-11 08:27:32 UTC): This issue is caused by `cri-o` it would be tracked via https://github.com/cri-o/cri-o/issues/6349

/close

k8s-ci-robot on (2023-02-11 08:27:36 UTC): @n4j: Closing this issue.

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/115008#issuecomment-1426660489):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

"
1530259597,issue,closed,not_planned,"Kubelet's volume manager called NodeUnpublishVolume for a volume which is being referred in a running pod, leading to I/O error ","### What happened?

Kubelet's volume manager reconciller called UnPublish on a pvc which was being referred in a running pod due to which pod started getting I/O error on that volume.

Reason -
Upon kubelet restart, before marking hasAddedPods = true Kubelet's volume manager populates it's desired state of world (dsw) and waits for first run of populatorLoop() to complete without checking an error. Ref - https://github.com/kubernetes/kubernetes/blob/c9ed04762f94a319d7b1fb718dc345491a32bea6/pkg/kubelet/volumemanager/populator/desired_state_of_world_populator.go#L152

Hence even  if the dsw population failed for a pod and  it's volumes were not added to dsw cache, since hasAddedPods will be set as true, rc.Sync() will be run (ref - https://github.com/kubernetes/kubernetes/blob/c9ed04762f94a319d7b1fb718dc345491a32bea6/pkg/kubelet/volumemanager/reconciler/reconciler.go#L40) which will later lead reconciller to UnPublish the volume that is being referred in a running pod. 

Proof from logs -

`
* Desired state population failed 

E1120 05:42:09.926885  119395 desired_state_of_world_populator.go:320] Error processing volume ""pgdata"" for pod ""xyzp-qbcq-5d548b897b-7vtbm_xxx(593281ff-6dd7-4739-bb32-a8d7613a05f6)"": error processing PVC xxx/pgdata: failed to fetch PVC from API server: Get https://x.y.z.z:8773/api/v1/namespaces/xxx/persistentvolumeclaims/cape-qbcq: write tcp x.y.z.z:53950-> x.y.z.z:8773: connection refused 

* rc.Sync() started. 
I1120 05:42:56.284186  119395 reconciler.go:156] Reconciler: start to sync state 

* No pod present in desired state for the pvc since add to desired state failed, hence pvc was updated in asw. 

I1120 05:43:03.221985  119395 reconciler.go:413] Reconciler sync states: could not find pod information in desired state, update it in actual state: &{volumeName:kubernetes.io/csi/csi.xyz.com^XYZVolumes-82daf8b8-e560-45b0-b5d8-28e01b504d55 podName:593281ff-6dd7-4739-bb32-a8d7613a05f6 volumeSpec:0xc000eda400 outerVolumeSpecName:pvc-aae65597-8774-4494-9b39-c6227c20c977 pod:0xc000907800 volumeGidValue: devicePath: mounter:0xc003a75e00 deviceMounter:0xc00226d170 blockVolumeMapper:<nil>} 

* Volume unpublished since it's not in dsw but in asw - 
I1120 05:44:41.031528  119395 csi_client.go:297] xyz.io/csi: calling NodeUnpublishVolume rpc: [volid=XYZVolumes-82daf8b8-e560-45b0-b5d8-28e01b504d55, target_path=/var/lib/kubelet/pods/593281ff-6dd7-4739-bb32-a8d7613a05f6/volumes/kubernetes.io~csi/pvc-aae65597-8774-4494-9b39-c6227c20c977/mount `




### What did you expect to happen?

Upon kubelet start Kubelet should wait for first succcessful run of volume manager's desired state populator before setting hasAddedPods = true. 

### How can we reproduce it (as minimally and precisely as possible)?

Restart kubelet, etcd concurrently on one node setup and we can hit this. 

### Anything else we need to know?

_No response_

### Kubernetes version

```
kubectl version
Client Version: version.Info{Major:""1"", Minor:""17+"", GitVersion:""v1.17.17-14+fe67ddaa1a766a"", GitCommit:""fe67ddaa1a766adef42da57cd7e8ca9e95b9d177"", GitTreeState:""dirty"", BuildDate:""2023-01-11T08:40:04Z"", GoVersion:""go1.13.15"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""17+"", GitVersion:""v1.17.17-14+22072f0c39bf60"", GitCommit:""22072f0c39bf6070774ae6ea522b16b70b3038a2"", GitTreeState:""dirty"", BuildDate:""2022-12-21T09:33:33Z"", GoVersion:""go1.13.15"", Compiler:""gc"", Platform:""linux/amd64""}
```

from the code looks like that bug is present in master as well. 

### Cloud provider

On-Prem


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",Adarsh-kumar,2023-01-12 07:51:30+00:00,[],2023-06-17 20:19:59+00:00,2023-06-17 20:19:58+00:00,https://github.com/kubernetes/kubernetes/issues/115006,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('sig/storage', 'Categorizes an issue or PR as relevant to SIG Storage.'), ('lifecycle/rotten', 'Denotes an issue or PR that has aged beyond stale and will be auto-closed.'), ('triage/needs-information', 'Indicates an issue needs more information in order to work on it.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1379934079, 'issue_id': 1530259597, 'author': 'k8s-ci-robot', 'body': '@Adarsh-kumar: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 12, 7, 51, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 1379938714, 'issue_id': 1530259597, 'author': 'Adarsh-kumar', 'body': '/sig node', 'created_at': datetime.datetime(2023, 1, 12, 7, 57, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 1379968986, 'issue_id': 1530259597, 'author': 'pacoxu', 'body': 'v1.17.17 is the end of life for a long time. Could you possibly reproduce this with a newer version?  See https://kubernetes.io/releases/', 'created_at': datetime.datetime(2023, 1, 12, 8, 26, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 1387594897, 'issue_id': 1530259597, 'author': 'SergeyKanzhelev', 'body': '> v1.17.17 is the end of life for a long time. Could you possibly reproduce this with a newer version? See https://kubernetes.io/releases/\r\n\r\n/triage needs-information\r\n/sig storage\r\n/remove-sig node\r\n\r\nstorage may be more appropriate for the original triage', 'created_at': datetime.datetime(2023, 1, 18, 18, 57, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 1513688461, 'issue_id': 1530259597, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 4, 18, 19, 29, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 1553563003, 'issue_id': 1530259597, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 5, 18, 19, 47, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 1595851423, 'issue_id': 1530259597, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 6, 17, 20, 19, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 1595851437, 'issue_id': 1530259597, 'author': 'k8s-ci-robot', 'body': '@k8s-triage-robot: Closing this issue, marking it as ""Not Planned"".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/115006#issuecomment-1595851423):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 6, 17, 20, 19, 58, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-12 07:51:39 UTC): @Adarsh-kumar: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

Adarsh-kumar (Issue Creator) on (2023-01-12 07:57:05 UTC): /sig node

pacoxu on (2023-01-12 08:26:13 UTC): v1.17.17 is the end of life for a long time. Could you possibly reproduce this with a newer version?  See https://kubernetes.io/releases/

SergeyKanzhelev on (2023-01-18 18:57:06 UTC): /triage needs-information
/sig storage
/remove-sig node

storage may be more appropriate for the original triage

k8s-triage-robot on (2023-04-18 19:29:14 UTC): The Kubernetes project currently lacks enough contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle stale`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle stale

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-triage-robot on (2023-05-18 19:47:25 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle rotten

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-triage-robot on (2023-06-17 20:19:53 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.

This bot triages issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Reopen this issue with `/reopen`
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/close not-planned

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-ci-robot on (2023-06-17 20:19:58 UTC): @k8s-triage-robot: Closing this issue, marking it as ""Not Planned"".

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/115006#issuecomment-1595851423):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

"
1529898775,issue,closed,completed,[KMSv2] Add symlink swap integration test for Hot Reload,"/sig auth
/triage accepted
/assign


Tests to implement -
- symlink swap integration test.
- update unit tests for encryption config controller to remove sleep. (Ref - https://github.com/kubernetes/kubernetes/pull/113896#issuecomment-1409508875)
- integration test for encrypting `events` resources. (Ref - https://github.com/kubernetes/kubernetes/pull/115149#issuecomment-1398602700)",nilekhc,2023-01-12 00:15:19+00:00,['nilekhc'],2023-11-28 08:23:19+00:00,2023-11-28 08:23:17+00:00,https://github.com/kubernetes/kubernetes/issues/114999,"[('sig/auth', 'Categorizes an issue or PR as relevant to SIG Auth.'), ('triage/accepted', 'Indicates an issue or PR is ready to be actively worked on.')]","[{'comment_id': 1699439952, 'issue_id': 1529898775, 'author': 'enj', 'body': 'Once we make the changes for #117728, if we confirm that the existing automatic reload integration tests pass, then we can probably just skip this specific test case because it will no longer be any different.', 'created_at': datetime.datetime(2023, 8, 30, 15, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 1829319581, 'issue_id': 1529898775, 'author': 'nilekhc', 'body': ""/close\r\n\r\nWith https://github.com/kubernetes/kubernetes/pull/121310 we don't need this particular test.\r\n\r\ncc @enj @aramase @ritazh"", 'created_at': datetime.datetime(2023, 11, 28, 8, 23, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 1829319751, 'issue_id': 1529898775, 'author': 'k8s-ci-robot', 'body': ""@nilekhc: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114999#issuecomment-1829319581):\n\n>/close\r\n>\r\n>With https://github.com/kubernetes/kubernetes/pull/121310 we don't need this particular test.\r\n>\r\n>cc @enj @aramase @ritazh \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"", 'created_at': datetime.datetime(2023, 11, 28, 8, 23, 17, tzinfo=datetime.timezone.utc)}]","enj on (2023-08-30 15:55:00 UTC): Once we make the changes for #117728, if we confirm that the existing automatic reload integration tests pass, then we can probably just skip this specific test case because it will no longer be any different.

nilekhc (Issue Creator) on (2023-11-28 08:23:11 UTC): /close

With https://github.com/kubernetes/kubernetes/pull/121310 we don't need this particular test.

cc @enj @aramase @ritazh

k8s-ci-robot on (2023-11-28 08:23:17 UTC): @nilekhc: Closing this issue.

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114999#issuecomment-1829319581):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

"
1529621238,issue,closed,completed,VolumeZone scheduling plugin should handle csi migration,"### What happened?

In https://github.com/kubernetes/kubernetes/issues/114268, we discovered that our plan for eventually removing the [VolumeZone](https://github.com/kubernetes/kubernetes/tree/master/pkg/scheduler/framework/plugins/volumezone) scheduling plugin when CSI migration is GA is not possible because the plugin also applies to non-migratable volume types.

However, if we leave the plugin as is, then even migratable PVs will be broken if we remove beta topology labels from Nodes in the future. This is because the plugin does not have any handling for migrated volumes.


### What did you expect to happen?

I suggest we add CSI migration handling to the VolumeZone plugin. I think we just need to detect if the volume is migrated, and if so, skip the VolumeZone processing. For migrated volumes, we translate `PV.Labels` into `PV.NodeAffinity` so they end up getting processed by the [VolumeBinding](https://github.com/kubernetes/kubernetes/tree/master/pkg/scheduler/framework/plugins/volumebinding) plugin.

### How can we reproduce it (as minimally and precisely as possible)?

In a cluster with csi migration for a specific intree volume type enabled,
1. create an intree PV with beta topology labels
2. remove the beta topology label from a node
3. try to schedule a pod using that pv

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


### Cloud provider

<details>
Any that supports csi migration
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",msau42,2023-01-11 20:08:32+00:00,['AxeZhan'],2023-08-18 03:20:29+00:00,2023-08-18 03:20:29+00:00,https://github.com/kubernetes/kubernetes/issues/114996,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('priority/important-soon', 'Must be staffed and worked on either currently, or very soon, ideally in time for the next release.'), ('sig/storage', 'Categorizes an issue or PR as relevant to SIG Storage.'), ('triage/accepted', 'Indicates an issue or PR is ready to be actively worked on.')]","[{'comment_id': 1379424696, 'issue_id': 1529621238, 'author': 'msau42', 'body': '/sig storage\r\n/triage accepted\r\n/priority important-soon\r\n\r\nFor e2e testing, this needs an environment that is configured with csi migration', 'created_at': datetime.datetime(2023, 1, 11, 20, 10, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 1379860099, 'issue_id': 1529621238, 'author': 'AxeZhan', 'body': ""Hi,I'd like to work on this issue. I previously wrote `PreFilter` for `volume_zone` plugin, so I'm familiar with it's logic. However I'm not familiar with the whole `CSI` thing, and I may ask you some questions during the work :(  \r\nCan I try this one?\r\n***\r\nHere are some questions ahead:\r\nI think what you want to do is to schedule a pv with old labels to a node with only new labels.\r\nAnd you think we can translate `PV.Labels` into `PV.NodeAffinity` in volume_zone during `PreFilter`(maybe), and update this pv so that in `Filter` stage, `volumebinding` can `CheckNodeAffinity`, so we don't have compare Topology labels in `volume_zone`.\r\n\r\nMy question is:\r\n>  For migrated volumes, we translate PV.Labels into PV.NodeAffinity\r\n\r\nCan't we just translate this for all pvs? Why do we need to detect if the volume is migrated? What's the difference between a migrated pv and a non-migratable pv in this scenario?"", 'created_at': datetime.datetime(2023, 1, 12, 6, 9, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 1380997618, 'issue_id': 1529621238, 'author': 'msau42', 'body': ""> Can't we just translate this for all pvs? Why do we need to detect if the volume is migrated? What's the difference between a migrated pv and a non-migratable pv in this scenario?\r\n\r\nI think this is also another alternative solution that could also work, but from https://github.com/kubernetes/kubernetes/issues/114268#issuecomment-1337414393, it may not scalable to do an automatic migration for all the different APIs and controllers that use the node labels. The main difference with CSI migration, is we have a good idea of all the different components that would be impacted and also have reasonable control over those components."", 'created_at': datetime.datetime(2023, 1, 12, 21, 10, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 1381427878, 'issue_id': 1529621238, 'author': 'AxeZhan', 'body': ""> I think we just need to detect if the volume is migrated, and if so, skip the VolumeZone processing. For migrated volumes, we translate PV.Labels into PV.NodeAffinity so they end up getting processed by the [VolumeBinding](https://github.com/kubernetes/kubernetes/tree/master/pkg/scheduler/framework/plugins/volumebinding) plugin.\r\n\r\nA few more questions😂\r\n\r\n1.  > detect if the volume is migrated\r\n\r\nWill the volumes be migrated before we take a pod off the scheduler queue? If so, how can I detect this. Currently I find a function `IsPVMigratable` in `csi-translation-lib`, but nothing like  `IsPVMigrated`.\r\n\r\n2.  > For migrated volumes, we translate PV.Labels into PV.NodeAffinity\r\n\r\nIs this translation  done in `volume_zone` plugin or before scheduling?\r\n\r\n3.  I think currently in `volume_zone`, we consider topology label and beta label as different labels. That is ,if we have a pv with `LabelFailureDomainBetaZone` and a node with `LabelTopologyZone`, even if they have same values, `volume_zone` will `not` let the pod pass filter. If we're going to remove beta labels on node,maybe we should try to consider two labels as the same in `volume_zone`?\r\n***\r\nHere's my understanding:\r\n- For migrated volumes, we translate Topology labels to pv.NodeAffinity and update the pv, then we skip volume_zone(that is,don't compare labels with nodes).\r\n- What about non-migratable volumes, I think we should continue to compare pv.labels with node labels for non-migratable volumes?"", 'created_at': datetime.datetime(2023, 1, 13, 7, 39, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 1382579817, 'issue_id': 1529621238, 'author': 'msau42', 'body': ""For detecting if a PV is migrated, you can use the same method that's used in the VolumeBinding plugin: https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/framework/plugins/volumebinding/binder.go#L1129. The only difference is you don't actually need to translate the PV in the VolumeZone plugin since it's already handled by the VolumeBinding plugin.\r\n\r\nFor non-migratable volumes, I think we can continue with the same VolumeZone logic. For now, we'll rely on users to update their PVs with the GA labels."", 'created_at': datetime.datetime(2023, 1, 13, 23, 22, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 1382639345, 'issue_id': 1529621238, 'author': 'AxeZhan', 'body': ""Thanks @msau42 ,for your detailed explanation. This is the implementation plan I prepared:\r\nIn `volume_zone prefilter`, we check if a pv is migrtable by using `IsPVMigratable`,if so we skip volume_zone since `volume_binding` will translate this pv and handle it later, otherwise we save the label of this pv in state,and compare it with node's label in `filter`.Is that feasible to you?"", 'created_at': datetime.datetime(2023, 1, 14, 2, 31, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 1386426636, 'issue_id': 1529621238, 'author': 'msau42', 'body': 'Yup that sounds good to me!', 'created_at': datetime.datetime(2023, 1, 18, 3, 19, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 1386430423, 'issue_id': 1529621238, 'author': 'AxeZhan', 'body': ""Thanks! I'll try to work on this one. I'll file a pr a few days later and we can talk about the details in there.\r\nIn the meantime, I think I previously wrote a bug in `volume_zone` plugin😢, I wonder if you have time to confirm it #115052 \r\n\r\n/assign"", 'created_at': datetime.datetime(2023, 1, 18, 3, 25, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 1512414140, 'issue_id': 1529621238, 'author': 'k8s-triage-robot', 'body': 'This issue is labeled with `priority/important-soon` but has not been updated in over 90 days, and should be re-triaged.\nImportant-soon issues must be staffed and worked on either currently, or very soon, ideally in time for the next release.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Deprioritize it with `/priority important-longterm` or `/priority backlog`\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted', 'created_at': datetime.datetime(2023, 4, 18, 4, 22, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 1571516633, 'issue_id': 1529621238, 'author': 'msau42', 'body': '/triage accepted', 'created_at': datetime.datetime(2023, 6, 1, 7, 37, 40, tzinfo=datetime.timezone.utc)}]","msau42 (Issue Creator) on (2023-01-11 20:10:19 UTC): /sig storage
/triage accepted
/priority important-soon

For e2e testing, this needs an environment that is configured with csi migration

AxeZhan (Assginee) on (2023-01-12 06:09:50 UTC): Hi,I'd like to work on this issue. I previously wrote `PreFilter` for `volume_zone` plugin, so I'm familiar with it's logic. However I'm not familiar with the whole `CSI` thing, and I may ask you some questions during the work :(  
Can I try this one?
***
Here are some questions ahead:
I think what you want to do is to schedule a pv with old labels to a node with only new labels.
And you think we can translate `PV.Labels` into `PV.NodeAffinity` in volume_zone during `PreFilter`(maybe), and update this pv so that in `Filter` stage, `volumebinding` can `CheckNodeAffinity`, so we don't have compare Topology labels in `volume_zone`.

My question is:

Can't we just translate this for all pvs? Why do we need to detect if the volume is migrated? What's the difference between a migrated pv and a non-migratable pv in this scenario?

msau42 (Issue Creator) on (2023-01-12 21:10:52 UTC): I think this is also another alternative solution that could also work, but from https://github.com/kubernetes/kubernetes/issues/114268#issuecomment-1337414393, it may not scalable to do an automatic migration for all the different APIs and controllers that use the node labels. The main difference with CSI migration, is we have a good idea of all the different components that would be impacted and also have reasonable control over those components.

AxeZhan (Assginee) on (2023-01-13 07:39:46 UTC): A few more questions😂

1.  > detect if the volume is migrated

Will the volumes be migrated before we take a pod off the scheduler queue? If so, how can I detect this. Currently I find a function `IsPVMigratable` in `csi-translation-lib`, but nothing like  `IsPVMigrated`.

2.  > For migrated volumes, we translate PV.Labels into PV.NodeAffinity

Is this translation  done in `volume_zone` plugin or before scheduling?

3.  I think currently in `volume_zone`, we consider topology label and beta label as different labels. That is ,if we have a pv with `LabelFailureDomainBetaZone` and a node with `LabelTopologyZone`, even if they have same values, `volume_zone` will `not` let the pod pass filter. If we're going to remove beta labels on node,maybe we should try to consider two labels as the same in `volume_zone`?
***
Here's my understanding:
- For migrated volumes, we translate Topology labels to pv.NodeAffinity and update the pv, then we skip volume_zone(that is,don't compare labels with nodes).
- What about non-migratable volumes, I think we should continue to compare pv.labels with node labels for non-migratable volumes?

msau42 (Issue Creator) on (2023-01-13 23:22:36 UTC): For detecting if a PV is migrated, you can use the same method that's used in the VolumeBinding plugin: https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/framework/plugins/volumebinding/binder.go#L1129. The only difference is you don't actually need to translate the PV in the VolumeZone plugin since it's already handled by the VolumeBinding plugin.

For non-migratable volumes, I think we can continue with the same VolumeZone logic. For now, we'll rely on users to update their PVs with the GA labels.

AxeZhan (Assginee) on (2023-01-14 02:31:30 UTC): Thanks @msau42 ,for your detailed explanation. This is the implementation plan I prepared:
In `volume_zone prefilter`, we check if a pv is migrtable by using `IsPVMigratable`,if so we skip volume_zone since `volume_binding` will translate this pv and handle it later, otherwise we save the label of this pv in state,and compare it with node's label in `filter`.Is that feasible to you?

msau42 (Issue Creator) on (2023-01-18 03:19:09 UTC): Yup that sounds good to me!

AxeZhan (Assginee) on (2023-01-18 03:25:02 UTC): Thanks! I'll try to work on this one. I'll file a pr a few days later and we can talk about the details in there.
In the meantime, I think I previously wrote a bug in `volume_zone` plugin😢, I wonder if you have time to confirm it #115052 

/assign

k8s-triage-robot on (2023-04-18 04:22:57 UTC): This issue is labeled with `priority/important-soon` but has not been updated in over 90 days, and should be re-triaged.
Important-soon issues must be staffed and worked on either currently, or very soon, ideally in time for the next release.

You can:
- Confirm that this issue is still relevant with `/triage accepted` (org members only)
- Deprioritize it with `/priority important-longterm` or `/priority backlog`
- Close this issue with `/close`

For more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/

/remove-triage accepted

msau42 (Issue Creator) on (2023-06-01 07:37:40 UTC): /triage accepted

"
1529567341,issue,closed,completed,Allow PV NodeAffinity to update from beta to GA topology labels,"### What happened?

In https://github.com/kubernetes/kubernetes/issues/114268, we realized that non in-tree PVs can be impacted by a future removal of beta topology labels from Nodes if they are supporting the beta topology labels.

### What did you expect to happen?

We should allow users to mutate the PV NodeAffinity to update uses of the beta topology label to the GA label.

### How can we reproduce it (as minimally and precisely as possible)?

Remove beta topology label from a Node, and try to schedule a Pod using a non-migratable PVC with beta topology label in the PV.NodeAffinity.

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


### Cloud provider

<details>
n/a
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",msau42,2023-01-11 19:29:26+00:00,['haoruan'],2023-03-07 05:56:19+00:00,2023-03-07 05:56:19+00:00,https://github.com/kubernetes/kubernetes/issues/114995,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('priority/important-soon', 'Must be staffed and worked on either currently, or very soon, ideally in time for the next release.'), ('sig/storage', 'Categorizes an issue or PR as relevant to SIG Storage.'), ('help wanted', 'Denotes an issue that needs help from a contributor. Must meet ""help wanted"" guidelines.'), ('triage/accepted', 'Indicates an issue or PR is ready to be actively worked on.')]","[{'comment_id': 1379381443, 'issue_id': 1529567341, 'author': 'msau42', 'body': '/sig storage\r\n/triage accepted\r\n/priority important-soon\r\n/help', 'created_at': datetime.datetime(2023, 1, 11, 19, 29, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 1379381466, 'issue_id': 1529567341, 'author': 'k8s-ci-robot', 'body': '@msau42: \n\tThis request has been marked as needing help from a contributor.\n\n### Guidelines\nPlease ensure that the issue body includes answers to the following questions:\n- Why are we solving this issue?\n- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?\n- Does this issue have zero to low barrier of entry?\n- How can the assignee reach out to you for help?\n\n\nFor more details on the requirements of such an issue, please see [here](https://git.k8s.io/community/contributors/guide/help-wanted.md) and ensure that they are met.\n\nIf this request no longer meets these requirements, the label can be removed\nby commenting with the `/remove-help` command.\n\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114995):\n\n>/sig storage\r\n>/triage accepted\r\n>/priority important-soon\r\n>/help\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 11, 19, 29, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 1397910005, 'issue_id': 1529567341, 'author': 'haoruan', 'body': '/assign', 'created_at': datetime.datetime(2023, 1, 20, 4, 28, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 1407596422, 'issue_id': 1529567341, 'author': 'haoruan', 'body': 'Do we allow users to mutate both non in-tree PVs and in-tree PVs, or only non in-tree PVs?', 'created_at': datetime.datetime(2023, 1, 29, 8, 21, 57, tzinfo=datetime.timezone.utc)}]","msau42 (Issue Creator) on (2023-01-11 19:29:51 UTC): /sig storage
/triage accepted
/priority important-soon
/help

k8s-ci-robot on (2023-01-11 19:29:52 UTC): @msau42: 
	This request has been marked as needing help from a contributor.

### Guidelines
Please ensure that the issue body includes answers to the following questions:
- Why are we solving this issue?
- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?
- Does this issue have zero to low barrier of entry?
- How can the assignee reach out to you for help?


For more details on the requirements of such an issue, please see [here](https://git.k8s.io/community/contributors/guide/help-wanted.md) and ensure that they are met.

If this request no longer meets these requirements, the label can be removed
by commenting with the `/remove-help` command.


<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114995):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

haoruan (Assginee) on (2023-01-20 04:28:16 UTC): /assign

haoruan (Assginee) on (2023-01-29 08:21:57 UTC): Do we allow users to mutate both non in-tree PVs and in-tree PVs, or only non in-tree PVs?

"
1529246475,issue,closed,completed,Different behaviour for regular and static pods upon deletion,"### What happened?

When the regular pod is deleted it's status immediately transitions into `Terminating`, however when the static pod is deleted it stays in `Running` status until almost till the end just before it gets wiped out. 

### What did you expect to happen?

The reported status of the regular pod and static pod should be identical after deletion. 

### How can we reproduce it (as minimally and precisely as possible)?

Sample pod, 

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test 
spec:
  containers:
  - name: test 
    image: busybox:latest 
    command:
    - sleep 
    - inf 
  terminationGracePeriodSeconds: 30
  restartPolicy: Never
```

Scenario 1, 

Deploy it as a regular pod. e.g. `kubectl create -f test.yaml` and once it's running delete it using `kubectl delete -f test.yaml`. The prompt won't return until the pod is actually deleted, so from another terminal check the pod status,

```bash
$ kubectl get pods
No resources found in default namespace.
$ kubectl get pods
NAME   READY   STATUS    RESTARTS   AGE
test   1/1     Running   0          6s
$ kubectl get pods
NAME   READY   STATUS        RESTARTS   AGE
test   1/1     Terminating   0          12s
$ kubectl get pods
NAME   READY   STATUS        RESTARTS   AGE
test   1/1     Terminating   0          19s
$ kubectl get pods
NAME   READY   STATUS        RESTARTS   AGE
test   1/1     Terminating   0          23s
$ kubectl get pods
NAME   READY   STATUS        RESTARTS   AGE
test   1/1     Terminating   0          25s
$ kubectl get pods
NAME   READY   STATUS        RESTARTS   AGE
test   1/1     Terminating   0          29s
$ kubectl get pods
NAME   READY   STATUS        RESTARTS   AGE
test   1/1     Terminating   0          32s
$ kubectl get pods
NAME   READY   STATUS        RESTARTS   AGE
test   1/1     Terminating   0          39s
```

As you can see the pod immediately goes in `Terminating` state. 

Scenario 2,

Now deploy the same pod as a static pod by, `cp test.yaml /var/run/kubernetes/static-pods/` and after it starts running remove it using `rm /var/run/kubernetes/static-pods/test.yaml`. However in this case, the pod doesn't immediately go in `Terminating` state. Instead, it stays in `Running` until a last split second where it transitions into `Terminating` just before getting wiped out. 

```bash
$ kubectl get pods
No resources found in default namespace.
$ kubectl get pods
NAME             READY   STATUS    RESTARTS   AGE
test-127.0.0.1   1/1     Running   0          3s
$ kubectl get pods
NAME             READY   STATUS    RESTARTS   AGE
test-127.0.0.1   1/1     Running   0          22s
$ kubectl get pods
NAME             READY   STATUS    RESTARTS   AGE
test-127.0.0.1   1/1     Running   0          24s
$ kubectl get pods
NAME             READY   STATUS    RESTARTS   AGE
test-127.0.0.1   1/1     Running   0          25s
$ kubectl get pods
NAME             READY   STATUS    RESTARTS   AGE
test-127.0.0.1   1/1     Running   0          27s
$ kubectl get pods
NAME             READY   STATUS    RESTARTS   AGE
test-127.0.0.1   1/1     Running   0          29s
$ kubectl get pods -w
NAME             READY   STATUS    RESTARTS   AGE
test-127.0.0.1   1/1     Running   0          33s
$ kubectl get pods 
NAME             READY   STATUS    RESTARTS   AGE
test-127.0.0.1   1/1     Running   0          42s
$ kubectl get pods -w
NAME             READY   STATUS    RESTARTS   AGE
test-127.0.0.1   1/1     Running   0          49s
test-127.0.0.1   0/1     Error     0          50s
test-127.0.0.1   0/1     Terminating   0          51s
test-127.0.0.1   0/1     Terminating   0          51s
```
 

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


### Cloud provider

<details>

</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",harche,2023-01-11 15:24:55+00:00,"['harche', 'Sajiyah-Salat']",2023-05-03 15:09:21+00:00,2023-05-03 15:09:19+00:00,https://github.com/kubernetes/kubernetes/issues/114986,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('sig/node', 'Categorizes an issue or PR as relevant to SIG Node.'), ('sig/cli', 'Categorizes an issue or PR as relevant to SIG CLI.'), ('triage/accepted', 'Indicates an issue or PR is ready to be actively worked on.')]","[{'comment_id': 1378943327, 'issue_id': 1529246475, 'author': 'harche', 'body': 'It could be kubelet issue or the issue with how kubectl reports the status. Not sure at this point.\r\n\r\n/sig node\r\n/sig cli', 'created_at': datetime.datetime(2023, 1, 11, 15, 25, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 1378944629, 'issue_id': 1529246475, 'author': 'harche', 'body': '/cc @rphillips', 'created_at': datetime.datetime(2023, 1, 11, 15, 26, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 1379345817, 'issue_id': 1529246475, 'author': 'SergeyKanzhelev', 'body': 'keep it untriaged to understand if this is a cli or kubelet issue. If it is just a visualization issue on cli, priority would be different', 'created_at': datetime.datetime(2023, 1, 11, 18, 57, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 1379346108, 'issue_id': 1529246475, 'author': 'SergeyKanzhelev', 'body': '/assign @harche', 'created_at': datetime.datetime(2023, 1, 11, 18, 57, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 1379752208, 'issue_id': 1529246475, 'author': 'Sajiyah-Salat', 'body': 'Hello harche, can we work together. Though I am beginner, I have a good understanding of kubernetes.', 'created_at': datetime.datetime(2023, 1, 12, 3, 4, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 1380322006, 'issue_id': 1529246475, 'author': 'harche', 'body': '@Sajiyah-Salat Sure, thanks.\r\n\r\n/assign @Sajiyah-Salat', 'created_at': datetime.datetime(2023, 1, 12, 13, 8, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 1381813390, 'issue_id': 1529246475, 'author': 'Sajiyah-Salat', 'body': 'Can we connect somewhere to discuss about it.', 'created_at': datetime.datetime(2023, 1, 13, 12, 56, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 1381984600, 'issue_id': 1529246475, 'author': 'harche', 'body': ""As a first step we need to decide if it's just issue with `kubectl` or really something isn't quite right within kubelet. I would start with tracing how `kubectl get pods` lists the pods and see how to proceed from there."", 'created_at': datetime.datetime(2023, 1, 13, 15, 0, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 1382735241, 'issue_id': 1529246475, 'author': 'Sajiyah-Salat', 'body': 'Great. Let me know your social handle.', 'created_at': datetime.datetime(2023, 1, 14, 13, 13, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 1384207747, 'issue_id': 1529246475, 'author': 'harche', 'body': 'We usually use github for collaboration, so I would prefer to stick with that.', 'created_at': datetime.datetime(2023, 1, 16, 15, 22, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 1384812667, 'issue_id': 1529246475, 'author': 'Sajiyah-Salat', 'body': 'Great then. Any updates?', 'created_at': datetime.datetime(2023, 1, 17, 4, 17, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 1387545418, 'issue_id': 1529246475, 'author': 'SergeyKanzhelev', 'body': '@Sajiyah-Salat do you plan to work on this and try to repro it as @harche suggested in https://github.com/kubernetes/kubernetes/issues/114986#issuecomment-1381984600. If we can stick with updating this issue with relevant finding instead of moving conversation to Slack - it will be useful for everybody.', 'created_at': datetime.datetime(2023, 1, 18, 18, 38, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 1387554865, 'issue_id': 1529246475, 'author': 'Sajiyah-Salat', 'body': 'I will try to get some findings', 'created_at': datetime.datetime(2023, 1, 18, 18, 41, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 1403039296, 'issue_id': 1529246475, 'author': 'Sajiyah-Salat', 'body': 'Hello any updates.', 'created_at': datetime.datetime(2023, 1, 25, 3, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 1404063852, 'issue_id': 1529246475, 'author': 'SergeyKanzhelev', 'body': '/triage accepted\r\n\r\n@Sajiyah-Salat myself and @harche are confused whether you plan to work on this. Please let us know.', 'created_at': datetime.datetime(2023, 1, 25, 18, 42, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 1404503089, 'issue_id': 1529246475, 'author': 'Sajiyah-Salat', 'body': ""Sorry @SergeyKanzhelev for the late reply. I tried to reproduce but I have minikube on my system. and while running command ``cp test.yaml /var/run/kubernetes/static-pods/`` I got this error ``cp: cannot create regular file '/var/run/kubernetes/static-pods/': No such file or directory`` I tried in killercoda and got the same error."", 'created_at': datetime.datetime(2023, 1, 26, 2, 51, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 1404510386, 'issue_id': 1529246475, 'author': 'Sajiyah-Salat', 'body': 'In this issue the author has **deleted**  the regular pod with this command\r\n``k delete -f test.yaml``\r\nwhile in static pod he had **remove** the static pod with this command\r\n``rm test.yaml``\r\nDont you guys think that both command works differently?', 'created_at': datetime.datetime(2023, 1, 26, 3, 3, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 1405283241, 'issue_id': 1529246475, 'author': 'harche', 'body': '@Sajiyah-Salat you can read more on static pods here, https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/\r\n\r\nBTW, would it be okay with you if I assign this issue to me?', 'created_at': datetime.datetime(2023, 1, 26, 16, 38, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 1410104455, 'issue_id': 1529246475, 'author': 'Sajiyah-Salat', 'body': 'Its completely fine if you take this. I am already working on other issues. Thank you.', 'created_at': datetime.datetime(2023, 1, 31, 10, 22, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 1446849128, 'issue_id': 1529246475, 'author': 'harche', 'body': 'I just verified that upcoming pod manager refactor, https://github.com/kubernetes/kubernetes/pull/115342, has no effect on this issue.', 'created_at': datetime.datetime(2023, 2, 27, 18, 31, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 1447057102, 'issue_id': 1529246475, 'author': 'harche', 'body': ""> I just verified that upcoming pod manager refactor, #115342, has no effect on this issue.\r\n\r\nAlthough it's not a complete code and still work in progress."", 'created_at': datetime.datetime(2023, 2, 27, 20, 42, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 1452539891, 'issue_id': 1529246475, 'author': 'smarterclayton', 'body': 'Copying from #114994 (I noticed this while looking at e2e tests for #113145 that @bobbypage was adding):\r\n\r\nWe discussed adding conditions to help api server clients understand when a pod had released its resources (right now there is no unambiguous signal that callers can safely use other than the transition to succeded or failed state, which is blocked by a bug on pending pods not going into a terminal phase that @bobbypage was gonna help the Job termination folks do...). We might want to consider one for indicating when a mirror pod is going away, OR we could just use graceful deletion with a finalizer tied to the kubelet and have the kubelet do the removal once the static pod is gone.\r\n\r\nStatic mirror pods need to behave like regular pods during deletion, update, and creation IMO. I think that includes readiness, pending termination and others.\r\n\r\nOne scenario we can\'t fix is if you reuse your static pod UID there\'s no way to represent ""completely changing"" in the API except via the spec update (which I think gets blocked?). Now I have to go figure out what that does...', 'created_at': datetime.datetime(2023, 3, 2, 21, 2, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 1460695923, 'issue_id': 1529246475, 'author': 'xmcqueen', 'body': '/cc xmcqueen', 'created_at': datetime.datetime(2023, 3, 8, 18, 51, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 1473112148, 'issue_id': 1529246475, 'author': 'rajeeshckr', 'body': ""We have self managed kubernetes cluster. We are running the API-server as a mirror pod. During shutdown of node, we wanted the API-server mirror pod to shutdown gracefully. Since [graceful node termination](https://kubernetes.io/blog/2021/04/21/graceful-node-shutdown-beta/#how-do-i-use-it) handles only regular pods, we can't use this approach.\r\nWe are using the systemd hooks to shutdown the API mirror pod gracefully once kubelet is shutting down on this node to get around this issue.\r\nWill this scenario can also be considered while working on this? Or we need a new issue?"", 'created_at': datetime.datetime(2023, 3, 17, 4, 28, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 1533212967, 'issue_id': 1529246475, 'author': 'harche', 'body': 'fixed by https://github.com/kubernetes/kubernetes/pull/116995 \r\n\r\n/close', 'created_at': datetime.datetime(2023, 5, 3, 15, 9, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 1533213172, 'issue_id': 1529246475, 'author': 'k8s-ci-robot', 'body': '@harche: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114986#issuecomment-1533212967):\n\n>fixed by https://github.com/kubernetes/kubernetes/pull/116995 \r\n>\r\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 5, 3, 15, 9, 20, tzinfo=datetime.timezone.utc)}]","harche (Issue Creator) on (2023-01-11 15:25:42 UTC): It could be kubelet issue or the issue with how kubectl reports the status. Not sure at this point.

/sig node
/sig cli

harche (Issue Creator) on (2023-01-11 15:26:17 UTC): /cc @rphillips

SergeyKanzhelev on (2023-01-11 18:57:26 UTC): keep it untriaged to understand if this is a cli or kubelet issue. If it is just a visualization issue on cli, priority would be different

SergeyKanzhelev on (2023-01-11 18:57:43 UTC): /assign @harche

Sajiyah-Salat (Assginee) on (2023-01-12 03:04:18 UTC): Hello harche, can we work together. Though I am beginner, I have a good understanding of kubernetes.

harche (Issue Creator) on (2023-01-12 13:08:30 UTC): @Sajiyah-Salat Sure, thanks.

/assign @Sajiyah-Salat

Sajiyah-Salat (Assginee) on (2023-01-13 12:56:39 UTC): Can we connect somewhere to discuss about it.

harche (Issue Creator) on (2023-01-13 15:00:23 UTC): As a first step we need to decide if it's just issue with `kubectl` or really something isn't quite right within kubelet. I would start with tracing how `kubectl get pods` lists the pods and see how to proceed from there.

Sajiyah-Salat (Assginee) on (2023-01-14 13:13:48 UTC): Great. Let me know your social handle.

harche (Issue Creator) on (2023-01-16 15:22:13 UTC): We usually use github for collaboration, so I would prefer to stick with that.

Sajiyah-Salat (Assginee) on (2023-01-17 04:17:33 UTC): Great then. Any updates?

SergeyKanzhelev on (2023-01-18 18:38:33 UTC): @Sajiyah-Salat do you plan to work on this and try to repro it as @harche suggested in https://github.com/kubernetes/kubernetes/issues/114986#issuecomment-1381984600. If we can stick with updating this issue with relevant finding instead of moving conversation to Slack - it will be useful for everybody.

Sajiyah-Salat (Assginee) on (2023-01-18 18:41:59 UTC): I will try to get some findings

Sajiyah-Salat (Assginee) on (2023-01-25 03:11:00 UTC): Hello any updates.

SergeyKanzhelev on (2023-01-25 18:42:55 UTC): /triage accepted

@Sajiyah-Salat myself and @harche are confused whether you plan to work on this. Please let us know.

Sajiyah-Salat (Assginee) on (2023-01-26 02:51:42 UTC): Sorry @SergeyKanzhelev for the late reply. I tried to reproduce but I have minikube on my system. and while running command ``cp test.yaml /var/run/kubernetes/static-pods/`` I got this error ``cp: cannot create regular file '/var/run/kubernetes/static-pods/': No such file or directory`` I tried in killercoda and got the same error.

Sajiyah-Salat (Assginee) on (2023-01-26 03:03:14 UTC): In this issue the author has **deleted**  the regular pod with this command
``k delete -f test.yaml``
while in static pod he had **remove** the static pod with this command
``rm test.yaml``
Dont you guys think that both command works differently?

harche (Issue Creator) on (2023-01-26 16:38:33 UTC): @Sajiyah-Salat you can read more on static pods here, https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/

BTW, would it be okay with you if I assign this issue to me?

Sajiyah-Salat (Assginee) on (2023-01-31 10:22:45 UTC): Its completely fine if you take this. I am already working on other issues. Thank you.

harche (Issue Creator) on (2023-02-27 18:31:35 UTC): I just verified that upcoming pod manager refactor, https://github.com/kubernetes/kubernetes/pull/115342, has no effect on this issue.

harche (Issue Creator) on (2023-02-27 20:42:21 UTC): Although it's not a complete code and still work in progress.

smarterclayton on (2023-03-02 21:02:10 UTC): Copying from #114994 (I noticed this while looking at e2e tests for #113145 that @bobbypage was adding):

We discussed adding conditions to help api server clients understand when a pod had released its resources (right now there is no unambiguous signal that callers can safely use other than the transition to succeded or failed state, which is blocked by a bug on pending pods not going into a terminal phase that @bobbypage was gonna help the Job termination folks do...). We might want to consider one for indicating when a mirror pod is going away, OR we could just use graceful deletion with a finalizer tied to the kubelet and have the kubelet do the removal once the static pod is gone.

Static mirror pods need to behave like regular pods during deletion, update, and creation IMO. I think that includes readiness, pending termination and others.

One scenario we can't fix is if you reuse your static pod UID there's no way to represent ""completely changing"" in the API except via the spec update (which I think gets blocked?). Now I have to go figure out what that does...

xmcqueen on (2023-03-08 18:51:27 UTC): /cc xmcqueen

rajeeshckr on (2023-03-17 04:28:34 UTC): We have self managed kubernetes cluster. We are running the API-server as a mirror pod. During shutdown of node, we wanted the API-server mirror pod to shutdown gracefully. Since [graceful node termination](https://kubernetes.io/blog/2021/04/21/graceful-node-shutdown-beta/#how-do-i-use-it) handles only regular pods, we can't use this approach.
We are using the systemd hooks to shutdown the API mirror pod gracefully once kubelet is shutting down on this node to get around this issue.
Will this scenario can also be considered while working on this? Or we need a new issue?

harche (Issue Creator) on (2023-05-03 15:09:15 UTC): fixed by https://github.com/kubernetes/kubernetes/pull/116995 

/close

k8s-ci-robot on (2023-05-03 15:09:20 UTC): @harche: Closing this issue.

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114986#issuecomment-1533212967):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

"
1529104822,issue,closed,completed,envFrom not working in ephemeral containers: failed to sync secret cache,"### What happened?

I have the following deployment:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mytarget
spec:
  selector:
    matchLabels:
      mylabel: mytarget
  replicas: 2
  template:
    metadata:
      labels:
        mylabel: mytarget
    spec:
      serviceAccountName: default
      containers:
        - name: mytarget
          image: <custom-image>
          command: [""sleep"", ""100d""]
```

I am working on an automated script (written in Go) that, among other things, does the following:

* Creates a Secret resource

```go
s := apiv1.Secret{
	ObjectMeta: metav1.ObjectMeta{
		Name:      name,
		Namespace: metav1.NamespaceDefault,
	},
	StringData: map[string]string{
		""username"": ""foo"",
		""password"": ""bar"",
	},
}

_, err := clientset.CoreV1().Secrets(metav1.NamespaceDefault).Create(ctx, &s, metav1.CreateOptions{})
```

* Lists pods and creates one ephemeral container for each one of them with the Secret added as env variable using `envFrom`:

```go
pod.Spec.EphemeralContainers = append(pod.Spec.EphemeralContainers, apiv1.EphemeralContainer{
	TargetContainerName: targetContainerName,
	EphemeralContainerCommon: apiv1.EphemeralContainerCommon{
		Image:   imageURL,
		Name:    ephemeralContainerName,
		Command: getCommand(),
		EnvFrom: []apiv1.EnvFromSource{
			{
				SecretRef: &apiv1.SecretEnvSource{
					LocalObjectReference: apiv1.LocalObjectReference{
						Name: name,
					},
					Optional: nil,
				},
			},
		},
		SecurityContext: &apiv1.SecurityContext{
			Privileged: &[]bool{true}[0],
		},
	},
})

clientset.CoreV1().Pods(metav1.NamespaceDefault).UpdateEphemeralContainers(
	ctx, pod.Name, &pod, metav1.UpdateOptions{},
)
```

When I run it, the ephemeral container is created but never started, it is left in `Waiting` state with Reason `CreateContainerConfigError`.

```bash
Ephemeral Containers:
  ephemeral1234567:
    Container ID:
    Image:         <image>
    Image ID:
    Port:          <none>
    Host Port:     <none>
    Command: <command>
    State:          Waiting
      Reason:       CreateContainerConfigError
    Ready:          False
    Restart Count:  0
    Environment Variables from:
      secret-name  Secret  Optional: false
    Environment:     <none>
    Mounts:          <none>
```

The code seems to be correct since I can see the Secret is referenced in the Environment Variables.

Checking the events I can see the following:

Type | Reason | Age | From | Message
--|--|--|--|--
Warning | Failed | 3s | kubelet | Error: failed to sync secret cache: timed out waiting for the condition

It seems the cache is not synced when an ephemeral container is created in a pod and therefore its creation fails because the Secret is not present in the cache.

Based on that previous assumption, I tried referencing the secret in the regular container defined in the deployment. 

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mytarget
spec:
  selector:
    matchLabels:
      mylabel: mytarget
  replicas: 2
  template:
    metadata:
      labels:
        mylabel: mytarget
    spec:
      serviceAccountName: default
      containers:
        - name: mytarget
          image: <custom-image>
          command: [""sleep"", ""100d""]
          envFrom:
          - secretRef:
              name: secret-name
```

Running the same code that creates the ephemeral container is now run successfully and it creates the container with the secret as an environment variable.

```bash
exp123456:
    Container ID:  containerd://2398e96b9add961fd292d127d9e383b98c0c88100f87e0b8c7a8ff46aa1becdc
    Image:         <image>
    Image ID:      <imageId>
    Port:          <none>
    Host Port:     <none>
    Command: <command>
    State:          Running
      Started:      Wed, 11 Jan 2023 13:28:57 +0000
    Ready:          False
    Restart Count:  0
    Environment Variables from:
      manual-secret  Secret  Optional: false
    Environment:     <none>
    Mounts:          <none>
```

### What did you expect to happen?

I would expect the ephemeral container to be created and successfully started with the appropriate environment variable containing the data in the secret.

### How can we reproduce it (as minimally and precisely as possible)?

Using the code provided in the explanation of the issue should be enough to reproduce the problem.

Deployment:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mytarget
spec:
  selector:
    matchLabels:
      mylabel: mytarget
  replicas: 2
  template:
    metadata:
      labels:
        mylabel: mytarget
    spec:
      serviceAccountName: default
      containers:
        - name: mytarget
          image: <custom-image>
          command: [""sleep"", ""100d""]
```

Secret:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: manual-secret
type: Opaque
stringData:
  token: test1234
```

Request to update the Pod to create an ephemeral container:

`PATCH http://localhost:8080/api/v1/namespaces/{namespace}/pods/{podname}/ephemeralcontainers`

Add the following ephemeral container
```
ephemeralContainers:
  - name: ephemeral-uid-123456
    image: nginx
    command:
    - ""sleep 100d""
    envFrom:
    - secretRef:
        name: manual-secret
    resources: {}
    terminationMessagePath: ""/dev/termination-log""
    terminationMessagePolicy: File
    imagePullPolicy: IfNotPresent
    securityContext:
      privileged: true
    targetContainerName: mytarget
```

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:""1"", Minor:""26"", GitVersion:""v1.26.0"", GitCommit:""b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d"", GitTreeState:""clean"", BuildDate:""2022-12-08T19:58:30Z"", GoVersion:""go1.19.4"", Compiler:""gc"", Platform:""linux/amd64""}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:""1"", Minor:""24+"", GitVersion:""v1.24.7-eks-fb459a0"", GitCommit:""c240013134c03a740781ffa1436ba2688b50b494"", GitTreeState:""clean"", BuildDate:""2022-10-24T20:36:26Z"", GoVersion:""go1.18.7"", Compiler:""gc"", Platform:""linux/amd64""}
WARNING: version difference between client (1.26) and server (1.24) exceeds the supported minor version skew of +/-1
```

</details>


### Cloud provider

<details>

</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME=""Alpine Linux""
ID=alpine
VERSION_ID=3.17.1
PRETTY_NAME=""Alpine Linux v3.17""
HOME_URL=""https://alpinelinux.org/""
BUG_REPORT_URL=""https://gitlab.alpinelinux.org/alpine/aports/-/issues""
$ uname -a
Linux pod-6bb864f684-tl5l7 5.4.219-126.411.amzn2.x86_64 #1 SMP Wed Nov 2 17:44:17 UTC 2022 x86_64 Linux
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",AdriaGS,2023-01-11 14:02:09+00:00,['cslink'],2025-01-23 23:53:22+00:00,2025-01-23 23:53:22+00:00,https://github.com/kubernetes/kubernetes/issues/114984,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('sig/node', 'Categorizes an issue or PR as relevant to SIG Node.'), ('sig/auth', 'Categorizes an issue or PR as relevant to SIG Auth.'), ('priority/important-longterm', 'Important over the long term, but may not be staffed and/or may need multiple releases to complete.'), ('triage/accepted', 'Indicates an issue or PR is ready to be actively worked on.')]","[{'comment_id': 1378805507, 'issue_id': 1529104822, 'author': 'pacoxu', 'body': '/sig node', 'created_at': datetime.datetime(2023, 1, 11, 14, 7, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 1379348297, 'issue_id': 1529104822, 'author': 'SergeyKanzhelev', 'body': 'Just a wild guess, can this be related to this bug: https://github.com/kubernetes/kubernetes/issues/114167?', 'created_at': datetime.datetime(2023, 1, 11, 18, 59, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 1379349006, 'issue_id': 1529104822, 'author': 'SergeyKanzhelev', 'body': '@AdriaGS is it 100% repro or there should be special conditions to repro this issue?\r\n\r\n/triage needs-information', 'created_at': datetime.datetime(2023, 1, 11, 19, 0, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 1379515221, 'issue_id': 1529104822, 'author': 'liggitt', 'body': 'I think the issue is the node authorizer not considering that pod updates might add a new secret reference\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/97bbf07d3f3f20332912ee411fdf75ce84425e28/plugin/pkg/auth/authorizer/node/graph_populator.go#L80-L92\r\n\r\nA pod update that adds a new container with a new envFrom referencing a new secret needs to call g.graph.AddPod so the node gets access to that secret\r\n\r\nDo you see authorization errors for that secret in the kubelet log?\r\n\r\n/sig auth', 'created_at': datetime.datetime(2023, 1, 11, 21, 36, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 1379565326, 'issue_id': 1529104822, 'author': 'enj', 'body': '/assign\r\n\r\nSounds fun :)', 'created_at': datetime.datetime(2023, 1, 11, 22, 26, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 1380093827, 'issue_id': 1529104822, 'author': 'AdriaGS', 'body': ""> @AdriaGS is it 100% repro or there should be special conditions to repro this issue?\r\n\r\nThe only special condition I can think of is that I'm executing the go script inside a pod from within the cluster. The pods I am updating (by creating new ephemeral containers) are inside the same cluster."", 'created_at': datetime.datetime(2023, 1, 12, 10, 8, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 1380118869, 'issue_id': 1529104822, 'author': 'AdriaGS', 'body': '> Do you see authorization errors for that secret in the kubelet log?\r\n\r\nI\'m not sure I understood exactly what you\'re asking for. Checking the events for a specific pod I get the following:\r\n\r\nEvents:\r\n```\r\n  Type     Reason     Age   From               Message\r\n  ----     ------     ----  ----               -------\r\n  Normal   Scheduled  75s   default-scheduler  Successfully assigned default/mytarget-7b495fcf6b-pf586 to ip-10-0-222-168.ec2.internal\r\n  Normal   Pulled     75s   kubelet            Container image ""<image>"" already present on machine\r\n  Normal   Created    75s   kubelet            Created container mytarget\r\n  Normal   Started    75s   kubelet            Started container mytarget\r\n  Normal   Pulled     6s    kubelet            Container image ""<image>"" already present on machine\r\n  Warning  Failed     5s    kubelet            Error: failed to sync secret cache: timed out waiting for the condition\r\n```', 'created_at': datetime.datetime(2023, 1, 12, 10, 28, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 1416266089, 'issue_id': 1529104822, 'author': 'liggitt', 'body': '@enj any update on this?', 'created_at': datetime.datetime(2023, 2, 3, 18, 50, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 1474878102, 'issue_id': 1529104822, 'author': 'enj', 'body': '/milestone v1.28\n\nSo I remember to actually fix this :)', 'created_at': datetime.datetime(2023, 3, 18, 15, 25, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 1587709590, 'issue_id': 1529104822, 'author': 'moficodes', 'body': '@enj \r\n\r\n1.28 Bug Triage Shadow here.\r\n\r\nJust checking-in, if this is still on track for the 1.28 release?', 'created_at': datetime.datetime(2023, 6, 12, 16, 55, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 1591717972, 'issue_id': 1529104822, 'author': 'SergeyKanzhelev', 'body': '/triage accepted\r\n/priority important-longterm', 'created_at': datetime.datetime(2023, 6, 14, 17, 35, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 1591718546, 'issue_id': 1529104822, 'author': 'SergeyKanzhelev', 'body': '/remove-triage needs-information', 'created_at': datetime.datetime(2023, 6, 14, 17, 36, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 1625242972, 'issue_id': 1529104822, 'author': 'furkatgofurov7', 'body': ""@AdriaGS @SergeyKanzhelev hi folks, 1.28 Bug Triage Lead here 👋🏼\r\n\r\nI'd like to check what's the status of this issue and if there are plans moving it forward. Please be aware that the code freeze is starting [01:00 UTC Wednesday 19th July 2023 / 18:00 PDT Tuesday 18th July 2023](https://everytimezone.com/s/72ee8496) (about less than 2 weeks from now).\r\n\r\nAs the issue is tagged for 1.28, is it still planned for this release?"", 'created_at': datetime.datetime(2023, 7, 7, 11, 2, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 1625952506, 'issue_id': 1529104822, 'author': 'SergeyKanzhelev', 'body': 'Ping @enj =)', 'created_at': datetime.datetime(2023, 7, 7, 19, 24, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 1632782673, 'issue_id': 1529104822, 'author': 'moficodes', 'body': 'Week 9 update: Code Freeze is approaching!\r\n\r\nHey! We are currently in Week 9, and Code Freeze is just over a week away. It will begin at 01:00 UTC on Wednesday, 19th July 2023 / 18:00 PDT on Tuesday, 18th July 2023. Please ensure all necessary submissions are made before the deadline. Let me know if you need any assistance. :)', 'created_at': datetime.datetime(2023, 7, 12, 15, 44, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 1642077930, 'issue_id': 1529104822, 'author': 'enj', 'body': '> Ping @enj =)\r\n\r\nGot too busy with other stuff 😭\r\n\r\n/milestone next-candidate', 'created_at': datetime.datetime(2023, 7, 19, 13, 24, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 1670499345, 'issue_id': 1529104822, 'author': 'enj', 'body': '/milestone v1.29', 'created_at': datetime.datetime(2023, 8, 9, 0, 50, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 1765117384, 'issue_id': 1529104822, 'author': 'moficodes', 'body': ""Hello @enj ! 👋🏼\r\nBug triage for the 1.29 release cycle is here!\r\nThis issue hasn't been updated for a long time, so I wanted to check what the status is. The code freeze will start ([01:00 UTC Wednesday 1st November 2023 / 18:00 PDT Tuesday 31st October 2023](https://everytimezone.com/s/3b849751)), which is about 2 weeks from now. And while there is still plenty of time, we want to make sure that every PR has a chance to be merged on time.\r\n\r\nAs the issue is tagged for 1.29, is it still planned for that release?"", 'created_at': datetime.datetime(2023, 10, 16, 19, 10, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 1784375821, 'issue_id': 1529104822, 'author': 'moficodes', 'body': ""Hello @enj ! 👋🏼\r\nBug triage for the 1.29 release cycle is here!\r\nThis issue hasn't been updated for a long time, so I wanted to check what the status is. The code freeze will start ([01:00 UTC Wednesday 1st November 2023 / 18:00 PDT Tuesday 31st October 2023](https://everytimezone.com/s/3b849751)), which is this week. We want to make sure that every PR has a chance to be merged on time.\r\n\r\nAs the issue is tagged for 1.29, is it still planned for that release?"", 'created_at': datetime.datetime(2023, 10, 30, 2, 23, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 1808256231, 'issue_id': 1529104822, 'author': 'dims', 'body': '/milestone v1.30\r\n\r\nkicking the can, we are way past code freeze', 'created_at': datetime.datetime(2023, 11, 13, 14, 21, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 1959579817, 'issue_id': 1529104822, 'author': 'SubhasmitaSw', 'body': ""Hi there! @enj @dims  👋🏼 \r\nRelease Signal shadow here.\r\nThis issue has not been updated for a long time, so I'd like to check the status. The code freeze is starting [02:00 UTC Wednesday 6th March 2024 / 18:00 PDT Tuesday 5th March 2024](https://everytimezone.com/s/5056bace) (less than 2 weeks from now), and while there is still some time, we want to ensure that each PR has a chance to be merged on time. \r\n\r\nIs this issue still intended for this release, given that it is labeled for 1.30?"", 'created_at': datetime.datetime(2024, 2, 22, 14, 34, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 1961613069, 'issue_id': 1529104822, 'author': 'shiftyp', 'body': ""Hi @enj. Any chance you're not actively working on this and could use some help knocking this one out? I'd like to take a look if that's ok."", 'created_at': datetime.datetime(2024, 2, 23, 16, 16, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 1972882818, 'issue_id': 1529104822, 'author': 'SubhasmitaSw', 'body': ""Hello!\r\nRelease Signal shadow here.\r\nThis issue has not been updated for a long time, so I'd like to check what's the status. The code freeze is starting [02:00 UTC Wednesday 6th March 2024 / 18:00 PDT Tuesday 5th March 2024](https://everytimezone.com/s/5056bace) in a week, and we want to ensure that each PR has a chance to be merged on time.\r\n\r\nAs this issue is tagged for 1.30, is it still planned for this release?\r\n\r\nPlease let us know the status if possible."", 'created_at': datetime.datetime(2024, 3, 1, 10, 2, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 1987801589, 'issue_id': 1529104822, 'author': 'Vyom-Yadav', 'body': 'We are past the code freeze stage. Moving this to the next release.\n\n/milestone v1.31', 'created_at': datetime.datetime(2024, 3, 11, 7, 46, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2166053524, 'issue_id': 1529104822, 'author': 'Atharva-Shinde', 'body': ""Hello @AdriaGS! This issue has not been updated for a long time, so I'd like to check what's the status. The code freeze is starting 02:00 UTC Wednesday 10th July 2024 (about 4 weeks from now), and while there is still plenty of time, we want to ensure that each PR has a chance to be merged on time.\r\n\r\nAs the issue is tagged for 1.31, is it still planned for this release?"", 'created_at': datetime.datetime(2024, 6, 13, 15, 45, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2234788526, 'issue_id': 1529104822, 'author': 'drewhagen', 'body': ""Hello @AdriaGS !\n\nThis issue hasn't been updated in a while. What's the current status?\n\nReminder: Code freeze starts `02:00 UTC Wednesday 24th July 2024 / 19:00 PDT Tuesday 23rd July 2024` (less than **1 week** from now). Please make sure the related PR has both `lgtm` and `approved` labels before the code freeze.\n\nThanks!"", 'created_at': datetime.datetime(2024, 7, 17, 23, 54, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2246444473, 'issue_id': 1529104822, 'author': 'enj', 'body': '/milestone v1.32', 'created_at': datetime.datetime(2024, 7, 23, 23, 0, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2417314408, 'issue_id': 1529104822, 'author': 'drewhagen', 'body': ""Hello @AdriaGS @enj!\n\nThis issue has not been updated for a long time, so I'd like to check what's the status. The code freeze is starting `02:00 UTC Friday November 8th 2024` (about **3 weeks** from now), and while there is still time, we want to ensure that each PR has a chance to be merged on time.\nAs the issue is tagged for `1.32`, is it still planned for this release?\nThanks!"", 'created_at': datetime.datetime(2024, 10, 16, 16, 22, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2418220394, 'issue_id': 1529104822, 'author': 'SergeyKanzhelev', 'body': ""/milestone clear\r\n\r\n@enj are you still up for this?\r\n\r\nLet's not use the milestone for something actually blocking the release. This bug is not a regression. Even though it is nice to fix, we should not play the game of pushing the milestone every release."", 'created_at': datetime.datetime(2024, 10, 17, 0, 26, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2575939622, 'issue_id': 1529104822, 'author': 'cslink', 'body': 'I can take a look at this.', 'created_at': datetime.datetime(2025, 1, 7, 18, 12, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2575940068, 'issue_id': 1529104822, 'author': 'liggitt', 'body': 'thanks\r\n\r\n/unassign @enj\r\n/assign @cslink', 'created_at': datetime.datetime(2025, 1, 7, 18, 13, 4, tzinfo=datetime.timezone.utc)}]","pacoxu on (2023-01-11 14:07:19 UTC): /sig node

SergeyKanzhelev on (2023-01-11 18:59:56 UTC): Just a wild guess, can this be related to this bug: https://github.com/kubernetes/kubernetes/issues/114167?

SergeyKanzhelev on (2023-01-11 19:00:37 UTC): @AdriaGS is it 100% repro or there should be special conditions to repro this issue?

/triage needs-information

liggitt on (2023-01-11 21:36:02 UTC): I think the issue is the node authorizer not considering that pod updates might add a new secret reference

https://github.com/kubernetes/kubernetes/blob/97bbf07d3f3f20332912ee411fdf75ce84425e28/plugin/pkg/auth/authorizer/node/graph_populator.go#L80-L92

A pod update that adds a new container with a new envFrom referencing a new secret needs to call g.graph.AddPod so the node gets access to that secret

Do you see authorization errors for that secret in the kubelet log?

/sig auth

enj on (2023-01-11 22:26:59 UTC): /assign

Sounds fun :)

AdriaGS (Issue Creator) on (2023-01-12 10:08:26 UTC): The only special condition I can think of is that I'm executing the go script inside a pod from within the cluster. The pods I am updating (by creating new ephemeral containers) are inside the same cluster.

AdriaGS (Issue Creator) on (2023-01-12 10:28:03 UTC): I'm not sure I understood exactly what you're asking for. Checking the events for a specific pod I get the following:

Events:
```
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  75s   default-scheduler  Successfully assigned default/mytarget-7b495fcf6b-pf586 to ip-10-0-222-168.ec2.internal
  Normal   Pulled     75s   kubelet            Container image ""<image>"" already present on machine
  Normal   Created    75s   kubelet            Created container mytarget
  Normal   Started    75s   kubelet            Started container mytarget
  Normal   Pulled     6s    kubelet            Container image ""<image>"" already present on machine
  Warning  Failed     5s    kubelet            Error: failed to sync secret cache: timed out waiting for the condition
```

liggitt on (2023-02-03 18:50:22 UTC): @enj any update on this?

enj on (2023-03-18 15:25:11 UTC): /milestone v1.28

So I remember to actually fix this :)

moficodes on (2023-06-12 16:55:30 UTC): @enj 

1.28 Bug Triage Shadow here.

Just checking-in, if this is still on track for the 1.28 release?

SergeyKanzhelev on (2023-06-14 17:35:59 UTC): /triage accepted
/priority important-longterm

SergeyKanzhelev on (2023-06-14 17:36:27 UTC): /remove-triage needs-information

furkatgofurov7 on (2023-07-07 11:02:07 UTC): @AdriaGS @SergeyKanzhelev hi folks, 1.28 Bug Triage Lead here 👋🏼

I'd like to check what's the status of this issue and if there are plans moving it forward. Please be aware that the code freeze is starting [01:00 UTC Wednesday 19th July 2023 / 18:00 PDT Tuesday 18th July 2023](https://everytimezone.com/s/72ee8496) (about less than 2 weeks from now).

As the issue is tagged for 1.28, is it still planned for this release?

SergeyKanzhelev on (2023-07-07 19:24:58 UTC): Ping @enj =)

moficodes on (2023-07-12 15:44:57 UTC): Week 9 update: Code Freeze is approaching!

Hey! We are currently in Week 9, and Code Freeze is just over a week away. It will begin at 01:00 UTC on Wednesday, 19th July 2023 / 18:00 PDT on Tuesday, 18th July 2023. Please ensure all necessary submissions are made before the deadline. Let me know if you need any assistance. :)

enj on (2023-07-19 13:24:18 UTC): Got too busy with other stuff 😭

/milestone next-candidate

enj on (2023-08-09 00:50:09 UTC): /milestone v1.29

moficodes on (2023-10-16 19:10:30 UTC): Hello @enj ! 👋🏼
Bug triage for the 1.29 release cycle is here!
This issue hasn't been updated for a long time, so I wanted to check what the status is. The code freeze will start ([01:00 UTC Wednesday 1st November 2023 / 18:00 PDT Tuesday 31st October 2023](https://everytimezone.com/s/3b849751)), which is about 2 weeks from now. And while there is still plenty of time, we want to make sure that every PR has a chance to be merged on time.

As the issue is tagged for 1.29, is it still planned for that release?

moficodes on (2023-10-30 02:23:18 UTC): Hello @enj ! 👋🏼
Bug triage for the 1.29 release cycle is here!
This issue hasn't been updated for a long time, so I wanted to check what the status is. The code freeze will start ([01:00 UTC Wednesday 1st November 2023 / 18:00 PDT Tuesday 31st October 2023](https://everytimezone.com/s/3b849751)), which is this week. We want to make sure that every PR has a chance to be merged on time.

As the issue is tagged for 1.29, is it still planned for that release?

dims on (2023-11-13 14:21:39 UTC): /milestone v1.30

kicking the can, we are way past code freeze

SubhasmitaSw on (2024-02-22 14:34:39 UTC): Hi there! @enj @dims  👋🏼 
Release Signal shadow here.
This issue has not been updated for a long time, so I'd like to check the status. The code freeze is starting [02:00 UTC Wednesday 6th March 2024 / 18:00 PDT Tuesday 5th March 2024](https://everytimezone.com/s/5056bace) (less than 2 weeks from now), and while there is still some time, we want to ensure that each PR has a chance to be merged on time. 

Is this issue still intended for this release, given that it is labeled for 1.30?

shiftyp on (2024-02-23 16:16:02 UTC): Hi @enj. Any chance you're not actively working on this and could use some help knocking this one out? I'd like to take a look if that's ok.

SubhasmitaSw on (2024-03-01 10:02:14 UTC): Hello!
Release Signal shadow here.
This issue has not been updated for a long time, so I'd like to check what's the status. The code freeze is starting [02:00 UTC Wednesday 6th March 2024 / 18:00 PDT Tuesday 5th March 2024](https://everytimezone.com/s/5056bace) in a week, and we want to ensure that each PR has a chance to be merged on time.

As this issue is tagged for 1.30, is it still planned for this release?

Please let us know the status if possible.

Vyom-Yadav on (2024-03-11 07:46:33 UTC): We are past the code freeze stage. Moving this to the next release.

/milestone v1.31

Atharva-Shinde on (2024-06-13 15:45:27 UTC): Hello @AdriaGS! This issue has not been updated for a long time, so I'd like to check what's the status. The code freeze is starting 02:00 UTC Wednesday 10th July 2024 (about 4 weeks from now), and while there is still plenty of time, we want to ensure that each PR has a chance to be merged on time.

As the issue is tagged for 1.31, is it still planned for this release?

drewhagen on (2024-07-17 23:54:15 UTC): Hello @AdriaGS !

This issue hasn't been updated in a while. What's the current status?

Reminder: Code freeze starts `02:00 UTC Wednesday 24th July 2024 / 19:00 PDT Tuesday 23rd July 2024` (less than **1 week** from now). Please make sure the related PR has both `lgtm` and `approved` labels before the code freeze.

Thanks!

enj on (2024-07-23 23:00:59 UTC): /milestone v1.32

drewhagen on (2024-10-16 16:22:37 UTC): Hello @AdriaGS @enj!

This issue has not been updated for a long time, so I'd like to check what's the status. The code freeze is starting `02:00 UTC Friday November 8th 2024` (about **3 weeks** from now), and while there is still time, we want to ensure that each PR has a chance to be merged on time.
As the issue is tagged for `1.32`, is it still planned for this release?
Thanks!

SergeyKanzhelev on (2024-10-17 00:26:21 UTC): /milestone clear

@enj are you still up for this?

Let's not use the milestone for something actually blocking the release. This bug is not a regression. Even though it is nice to fix, we should not play the game of pushing the milestone every release.

cslink (Assginee) on (2025-01-07 18:12:48 UTC): I can take a look at this.

liggitt on (2025-01-07 18:13:04 UTC): thanks

/unassign @enj
/assign @cslink

"
1528621552,issue,closed,completed,"kubeadm should not pull images for skipped addon phases (coredns, kube-proxy)","### What happened?

- Cluster created with custom CoreDNS installation, i.e. `skipPhases: [ addon/coredns ]` specified in kubeadm init configuration
- When joining a new node using `kubeadm join`, kubeadm still tries to pull all images
- If that image pull fails, kubeadm bails out despite the image not being required. See https://github.com/kubernetes/kubernetes/pull/114978 for how the CoreDNS image pull is special and could therefore fail.

### What did you expect to happen?

No image pull attempt for images belonging to skipped phases.

### How can we reproduce it (as minimally and precisely as possible)?

Not needed. The current functionality is very clear – see `GetControlPlaneImages` in code:

```
// GetControlPlaneImages returns a list of container images kubeadm expects to use on a control plane node
func GetControlPlaneImages(cfg *kubeadmapi.ClusterConfiguration) []string {
	images := make([]string, 0)

	// start with core kubernetes images
	images = append(images, GetKubernetesImage(constants.KubeAPIServer, cfg))
	images = append(images, GetKubernetesImage(constants.KubeControllerManager, cfg))
	images = append(images, GetKubernetesImage(constants.KubeScheduler, cfg))
	images = append(images, GetKubernetesImage(constants.KubeProxy, cfg))

	// pause is not available on the ci image repository so use the default image repository.
	images = append(images, GetPauseImage(cfg))

	// if etcd is not external then add the image as it will be required
	if cfg.Etcd.Local != nil {
		images = append(images, GetEtcdImage(cfg))
	}

	// Append the appropriate DNS images
	images = append(images, GetDNSImage(cfg))

	return images
}
```

### Anything else we need to know?

This also reduces unnecessary network throughput and is therefore in line with [KEP 3000](https://github.com/kubernetes/enhancements/tree/master/keps/sig-release/3000-artifact-distribution) goals.

https://github.com/kubernetes/kubeadm/issues/1842 is quite similar.

### Kubernetes version

`master`

### Cloud provider

any

### OS version

_No response_

### Install tools

_No response_

### Container runtime (CRI) and version (if applicable)

_No response_

### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_",AndiDog,2023-01-11 08:23:53+00:00,[],2023-01-11 08:33:54+00:00,2023-01-11 08:33:53+00:00,https://github.com/kubernetes/kubernetes/issues/114979,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('needs-sig', 'Indicates an issue or PR lacks a `sig/foo` label and requires one.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1378389897, 'issue_id': 1528621552, 'author': 'k8s-ci-robot', 'body': '@AndiDog: There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `/sig <group-name>`\n- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 11, 8, 24, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 1378389926, 'issue_id': 1528621552, 'author': 'k8s-ci-robot', 'body': '@AndiDog: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 11, 8, 24, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 1378400290, 'issue_id': 1528621552, 'author': 'neolit123', 'body': 'duplicate of\r\nhttps://github.com/kubernetes/kubeadm/issues/2603\r\n\r\n/close', 'created_at': datetime.datetime(2023, 1, 11, 8, 33, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 1378400383, 'issue_id': 1528621552, 'author': 'k8s-ci-robot', 'body': '@neolit123: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114979#issuecomment-1378400290):\n\n>duplicate of\r\n>https://github.com/kubernetes/kubeadm/issues/2603\r\n>\r\n>/close\r\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 11, 8, 33, 54, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-11 08:24:01 UTC): @AndiDog: There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:
- `/sig <group-name>`
- `/wg <group-name>`
- `/committee <group-name>`

Please see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

k8s-ci-robot on (2023-01-11 08:24:03 UTC): @AndiDog: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

neolit123 on (2023-01-11 08:33:49 UTC): duplicate of
https://github.com/kubernetes/kubeadm/issues/2603

/close

k8s-ci-robot on (2023-01-11 08:33:54 UTC): @neolit123: Closing this issue.

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114979#issuecomment-1378400290):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

"
1528037378,issue,open,,Failure cluster [6140da15...] e2e Flake: Finished pod is deleted before events can be retrieved,"### Failure cluster [6140da152b810b30d99b](https://go.k8s.io/triage#6140da152b810b30d99b)

##### Error text:
```
[FAILED] timed out waiting for the condition
In [It] at: test/e2e/node/pods.go:276 @ 01/02/23 10:40:40.11

```
#### Recent failures:
[1/9/2023, 3:08:19 PM pr:pull-kubernetes-e2e-kind](https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/114879/pull-kubernetes-e2e-kind/1612541338065047552)
[1/9/2023, 12:37:26 PM pr:pull-kubernetes-e2e-kind-ipv6](https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/batch/pull-kubernetes-e2e-kind-ipv6/1612502921440661504)
[1/9/2023, 9:57:40 AM pr:pull-kubernetes-e2e-kind-ipv6](https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/114846/pull-kubernetes-e2e-kind-ipv6/1612463069030518784)
[1/6/2023, 6:35:47 PM pr:pull-kubernetes-e2e-kind-ipv6](https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/113744/pull-kubernetes-e2e-kind-ipv6/1611506307183939584)
[1/6/2023, 4:48:42 PM pr:pull-kubernetes-e2e-kind-ipv6](https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/114883/pull-kubernetes-e2e-kind-ipv6/1611479334852235264)


/kind failing-test
/kind flake
<!-- If this is a flake, please add: /kind flake -->

/sig node

#### Additional Information

It looks like the [test](https://github.com/kubernetes/kubernetes/blob/2f6c4f5eab85d3f15cd80d21f4a0c353a8ceb10b/test/e2e/node/pods.go#L232) does the following:
1. [Creates a pod that has a container that immediately exits (command is `/bin/true`)](https://github.com/kubernetes/kubernetes/blob/2f6c4f5eab85d3f15cd80d21f4a0c353a8ceb10b/test/e2e/node/pods.go#L237-L265)
2. [Confirms that the pod has completed successfully](https://github.com/kubernetes/kubernetes/blob/2f6c4f5eab85d3f15cd80d21f4a0c353a8ceb10b/test/e2e/node/pods.go#L271)
3. [Attempts to read the events for the pod](https://github.com/kubernetes/kubernetes/blob/2f6c4f5eab85d3f15cd80d21f4a0c353a8ceb10b/test/e2e/node/pods.go#L276-L292)

When this test flakes/fails, it seems like steps 1 and 2 are successful, but it times out on step 3 because apparently the pod no longer exists.

This makes me wonder if, under certain load conditions, the pod GC is cleaning up the terminated pod before step 3 (or even step 2) can do its thing.

It [looks like the default value for `--terminated-pod-gc-threshold` is 12500](https://github.com/kubernetes/kubernetes/blob/e01ff1641c7321ac81fe5775f6ccb21aa6775c04/pkg/controller/podgc/config/v1alpha1/defaults.go#L34),

That seems high enough that it would be unlikely that the threshold would be reached (could there really be 12500 completed pods?), but I wonder if something in the e2e tests is lowering it.  I noticed [this line which sets `TERMINATED_POD_GC_THRESHOLD` to 100](https://github.com/kubernetes/kubernetes/blob/545835e28826d888b3d236f71fbbd88107d3b635/cluster/gce/config-test.sh#L163), but that is gce-specific.  Is it possible something similar is being set to lower the threshold for the e2e-kind tests?  I did a search but didn't see anywhere that it was being set.



",brianpursley,2023-01-10 21:33:43+00:00,['esotsal'],2025-01-21 16:44:56+00:00,,https://github.com/kubernetes/kubernetes/issues/114971,"[('priority/important-soon', 'Must be staffed and worked on either currently, or very soon, ideally in time for the next release.'), ('sig/node', 'Categorizes an issue or PR as relevant to SIG Node.'), ('kind/flake', 'Categorizes issue or PR as related to a flaky test.'), ('kind/failing-test', 'Categorizes issue or PR as related to a consistently or frequently failing test.'), ('triage/accepted', 'Indicates an issue or PR is ready to be actively worked on.')]","[{'comment_id': 1377958562, 'issue_id': 1528037378, 'author': 'aojea', 'body': '/cc @aojea @BenTheElder \r\n\r\ninteresting 👀 , we should not have tests that depends on Events , they are already documented to be best effort\r\n\r\nThe test must be changed to assert in something different, this test will be always be flaky if rely on events', 'created_at': datetime.datetime(2023, 1, 10, 22, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 1379312421, 'issue_id': 1528037378, 'author': 'SergeyKanzhelev', 'body': '/triage accepted\r\n/priority important-soon', 'created_at': datetime.datetime(2023, 1, 11, 18, 28, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 1398158116, 'issue_id': 1528037378, 'author': 'aojea', 'body': 'Events are not guaranteed to be delivered\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/2f6c4f5eab85d3f15cd80d21f4a0c353a8ceb10b/test/e2e/node/pods.go#L276-L292\r\n\r\nwe should have another way of asserting , this is always going to be flaky', 'created_at': datetime.datetime(2023, 1, 20, 9, 58, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 1404393338, 'issue_id': 1528037378, 'author': 'brianpursley', 'body': ""Agreed, it doesn't sound like events are meant to be used for something that requires 100% reliability.\r\n\r\nThe test in question is called `should not create extra sandbox if all containers are done`.  Is there another way for an e2e test to know whether an extra sandbox was created?\r\n\r\nShould this even be an e2e test, or would it be better as a unit test in kubelet (`SyncPod()` or `computePodActions()`)?\r\n\r\nThere must be some reason this test was created in the first place.  A regression maybe?\r\n\r\nWithout context, it doesn't seem particularly useful. It seems obvious that an extra sandbox should not be created.  Is this even something we need to check?  It must have happened at some point I guess."", 'created_at': datetime.datetime(2023, 1, 26, 0, 19, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 1404394169, 'issue_id': 1528037378, 'author': 'brianpursley', 'body': 'Related Issues:\r\n* https://github.com/kubernetes/kubernetes/issues/106904\r\n* https://github.com/kubernetes/kubernetes/issues/109118\r\n* https://github.com/kubernetes/kubernetes/issues/75441 (Possibly)', 'created_at': datetime.datetime(2023, 1, 26, 0, 21, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 1522812982, 'issue_id': 1528037378, 'author': 'k8s-triage-robot', 'body': 'This issue is labeled with `priority/important-soon` but has not been updated in over 90 days, and should be re-triaged.\nImportant-soon issues must be staffed and worked on either currently, or very soon, ideally in time for the next release.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Deprioritize it with `/priority important-longterm` or `/priority backlog`\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted', 'created_at': datetime.datetime(2023, 4, 26, 5, 36, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 1899661052, 'issue_id': 1528037378, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2024, 1, 19, 3, 59, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 1950962148, 'issue_id': 1528037378, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2024, 2, 18, 4, 56, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2329602696, 'issue_id': 1528037378, 'author': 'AnishShah', 'body': '/cc @esotsal', 'created_at': datetime.datetime(2024, 9, 4, 17, 19, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2330797519, 'issue_id': 1528037378, 'author': 'esotsal', 'body': '/assign', 'created_at': datetime.datetime(2024, 9, 5, 7, 24, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2330802425, 'issue_id': 1528037378, 'author': 'esotsal', 'body': 'Will give it a try to update this test to not depend on Events as suggested by @aojea and @brianpursley.', 'created_at': datetime.datetime(2024, 9, 5, 7, 27, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2334796098, 'issue_id': 1528037378, 'author': 'SergeyKanzhelev', 'body': '/remove-lifecycle rotten', 'created_at': datetime.datetime(2024, 9, 6, 20, 54, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2334796901, 'issue_id': 1528037378, 'author': 'SergeyKanzhelev', 'body': '/triage accepted', 'created_at': datetime.datetime(2024, 9, 6, 20, 55, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2498432045, 'issue_id': 1528037378, 'author': 'dims', 'body': '@esotsal is https://github.com/kubernetes/kubernetes/issues/127172 supposed to help fix this issue?', 'created_at': datetime.datetime(2024, 11, 25, 16, 6, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2498481684, 'issue_id': 1528037378, 'author': 'esotsal', 'body': '> @esotsal is #127172 supposed to help fix this issue?\r\n\r\nI think #128880 DeleteSync parts would help. I propose first to merge #128889 , then rebase  #128880 , remove the windows part from 128880 ( which has its own PR #128936 ) and update the PR description.', 'created_at': datetime.datetime(2024, 11, 25, 16, 26, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2498553411, 'issue_id': 1528037378, 'author': 'esotsal', 'body': '> > @esotsal is #127172 supposed to help fix this issue?\r\n> \r\n> I think #128880 DeleteSync parts would help. I propose first to merge #128889 , then rebase #128880 , remove the windows part from 128880 ( which has its own PR #128936 ) and update the PR description.\r\n\r\nOh sorry , messed up the issues. No @dims , I mentioned this issue in #127172 as an example why we should avoid events. This PR is not related with 128880. I have this in my todo to update the related tests for this issue.', 'created_at': datetime.datetime(2024, 11, 25, 16, 57, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2498903170, 'issue_id': 1528037378, 'author': 'dims', 'body': '@esotsal thanks for the context!', 'created_at': datetime.datetime(2024, 11, 25, 19, 52, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2605240219, 'issue_id': 1528037378, 'author': 'esotsal', 'body': 'Update, will focus on this issue after #129717 is merged.', 'created_at': datetime.datetime(2025, 1, 21, 16, 44, 55, tzinfo=datetime.timezone.utc)}]","aojea on (2023-01-10 22:09:00 UTC): /cc @aojea @BenTheElder 

interesting 👀 , we should not have tests that depends on Events , they are already documented to be best effort

The test must be changed to assert in something different, this test will be always be flaky if rely on events

SergeyKanzhelev on (2023-01-11 18:28:22 UTC): /triage accepted
/priority important-soon

aojea on (2023-01-20 09:58:21 UTC): Events are not guaranteed to be delivered

https://github.com/kubernetes/kubernetes/blob/2f6c4f5eab85d3f15cd80d21f4a0c353a8ceb10b/test/e2e/node/pods.go#L276-L292

we should have another way of asserting , this is always going to be flaky

brianpursley (Issue Creator) on (2023-01-26 00:19:54 UTC): Agreed, it doesn't sound like events are meant to be used for something that requires 100% reliability.

The test in question is called `should not create extra sandbox if all containers are done`.  Is there another way for an e2e test to know whether an extra sandbox was created?

Should this even be an e2e test, or would it be better as a unit test in kubelet (`SyncPod()` or `computePodActions()`)?

There must be some reason this test was created in the first place.  A regression maybe?

Without context, it doesn't seem particularly useful. It seems obvious that an extra sandbox should not be created.  Is this even something we need to check?  It must have happened at some point I guess.

brianpursley (Issue Creator) on (2023-01-26 00:21:01 UTC): Related Issues:
* https://github.com/kubernetes/kubernetes/issues/106904
* https://github.com/kubernetes/kubernetes/issues/109118
* https://github.com/kubernetes/kubernetes/issues/75441 (Possibly)

k8s-triage-robot on (2023-04-26 05:36:46 UTC): This issue is labeled with `priority/important-soon` but has not been updated in over 90 days, and should be re-triaged.
Important-soon issues must be staffed and worked on either currently, or very soon, ideally in time for the next release.

You can:
- Confirm that this issue is still relevant with `/triage accepted` (org members only)
- Deprioritize it with `/priority important-longterm` or `/priority backlog`
- Close this issue with `/close`

For more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/

/remove-triage accepted

k8s-triage-robot on (2024-01-19 03:59:58 UTC): The Kubernetes project currently lacks enough contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle stale`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle stale

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-triage-robot on (2024-02-18 04:56:10 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle rotten

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

AnishShah on (2024-09-04 17:19:37 UTC): /cc @esotsal

esotsal (Assginee) on (2024-09-05 07:24:32 UTC): /assign

esotsal (Assginee) on (2024-09-05 07:27:19 UTC): Will give it a try to update this test to not depend on Events as suggested by @aojea and @brianpursley.

SergeyKanzhelev on (2024-09-06 20:54:52 UTC): /remove-lifecycle rotten

SergeyKanzhelev on (2024-09-06 20:55:32 UTC): /triage accepted

dims on (2024-11-25 16:06:26 UTC): @esotsal is https://github.com/kubernetes/kubernetes/issues/127172 supposed to help fix this issue?

esotsal (Assginee) on (2024-11-25 16:26:08 UTC): I think #128880 DeleteSync parts would help. I propose first to merge #128889 , then rebase  #128880 , remove the windows part from 128880 ( which has its own PR #128936 ) and update the PR description.

esotsal (Assginee) on (2024-11-25 16:57:32 UTC): Oh sorry , messed up the issues. No @dims , I mentioned this issue in #127172 as an example why we should avoid events. This PR is not related with 128880. I have this in my todo to update the related tests for this issue.

dims on (2024-11-25 19:52:12 UTC): @esotsal thanks for the context!

esotsal (Assginee) on (2025-01-21 16:44:55 UTC): Update, will focus on this issue after #129717 is merged.

"
1527864461,issue,closed,completed,Python2 still in use,"### What happened?

python2 is long deprecated, everything should be on python3, but that doesn't appear to be the case in this repo.

/sig testing",BenTheElder,2023-01-10 18:59:13+00:00,['AxeZhan'],2023-01-19 15:24:51+00:00,2023-01-19 15:24:51+00:00,https://github.com/kubernetes/kubernetes/issues/114967,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('priority/important-soon', 'Must be staffed and worked on either currently, or very soon, ideally in time for the next release.'), ('sig/testing', 'Categorizes an issue or PR as relevant to SIG Testing.'), ('triage/accepted', 'Indicates an issue or PR is ready to be actively worked on.')]","[{'comment_id': 1377719117, 'issue_id': 1527864461, 'author': 'aojea', 'body': '/cc', 'created_at': datetime.datetime(2023, 1, 10, 19, 5, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 1377991299, 'issue_id': 1527864461, 'author': 'aroradaman', 'body': ""Hi !\r\nI'm new to the community, can I pick this issue?\r\n\r\nSo far have found the following files using python:\r\n\r\n- https://github.com/kubernetes/test-infra/blob/master/gubernator/filters_test.py\r\n- https://github.com/kubernetes/test-infra/blob/master/gubernator/gcs_async_test.py\r\n- https://github.com/kubernetes/test-infra/blob/master/gubernator/pull_request.py\r\n- https://github.com/kubernetes/test-infra/blob/master/gubernator/view_build.py\r\n- https://github.com/kubernetes/test-infra/blob/master/gubernator/view_pr.py\r\n- https://github.com/kubernetes/test-infra/blob/master/gubernator/view_pr_test.py\r\n- https://github.com/kubernetes/test-infra/blob/master/gubernator/github/main_test.py\r\n- https://github.com/kubernetes/test-infra/blob/master/gubernator/github/models.py\r\n- https://github.com/kubernetes/test-infra/blob/master/gubernator/github/periodic_sync_test.py\r\n- https://github.com/kubernetes/test-infra/blob/master/gubernator/third_party/cloudstorage/common.py\r\n- https://github.com/kubernetes/test-infra/blob/master/gubernator/third_party/cloudstorage/storage_api.py\r\n- https://github.com/kubernetes/test-infra/blob/master/scenarios/kubernetes_janitor.py"", 'created_at': datetime.datetime(2023, 1, 10, 22, 43, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 1378049159, 'issue_id': 1527864461, 'author': 'BenTheElder', 'body': ""This issue is intended to be scoped to this repo specifically, not test-infra. test-infra has tracked in it's own issues https://github.com/kubernetes/test-infra/issues?q=is%3Aissue+python3+is%3Aclosed\r\n\r\nThere are files in this repo that need updating, and anyone is welcome to work on this."", 'created_at': datetime.datetime(2023, 1, 10, 23, 53, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 1378210235, 'issue_id': 1527864461, 'author': 'AxeZhan', 'body': 'I think python files are mainly used in hack. I can help with this one.\r\n\r\n```\r\nfind ./ -name ""*.py""                 \r\n.//staging/src/k8s.io/kubectl/pkg/util/i18n/translations/extract.py\r\n.//hack/boilerplate/test/pass.py\r\n.//hack/boilerplate/test/fail.py\r\n.//hack/boilerplate/boilerplate.py\r\n.//hack/boilerplate/boilerplate_test.py\r\n.//hack/verify-flags-underscore.py\r\n.//hack/verify-publishing-bot.py\r\n```\r\n\r\n/assign', 'created_at': datetime.datetime(2023, 1, 11, 3, 58, 12, tzinfo=datetime.timezone.utc)}]","aojea on (2023-01-10 19:05:29 UTC): /cc

aroradaman on (2023-01-10 22:43:02 UTC): Hi !
I'm new to the community, can I pick this issue?

So far have found the following files using python:

- https://github.com/kubernetes/test-infra/blob/master/gubernator/filters_test.py
- https://github.com/kubernetes/test-infra/blob/master/gubernator/gcs_async_test.py
- https://github.com/kubernetes/test-infra/blob/master/gubernator/pull_request.py
- https://github.com/kubernetes/test-infra/blob/master/gubernator/view_build.py
- https://github.com/kubernetes/test-infra/blob/master/gubernator/view_pr.py
- https://github.com/kubernetes/test-infra/blob/master/gubernator/view_pr_test.py
- https://github.com/kubernetes/test-infra/blob/master/gubernator/github/main_test.py
- https://github.com/kubernetes/test-infra/blob/master/gubernator/github/models.py
- https://github.com/kubernetes/test-infra/blob/master/gubernator/github/periodic_sync_test.py
- https://github.com/kubernetes/test-infra/blob/master/gubernator/third_party/cloudstorage/common.py
- https://github.com/kubernetes/test-infra/blob/master/gubernator/third_party/cloudstorage/storage_api.py
- https://github.com/kubernetes/test-infra/blob/master/scenarios/kubernetes_janitor.py

BenTheElder (Issue Creator) on (2023-01-10 23:53:28 UTC): This issue is intended to be scoped to this repo specifically, not test-infra. test-infra has tracked in it's own issues https://github.com/kubernetes/test-infra/issues?q=is%3Aissue+python3+is%3Aclosed

There are files in this repo that need updating, and anyone is welcome to work on this.

AxeZhan (Assginee) on (2023-01-11 03:58:12 UTC): I think python files are mainly used in hack. I can help with this one.

```
find ./ -name ""*.py""                 
.//staging/src/k8s.io/kubectl/pkg/util/i18n/translations/extract.py
.//hack/boilerplate/test/pass.py
.//hack/boilerplate/test/fail.py
.//hack/boilerplate/boilerplate.py
.//hack/boilerplate/boilerplate_test.py
.//hack/verify-flags-underscore.py
.//hack/verify-publishing-bot.py
```

/assign

"
1527725181,issue,closed,completed,CR conversion needs to protect against a converter that mutates the input list,"### What happened?

If a CR converter is wired in and it modifies the input list, the delegating CR converter does not protect against this.

### What did you expect to happen?

A converter cannot effect the input list

### How can we reproduce it (as minimally and precisely as possible)?

Not really possible externally - it's a coding issue that the current converters don't hit.

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
master```

</details>


### Cloud provider

<details>
N/A
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",ncdc,2023-01-10 17:00:56+00:00,['ncdc'],2023-01-31 21:20:31+00:00,2023-01-10 20:05:38+00:00,https://github.com/kubernetes/kubernetes/issues/114958,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('sig/api-machinery', 'Categorizes an issue or PR as relevant to SIG API Machinery.'), ('triage/accepted', 'Indicates an issue or PR is ready to be actively worked on.')]","[{'comment_id': 1377572538, 'issue_id': 1527725181, 'author': 'ncdc', 'body': '/sig api-machinery\r\n/assign', 'created_at': datetime.datetime(2023, 1, 10, 17, 1, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 1411085289, 'issue_id': 1527725181, 'author': 'cici37', 'body': '/triage accepted', 'created_at': datetime.datetime(2023, 1, 31, 21, 20, 26, tzinfo=datetime.timezone.utc)}]","ncdc (Issue Creator) on (2023-01-10 17:01:03 UTC): /sig api-machinery
/assign

cici37 on (2023-01-31 21:20:26 UTC): /triage accepted

"
1527712931,issue,closed,completed,The CRI v1 implementation of validateServiceConnection does not check for unavailable,"### What happened?

It ""connects"" to dockershim
```
DEBU[0000] Connected successfully using endpoint: unix:///var/run/dockershim.sock 
DEBU[0000] VersionRequest: &VersionRequest{Version:v1,} 
E1213 20:22:57.677536   53350 remote_runtime.go:145] ""Version from runtime service failed"" err=""rpc error: code = Unavailable desc = connection error: desc = \""transport: Error while dialing dial unix /var/run/dockershim.sock: connect: no such file or directory\""""
DEBU[0000] VersionResponse: nil                         
```

### What did you expect to happen?

There is no dockershim.sock
```
ERRO[0000] validate service connection: CRI v1 runtime API is not available for endpoint ""unix:///var/run/dockershim.sock"": rpc error: code = Unavailable desc = connection error: desc = ""transport: Error while dialing dial unix /var/run/dockershim.sock: connect: no such file or directory"" 
```

### How can we reproduce it (as minimally and precisely as possible)?

Run crictl v1.26.0, without setting up /etc/crictl.yaml (or using -r)

* https://github.com/kubernetes-sigs/cri-tools/issues/1041

### Anything else we need to know?

Worked in crictl v1.25.0, that still had CRI v1alpha2

This patch fixed the issue, to make the fallbacks work:

```diff
--- a/vendor/k8s.io/kubernetes/pkg/kubelet/cri/remote/remote_runtime.go
+++ b/vendor/k8s.io/kubernetes/pkg/kubelet/cri/remote/remote_runtime.go
@@ -122,6 +122,8 @@ func (r *remoteRuntimeService) validateServiceConnection(ctx context.Context, co
 
        } else if status.Code(err) == codes.Unimplemented {
                return fmt.Errorf(""CRI v1 runtime API is not implemented for endpoint %q: %w"", endpoint, err)
+       } else if status.Code(err) == codes.Unavailable {
+               return fmt.Errorf(""CRI v1 runtime API is not available for endpoint %q: %w"", endpoint, err)
        }
 
        return nil
```

### Kubernetes version

<details>
v1.26.0
</details>


### Cloud provider

<details>
Lima
</details>


### OS version

<details>
PRETTY_NAME=""Ubuntu 22.04.1 LTS""
</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>
v1.6.12
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",afbjorklund,2023-01-10 16:51:39+00:00,['saschagrunert'],2023-01-24 10:24:19+00:00,2023-01-24 10:24:19+00:00,https://github.com/kubernetes/kubernetes/issues/114956,"[('area/kubelet', None), ('kind/cleanup', 'Categorizes issue or PR as related to cleaning up code, process, or technical debt.'), ('sig/node', 'Categorizes an issue or PR as relevant to SIG Node.'), ('triage/accepted', 'Indicates an issue or PR is ready to be actively worked on.')]","[{'comment_id': 1377560406, 'issue_id': 1527712931, 'author': 'k8s-ci-robot', 'body': '@afbjorklund: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 10, 16, 51, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 1377562011, 'issue_id': 1527712931, 'author': 'afbjorklund', 'body': '/sig node', 'created_at': datetime.datetime(2023, 1, 10, 16, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 1383729991, 'issue_id': 1527712931, 'author': 'saschagrunert', 'body': '@afbjorklund I agree that we can check for unavailable connection because it will only be done on kubelet startup.', 'created_at': datetime.datetime(2023, 1, 16, 9, 26, 6, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-10 16:51:48 UTC): @afbjorklund: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

afbjorklund (Issue Creator) on (2023-01-10 16:53:00 UTC): /sig node

saschagrunert (Assginee) on (2023-01-16 09:26:06 UTC): @afbjorklund I agree that we can check for unavailable connection because it will only be done on kubelet startup.

"
1527699854,issue,closed,not_planned,Improve coverage of e2e tests requiring a default storage class,"Since the cloud provider extraction, it's become clear that many if not all of the e2e using a default storage class have been dropped, eg the sig-apps statefulset tests in [gce-cos-master-alpha-features](https://testgrid.k8s.io/google-gce#gce-cos-master-alpha-features). Even though these tests are running on GCE and so have access to PD, since CSI migration the in-tree gce-pd plugin does not function without the PD CSI driver being installed. There is [machinery](https://github.com/kubernetes/kubernetes/blob/release-1.26/test/e2e/storage/csi_volumes.go) to install this driver for storage/ tests, but it's not usable from, for example, the apps/ e2e tests.

Furthermore, the driver mentioned above is over a year out of date so is clearly not maintainable.

We propose to do the following:

* add e2e prow jobs using kind with a default local provisioner storage class, and confirm things like the statefulset tests pass (my particular axe to grind is the stateful set alpha cluster tests but will try to be comprehensive)
* To the existing GCE tests, install the pd csi driver at cluster setup and use the [external driver](https://github.com/kubernetes/kubernetes/tree/release-1.26/test/e2e/storage/external) mechanism for testing. This will be done from cloud-provider-gce which is the right place to maintain current pd csi driver versions anyway (rather than k/k).
* Remove PD CSI from csi_volumes.go.

The first two steps won't cause any disruption of existing tests. The final step will remove coverage of (an old version of) PD CSI from some GCE test grids, but this should be covered by the second step.

I've discussed this with a few folks but please correct what I've misunderstood and gotten wrong!

/assign @mattcary 
/sig storage
/cc @msau42 
/cc @BenTheElder 
",mattcary,2023-01-10 16:42:17+00:00,['mattcary'],2024-08-08 00:51:51+00:00,2024-08-08 00:51:49+00:00,https://github.com/kubernetes/kubernetes/issues/114955,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('sig/storage', 'Categorizes an issue or PR as relevant to SIG Storage.'), ('lifecycle/rotten', 'Denotes an issue or PR that has aged beyond stale and will be auto-closed.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1377548170, 'issue_id': 1527699854, 'author': 'mattcary', 'body': '/triage accepted', 'created_at': datetime.datetime(2023, 1, 10, 16, 42, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 1377553494, 'issue_id': 1527699854, 'author': 'mattcary', 'body': 'From @BenTheElder \r\n\r\n> Getting something running probably involves https://github.com/kubernetes-sigs/kind/pull/3023 and we should talk to pohly.\r\n\r\n/cc @pohly', 'created_at': datetime.datetime(2023, 1, 10, 16, 46, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 1377681763, 'issue_id': 1527699854, 'author': 'msau42', 'body': ""Plan sounds good to me. We'll want to migrate the existing optional presubmit jobs from using k/k gce cluster to external gcp cluster as part of this: https://github.com/kubernetes/test-infra/blob/master/config/jobs/kubernetes/sig-storage/sig-storage-gce-config.yaml\r\n\r\ncc @jsafrane if you have any concerns about dropping the built-in pd driver test config: https://github.com/kubernetes/kubernetes/blob/master/test/e2e/storage/csi_volumes.go#L31"", 'created_at': datetime.datetime(2023, 1, 10, 18, 31, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 1378883924, 'issue_id': 1527699854, 'author': 'aojea', 'body': '/cc', 'created_at': datetime.datetime(2023, 1, 11, 14, 57, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 1380039892, 'issue_id': 1527699854, 'author': 'jsafrane', 'body': '> Remove PD CSI from csi_volumes.go.\r\n\r\nHow will we test CSI migration? There are tests with pre-provisioned PVs + in-line volumes in k/k that we still run downstream (OpenShift) and that cannot be added via an external YAML manifest.', 'created_at': datetime.datetime(2023, 1, 12, 9, 29, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 1380709097, 'issue_id': 1527699854, 'author': 'mattcary', 'body': ""There is no CSI migration for PD CSI, it's on by default from 1.23. and now GA. That's actually the root cause here---since the in-tree gce-pd requires a pd csi driver to be installed, it's been complicated for any GCE job that wants a default storage class.\r\n\r\nI'm not proposing changing the CSI volume test framework, only removing GCE PD from it.  OpenShift tests or any other cloud provider won't be affected."", 'created_at': datetime.datetime(2023, 1, 12, 16, 48, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 1380937159, 'issue_id': 1527699854, 'author': 'msau42', 'body': ""@jsafrane to clarify, we're proposing removing the CSI test driver definition, which explicitly installs/uninstalls the CSI driver with every test case: https://github.com/kubernetes/kubernetes/blob/master/test/e2e/storage/csi_volumes.go#L31\r\n\r\nThe test driver config used for intree/migration (dynamic, preprovisioned, and inline) is here: https://github.com/kubernetes/kubernetes/blob/master/test/e2e/storage/in_tree_volumes.go#L73"", 'created_at': datetime.datetime(2023, 1, 12, 20, 3, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 1381885179, 'issue_id': 1527699854, 'author': 'jsafrane', 'body': 'I see, sorry about the noise. It\'s sad that k/k tests will loose a ""real"" storage to test with, but there are not so many tests that need a default SC anyway.', 'created_at': datetime.datetime(2023, 1, 13, 13, 52, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 1382160133, 'issue_id': 1527699854, 'author': 'mattcary', 'body': 'k/k already lost the ""real"" storage -- all tests outside of e2e/storage lost gce-pd with migration & cloud-provider extraction. That will be tested through k8s.io/cloud-provider-gcp eventually.\r\n\r\nFor most of the e2e tests I think it\'s overkill to bring up a full gce cluster, a kind cluster should be enough I think?', 'created_at': datetime.datetime(2023, 1, 13, 17, 20, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 1465047146, 'issue_id': 1527699854, 'author': 'mattcary', 'body': ""I've figured out how to make kind work with the mutation detector, see https://github.com/kubernetes-sigs/kind/pull/3123. I'll work on adding this to the jobs that expect a default storage class."", 'created_at': datetime.datetime(2023, 3, 11, 23, 14, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 1987425091, 'issue_id': 1527699854, 'author': 'k8s-triage-robot', 'body': 'This issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted', 'created_at': datetime.datetime(2024, 3, 11, 0, 10, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2156236909, 'issue_id': 1527699854, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2024, 6, 9, 0, 14, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2215667288, 'issue_id': 1527699854, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2024, 7, 9, 0, 14, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2274653016, 'issue_id': 1527699854, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2024, 8, 8, 0, 51, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2274653177, 'issue_id': 1527699854, 'author': 'k8s-ci-robot', 'body': '@k8s-triage-robot: Closing this issue, marking it as ""Not Planned"".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114955#issuecomment-2274653016):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2024, 8, 8, 0, 51, 50, tzinfo=datetime.timezone.utc)}]","mattcary (Issue Creator) on (2023-01-10 16:42:56 UTC): /triage accepted

mattcary (Issue Creator) on (2023-01-10 16:46:42 UTC): From @BenTheElder 


/cc @pohly

msau42 on (2023-01-10 18:31:55 UTC): Plan sounds good to me. We'll want to migrate the existing optional presubmit jobs from using k/k gce cluster to external gcp cluster as part of this: https://github.com/kubernetes/test-infra/blob/master/config/jobs/kubernetes/sig-storage/sig-storage-gce-config.yaml

cc @jsafrane if you have any concerns about dropping the built-in pd driver test config: https://github.com/kubernetes/kubernetes/blob/master/test/e2e/storage/csi_volumes.go#L31

aojea on (2023-01-11 14:57:18 UTC): /cc

jsafrane on (2023-01-12 09:29:11 UTC): How will we test CSI migration? There are tests with pre-provisioned PVs + in-line volumes in k/k that we still run downstream (OpenShift) and that cannot be added via an external YAML manifest.

mattcary (Issue Creator) on (2023-01-12 16:48:19 UTC): There is no CSI migration for PD CSI, it's on by default from 1.23. and now GA. That's actually the root cause here---since the in-tree gce-pd requires a pd csi driver to be installed, it's been complicated for any GCE job that wants a default storage class.

I'm not proposing changing the CSI volume test framework, only removing GCE PD from it.  OpenShift tests or any other cloud provider won't be affected.

msau42 on (2023-01-12 20:03:40 UTC): @jsafrane to clarify, we're proposing removing the CSI test driver definition, which explicitly installs/uninstalls the CSI driver with every test case: https://github.com/kubernetes/kubernetes/blob/master/test/e2e/storage/csi_volumes.go#L31

The test driver config used for intree/migration (dynamic, preprovisioned, and inline) is here: https://github.com/kubernetes/kubernetes/blob/master/test/e2e/storage/in_tree_volumes.go#L73

jsafrane on (2023-01-13 13:52:42 UTC): I see, sorry about the noise. It's sad that k/k tests will loose a ""real"" storage to test with, but there are not so many tests that need a default SC anyway.

mattcary (Issue Creator) on (2023-01-13 17:20:09 UTC): k/k already lost the ""real"" storage -- all tests outside of e2e/storage lost gce-pd with migration & cloud-provider extraction. That will be tested through k8s.io/cloud-provider-gcp eventually.

For most of the e2e tests I think it's overkill to bring up a full gce cluster, a kind cluster should be enough I think?

mattcary (Issue Creator) on (2023-03-11 23:14:16 UTC): I've figured out how to make kind work with the mutation detector, see https://github.com/kubernetes-sigs/kind/pull/3123. I'll work on adding this to the jobs that expect a default storage class.

k8s-triage-robot on (2024-03-11 00:10:57 UTC): This issue has not been updated in over 1 year, and should be re-triaged.

You can:
- Confirm that this issue is still relevant with `/triage accepted` (org members only)
- Close this issue with `/close`

For more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/

/remove-triage accepted

k8s-triage-robot on (2024-06-09 00:14:33 UTC): The Kubernetes project currently lacks enough contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle stale`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle stale

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-triage-robot on (2024-07-09 00:14:45 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle rotten

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-triage-robot on (2024-08-08 00:51:45 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.

This bot triages issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Reopen this issue with `/reopen`
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/close not-planned

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-ci-robot on (2024-08-08 00:51:50 UTC): @k8s-triage-robot: Closing this issue, marking it as ""Not Planned"".

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114955#issuecomment-2274653016):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.
</details>

"
1526870539,issue,closed,not_planned,Umbrella issue: github archived dependencies (2023 Jan),"/sig architecture
/area code-organization
/kind cleanup

> Opened issues for tracking these deps in those repo and `github.com/pkg/errors` is tracked in https://github.com/kubernetes/kubernetes/issues/113627.

[archived repos: auto-updated per month](https://github.com/pacoxu/github-repos-stats/tree/add-archived) (the list needs to be updated manually at the moment.)
-  github.com/pkg/errors 
- - [x] Track in https://github.com/kubernetes/kubernetes/issues/113627 
- - [ ] https://github.com/microsoft/hcsshim/issues/1614
- - [ ] https://github.com/google/cadvisor/issues/3227
- - [x] https://github.com/kubernetes-sigs/kustomize/issues/4964
- - [x] **Wait Next Release** https://github.com/kubernetes/system-validators/issues/33
- - [x] https://github.com/aws/aws-sdk-go/issues/4683
- - [x] **Wait Next Etcd Release? 3.5.8+** https://github.com/etcd-io/raft/pull/20/[v3.5.6] & https://github.com/etcd-io/etcd/pull/15225(https://pkg.go.dev/go.etcd.io/etcd/raft/v3?tab=versions)
- - [x] https://github.com/uber-go/zap/issues/1218 ( github.com/go-logr/zapr and https://github.com/grpc-ecosystem/go-grpc-middleware depend on it.)
- - [x] https://github.com/moby/ipvs/pull/29 &  https://github.com/moby/ipvs/pull/28 &  https://github.com/kubernetes/kubernetes/pull/115079
- github.com/getsentry/raven-go
- - [x]  https://github.com/etcd-io/raft/issues/19 : after upgrading cockroachdb/datadriven to v1.0.2, this can be fixed in etcd/raft. Pending on etcd/raft next release. 
- github.com/google/shlex
- - [x] https://github.com/kubernetes-sigs/kustomize/issues/4963
- github.com/form3tech-oss/jwt-go
- - https://github.com/kubernetes/kubernetes/pull/114921
- - [x]  **Pending on** https://github.com/kubernetes/kubernetes/pull/114403 after upgrade to v3.5.6 this will be fixed. 
- github.com/PuerkitoBio/urlesc 
- - [x] **Wait Next Release** https://github.com/kubernetes-sigs/kustomize/issues/4962
- - [x] https://github.com/kubernetes/kube-openapi/issues/356

_Originally posted by @pacoxu in https://github.com/kubernetes/kubernetes/issues/114912#issuecomment-1375930915_
      

- [x] https://github.com/prometheus/common/issues/431: github.com/pkg/errors  will always be in go.sum.",pacoxu,2023-01-10 07:04:12+00:00,[],2024-12-25 10:12:43+00:00,2023-06-30 07:28:23+00:00,https://github.com/kubernetes/kubernetes/issues/114942,"[('kind/cleanup', 'Categorizes issue or PR as related to cleaning up code, process, or technical debt.'), ('sig/architecture', 'Categorizes an issue or PR as relevant to SIG Architecture.'), ('lifecycle/rotten', 'Denotes an issue or PR that has aged beyond stale and will be auto-closed.'), ('area/code-organization', 'Issues or PRs related to kubernetes code organization'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1376817546, 'issue_id': 1526870539, 'author': 'k8s-ci-robot', 'body': '@pacoxu: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 10, 7, 4, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 1529405225, 'issue_id': 1526870539, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 5, 1, 6, 38, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 1569604638, 'issue_id': 1526870539, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 5, 31, 7, 0, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 1614246735, 'issue_id': 1526870539, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 6, 30, 7, 28, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 1614246828, 'issue_id': 1526870539, 'author': 'k8s-ci-robot', 'body': '@k8s-triage-robot: Closing this issue, marking it as ""Not Planned"".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114942#issuecomment-1614246735):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 6, 30, 7, 28, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561785725, 'issue_id': 1526870539, 'author': 'pacoxu', 'body': 'Almost done:\r\n```\r\n      ""github.com/pkg/errors"": [\r\n        ""github.com/Microsoft/hnslib"",\r\n        ""github.com/google/cadvisor"",\r\n        ""github.com/grpc-ecosystem/go-grpc-middleware"",\r\n        ""k8s.io/kubectl"",\r\n        ""k8s.io/kubernetes"",\r\n        ""sigs.k8s.io/kustomize/api"",\r\n        ""sigs.k8s.io/kustomize/kustomize/v5""\r\n      ],\r\n```', 'created_at': datetime.datetime(2024, 12, 25, 10, 12, 42, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-10 07:04:19 UTC): @pacoxu: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

k8s-triage-robot on (2023-05-01 06:38:52 UTC): The Kubernetes project currently lacks enough contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle stale`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle stale

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-triage-robot on (2023-05-31 07:00:26 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle rotten

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-triage-robot on (2023-06-30 07:28:17 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.

This bot triages issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Reopen this issue with `/reopen`
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/close not-planned

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-ci-robot on (2023-06-30 07:28:24 UTC): @k8s-triage-robot: Closing this issue, marking it as ""Not Planned"".

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114942#issuecomment-1614246735):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

pacoxu (Issue Creator) on (2024-12-25 10:12:42 UTC): Almost done:
```
      ""github.com/pkg/errors"": [
        ""github.com/Microsoft/hnslib"",
        ""github.com/google/cadvisor"",
        ""github.com/grpc-ecosystem/go-grpc-middleware"",
        ""k8s.io/kubectl"",
        ""k8s.io/kubernetes"",
        ""sigs.k8s.io/kustomize/api"",
        ""sigs.k8s.io/kustomize/kustomize/v5""
      ],
```

"
1526709486,issue,closed,completed,kube-apiserver  http2: server: xxx read: connection reset by peer,"### What happened?

http2: server: error reading preface from client 192.168.2.13:41264: read tcp 192.168.2.11:6443->192.168.2.13:41264: read: connection reset by peer
http2: server: error reading preface from client 10.224.157.55:57398: read tcp 192.168.2.11:6443->10.224.157.55:57398: read: connection reset by peer
E0109 22:29:07.705196       1 timeout.go:135] post-timeout activity - time-elapsed: 5.618753ms, GET ""/version"" result: <nil>
http2: server: error reading preface from client 10.224.157.55:37356: read tcp 192.168.2.11:6443->10.224.157.55:37356: read: connection reset by peer
http2: server: error reading preface from client 192.168.2.13:54878: read tcp 192.168.2.11:6443->192.168.2.13:54878: read: connection reset by peer
E0109 23:05:07.717026       1 timeout.go:135] post-timeout activity - time-elapsed: 875.699µs, GET ""/version"" result: <nil>
http2: server: error reading preface from client 192.168.2.12:50746: read tcp 192.168.2.11:6443->192.168.2.12:50746: read: connection reset by peer
E0109 23:10:37.756434       1 timeout.go:135] post-timeout activity - time-elapsed: 1.343291ms, GET ""/version"" result: <nil>
http2: server: error reading preface from client 192.168.2.13:42396: read tcp 192.168.2.11:6443->192.168.2.13:42396: read: connection reset by peer
E0109 23:28:07.776469       1 timeout.go:135] post-timeout activity - time-elapsed: 7.118557ms, GET ""/version"" result: <nil>
E0109 23:35:07.752700       1 timeout.go:135] post-timeout activity - time-elapsed: 109.624µs, GET ""/version"" result: <nil>
http2: server: error reading preface from client 192.168.2.15:55382: read tcp 192.168.2.11:6443->192.168.2.15:55382: read: connection reset by peer
E0109 23:54:47.680147       1 timeout.go:135] post-timeout activity - time-elapsed: 5.597974ms, GET ""/version"" result: <nil>
http2: server: error reading preface from client 192.168.2.13:42122: read tcp 192.168.2.11:6443->192.168.2.13:42122: read: connection reset by peer
E0110 00:02:47.766988       1 timeout.go:135] post-timeout activity - time-elapsed: 5.113149ms, GET ""/version"" result: <nil>
http2: server: error reading preface from client 192.168.2.14:50620: read tcp 192.168.2.11:6443->192.168.2.14:50620: read: connection reset by peer
http2: server: error reading preface from client 192.168.2.15:48050: read tcp 192.168.2.11:6443->192.168.2.15:48050: read: connection reset by peer
http2: server: error reading preface from client 192.168.2.14:55950: read tcp 192.168.2.11:6443->192.168.2.14:55950: read: connection reset by peer
E0110 00:12:17.774643       1 timeout.go:135] post-timeout activity - time-elapsed: 3.862474ms, GET ""/version"" result: <nil>
http2: server: error reading preface from client 192.168.2.13:52288: read tcp 192.168.2.11:6443->192.168.2.13:52288: read: connection reset by peer
http2: server: error reading preface from client 192.168.2.15:35554: read tcp 192.168.2.11:6443->192.168.2.15:35554: read: connection reset by peer
http2: server: error reading preface from client 10.224.157.55:36238: read tcp 192.168.2.11:6443->10.224.157.55:36238: read: connection reset by peer
http2: server: error reading preface from client 192.168.2.14:47204: read tcp 192.168.2.11:6443->192.168.2.14:47204: read: connection reset by peer
E0110 00:41:37.676166       1 timeout.go:135] post-timeout activity - time-elapsed: 925.528µs, GET ""/version"" result: <nil>
http2: server: error reading preface from client 192.168.2.15:50462: read tcp 192.168.2.11:6443->192.168.2.15:50462: read: connection reset by peer
http2: server: error reading preface from client 192.168.2.15:53810: read tcp 192.168.2.11:6443->192.168.2.15:53810: read: connection reset by peer
http2: server: error reading preface from client 192.168.2.15:60018: read tcp 192.168.2.11:6443->192.168.2.15:60018: read: connection reset by peer
http2: server: error reading preface from client 10.224.157.55:50628: read tcp 192.168.2.11:6443->10.224.157.55:50628: read: connection reset by peer
E0110 01:06:07.709770       1 timeout.go:135] post-timeout activity - time-elapsed: 3.797188ms, GET ""/version"" result: <nil>
http2: server: error reading preface from client 192.168.2.14:34996: read tcp 192.168.2.11:6443->192.168.2.14:34996: read: connection reset by peer
http2: server: error reading preface from client 192.168.2.12:35318: read tcp 192.168.2.11:6443->192.168.2.12:35318: read: connection reset by peer
http2: server: error reading preface from client 192.168.2.15:58488: read tcp 192.168.2.11:6443->192.168.2.15:58488: read: connection reset by peer

### What did you expect to happen?

kube-apiserver pod outputs lots of ""http2: server: error reading preface from client 192.168.2.15:58488: read tcp 192.168.2.11:6443->192.168.2.15:58488: read: connection reset by peer""

### How can we reproduce it (as minimally and precisely as possible)?

v1.22.8

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
# kubectl version
Client Version: version.Info{Major:""1"", Minor:""22"", GitVersion:""v1.22.8"", GitCommit:""7061dbbf75f9f82e8ab21f9be7e8ffcaae8e0d44"", GitTreeState:""clean"", BuildDate:""2022-03-16T14:10:06Z"", GoVersion:""go1.16.15"", Com
piler:""gc"", Platform:""linux/amd64""}Server Version: version.Info{Major:""1"", Minor:""22"", GitVersion:""v1.22.8"", GitCommit:""7061dbbf75f9f82e8ab21f9be7e8ffcaae8e0d44"", GitTreeState:""clean"", BuildDate:""2022-03-16T14:04:34Z"", GoVersion:""go1.16.15"", Com
piler:""gc"", Platform:""linux/amd64""}
```

</details>


### Cloud provider

<details>

</details>


### OS version

<details>

```console
# cat /etc/os-release
NAME=""CentOS Linux""
VERSION=""7 (Core)""
ID=""centos""
ID_LIKE=""rhel fedora""
VERSION_ID=""7""
PRETTY_NAME=""CentOS Linux 7 (Core)""
ANSI_COLOR=""0;31""
CPE_NAME=""cpe:/o:centos:centos:7""
HOME_URL=""https://www.centos.org/""
BUG_REPORT_URL=""https://bugs.centos.org/""

CENTOS_MANTISBT_PROJECT=""CentOS-7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT=""centos""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""


# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",lspjx,2023-01-10 03:38:39+00:00,[],2023-09-07 06:12:47+00:00,2023-09-07 06:12:46+00:00,https://github.com/kubernetes/kubernetes/issues/114939,"[('kind/support', 'Categorizes issue or PR as a support question.'), ('needs-sig', 'Indicates an issue or PR lacks a `sig/foo` label and requires one.'), ('lifecycle/rotten', 'Denotes an issue or PR that has aged beyond stale and will be auto-closed.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1376685498, 'issue_id': 1526709486, 'author': 'k8s-ci-robot', 'body': '@lspjx: There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `/sig <group-name>`\n- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 10, 3, 38, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 1376685510, 'issue_id': 1526709486, 'author': 'k8s-ci-robot', 'body': '@lspjx: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 10, 3, 38, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 1376750967, 'issue_id': 1526709486, 'author': 'pacoxu', 'body': ""/kind support\r\n\r\nfor APIServer error logs, firstly check that apiserver and etcd are ready\r\n- is apiserver serving healthy? https://kubernetes.io/docs/reference/using-api/health-checks/\r\n- is etcd healthy?\r\n\r\nThen, check clients and networking\r\n- check if all [clients' version match](https://github.com/rootsongjc/kubernetes-vagrant-centos-cluster/issues/9) and if there is no[ firewall between clients and server](https://www.infiniroot.com/blog/1051/fixing-monitoring-api-is-not-ready-error-rancher-2-kubernetes-cluster)."", 'created_at': datetime.datetime(2023, 1, 10, 5, 20, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 1376806735, 'issue_id': 1526709486, 'author': 'lspjx', 'body': ""> /亲切的支持\r\n> \r\n> 对于 APIServer 错误日志，首先检查 apiserver 和 etcd 是否准备就绪\r\n> \r\n> * API Server 服务健康吗？ https://kubernetes.io/docs/reference/using-api/health-checks/\r\n> * 蚀刻健康吗？\r\n> \r\n> 然后，检查客户端和网络\r\n> \r\n> * 检查是否全部 [客户端版本匹配](https://github.com/rootsongjc/kubernetes-vagrant-centos-cluster/issues/9) 如果没有[ 客户端和服务器之间的防火墙](https://www.infiniroot.com/blog/1051/fixing-monitoring-api-is-not-ready-error-rancher-2-kubernetes-cluster).\r\n\r\n\r\ncheck apiserver and etcd service health, firewall closed.\r\n```sehll\r\n# kubectl get --raw='/readyz?verbose'\r\n[+]ping ok\r\n[+]log ok\r\n[+]etcd ok\r\n[+]informer-sync ok\r\n[+]poststarthook/start-kube-apiserver-admission-initializer ok\r\n[+]poststarthook/generic-apiserver-start-informers ok\r\n[+]poststarthook/priority-and-fairness-config-consumer ok\r\n[+]poststarthook/priority-and-fairness-filter ok\r\n[+]poststarthook/start-apiextensions-informers ok\r\n[+]poststarthook/start-apiextensions-controllers ok\r\n[+]poststarthook/crd-informer-synced ok\r\n[+]poststarthook/bootstrap-controller ok\r\n[+]poststarthook/rbac/bootstrap-roles ok\r\n[+]poststarthook/scheduling/bootstrap-system-priority-classes ok\r\n[+]poststarthook/priority-and-fairness-config-producer ok\r\n[+]poststarthook/start-cluster-authentication-info-controller ok\r\n[+]poststarthook/aggregator-reload-proxy-client-cert ok\r\n[+]poststarthook/start-kube-aggregator-informers ok\r\n[+]poststarthook/apiservice-registration-controller ok\r\n[+]poststarthook/apiservice-status-available-controller ok\r\n[+]poststarthook/kube-apiserver-autoregistration ok\r\n[+]autoregister-completion ok\r\n[+]poststarthook/apiservice-openapi-controller ok\r\n[+]shutdown ok\r\nreadyz check passed\r\n\r\n# ./etcdctl  --endpoints=https://192.168.2.11:2379,https://192.168.2.12:2379,https://192.168.2.13:2379   \\\r\n> --cacert=/etc/kubernetes/pki/etcd/ca.crt   \\\r\n> --cert=/etc/kubernetes/pki/etcd/server.crt  \\\r\n> --key=/etc/kubernetes/pki/etcd/server.key \\\r\n> endpoint health\r\nhttps://192.168.2.11:2379 is healthy: successfully committed proposal: took = 21.518036ms\r\nhttps://192.168.2.13:2379 is healthy: successfully committed proposal: took = 32.778129ms\r\nhttps://192.168.2.12:2379 is healthy: successfully committed proposal: took = 37.356129ms\r\n\r\n# ./etcdctl  --endpoints=https://192.168.2.11:2379,https://192.168.2.12:2379,https://192.168.2.13:2379   \\\r\n> --cacert=/etc/kubernetes/pki/etcd/ca.crt   \\\r\n> --cert=/etc/kubernetes/pki/etcd/server.crt  \\\r\n> --key=/etc/kubernetes/pki/etcd/server.key \\\r\n> endpoint status\r\nhttps://192.168.2.11:2379, 84df052d1edb239e, 3.5.0, 63 MB, false, false, 19, 47991927, 47991927, \r\nhttps://192.168.2.12:2379, 8c4354a70811bf52, 3.5.0, 63 MB, true, false, 19, 47991927, 47991927, \r\nhttps://192.168.2.13:2379, a0ca49ec04d166bf, 3.5.0, 65 MB, false, false, 19, 47991927, 47991927,\r\n```"", 'created_at': datetime.datetime(2023, 1, 10, 6, 50, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 1480835896, 'issue_id': 1526709486, 'author': 'porridge', 'body': '""http2: server: error reading preface from client [...] read: connection reset by peer"" sounds like a client is connecting to your API server, but by the time the API server gets round to reading the initial bytes that the client should be sending, the client resets the connection.\r\n\r\nThere does not seem to be anything wrong in the API server. You might want to check why your client is behaving like this.', 'created_at': datetime.datetime(2023, 3, 23, 9, 11, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 1600484240, 'issue_id': 1526709486, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 6, 21, 9, 14, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 1645292351, 'issue_id': 1526709486, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 7, 21, 9, 30, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 1709534570, 'issue_id': 1526709486, 'author': 'pacoxu', 'body': '/remove-kind bug\r\n/close', 'created_at': datetime.datetime(2023, 9, 7, 6, 12, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 1709534668, 'issue_id': 1526709486, 'author': 'k8s-ci-robot', 'body': '@pacoxu: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114939#issuecomment-1709534570):\n\n>/remove-kind bug\r\n>/close\r\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 9, 7, 6, 12, 46, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-10 03:38:46 UTC): @lspjx: There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:
- `/sig <group-name>`
- `/wg <group-name>`
- `/committee <group-name>`

Please see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

k8s-ci-robot on (2023-01-10 03:38:47 UTC): @lspjx: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

pacoxu on (2023-01-10 05:20:33 UTC): /kind support

for APIServer error logs, firstly check that apiserver and etcd are ready
- is apiserver serving healthy? https://kubernetes.io/docs/reference/using-api/health-checks/
- is etcd healthy?

Then, check clients and networking
- check if all [clients' version match](https://github.com/rootsongjc/kubernetes-vagrant-centos-cluster/issues/9) and if there is no[ firewall between clients and server](https://www.infiniroot.com/blog/1051/fixing-monitoring-api-is-not-ready-error-rancher-2-kubernetes-cluster).

lspjx (Issue Creator) on (2023-01-10 06:50:13 UTC): check apiserver and etcd service health, firewall closed.
```sehll
# kubectl get --raw='/readyz?verbose'
[+]ping ok
[+]log ok
[+]etcd ok
[+]informer-sync ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]shutdown ok
readyz check passed

# ./etcdctl  --endpoints=https://192.168.2.11:2379,https://192.168.2.12:2379,https://192.168.2.13:2379   \
https://192.168.2.11:2379 is healthy: successfully committed proposal: took = 21.518036ms
https://192.168.2.13:2379 is healthy: successfully committed proposal: took = 32.778129ms
https://192.168.2.12:2379 is healthy: successfully committed proposal: took = 37.356129ms

# ./etcdctl  --endpoints=https://192.168.2.11:2379,https://192.168.2.12:2379,https://192.168.2.13:2379   \
https://192.168.2.11:2379, 84df052d1edb239e, 3.5.0, 63 MB, false, false, 19, 47991927, 47991927, 
https://192.168.2.12:2379, 8c4354a70811bf52, 3.5.0, 63 MB, true, false, 19, 47991927, 47991927, 
https://192.168.2.13:2379, a0ca49ec04d166bf, 3.5.0, 65 MB, false, false, 19, 47991927, 47991927,
```

porridge on (2023-03-23 09:11:24 UTC): ""http2: server: error reading preface from client [...] read: connection reset by peer"" sounds like a client is connecting to your API server, but by the time the API server gets round to reading the initial bytes that the client should be sending, the client resets the connection.

There does not seem to be anything wrong in the API server. You might want to check why your client is behaving like this.

k8s-triage-robot on (2023-06-21 09:14:23 UTC): The Kubernetes project currently lacks enough contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle stale`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle stale

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-triage-robot on (2023-07-21 09:30:22 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle rotten

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

pacoxu on (2023-09-07 06:12:41 UTC): /remove-kind bug
/close

k8s-ci-robot on (2023-09-07 06:12:46 UTC): @pacoxu: Closing this issue.

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114939#issuecomment-1709534570):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

"
1526374700,issue,closed,completed,Failure cluster [dd3d4ece...] net/http: TLS handshake timeout,"### Failure cluster [dd3d4eceb1d8abf2dc14](https://go.k8s.io/triage#dd3d4eceb1d8abf2dc14)

##### Error text:
```
[FAILED] failed to wait for definition ""com.example.crd-publish-openapi-test-multi-to-single-ver.v6alpha1.e2e-test-crd-publish-openapi-393-crd"" not to be served anymore: failed to wait for OpenAPI spec validating condition: Get ""https://35.230.96.113/openapi/v2"": net/http: TLS handshake timeout; lastMsg: 
In [It] at: test/e2e/apimachinery/crd_publish_openapi.go:469 @ 01/07/23 04:26:57.677

```
#### Recent failures:
[1/9/2023, 7:21:38 AM pr:pull-kubernetes-e2e-gce](https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/114548/pull-kubernetes-e2e-gce/1612424324973596672)
[1/4/2023, 11:25:32 AM pr:pull-kubernetes-e2e-gce](https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/114678/pull-kubernetes-e2e-gce/1610673771734110208)
[1/4/2023, 3:32:08 AM pr:pull-kubernetes-e2e-gce](https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/114801/pull-kubernetes-e2e-gce/1610554624673058816)


/kind failing-test
/kind flake

/sig api-machinery

#### See also:
There are other sporadic failures with a similar ""TLS handshake timeout"" error as shown here:
https://storage.googleapis.com/k8s-triage/index.html?ci=0&pr=1&text=TLS%20handshake%20timeout


```
error: error when deleting ""STDIN"": Delete ""https://35.197.34.50/api/v1/namespaces/kubectl-1446/pods/httpd"": net/http: TLS handshake timeout
```
* https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/114914/pull-kubernetes-e2e-gce/1612375254447951872


```
failed to wait for definition ""com.example.crd-publish-openapi-test-multi-to-single-ver.v5.e2e-test-crd-publish-openapi-3454-crd"" to be served with the right OpenAPI schema: failed to wait for OpenAPI spec validating condition: Get ""https://35.230.38.24/openapi/v2"": net/http: TLS handshake timeout; lastMsg:
```
* https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/114850/pull-kubernetes-e2e-gce/1611006645284900864
* https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/114830/pull-kubernetes-e2e-ubuntu-gce-network-policies/1610985514557509632
* https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/110838/pull-kubernetes-e2e-gce/1610281231612645376
* https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/114508/pull-kubernetes-e2e-gce/1609890328821633024


```
failed to wait for definition ""com.example.crd-publish-openapi-test-multi-to-single-ver.v6alpha1.e2e-test-crd-publish-openapi-3074-crd"" to be served with the right OpenAPI schema: failed to wait for OpenAPI spec validating condition: Get ""https://35.197.105.7/openapi/v2"": net/http: TLS handshake timeout; lastMsg: 
```
* https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/14960/14874/pull-kops-e2e-k8s-gce-cilium/1611622759018795008
* https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/14930/14874/pull-kops-e2e-k8s-gce-cilium/1610034333370814464


```
failed to wait for definition ""com.example.crd-publish-openapi-test-multi-ver.v3.e2e-test-crd-publish-openapi-50-crd"" to be served with the right OpenAPI schema: failed to wait for OpenAPI spec validating condition: Get ""https://api.e2e-e2e-kops-aws-arm64-release.test-cncf-aws.k8s.io/openapi/v2"": net/http: TLS handshake timeout; lastMsg:
```
* https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/14870/14870/pull-kops-e2e-cni-cilium-eni/1608902630061379584


```
Unable to connect to the server: net/http: TLS handshake timeout
```
* https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/batch/pull-kubernetes-e2e-gce/1611048455520653312


```
failed to wait for definition ""com.example.crd-publish-openapi-test-multi-to-single-ver.v6alpha1.e2e-test-crd-publish-openapi-3584-crd"" to be served with the right OpenAPI schema: failed to wait for OpenAPI spec validating condition: Get ""https://34.105.15.206/openapi/v2"": net/http: TLS handshake timeout; lastMsg: 
```
* https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/114865/pull-kubernetes-e2e-gce-ubuntu-containerd/1611181266290348032
",brianpursley,2023-01-09 21:45:53+00:00,[],2024-07-18 20:17:26+00:00,2024-07-18 20:17:24+00:00,https://github.com/kubernetes/kubernetes/issues/114934,"[('sig/api-machinery', 'Categorizes an issue or PR as relevant to SIG API Machinery.'), ('kind/flake', 'Categorizes issue or PR as related to a flaky test.'), ('kind/failing-test', 'Categorizes issue or PR as related to a consistently or frequently failing test.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1376371754, 'issue_id': 1526374700, 'author': 'brianpursley', 'body': 'Related kubectl issue requesting retry logic for TLS handshake timeout:\r\nhttps://github.com/kubernetes/kubernetes/issues/119448', 'created_at': datetime.datetime(2023, 1, 9, 21, 46, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 1376385136, 'issue_id': 1526374700, 'author': 'brianpursley', 'body': 'I suppose one answer might be that we accept that there will be occasional timeouts related to transient network problems, and therefore this flake is a non-issue, but I wanted to call attention to the linked issue in case there is some interest in making dial more resilient to TLS handshake timeouts.\r\n\r\nOne thing to note is the TLS handshake timeout occurs after only 10 seconds, whereas the overall connection timeout is set to 30 seconds.', 'created_at': datetime.datetime(2023, 1, 9, 21, 59, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 1380786290, 'issue_id': 1526374700, 'author': 'cici37', 'body': '/cc @Jefftree @apelisse \r\n/triage accepted', 'created_at': datetime.datetime(2023, 1, 12, 17, 53, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 1385785434, 'issue_id': 1526374700, 'author': 'apelisse', 'body': ""Thanks @brianpursley, that's useful. We can probably do better!"", 'created_at': datetime.datetime(2023, 1, 17, 17, 35, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2237143974, 'issue_id': 1526374700, 'author': 'k8s-triage-robot', 'body': 'This issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted', 'created_at': datetime.datetime(2024, 7, 18, 17, 37, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2237487321, 'issue_id': 1526374700, 'author': 'cici37', 'body': ""/close\r\nWe looked at this during bug triage meeting and seems like it's not often. Please feel free to reopen if this flaky is still happening."", 'created_at': datetime.datetime(2024, 7, 18, 20, 17, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2237487429, 'issue_id': 1526374700, 'author': 'k8s-ci-robot', 'body': ""@cici37: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114934#issuecomment-2237487321):\n\n>/close\r\n>We looked at this during bug triage meeting and seems like it's not often. Please feel free to reopen if this flaky is still happening.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>"", 'created_at': datetime.datetime(2024, 7, 18, 20, 17, 25, tzinfo=datetime.timezone.utc)}]","brianpursley (Issue Creator) on (2023-01-09 21:46:26 UTC): Related kubectl issue requesting retry logic for TLS handshake timeout:
https://github.com/kubernetes/kubernetes/issues/119448

brianpursley (Issue Creator) on (2023-01-09 21:59:57 UTC): I suppose one answer might be that we accept that there will be occasional timeouts related to transient network problems, and therefore this flake is a non-issue, but I wanted to call attention to the linked issue in case there is some interest in making dial more resilient to TLS handshake timeouts.

One thing to note is the TLS handshake timeout occurs after only 10 seconds, whereas the overall connection timeout is set to 30 seconds.

cici37 on (2023-01-12 17:53:38 UTC): /cc @Jefftree @apelisse 
/triage accepted

apelisse on (2023-01-17 17:35:59 UTC): Thanks @brianpursley, that's useful. We can probably do better!

k8s-triage-robot on (2024-07-18 17:37:40 UTC): This issue has not been updated in over 1 year, and should be re-triaged.

You can:
- Confirm that this issue is still relevant with `/triage accepted` (org members only)
- Close this issue with `/close`

For more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/

/remove-triage accepted

cici37 on (2024-07-18 20:17:20 UTC): /close
We looked at this during bug triage meeting and seems like it's not often. Please feel free to reopen if this flaky is still happening.

k8s-ci-robot on (2024-07-18 20:17:25 UTC): @cici37: Closing this issue.

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114934#issuecomment-2237487321):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.
</details>

"
1526343604,issue,closed,completed,topologySpreadConstraints[*].labelSelector is not correctly validated,"### What happened?

```
apiVersion: v1
kind: Pod
metadata:
  name: correctly-invalid
  labels:
    foo: ""{bar}""
spec:
  containers:
    - name: pause
      image: ""public.ecr.aws/eks-distro/kubernetes/pause:3.2""
---
apiVersion: v1
kind: Pod
metadata:
  name: incorrectly-valid
spec:
  containers:
    - name: pause
      image: ""public.ecr.aws/eks-distro/kubernetes/pause:3.2""
  topologySpreadConstraints:
  - labelSelector:
      matchLabels:
        foo: ""{bar}""
    maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway
```
```
kubectl apply -f example.yaml
pod/incorrectly-valid created
The Pod ""correctly-invalid"" is invalid: metadata.labels: Invalid value: ""{bar}"": a valid label must be an empty string or consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyValue',  or 'my_value',  or '12345', regex used for validation is '(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?')
```

### What did you expect to happen?

The pod named `incorrectly-valid` should be rejected by the API Server.

### How can we reproduce it (as minimally and precisely as possible)?

apiVersion: v1
kind: Pod
metadata:
  name: correctly-invalid
  labels:
    foo: ""{bar}""
spec:
  containers:
    - name: pause
      image: ""public.ecr.aws/eks-distro/kubernetes/pause:3.2""
---
apiVersion: v1
kind: Pod
metadata:
  name: incorrectly-valid
spec:
  containers:
    - name: pause
      image: ""public.ecr.aws/eks-distro/kubernetes/pause:3.2""
  topologySpreadConstraints:
  - labelSelector:
      matchLabels:
        foo: ""{bar}""
    maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway


### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
k version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:""1"", Minor:""25"", GitVersion:""v1.25.4"", GitCommit:""872a965c6c6526caa949f0c6ac028ef7aff3fb78"", GitTreeState:""clean"", BuildDate:""2022-11-09T13:28:30Z"", GoVersion:""go1.19.3"", Compiler:""gc"", Platform:""darwin/amd64""}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:""1"", Minor:""23+"", GitVersion:""v1.23.13-eks-fb459a0"", GitCommit:""55bd5d5cb7d32bc35e4e050f536181196fb8c6f7"", GitTreeState:""clean"", BuildDate:""2022-10-24T20:35:40Z"", GoVersion:""go1.17.13"", Compiler:""gc"", Platform:""linux/amd64""}
WARNING: version difference between client (1.25) and server (1.23) exceeds the supported minor version skew of +/-1
```

</details>


### Cloud provider

<details>

</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",ellistarn,2023-01-09 21:21:50+00:00,[],2023-01-11 18:06:04+00:00,2023-01-11 18:06:04+00:00,https://github.com/kubernetes/kubernetes/issues/114932,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('sig/scheduling', 'Categorizes an issue or PR as relevant to SIG Scheduling.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1376335129, 'issue_id': 1526343604, 'author': 'k8s-ci-robot', 'body': '@ellistarn: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 9, 21, 21, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 1376338240, 'issue_id': 1526343604, 'author': 'ellistarn', 'body': '/sig scheduling', 'created_at': datetime.datetime(2023, 1, 9, 21, 24, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 1378489471, 'issue_id': 1526343604, 'author': 'kerthcet', 'body': 'Thanks for opening this @ellistarn , I think it was fixed by https://github.com/kubernetes/kubernetes/pull/111802 in v1.27', 'created_at': datetime.datetime(2023, 1, 11, 9, 50, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 1379286162, 'issue_id': 1526343604, 'author': 'ellistarn', 'body': 'Beautiful, thank you!', 'created_at': datetime.datetime(2023, 1, 11, 18, 6, 1, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-09 21:21:58 UTC): @ellistarn: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

ellistarn (Issue Creator) on (2023-01-09 21:24:15 UTC): /sig scheduling

kerthcet on (2023-01-11 09:50:12 UTC): Thanks for opening this @ellistarn , I think it was fixed by https://github.com/kubernetes/kubernetes/pull/111802 in v1.27

ellistarn (Issue Creator) on (2023-01-11 18:06:01 UTC): Beautiful, thank you!

"
1526200993,issue,closed,completed,Increase the update interval of perf counters to 10 seconds,"### What happened?

Perf counters are currently updated every second which doesn't match the default cadvisor collection period as 10 seconds.

https://github.com/kubernetes/kubernetes/blob/eb7fd7f51c166591adbc52eacef3ce1e4d17bf04/pkg/kubelet/winstats/perfcounters.go#L37


### What did you expect to happen?

Increase the update interval of perf counters to 10 seconds to match the house keeping interval of cadvisor.

https://github.com/kubernetes/kubernetes/blob/eb7fd7f51c166591adbc52eacef3ce1e4d17bf04/pkg/kubelet/cadvisor/cadvisor_linux.go#L60

### How can we reproduce it (as minimally and precisely as possible)?

The fresh install of our internal AKS clusters.

### Anything else we need to know?

The perf counter query (pdh!PdhCollectQueryData) is responsible for >20% of CPU time in kubelet process on internal clusters, which can be reduced by lowering the perf counter update interval.


### Kubernetes version

<details>

```console
$ kubectl version
# K8s v1.24.6
```

</details>


### Cloud provider

<details>
Azure Kubernetes Service (AKS)
</details>


### OS version

<details>

Operating System: Windows Server 2019 Datacenter 
OS Build Number: 17763.3770.amd64fre.rs5_release.180914-1434 

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",yungjenhung,2023-01-09 19:38:50+00:00,[],2023-03-14 21:13:13+00:00,2023-03-14 21:13:13+00:00,https://github.com/kubernetes/kubernetes/issues/114928,"[('sig/node', 'Categorizes an issue or PR as relevant to SIG Node.'), ('kind/feature', 'Categorizes issue or PR as related to a new feature.'), ('sig/windows', 'Categorizes an issue or PR as relevant to SIG Windows.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1376181820, 'issue_id': 1526200993, 'author': 'k8s-ci-robot', 'body': '@yungjenhung: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 9, 19, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 1376185617, 'issue_id': 1526200993, 'author': 'yungjenhung', 'body': '/sig node\r\n/sig windows', 'created_at': datetime.datetime(2023, 1, 9, 19, 42, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 1387552353, 'issue_id': 1526200993, 'author': 'SergeyKanzhelev', 'body': '/area cadvisor\r\n\r\nSince it is a performance overhead and not a bug per se, I will change the kind of this issue:\r\n\r\n/remove-kind bug\r\n/kind feature\r\n\r\n@bobbypage I bet there is an issue for this already. Can you comment/dedup?', 'created_at': datetime.datetime(2023, 1, 18, 18, 41, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 1396299183, 'issue_id': 1526200993, 'author': 'bobbypage', 'body': 'I believe the ask here is to increase the `perfCounterUpdatePeriod` which is only used on Windows to fetch stats from windows specific per counters. Windows does not use cAdvisor and instead uses  `kubernetes/pkg/kubelet/winstats` which reads from the perf counters directly.\r\n\r\n/cc @jsturtevant @marosset', 'created_at': datetime.datetime(2023, 1, 19, 0, 49, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 1396299804, 'issue_id': 1526200993, 'author': 'bobbypage', 'body': '(removing cadvisor since it is ask for windows stats)\r\n\r\n/remove-area cadvisor', 'created_at': datetime.datetime(2023, 1, 19, 0, 50, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 1397230830, 'issue_id': 1526200993, 'author': 'jsturtevant', 'body': ""> I believe the ask here is to increase the `perfCounterUpdatePeriod` which is only used on Windows to fetch stats from windows specific per counters. Windows does not use cAdvisor and instead uses `kubernetes/pkg/kubelet/winstats` which reads from the perf counters directly.\r\n\r\nagreed that is the ask. I don't have any major issue with it but would be worried about change in behavior for existing users.  What are our options for making this configurable?"", 'created_at': datetime.datetime(2023, 1, 19, 16, 13, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 1397781838, 'issue_id': 1526200993, 'author': 'yungjenhung', 'body': 'That would be great if making it configurable.', 'created_at': datetime.datetime(2023, 1, 20, 0, 31, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 1398660646, 'issue_id': 1526200993, 'author': 'jsturtevant', 'body': 'It might not be worth the overhead to pass this as configuration through kubelet parameters.  I am unsure what affect it would have on folks who rely on it for current reporting or if it would be ok to bump the querying time.\r\n\r\n@yungjenhung if you increase it to 10s do you see a large drop in cpu usage?', 'created_at': datetime.datetime(2023, 1, 20, 16, 53, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 1398661071, 'issue_id': 1526200993, 'author': 'jsturtevant', 'body': '@marosset might have thoughts as well', 'created_at': datetime.datetime(2023, 1, 20, 16, 53, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 1398782171, 'issue_id': 1526200993, 'author': 'yungjenhung', 'body': 'Based on internal cloud profiling data from thousands of Windows clusters, pdh!PdhCollectQueryData takes 33% CPU usage as of today, 01/20/2023. If the updating interval increases from 1s to 10s, pdh!PdhCollectQueryData usage can be reduced 90% and CPU usage of kubelet can potentially drop 30%.', 'created_at': datetime.datetime(2023, 1, 20, 18, 33, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 1400823815, 'issue_id': 1526200993, 'author': 'jsturtevant', 'body': 'that is a compelling improvement.  I am still concerned about changing the default on folks who might rely on it, I am not sure if it is a valid concern though', 'created_at': datetime.datetime(2023, 1, 23, 18, 55, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 1401093406, 'issue_id': 1526200993, 'author': 'yungjenhung', 'body': 'To understand the impact of changing the interval, what is the purpose for querying those Windows counters?', 'created_at': datetime.datetime(2023, 1, 23, 22, 26, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 1402378296, 'issue_id': 1526200993, 'author': 'marosset', 'body': ""@yungjenhung - Can you share a little bit more information about your setup?\r\nWhat node size are you using in AKS? \r\nAre you overprovisioning your nodes?\r\nDo you have any addons running in your AKS cluster? \r\nDoes the drop eventually go down?\r\nHave you seen this issue on Windows Server 2022 too?\r\n\r\nWe don't see this spike in CPU usage in our clusters provisioned with CAPZ (cluster api provider for azure) so I suspect something else is going on too.\r\nhttps://perf-dash.k8s.io/#/?jobname=soak-tests-capz-windows-2019&metriccategoryname=E2E&metricname=CPUUsage&Process=kubelet\r\nI'm not opposed to increasing the timeout but currently I think this issue may be environmental."", 'created_at': datetime.datetime(2023, 1, 24, 18, 12, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 1402737720, 'issue_id': 1526200993, 'author': 'yungjenhung', 'body': ""1\\ The node size is D32_Flex which has 32 vCPUs and 128 GiB memory. \r\n2\\ The perf counter query usage is the same across all clusters regardless of the cluster is running hot or cold.\r\n3\\ azurepolicy is the only addon enabled.\r\n4\\ The trending of perf counter query usage is quite stable.\r\n5\\ Not applicable at this moment, all clusters are running with Windows Server 2019.\r\n\r\nDo you have CPU traces from CAPZ to check the time spent on PdhCollectQueryData? For our internal profiling data, perf counter usage is pretty stable in kubelet, and it's not related to the CPU spike."", 'created_at': datetime.datetime(2023, 1, 24, 21, 50, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 1405397606, 'issue_id': 1526200993, 'author': 'jsturtevant', 'body': "">  For our internal profiling data, perf counter usage is pretty stable in kubelet, and it's not related to the CPU spike.\r\n\r\nI am not sure I understand this statement.  The cpu usage from the the winstats pkg would be attributed to the kubelet since it is the own process.  What is the owning process for `PdhCollectQueryData` usage you are seeing?"", 'created_at': datetime.datetime(2023, 1, 26, 18, 7, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 1405450003, 'issue_id': 1526200993, 'author': 'yungjenhung', 'body': '`PdhCollectQueryData` usage is from kubelet process itself. One thing I misunderstood is that, the `PdhCollectQueryData` usage in kubelet is not stable. Actually, it keeps growing. The usage is almost zero originally, but it grows up to 30+% cpu usage in kubelet. Looking into it...', 'created_at': datetime.datetime(2023, 1, 26, 18, 49, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 1405588129, 'issue_id': 1526200993, 'author': 'yungjenhung', 'body': ""The tool wasn't getting the correct data at the first time. It turns out `PdhCollectQueryData` usage in kubelet is growing from 20% to 30+% across all clusters, and the usage is never close to zero. I will investigate the growing cost of `PdhCollectQueryData`. It'd be great if we can increase the query interval to reduce the cost."", 'created_at': datetime.datetime(2023, 1, 26, 20, 7, 52, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-09 19:39:00 UTC): @yungjenhung: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

yungjenhung (Issue Creator) on (2023-01-09 19:42:58 UTC): /sig node
/sig windows

SergeyKanzhelev on (2023-01-18 18:41:07 UTC): /area cadvisor

Since it is a performance overhead and not a bug per se, I will change the kind of this issue:

/remove-kind bug
/kind feature

@bobbypage I bet there is an issue for this already. Can you comment/dedup?

bobbypage on (2023-01-19 00:49:41 UTC): I believe the ask here is to increase the `perfCounterUpdatePeriod` which is only used on Windows to fetch stats from windows specific per counters. Windows does not use cAdvisor and instead uses  `kubernetes/pkg/kubelet/winstats` which reads from the perf counters directly.

/cc @jsturtevant @marosset

bobbypage on (2023-01-19 00:50:40 UTC): (removing cadvisor since it is ask for windows stats)

/remove-area cadvisor

jsturtevant on (2023-01-19 16:13:59 UTC): agreed that is the ask. I don't have any major issue with it but would be worried about change in behavior for existing users.  What are our options for making this configurable?

yungjenhung (Issue Creator) on (2023-01-20 00:31:45 UTC): That would be great if making it configurable.

jsturtevant on (2023-01-20 16:53:12 UTC): It might not be worth the overhead to pass this as configuration through kubelet parameters.  I am unsure what affect it would have on folks who rely on it for current reporting or if it would be ok to bump the querying time.

@yungjenhung if you increase it to 10s do you see a large drop in cpu usage?

jsturtevant on (2023-01-20 16:53:36 UTC): @marosset might have thoughts as well

yungjenhung (Issue Creator) on (2023-01-20 18:33:23 UTC): Based on internal cloud profiling data from thousands of Windows clusters, pdh!PdhCollectQueryData takes 33% CPU usage as of today, 01/20/2023. If the updating interval increases from 1s to 10s, pdh!PdhCollectQueryData usage can be reduced 90% and CPU usage of kubelet can potentially drop 30%.

jsturtevant on (2023-01-23 18:55:49 UTC): that is a compelling improvement.  I am still concerned about changing the default on folks who might rely on it, I am not sure if it is a valid concern though

yungjenhung (Issue Creator) on (2023-01-23 22:26:38 UTC): To understand the impact of changing the interval, what is the purpose for querying those Windows counters?

marosset on (2023-01-24 18:12:33 UTC): @yungjenhung - Can you share a little bit more information about your setup?
What node size are you using in AKS? 
Are you overprovisioning your nodes?
Do you have any addons running in your AKS cluster? 
Does the drop eventually go down?
Have you seen this issue on Windows Server 2022 too?

We don't see this spike in CPU usage in our clusters provisioned with CAPZ (cluster api provider for azure) so I suspect something else is going on too.
https://perf-dash.k8s.io/#/?jobname=soak-tests-capz-windows-2019&metriccategoryname=E2E&metricname=CPUUsage&Process=kubelet
I'm not opposed to increasing the timeout but currently I think this issue may be environmental.

yungjenhung (Issue Creator) on (2023-01-24 21:50:53 UTC): 1\ The node size is D32_Flex which has 32 vCPUs and 128 GiB memory. 
2\ The perf counter query usage is the same across all clusters regardless of the cluster is running hot or cold.
3\ azurepolicy is the only addon enabled.
4\ The trending of perf counter query usage is quite stable.
5\ Not applicable at this moment, all clusters are running with Windows Server 2019.

Do you have CPU traces from CAPZ to check the time spent on PdhCollectQueryData? For our internal profiling data, perf counter usage is pretty stable in kubelet, and it's not related to the CPU spike.

jsturtevant on (2023-01-26 18:07:07 UTC): I am not sure I understand this statement.  The cpu usage from the the winstats pkg would be attributed to the kubelet since it is the own process.  What is the owning process for `PdhCollectQueryData` usage you are seeing?

yungjenhung (Issue Creator) on (2023-01-26 18:49:40 UTC): `PdhCollectQueryData` usage is from kubelet process itself. One thing I misunderstood is that, the `PdhCollectQueryData` usage in kubelet is not stable. Actually, it keeps growing. The usage is almost zero originally, but it grows up to 30+% cpu usage in kubelet. Looking into it...

yungjenhung (Issue Creator) on (2023-01-26 20:07:52 UTC): The tool wasn't getting the correct data at the first time. It turns out `PdhCollectQueryData` usage in kubelet is growing from 20% to 30+% across all clusters, and the usage is never close to zero. I will investigate the growing cost of `PdhCollectQueryData`. It'd be great if we can increase the query interval to reduce the cost.

"
1526199890,issue,closed,completed,Unable to connect to the server: dial tcp: lookup raw.githubusercontent.com on 10.129.251.125:53: server misbehaving,"Hello! Im having some problems with kubectl/kubernetes.

I have a kubernetes cluster running on Rancher, my customer have an internet proxy and racher has been setted to use this proxy, all the U.I are OK and my cluster is active.

Cluster Structure:

```
1 Rancher Node - RKE2 

Custom Cluster created by Rancher U.I 
1 master
4 workers
 ```
OS: RockyLinux
 ```
Linux vmrmmstnodehom01.lnx.ossclaro.srv 4.18.0-425.3.1.el8.x86_64 #1 SMP Wed Nov 9 20:13:27 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

 ```


So i've tried to install Longhorn with the kubectl command to do it and get this: 

```
kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.3.2/deploy/longhorn.yaml 
Unable to connect to the server: dial tcp: lookup raw.githubusercontent.com on 10.129.251.125:53: server misbehaving

```

I've defined the customer proxy on RKE2 installation and in rancher installation too, so i don't know if it is a proxy config here, happy to attach in this posts all the logs if necessary, thanks.",Anddiy,2023-01-09 19:37:55+00:00,[],2023-01-13 00:40:43+00:00,2023-01-13 00:40:42+00:00,https://github.com/kubernetes/kubernetes/issues/114926,"[('sig/node', 'Categorizes an issue or PR as relevant to SIG Node.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1376180846, 'issue_id': 1526199890, 'author': 'k8s-ci-robot', 'body': '@Anddiy: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 9, 19, 38, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 1379329689, 'issue_id': 1526199890, 'author': 'Anddiy', 'body': '/sig node', 'created_at': datetime.datetime(2023, 1, 11, 18, 42, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 1380610402, 'issue_id': 1526199890, 'author': 'amartyaa', 'body': 'Hey This feels like dns/connectivity issue as your bash is not able to resolve this given url, can you please try  to ""curl""  on that url from same cli.\r\n\r\n```\r\ncurl https://raw.githubusercontent.com/longhorn/longhorn/v1.3.2/deploy/longhorn.yaml \r\n```', 'created_at': datetime.datetime(2023, 1, 12, 15, 58, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 1380801624, 'issue_id': 1526199890, 'author': 'Anddiy', 'body': 'Hello @amartyaa , thanks for your attention. \r\n\r\nI\'ve tried to curl this url and got this: \r\n\r\n``` \r\ncurl https://raw.githubusercontent.com/longhorn/longhorn/v1.3.2/deploy/longhorn.yaml\r\ncurl: (6) Could not resolve host: raw.githubusercontent.com\r\n\r\n``` \r\n\r\nMy machine don\'t have a direct access to Internet, its a customer locked machine, if i need to use internet, i need to set a proxy to connect to it, so i setted the proxy on my machine and succesfully complete the curl command.\r\n\r\n``` \r\n[14:58] root@vmrmmstnodehom01 [~]:# export http_proxy=http://10.221.199.15:3128\r\n[14:58] root@vmrmmstnodehom01 [~]:# export https_proxy=$http_proxy\r\n[14:58] root@vmrmmstnodehom01 [~]:# printenv |grep proxy\r\nhttps_proxy=http://10.221.199.15:3128\r\nhttp_proxy=http://10.221.199.15:3128\r\n[14:58] root@vmrmmstnodehom01 [~]:# curl https://raw.githubusercontent.com/longhorn/longhorn/v1.3.2/deploy/longhorn.yaml\r\n---\r\n# Builtin: ""helm template"" does not respect --create-namespace\r\napiVersion: v1\r\nkind: Namespace\r\nmetadata:\r\n  name: longhorn-system\r\n---\r\n# Source: longhorn/templates/psp.yaml\r\napiVersion: policy/v1beta1\r\nkind: PodSecurityPolicy\r\nmetadata:\r\n  name: longhorn-psp\r\n  labels:\r\n    app.kubernetes.io/name: longhorn\r\n    app.kubernetes.io/instance: longhorn\r\n    app.kubernetes.io/version: v1.3.2\r\nspec:\r\n  privileged: true\r\n  allowPrivilegeEscalation: true\r\n  requiredDropCapabilities:\r\n  - NET_RAW\r\n  allowedCapabilities:\r\n  - SYS_ADMIN\r\n  hostNetwork: false\r\n  hostIPC: false\r\n  hostPID: true\r\n  runAsUser:\r\n    rule: RunAsAny\r\n  seLinux:\r\n    rule: RunAsAny\r\n\r\n\r\n``` \r\n\r\nBut now when i tried to run de apply command again, i got this: \r\n\r\n```\r\n kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.4.0/deploy/longhorn.yaml\r\nUnable to connect to the server: Service Unavailable\r\n```\r\n\r\nTried again to debug this with the -v 9 option of kubectl and get this:\r\n\r\n```\r\nkubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.4.0/deploy/longhorn.yaml -v 9\r\nI0112 15:05:36.951943 2513714 loader.go:372] Config loaded from file:  /root/.kube/config\r\nI0112 15:05:36.952837 2513714 round_trippers.go:466] curl -v -XGET  -H ""Accept: application/com.github.proto-openapi.spec.v2@v1.0+protobuf"" -H ""User-Agent: kubectl/v1.24.8 (linux/amd64) kubernetes/fdc7750"" -H ""Authorization: Bearer <masked>"" \'https://rancher.vmrminfnodhom01.lnx.ossclaro.srv/k8s/clusters/c-z5qmz/openapi/v2?timeout=32s\'\r\nI0112 15:05:37.069959 2513714 round_trippers.go:510] HTTP Trace: Dial to tcp:10.221.199.15:3128 succeed\r\nI0112 15:05:37.320618 2513714 round_trippers.go:553] GET https://rancher.vmrminfnodhom01.lnx.ossclaro.srv/k8s/clusters/c-z5qmz/openapi/v2?timeout=32s  in 367 milliseconds\r\nI0112 15:05:37.320690 2513714 round_trippers.go:570] HTTP Statistics: DNSLookup 0 ms Dial 116 ms TLSHandshake 0 ms Duration 367 ms\r\nI0112 15:05:37.320862 2513714 round_trippers.go:577] Response Headers:\r\nI0112 15:05:37.321055 2513714 helpers.go:264] Connection error: Get https://rancher.vmrminfnodhom01.lnx.ossclaro.srv/k8s/clusters/c-z5qmz/openapi/v2?timeout=32s: Service Unavailable\r\nUnable to connect to the server: Service Unavailable\r\n\r\n\r\n```\r\n\r\nI don\'t know much about connections when using kubectl, here i have a rke2 ( setted the proxy during the instalation ) so  can you help me with that?', 'created_at': datetime.datetime(2023, 1, 12, 18, 8, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 1381160414, 'issue_id': 1526199890, 'author': 'aojea', 'body': 'you should open the issue in the rancher project\r\n\r\nif this turns out to be a kubernetes problem you can always reopen\r\n\r\nThanks\r\n\r\n/close', 'created_at': datetime.datetime(2023, 1, 13, 0, 40, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 1381160494, 'issue_id': 1526199890, 'author': 'k8s-ci-robot', 'body': '@aojea: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114926#issuecomment-1381160414):\n\n>you should open the issue in the rancher project\r\n>\r\n>if this turns out to be a kubernetes problem you can always reopen\r\n>\r\n>Thanks\r\n>\r\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 13, 0, 40, 43, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-09 19:38:03 UTC): @Anddiy: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

Anddiy (Issue Creator) on (2023-01-11 18:42:22 UTC): /sig node

amartyaa on (2023-01-12 15:58:57 UTC): Hey This feels like dns/connectivity issue as your bash is not able to resolve this given url, can you please try  to ""curl""  on that url from same cli.

```
curl https://raw.githubusercontent.com/longhorn/longhorn/v1.3.2/deploy/longhorn.yaml 
```

Anddiy (Issue Creator) on (2023-01-12 18:08:36 UTC): Hello @amartyaa , thanks for your attention. 

I've tried to curl this url and got this: 

``` 
curl https://raw.githubusercontent.com/longhorn/longhorn/v1.3.2/deploy/longhorn.yaml
curl: (6) Could not resolve host: raw.githubusercontent.com

``` 

My machine don't have a direct access to Internet, its a customer locked machine, if i need to use internet, i need to set a proxy to connect to it, so i setted the proxy on my machine and succesfully complete the curl command.

``` 
[14:58] root@vmrmmstnodehom01 [~]:# export http_proxy=http://10.221.199.15:3128
[14:58] root@vmrmmstnodehom01 [~]:# export https_proxy=$http_proxy
[14:58] root@vmrmmstnodehom01 [~]:# printenv |grep proxy
https_proxy=http://10.221.199.15:3128
http_proxy=http://10.221.199.15:3128
[14:58] root@vmrmmstnodehom01 [~]:# curl https://raw.githubusercontent.com/longhorn/longhorn/v1.3.2/deploy/longhorn.yaml
---
# Builtin: ""helm template"" does not respect --create-namespace
apiVersion: v1
kind: Namespace
metadata:
  name: longhorn-system
---
# Source: longhorn/templates/psp.yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: longhorn-psp
  labels:
    app.kubernetes.io/name: longhorn
    app.kubernetes.io/instance: longhorn
    app.kubernetes.io/version: v1.3.2
spec:
  privileged: true
  allowPrivilegeEscalation: true
  requiredDropCapabilities:
  - NET_RAW
  allowedCapabilities:
  - SYS_ADMIN
  hostNetwork: false
  hostIPC: false
  hostPID: true
  runAsUser:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny


``` 

But now when i tried to run de apply command again, i got this: 

```
 kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.4.0/deploy/longhorn.yaml
Unable to connect to the server: Service Unavailable
```

Tried again to debug this with the -v 9 option of kubectl and get this:

```
kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.4.0/deploy/longhorn.yaml -v 9
I0112 15:05:36.951943 2513714 loader.go:372] Config loaded from file:  /root/.kube/config
I0112 15:05:36.952837 2513714 round_trippers.go:466] curl -v -XGET  -H ""Accept: application/com.github.proto-openapi.spec.v2@v1.0+protobuf"" -H ""User-Agent: kubectl/v1.24.8 (linux/amd64) kubernetes/fdc7750"" -H ""Authorization: Bearer <masked>"" 'https://rancher.vmrminfnodhom01.lnx.ossclaro.srv/k8s/clusters/c-z5qmz/openapi/v2?timeout=32s'
I0112 15:05:37.069959 2513714 round_trippers.go:510] HTTP Trace: Dial to tcp:10.221.199.15:3128 succeed
I0112 15:05:37.320618 2513714 round_trippers.go:553] GET https://rancher.vmrminfnodhom01.lnx.ossclaro.srv/k8s/clusters/c-z5qmz/openapi/v2?timeout=32s  in 367 milliseconds
I0112 15:05:37.320690 2513714 round_trippers.go:570] HTTP Statistics: DNSLookup 0 ms Dial 116 ms TLSHandshake 0 ms Duration 367 ms
I0112 15:05:37.320862 2513714 round_trippers.go:577] Response Headers:
I0112 15:05:37.321055 2513714 helpers.go:264] Connection error: Get https://rancher.vmrminfnodhom01.lnx.ossclaro.srv/k8s/clusters/c-z5qmz/openapi/v2?timeout=32s: Service Unavailable
Unable to connect to the server: Service Unavailable


```

I don't know much about connections when using kubectl, here i have a rke2 ( setted the proxy during the instalation ) so  can you help me with that?

aojea on (2023-01-13 00:40:38 UTC): you should open the issue in the rancher project

if this turns out to be a kubernetes problem you can always reopen

Thanks

/close

k8s-ci-robot on (2023-01-13 00:40:43 UTC): @aojea: Closing this issue.

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114926#issuecomment-1381160414):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

"
1525336172,issue,closed,completed,dynamic resource allocation: quota = limit number of resource claims per namespace,"As discussed [in the KEP](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/3063-dynamic-resource-allocation#user-permissions-and-quotas), most likely useful quotas need to be implemented by resource drivers because only they know how much resources get consumed by a specific claim.

What can be limited in Kubernetes itself is the number of ResourceClaims per namespace. For this, two new ResourceQuota resource names get added:

* ` resourceclaims` limits the number of ResourceClaim objects in a namespace across all resource class.
* `<resource-class-name>.resourceclass.node.k8s.io/resourceclaims` limits the number of ResourceClaim objects for the specific resource class.

",pohly,2023-01-09 10:49:00+00:00,['pohly'],2024-07-23 19:20:45+00:00,2024-07-23 19:20:45+00:00,https://github.com/kubernetes/kubernetes/issues/114916,"[('priority/important-soon', 'Must be staffed and worked on either currently, or very soon, ideally in time for the next release.'), ('priority/backlog', 'Higher priority than priority/awaiting-more-evidence.'), ('sig/node', 'Categorizes an issue or PR as relevant to SIG Node.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1375423842, 'issue_id': 1525336172, 'author': 'pohly', 'body': '/sig node\n/triage accepted\n/priority backlog\n\nBefore we start working on this, we should know how one resource driver handles quotas and whether this is needed.', 'created_at': datetime.datetime(2023, 1, 9, 10, 50, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 1713551378, 'issue_id': 1525336172, 'author': 'pohly', 'body': ""/priority important-soon\r\n\r\nLet's implement this for 1.29 because it should be in place before promoting DRA to beta."", 'created_at': datetime.datetime(2023, 9, 11, 9, 50, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 1713552225, 'issue_id': 1525336172, 'author': 'pohly', 'body': '/assign', 'created_at': datetime.datetime(2023, 9, 11, 9, 51, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 1899566068, 'issue_id': 1525336172, 'author': 'k8s-triage-robot', 'body': 'This issue is labeled with `priority/important-soon` but has not been updated in over 90 days, and should be re-triaged.\nImportant-soon issues must be staffed and worked on either currently, or very soon, ideally in time for the next release.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Deprioritize it with `/priority important-longterm` or `/priority backlog`\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted', 'created_at': datetime.datetime(2024, 1, 19, 3, 0, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2062914444, 'issue_id': 1525336172, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2024, 4, 18, 3, 11, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2118623468, 'issue_id': 1525336172, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2024, 5, 18, 3, 39, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2171832719, 'issue_id': 1525336172, 'author': 'vaibhav2107', 'body': '/remove-lifecycle rotten', 'created_at': datetime.datetime(2024, 6, 16, 20, 2, 36, tzinfo=datetime.timezone.utc)}]","pohly (Issue Creator) on (2023-01-09 10:50:32 UTC): /sig node
/triage accepted
/priority backlog

Before we start working on this, we should know how one resource driver handles quotas and whether this is needed.

pohly (Issue Creator) on (2023-09-11 09:50:48 UTC): /priority important-soon

Let's implement this for 1.29 because it should be in place before promoting DRA to beta.

pohly (Issue Creator) on (2023-09-11 09:51:07 UTC): /assign

k8s-triage-robot on (2024-01-19 03:00:46 UTC): This issue is labeled with `priority/important-soon` but has not been updated in over 90 days, and should be re-triaged.
Important-soon issues must be staffed and worked on either currently, or very soon, ideally in time for the next release.

You can:
- Confirm that this issue is still relevant with `/triage accepted` (org members only)
- Deprioritize it with `/priority important-longterm` or `/priority backlog`
- Close this issue with `/close`

For more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/

/remove-triage accepted

k8s-triage-robot on (2024-04-18 03:11:56 UTC): The Kubernetes project currently lacks enough contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle stale`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle stale

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-triage-robot on (2024-05-18 03:39:21 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle rotten

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

vaibhav2107 on (2024-06-16 20:02:36 UTC): /remove-lifecycle rotten

"
1525007796,issue,closed,completed,Nodeport is already allocated,"### What happened?

On creating following service

apiVersion: v1
kind: Service
metadata:
  name: coordinator-mimic
spec:
  type: LoadBalancer
  selector:
    app.kubernetes.io/name: hke-mimic
  ports:
    - port: 25025
      targetPort: 8000
      nodePort: 32444
`

Deleted it and created it again getting error
`The Service ""coordinator-mimic"" is invalid: spec.ports[0].nodePort: Invalid value: 32448: provided port is already allocated`
Verified port does not shows up as in use and lb in aws console is deleted.

![image](https://user-images.githubusercontent.com/9080276/211250746-301c9013-635a-4774-9884-14ffb46c61e3.png)



### What did you expect to happen?

It should be able to create service again and nodeport should get freed and become reusable.

### How can we reproduce it (as minimally and precisely as possible)?

mimic.yaml
`
apiVersion: v1
kind: Service
metadata:
  name: coordinator-mimic
spec:
  type: LoadBalancer
  selector:
    app.kubernetes.io/name: hke-mimic
  ports:
    - port: 25025
      targetPort: 8000
      nodePort: 32444
`
      
kubectl apply -f mimic.yaml
kubectl delete -f mimic.yaml
kubectl apply -f mimic.yaml
apply after delete errors out.

### Anything else we need to know?

I check 

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.23
</details>


### Cloud provider

<details>
AWS EKS
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```
 Amazon Linux 2   5.4.226-129.415.amzn2.x86_64
</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>
Docker
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",roylkng,2023-01-09 06:23:16+00:00,[],2023-01-09 21:47:15+00:00,2023-01-09 10:01:17+00:00,https://github.com/kubernetes/kubernetes/issues/114911,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('sig/network', 'Categorizes an issue or PR as relevant to SIG Network.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1375160921, 'issue_id': 1525007796, 'author': 'k8s-ci-robot', 'body': '@roylkng: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 9, 6, 23, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 1375201431, 'issue_id': 1525007796, 'author': 'uablrek', 'body': '/sig network', 'created_at': datetime.datetime(2023, 1, 9, 7, 23, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 1375260530, 'issue_id': 1525007796, 'author': 'uablrek', 'body': 'I suspect that this is an AWS problem, not a K8s problem. I will try to re-create this outside AWS.', 'created_at': datetime.datetime(2023, 1, 9, 8, 27, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 1375361551, 'issue_id': 1525007796, 'author': 'uablrek', 'body': 'I can\'t re-create this on a local K8s cluster;\r\n\r\n```\r\n# kubectl get svc alpine\r\nNAME     TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE\r\nalpine   LoadBalancer   12.0.199.250   <pending>     5001:32444/TCP   64s\r\n# kubectl delete -f $yamld/svc.yaml; kubectl apply -f $yamld/svc.yaml\r\nservice ""alpine"" deleted\r\nservice/alpine created\r\n# kubectl get svc alpine\r\nNAME     TYPE           CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE\r\nalpine   LoadBalancer   12.0.150.20   <pending>     5001:32444/TCP   1s\r\n```\r\nThe svc is deleted and re-created back-to-back without problems.\r\n\r\n<details><summary>service manifest</summary>\r\n\r\n```yaml\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: alpine\r\nspec:\r\n  ipFamilyPolicy: RequireDualStack\r\n  selector:\r\n    app: alpine\r\n  type: LoadBalancer\r\n  ports:\r\n  - port: 5001\r\n    name: nc\r\n    nodePort: 32444\r\n```\r\n\r\n</details>\r\n\r\nIn a public cloud, like AWS, things work differently. The coud-provider watches services with `type: LoadBalancer` and set\'s up external load balancing.\r\n\r\nBut this is not handled by Kubernetes, and thus not anything that can be fixed by any modification in K8s. So I close this issue as it is not a K8s bug.\r\n\r\n/close', 'created_at': datetime.datetime(2023, 1, 9, 10, 1, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 1375361698, 'issue_id': 1525007796, 'author': 'k8s-ci-robot', 'body': '@uablrek: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114911#issuecomment-1375361551):\n\n>I can\'t re-create this on a local K8s cluster;\r\n>\r\n>```\r\n># kubectl get svc alpine\r\n>NAME     TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE\r\n>alpine   LoadBalancer   12.0.199.250   <pending>     5001:32444/TCP   64s\r\n># kubectl delete -f $yamld/svc.yaml; kubectl apply -f $yamld/svc.yaml\r\n>service ""alpine"" deleted\r\n>service/alpine created\r\n># kubectl get svc alpine\r\n>NAME     TYPE           CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE\r\n>alpine   LoadBalancer   12.0.150.20   <pending>     5001:32444/TCP   1s\r\n>```\r\n>The svc is deleted and re-created back-to-back without problems.\r\n>\r\n><details><summary>service manifest</summary>\r\n>\r\n>```yaml\r\n>apiVersion: v1\r\n>kind: Service\r\n>metadata:\r\n>  name: alpine\r\n>spec:\r\n>  ipFamilyPolicy: RequireDualStack\r\n>  selector:\r\n>    app: alpine\r\n>  type: LoadBalancer\r\n>  ports:\r\n>  - port: 5001\r\n>    name: nc\r\n>    nodePort: 32444\r\n>```\r\n>\r\n></details>\r\n>\r\n>In a public cloud, like AWS, things work differently. The coud-provider watches services with `type: LoadBalancer` and set\'s up external load balancing.\r\n>\r\n>But this is not handled by Kubernetes, and thus not anything that can be fixed by any modification in K8s. So I close this issue as it is not a K8s bug.\r\n>\r\n>/close\r\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 9, 10, 1, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 1375822930, 'issue_id': 1525007796, 'author': 'uablrek', 'body': '@roylkng I should have checked cloud-providers more closely before answering. Sorry about that.\r\n\r\nChecking a bit more I found that there may be a way to use `finalizers` in the service to prevent this problem. I only found this for Azure;\r\nhttps://learn.microsoft.com/en-us/answers/questions/630765/aks-cluster-stop-deleting-load-balancer-event.html\r\n\r\nBut AWS probably have something similar. The finalizer should stall the delete of the service object until the cloud-provider load-balancer has been cleared. When the service is re-created it should get a new external address (which may be what you want?).', 'created_at': datetime.datetime(2023, 1, 9, 15, 41, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 1375826564, 'issue_id': 1525007796, 'author': 'uablrek', 'body': '(but it still not a K8s bug :smile: )', 'created_at': datetime.datetime(2023, 1, 9, 15, 42, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 1376372740, 'issue_id': 1525007796, 'author': 'aojea', 'body': 'or check if it was allocated by another Service 🤔', 'created_at': datetime.datetime(2023, 1, 9, 21, 47, 15, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-09 06:23:24 UTC): @roylkng: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

uablrek on (2023-01-09 07:23:57 UTC): /sig network

uablrek on (2023-01-09 08:27:52 UTC): I suspect that this is an AWS problem, not a K8s problem. I will try to re-create this outside AWS.

uablrek on (2023-01-09 10:01:12 UTC): I can't re-create this on a local K8s cluster;

```
# kubectl get svc alpine
NAME     TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
alpine   LoadBalancer   12.0.199.250   <pending>     5001:32444/TCP   64s
# kubectl delete -f $yamld/svc.yaml; kubectl apply -f $yamld/svc.yaml
service ""alpine"" deleted
service/alpine created
# kubectl get svc alpine
NAME     TYPE           CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
alpine   LoadBalancer   12.0.150.20   <pending>     5001:32444/TCP   1s
```
The svc is deleted and re-created back-to-back without problems.

<details><summary>service manifest</summary>

```yaml
apiVersion: v1
kind: Service
metadata:
  name: alpine
spec:
  ipFamilyPolicy: RequireDualStack
  selector:
    app: alpine
  type: LoadBalancer
  ports:
  - port: 5001
    name: nc
    nodePort: 32444
```

</details>

In a public cloud, like AWS, things work differently. The coud-provider watches services with `type: LoadBalancer` and set's up external load balancing.

But this is not handled by Kubernetes, and thus not anything that can be fixed by any modification in K8s. So I close this issue as it is not a K8s bug.

/close

k8s-ci-robot on (2023-01-09 10:01:18 UTC): @uablrek: Closing this issue.

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114911#issuecomment-1375361551):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

uablrek on (2023-01-09 15:41:09 UTC): @roylkng I should have checked cloud-providers more closely before answering. Sorry about that.

Checking a bit more I found that there may be a way to use `finalizers` in the service to prevent this problem. I only found this for Azure;
https://learn.microsoft.com/en-us/answers/questions/630765/aks-cluster-stop-deleting-load-balancer-event.html

But AWS probably have something similar. The finalizer should stall the delete of the service object until the cloud-provider load-balancer has been cleared. When the service is re-created it should get a new external address (which may be what you want?).

uablrek on (2023-01-09 15:42:37 UTC): (but it still not a K8s bug :smile: )

aojea on (2023-01-09 21:47:15 UTC): or check if it was allocated by another Service 🤔

"
1524875871,issue,closed,completed,kubectl warning on autoscaling/v2 hpa,"### What happened?

- Create a `HorizontalPodAutoscaler` using the `autoscaling/v2` API
- Perform a describe using kubectl

### What did you expect to happen?

Proper HPA configuration without any warnings/errors. Received a warning instead:

```
autoscaling/v2beta2 HorizontalPodAutoscaler is deprecated in v1.23+, unavailable in v1.26+; use autoscaling/v2 HorizontalPodAutoscaler
```

### How can we reproduce it (as minimally and precisely as possible)?

- Create an HPA using autoscaling/v2 apiGroup
- Perform a kubectl describek

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""25"", GitVersion:""v1.25.2"", GitCommit:""5835544ca568b757a8ecae5c153f317e5736700e"", GitTreeState:""clean"", BuildDate:""2022-09-21T14:25:45Z"", GoVersion:""go1.19.1"", Compiler:""gc"", Platform:""darwin/amd64""}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:""1"", Minor:""22+"", GitVersion:""v1.22.15-eks-fb459a0"", GitCommit:""be82fa628e60d024275efaa239bfe53a9119c2d9"", GitTreeState:""clean"", BuildDate:""2022-10-24T20:33:23Z"", GoVersion:""go1.16.15"", Compiler:""gc"", Platform:""linux/amd64""}
```

</details>


### Cloud provider

<details>
N/A
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```
N/A
</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>
N/A
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
N/A
</details>",a7i,2023-01-09 03:34:32+00:00,['a7i'],2023-01-17 10:54:34+00:00,2023-01-17 10:54:34+00:00,https://github.com/kubernetes/kubernetes/issues/114908,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('sig/cli', 'Categorizes an issue or PR as relevant to SIG CLI.'), ('triage/accepted', 'Indicates an issue or PR is ready to be actively worked on.')]","[{'comment_id': 1375067200, 'issue_id': 1524875871, 'author': 'a7i', 'body': '/assign\r\n/sig cli', 'created_at': datetime.datetime(2023, 1, 9, 3, 34, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 1385114135, 'issue_id': 1524875871, 'author': 'soltysh', 'body': '/triage accepted', 'created_at': datetime.datetime(2023, 1, 17, 9, 47, 16, tzinfo=datetime.timezone.utc)}]","a7i (Issue Creator) on (2023-01-09 03:34:51 UTC): /assign
/sig cli

soltysh on (2023-01-17 09:47:16 UTC): /triage accepted

"
1524504542,issue,closed,completed,pod_startup_latency_tracker doesn't track the lastFinishedPulling time,"It can be observed in any kubelet log, just pick one from a random job in CI, per example

```sh
$ wget https://storage.googleapis.com/kubernetes-jenkins/pr-logs/pull/114703/pull-kubernetes-e2e-gce/1608841379780235264/artifacts/e2e-022435249c-674b9-minion-group-gdj4/kubelet.log

$ grep tracker kubelet.log  | grep -v "" firstStartedPulling=\""0001-01-01"" | grep Observed
Dec 30 15:33:06.941831 e2e-022435249c-674b9-minion-group-gdj4 kubelet[8362]: I1230 15:33:06.941726    8362 pod_startup_latency_tracker.go:102] ""Observed pod startup duration"" pod=""kube-system/metadata-proxy-v0.1-d59wc"" podStartSLOduration=-9.223372025913092e+09 pod.CreationTimestamp=""2022-12-30 15:32:56 +0000 UTC"" firstStartedPulling=""2022-12-30 15:32:58.409218371 +0000 UTC m=+2.180058389"" lastFinishedPulling=""0001-01-01 00:00:00 +0000 UTC"" observedRunningTime=""2022-12-30 15:33:01.628463642 +0000 UTC m=+5.399303661"" watchObservedRunningTime=""2022-12-30 15:33:06.941684349 +0000 UTC m=+10.712524397""
Dec 30 15:33:13.374424 e2e-022435249c-674b9-minion-group-gdj4 kubelet[8362]: I1230 15:33:13.374400    8362 pod_startup_latency_tracker.go:102] ""Observed pod startup duration"" pod=""kube-system/kube-dns-autoscaler-5f6455f985-n4pqn"" podStartSLOduration=-9.223372017480421e+09 pod.CreationTimestamp=""2022-12-30 15:32:54 +0000 UTC"" firstStartedPulling=""2022-12-30 15:33:07.759304457 +0000 UTC m=+11.530144475"" lastFinishedPulling=""0001-01-01 00:00:00 +0000 UTC"" observedRunningTime=""2022-12-30 15:33:13.267182708 +0000 UTC m=+17.038022722"" watchObservedRunningTime=""2022-12-30 15:33:13.374355038 +0000 UTC m=+17.145195062""
Dec 30 15:33:13.375379 e2e-022435249c-674b9-minion-group-gdj4 kubelet[8362]: I1230 15:33:13.375359    8362 pod_startup_latency_tracker.go:102] ""Observed pod startup duration"" pod=""kube-system/konnectivity-agent-gnc9k"" podStartSLOduration=-9.223372029479458e+09 pod.CreationTimestamp=""2022-12-30 15:33:06 +0000 UTC"" firstStartedPulling=""2022-12-30 15:33:09.258791695 +0000 UTC m=+13.029631711"" lastFinishedPulling=""0001-01-01 00:00:00 +0000 UTC"" observedRunningTime=""2022-12-30 15:33:13.375009262 +0000 UTC m=+17.145849275"" watchObservedRunningTime=""2022-12-30 15:33:13.375317944 +0000 UTC m=+17.146157970""
```

The `lastFinishedPulling` time is always `0001-01-01 00:00:00 +0000`, that is the golang zero time IIRC.
In addition, when the image is already present, neither `firstStartedPulling` and `lastFinishedPulling` are set.

I think that the fix has to be something like this

```diff
diff --git a/pkg/kubelet/images/image_manager.go b/pkg/kubelet/images/image_manager.go
index 3fbec1f2b56..863e8ccfa98 100644
--- a/pkg/kubelet/images/image_manager.go
+++ b/pkg/kubelet/images/image_manager.go
@@ -134,6 +134,8 @@ func (m *imageManager) EnsureImageExists(ctx context.Context, pod *v1.Pod, conta
        if !shouldPullImage(container, present) {
                if present {
                        msg := fmt.Sprintf(""Container image %q already present on machine"", container.Image)
+                       m.podPullingTimeRecorder.RecordImageStartedPulling(pod.UID)
+                       m.podPullingTimeRecorder.RecordImageFinishedPulling(pod.UID)
                        m.logIt(ref, v1.EventTypeNormal, events.PulledImage, logPrefix, msg, klog.Info)
                        return imageRef, """", nil
                }
@@ -164,6 +166,7 @@ func (m *imageManager) EnsureImageExists(ctx context.Context, pod *v1.Pod, conta
 
                return """", imagePullResult.err.Error(), ErrImagePull
        }
+       m.podPullingTimeRecorder.RecordImageFinishedPulling(pod.UID)
```

but this means that we are lacking testing on these, feature, I'd expected to see at least 2 new tests in

`pkg/kubelet/util/pod_startup_latency_tracker_test.go`
`pkg/kubelet/images/image_manager_test.go`

/sig node
/kind bug
/help

/cc @bobbypage @azylinski ",aojea,2023-01-08 13:24:50+00:00,['TommyStarK'],2023-03-14 16:39:08+00:00,2023-03-14 16:39:08+00:00,https://github.com/kubernetes/kubernetes/issues/114903,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('sig/node', 'Categorizes an issue or PR as relevant to SIG Node.'), ('help wanted', 'Denotes an issue that needs help from a contributor. Must meet ""help wanted"" guidelines.'), ('priority/important-longterm', 'Important over the long term, but may not be staffed and/or may need multiple releases to complete.'), ('triage/accepted', 'Indicates an issue or PR is ready to be actively worked on.')]","[{'comment_id': 1374835470, 'issue_id': 1524504542, 'author': 'k8s-ci-robot', 'body': '@aojea: \n\tThis request has been marked as needing help from a contributor.\n\n### Guidelines\nPlease ensure that the issue body includes answers to the following questions:\n- Why are we solving this issue?\n- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?\n- Does this issue have zero to low barrier of entry?\n- How can the assignee reach out to you for help?\n\n\nFor more details on the requirements of such an issue, please see [here](https://git.k8s.io/community/contributors/guide/help-wanted.md) and ensure that they are met.\n\nIf this request no longer meets these requirements, the label can be removed\nby commenting with the `/remove-help` command.\n\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114903):\n\n>It can be observed in any kubelet log, just pick one from a random job in CI, per example\r\n>\r\n>```sh\r\n>$ wget https://storage.googleapis.com/kubernetes-jenkins/pr-logs/pull/114703/pull-kubernetes-e2e-gce/1608841379780235264/artifacts/e2e-022435249c-674b9-minion-group-gdj4/kubelet.log\r\n>\r\n>$ grep tracker kubelet.log  | grep -v "" firstStartedPulling=\\""0001-01-01"" | grep Observed\r\n>Dec 30 15:33:06.941831 e2e-022435249c-674b9-minion-group-gdj4 kubelet[8362]: I1230 15:33:06.941726    8362 pod_startup_latency_tracker.go:102] ""Observed pod startup duration"" pod=""kube-system/metadata-proxy-v0.1-d59wc"" podStartSLOduration=-9.223372025913092e+09 pod.CreationTimestamp=""2022-12-30 15:32:56 +0000 UTC"" firstStartedPulling=""2022-12-30 15:32:58.409218371 +0000 UTC m=+2.180058389"" lastFinishedPulling=""0001-01-01 00:00:00 +0000 UTC"" observedRunningTime=""2022-12-30 15:33:01.628463642 +0000 UTC m=+5.399303661"" watchObservedRunningTime=""2022-12-30 15:33:06.941684349 +0000 UTC m=+10.712524397""\r\n>Dec 30 15:33:13.374424 e2e-022435249c-674b9-minion-group-gdj4 kubelet[8362]: I1230 15:33:13.374400    8362 pod_startup_latency_tracker.go:102] ""Observed pod startup duration"" pod=""kube-system/kube-dns-autoscaler-5f6455f985-n4pqn"" podStartSLOduration=-9.223372017480421e+09 pod.CreationTimestamp=""2022-12-30 15:32:54 +0000 UTC"" firstStartedPulling=""2022-12-30 15:33:07.759304457 +0000 UTC m=+11.530144475"" lastFinishedPulling=""0001-01-01 00:00:00 +0000 UTC"" observedRunningTime=""2022-12-30 15:33:13.267182708 +0000 UTC m=+17.038022722"" watchObservedRunningTime=""2022-12-30 15:33:13.374355038 +0000 UTC m=+17.145195062""\r\n>Dec 30 15:33:13.375379 e2e-022435249c-674b9-minion-group-gdj4 kubelet[8362]: I1230 15:33:13.375359    8362 pod_startup_latency_tracker.go:102] ""Observed pod startup duration"" pod=""kube-system/konnectivity-agent-gnc9k"" podStartSLOduration=-9.223372029479458e+09 pod.CreationTimestamp=""2022-12-30 15:33:06 +0000 UTC"" firstStartedPulling=""2022-12-30 15:33:09.258791695 +0000 UTC m=+13.029631711"" lastFinishedPulling=""0001-01-01 00:00:00 +0000 UTC"" observedRunningTime=""2022-12-30 15:33:13.375009262 +0000 UTC m=+17.145849275"" watchObservedRunningTime=""2022-12-30 15:33:13.375317944 +0000 UTC m=+17.146157970""\r\n>```\r\n>\r\n>The `lastFinishedPulling` time is always `0001-01-01 00:00:00 +0000`, that is the golang zero time IIRC.\r\n>In addition, when the image is already present, neither `firstStartedPulling` and `lastFinishedPulling` are set.\r\n>\r\n>I think that the fix has to be something like this\r\n>\r\n>```diff\r\n>diff --git a/pkg/kubelet/images/image_manager.go b/pkg/kubelet/images/image_manager.go\r\n>index 3fbec1f2b56..863e8ccfa98 100644\r\n>--- a/pkg/kubelet/images/image_manager.go\r\n>+++ b/pkg/kubelet/images/image_manager.go\r\n>@@ -134,6 +134,8 @@ func (m *imageManager) EnsureImageExists(ctx context.Context, pod *v1.Pod, conta\r\n>        if !shouldPullImage(container, present) {\r\n>                if present {\r\n>                        msg := fmt.Sprintf(""Container image %q already present on machine"", container.Image)\r\n>+                       m.podPullingTimeRecorder.RecordImageStartedPulling(pod.UID)\r\n>+                       m.podPullingTimeRecorder.RecordImageFinishedPulling(pod.UID)\r\n>                        m.logIt(ref, v1.EventTypeNormal, events.PulledImage, logPrefix, msg, klog.Info)\r\n>                        return imageRef, """", nil\r\n>                }\r\n>@@ -164,6 +166,7 @@ func (m *imageManager) EnsureImageExists(ctx context.Context, pod *v1.Pod, conta\r\n> \r\n>                return """", imagePullResult.err.Error(), ErrImagePull\r\n>        }\r\n>+       m.podPullingTimeRecorder.RecordImageFinishedPulling(pod.UID)\r\n>aojea@aojea:~/src/kubernetes$ git diff\r\n>diff --git a/pkg/kubelet/images/image_manager.go b/pkg/kubelet/images/image_manager.go\r\n>index 3fbec1f2b56..863e8ccfa98 100644\r\n>--- a/pkg/kubelet/images/image_manager.go\r\n>+++ b/pkg/kubelet/images/image_manager.go\r\n>@@ -134,6 +134,8 @@ func (m *imageManager) EnsureImageExists(ctx context.Context, pod *v1.Pod, conta\r\n>        if !shouldPullImage(container, present) {\r\n>                if present {\r\n>                        msg := fmt.Sprintf(""Container image %q already present on machine"", container.Image)\r\n>+                       m.podPullingTimeRecorder.RecordImageStartedPulling(pod.UID)\r\n>+                       m.podPullingTimeRecorder.RecordImageFinishedPulling(pod.UID)\r\n>                        m.logIt(ref, v1.EventTypeNormal, events.PulledImage, logPrefix, msg, klog.Info)\r\n>                        return imageRef, """", nil\r\n>                }\r\n>@@ -164,6 +166,7 @@ func (m *imageManager) EnsureImageExists(ctx context.Context, pod *v1.Pod, conta\r\n> \r\n>                return """", imagePullResult.err.Error(), ErrImagePull\r\n>        }\r\n>+       m.podPullingTimeRecorder.RecordImageFinishedPulling(pod.UID)\r\n>        m.logIt(ref, v1.EventTypeNormal, events.PulledImage, logPrefix, fmt.Sprintf(""Successfully pulled image %q in %v (%v including waiting)"", container.Image, imagePullResult.pullDuration, time.Since(startTime)), klog.Info)\r\n>        m.backOff.GC()\r\n>        return imagePullResult.imageRef, """", nil\r\n>```\r\n>\r\n>but this means that we are lacking testing on these, feature, I\'d expected to see at least 2 new tests in\r\n>\r\n>`pkg/kubelet/util/pod_startup_latency_tracker_test.go`\r\n>`pkg/kubelet/images/image_manager_test.go`\r\n>\r\n>/sig node\r\n>/kind bug\r\n>/help\r\n>\r\n>/cc @bobbypage @azylinski \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 8, 13, 24, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374838567, 'issue_id': 1524504542, 'author': 'TommyStarK', 'body': '/assign', 'created_at': datetime.datetime(2023, 1, 8, 13, 39, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 1387555070, 'issue_id': 1524504542, 'author': 'SergeyKanzhelev', 'body': '/triage accepted\r\n/priority important-longterm', 'created_at': datetime.datetime(2023, 1, 18, 18, 42, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 1387555989, 'issue_id': 1524504542, 'author': 'SergeyKanzhelev', 'body': '/assign @TommyStarK \r\n\r\nsince you are working on it', 'created_at': datetime.datetime(2023, 1, 18, 18, 42, 23, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-08 13:24:52 UTC): @aojea: 
	This request has been marked as needing help from a contributor.

### Guidelines
Please ensure that the issue body includes answers to the following questions:
- Why are we solving this issue?
- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?
- Does this issue have zero to low barrier of entry?
- How can the assignee reach out to you for help?


For more details on the requirements of such an issue, please see [here](https://git.k8s.io/community/contributors/guide/help-wanted.md) and ensure that they are met.

If this request no longer meets these requirements, the label can be removed
by commenting with the `/remove-help` command.


<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114903):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

TommyStarK (Assginee) on (2023-01-08 13:39:09 UTC): /assign

SergeyKanzhelev on (2023-01-18 18:42:03 UTC): /triage accepted
/priority important-longterm

SergeyKanzhelev on (2023-01-18 18:42:23 UTC): /assign @TommyStarK 

since you are working on it

"
1523633754,issue,closed,completed,creat a issue for test,"### What happened?

just for test

### What did you expect to happen?

just for test

### How can we reproduce it (as minimally and precisely as possible)?

just for test

### Anything else we need to know?

_No response_

### Kubernetes version

<details>
v1.26.0
```console
$ kubectl version
# paste output here
```

</details>


### Cloud provider

<details>
just for test 
</details>


### OS version

<details>
Linux 5.15.0-57-generic #63-Ubuntu SMP Thu Nov 24 13:43:17 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",jinguang-dong,2023-01-07 09:04:16+00:00,[],2023-01-07 09:06:23+00:00,2023-01-07 09:06:23+00:00,https://github.com/kubernetes/kubernetes/issues/114893,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('sig/docs', 'Categorizes an issue or PR as relevant to SIG Docs.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1374421233, 'issue_id': 1523633754, 'author': 'k8s-ci-robot', 'body': '@jinguang-dong: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 7, 9, 4, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374421572, 'issue_id': 1523633754, 'author': 'jinguang-dong', 'body': '/sig Docs', 'created_at': datetime.datetime(2023, 1, 7, 9, 6, 13, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-07 09:04:24 UTC): @jinguang-dong: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

jinguang-dong (Issue Creator) on (2023-01-07 09:06:13 UTC): /sig Docs

"
1523585762,issue,closed,completed,ci-benchmark-scheduler-perf-master is failing,"### Which jobs are failing?

ci-benchmark-scheduler-perf-master

### Which tests are failing?

scheduler_perf

### Since when has it been failing?

This Job was keep failing by another issue until https://github.com/kubernetes/kubernetes/pull/114796. So, we don't know when the current problem is started.

### Testgrid link

https://testgrid.k8s.io/sig-scalability-benchmarks#scheduler-perf

### Reason for failure (if possible)

We're observing several errors on scheduler_perf benchmark job now.
https://storage.googleapis.com/kubernetes-jenkins/logs/ci-benchmark-scheduler-perf-master/1610954337838698496/build-log.txt


- `Error while deleting Node: client rate limiter Wait returned an error: context canceled`
  - will be fixed in https://github.com/kubernetes/kubernetes/pull/114843.
- `Process did not finish before 1h55m0s timeout`, `Could not interrupt process after timeout`
  - will be fixed in https://github.com/kubernetes/test-infra/pull/28368
  - But, if #114843 contributes to reducing the time, we may not need ^.
- `unexpected number of goroutines: before: 2 after 43`


### Anything else we need to know?

_No response_

### Relevant SIG(s)

/sig scheduling

/assign @sanposhiho
/assign @kerthcet ",sanposhiho,2023-01-07 07:54:42+00:00,"['kerthcet', 'sanposhiho']",2023-05-26 06:57:08+00:00,2023-05-26 06:57:07+00:00,https://github.com/kubernetes/kubernetes/issues/114890,"[('sig/scheduling', 'Categorizes an issue or PR as relevant to SIG Scheduling.'), ('kind/failing-test', 'Categorizes issue or PR as related to a consistently or frequently failing test.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1374408993, 'issue_id': 1523585762, 'author': 'k8s-ci-robot', 'body': '@sanposhiho: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 7, 7, 54, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374409251, 'issue_id': 1523585762, 'author': 'sanposhiho', 'body': ""The direct reason why the `ci-benchmark-scheduler-perf-master` is marked as fail is `unexpected number of goroutines: before: 2 after 43`.\r\nhttps://github.com/kubernetes/kubernetes/blob/643353abfe180ded1fae1fe6fd48d3930d26a9e4/test/integration/framework/etcd.go#L217\r\n\r\nI'm not sure if it can be fixed by https://github.com/kubernetes/kubernetes/pull/114843 or not. Let's see.."", 'created_at': datetime.datetime(2023, 1, 7, 7, 56, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 1404131580, 'issue_id': 1523585762, 'author': 'pohly', 'body': 'Here\'s a patch that makes the reporting of leaked goroutines more useful:\r\n\r\n```patch\r\ncommit 469526fab53efb4cc26fb70b610239a232890a73\r\nAuthor: Patrick Ohly <patrick.ohly@intel.com>\r\nDate:   Wed Jan 25 17:59:10 2023 +0100\r\n\r\n    test/integration: improve output when etcd detects leaked goroutines\r\n    \r\n    With the current output one was only told how many additional goroutines were\r\n    running after a test and a dump of the current ones. But without knowing what\r\n    was running initially, that information was not enough.\r\n    \r\n    Now a diff that shows changes in the stack traces before and after a test is\r\n    shown.\r\n    \r\n    This currently gets triggered and leads to output like this:\r\n    \r\n    F0125 17:57:26.577440  462703 etcd.go:216] unexpected number of goroutines: before: 3, after: 9\r\n    \xa0\xa0(\r\n    \xa0\xa0      """"""\r\n    \xa0\xa0      goroutine 1 [running]:\r\n    -\xa0      k8s.io/kubernetes/test/integration/framework.stacks(0x10000)\r\n    +\xa0      k8s.io/kubernetes/test/integration/framework.stacks(0x100000)\r\n    \xa0\xa0              /nvme/gopath/src/k8s.io/kubernetes/test/integration/framework/etcd.go:224 +0x8a\r\n    \xa0\xa0      k8s.io/kubernetes/test/integration/framework.EtcdMain(0xc00139fe98)\r\n    -\xa0              /nvme/gopath/src/k8s.io/kubernetes/test/integration/framework/etcd.go:189 +0x85\r\n    +\xa0              /nvme/gopath/src/k8s.io/kubernetes/test/integration/framework/etcd.go:215 +0x1c5\r\n    \xa0\xa0      k8s.io/kubernetes/test/integration/scheduler_perf.TestMain(0xc0011680a0)\r\n    \xa0\xa0              /nvme/gopath/src/k8s.io/kubernetes/test/integration/scheduler_perf/main_test.go:58 +0x1c8\r\n    \xa0\xa0      ... // 7 identical lines\r\n    \xa0\xa0              /nvme/gopath/src/k8s.io/kubernetes/vendor/go.opencensus.io/stats/view/worker.go:34 +0x8d\r\n    \xa0\xa0\r\n    -\xa0      goroutine 203 [runnable]:\r\n    +\xa0      goroutine 203 [select]:\r\n    \xa0\xa0      k8s.io/klog/v2.(*flushDaemon).run.func1()\r\n    -\xa0              /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/klog/v2/klog.go:1131\r\n    +\xa0              /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/klog/v2/klog.go:1135 +0x11e\r\n    \xa0\xa0      created by k8s.io/klog/v2.(*flushDaemon).run\r\n    \xa0\xa0              /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/klog/v2/klog.go:1131 +0x17b\r\n    +\xa0\r\n    +\xa0      goroutine 1514 [select]:\r\n    +\xa0      k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x0?, {0x5906aa0, 0xc005ec4030}, 0x1, 0xc0000863c0)\r\n    +\xa0              /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:169 +0x135\r\n    +\xa0      k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x0?, 0xdf8475800, 0x0, 0x0?, 0x0?)\r\n    +\xa0              /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:135 +0x89\r\n    +\xa0      k8s.io/apimachinery/pkg/util/wait.Until(...)\r\n    +\xa0              /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:92\r\n    +\xa0      k8s.io/apimachinery/pkg/util/wait.Forever(0x0?, 0x0?)\r\n    +\xa0              /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:83 +0x28\r\n    +\xa0      created by k8s.io/apiserver/pkg/server/healthz.(*log).Check.func1\r\n    +\xa0              /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/server/healthz/healthz.go:74 +0xe5\r\n    +\xa0\r\n    +\xa0      goroutine 2473 [chan receive]:\r\n    +\xa0      k8s.io/apimachinery/pkg/watch.(*Broadcaster).loop(0xc0031c0d70)\r\n    +\xa0              /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/watch/mux.go:268 +0x65\r\n    +\xa0      created by k8s.io/apimachinery/pkg/watch.NewBroadcaster\r\n    +\xa0              /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/watch/mux.go:77 +0x116\r\n    +\xa0\r\n    +\xa0      goroutine 5540 [syscall]:\r\n    +\xa0      os/signal.signal_recv()\r\n    +\xa0              /nvme/gopath/go-1.19/src/runtime/sigqueue.go:152 +0x2f\r\n    +\xa0      os/signal.loop()\r\n    +\xa0              /nvme/gopath/go-1.19/src/os/signal/signal_unix.go:23 +0x19\r\n    +\xa0      created by os/signal.Notify.func1.1\r\n    +\xa0              /nvme/gopath/go-1.19/src/os/signal/signal.go:151 +0x2a\r\n    +\xa0\r\n    +\xa0      goroutine 5956 [chan receive]:\r\n    +\xa0      k8s.io/apimachinery/pkg/watch.(*Broadcaster).loop(0xc004eb24b0)\r\n    +\xa0              /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/watch/mux.go:268 +0x65\r\n    +\xa0      created by k8s.io/apimachinery/pkg/watch.NewLongQueueBroadcaster\r\n    +\xa0              /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/watch/mux.go:93 +0x116\r\n    +\xa0\r\n    +\xa0      goroutine 5957 [chan receive]:\r\n    +\xa0      k8s.io/client-go/tools/record.(*eventBroadcasterImpl).StartEventWatcher.func1()\r\n    +\xa0              /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/client-go/tools/record/event.go:307 +0x73\r\n    +\xa0      created by k8s.io/client-go/tools/record.(*eventBroadcasterImpl).StartEventWatcher\r\n    +\xa0              /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/client-go/tools/record/event.go:305 +0x13d\r\n    +\xa0\r\n    +\xa0      goroutine 5958 [chan receive]:\r\n    +\xa0      k8s.io/client-go/tools/record.(*eventBroadcasterImpl).StartEventWatcher.func1()\r\n    +\xa0              /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/client-go/tools/record/event.go:307 +0x73\r\n    +\xa0      created by k8s.io/client-go/tools/record.(*eventBroadcasterImpl).StartEventWatcher\r\n    +\xa0              /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/client-go/tools/record/event.go:305 +0x13d\r\n    \xa0\xa0      ... // 1 identical line\r\n    \xa0\xa0      """"""\r\n    \xa0\xa0)\r\n\r\ndiff --git a/test/integration/framework/etcd.go b/test/integration/framework/etcd.go\r\nindex 966f6538394..1eaf0f36b2e 100644\r\n--- a/test/integration/framework/etcd.go\r\n+++ b/test/integration/framework/etcd.go\r\n@@ -29,6 +29,7 @@ import (\r\n \t""syscall""\r\n \t""time""\r\n \r\n+\t""github.com/google/go-cmp/cmp""\r\n \t""google.golang.org/grpc/grpclog""\r\n \t""k8s.io/klog/v2""\r\n \r\n@@ -185,7 +186,7 @@ func EtcdMain(tests func() int) {\r\n \t// Bail out early when -help was given as parameter.\r\n \tflag.Parse()\r\n \r\n-\tbefore := runtime.NumGoroutine()\r\n+\tbefore, stacktracesBefore := stacks(1 << 16)\r\n \tstop, err := startEtcd()\r\n \tif err != nil {\r\n \t\tklog.Fatalf(""cannot run integration tests: unable to start etcd: %v"", err)\r\n@@ -211,14 +212,19 @@ func EtcdMain(tests func() int) {\r\n \t// It generally takes visibly less than 1s to finish all goroutines.\r\n \t// But we keep the limit higher to account for cpu-starved environments.\r\n \tif err := wait.Poll(100*time.Millisecond, 5*time.Second, checkNumberOfGoroutines); err != nil {\r\n-\t\tafter := runtime.NumGoroutine()\r\n-\t\tstacktraces := make([]byte, 1<<20)\r\n-\t\truntime.Stack(stacktraces, true)\r\n-\t\tklog.Fatalf(""unexpected number of goroutines: before: %d after %d\\n%sd"", before, after, string(stacktraces))\r\n+\t\tafter, stacktracesAfter := stacks(1 << 20)\r\n+\t\tklog.Fatalf(""unexpected number of goroutines: before: %d, after: %d\\n%s"", before, after, cmp.Diff(stacktracesBefore, stacktracesAfter))\r\n \t}\r\n \tos.Exit(result)\r\n }\r\n \r\n+func stacks(bufferSize int) (int, string) {\r\n+\tnum := runtime.NumGoroutine()\r\n+\tstacktraces := make([]byte, bufferSize)\r\n+\tlen := runtime.Stack(stacktraces, true)\r\n+\treturn num, string(stacktraces[:len])\r\n+}\r\n+\r\n // GetEtcdURL returns the URL of the etcd instance started by EtcdMain.\r\n func GetEtcdURL() string {\r\n \treturn etcdURL\r\n```\r\n\r\nI\'ll put that into a PR at some point, either stand-alone or in combination with some other cleanup commits.', 'created_at': datetime.datetime(2023, 1, 25, 19, 35, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 1410460659, 'issue_id': 1523585762, 'author': 'sanposhiho', 'body': 'https://github.com/kubernetes/kubernetes/pull/114843 got merged\r\n\r\nTwo remaining issues and fixes:\r\n- [x] goroutine leak: #115423 @pohly \r\n- [ ] Process did not finish before 1h55m0s timeout, Could not interrupt process after timeout: https://github.com/kubernetes/test-infra/pull/28368 @sanposhiho', 'created_at': datetime.datetime(2023, 1, 31, 14, 31, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 1415185315, 'issue_id': 1523585762, 'author': 'kerthcet', 'body': 'The goroutine leakage is solved, but I still saw perf-test occasionally failed for time out, instead of increasing the TTL directly, or maybe we can measure the durations for each test and find out the slowest one and maybe the reason, I think this can also help to find some potential performance issues u-turn.', 'created_at': datetime.datetime(2023, 2, 3, 7, 8, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 1415368625, 'issue_id': 1523585762, 'author': 'sanposhiho', 'body': ""> The goroutine leakage is solved\r\n\r\n🎉 \r\n\r\n----\r\n\r\nSeems sometimes timeout-ed and sometimes passes.\r\nFor me, it makes sense to just increase the TTL given we've got additional performance tests in https://github.com/kubernetes/kubernetes/pull/113615. (Probably my current proposed `3h` in [the PR](https://github.com/kubernetes/test-infra/pull/28368) is too much though.)"", 'created_at': datetime.datetime(2023, 2, 3, 8, 44, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 1429538398, 'issue_id': 1523585762, 'author': 'sanposhiho', 'body': ""Another problem.\r\nI believe the [perf-dash](https://perf-dash.k8s.io/#/?jobname=scheduler-perf-benchmark&metriccategoryname=Scheduler&metricname=BenchmarkResults&benchmark=BenchmarkPerfScheduling%2FSchedulingWithNodeInclusionPolicy%2F5000Nodes-8&metricName=time) should show the scheduler's metric result as BenchmarkPerfResults but currently not.\r\n\r\nhttps://github.com/kubernetes/perf-tests/blob/master/perfdash/config.go#L486-L494"", 'created_at': datetime.datetime(2023, 2, 14, 11, 4, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 1547661250, 'issue_id': 1523585762, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 5, 15, 11, 13, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 1547973764, 'issue_id': 1523585762, 'author': 'pohly', 'body': '/remove-lifecycle stale', 'created_at': datetime.datetime(2023, 5, 15, 14, 23, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 1559724833, 'issue_id': 1523585762, 'author': 'pohly', 'body': 'Shall we continue keeping this issue open to discuss open problems regarding the job?\r\n\r\nIt is no longer failing and after some changes (https://github.com/kubernetes/perf-tests/pull/2266, https://github.com/kubernetes/kubernetes/pull/118164), some data is shown again in perf-dash.\r\n\r\nHowever, only `scheduler_framework_extension_point_duration_seconds` works. Even then, some workloads don\'t seem to have data for ""Score"", so only ""Filter"" actually renders anything.\r\n\r\nFor results where there is no plugin label, for example `SchedulingThroughput`, nothing is shown because no data has ""Score"" or ""Filter"" as label value.\r\n\r\nThe conclusion [from Slack](https://kubernetes.slack.com/archives/C09QZTRH7/p1684856425322669?thread_ts=1676392759.560789&cid=C09QZTRH7) was that ""you should not mix labels in the same file"", i.e. all data items in the same file must have the same labels.', 'created_at': datetime.datetime(2023, 5, 23, 15, 56, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 1563839024, 'issue_id': 1523585762, 'author': 'kerthcet', 'body': ""I think we can close this issue, metric is something out of this issue's scope."", 'created_at': datetime.datetime(2023, 5, 26, 5, 43, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 1563895643, 'issue_id': 1523585762, 'author': 'pohly', 'body': ""Okay. Let's close. I added a comment to https://github.com/kubernetes/kubernetes/issues/116204 about the perf-dash problem.\r\n\r\n/close"", 'created_at': datetime.datetime(2023, 5, 26, 6, 57, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 1563895741, 'issue_id': 1523585762, 'author': 'k8s-ci-robot', 'body': ""@pohly: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114890#issuecomment-1563895643):\n\n>Okay. Let's close. I added a comment to https://github.com/kubernetes/kubernetes/issues/116204 about the perf-dash problem.\r\n>\r\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"", 'created_at': datetime.datetime(2023, 5, 26, 6, 57, 8, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-07 07:54:49 UTC): @sanposhiho: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

sanposhiho (Issue Creator) on (2023-01-07 07:56:47 UTC): The direct reason why the `ci-benchmark-scheduler-perf-master` is marked as fail is `unexpected number of goroutines: before: 2 after 43`.
https://github.com/kubernetes/kubernetes/blob/643353abfe180ded1fae1fe6fd48d3930d26a9e4/test/integration/framework/etcd.go#L217

I'm not sure if it can be fixed by https://github.com/kubernetes/kubernetes/pull/114843 or not. Let's see..

pohly on (2023-01-25 19:35:36 UTC): Here's a patch that makes the reporting of leaked goroutines more useful:

```patch
commit 469526fab53efb4cc26fb70b610239a232890a73
Author: Patrick Ohly <patrick.ohly@intel.com>
Date:   Wed Jan 25 17:59:10 2023 +0100

    test/integration: improve output when etcd detects leaked goroutines
    
    With the current output one was only told how many additional goroutines were
    running after a test and a dump of the current ones. But without knowing what
    was running initially, that information was not enough.
    
    Now a diff that shows changes in the stack traces before and after a test is
    shown.
    
    This currently gets triggered and leads to output like this:
    
    F0125 17:57:26.577440  462703 etcd.go:216] unexpected number of goroutines: before: 3, after: 9
      (
            """"""
            goroutine 1 [running]:
    -       k8s.io/kubernetes/test/integration/framework.stacks(0x10000)
    +       k8s.io/kubernetes/test/integration/framework.stacks(0x100000)
                    /nvme/gopath/src/k8s.io/kubernetes/test/integration/framework/etcd.go:224 +0x8a
            k8s.io/kubernetes/test/integration/framework.EtcdMain(0xc00139fe98)
    -               /nvme/gopath/src/k8s.io/kubernetes/test/integration/framework/etcd.go:189 +0x85
    +               /nvme/gopath/src/k8s.io/kubernetes/test/integration/framework/etcd.go:215 +0x1c5
            k8s.io/kubernetes/test/integration/scheduler_perf.TestMain(0xc0011680a0)
                    /nvme/gopath/src/k8s.io/kubernetes/test/integration/scheduler_perf/main_test.go:58 +0x1c8
            ... // 7 identical lines
                    /nvme/gopath/src/k8s.io/kubernetes/vendor/go.opencensus.io/stats/view/worker.go:34 +0x8d
      
    -       goroutine 203 [runnable]:
    +       goroutine 203 [select]:
            k8s.io/klog/v2.(*flushDaemon).run.func1()
    -               /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/klog/v2/klog.go:1131
    +               /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/klog/v2/klog.go:1135 +0x11e
            created by k8s.io/klog/v2.(*flushDaemon).run
                    /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/klog/v2/klog.go:1131 +0x17b
    + 
    +       goroutine 1514 [select]:
    +       k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x0?, {0x5906aa0, 0xc005ec4030}, 0x1, 0xc0000863c0)
    +               /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:169 +0x135
    +       k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x0?, 0xdf8475800, 0x0, 0x0?, 0x0?)
    +               /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:135 +0x89
    +       k8s.io/apimachinery/pkg/util/wait.Until(...)
    +               /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:92
    +       k8s.io/apimachinery/pkg/util/wait.Forever(0x0?, 0x0?)
    +               /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:83 +0x28
    +       created by k8s.io/apiserver/pkg/server/healthz.(*log).Check.func1
    +               /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/server/healthz/healthz.go:74 +0xe5
    + 
    +       goroutine 2473 [chan receive]:
    +       k8s.io/apimachinery/pkg/watch.(*Broadcaster).loop(0xc0031c0d70)
    +               /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/watch/mux.go:268 +0x65
    +       created by k8s.io/apimachinery/pkg/watch.NewBroadcaster
    +               /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/watch/mux.go:77 +0x116
    + 
    +       goroutine 5540 [syscall]:
    +       os/signal.signal_recv()
    +               /nvme/gopath/go-1.19/src/runtime/sigqueue.go:152 +0x2f
    +       os/signal.loop()
    +               /nvme/gopath/go-1.19/src/os/signal/signal_unix.go:23 +0x19
    +       created by os/signal.Notify.func1.1
    +               /nvme/gopath/go-1.19/src/os/signal/signal.go:151 +0x2a
    + 
    +       goroutine 5956 [chan receive]:
    +       k8s.io/apimachinery/pkg/watch.(*Broadcaster).loop(0xc004eb24b0)
    +               /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/watch/mux.go:268 +0x65
    +       created by k8s.io/apimachinery/pkg/watch.NewLongQueueBroadcaster
    +               /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/watch/mux.go:93 +0x116
    + 
    +       goroutine 5957 [chan receive]:
    +       k8s.io/client-go/tools/record.(*eventBroadcasterImpl).StartEventWatcher.func1()
    +               /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/client-go/tools/record/event.go:307 +0x73
    +       created by k8s.io/client-go/tools/record.(*eventBroadcasterImpl).StartEventWatcher
    +               /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/client-go/tools/record/event.go:305 +0x13d
    + 
    +       goroutine 5958 [chan receive]:
    +       k8s.io/client-go/tools/record.(*eventBroadcasterImpl).StartEventWatcher.func1()
    +               /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/client-go/tools/record/event.go:307 +0x73
    +       created by k8s.io/client-go/tools/record.(*eventBroadcasterImpl).StartEventWatcher
    +               /nvme/gopath/src/k8s.io/kubernetes/vendor/k8s.io/client-go/tools/record/event.go:305 +0x13d
            ... // 1 identical line
            """"""
      )

diff --git a/test/integration/framework/etcd.go b/test/integration/framework/etcd.go
index 966f6538394..1eaf0f36b2e 100644
--- a/test/integration/framework/etcd.go
+++ b/test/integration/framework/etcd.go
@@ -29,6 +29,7 @@ import (
 	""syscall""
 	""time""
 
+	""github.com/google/go-cmp/cmp""
 	""google.golang.org/grpc/grpclog""
 	""k8s.io/klog/v2""
 
@@ -185,7 +186,7 @@ func EtcdMain(tests func() int) {
 	// Bail out early when -help was given as parameter.
 	flag.Parse()
 
-	before := runtime.NumGoroutine()
+	before, stacktracesBefore := stacks(1 << 16)
 	stop, err := startEtcd()
 	if err != nil {
 		klog.Fatalf(""cannot run integration tests: unable to start etcd: %v"", err)
@@ -211,14 +212,19 @@ func EtcdMain(tests func() int) {
 	// It generally takes visibly less than 1s to finish all goroutines.
 	// But we keep the limit higher to account for cpu-starved environments.
 	if err := wait.Poll(100*time.Millisecond, 5*time.Second, checkNumberOfGoroutines); err != nil {
-		after := runtime.NumGoroutine()
-		stacktraces := make([]byte, 1<<20)
-		runtime.Stack(stacktraces, true)
-		klog.Fatalf(""unexpected number of goroutines: before: %d after %d\n%sd"", before, after, string(stacktraces))
+		after, stacktracesAfter := stacks(1 << 20)
+		klog.Fatalf(""unexpected number of goroutines: before: %d, after: %d\n%s"", before, after, cmp.Diff(stacktracesBefore, stacktracesAfter))
 	}
 	os.Exit(result)
 }
 
+func stacks(bufferSize int) (int, string) {
+	num := runtime.NumGoroutine()
+	stacktraces := make([]byte, bufferSize)
+	len := runtime.Stack(stacktraces, true)
+	return num, string(stacktraces[:len])
+}
+
 // GetEtcdURL returns the URL of the etcd instance started by EtcdMain.
 func GetEtcdURL() string {
 	return etcdURL
```

I'll put that into a PR at some point, either stand-alone or in combination with some other cleanup commits.

sanposhiho (Issue Creator) on (2023-01-31 14:31:44 UTC): https://github.com/kubernetes/kubernetes/pull/114843 got merged

Two remaining issues and fixes:
- [x] goroutine leak: #115423 @pohly 
- [ ] Process did not finish before 1h55m0s timeout, Could not interrupt process after timeout: https://github.com/kubernetes/test-infra/pull/28368 @sanposhiho

kerthcet (Assginee) on (2023-02-03 07:08:31 UTC): The goroutine leakage is solved, but I still saw perf-test occasionally failed for time out, instead of increasing the TTL directly, or maybe we can measure the durations for each test and find out the slowest one and maybe the reason, I think this can also help to find some potential performance issues u-turn.

sanposhiho (Issue Creator) on (2023-02-03 08:44:26 UTC): 🎉 

----

Seems sometimes timeout-ed and sometimes passes.
For me, it makes sense to just increase the TTL given we've got additional performance tests in https://github.com/kubernetes/kubernetes/pull/113615. (Probably my current proposed `3h` in [the PR](https://github.com/kubernetes/test-infra/pull/28368) is too much though.)

sanposhiho (Issue Creator) on (2023-02-14 11:04:49 UTC): Another problem.
I believe the [perf-dash](https://perf-dash.k8s.io/#/?jobname=scheduler-perf-benchmark&metriccategoryname=Scheduler&metricname=BenchmarkResults&benchmark=BenchmarkPerfScheduling%2FSchedulingWithNodeInclusionPolicy%2F5000Nodes-8&metricName=time) should show the scheduler's metric result as BenchmarkPerfResults but currently not.

https://github.com/kubernetes/perf-tests/blob/master/perfdash/config.go#L486-L494

k8s-triage-robot on (2023-05-15 11:13:37 UTC): The Kubernetes project currently lacks enough contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle stale`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle stale

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

pohly on (2023-05-15 14:23:45 UTC): /remove-lifecycle stale

pohly on (2023-05-23 15:56:47 UTC): Shall we continue keeping this issue open to discuss open problems regarding the job?

It is no longer failing and after some changes (https://github.com/kubernetes/perf-tests/pull/2266, https://github.com/kubernetes/kubernetes/pull/118164), some data is shown again in perf-dash.

However, only `scheduler_framework_extension_point_duration_seconds` works. Even then, some workloads don't seem to have data for ""Score"", so only ""Filter"" actually renders anything.

For results where there is no plugin label, for example `SchedulingThroughput`, nothing is shown because no data has ""Score"" or ""Filter"" as label value.

The conclusion [from Slack](https://kubernetes.slack.com/archives/C09QZTRH7/p1684856425322669?thread_ts=1676392759.560789&cid=C09QZTRH7) was that ""you should not mix labels in the same file"", i.e. all data items in the same file must have the same labels.

kerthcet (Assginee) on (2023-05-26 05:43:53 UTC): I think we can close this issue, metric is something out of this issue's scope.

pohly on (2023-05-26 06:57:02 UTC): Okay. Let's close. I added a comment to https://github.com/kubernetes/kubernetes/issues/116204 about the perf-dash problem.

/close

k8s-ci-robot on (2023-05-26 06:57:08 UTC): @pohly: Closing this issue.

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114890#issuecomment-1563895643):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

"
1523473410,issue,closed,not_planned,[FG:InPlacePodVerticalScaling] Test cases we need to add for in-place pod resize feature,"### What would you like to be added?

1. Add E2E test to ensure reasonable and correct behavior when use rolls back from FG=beta version to FG=alpha version. Expected behavior of currently running pods that may have resize in progress.
2. Add test to validate that when pod.restartPolicy = Never, resizePolicy must be RestartNotRequired.

### Why is this needed?

These are test cases from reviews that is not currently enumerated or tracked in enhancement doc.",vinaykul,2023-01-07 04:53:42+00:00,['vinaykul'],2023-06-25 20:45:04+00:00,2023-06-25 20:45:03+00:00,https://github.com/kubernetes/kubernetes/issues/114888,"[('sig/node', 'Categorizes an issue or PR as relevant to SIG Node.'), ('kind/feature', 'Categorizes issue or PR as related to a new feature.'), ('lifecycle/rotten', 'Denotes an issue or PR that has aged beyond stale and will be auto-closed.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1374378984, 'issue_id': 1523473410, 'author': 'k8s-ci-robot', 'body': '@vinaykul: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 7, 4, 53, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374378999, 'issue_id': 1523473410, 'author': 'vinaykul', 'body': '/assign vinaykul', 'created_at': datetime.datetime(2023, 1, 7, 4, 53, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374379042, 'issue_id': 1523473410, 'author': 'vinaykul', 'body': '/sig node', 'created_at': datetime.datetime(2023, 1, 7, 4, 54, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 1523953176, 'issue_id': 1523473410, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 4, 26, 19, 45, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 1564877914, 'issue_id': 1523473410, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 5, 26, 20, 3, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 1606256791, 'issue_id': 1523473410, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 6, 25, 20, 44, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 1606256806, 'issue_id': 1523473410, 'author': 'k8s-ci-robot', 'body': '@k8s-triage-robot: Closing this issue, marking it as ""Not Planned"".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114888#issuecomment-1606256791):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 6, 25, 20, 45, 4, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-07 04:53:49 UTC): @vinaykul: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

vinaykul (Issue Creator) on (2023-01-07 04:53:54 UTC): /assign vinaykul

vinaykul (Issue Creator) on (2023-01-07 04:54:10 UTC): /sig node

k8s-triage-robot on (2023-04-26 19:45:49 UTC): The Kubernetes project currently lacks enough contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle stale`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle stale

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-triage-robot on (2023-05-26 20:03:52 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle rotten

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-triage-robot on (2023-06-25 20:44:59 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.

This bot triages issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Reopen this issue with `/reopen`
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/close not-planned

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-ci-robot on (2023-06-25 20:45:04 UTC): @k8s-triage-robot: Closing this issue, marking it as ""Not Planned"".

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114888#issuecomment-1606256791):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

"
1523189848,issue,closed,completed,kubeproxy.config.k8s.io/v1alpha1 Breaking Change,"### What happened?

There appears to be a breaking change to kubeproxy.config.k8s.io/v1alpha1 in PR https://github.com/kubernetes/kubernetes/pull/112133

The schema for kubeproxy.config.k8s.io/v1alpha1 changed by removing the `udpIdleTimeout` without being deprecated. 

We based our manifest on what `kubeadm config print init-defaults --component-configs=KubeProxyConfiguration` provided to us in a previous version of kubeadm (pre 1.26)

When we tried create a new cluster with Kubernetes 1.26 using existing configuration we receive this error from kubeadm:

```
W0106 20:27:24.102630   31333 initconfiguration.go:305] error unmarshaling configuration schema.GroupVersionKind{Group:""kubeproxy.config.k8s.io"", Version:""v1alpha1"", Kind:""KubeProxyConfiguration""}: strict decoding error: unknown field ""udpIdleTimeout""
W0106 20:27:24.106622   31333 configset.go:177] error unmarshaling configuration schema.GroupVersionKind{Group:""kubeproxy.config.k8s.io"", Version:""v1alpha1"", Kind:""KubeProxyConfiguration""}: strict decoding error: unknown field ""udpIdleTimeout""
```

### What did you expect to happen?

Existing kubeproxy.config.k8s.io/v1alpha1 manifests to be valid regardless of version of kubeadm we use.

### How can we reproduce it (as minimally and precisely as possible)?

Use kubeadm version lower than 1.26 to generate the default manifest for kube-proxy by using the command below:

kubeadm config print init-defaults --component-configs=KubeProxyConfiguration

Then use the same manifest to bootstrap a cluster using kubeadm greater than 1.26 or newer.

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version

WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:""1"", Minor:""26"", GitVersion:""v1.26.0"", GitCommit:""b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d"", GitTreeState:""clean"", BuildDate:""2022-12-08T19:58:30Z"", GoVersion:""go1.19.4"", Compiler:""gc"", Platform:""linux/amd64""}
Kustomize Version: v4.5.7
```

</details>


### Cloud provider

<details>
N/A
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
N/A
$ uname -a
N/A

```

</details>


### Install tools

<details>
$ kubeadm version

kubeadm version: &version.Info{Major:""1"", Minor:""26"", GitVersion:""v1.26.0"", GitCommit:""b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d"", GitTreeState:""clean"", BuildDate:""2022-12-08T19:57:06Z"", GoVersion:""go1.19.4"", Compiler:""gc"", Platform:""linux/amd64""}
</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",mathewpeterson,2023-01-06 21:26:15+00:00,[],2023-01-07 20:38:05+00:00,2023-01-07 17:11:27+00:00,https://github.com/kubernetes/kubernetes/issues/114884,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('sig/network', 'Categorizes an issue or PR as relevant to SIG Network.'), ('sig/cluster-lifecycle', 'Categorizes an issue or PR as relevant to SIG Cluster Lifecycle.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1374152250, 'issue_id': 1523189848, 'author': 'k8s-ci-robot', 'body': '@mathewpeterson: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 6, 21, 26, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374153173, 'issue_id': 1523189848, 'author': 'mathewpeterson', 'body': '/sig cluster-lifecycle', 'created_at': datetime.datetime(2023, 1, 6, 21, 27, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374197837, 'issue_id': 1523189848, 'author': 'neolit123', 'body': 'on paper alpha apis have little to no guarantees. yet preferably, removal of fields should be done after incrementing the api version especially if the api is widely spread and in production.\r\n\r\nreading briefly the related pr it seems the feature was removed completely, which means the field is no longer needed?\r\n\r\nup to sig network to comment / triage.\r\n\r\n/sig network', 'created_at': datetime.datetime(2023, 1, 6, 22, 16, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374208064, 'issue_id': 1523189848, 'author': 'mathewpeterson', 'body': 'Correct, the field is no longer required and we specifically were not using that setting - it came along when we created our initial configuration file. \r\n\r\nIt seems your statement about alpha apis having ""little to no guarantees"" contradicts the [deprecation policy](https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api)?', 'created_at': datetime.datetime(2023, 1, 6, 22, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374422632, 'issue_id': 1523189848, 'author': 'neolit123', 'body': '> It seems your statement about alpha apis having ""little to no guarantees"" contradicts the [deprecation policy](https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api)?\r\n\r\nfields should only be removed on version increment, like i pointed out. the bigger non-guarantee is:\r\n\r\n>> Alpha API versions may be removed in any release without prior deprecation notice\r\n\r\nit\'s important to note the component config apis in k8s should be treated more like ga apis, because of consumption numbers.', 'created_at': datetime.datetime(2023, 1, 7, 9, 13, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374447890, 'issue_id': 1523189848, 'author': 'aojea', 'body': 'The deprecation of the userspace proxy was announced and started in 1.23 https://github.com/kubernetes/kubernetes/issues/103860', 'created_at': datetime.datetime(2023, 1, 7, 11, 28, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374542278, 'issue_id': 1523189848, 'author': 'mathewpeterson', 'body': 'Sorry, I am not trying to be pedantic and perhaps I am confused on terminology but:\r\n\r\n> Alpha API versions may be removed in any release without prior deprecation notice\r\n\r\nThe _API version_ was not removed, rather just a field.\r\n\r\nEither way, thanks to everyone who responded. Hopefully this issue can help shed light for users in the future. \r\n\r\nI believe this issue can be closed now.', 'created_at': datetime.datetime(2023, 1, 7, 17, 11, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374604374, 'issue_id': 1523189848, 'author': 'aojea', 'body': '> The _API version_ was not removed, rather just a field.\r\n\r\nwe are not doing great with the configuration versioning, but it is more a problem of how component configuration has progressed and it was never fully implemented, causing still issues and inconsistencies between flags vs config per example\r\n\r\nhttps://github.com/kubernetes/enhancements/issues/784', 'created_at': datetime.datetime(2023, 1, 7, 20, 38, 5, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-06 21:26:23 UTC): @mathewpeterson: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

mathewpeterson (Issue Creator) on (2023-01-06 21:27:27 UTC): /sig cluster-lifecycle

neolit123 on (2023-01-06 22:16:14 UTC): on paper alpha apis have little to no guarantees. yet preferably, removal of fields should be done after incrementing the api version especially if the api is widely spread and in production.

reading briefly the related pr it seems the feature was removed completely, which means the field is no longer needed?

up to sig network to comment / triage.

/sig network

mathewpeterson (Issue Creator) on (2023-01-06 22:30:00 UTC): Correct, the field is no longer required and we specifically were not using that setting - it came along when we created our initial configuration file. 

It seems your statement about alpha apis having ""little to no guarantees"" contradicts the [deprecation policy](https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api)?

neolit123 on (2023-01-07 09:13:22 UTC): fields should only be removed on version increment, like i pointed out. the bigger non-guarantee is:


it's important to note the component config apis in k8s should be treated more like ga apis, because of consumption numbers.

aojea on (2023-01-07 11:28:30 UTC): The deprecation of the userspace proxy was announced and started in 1.23 https://github.com/kubernetes/kubernetes/issues/103860

mathewpeterson (Issue Creator) on (2023-01-07 17:11:27 UTC): Sorry, I am not trying to be pedantic and perhaps I am confused on terminology but:


The _API version_ was not removed, rather just a field.

Either way, thanks to everyone who responded. Hopefully this issue can help shed light for users in the future. 

I believe this issue can be closed now.

aojea on (2023-01-07 20:38:05 UTC): we are not doing great with the configuration versioning, but it is more a problem of how component configuration has progressed and it was never fully implemented, causing still issues and inconsistencies between flags vs config per example

https://github.com/kubernetes/enhancements/issues/784

"
1523167471,issue,closed,completed,Leaked pod metadata across namespaces on preemption,"### What happened?

When kube-scheduler does preemption and the PodDisruptionConditions feature gate is enabled, it might leak the namespace and name of a pod in a different namespace.

### What did you expect to happen?

While debuggability is important, this is too much information to give to an end user. This information could be just in logs, which are only accessible to administrators.

We could print the name of the pod only it's in the same namespace, but I don't think it's worth the complexity.

### How can we reproduce it (as minimally and precisely as possible)?

Have PodDisruptionConditions enabled and generate a case of preemption. The PodDisruption condition message in the preempted pod will have the name of the Pod.

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

1.26+
</details>


### Cloud provider

<details>
any
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",alculquicondor,2023-01-06 21:05:29+00:00,['mimowo'],2023-08-09 16:17:19+00:00,2023-01-17 05:57:22+00:00,https://github.com/kubernetes/kubernetes/issues/114882,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('priority/important-soon', 'Must be staffed and worked on either currently, or very soon, ideally in time for the next release.'), ('sig/scheduling', 'Categorizes an issue or PR as relevant to SIG Scheduling.'), ('sig/auth', 'Categorizes an issue or PR as relevant to SIG Auth.'), ('sig/apps', 'Categorizes an issue or PR as relevant to SIG Apps.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1374131046, 'issue_id': 1523167471, 'author': 'k8s-ci-robot', 'body': '@alculquicondor: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 6, 21, 5, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374131284, 'issue_id': 1523167471, 'author': 'alculquicondor', 'body': '/sig apps\r\n/priority important-soon', 'created_at': datetime.datetime(2023, 1, 6, 21, 5, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374131492, 'issue_id': 1523167471, 'author': 'alculquicondor', 'body': '/sig scheduling', 'created_at': datetime.datetime(2023, 1, 6, 21, 6, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374131589, 'issue_id': 1523167471, 'author': 'alculquicondor', 'body': '/assign @mimowo', 'created_at': datetime.datetime(2023, 1, 6, 21, 6, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374162454, 'issue_id': 1523167471, 'author': 'Huang-Wei', 'body': ""@mimowo would you mind using this chance to add the preemptor pod's schedulerName in the message?"", 'created_at': datetime.datetime(2023, 1, 6, 21, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 1375283897, 'issue_id': 1523167471, 'author': 'mimowo', 'body': ""> would you mind using this chance to add the preemptor pod's schedulerName in the message?\r\n\r\nYes, I added in the newly open PR, please review: https://github.com/kubernetes/kubernetes/pull/114914"", 'created_at': datetime.datetime(2023, 1, 9, 8, 55, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 1375407935, 'issue_id': 1523167471, 'author': 'denkensk', 'body': 'Do we also need to update the message in event?\r\nhttps://github.com/kubernetes/kubernetes/blob/cf7a3c5bbbebf295a15b37f3e67ae0e2f4c7dd84/pkg/scheduler/framework/preemption/preemption.go#L381-L382', 'created_at': datetime.datetime(2023, 1, 9, 10, 35, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 1375470590, 'issue_id': 1523167471, 'author': 'mimowo', 'body': '> Do we also need to update the message in event?\r\n\r\nGood question,  WDYT @alculquicondor @liggitt? It was like that since at least 1.19: \r\nhttps://github.com/kubernetes/kubernetes/blob/release-1.19/pkg/scheduler/framework/plugins/defaultpreemption/default_preemption.go#L603-L604', 'created_at': datetime.datetime(2023, 1, 9, 11, 17, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 1375546108, 'issue_id': 1523167471, 'author': 'liggitt', 'body': '> Do we also need to update the message in event?\r\n\r\nyes, sounds like we should', 'created_at': datetime.datetime(2023, 1, 9, 12, 20, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 1375563557, 'issue_id': 1523167471, 'author': 'mimowo', 'body': ""> > Do we also need to update the message in event?\r\n> \r\n> yes, sounds like we should\r\n\r\nCool, I'm going to prepare a dedicated PR as this one might be cherry-picked for more releases, than the condition message PR."", 'created_at': datetime.datetime(2023, 1, 9, 12, 35, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 1384689682, 'issue_id': 1523167471, 'author': 'ahg-g', 'body': 'can we close this?', 'created_at': datetime.datetime(2023, 1, 17, 0, 15, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 1384871052, 'issue_id': 1523167471, 'author': 'Huang-Wei', 'body': '/close', 'created_at': datetime.datetime(2023, 1, 17, 5, 57, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 1384871087, 'issue_id': 1523167471, 'author': 'k8s-ci-robot', 'body': '@Huang-Wei: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114882#issuecomment-1384871052):\n\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 17, 5, 57, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 1593430961, 'issue_id': 1523167471, 'author': 'wmgroot', 'body': 'This is the equivalent of not being given the name of your accuser in a court of law. :smile: \r\n\r\nVery frustrating for users in multi-tenant environments who are forced to work with the cluster administrators to identify why their pods were preempted.', 'created_at': datetime.datetime(2023, 6, 15, 17, 0, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 1593468069, 'issue_id': 1523167471, 'author': 'alculquicondor', 'body': ""It might be ok for your multitenant use case not to expose this information, but you cannot speak for every user of kubernetes.\r\n\r\n> Very frustrating for users in multi-tenant environments who are forced to work with the cluster administrators to identify why their pods were preempted.\r\n\r\nCan you actually give more context? Why do users need to know the name of the preemptor? What would it change? The preemption would still happen. If there are some priorities that need to be adjusted, it shouldn't be on the users to set an arbitrary priority without consulting the administrators."", 'created_at': datetime.datetime(2023, 6, 15, 17, 25, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 1593470612, 'issue_id': 1523167471, 'author': 'alculquicondor', 'body': '/sig auth\r\n\r\nAs the SIG that suggested we hide this information from end-users.', 'created_at': datetime.datetime(2023, 6, 15, 17, 27, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 1593496698, 'issue_id': 1523167471, 'author': 'wmgroot', 'body': ""Our monitoring stack (Datadog) indexes all k8s events and makes it very easy for users to identify the source of preemption and reach out to the team that owns the pods when that information is retained in the event message.\r\n\r\nThis is efficient because the involved teams (preemptor and preemptee) can resolve the issue on their own without involving cluster-admins who are very busy with other work. The mitigations are all user operations, not requiring cluster admin intervention.\r\n1. Address the health of the preempting pods.\r\n2. Adjust the rollout configuration of the preempting pods.\r\n3. Adjust the priority classes to avoid the preemption.\r\n\r\nI can also understand supporting cases where a company is offering a multi-tenant environment and tenant A should not see any information, including namespaces and pod names of tenant B. But I would also call out that it's very weird for a Preemption configuration to be in place between two extremely isolated use cases.\r\n\r\nIs it unreasonable to support a configuration parameter to allowing higher verbosity in these event messages? We currently allow all multi-tenant users full read access on the k8s scheduler logs, but event data is more readily available to our users."", 'created_at': datetime.datetime(2023, 6, 15, 17, 50, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 1593516548, 'issue_id': 1523167471, 'author': 'alculquicondor', 'body': ""This was initially proposed by @liggitt , but he's out on vacation.\r\n@deads2k maybe you have some thoughts about this?"", 'created_at': datetime.datetime(2023, 6, 15, 18, 6, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 1609305796, 'issue_id': 1523167471, 'author': 'kerthcet', 'body': ""One thing I thought weird here is we can preempt the pod but we can't know its name. 🤔"", 'created_at': datetime.datetime(2023, 6, 27, 11, 21, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 1670346788, 'issue_id': 1523167471, 'author': 'grosser', 'body': '+1 for knowing who caused the pre-emption, it makes debugging bad deploys so much harder if we don\'t know\r\n\r\nAlso the [code](https://github.com/kubernetes/kubernetes/blame/f0dcf0614036d8c3cd1c9f3b3cf8df4bb1d8e44e/staging/src/k8s.io/client-go/tools/events/event_recorder.go#L88) seems to still have ""Related"" field set, but in the actual event that\'s gone, not sure what\'s up with that.\r\n```\r\n---\r\ninvolvedObject:\r\n  apiVersion: v1\r\n  kind: Pod\r\n  name: foo\r\n  namespace: kube-system\r\nkind: Event\r\nmessage: Preempted by a pod on node ip-172-30-123-123.us-west-2.compute.internal\r\nmetadata:\r\n  name: foo-tdfdt.177985f84016fce8\r\n  namespace: kube-system\r\nreason: Preempted\r\nreportingComponent: """"\r\nreportingInstance: """"\r\nsource:\r\n  component: default-scheduler\r\ntype: Normal\r\n```', 'created_at': datetime.datetime(2023, 8, 8, 21, 41, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 1670906149, 'issue_id': 1523167471, 'author': 'mimowo', 'body': ""How about returning preemptor UID instead of name & namespace? It shouldn't leak anything important, unless the UIDs be set by users (can they?). Yet, this would give admins more info for debugging."", 'created_at': datetime.datetime(2023, 8, 9, 8, 36, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 1671352390, 'issue_id': 1523167471, 'author': 'alculquicondor', 'body': '@liggitt we need some guidance from sig-auth here.', 'created_at': datetime.datetime(2023, 8, 9, 13, 42, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 1671463275, 'issue_id': 1523167471, 'author': 'grosser', 'body': 'uid sounds like a great compromise\r\n\r\nOn Wed, Aug 9, 2023 at 6:42\u202fAM Aldo Culquicondor ***@***.***>\r\nwrote:\r\n\r\n> @liggitt <https://github.com/liggitt> we need some guidance from sig-auth\r\n> here.\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/kubernetes/kubernetes/issues/114882#issuecomment-1671352390>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AAACYZ4HUW6DEZYBWJHH5LTXUOHV3ANCNFSM6AAAAAATTOVQYM>\r\n> .\r\n> You are receiving this because you commented.Message ID:\r\n> ***@***.***>\r\n>', 'created_at': datetime.datetime(2023, 8, 9, 14, 24, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 1671698348, 'issue_id': 1523167471, 'author': 'liggitt', 'body': 'uid is certainly better than name/namespace, and is not user-assignable', 'created_at': datetime.datetime(2023, 8, 9, 15, 58, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 1671739469, 'issue_id': 1523167471, 'author': 'alculquicondor', 'body': 'Opened #119866', 'created_at': datetime.datetime(2023, 8, 9, 16, 17, 18, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-06 21:05:38 UTC): @alculquicondor: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

alculquicondor (Issue Creator) on (2023-01-06 21:05:52 UTC): /sig apps
/priority important-soon

alculquicondor (Issue Creator) on (2023-01-06 21:06:06 UTC): /sig scheduling

alculquicondor (Issue Creator) on (2023-01-06 21:06:13 UTC): /assign @mimowo

Huang-Wei on (2023-01-06 21:37:00 UTC): @mimowo would you mind using this chance to add the preemptor pod's schedulerName in the message?

mimowo (Assginee) on (2023-01-09 08:55:03 UTC): Yes, I added in the newly open PR, please review: https://github.com/kubernetes/kubernetes/pull/114914

denkensk on (2023-01-09 10:35:25 UTC): Do we also need to update the message in event?
https://github.com/kubernetes/kubernetes/blob/cf7a3c5bbbebf295a15b37f3e67ae0e2f4c7dd84/pkg/scheduler/framework/preemption/preemption.go#L381-L382

mimowo (Assginee) on (2023-01-09 11:17:04 UTC): Good question,  WDYT @alculquicondor @liggitt? It was like that since at least 1.19: 
https://github.com/kubernetes/kubernetes/blob/release-1.19/pkg/scheduler/framework/plugins/defaultpreemption/default_preemption.go#L603-L604

liggitt on (2023-01-09 12:20:09 UTC): yes, sounds like we should

mimowo (Assginee) on (2023-01-09 12:35:22 UTC): Cool, I'm going to prepare a dedicated PR as this one might be cherry-picked for more releases, than the condition message PR.

ahg-g on (2023-01-17 00:15:24 UTC): can we close this?

Huang-Wei on (2023-01-17 05:57:18 UTC): /close

k8s-ci-robot on (2023-01-17 05:57:23 UTC): @Huang-Wei: Closing this issue.

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114882#issuecomment-1384871052):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

wmgroot on (2023-06-15 17:00:36 UTC): This is the equivalent of not being given the name of your accuser in a court of law. :smile: 

Very frustrating for users in multi-tenant environments who are forced to work with the cluster administrators to identify why their pods were preempted.

alculquicondor (Issue Creator) on (2023-06-15 17:25:47 UTC): It might be ok for your multitenant use case not to expose this information, but you cannot speak for every user of kubernetes.


Can you actually give more context? Why do users need to know the name of the preemptor? What would it change? The preemption would still happen. If there are some priorities that need to be adjusted, it shouldn't be on the users to set an arbitrary priority without consulting the administrators.

alculquicondor (Issue Creator) on (2023-06-15 17:27:33 UTC): /sig auth

As the SIG that suggested we hide this information from end-users.

wmgroot on (2023-06-15 17:50:26 UTC): Our monitoring stack (Datadog) indexes all k8s events and makes it very easy for users to identify the source of preemption and reach out to the team that owns the pods when that information is retained in the event message.

This is efficient because the involved teams (preemptor and preemptee) can resolve the issue on their own without involving cluster-admins who are very busy with other work. The mitigations are all user operations, not requiring cluster admin intervention.
1. Address the health of the preempting pods.
2. Adjust the rollout configuration of the preempting pods.
3. Adjust the priority classes to avoid the preemption.

I can also understand supporting cases where a company is offering a multi-tenant environment and tenant A should not see any information, including namespaces and pod names of tenant B. But I would also call out that it's very weird for a Preemption configuration to be in place between two extremely isolated use cases.

Is it unreasonable to support a configuration parameter to allowing higher verbosity in these event messages? We currently allow all multi-tenant users full read access on the k8s scheduler logs, but event data is more readily available to our users.

alculquicondor (Issue Creator) on (2023-06-15 18:06:59 UTC): This was initially proposed by @liggitt , but he's out on vacation.
@deads2k maybe you have some thoughts about this?

kerthcet on (2023-06-27 11:21:52 UTC): One thing I thought weird here is we can preempt the pod but we can't know its name. 🤔

grosser on (2023-08-08 21:41:57 UTC): +1 for knowing who caused the pre-emption, it makes debugging bad deploys so much harder if we don't know

Also the [code](https://github.com/kubernetes/kubernetes/blame/f0dcf0614036d8c3cd1c9f3b3cf8df4bb1d8e44e/staging/src/k8s.io/client-go/tools/events/event_recorder.go#L88) seems to still have ""Related"" field set, but in the actual event that's gone, not sure what's up with that.
```
---
involvedObject:
  apiVersion: v1
  kind: Pod
  name: foo
  namespace: kube-system
kind: Event
message: Preempted by a pod on node ip-172-30-123-123.us-west-2.compute.internal
metadata:
  name: foo-tdfdt.177985f84016fce8
  namespace: kube-system
reason: Preempted
reportingComponent: """"
reportingInstance: """"
source:
  component: default-scheduler
type: Normal
```

mimowo (Assginee) on (2023-08-09 08:36:18 UTC): How about returning preemptor UID instead of name & namespace? It shouldn't leak anything important, unless the UIDs be set by users (can they?). Yet, this would give admins more info for debugging.

alculquicondor (Issue Creator) on (2023-08-09 13:42:41 UTC): @liggitt we need some guidance from sig-auth here.

grosser on (2023-08-09 14:24:19 UTC): uid sounds like a great compromise

On Wed, Aug 9, 2023 at 6:42 AM Aldo Culquicondor ***@***.***>
wrote:

liggitt on (2023-08-09 15:58:46 UTC): uid is certainly better than name/namespace, and is not user-assignable

alculquicondor (Issue Creator) on (2023-08-09 16:17:18 UTC): Opened #119866

"
1522803056,issue,open,,maxSurge for node draining or how to meet availability requirements when draining nodes by adding pods,"### What would you like to be added?

A way to drain nodes by adding more pods elsewhere to meet PodDisruptionBudgets.

### Why is this needed?

Currently, when there is a Deployment, it can be configured to have a `maxSurge` to avoid going under the amount of replicas the deployment requires while allowing for a new release to be rolled out. This parameter allows adding extra pods before subtracting the old ones so that the ""replicas"" number required is always met as a minimum,

This feature (to my knowledge) is only available when releasing new versions of an application, however when draining nodes this would be extremely useful.

Usual cluster maintenance is done by adding new nodes before removing old ones. This means all the pods in the node need to be evicted and there is usually space for one more of each of the old node in the new node. Current solutions such as the PodDisruptionBudget or Eviction API are trying to make sure that substracting pods from the current amount don´t break anything, however the possibility of temporarily having one extra pod of each deployment is not contemplated at the moment.

This request is asking for the ability to use a surplus of pods to meet all constraints for safe eviction.

Some side notes to stress the importance. Although when operating evictions on large workloads lack of PDBs or PDBs with minAvailable/maxUnavailable settings work fine. When moving deployments with 1 replicas or HPA controlled deployments that are currently scaled down enough the problem is aggravated and can only be solved through a few inefficient means, which is acerbated if node maintenance is done automatically (such as GKE, and other cloud services)

Just in case, this is a limitation that should only be counted against Deployments with `strategy.type=RollingUpdate`.

Ways to deal with this situation currently:

1. Have a `minReplicas`/`replicas` to `>1`, and a PDB with `maxUnavailable=1` when it's known that the autoscaler if in use, it's usually scaled on the lower end. Pros: There is no downtime, Cons: Waste of resources.
2. Do nothing, and deal with eventual downtimes. Pros: No waste of resources, Cons: There is downtime in the deployment",txomon,2023-01-06 16:05:01+00:00,[],2024-12-09 20:14:07+00:00,,https://github.com/kubernetes/kubernetes/issues/114877,"[('sig/node', 'Categorizes an issue or PR as relevant to SIG Node.'), ('kind/feature', 'Categorizes issue or PR as related to a new feature.'), ('sig/apps', 'Categorizes an issue or PR as relevant to SIG Apps.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1373838886, 'issue_id': 1522803056, 'author': 'k8s-ci-robot', 'body': '@txomon: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 6, 16, 5, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 1373846204, 'issue_id': 1522803056, 'author': 'txomon', 'body': '/wg reliability\r\n/sig scalability\r\n/sig cluster-lifecycle\r\n/sig autoscaling\r\n/sig apps', 'created_at': datetime.datetime(2023, 1, 6, 16, 10, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 1499367780, 'issue_id': 1522803056, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 4, 6, 17, 5, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 1537189423, 'issue_id': 1522803056, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 5, 6, 17, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 1557325592, 'issue_id': 1522803056, 'author': 'runephilosof-karnovgroup', 'body': 'It seems @0xmichalis also advocated this [on Mar 8, 2017](https://github.com/kubernetes/kubernetes/issues/35318#issuecomment-285033122) when drain was made to honor PodDisruptionBudgets. But is seems no one took notice of it.\r\n\r\nIt seems a lot of users are frustrated/confused by this https://duckduckgo.com/?q=poddisruptionbudget+single+replica. I think it would help if the `maxSurge` was utilized instead of getting stuck.\r\n\r\n/remove-lifecycle rotten', 'created_at': datetime.datetime(2023, 5, 22, 14, 29, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 1657474907, 'issue_id': 1522803056, 'author': 'james-callahan', 'body': 'This is still a huge obstacle for gracefully replacing nodes.\r\nIs anyone trying to take this on?', 'created_at': datetime.datetime(2023, 7, 31, 3, 45, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 1658134331, 'issue_id': 1522803056, 'author': 'txomon', 'body': ""I don't think so, it seems like it isn't gathering much attention, I'm not sure if the working group is even aware of it..."", 'created_at': datetime.datetime(2023, 7, 31, 10, 54, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 1704733353, 'issue_id': 1522803056, 'author': 'neolit123', 'body': '/remove-wg reliability\r\n/remove-sig scalability\r\n/remove-sig cluster-lifecycle\r\n/remove-sig autoscaling\r\n/sig node\r\n\r\nsig apps is the primary owner of this FR due to the maxSurge feature. you can join their zoom meeting and present it:\r\nhttps://github.com/kubernetes/community/tree/master/sig-apps\r\nsig node can participate with respect to node draining (as the title indicates).\r\n\r\nother sigs / wg should only be added once maintainers have a look at this ticket.', 'created_at': datetime.datetime(2023, 9, 4, 7, 12, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 1706475476, 'issue_id': 1522803056, 'author': 'soltysh', 'body': ""It's important to understand that any controller (deployment, or any other workload) and disruption are two distinct mechanism having different roles, although their  functionality is complementary when we're talking about ensuring high availability.\r\nI'd like to stress here the high availability factor, which is at the front and center when talking about PDBs. I will admit it's challenging to have any further discussion without that prerequisite fullfilled. \r\nIn a similar vein, when talking about HPAs, there's an option for minimum number of replicas, which ensure the application always maintains the HA pre-reqs. \r\n\r\nMoving on to the subject of configuring a rolling update (in case of a deployment), versus responding to external disruptions (in this case, PDB being an external actor is equal to invoking `kubectl delete` by the user). The reason we have\r\nthe ability to have a detailed rollout is coming from the fact that it is a process fully operated by the owning controller. Whereas in all other situations (ie. any external errors or disruptions), the controller's goal is to reach the desired state as quickly as possible. \r\n\r\nSpeaking from my own experience, we've had multiple instances of problems when PDBs were blocking the upgrades, due to users setting the allowed replicas down to bare minimum. We solved it by [adding an alert](https://github.com/openshift/cluster-kube-controller-manager-operator/blob/d95b0c25ba55c4ef8e09e56461562ee60b22d51c/manifests/0000_90_kube-controller-manager-operator_05_alerts.yaml#L25-L44) which looks at PDBs and notifies administrators in those cases. Since then we haven't seen any problems, or cluster administrators were aware that problems might popup during upgrade and were able to solve these problems even before initiating the upgrades. \r\n\r\nHaving said all of the above, [SIG Apps](https://github.com/kubernetes/community/tree/master/sig-apps) meeting happens on every other Monday, next occurrence is planned for September 18. If you're interested, I'd be happy to hear more about your use cases and the problems you're struggling with."", 'created_at': datetime.datetime(2023, 9, 5, 11, 50, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 1706499737, 'issue_id': 1522803056, 'author': 'txomon', 'body': 'Hello Maciej,\r\n\r\nI will do my best to join the monday call.\r\n\r\nJust for other people reading the thread, my use case are all the\r\nmiscellaneous apps that are needed to run some services on top of\r\nkubernetes (such as knative admission controller, external-dns, etc.), as\r\nwell as any other app that just doesn\'t need scaling.\r\n\r\nThese apps have PDBs that make sure that there is always at least a single\r\ninstance running (hence it makes sense that the minimum availability of the\r\nPDB is 1), however running more than one instance at a time would be a\r\nwaste of resources.\r\n\r\nI understand there is ownership and interactions between the different\r\ncontrollers, and some of my ideas were revolving around creating a new\r\nattribute, however it was brought to my attention that the `maxSurge` field\r\nperfectly describes the situation, ""I\'m okay with having up to X instances\r\nfor a while"".\r\n\r\nRegarding the PDB blocking upgrade issue, I had those too, however because\r\nwe are running in GKE and nodes get recycled in a regular basis, we can\'t\r\nhave a cluster operator be waiting for a node maintenance to happen (nor we\r\nwant to).\r\n\r\nI hope I was able to give enough context,\r\n\r\nCheers, Javier\r\n\r\nOn Tue, Sep 5, 2023 at 1:50\u202fPM Maciej Szulik ***@***.***>\r\nwrote:\r\n\r\n> It\'s important to understand that any controller (deployment, or any other\r\n> workload) and disruption are two distinct mechanism having different roles,\r\n> although their functionality is complementary when we\'re talking about\r\n> ensuring high availability.\r\n> I\'d like to stress here the high availability factor, which is at the\r\n> front and center when talking about PDBs. I will admit it\'s challenging to\r\n> have any further discussion without that prerequisite fullfilled.\r\n> In a similar vein, when talking about HPAs, there\'s an option for minimum\r\n> number of replicas, which ensure the application always maintains the HA\r\n> pre-reqs.\r\n>\r\n> Moving on to the subject of configuring a rolling update (in case of a\r\n> deployment), versus responding to external disruptions (in this case, PDB\r\n> being an external actor is equal to invoking kubectl delete by the user).\r\n> The reason we have\r\n> the ability to have a detailed rollout is coming from the fact that it is\r\n> a process fully operated by the owning controller. Whereas in all other\r\n> situations (ie. any external errors or disruptions), the controller\'s goal\r\n> is to reach the desired state as quickly as possible.\r\n>\r\n> Speaking from my own experience, we\'ve had multiple instances of problems\r\n> when PDBs were blocking the upgrades, due to users setting the allowed\r\n> replicas down to bare minimum. We solved it by adding an alert\r\n> <https://github.com/openshift/cluster-kube-controller-manager-operator/blob/d95b0c25ba55c4ef8e09e56461562ee60b22d51c/manifests/0000_90_kube-controller-manager-operator_05_alerts.yaml#L25-L44>\r\n> which looks at PDBs and notifies administrators in those cases. Since then\r\n> we haven\'t seen any problems, or cluster administrators were aware that\r\n> problems might popup during upgrade and were able to solve these problems\r\n> even before initiating the upgrades.\r\n>\r\n> Having said all of the above, SIG Apps\r\n> <https://github.com/kubernetes/community/tree/master/sig-apps> meeting\r\n> happens on every other Monday, next occurrence is planned for September 18.\r\n> If you\'re interested, I\'d be happy to hear more about your use cases and\r\n> the problems you\'re struggling with.\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/kubernetes/kubernetes/issues/114877#issuecomment-1706475476>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AABXXGQOBDXTKBTHRAUQZ2DXY4GZ5ANCNFSM6AAAAAATTG7BPM>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>', 'created_at': datetime.datetime(2023, 9, 5, 12, 8, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 1706517511, 'issue_id': 1522803056, 'author': 'james-callahan', 'body': ""Likewise: usually when I run into this its when a low availability requirement cluster is running cluster-critical services (e.g. an admission controller) where it is a complete waste of resources to run multiple replicas, but the need to drain without manual interaction (e.g. in response to a spot instance removal) is important.\r\n\r\nI'm unable to join the SIG apps call, as the meeting time is incompatible with Australian timezones."", 'created_at': datetime.datetime(2023, 9, 5, 12, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 1828342480, 'issue_id': 1522803056, 'author': 'txomon', 'body': 'For anyone following the thread, https://github.com/kubernetes/enhancements/pull/4213/files was brought forward to take into account for this situation during the sig-apps weekly.', 'created_at': datetime.datetime(2023, 11, 27, 17, 56, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 1963023794, 'issue_id': 1522803056, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2024, 2, 25, 18, 35, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 1963097476, 'issue_id': 1522803056, 'author': 'james-callahan', 'body': '/remove-lifecycle stale\r\n\r\nWhere can I follow any ongoing discussion?', 'created_at': datetime.datetime(2024, 2, 25, 23, 20, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2131679490, 'issue_id': 1522803056, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2024, 5, 25, 23, 33, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2187644644, 'issue_id': 1522803056, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2024, 6, 24, 23, 47, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2227403590, 'issue_id': 1522803056, 'author': 'vaibhav2107', 'body': '/remove-lifecycle rotten', 'created_at': datetime.datetime(2024, 7, 14, 16, 25, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408619618, 'issue_id': 1522803056, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2024, 10, 12, 16, 32, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2433106017, 'issue_id': 1522803056, 'author': 'ggarcia-trimble', 'body': '/remove-lifecycle stale', 'created_at': datetime.datetime(2024, 10, 23, 18, 26, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2529238939, 'issue_id': 1522803056, 'author': 'paulgmiller', 'body': 'Still need to read this [proposed KEP]( https://github.com/kubernetes/enhancements/pull/4213/files ) from above but generally trying to deal with similar situations where a deployment surge might unblock evictions and let node drains proceed\r\nhttps://github.com/paulgmiller/k8s-pdb-autoscaler\r\n\r\nBut I see that as more of a proof of concept. It gets better if we had something like https://github.com/kubernetes/kubernetes/issues/128815\r\nTo tell us about blocked evictions (the webhook solution I have today works but who wants a webhook)\r\n\r\nIf this was useful other operators could use it and maybe eventually replicasets could bubble up to deployment that they were under eviction pressure then the deployment could decide if they want to use surge or not to try and alleviate that pressure.', 'created_at': datetime.datetime(2024, 12, 9, 19, 41, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2529350161, 'issue_id': 1522803056, 'author': 'atiratree', 'body': '> Still need to read this [proposed KEP](https://github.com/kubernetes/enhancements/pull/4213/files)\r\n\r\nThis is not the relevant KEP anymore, it has been spun off into https://github.com/kubernetes/enhancements/pull/4565 which is trying to address this issue.', 'created_at': datetime.datetime(2024, 12, 9, 20, 14, 5, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-06 16:05:09 UTC): @txomon: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

txomon (Issue Creator) on (2023-01-06 16:10:34 UTC): /wg reliability
/sig scalability
/sig cluster-lifecycle
/sig autoscaling
/sig apps

k8s-triage-robot on (2023-04-06 17:05:44 UTC): The Kubernetes project currently lacks enough contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle stale`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle stale

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-triage-robot on (2023-05-06 17:42:00 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle rotten

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

runephilosof-karnovgroup on (2023-05-22 14:29:55 UTC): It seems @0xmichalis also advocated this [on Mar 8, 2017](https://github.com/kubernetes/kubernetes/issues/35318#issuecomment-285033122) when drain was made to honor PodDisruptionBudgets. But is seems no one took notice of it.

It seems a lot of users are frustrated/confused by this https://duckduckgo.com/?q=poddisruptionbudget+single+replica. I think it would help if the `maxSurge` was utilized instead of getting stuck.

/remove-lifecycle rotten

james-callahan on (2023-07-31 03:45:50 UTC): This is still a huge obstacle for gracefully replacing nodes.
Is anyone trying to take this on?

txomon (Issue Creator) on (2023-07-31 10:54:23 UTC): I don't think so, it seems like it isn't gathering much attention, I'm not sure if the working group is even aware of it...

neolit123 on (2023-09-04 07:12:57 UTC): /remove-wg reliability
/remove-sig scalability
/remove-sig cluster-lifecycle
/remove-sig autoscaling
/sig node

sig apps is the primary owner of this FR due to the maxSurge feature. you can join their zoom meeting and present it:
https://github.com/kubernetes/community/tree/master/sig-apps
sig node can participate with respect to node draining (as the title indicates).

other sigs / wg should only be added once maintainers have a look at this ticket.

soltysh on (2023-09-05 11:50:42 UTC): It's important to understand that any controller (deployment, or any other workload) and disruption are two distinct mechanism having different roles, although their  functionality is complementary when we're talking about ensuring high availability.
I'd like to stress here the high availability factor, which is at the front and center when talking about PDBs. I will admit it's challenging to have any further discussion without that prerequisite fullfilled. 
In a similar vein, when talking about HPAs, there's an option for minimum number of replicas, which ensure the application always maintains the HA pre-reqs. 

Moving on to the subject of configuring a rolling update (in case of a deployment), versus responding to external disruptions (in this case, PDB being an external actor is equal to invoking `kubectl delete` by the user). The reason we have
the ability to have a detailed rollout is coming from the fact that it is a process fully operated by the owning controller. Whereas in all other situations (ie. any external errors or disruptions), the controller's goal is to reach the desired state as quickly as possible. 

Speaking from my own experience, we've had multiple instances of problems when PDBs were blocking the upgrades, due to users setting the allowed replicas down to bare minimum. We solved it by [adding an alert](https://github.com/openshift/cluster-kube-controller-manager-operator/blob/d95b0c25ba55c4ef8e09e56461562ee60b22d51c/manifests/0000_90_kube-controller-manager-operator_05_alerts.yaml#L25-L44) which looks at PDBs and notifies administrators in those cases. Since then we haven't seen any problems, or cluster administrators were aware that problems might popup during upgrade and were able to solve these problems even before initiating the upgrades. 

Having said all of the above, [SIG Apps](https://github.com/kubernetes/community/tree/master/sig-apps) meeting happens on every other Monday, next occurrence is planned for September 18. If you're interested, I'd be happy to hear more about your use cases and the problems you're struggling with.

txomon (Issue Creator) on (2023-09-05 12:08:03 UTC): Hello Maciej,

I will do my best to join the monday call.

Just for other people reading the thread, my use case are all the
miscellaneous apps that are needed to run some services on top of
kubernetes (such as knative admission controller, external-dns, etc.), as
well as any other app that just doesn't need scaling.

These apps have PDBs that make sure that there is always at least a single
instance running (hence it makes sense that the minimum availability of the
PDB is 1), however running more than one instance at a time would be a
waste of resources.

I understand there is ownership and interactions between the different
controllers, and some of my ideas were revolving around creating a new
attribute, however it was brought to my attention that the `maxSurge` field
perfectly describes the situation, ""I'm okay with having up to X instances
for a while"".

Regarding the PDB blocking upgrade issue, I had those too, however because
we are running in GKE and nodes get recycled in a regular basis, we can't
have a cluster operator be waiting for a node maintenance to happen (nor we
want to).

I hope I was able to give enough context,

Cheers, Javier

On Tue, Sep 5, 2023 at 1:50 PM Maciej Szulik ***@***.***>
wrote:

james-callahan on (2023-09-05 12:21:00 UTC): Likewise: usually when I run into this its when a low availability requirement cluster is running cluster-critical services (e.g. an admission controller) where it is a complete waste of resources to run multiple replicas, but the need to drain without manual interaction (e.g. in response to a spot instance removal) is important.

I'm unable to join the SIG apps call, as the meeting time is incompatible with Australian timezones.

txomon (Issue Creator) on (2023-11-27 17:56:11 UTC): For anyone following the thread, https://github.com/kubernetes/enhancements/pull/4213/files was brought forward to take into account for this situation during the sig-apps weekly.

k8s-triage-robot on (2024-02-25 18:35:26 UTC): The Kubernetes project currently lacks enough contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle stale`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle stale

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

james-callahan on (2024-02-25 23:20:13 UTC): /remove-lifecycle stale

Where can I follow any ongoing discussion?

k8s-triage-robot on (2024-05-25 23:33:17 UTC): The Kubernetes project currently lacks enough contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle stale`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle stale

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-triage-robot on (2024-06-24 23:47:42 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle rotten

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

vaibhav2107 on (2024-07-14 16:25:17 UTC): /remove-lifecycle rotten

k8s-triage-robot on (2024-10-12 16:32:25 UTC): The Kubernetes project currently lacks enough contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle stale`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle stale

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

ggarcia-trimble on (2024-10-23 18:26:29 UTC): /remove-lifecycle stale

paulgmiller on (2024-12-09 19:41:46 UTC): Still need to read this [proposed KEP]( https://github.com/kubernetes/enhancements/pull/4213/files ) from above but generally trying to deal with similar situations where a deployment surge might unblock evictions and let node drains proceed
https://github.com/paulgmiller/k8s-pdb-autoscaler

But I see that as more of a proof of concept. It gets better if we had something like https://github.com/kubernetes/kubernetes/issues/128815
To tell us about blocked evictions (the webhook solution I have today works but who wants a webhook)

If this was useful other operators could use it and maybe eventually replicasets could bubble up to deployment that they were under eviction pressure then the deployment could decide if they want to use surge or not to try and alleviate that pressure.

atiratree on (2024-12-09 20:14:05 UTC): This is not the relevant KEP anymore, it has been spun off into https://github.com/kubernetes/enhancements/pull/4565 which is trying to address this issue.

"
1521408495,issue,closed,completed,Elastic Indexed Job,"### What would you like to be added?

Currently we validate that `spec.completions` is set when `completionMode=Indexed` for job. I suggest we relax this validation. 

This will require the following changes:

1) We need to update the job controller to look at `spec.parallelism` instead of `spec.completions` when calculating the range of indexes.
2) When reducing parallelism, we should remove the higher indexes first (otherwise the controller will eventually do that anyways to ensure that the pods have a continuous range).

both changes only apply when `spec.completions=nil`

### Why is this needed?

The use case is jobs with workers that require a form of autoscaling, like Ray, Spark or Horovord. By modeling these as jobs with `spec.completion=nil`, `spec.parallelism` can be used to scale the job.

While such jobs could be modeled as a StatefulSet, the job API offers batch-specific features that makes it a better fit (like [retriable and non retriable failures](https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/3329-retriable-and-non-retriable-failures))

/sig apps
@soltysh @alculquicondor wdyt?",ahg-g,2023-01-05 20:55:20+00:00,['danielvegamyhre'],2023-07-27 14:17:22+00:00,2023-07-27 12:21:33+00:00,https://github.com/kubernetes/kubernetes/issues/114862,"[('kind/feature', 'Categorizes issue or PR as related to a new feature.'), ('sig/apps', 'Categorizes an issue or PR as relevant to SIG Apps.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.'), ('wg/batch', 'Categorizes an issue or PR as relevant to WG Batch.')]","[{'comment_id': 1372735415, 'issue_id': 1521408495, 'author': 'k8s-ci-robot', 'body': '@ahg-g: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 5, 20, 55, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 1372758334, 'issue_id': 1521408495, 'author': 'alculquicondor', 'body': '+1\r\nThe only thing to note is that if we do the controller changes in release X, then the API validation changes need to go in release X+1', 'created_at': datetime.datetime(2023, 1, 5, 21, 6, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 1372759524, 'issue_id': 1521408495, 'author': 'alculquicondor', 'body': '/wg batch', 'created_at': datetime.datetime(2023, 1, 5, 21, 7, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 1372896041, 'issue_id': 1521408495, 'author': 'ahg-g', 'body': '/assign @danielvegamyhre', 'created_at': datetime.datetime(2023, 1, 5, 22, 55, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 1372896063, 'issue_id': 1521408495, 'author': 'k8s-ci-robot', 'body': ""@ahg-g: GitHub didn't allow me to assign the following users: danielvegamyhre.\n\nNote that only [kubernetes members](https://github.com/orgs/kubernetes/people) with read permissions, repo collaborators and people who have commented on this issue/PR can be assigned. Additionally, issues/PRs can only have 10 assignees at the same time.\nFor more information please see [the contributor guide](https://git.k8s.io/community/contributors/guide/first-contribution.md#issue-assignment-in-github)\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114862#issuecomment-1372896041):\n\n>/assign @danielvegamyhre\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"", 'created_at': datetime.datetime(2023, 1, 5, 22, 55, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 1372896796, 'issue_id': 1521408495, 'author': 'danielvegamyhre', 'body': '/assign danielvegamyhre', 'created_at': datetime.datetime(2023, 1, 5, 22, 56, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 1373564424, 'issue_id': 1521408495, 'author': 'sftim', 'body': 'If we make this change, what are the criteria for considering the Job as complete?', 'created_at': datetime.datetime(2023, 1, 6, 12, 27, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 1373865611, 'issue_id': 1521408495, 'author': 'alculquicondor', 'body': 'Probably worth bringing this up to the next SIG meeting, to make sure we are not missing anything.\r\n\r\n> If we make this change, what are the criteria for considering the Job as complete?\r\n\r\nWe should use the same behavior that NonIndexed jobs have: once one pod finishes successfully, no new pods are created.', 'created_at': datetime.datetime(2023, 1, 6, 16, 27, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 1373870464, 'issue_id': 1521408495, 'author': 'sftim', 'body': ""> The Job is considered _complete_ when there is one successfully completed Pod for each index\r\n\r\n&mdash; https://kubernetes.io/docs/concepts/workloads/controllers/job/#completion-mode\r\n\r\nSo it's a change we should communicate carefully. I think this wants, if not a KEP, then a couple of user stories so that we're agreeing on the end user experience and how that changes from today."", 'created_at': datetime.datetime(2023, 1, 6, 16, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 1373872954, 'issue_id': 1521408495, 'author': 'sftim', 'body': ':thought_balloon: How about using `Indexed` to describe the completion mode we already know, and `IndexedFirstToComplete` for the new behavior? Something like that.', 'created_at': datetime.datetime(2023, 1, 6, 16, 35, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 1373878612, 'issue_id': 1521408495, 'author': 'alculquicondor', 'body': 'We are not changing existing behavior, because `completions=nil` is just disallowed during validation. Adding a new completionMode sounds like an overkill.', 'created_at': datetime.datetime(2023, 1, 6, 16, 41, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 1373885920, 'issue_id': 1521408495, 'author': 'sftim', 'body': 'My worry: someone might watch for a Job and not write code to check `completions`, because they see that the mode is `""Indexed""`. Their scheduler incorrectly ignores the Job because they think `completions: null` is guaranteed by the API, based on their reading of the docs (even if we never made such a guarantee in terms of API versioning).', 'created_at': datetime.datetime(2023, 1, 6, 16, 49, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 1373886500, 'issue_id': 1521408495, 'author': 'sftim', 'body': ""What can help: draft the change to https://kubernetes.io/docs/concepts/workloads/controllers/job/#completion-mode _first_, and then once we're happy that it's feasible to explain, start on the code."", 'created_at': datetime.datetime(2023, 1, 6, 16, 50, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 1373905573, 'issue_id': 1521408495, 'author': 'ahg-g', 'body': '> My worry: someone might watch for a Job and not write code to check completions, because they see that the mode is ""Indexed"". Their scheduler incorrectly ignores the Job because they think completions: null is guaranteed by the API, based on their reading of the docs (even if we never made such a guarantee in terms of API versioning).\r\n\r\nThanks @sftim for the feedback, relaxing validation in general is allowed. As to this specific example, the change we are proposing doesn\'t break this hypothetical scheduler, it will continue to operate as expected from that scheduler\'s perspective.\r\n\r\n> What can help: draft the change to https://kubernetes.io/docs/concepts/workloads/controllers/job/#completion-mode first, and then once we\'re happy that it\'s feasible to explain, start on the code.\r\n\r\n `completions=nil` is historically a permitted mode of operation and have clear and documented semantics, and this proposal doesn\'t change those semantics.', 'created_at': datetime.datetime(2023, 1, 6, 17, 9, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 1373913786, 'issue_id': 1521408495, 'author': 'ahg-g', 'body': 'But let me draft one on this issue, I will post in a moment', 'created_at': datetime.datetime(2023, 1, 6, 17, 18, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 1373945080, 'issue_id': 1521408495, 'author': 'ahg-g', 'body': '### Completion mode\r\n\r\nJobs can have the following modes of operation specified in `.spec.completionMode`:\r\n\r\n- `NonIndexed` (default): the Pods of a Job are considered homologous of each other. When `spec.completions` is set, the Job is considered complete when there have been `.spec.completions` successfully completed Pods.\r\n- `Indexed`: the Pods of a Job get an associated completion index from 0 to\r\n  `$(count)-1`, where count is .spec.completions if set, .spec.parallelism if not. The index is available \r\n     through three mechanisms:\r\n  - The Pod annotation `batch.kubernetes.io/job-completion-index`.\r\n  - As part of the Pod hostname, following the pattern `$(job-name)-$(index)`.\r\n    When you use an Indexed Job in combination with a\r\n    {{< glossary_tooltip term_id=""Service"" >}}, Pods within the Job can use\r\n    the deterministic hostnames to address each other via DNS. For more information about\r\n    how to configure this, see [Job with Pod-to-Pod Communication](/docs/tasks/job/job-with-pod-to-pod-communication/).\r\n  - From the containerized task, in the environment variable `JOB_COMPLETION_INDEX`.\r\n  - When scaling `spec.parallelism` down, the higher active indexes will be removed first.\r\n  \r\n  When spec.completions is set, and indexed Job is considered complete when there is one successfully completed Pod\r\n  for each index. For more information about how to use this mode, see\r\n  [Indexed Job for Parallel Processing with Static Work Assignment](/docs/tasks/job/indexed-parallel-processing-static/).\r\n  Note that, although rare, more than one Pod could be started for the same\r\n  index, but only one of them will count towards the completion count.\r\n\r\nFinally, in both modes, when `spec.completions` is not set, then the job is considered successful when at least one pod successfully completes.', 'created_at': datetime.datetime(2023, 1, 6, 17, 42, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374517733, 'issue_id': 1521408495, 'author': 'sftim', 'body': ""I think we need to explain that if `completions` isn't set, indexes can be reused. At least, that's what the phrase\r\n> an associated completion index from 0 to `$(count)-1`, where count is `.spec.completions` if set, `.spec.parallelism` if not\r\n\r\nimplies.\r\n\r\nIf we didn't want to reuse the indices, then let's explain (and, by implication, consider the design details for) what happens if we create more Pods than we have parallelism.\r\n\r\nI'd like this to have KEP: having an enhancement proposal makes it easier for SIG Release to track the work around updating the docs."", 'created_at': datetime.datetime(2023, 1, 7, 15, 33, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374529267, 'issue_id': 1521408495, 'author': 'ahg-g', 'body': 'What do you mean by indexes can be reused?', 'created_at': datetime.datetime(2023, 1, 7, 16, 14, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374536749, 'issue_id': 1521408495, 'author': 'sftim', 'body': ""In the Kubernetes blog, [Introducing Indexed Jobs](https://kubernetes.io/blog/2021/04/19/introducing-indexed-jobs/) suggests that\r\n> each worker Pod can have a statically assigned partition of the data based on the index\r\n\r\nand someone may have made a container image that relies on that assumption. Does this change create the risk of confusing people who do assume this?\r\n\r\nLet's say I have a work queue that will turn out to hold 99 items, and I want to use a parallelism of 10. I don't know the size of the work queue at the time I define the Job. I set this new Job to be indexed because all my other (fixed-size) Jobs are indexed.\r\n\r\nSome people will want to have an index that matches the ordinal of running Pods like with a StatefulSet, but other people might use the index for something like, say, interpolating into the filename of the Job's output.\r\n\r\nIf we can handle both of the cases I've sketched here, great. If not, let's document which case we're supporting and add a [callout](https://kubernetes.io/docs/contribute/style/style-guide/#caution) to alert the people who were expecting the opposite behavior.\r\n\r\n(This is why I think it's good to draft the docs first - if we're making a design that requires warning users, we might want to reconsider it)."", 'created_at': datetime.datetime(2023, 1, 7, 16, 41, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374591802, 'issue_id': 1521408495, 'author': 'ahg-g', 'body': '> and someone may have made a container image that relies on that assumption. Does this change create the risk of confusing people who do assume this?\r\n\r\nNote that he proposed change does not impact existing workloads. We are adding new semantics, not changing existing ones.\r\n\r\n> Some people will want to have an index that matches the ordinal of running Pods like with a StatefulSet, but other people might use the index for something like, say, interpolating into the filename of the Job\'s output.\r\n\r\nThe proposed semantics supports a StatefulSet like behavior, the indexes will cover the number of replicas (parallelism). The second use case is an interesting one, but it will not be enabled by the proposed change, and I don\'t know why one would think that if we are clearly stating that the indexes will range from 0 to spec.parallelism-1 and that success is defined as any pod finishing successfully. I will send out a KEP.\r\n\r\n> but other people might use the index for something like, say, interpolating into the filename of the Job\'s output.\r\n\r\nI haven\'t seen a batch API with these semantics, and to address this use case (number of indexes is not defined before hand) the queue can associate the item with an index, not the job. The work queue pattern by design aims to reuse the pods and treat them as stateless because the state is in the queue. \r\n\r\nSlightly related, we are thinking about adding a `completionPolicy` API to Job (e.g., the job should be declared successful if 80% of the pods successfully completes);  so, if we set `spec.completions` to some maximum number and come up with a completion policy that says ""exit if the queue is empty"", then we could support the case you mention via job, but I am not really convinced that this is the right approach because I believe indexed job isn\'t the right mode for this use case as I mentioned in the paragraph above.', 'created_at': datetime.datetime(2023, 1, 7, 19, 54, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374855574, 'issue_id': 1521408495, 'author': 'sathyanarays', 'body': '> Finally, in both modes, when spec.completions is not set, then the job is considered successful when at least one pod successfully completes.\r\n\r\nIs it more accurate to say ""Finally, in both modes, when spec.completions is not set, then the job is considered successful when at least `N` pod successfully completes where N is the `spec.parallelism`""?', 'created_at': datetime.datetime(2023, 1, 8, 14, 58, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374971374, 'issue_id': 1521408495, 'author': 'ahg-g', 'body': '> Is it more accurate to say ""Finally, in both modes, when spec.completions is not set, then the job is considered successful when at least N pod successfully completes where N is the spec.parallelism""?\r\n\r\nNo, that is not the current semantics of `spec.completions=nil`; as explained in  https://kubernetes.io/docs/concepts/workloads/controllers/job/#parallel-jobs: \r\n```\r\nonce any Pod has exited with success, no other Pod should still be doing any work for this task or writing any output. They should all be in the process of exiting.\r\n```\r\n\r\nThe behavior you are asking about can be achieved by setting `spec.completions=spec.parallelism`', 'created_at': datetime.datetime(2023, 1, 9, 0, 17, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 1375088796, 'issue_id': 1521408495, 'author': 'sathyanarays', 'body': '@ahg-g ,\r\nI created a job with the following spec:\r\n\r\n```\r\napiVersion: batch/v1\r\nkind: Job\r\nmetadata:\r\n  name: backoff-exp\r\nspec:\r\n  backoffLimit: 4\r\n  parallelism: 10\r\n  template:\r\n    spec:\r\n      containers:\r\n        - name: die\r\n          image: myubuntu:1\r\n          command: [""/bin/sh"",""-c""]\r\n          args: [""/bin/sleep `rand -M 300` && /bin/true""]\r\n          imagePullPolicy: Never\r\n      restartPolicy: Never\r\n```\r\n\r\nEach pod sleeps `random` seconds and exits successfully. In this case, the job was marked completed only after all the 10 pods finished.\r\n\r\n#### kubectl get pods\r\n```\r\nNAME                READY   STATUS      RESTARTS   AGE\r\nbackoff-exp-5mb6m   0/1     Completed   0          4m41s\r\nbackoff-exp-6j76z   0/1     Completed   0          4m41s\r\nbackoff-exp-6vv88   0/1     Completed   0          4m41s\r\nbackoff-exp-7vp8c   0/1     Completed   0          4m41s\r\nbackoff-exp-gtz7n   0/1     Completed   0          4m41s\r\nbackoff-exp-q9kvt   0/1     Completed   0          4m41s\r\nbackoff-exp-vnw7m   0/1     Completed   0          4m41s\r\nbackoff-exp-wngkf   0/1     Completed   0          4m41s\r\nbackoff-exp-xhq9x   0/1     Completed   0          4m41s\r\nbackoff-exp-z4cc9   0/1     Completed   0          4m41s\r\n``` \r\n\r\n#### Start and Finish time for all the above pods\r\n```\r\n2023-01-09T03:56:35Z | 2023-01-09T03:59:55Z\r\n2023-01-09T03:56:35Z | 2023-01-09T03:59:55Z\r\n2023-01-09T03:56:36Z | 2023-01-09T03:59:56Z\r\n2023-01-09T03:56:36Z | 2023-01-09T04:01:14Z\r\n2023-01-09T03:56:36Z | 2023-01-09T04:01:14Z\r\n2023-01-09T03:56:36Z | 2023-01-09T04:01:14Z\r\n2023-01-09T03:56:36Z | 2023-01-09T04:01:14Z\r\n2023-01-09T03:56:37Z | 2023-01-09T03:59:44Z\r\n2023-01-09T03:56:37Z | 2023-01-09T03:59:44Z\r\n2023-01-09T03:56:37Z | 2023-01-09T03:59:44Z\r\n```\r\n\r\n#### Status of the completed job\r\n```\r\n  status:\r\n    completionTime: ""2023-01-09T04:01:17Z""\r\n    conditions:\r\n    - lastProbeTime: ""2023-01-09T04:01:17Z""\r\n      lastTransitionTime: ""2023-01-09T04:01:17Z""\r\n      status: ""True""\r\n      type: Complete\r\n    ready: 0\r\n    startTime: ""2023-01-09T03:56:34Z""\r\n    succeeded: 10\r\n    uncountedTerminatedPods: {}\r\n```\r\nThe job is marked complete only after the last pod has finished. Is the above behavior correct? \r\n\r\nPlease let me know your thoughts on this!', 'created_at': datetime.datetime(2023, 1, 9, 4, 17, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 1375108205, 'issue_id': 1521408495, 'author': 'ahg-g', 'body': 'Any active pods will not be deleted and will be allowed to run to completion (fail or success), but the job will be marked as **successful** if one pod finishes successfully.', 'created_at': datetime.datetime(2023, 1, 9, 5, 1, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 1375819420, 'issue_id': 1521408495, 'author': 'ahg-g', 'body': ""I have this one on today's (Jan 9th) sig-apps meeting"", 'created_at': datetime.datetime(2023, 1, 9, 15, 39, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 1376481261, 'issue_id': 1521408495, 'author': 'ahg-g', 'body': 'Meeting outcome: we will write a KEP, but there seems to be an agreement on the general direction as proposed in the initial post.', 'created_at': datetime.datetime(2023, 1, 9, 23, 21, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 1376657594, 'issue_id': 1521408495, 'author': 'asm582', 'body': 'Curious to know If `spec.completions=nil` then will a failed pod will ever be retried?', 'created_at': datetime.datetime(2023, 1, 10, 2, 57, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 1377446280, 'issue_id': 1521408495, 'author': 'ahg-g', 'body': '> Curious to know If `spec.completions=nil` then will a failed pod will ever be retried?\r\n\r\nYes, just like it is today, it will be retried while respecting `backoffLimit`', 'created_at': datetime.datetime(2023, 1, 10, 15, 29, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 1468491400, 'issue_id': 1521408495, 'author': 'danielvegamyhre', 'body': 'PR for Elastic Index Job docs (in progress): https://github.com/kubernetes/website/pull/39999', 'created_at': datetime.datetime(2023, 3, 14, 17, 5, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 1587747990, 'issue_id': 1521408495, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 6, 12, 17, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 1587798945, 'issue_id': 1521408495, 'author': 'vsoch', 'body': '/remove-lifecycle stale the work is underway!', 'created_at': datetime.datetime(2023, 6, 12, 17, 48, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 1587799229, 'issue_id': 1521408495, 'author': 'vsoch', 'body': '/remove-lifecycle stale', 'created_at': datetime.datetime(2023, 6, 12, 17, 48, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 1652983793, 'issue_id': 1521408495, 'author': 'vsoch', 'body': 'hey folks! I wanted to check in on the state of this - it looks like the elastic index job docs PR was merged https://github.com/kubernetes/website/pull/39999 what comes next?', 'created_at': datetime.datetime(2023, 7, 27, 6, 24, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 1653512219, 'issue_id': 1521408495, 'author': 'alculquicondor', 'body': 'Since this was completed:\r\n/close\r\n\r\nWhat would you like to see next regarding this topic? However, we probably should open new issue(s)', 'created_at': datetime.datetime(2023, 7, 27, 12, 21, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 1653512581, 'issue_id': 1521408495, 'author': 'k8s-ci-robot', 'body': '@alculquicondor: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114862#issuecomment-1653512219):\n\n>Since this was completed:\r\n>/close\r\n>\r\n>What would you like to see next regarding this topic? However, we probably should open new issue(s)\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 7, 27, 12, 21, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 1653716998, 'issue_id': 1521408495, 'author': 'vsoch', 'body': ""> Since this was completed:\r\n\r\nI thought that might be the case!\r\n\r\n> What would you like to see next regarding this topic? However, we probably should open new issue(s)\r\n\r\nWe are using this for experiments to scale / reduce our Flux MiniClusters and it's working great! I don't have further comments here but will definitely open new issues or bring up discussion as needed."", 'created_at': datetime.datetime(2023, 7, 27, 14, 17, 21, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-05 20:55:27 UTC): @ahg-g: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

alculquicondor on (2023-01-05 21:06:41 UTC): +1
The only thing to note is that if we do the controller changes in release X, then the API validation changes need to go in release X+1

alculquicondor on (2023-01-05 21:07:14 UTC): /wg batch

ahg-g (Issue Creator) on (2023-01-05 22:55:43 UTC): /assign @danielvegamyhre

k8s-ci-robot on (2023-01-05 22:55:44 UTC): @ahg-g: GitHub didn't allow me to assign the following users: danielvegamyhre.

Note that only [kubernetes members](https://github.com/orgs/kubernetes/people) with read permissions, repo collaborators and people who have commented on this issue/PR can be assigned. Additionally, issues/PRs can only have 10 assignees at the same time.
For more information please see [the contributor guide](https://git.k8s.io/community/contributors/guide/first-contribution.md#issue-assignment-in-github)

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114862#issuecomment-1372896041):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

danielvegamyhre (Assginee) on (2023-01-05 22:56:40 UTC): /assign danielvegamyhre

sftim on (2023-01-06 12:27:08 UTC): If we make this change, what are the criteria for considering the Job as complete?

alculquicondor on (2023-01-06 16:27:46 UTC): Probably worth bringing this up to the next SIG meeting, to make sure we are not missing anything.


We should use the same behavior that NonIndexed jobs have: once one pod finishes successfully, no new pods are created.

sftim on (2023-01-06 16:33:00 UTC): &mdash; https://kubernetes.io/docs/concepts/workloads/controllers/job/#completion-mode

So it's a change we should communicate carefully. I think this wants, if not a KEP, then a couple of user stories so that we're agreeing on the end user experience and how that changes from today.

sftim on (2023-01-06 16:35:43 UTC): :thought_balloon: How about using `Indexed` to describe the completion mode we already know, and `IndexedFirstToComplete` for the new behavior? Something like that.

alculquicondor on (2023-01-06 16:41:53 UTC): We are not changing existing behavior, because `completions=nil` is just disallowed during validation. Adding a new completionMode sounds like an overkill.

sftim on (2023-01-06 16:49:43 UTC): My worry: someone might watch for a Job and not write code to check `completions`, because they see that the mode is `""Indexed""`. Their scheduler incorrectly ignores the Job because they think `completions: null` is guaranteed by the API, based on their reading of the docs (even if we never made such a guarantee in terms of API versioning).

sftim on (2023-01-06 16:50:21 UTC): What can help: draft the change to https://kubernetes.io/docs/concepts/workloads/controllers/job/#completion-mode _first_, and then once we're happy that it's feasible to explain, start on the code.

ahg-g (Issue Creator) on (2023-01-06 17:09:47 UTC): Thanks @sftim for the feedback, relaxing validation in general is allowed. As to this specific example, the change we are proposing doesn't break this hypothetical scheduler, it will continue to operate as expected from that scheduler's perspective.


 `completions=nil` is historically a permitted mode of operation and have clear and documented semantics, and this proposal doesn't change those semantics.

ahg-g (Issue Creator) on (2023-01-06 17:18:12 UTC): But let me draft one on this issue, I will post in a moment

ahg-g (Issue Creator) on (2023-01-06 17:42:29 UTC): ### Completion mode

Jobs can have the following modes of operation specified in `.spec.completionMode`:

- `NonIndexed` (default): the Pods of a Job are considered homologous of each other. When `spec.completions` is set, the Job is considered complete when there have been `.spec.completions` successfully completed Pods.
- `Indexed`: the Pods of a Job get an associated completion index from 0 to
  `$(count)-1`, where count is .spec.completions if set, .spec.parallelism if not. The index is available 
     through three mechanisms:
  - The Pod annotation `batch.kubernetes.io/job-completion-index`.
  - As part of the Pod hostname, following the pattern `$(job-name)-$(index)`.
    When you use an Indexed Job in combination with a
    {{< glossary_tooltip term_id=""Service"" >}}, Pods within the Job can use
    the deterministic hostnames to address each other via DNS. For more information about
    how to configure this, see [Job with Pod-to-Pod Communication](/docs/tasks/job/job-with-pod-to-pod-communication/).
  - From the containerized task, in the environment variable `JOB_COMPLETION_INDEX`.
  - When scaling `spec.parallelism` down, the higher active indexes will be removed first.
  
  When spec.completions is set, and indexed Job is considered complete when there is one successfully completed Pod
  for each index. For more information about how to use this mode, see
  [Indexed Job for Parallel Processing with Static Work Assignment](/docs/tasks/job/indexed-parallel-processing-static/).
  Note that, although rare, more than one Pod could be started for the same
  index, but only one of them will count towards the completion count.

Finally, in both modes, when `spec.completions` is not set, then the job is considered successful when at least one pod successfully completes.

sftim on (2023-01-07 15:33:56 UTC): I think we need to explain that if `completions` isn't set, indexes can be reused. At least, that's what the phrase

implies.

If we didn't want to reuse the indices, then let's explain (and, by implication, consider the design details for) what happens if we create more Pods than we have parallelism.

I'd like this to have KEP: having an enhancement proposal makes it easier for SIG Release to track the work around updating the docs.

ahg-g (Issue Creator) on (2023-01-07 16:14:14 UTC): What do you mean by indexes can be reused?

sftim on (2023-01-07 16:41:15 UTC): In the Kubernetes blog, [Introducing Indexed Jobs](https://kubernetes.io/blog/2021/04/19/introducing-indexed-jobs/) suggests that

and someone may have made a container image that relies on that assumption. Does this change create the risk of confusing people who do assume this?

Let's say I have a work queue that will turn out to hold 99 items, and I want to use a parallelism of 10. I don't know the size of the work queue at the time I define the Job. I set this new Job to be indexed because all my other (fixed-size) Jobs are indexed.

Some people will want to have an index that matches the ordinal of running Pods like with a StatefulSet, but other people might use the index for something like, say, interpolating into the filename of the Job's output.

If we can handle both of the cases I've sketched here, great. If not, let's document which case we're supporting and add a [callout](https://kubernetes.io/docs/contribute/style/style-guide/#caution) to alert the people who were expecting the opposite behavior.

(This is why I think it's good to draft the docs first - if we're making a design that requires warning users, we might want to reconsider it).

ahg-g (Issue Creator) on (2023-01-07 19:54:32 UTC): Note that he proposed change does not impact existing workloads. We are adding new semantics, not changing existing ones.


The proposed semantics supports a StatefulSet like behavior, the indexes will cover the number of replicas (parallelism). The second use case is an interesting one, but it will not be enabled by the proposed change, and I don't know why one would think that if we are clearly stating that the indexes will range from 0 to spec.parallelism-1 and that success is defined as any pod finishing successfully. I will send out a KEP.


I haven't seen a batch API with these semantics, and to address this use case (number of indexes is not defined before hand) the queue can associate the item with an index, not the job. The work queue pattern by design aims to reuse the pods and treat them as stateless because the state is in the queue. 

Slightly related, we are thinking about adding a `completionPolicy` API to Job (e.g., the job should be declared successful if 80% of the pods successfully completes);  so, if we set `spec.completions` to some maximum number and come up with a completion policy that says ""exit if the queue is empty"", then we could support the case you mention via job, but I am not really convinced that this is the right approach because I believe indexed job isn't the right mode for this use case as I mentioned in the paragraph above.

sathyanarays on (2023-01-08 14:58:09 UTC): Is it more accurate to say ""Finally, in both modes, when spec.completions is not set, then the job is considered successful when at least `N` pod successfully completes where N is the `spec.parallelism`""?

ahg-g (Issue Creator) on (2023-01-09 00:17:54 UTC): No, that is not the current semantics of `spec.completions=nil`; as explained in  https://kubernetes.io/docs/concepts/workloads/controllers/job/#parallel-jobs: 
```
once any Pod has exited with success, no other Pod should still be doing any work for this task or writing any output. They should all be in the process of exiting.
```

The behavior you are asking about can be achieved by setting `spec.completions=spec.parallelism`

sathyanarays on (2023-01-09 04:17:16 UTC): @ahg-g ,
I created a job with the following spec:

```
apiVersion: batch/v1
kind: Job
metadata:
  name: backoff-exp
spec:
  backoffLimit: 4
  parallelism: 10
  template:
    spec:
      containers:
        - name: die
          image: myubuntu:1
          command: [""/bin/sh"",""-c""]
          args: [""/bin/sleep `rand -M 300` && /bin/true""]
          imagePullPolicy: Never
      restartPolicy: Never
```

Each pod sleeps `random` seconds and exits successfully. In this case, the job was marked completed only after all the 10 pods finished.

#### kubectl get pods
```
NAME                READY   STATUS      RESTARTS   AGE
backoff-exp-5mb6m   0/1     Completed   0          4m41s
backoff-exp-6j76z   0/1     Completed   0          4m41s
backoff-exp-6vv88   0/1     Completed   0          4m41s
backoff-exp-7vp8c   0/1     Completed   0          4m41s
backoff-exp-gtz7n   0/1     Completed   0          4m41s
backoff-exp-q9kvt   0/1     Completed   0          4m41s
backoff-exp-vnw7m   0/1     Completed   0          4m41s
backoff-exp-wngkf   0/1     Completed   0          4m41s
backoff-exp-xhq9x   0/1     Completed   0          4m41s
backoff-exp-z4cc9   0/1     Completed   0          4m41s
``` 

#### Start and Finish time for all the above pods
```
2023-01-09T03:56:35Z | 2023-01-09T03:59:55Z
2023-01-09T03:56:35Z | 2023-01-09T03:59:55Z
2023-01-09T03:56:36Z | 2023-01-09T03:59:56Z
2023-01-09T03:56:36Z | 2023-01-09T04:01:14Z
2023-01-09T03:56:36Z | 2023-01-09T04:01:14Z
2023-01-09T03:56:36Z | 2023-01-09T04:01:14Z
2023-01-09T03:56:36Z | 2023-01-09T04:01:14Z
2023-01-09T03:56:37Z | 2023-01-09T03:59:44Z
2023-01-09T03:56:37Z | 2023-01-09T03:59:44Z
2023-01-09T03:56:37Z | 2023-01-09T03:59:44Z
```

#### Status of the completed job
```
  status:
    completionTime: ""2023-01-09T04:01:17Z""
    conditions:
    - lastProbeTime: ""2023-01-09T04:01:17Z""
      lastTransitionTime: ""2023-01-09T04:01:17Z""
      status: ""True""
      type: Complete
    ready: 0
    startTime: ""2023-01-09T03:56:34Z""
    succeeded: 10
    uncountedTerminatedPods: {}
```
The job is marked complete only after the last pod has finished. Is the above behavior correct? 

Please let me know your thoughts on this!

ahg-g (Issue Creator) on (2023-01-09 05:01:55 UTC): Any active pods will not be deleted and will be allowed to run to completion (fail or success), but the job will be marked as **successful** if one pod finishes successfully.

ahg-g (Issue Creator) on (2023-01-09 15:39:40 UTC): I have this one on today's (Jan 9th) sig-apps meeting

ahg-g (Issue Creator) on (2023-01-09 23:21:54 UTC): Meeting outcome: we will write a KEP, but there seems to be an agreement on the general direction as proposed in the initial post.

asm582 on (2023-01-10 02:57:09 UTC): Curious to know If `spec.completions=nil` then will a failed pod will ever be retried?

ahg-g (Issue Creator) on (2023-01-10 15:29:35 UTC): Yes, just like it is today, it will be retried while respecting `backoffLimit`

danielvegamyhre (Assginee) on (2023-03-14 17:05:57 UTC): PR for Elastic Index Job docs (in progress): https://github.com/kubernetes/website/pull/39999

k8s-triage-robot on (2023-06-12 17:20:00 UTC): The Kubernetes project currently lacks enough contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle stale`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle stale

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

vsoch on (2023-06-12 17:48:36 UTC): /remove-lifecycle stale the work is underway!

vsoch on (2023-06-12 17:48:49 UTC): /remove-lifecycle stale

vsoch on (2023-07-27 06:24:50 UTC): hey folks! I wanted to check in on the state of this - it looks like the elastic index job docs PR was merged https://github.com/kubernetes/website/pull/39999 what comes next?

alculquicondor on (2023-07-27 12:21:28 UTC): Since this was completed:
/close

What would you like to see next regarding this topic? However, we probably should open new issue(s)

k8s-ci-robot on (2023-07-27 12:21:34 UTC): @alculquicondor: Closing this issue.

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114862#issuecomment-1653512219):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

vsoch on (2023-07-27 14:17:21 UTC): I thought that might be the case!


We are using this for experiments to scale / reduce our Flux MiniClusters and it's working great! I don't have further comments here but will definitely open new issues or bring up discussion as needed.

"
1521016082,issue,closed,not_planned,Add unit tests for `SelectVictimsOnNode` func in the default preemption,"/kind cleanup
/sig scheduling

`SelectVictimsOnNode` func in the default preemption is complicated, but we don't have any unit tests for it. 

https://github.com/kubernetes/kubernetes/blob/2d534e4bea7c7ef412a03c921867d9027f6e489f/pkg/scheduler/framework/plugins/defaultpreemption/default_preemption.go#L137",sanposhiho,2023-01-05 16:24:50+00:00,['sanposhiho'],2023-06-04 17:50:47+00:00,2023-06-04 17:50:46+00:00,https://github.com/kubernetes/kubernetes/issues/114853,"[('kind/cleanup', 'Categorizes issue or PR as related to cleaning up code, process, or technical debt.'), ('sig/scheduling', 'Categorizes an issue or PR as relevant to SIG Scheduling.'), ('lifecycle/rotten', 'Denotes an issue or PR that has aged beyond stale and will be auto-closed.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1372438503, 'issue_id': 1521016082, 'author': 'k8s-ci-robot', 'body': '@sanposhiho: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 5, 16, 24, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 1372438531, 'issue_id': 1521016082, 'author': 'sanposhiho', 'body': '/assign', 'created_at': datetime.datetime(2023, 1, 5, 16, 24, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 1497823550, 'issue_id': 1521016082, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 4, 5, 16, 53, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 1536565321, 'issue_id': 1521016082, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 5, 5, 17, 29, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 1575655348, 'issue_id': 1521016082, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 6, 4, 17, 50, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 1575655365, 'issue_id': 1521016082, 'author': 'k8s-ci-robot', 'body': '@k8s-triage-robot: Closing this issue, marking it as ""Not Planned"".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114853#issuecomment-1575655348):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 6, 4, 17, 50, 47, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-05 16:24:57 UTC): @sanposhiho: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

sanposhiho (Issue Creator) on (2023-01-05 16:24:59 UTC): /assign

k8s-triage-robot on (2023-04-05 16:53:47 UTC): The Kubernetes project currently lacks enough contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle stale`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle stale

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-triage-robot on (2023-05-05 17:29:22 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle rotten

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-triage-robot on (2023-06-04 17:50:41 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.

This bot triages issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Reopen this issue with `/reopen`
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/close not-planned

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-ci-robot on (2023-06-04 17:50:47 UTC): @k8s-triage-robot: Closing this issue, marking it as ""Not Planned"".

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114853#issuecomment-1575655348):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

"
1520995371,issue,closed,completed,[Flake Test] TestCreateHealthcheck,"### Which jobs are flaking?

`pull-kubernetes-unit`

### Which tests are flaking?

TestCreateHealthcheck/ok_if_response_time_lower_than_custom_timeout 

### Since when has it been flaking?

From https://storage.googleapis.com/k8s-triage/index.html?test=TestCreateHealthcheck, since 12/23.

### Testgrid link

https://testgrid.k8s.io/presubmits-kubernetes-blocking#pull-kubernetes-unit

### Reason for failure (if possible)

_No response_

### Anything else we need to know?

Test Log: https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/114653/pull-kubernetes-unit/1610998548210388992

### Relevant SIG(s)

/sig api-machinery",kerthcet,2023-01-05 16:10:40+00:00,['aojea'],2023-08-21 12:26:06+00:00,2023-08-21 12:26:05+00:00,https://github.com/kubernetes/kubernetes/issues/114852,"[('sig/api-machinery', 'Categorizes an issue or PR as relevant to SIG API Machinery.'), ('kind/flake', 'Categorizes issue or PR as related to a flaky test.'), ('triage/accepted', 'Indicates an issue or PR is ready to be actively worked on.')]","[{'comment_id': 1378170290, 'issue_id': 1520995371, 'author': 'brianpursley', 'body': 'I looked at this today, and here is what I found:\r\n\r\nThe test has a `ready` chan that is supposed to wait for the connection to be established:\r\nhttps://github.com/kubernetes/kubernetes/blob/510a85c53a5138babb1650fadd328e6f34baa03b/staging/src/k8s.io/apiserver/pkg/storage/storagebackend/factory/factory_test.go#L136-L138\r\n\r\nThe chan is closed when `newETCD3Client()` returns.\r\nhttps://github.com/kubernetes/kubernetes/blob/510a85c53a5138babb1650fadd328e6f34baa03b/staging/src/k8s.io/apiserver/pkg/storage/storagebackend/factory/factory_test.go#L112-L115\r\n\r\nHowever, in `newETCD3Check()`, it executes additional code after the call to `newETCD3Client()` returns, so even though the `ready` chan is closed, `clientErr` is not set to nil until line 177:\r\nhttps://github.com/kubernetes/kubernetes/blob/f507bc255382b2e2095351053bc17e74f7100d35/staging/src/k8s.io/apiserver/pkg/storage/storagebackend/factory/etcd3.go#L159-L179\r\n\r\nThere is a mutex, but it is not locked until after the call to `newETCD3Client` returns.\r\n\r\nHere is the race condition:  If you call the healthcheck func after read chan is closed, but before the mutex is locked, the healthcheck func will see `clientErr` is still set to ""etcd client connection not yet established"" and return it:\r\nhttps://github.com/kubernetes/kubernetes/blob/f507bc255382b2e2095351053bc17e74f7100d35/staging/src/k8s.io/apiserver/pkg/storage/storagebackend/factory/etcd3.go#L209-L211\r\n\r\nThoughts on how to fix this?\r\n\r\nCould it be as simple as moving `lock.Lock()` above the call to `newETCD3Client` on line 160?  I think that fixes the race condition, but could end up blocking an initial healthcheck call if `newETCD3Client` takes a long time to return (in the real world).\r\n\r\nI thought of a few other approaches involving using a chan to signal when the etcd3 client connection REALLY is established, but I don\'t know if that would be of general interest or only something that would be used by the unit test.', 'created_at': datetime.datetime(2023, 1, 11, 2, 40, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 1378859352, 'issue_id': 1520995371, 'author': 'aojea', 'body': ""good analysis\r\n\r\n> // PollUntil always waits interval before the first run of 'condition'.\r\n> // 'condition' will always be invoked at least once.\r\n\r\nso, it will not change the err until 1 second, no?\r\n\r\n```\r\n go wait.PollUntil(time.Second, func() (bool, error) { \r\n \tnewClient, err := newETCD3Client(c.Transport) \r\n```\r\n\r\nmaybe we can PollInmidiate or account for that well-known second?"", 'created_at': datetime.datetime(2023, 1, 11, 14, 45, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 1378861735, 'issue_id': 1520995371, 'author': 'aojea', 'body': '/triage accepted\r\n\r\nThe other tests in the same file are affected to, please add the proof that solves the flake by running the test with `stress` to the description of the PR and assign to me for review\r\n\r\nThanks', 'created_at': datetime.datetime(2023, 1, 11, 14, 47, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 1682778384, 'issue_id': 1520995371, 'author': 'liggitt', 'body': 'seeing this flake pretty regularly - https://storage.googleapis.com/k8s-triage/index.html?test=TestCreateHealthcheck', 'created_at': datetime.datetime(2023, 8, 17, 18, 37, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 1684926193, 'issue_id': 1520995371, 'author': 'aojea', 'body': '@liggitt https://github.com/kubernetes/kubernetes/pull/119824', 'created_at': datetime.datetime(2023, 8, 19, 11, 49, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 1684926219, 'issue_id': 1520995371, 'author': 'aojea', 'body': '/assign', 'created_at': datetime.datetime(2023, 8, 19, 11, 49, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 1686233109, 'issue_id': 1520995371, 'author': 'liggitt', 'body': 'fixed in https://github.com/kubernetes/kubernetes/pull/119824', 'created_at': datetime.datetime(2023, 8, 21, 12, 26, 5, tzinfo=datetime.timezone.utc)}]","brianpursley on (2023-01-11 02:40:26 UTC): I looked at this today, and here is what I found:

The test has a `ready` chan that is supposed to wait for the connection to be established:
https://github.com/kubernetes/kubernetes/blob/510a85c53a5138babb1650fadd328e6f34baa03b/staging/src/k8s.io/apiserver/pkg/storage/storagebackend/factory/factory_test.go#L136-L138

The chan is closed when `newETCD3Client()` returns.
https://github.com/kubernetes/kubernetes/blob/510a85c53a5138babb1650fadd328e6f34baa03b/staging/src/k8s.io/apiserver/pkg/storage/storagebackend/factory/factory_test.go#L112-L115

However, in `newETCD3Check()`, it executes additional code after the call to `newETCD3Client()` returns, so even though the `ready` chan is closed, `clientErr` is not set to nil until line 177:
https://github.com/kubernetes/kubernetes/blob/f507bc255382b2e2095351053bc17e74f7100d35/staging/src/k8s.io/apiserver/pkg/storage/storagebackend/factory/etcd3.go#L159-L179

There is a mutex, but it is not locked until after the call to `newETCD3Client` returns.

Here is the race condition:  If you call the healthcheck func after read chan is closed, but before the mutex is locked, the healthcheck func will see `clientErr` is still set to ""etcd client connection not yet established"" and return it:
https://github.com/kubernetes/kubernetes/blob/f507bc255382b2e2095351053bc17e74f7100d35/staging/src/k8s.io/apiserver/pkg/storage/storagebackend/factory/etcd3.go#L209-L211

Thoughts on how to fix this?

Could it be as simple as moving `lock.Lock()` above the call to `newETCD3Client` on line 160?  I think that fixes the race condition, but could end up blocking an initial healthcheck call if `newETCD3Client` takes a long time to return (in the real world).

I thought of a few other approaches involving using a chan to signal when the etcd3 client connection REALLY is established, but I don't know if that would be of general interest or only something that would be used by the unit test.

aojea (Assginee) on (2023-01-11 14:45:57 UTC): good analysis


so, it will not change the err until 1 second, no?

```
 go wait.PollUntil(time.Second, func() (bool, error) { 
 	newClient, err := newETCD3Client(c.Transport) 
```

maybe we can PollInmidiate or account for that well-known second?

aojea (Assginee) on (2023-01-11 14:47:25 UTC): /triage accepted

The other tests in the same file are affected to, please add the proof that solves the flake by running the test with `stress` to the description of the PR and assign to me for review

Thanks

liggitt on (2023-08-17 18:37:18 UTC): seeing this flake pretty regularly - https://storage.googleapis.com/k8s-triage/index.html?test=TestCreateHealthcheck

aojea (Assginee) on (2023-08-19 11:49:44 UTC): @liggitt https://github.com/kubernetes/kubernetes/pull/119824

aojea (Assginee) on (2023-08-19 11:49:52 UTC): /assign

liggitt on (2023-08-21 12:26:05 UTC): fixed in https://github.com/kubernetes/kubernetes/pull/119824

"
1520397896,issue,closed,completed,"Watchcache doesn't distinguish the RV=0 and RV="""" semantic for watches","As Jordan pointed out in:
https://github.com/kubernetes/enhancements/pull/3667/files#r1055906242

watchcache treats RV=0 and RV="""" the same way, even though it's not what we document:
https://kubernetes.io/docs/reference/using-api/api-concepts/#semantics-for-watch

This should be fixed.

/kind bug
/sig api-machinery
/priority important-soon",wojtek-t,2023-01-05 09:26:42+00:00,['MadhavJivrajani'],2023-02-21 12:07:57+00:00,2023-02-21 12:07:57+00:00,https://github.com/kubernetes/kubernetes/issues/114845,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('priority/important-soon', 'Must be staffed and worked on either currently, or very soon, ideally in time for the next release.'), ('sig/api-machinery', 'Categorizes an issue or PR as relevant to SIG API Machinery.'), ('triage/accepted', 'Indicates an issue or PR is ready to be actively worked on.')]","[{'comment_id': 1373469679, 'issue_id': 1520397896, 'author': 'MadhavJivrajani', 'body': '/assign', 'created_at': datetime.datetime(2023, 1, 6, 10, 58, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 1380780588, 'issue_id': 1520397896, 'author': 'cici37', 'body': '/triage accepted', 'created_at': datetime.datetime(2023, 1, 12, 17, 48, 2, tzinfo=datetime.timezone.utc)}]","MadhavJivrajani (Assginee) on (2023-01-06 10:58:14 UTC): /assign

cici37 on (2023-01-12 17:48:02 UTC): /triage accepted

"
1520297242,issue,closed,not_planned,move descheduler functions into kube-scheduler,"### What would you like to be added?

Descheduler no longer acts as a separate component, but combines with kube-scheduler. That is,  move descheduler functions into kube-scheduler.

### Why is this needed?

Descheduler and kube-scheduler should have consistent perspective about cluster nodes to make consistent decisions. ",snowplayfire,2023-01-05 08:16:24+00:00,[],2024-01-18 22:59:13+00:00,2024-01-18 22:59:12+00:00,https://github.com/kubernetes/kubernetes/issues/114841,"[('sig/scheduling', 'Categorizes an issue or PR as relevant to SIG Scheduling.'), ('kind/feature', 'Categorizes issue or PR as related to a new feature.'), ('lifecycle/rotten', 'Denotes an issue or PR that has aged beyond stale and will be auto-closed.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1371906255, 'issue_id': 1520297242, 'author': 'k8s-ci-robot', 'body': '@snowplayfire: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 5, 8, 16, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371907708, 'issue_id': 1520297242, 'author': 'snowplayfire', 'body': '/sig scheduling', 'created_at': datetime.datetime(2023, 1, 5, 8, 18, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 1436235442, 'issue_id': 1520297242, 'author': 'kerthcet', 'body': ""Descheduler does not play a role of scheduling evicted pods, but still rely on default kube-scheduler. And it's not a fully necessary component compared to kube-scheduler, keep it out of kubernetes is reasonable I think, if not, there're so many components can be plugged into k/k, like NFD etc.. I'm not fairly familiar with descheduler, cc @damemi as maintainer for advices.\r\n\r\nKubernetes is so complex right now, I think keep it as simple as possible is something we're chasing for but still very difficult..."", 'created_at': datetime.datetime(2023, 2, 20, 2, 52, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 1438739504, 'issue_id': 1520297242, 'author': 'damemi', 'body': ""+1 what @kerthcet said\r\n\r\nWhile it is true that the descheduler could provide an easier UX and some more functionality that worked out of the box if it were more closely coupled with the default scheduler, it would also add constraints. The effort to do so would also be contrary to the direction the kube-scheduler has been growing, with more emphasis on the scheduling framework and modular customizability over a centralized monolithic component.\r\n\r\nWe do recognize the importance of the descheduler remaining in-sync with the scheduler, which is why we started projects like [cleaning up scheduler dependencies on k/k](https://github.com/kubernetes/kubernetes/issues/89930), which has since stalled out. The goal with this was to achieve a similar goal through an opposite approach: descheduler could depend on the same functionality as the scheduler if that functionality was externalized.\r\n\r\nStill, the descheduler and scheduler sharing the same internal view of the cluster state could benefit the descheduler by allowing it to lean on what the scheduler already knows and make smarter, more efficient eviction decisions. However, the scheduler doesn't get any functional benefit from being tied to the descheduler. It only adds maintenance burden.\r\n\r\nI think there is a lot of work that could be done to make the descheduler easier to use by default (and most of that is probably on the descheduler side rather than kube-scheduler), but I'm not sure building that into kube-scheduler is the right solution. I'm open to hear what others' opinions are."", 'created_at': datetime.datetime(2023, 2, 21, 16, 8, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 1438740604, 'issue_id': 1520297242, 'author': 'damemi', 'body': 'cc @ingvagabund @knelasevero @seanmalloy @a7i', 'created_at': datetime.datetime(2023, 2, 21, 16, 9, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 1558458314, 'issue_id': 1520297242, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 5, 23, 3, 29, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 1601985338, 'issue_id': 1520297242, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 6, 22, 4, 10, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 1899352371, 'issue_id': 1520297242, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2024, 1, 18, 22, 59, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 1899352457, 'issue_id': 1520297242, 'author': 'k8s-ci-robot', 'body': '@k8s-triage-robot: Closing this issue, marking it as ""Not Planned"".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114841#issuecomment-1899352371):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2024, 1, 18, 22, 59, 12, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-05 08:16:33 UTC): @snowplayfire: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

snowplayfire (Issue Creator) on (2023-01-05 08:18:18 UTC): /sig scheduling

kerthcet on (2023-02-20 02:52:47 UTC): Descheduler does not play a role of scheduling evicted pods, but still rely on default kube-scheduler. And it's not a fully necessary component compared to kube-scheduler, keep it out of kubernetes is reasonable I think, if not, there're so many components can be plugged into k/k, like NFD etc.. I'm not fairly familiar with descheduler, cc @damemi as maintainer for advices.

Kubernetes is so complex right now, I think keep it as simple as possible is something we're chasing for but still very difficult...

damemi on (2023-02-21 16:08:36 UTC): +1 what @kerthcet said

While it is true that the descheduler could provide an easier UX and some more functionality that worked out of the box if it were more closely coupled with the default scheduler, it would also add constraints. The effort to do so would also be contrary to the direction the kube-scheduler has been growing, with more emphasis on the scheduling framework and modular customizability over a centralized monolithic component.

We do recognize the importance of the descheduler remaining in-sync with the scheduler, which is why we started projects like [cleaning up scheduler dependencies on k/k](https://github.com/kubernetes/kubernetes/issues/89930), which has since stalled out. The goal with this was to achieve a similar goal through an opposite approach: descheduler could depend on the same functionality as the scheduler if that functionality was externalized.

Still, the descheduler and scheduler sharing the same internal view of the cluster state could benefit the descheduler by allowing it to lean on what the scheduler already knows and make smarter, more efficient eviction decisions. However, the scheduler doesn't get any functional benefit from being tied to the descheduler. It only adds maintenance burden.

I think there is a lot of work that could be done to make the descheduler easier to use by default (and most of that is probably on the descheduler side rather than kube-scheduler), but I'm not sure building that into kube-scheduler is the right solution. I'm open to hear what others' opinions are.

damemi on (2023-02-21 16:09:20 UTC): cc @ingvagabund @knelasevero @seanmalloy @a7i

k8s-triage-robot on (2023-05-23 03:29:09 UTC): The Kubernetes project currently lacks enough contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle stale`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle stale

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-triage-robot on (2023-06-22 04:10:54 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle rotten

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-triage-robot on (2024-01-18 22:59:06 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.

This bot triages issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Reopen this issue with `/reopen`
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/close not-planned

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-ci-robot on (2024-01-18 22:59:12 UTC): @k8s-triage-robot: Closing this issue, marking it as ""Not Planned"".

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114841#issuecomment-1899352371):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

"
1520261886,issue,closed,completed,CHANGELOG.md: Modified the content of the symbol file ,"### What happened?

https://github.com/kubernetes/kubernetes/pull/113156

The change of this PR has resulted in the exception of the file. The file was originally a symbol file, but now other content has been added, resulting in the exception of the file symbol.

### What did you expect to happen?

Rollback to the correct symbol file

### How can we reproduce it (as minimally and precisely as possible)?

https://github.com/kubernetes/kubernetes/pull/113156#issuecomment-1357047936

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


### Cloud provider

<details>

</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",aimuz,2023-01-05 07:50:10+00:00,[],2023-01-05 13:32:11+00:00,2023-01-05 13:32:11+00:00,https://github.com/kubernetes/kubernetes/issues/114839,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('priority/important-soon', 'Must be staffed and worked on either currently, or very soon, ideally in time for the next release.'), ('kind/documentation', 'Categorizes issue or PR as related to documentation.'), ('sig/release', 'Categorizes an issue or PR as relevant to SIG Release.'), ('triage/accepted', 'Indicates an issue or PR is ready to be actively worked on.')]","[{'comment_id': 1371884613, 'issue_id': 1520261886, 'author': 'aimuz', 'body': '/kind documentation', 'created_at': datetime.datetime(2023, 1, 5, 7, 50, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 1372123073, 'issue_id': 1520261886, 'author': 'gjkim42', 'body': '/triage accepted\r\n/priority important-soon\r\n/sig release', 'created_at': datetime.datetime(2023, 1, 5, 11, 54, 52, tzinfo=datetime.timezone.utc)}]","aimuz (Issue Creator) on (2023-01-05 07:50:47 UTC): /kind documentation

gjkim42 on (2023-01-05 11:54:52 UTC): /triage accepted
/priority important-soon
/sig release

"
1520094160,issue,closed,completed,Gorilla WebSocket has been archived,"### What would you like to be added?

Replace wesocket package from gorila to
- https://github.com/nhooyr/websocket

### Why is this needed?

gorila websocket has been archived",fatelei,2023-01-05 05:18:48+00:00,[],2023-01-06 15:59:25+00:00,2023-01-06 15:59:25+00:00,https://github.com/kubernetes/kubernetes/issues/114836,"[('kind/feature', 'Categorizes issue or PR as related to a new feature.'), ('sig/architecture', 'Categorizes an issue or PR as relevant to SIG Architecture.'), ('area/code-organization', 'Issues or PRs related to kubernetes code organization'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1371793708, 'issue_id': 1520094160, 'author': 'k8s-ci-robot', 'body': '@fatelei: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 5, 5, 18, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371836381, 'issue_id': 1520094160, 'author': 'pacoxu', 'body': ""/sig architecture\r\n/area code-organization\r\n/retitle Gorilla WebSocket has been archived\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/293bf70916de8ef61d5f868f53959f1e15b3e091/hack/unwanted-dependencies.json#L17\r\n\r\nhttps://github.com/kubernetes/kubernetes/blob/293bf70916de8ef61d5f868f53959f1e15b3e091/hack/unwanted-dependencies.json#L59-L62\r\n\r\nWe don't use it directly \r\n```\r\n➜  kubernetes git:(master) go mod graph | grep gorilla\r\nk8s.io/kubernetes github.com/gorilla/websocket@v1.4.2\r\ngithub.com/moby/spdystream@v0.2.0 github.com/gorilla/websocket@v1.4.2\r\ngithub.com/tmc/grpc-websocket-proxy@v0.0.0-20220101234140-673ab2c3ae75 github.com/gorilla/websocket@v1.4.2\r\nk8s.io/apiextensions-apiserver@v0.0.0 github.com/gorilla/websocket@v1.4.2\r\nk8s.io/apiserver@v0.0.0 github.com/gorilla/websocket@v1.4.2\r\ngithub.com/spf13/viper@v1.7.0 github.com/gorilla/websocket@v1.4.2\r\ngithub.com/spf13/viper@v1.4.0 github.com/gorilla/websocket@v1.4.0\r\n```"", 'created_at': datetime.datetime(2023, 1, 5, 6, 36, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 1373822335, 'issue_id': 1520094160, 'author': 'aojea', 'body': '@liggitt already has done an analysis of this IIRC, I think that we only had gorilla dependency for some tests', 'created_at': datetime.datetime(2023, 1, 6, 15, 48, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 1373831385, 'issue_id': 1520094160, 'author': 'dims', 'body': 'Here: https://github.com/kubernetes/kubernetes/pull/114408', 'created_at': datetime.datetime(2023, 1, 6, 15, 57, 59, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-05 05:18:56 UTC): @fatelei: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

pacoxu on (2023-01-05 06:36:06 UTC): /sig architecture
/area code-organization
/retitle Gorilla WebSocket has been archived

https://github.com/kubernetes/kubernetes/blob/293bf70916de8ef61d5f868f53959f1e15b3e091/hack/unwanted-dependencies.json#L17

https://github.com/kubernetes/kubernetes/blob/293bf70916de8ef61d5f868f53959f1e15b3e091/hack/unwanted-dependencies.json#L59-L62

We don't use it directly 
```
➜  kubernetes git:(master) go mod graph | grep gorilla
k8s.io/kubernetes github.com/gorilla/websocket@v1.4.2
github.com/moby/spdystream@v0.2.0 github.com/gorilla/websocket@v1.4.2
github.com/tmc/grpc-websocket-proxy@v0.0.0-20220101234140-673ab2c3ae75 github.com/gorilla/websocket@v1.4.2
k8s.io/apiextensions-apiserver@v0.0.0 github.com/gorilla/websocket@v1.4.2
k8s.io/apiserver@v0.0.0 github.com/gorilla/websocket@v1.4.2
github.com/spf13/viper@v1.7.0 github.com/gorilla/websocket@v1.4.2
github.com/spf13/viper@v1.4.0 github.com/gorilla/websocket@v1.4.0
```

aojea on (2023-01-06 15:48:40 UTC): @liggitt already has done an analysis of this IIRC, I think that we only had gorilla dependency for some tests

dims on (2023-01-06 15:57:59 UTC): Here: https://github.com/kubernetes/kubernetes/pull/114408

"
1519928068,issue,closed,completed,container_fs_usage_bytes metrics still has no data on containerd,"### What happened?

`container_fs_xxx` metrics still has no data on containerd,

Setting `PodAndContainerStatsFromCRI=true` is also not work,but `crictl stats`is ok 

### What did you expect to happen?

The container_fs_usage_bytes metric has data.

### How can we reproduce it (as minimally and precisely as possible)?

init a k8s cluster use kubeadm

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
kubectl version 1.24.8
contained 1.5.11
```

</details>



### Cloud provider

<details>
none
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",vsxen,2023-01-05 02:26:43+00:00,['qiutongs'],2023-01-19 04:24:35+00:00,2023-01-18 21:00:53+00:00,https://github.com/kubernetes/kubernetes/issues/114833,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('area/cadvisor', None), ('sig/node', 'Categorizes an issue or PR as relevant to SIG Node.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1371691126, 'issue_id': 1519928068, 'author': 'k8s-ci-robot', 'body': '@vsxen: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 5, 2, 26, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371693332, 'issue_id': 1519928068, 'author': 'vsxen', 'body': '/sig node', 'created_at': datetime.datetime(2023, 1, 5, 2, 28, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371825658, 'issue_id': 1519928068, 'author': 'pacoxu', 'body': '/area cadvisor\r\n/cc @bobbypage', 'created_at': datetime.datetime(2023, 1, 5, 6, 15, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 1379334739, 'issue_id': 1519928068, 'author': 'SergeyKanzhelev', 'body': '/assign @qiutongs \r\n\r\n@qiutongs can you please de-dup this issue and suggest a workaround?', 'created_at': datetime.datetime(2023, 1, 11, 18, 46, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 1396081510, 'issue_id': 1519928068, 'author': 'SergeyKanzhelev', 'body': 'Unfortunately this is still the latest update we have: https://github.com/kubernetes/kubernetes/issues/89903#issuecomment-1044978911\r\n\r\n@bobbypage in case a switch to CRI metrics will help\r\n\r\n/close', 'created_at': datetime.datetime(2023, 1, 18, 21, 0, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 1396081618, 'issue_id': 1519928068, 'author': 'k8s-ci-robot', 'body': '@SergeyKanzhelev: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114833#issuecomment-1396081510):\n\n>Unfortunately this is still the latest update we have: https://github.com/kubernetes/kubernetes/issues/89903#issuecomment-1044978911\r\n>\r\n>@bobbypage in case a switch to CRI metrics will help\r\n>\r\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 18, 21, 0, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 1396087447, 'issue_id': 1519928068, 'author': 'bobbypage', 'body': '> @bobbypage in case a switch to CRI metrics will help\r\n\r\nYes, I believe long term switch to CRI metrics should address this.', 'created_at': datetime.datetime(2023, 1, 18, 21, 5, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 1396423209, 'issue_id': 1519928068, 'author': 'vsxen', 'body': 'So which k8s version fixes this bug? or use CRI-O?', 'created_at': datetime.datetime(2023, 1, 19, 4, 24, 35, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-05 02:26:51 UTC): @vsxen: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

vsxen (Issue Creator) on (2023-01-05 02:28:20 UTC): /sig node

pacoxu on (2023-01-05 06:15:43 UTC): /area cadvisor
/cc @bobbypage

SergeyKanzhelev on (2023-01-11 18:46:50 UTC): /assign @qiutongs 

@qiutongs can you please de-dup this issue and suggest a workaround?

SergeyKanzhelev on (2023-01-18 21:00:47 UTC): Unfortunately this is still the latest update we have: https://github.com/kubernetes/kubernetes/issues/89903#issuecomment-1044978911

@bobbypage in case a switch to CRI metrics will help

/close

k8s-ci-robot on (2023-01-18 21:00:53 UTC): @SergeyKanzhelev: Closing this issue.

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114833#issuecomment-1396081510):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

bobbypage on (2023-01-18 21:05:44 UTC): Yes, I believe long term switch to CRI metrics should address this.

vsxen (Issue Creator) on (2023-01-19 04:24:35 UTC): So which k8s version fixes this bug? or use CRI-O?

"
1519579666,issue,closed,completed,Allow PreScore to return `Skip` status to skip running the corresponding Score extension,"### What would you like to be added?

In some scheduler plugins, PreScore is implemented to pre-process state before running Score for each node. In some cases, PreScore can decide that Score shouldn't run at all (e.g., no relevant PodTopologySpread constraints), and so it will be useful to have an explicit way to communicate that.

### Why is this needed?

To potentially improve performance (likely negligible), but this also offer a better API (vs having each plugin communicate a special value internally)

/sig scheduling",ahg-g,2023-01-04 21:14:24+00:00,"['sanposhiho', 'AxeZhan']",2023-02-14 03:13:53+00:00,2023-02-13 22:53:31+00:00,https://github.com/kubernetes/kubernetes/issues/114827,"[('sig/scheduling', 'Categorizes an issue or PR as relevant to SIG Scheduling.'), ('kind/feature', 'Categorizes issue or PR as related to a new feature.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1371426929, 'issue_id': 1519579666, 'author': 'k8s-ci-robot', 'body': '@ahg-g: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 4, 21, 14, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371427758, 'issue_id': 1519579666, 'author': 'alculquicondor', 'body': '/assign @mwielgus', 'created_at': datetime.datetime(2023, 1, 4, 21, 15, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371428585, 'issue_id': 1519579666, 'author': 'alculquicondor', 'body': 'This is a follow up to #114125', 'created_at': datetime.datetime(2023, 1, 4, 21, 16, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 1422519400, 'issue_id': 1519579666, 'author': 'AxeZhan', 'body': 'Ummm... Do you need volunteers?', 'created_at': datetime.datetime(2023, 2, 8, 12, 30, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 1422605054, 'issue_id': 1519579666, 'author': 'alculquicondor', 'body': '/unassign\r\n@sanposhiho is this something you would like to take or mentor on?', 'created_at': datetime.datetime(2023, 2, 8, 13, 31, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 1423390593, 'issue_id': 1519579666, 'author': 'sanposhiho', 'body': ""I can work on the implementation on the framework side.\r\n\r\nFor each plugin's Skip implementation, we can ask for some volunteers like https://github.com/kubernetes/kubernetes/issues/114399 later.\r\n\r\n/assign\r\n/unassign @mwielgus"", 'created_at': datetime.datetime(2023, 2, 8, 23, 54, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 1423475284, 'issue_id': 1519579666, 'author': 'AxeZhan', 'body': ""@sanposhiho Can I try this one?:) I think this change is similar to #114125 and I should be able to handle it. Ignore this if you've already started working, or think I'm wrong."", 'created_at': datetime.datetime(2023, 2, 9, 1, 23, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 1423487221, 'issue_id': 1519579666, 'author': 'sanposhiho', 'body': '@kidddddddddddddddddddddd \r\n\r\nExactly, sure please work on the framework implementation side then. 👍 Feel free to ping me if any help is needed.\r\n\r\n/assign @kidddddddddddddddddddddd', 'created_at': datetime.datetime(2023, 2, 9, 1, 38, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 1429049946, 'issue_id': 1519579666, 'author': 'sanposhiho', 'body': 'Created: https://github.com/kubernetes/kubernetes/issues/115745', 'created_at': datetime.datetime(2023, 2, 14, 3, 13, 53, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-04 21:14:31 UTC): @ahg-g: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

alculquicondor on (2023-01-04 21:15:15 UTC): /assign @mwielgus

alculquicondor on (2023-01-04 21:16:06 UTC): This is a follow up to #114125

AxeZhan (Assginee) on (2023-02-08 12:30:03 UTC): Ummm... Do you need volunteers?

alculquicondor on (2023-02-08 13:31:08 UTC): /unassign
@sanposhiho is this something you would like to take or mentor on?

sanposhiho (Assginee) on (2023-02-08 23:54:16 UTC): I can work on the implementation on the framework side.

For each plugin's Skip implementation, we can ask for some volunteers like https://github.com/kubernetes/kubernetes/issues/114399 later.

/assign
/unassign @mwielgus

AxeZhan (Assginee) on (2023-02-09 01:23:15 UTC): @sanposhiho Can I try this one?:) I think this change is similar to #114125 and I should be able to handle it. Ignore this if you've already started working, or think I'm wrong.

sanposhiho (Assginee) on (2023-02-09 01:38:55 UTC): @kidddddddddddddddddddddd 

Exactly, sure please work on the framework implementation side then. 👍 Feel free to ping me if any help is needed.

/assign @kidddddddddddddddddddddd

sanposhiho (Assginee) on (2023-02-14 03:13:53 UTC): Created: https://github.com/kubernetes/kubernetes/issues/115745

"
1519476958,issue,closed,completed,kubeadm unit test requires root k8s.io/kubernetes/cmd/kubeadm/app/cmd/upgrade: TestEnforceRequirements/Fail_pre-flight_check ,"### What happened?

https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/92316/pull-kubernetes-unit-experimental/1610708038858051584

```
{Failed  === RUN   TestEnforceRequirements/Fail_pre-flight_check
[upgrade/config] Making sure the configuration is correct:
[preflight] Running pre-flight checks.
    common_test.go:113: enforceRequirements returned unexpected error, expected: ERROR CoreDNSUnsupportedPlugins, got [preflight] Some fatal errors occurred:
        	[ERROR IsPrivilegedUser]: user is not running as root
        [preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
    --- FAIL: TestEnforceRequirements/Fail_pre-flight_check (0.00s)
}
```

### What did you expect to happen?

_unit_ tests should run without root.

This test should be refactored or otherwise removed from `make test`. `make test` should pass as an ordinary user. https://github.com/kubernetes/kubernetes/issues/99881

### How can we reproduce it (as minimally and precisely as possible)?

run `cmd/kubeadm/app/cmd/upgrade` tests as non-root


",BenTheElder,2023-01-04 19:34:39+00:00,[],2023-01-05 08:52:00+00:00,2023-01-05 08:52:00+00:00,https://github.com/kubernetes/kubernetes/issues/114824,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('sig/cluster-lifecycle', 'Categorizes an issue or PR as relevant to SIG Cluster Lifecycle.'), ('area/kubeadm', None), ('triage/accepted', 'Indicates an issue or PR is ready to be actively worked on.')]","[{'comment_id': 1371337595, 'issue_id': 1519476958, 'author': 'BenTheElder', 'body': '/sig cluster-lifecycle\r\n/area kubeadm', 'created_at': datetime.datetime(2023, 1, 4, 19, 35, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371550925, 'issue_id': 1519476958, 'author': 'neolit123', 'body': '@BenTheElder thanks for logging the issue.\r\n\r\n@chendave this was changed in https://github.com/kubernetes/kubernetes/commit/2121ce17dacdd7d022b05fe6ec12d089dcf14cb4 so this root preflight test must be excluded.', 'created_at': datetime.datetime(2023, 1, 4, 23, 43, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371683883, 'issue_id': 1519476958, 'author': 'pacoxu', 'body': '/triage accepted\r\nlink https://github.com/kubernetes/kubernetes/pull/114080', 'created_at': datetime.datetime(2023, 1, 5, 2, 20, 29, tzinfo=datetime.timezone.utc)}]","BenTheElder (Issue Creator) on (2023-01-04 19:35:12 UTC): /sig cluster-lifecycle
/area kubeadm

neolit123 on (2023-01-04 23:43:39 UTC): @BenTheElder thanks for logging the issue.

@chendave this was changed in https://github.com/kubernetes/kubernetes/commit/2121ce17dacdd7d022b05fe6ec12d089dcf14cb4 so this root preflight test must be excluded.

pacoxu on (2023-01-05 02:20:29 UTC): /triage accepted
link https://github.com/kubernetes/kubernetes/pull/114080

"
1518924487,issue,closed,completed,NodePort will not listen on external IP if same IP is used as loadBalancerIP,"### What happened?

Hey, after upgrading kubernetes from 1.22 to 1.23, I'm experiencing strange service provisioning behavior using nodePort. I'm using kube-proxy (ipvs) + calico + metallb. 
If external IP is used by another service type (Loadbalancer - allocated by metallb), then the node owning this IP doesn't listen to NodePort on this interface.  
Unfortunately, I can't find any changes that could be causing this, any ideas?
Kubernetes 1.23
```yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    metallb.universe.tf/loadBalancerIPs: externalip1
  name: service
spec:
  ports:
  - name: client
    port: 31010
    protocol: TCP
    targetPort: client
  type: LoadBalancer
---
apiVersion: v1
kind: Service
metadata:
  name: service2
spec:
  ports:
  - appProtocol: http
    name: http
    nodePort: 32080
    port: 80
    protocol: TCP
    targetPort: http
  - appProtocol: https
    name: https
    nodePort: 32443
    port: 443
    protocol: TCP
    targetPort: https
  type: NodePort
```
``` console
ipvsadm -Ln | grep externalip1
node1:
    TCP  externalip1:31010 lc
    TCP  externalip1:32010 lc
node2:
    TCP  externalip1:31010 lc
    TCP  externalip1:32010 lc
node3:
    TCP  externalip1:31010 lc
    TCP  externalip1:32010 lc
```
Missing externalip1 here: 
```console
ipvsadm -L -n | grep 32443
node1:
    TCP  172.17.0.1:32443 lc
    TCP  10.20.10.13:32443 lc
    TCP  10.50.135.0:32443 lc
node2:
    TCP  externalip2:32443 lc
    TCP  172.17.0.1:32443 lc
    TCP  10.20.10.12:32443 lc
    TCP  10.58.88.192:32443 lc
node3:
    TCP  externalip3:32443 lc
    TCP  172.17.0.1:32443 lc
    TCP  172.26.0.1:32443 lc
    TCP  10.20.10.10:32443 lc
    TCP  10.20.10.11:32443 lc
    TCP  10.63.32.192:32443 lc
 ```

### What did you expect to happen?

Using k8s 1.22 service with nodeport caused listening on every interface, despite using external IP by another service.

Kubernetes 1.22
```yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    metallb.universe.tf/loadBalancerIPs: externalip1
  name: service
spec:
  ports:
  - name: client
    port: 31010
    protocol: TCP
    targetPort: client
  type: LoadBalancer
---
apiVersion: v1
kind: Service
metadata:
  name: service2
spec:
  ports:
  - appProtocol: http
    name: http
    nodePort: 32080
    port: 80
    protocol: TCP
    targetPort: http
  - appProtocol: https
    name: https
    nodePort: 32443
    port: 443
    protocol: TCP
    targetPort: https
  type: NodePort
```
``` console
ipvsadm -Ln | grep externalip1
node1:
    TCP  externalip1:31010 lc
    TCP  externalip1:32010 lc
node2:
    TCP  externalip1:31010 lc
    TCP  externalip1:32010 lc
node3:
    TCP  externalip1:31010 lc
    TCP  externalip1:32010 lc
```
```console
ipvsadm -Ln | grep 32443
node1:
    TCP  172.17.0.1:32443 lc
    TCP  externalip1:32443 lc
    TCP  10.10.10.1:32443 lc
    TCP  10.36.174.0:32443 lc
node2:
    TCP  172.17.0.1:32443 lc
    TCP  externalip2:32443 lc
    TCP  10.10.10.4:32443 lc
    TCP  10.45.192.0:32443 lc
node3:
    TCP  172.17.0.1:32443 lc
    TCP  externalip3:32443 lc
    TCP  10.10.10.2:32443 lc
    TCP  10.45.11.192:32443 lc
```


### How can we reproduce it (as minimally and precisely as possible)?

Upgrade k8s from 1.22.15 to 1.23.15

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
Client Version: version.Info{Major:""1"", Minor:""23"", GitVersion:""v1.23.15"", GitCommit:""b84cb8ab29366daa1bba65bc67f54de2f6c34848"", GitTreeState:""clean"", BuildDate:""2022-12-08T10:49:13Z"", GoVersion:""go1.17.13"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""23"", GitVersion:""v1.23.15"", GitCommit:""b84cb8ab29366daa1bba65bc67f54de2f6c34848"", GitTreeState:""clean"", BuildDate:""2022-12-08T10:42:57Z"", GoVersion:""go1.17.13"", Compiler:""gc"", Platform:""linux/amd64""}
```

</details>


### Cloud provider

<details>
on-premise instances
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME=""Ubuntu""
VERSION=""20.04.5 LTS (Focal Fossa)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 20.04.5 LTS""
VERSION_ID=""20.04""
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal
$ uname -a
Linux D001-FSN1DC17 6.0.8-060008-generic #202211110629-Ubuntu SMP PREEMPT_DYNAMIC Fri Nov 11 06:36:01 UTC x86_64 x86_64 x86_64 GNU/Linux

```

</details>


### Install tools

<details>
kubeadm
</details>


### Container runtime (CRI) and version (if applicable)

<details>
docker
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico v3.24.5

metallb v0.13.7
ipset v7.5, protocol version: 7
ipvsadm v1.31 2019/12/24 (compiled with popt and IPVS v1.2.1)

#ipvs config: 
```console
    ipvs:
      excludeCIDRs: null
      scheduler: lc
      strictARP: true
```
</details>
",adr-xyt,2023-01-04 12:50:03+00:00,['uablrek'],2023-04-28 16:14:18+00:00,2023-04-28 16:14:18+00:00,https://github.com/kubernetes/kubernetes/issues/114815,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('sig/network', 'Categorizes an issue or PR as relevant to SIG Network.'), ('area/ipvs', None), ('triage/accepted', 'Indicates an issue or PR is ready to be actively worked on.')]","[{'comment_id': 1370893708, 'issue_id': 1518924487, 'author': 'adr-xyt', 'body': '/sig network', 'created_at': datetime.datetime(2023, 1, 4, 12, 51, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370942566, 'issue_id': 1518924487, 'author': 'uablrek', 'body': '/assign', 'created_at': datetime.datetime(2023, 1, 4, 13, 40, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370956075, 'issue_id': 1518924487, 'author': 'uablrek', 'body': ""From [slack](https://kubernetes.slack.com/archives/C09QYUH5W/p1672832735009399?thread_ts=1672822866.185199&cid=C09QYUH5W):\r\n\r\n> loadBalancerIP is assigned to the physical interface (eno1). It accepts traffic from another network there and I use metallb in L2 mode for that. From what I can see, in the dummy interface I have all external addresses used as loadBalancerIP and they are excluded from listening to NodePort - it is as you mentioned\r\n\r\nThis is not a valid setup. A virtual IP (VIP), like the loadBalancerIP, _must not_ be assigned to a physical interface.\r\n\r\nThe reason to assign a VIP to a physical interface would be to attract traffic with L2 mechanisms, ARP for IPv4 and neighbor discovery for IPv6, but this way is flawed for several reasons. That's why you use metallb in L2 mode instead. Metallb in L2 mode answers L2 requests _without_ assigning the VIP to any interface."", 'created_at': datetime.datetime(2023, 1, 4, 13, 53, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370962620, 'issue_id': 1518924487, 'author': 'uablrek', 'body': 'When you send traffic to a nodePort, the destination address should be a node address, _not_ a loadBalancerIP.', 'created_at': datetime.datetime(2023, 1, 4, 13, 59, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370972691, 'issue_id': 1518924487, 'author': 'uablrek', 'body': 'There is however a change in the implementation (I will try to find the PR) in the way the nodePort addresses are computed. This is how i should be;\r\n```\r\nnodePort addresses = addresses on all node interface excluding link-local and loopback and all addresses on kube-ipvs0\r\n```\r\nOn `kube-ipvs0` all clusterIP, loadBalancerIP and externalIPs are assigned. So the current implementation is correct.\r\n\r\nWhat I suspect that pre v1.23 K8s did this wrong by adding _all_ addresses on interfaces except kube-ipvs0 (including loopback which would not work).', 'created_at': datetime.datetime(2023, 1, 4, 14, 7, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370977490, 'issue_id': 1518924487, 'author': 'adr-xyt', 'body': ""> This is not a valid setup. A virtual IP (VIP), like the loadBalancerIP, must not be assigned to a physical interface.\r\nThe reason to assign a VIP to a physical interface would be to attract traffic with L2 mechanisms, ARP for IPv4 and neighbor discovery for IPv6, but this way is flawed for several reasons. That's why you use metallb in L2 mode instead. Metallb in L2 mode answers L2 requests without assigning the VIP to any interface.\r\n\r\nThese loadBalancerIP addresses are just the external IP of my nodes, which I use in metallb as an available pool for service(loadbalancer). Provider automatically assigns this address to the physical interface as a lease, I don't really have the option of a different setup at the moment.\r\n\r\n> When you send traffic to a nodePort, the destination address should be a node address, not a loadBalancerIP.\r\n\r\nYe, I know. In my case the public IP address pool for node and loadBalancerIP addresses are the same"", 'created_at': datetime.datetime(2023, 1, 4, 14, 11, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370986921, 'issue_id': 1518924487, 'author': 'uablrek', 'body': '> These loadBalancerIP addresses are just the external IP of my nodes\r\n\r\nSo, the loadBalancerIPs assigned by metallb is actually the real addresses of your K8s nodes?', 'created_at': datetime.datetime(2023, 1, 4, 14, 18, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371001119, 'issue_id': 1518924487, 'author': 'adr-xyt', 'body': '> So, the loadBalancerIPs assigned by metallb is actually the real addresses of your K8s nodes?\r\n\r\nYes, these are public IP addresses (each node has its own), which I use as an entry point for some services(loadbalancer)\r\nbut I also have an external loadbalancer that targets nodePort. To eliminate SPoF, it directs traffic to the service (NodePort), which has been listening on all nodes so far, regardless of IP assignment to another service', 'created_at': datetime.datetime(2023, 1, 4, 14, 30, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371013929, 'issue_id': 1518924487, 'author': 'uablrek', 'body': ""> So, the loadBalancerIPs assigned by metallb is actually the real addresses of your K8s nodes?\r\n\r\n@thockin @danwinship Is this setup supported?\r\n\r\n@adr-xyt Thanks for explaining. I understand your setup now, . I am unsure if it's supposed to work, so I have to ask. Have you tested proxy-mode=iptables? It _may_ work, but I wouldn't be suprised if it doesn't."", 'created_at': datetime.datetime(2023, 1, 4, 14, 41, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371031837, 'issue_id': 1518924487, 'author': 'adr-xyt', 'body': ""I will try to test it with iptables, but I use ipvs because of specific services, so I can use the load balancing algorithm - least connection... this method won't solve my problem :("", 'created_at': datetime.datetime(2023, 1, 4, 14, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371042855, 'issue_id': 1518924487, 'author': 'uablrek', 'body': 'Found the PR https://github.com/kubernetes/kubernetes/pull/101429', 'created_at': datetime.datetime(2023, 1, 4, 15, 4, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371060465, 'issue_id': 1518924487, 'author': 'uablrek', 'body': ""Why do you need nodePorts services at all? Can't you use only loadBalancer?\r\n\r\nNodePort is mainly intended to be used by and external load-balancer, i.e. like in AWS, but when you manage your own cluster you usually don't have an external loadBalancer so use only loadBalancerIPs makes sense. We do that, and even [disable nodePort allocation](https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-nodeport-allocation)."", 'created_at': datetime.datetime(2023, 1, 4, 15, 18, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371866505, 'issue_id': 1518924487, 'author': 'uablrek', 'body': ""> ~Why do you need nodePorts services at all? Can't you use only loadBalancer?~\r\n\r\nScratch that! In your case where the node addresses are the external addresses you should use NodePort _only_.\r\n\r\nIf you use your node addresses as loadBalancerIP you will hit this PR https://github.com/kubernetes/kubernetes/pull/108460 when you upgrade to later K8s versions. It inserts a blocking rule in the INPUT chain for loadBalancerIPs.\r\n\r\n## WARNING\r\nThis will block input to your node if loadBalancerIP==nodeIP."", 'created_at': datetime.datetime(2023, 1, 5, 7, 23, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371890510, 'issue_id': 1518924487, 'author': 'adr-xyt', 'body': ""I understand, thanks for the info. In my case it's a breaking change and I will have to migrate some services to NodePort before kubernetes upgrade on prod environment."", 'created_at': datetime.datetime(2023, 1, 5, 7, 58, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371919093, 'issue_id': 1518924487, 'author': 'adr-xyt', 'body': ""> @adr-xyt Thanks for explaining. I understand your setup now, . I am unsure if it's supposed to work, so I have to ask. Have you tested proxy-mode=iptables? It may work, but I wouldn't be suprised if it doesn't.\r\n\r\n@uablrek \r\n\r\nwith proxy-mode=iptables, everything started working, node is listening on nodePort and I can get to loadBalancerIP, I cleared the rules `kube-proxy --cleanup` on each node, just to be sure\r\nHowever this is not a solution for me, I need the lc algorithm from ipvs"", 'created_at': datetime.datetime(2023, 1, 5, 8, 31, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371981912, 'issue_id': 1518924487, 'author': 'uablrek', 'body': '> I will have to migrate some services to NodePort before kubernetes upgrade on prod environment.\r\n\r\nPort 31010 used in the LoadBakancer service example is within the default NodePort range. I think it can be converted to a NodePort service without affecting external users. Or is 31010 just an example? Or are there other reasons for type:LoadBalancer?', 'created_at': datetime.datetime(2023, 1, 5, 9, 34, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 1372017006, 'issue_id': 1518924487, 'author': 'uablrek', 'body': '/area ipvs', 'created_at': datetime.datetime(2023, 1, 5, 10, 7, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 1372163632, 'issue_id': 1518924487, 'author': 'adr-xyt', 'body': ""> Port 31010 used in the LoadBakancer service example is within the default NodePort range. I think it can be converted to a NodePort service without affecting external users. Or is 31010 just an example? Or are there other reasons for type:LoadBalancer?\r\n\r\nThis is just an example, I won't have much trouble with the migration. At most, I will divide the available lease of IP addresses into smaller pools, for NodePort / Loadblancer services. \r\nI think I already know everything and there should be no more surprises. \r\nThanks for the information and your time!"", 'created_at': datetime.datetime(2023, 1, 5, 12, 37, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 1372411718, 'issue_id': 1518924487, 'author': 'thockin', 'body': 'It is a little ""unusal"" to use the host IP as the LB IP, but it\'s unfortunate that we broke something that used to work.  https://www.hyrumslaw.com/\r\n\r\nLars, is there any path back to ""working"" that doesn\'t lose the protections in that linked PR?\r\n\r\nI could easily see iptables *also* breaking this..', 'created_at': datetime.datetime(2023, 1, 5, 16, 4, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 1372486738, 'issue_id': 1518924487, 'author': 'uablrek', 'body': 'To not exclude loadBalancerIPs (and ecternalIPs) when they are on a physical interface is not hard, but messes up the nice set-operation.\r\n\r\nThe access protection in https://github.com/kubernetes/kubernetes/pull/108460 It should basically not be there if the any loadBalancerIPs (and ecternalIPs) is also on aphysical interface. Can be done, but may be messy.', 'created_at': datetime.datetime(2023, 1, 5, 17, 5, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 1372522872, 'issue_id': 1518924487, 'author': 'uablrek', 'body': ""/triage accepted\r\n\r\nEven though this issue is accepted, it's still under investigation.\r\n\r\nI will check the code if the old bevaivor can be restored in a fairly simple way. But there is a problem with back-port. If I make a PR now it will be in v1.26.x or v1.27 but the function was broken in v1.23 so a fix should really be back-ported all the way back.\r\n\r\nI would prefer to document that a real address on an interface on the node can't be used as a virtual address, like loadBalancerIP or externalIP."", 'created_at': datetime.datetime(2023, 1, 5, 17, 34, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 1379747331, 'issue_id': 1518924487, 'author': 'chlam4', 'body': ""Just wanted to chime in and say that I've also encountered this issue... very grateful to find the discussion and looking forward to a solution.  Thank you all!\r\n\r\nAlso just to share, I've tried the following 3 workarounds and they all work in my case:\r\n- Switching to iptables mode.  It works I think because the iptables NAT rule refers to the hostname as the destination, which gets resolved to the public node IP.\r\n- Disable IPv6 on my OS.  This works too as it makes the code fall back to the single stack mode, which grabs the node IP also from looking up the hostname.\r\n- Explicitly configure the `nodePortAddresses` field in the kube proxy config, that overrides the default get local addresses."", 'created_at': datetime.datetime(2023, 1, 12, 2, 55, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 1382090138, 'issue_id': 1518924487, 'author': 'uablrek', 'body': '@adr-xyt  I backed down to K8s v1.22.4 and I can\'t get it to work unless you have the master outside the cluster (i.e. not a node), or have a setup where the external-ip you set as loadBalancerIP is _not_ on the main K8s network, something like;\r\n\r\n<img src=""https://raw.githubusercontent.com/Nordix/xcluster/master/ovl/network-topology/backend.svg"" width=""65%"" />\r\n\r\nK8s is using the ""backend"" network, the ""external"" ips are on the ""frontend"" network.\r\n\r\nThe reason is when you assign a loadBalancerIP, that address will be assigned to the `kube-ipvs0` interface, even before v1.23. So, on an in-cluster master node you will see something like;\r\n\r\n```\r\nvm-001 ~ # ip addr show dev kube-ipvs0\r\n15: kube-ipvs0: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN group default \r\n    link/ether 1a:8d:7f:b9:0a:95 brd ff:ff:ff:ff:ff:ff\r\n    inet 12.0.0.1/32 scope global kube-ipvs0\r\n       valid_lft forever preferred_lft forever\r\n    inet 12.0.253.214/32 scope global kube-ipvs0\r\n       valid_lft forever preferred_lft forever\r\n    inet 12.0.117.25/32 scope global kube-ipvs0\r\n       valid_lft forever preferred_lft forever\r\n    inet 192.168.1.2/32 scope global kube-ipvs0\r\n```\r\nThe `192.168.1.2` address is the loadBalancerIP and is also the node-address of another node in the cluster, ""vm-002"". When the master receives ""normal"" packets from ""vm-002"" they will have src=192.168.1.2 but since that address is assigned to a local interface on the master the packets will be discarded as [martians](https://en.wikipedia.org/wiki/Martian_packet) or spoofed packets.\r\n\r\nThe result is that ""vm-002"" (e.g. `kubelet` on that node) loses contact with the master;\r\n```\r\nvm-002 ~ # kubectl version\r\nClient Version: version.Info{Major:""1"", Minor:""22"", GitVersion:""v1.22.4"", GitCommit:""b695d79d4f967c403a96986f1750a35eb75e75f1"", GitTreeState:""clean"", BuildDate:""2021-11-17T15:48:33Z"", GoVersion:""go1.16.10"", Compiler:""gc"", Platform:""linux/amd64""}\r\nUnable to connect to the server: dial tcp 192.168.1.1:6443: connect: no route to host\r\n```', 'created_at': datetime.datetime(2023, 1, 13, 16, 30, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 1382123493, 'issue_id': 1518924487, 'author': 'uablrek', 'body': 'I discovered this when testing PR https://github.com/kubernetes/kubernetes/pull/115019. I think I can restore the pre v1.23 behavior now, but I suffer from the problem described above.', 'created_at': datetime.datetime(2023, 1, 13, 16, 46, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 1382130935, 'issue_id': 1518924487, 'author': 'uablrek', 'body': ""@chlam4 I don't think your problem is precisely the same. To disable ipv6 or set nodePortAddresses can't really be used as work-arounds for this issue."", 'created_at': datetime.datetime(2023, 1, 13, 16, 53, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 1382892578, 'issue_id': 1518924487, 'author': 'chlam4', 'body': 'Thank you, @uablrek. You\'re right about IPv6 - disabling it didn\'t really help.  I forgot to remove the `nodePortAddresses` I put in while testing with disabling IPv6.  It was the `nodePortAddresses` that makes the difference.  That said, it really appears to work if we explicitly put in the `nodePortAddresses`.  Maybe I\'m still missing something in the code, but if I read it correctly, here is where we take out the load balancer IP:\r\n```\r\nfunc (r *realIPGetter) NodeIPs() (ips []net.IP, err error) {\r\n\r\n\tnodeAddress, err := r.nl.GetAllLocalAddresses()\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(""error listing LOCAL type addresses from host, error: %v"", err)\r\n\t}\r\n\r\n\t// We must exclude the addresses on the IPVS dummy interface\r\n\tbindedAddress, err := r.BindedIPs()\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\tipset := nodeAddress.Difference(bindedAddress)\r\n\r\n\t// translate ip string to IP\r\n\tfor _, ipStr := range ipset.UnsortedList() {\r\n\t\ta := netutils.ParseIPSloppy(ipStr)\r\n\t\tips = append(ips, a)\r\n\t}\r\n\treturn ips, nil\r\n}\r\n```\r\n\r\nIt is called from the folllowing, but only if `nodeAddrSet` contains any zero CIDRs.  If a node port address is explicitly provided and it is NOT a zero CIDR, then `ipGetter.NodeIPs()` will not be called.\r\n```\r\n\tif hasNodePort {\r\n\t\tnodeAddrSet, err := utilproxy.GetNodeAddresses(proxier.nodePortAddresses, proxier.networkInterfacer)\r\n\t\tif err != nil {\r\n\t\t\tklog.ErrorS(err, ""Failed to get node IP address matching nodeport cidr"")\r\n\t\t} else {\r\n\t\t\tnodeAddresses = nodeAddrSet.List()\r\n\t\t\tfor _, address := range nodeAddresses {\r\n\t\t\t\ta := netutils.ParseIPSloppy(address)\r\n\t\t\t\tif a.IsLoopback() {\r\n\t\t\t\t\tcontinue\r\n\t\t\t\t}\r\n\t\t\t\tif utilproxy.IsZeroCIDR(address) {\r\n\t\t\t\t\tnodeIPs, err = proxier.ipGetter.NodeIPs()\r\n\t\t\t\t\tif err != nil {\r\n\t\t\t\t\t\tklog.ErrorS(err, ""Failed to list all node IPs from host"")\r\n\t\t\t\t\t}\r\n\t\t\t\t\tbreak\r\n\t\t\t\t}\r\n\t\t\t\tnodeIPs = append(nodeIPs, a)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n```\r\n\r\nWe\'re planning to roll out this workaround before upgrading to a future Kubernetes release with your fix.  We would much appreciate it if you could shed more light why it wouldn\'t work.  We didn\'t want to change to iptables mode if this second workaround really works.  Thank you so much!', 'created_at': datetime.datetime(2023, 1, 14, 19, 15, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 1383804926, 'issue_id': 1518924487, 'author': 'uablrek', 'body': ""@chlam4 You are right. To explicitly set `nodePortAddresses` in the kube-proxy config works as a work-around.\r\n\r\nStill the loadBalancerIP (which is also an address on a node) can't be used by the main k8s network, or the master must not be a node in the cluster as described in https://github.com/kubernetes/kubernetes/issues/114815#issuecomment-1382090138. And...\r\n\r\n## Warning\r\n\r\nThis will work up to and including K8s v1.25.x, but **_NOT v1.26.0!_** I haven't checked yet but I think it's because of https://github.com/kubernetes/kubernetes/pull/108460."", 'created_at': datetime.datetime(2023, 1, 16, 10, 13, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 1383821417, 'issue_id': 1518924487, 'author': 'uablrek', 'body': 'Since the loadBalancerIP that is owned by a node is assigned to `kube-ipvs0` on _other_ nodes you must stop those other nodes from responding to ARP. For example with:\r\n```\r\nsysctl -w net.ipv4.conf.all.arp_ignore=1\r\n```', 'created_at': datetime.datetime(2023, 1, 16, 10, 24, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 1385023497, 'issue_id': 1518924487, 'author': 'adr-xyt', 'body': ""From my perspective, this is not a big [problem](https://github.com/kubernetes/kubernetes/issues/114815#issuecomment-1382090138), I have the whole architecture implemented in separate networks. I think it's worth adding this information somewhere in the documentation."", 'created_at': datetime.datetime(2023, 1, 17, 8, 35, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 1397667007, 'issue_id': 1518924487, 'author': 'chlam4', 'body': ""@uablrek Thank you so much for the response, epesically the info about v1.26.  We'll check it out for sure and will report back.  Our node port use case is for single-node deployments only, so luckily there are no other nodes."", 'created_at': datetime.datetime(2023, 1, 19, 22, 1, 40, tzinfo=datetime.timezone.utc)}]","adr-xyt (Issue Creator) on (2023-01-04 12:51:56 UTC): /sig network

uablrek (Assginee) on (2023-01-04 13:40:21 UTC): /assign

uablrek (Assginee) on (2023-01-04 13:53:06 UTC): From [slack](https://kubernetes.slack.com/archives/C09QYUH5W/p1672832735009399?thread_ts=1672822866.185199&cid=C09QYUH5W):


This is not a valid setup. A virtual IP (VIP), like the loadBalancerIP, _must not_ be assigned to a physical interface.

The reason to assign a VIP to a physical interface would be to attract traffic with L2 mechanisms, ARP for IPv4 and neighbor discovery for IPv6, but this way is flawed for several reasons. That's why you use metallb in L2 mode instead. Metallb in L2 mode answers L2 requests _without_ assigning the VIP to any interface.

uablrek (Assginee) on (2023-01-04 13:59:09 UTC): When you send traffic to a nodePort, the destination address should be a node address, _not_ a loadBalancerIP.

uablrek (Assginee) on (2023-01-04 14:07:44 UTC): There is however a change in the implementation (I will try to find the PR) in the way the nodePort addresses are computed. This is how i should be;
```
nodePort addresses = addresses on all node interface excluding link-local and loopback and all addresses on kube-ipvs0
```
On `kube-ipvs0` all clusterIP, loadBalancerIP and externalIPs are assigned. So the current implementation is correct.

What I suspect that pre v1.23 K8s did this wrong by adding _all_ addresses on interfaces except kube-ipvs0 (including loopback which would not work).

adr-xyt (Issue Creator) on (2023-01-04 14:11:06 UTC): The reason to assign a VIP to a physical interface would be to attract traffic with L2 mechanisms, ARP for IPv4 and neighbor discovery for IPv6, but this way is flawed for several reasons. That's why you use metallb in L2 mode instead. Metallb in L2 mode answers L2 requests without assigning the VIP to any interface.

These loadBalancerIP addresses are just the external IP of my nodes, which I use in metallb as an available pool for service(loadbalancer). Provider automatically assigns this address to the physical interface as a lease, I don't really have the option of a different setup at the moment.


Ye, I know. In my case the public IP address pool for node and loadBalancerIP addresses are the same

uablrek (Assginee) on (2023-01-04 14:18:18 UTC): So, the loadBalancerIPs assigned by metallb is actually the real addresses of your K8s nodes?

adr-xyt (Issue Creator) on (2023-01-04 14:30:18 UTC): Yes, these are public IP addresses (each node has its own), which I use as an entry point for some services(loadbalancer)
but I also have an external loadbalancer that targets nodePort. To eliminate SPoF, it directs traffic to the service (NodePort), which has been listening on all nodes so far, regardless of IP assignment to another service

uablrek (Assginee) on (2023-01-04 14:41:08 UTC): @thockin @danwinship Is this setup supported?

@adr-xyt Thanks for explaining. I understand your setup now, . I am unsure if it's supposed to work, so I have to ask. Have you tested proxy-mode=iptables? It _may_ work, but I wouldn't be suprised if it doesn't.

adr-xyt (Issue Creator) on (2023-01-04 14:56:00 UTC): I will try to test it with iptables, but I use ipvs because of specific services, so I can use the load balancing algorithm - least connection... this method won't solve my problem :(

uablrek (Assginee) on (2023-01-04 15:04:36 UTC): Found the PR https://github.com/kubernetes/kubernetes/pull/101429

uablrek (Assginee) on (2023-01-04 15:18:01 UTC): Why do you need nodePorts services at all? Can't you use only loadBalancer?

NodePort is mainly intended to be used by and external load-balancer, i.e. like in AWS, but when you manage your own cluster you usually don't have an external loadBalancer so use only loadBalancerIPs makes sense. We do that, and even [disable nodePort allocation](https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-nodeport-allocation).

uablrek (Assginee) on (2023-01-05 07:23:10 UTC): Scratch that! In your case where the node addresses are the external addresses you should use NodePort _only_.

If you use your node addresses as loadBalancerIP you will hit this PR https://github.com/kubernetes/kubernetes/pull/108460 when you upgrade to later K8s versions. It inserts a blocking rule in the INPUT chain for loadBalancerIPs.

## WARNING
This will block input to your node if loadBalancerIP==nodeIP.

adr-xyt (Issue Creator) on (2023-01-05 07:58:50 UTC): I understand, thanks for the info. In my case it's a breaking change and I will have to migrate some services to NodePort before kubernetes upgrade on prod environment.

adr-xyt (Issue Creator) on (2023-01-05 08:31:48 UTC): @uablrek 

with proxy-mode=iptables, everything started working, node is listening on nodePort and I can get to loadBalancerIP, I cleared the rules `kube-proxy --cleanup` on each node, just to be sure
However this is not a solution for me, I need the lc algorithm from ipvs

uablrek (Assginee) on (2023-01-05 09:34:39 UTC): Port 31010 used in the LoadBakancer service example is within the default NodePort range. I think it can be converted to a NodePort service without affecting external users. Or is 31010 just an example? Or are there other reasons for type:LoadBalancer?

uablrek (Assginee) on (2023-01-05 10:07:44 UTC): /area ipvs

adr-xyt (Issue Creator) on (2023-01-05 12:37:59 UTC): This is just an example, I won't have much trouble with the migration. At most, I will divide the available lease of IP addresses into smaller pools, for NodePort / Loadblancer services. 
I think I already know everything and there should be no more surprises. 
Thanks for the information and your time!

thockin on (2023-01-05 16:04:13 UTC): It is a little ""unusal"" to use the host IP as the LB IP, but it's unfortunate that we broke something that used to work.  https://www.hyrumslaw.com/

Lars, is there any path back to ""working"" that doesn't lose the protections in that linked PR?

I could easily see iptables *also* breaking this..

uablrek (Assginee) on (2023-01-05 17:05:35 UTC): To not exclude loadBalancerIPs (and ecternalIPs) when they are on a physical interface is not hard, but messes up the nice set-operation.

The access protection in https://github.com/kubernetes/kubernetes/pull/108460 It should basically not be there if the any loadBalancerIPs (and ecternalIPs) is also on aphysical interface. Can be done, but may be messy.

uablrek (Assginee) on (2023-01-05 17:34:17 UTC): /triage accepted

Even though this issue is accepted, it's still under investigation.

I will check the code if the old bevaivor can be restored in a fairly simple way. But there is a problem with back-port. If I make a PR now it will be in v1.26.x or v1.27 but the function was broken in v1.23 so a fix should really be back-ported all the way back.

I would prefer to document that a real address on an interface on the node can't be used as a virtual address, like loadBalancerIP or externalIP.

chlam4 on (2023-01-12 02:55:52 UTC): Just wanted to chime in and say that I've also encountered this issue... very grateful to find the discussion and looking forward to a solution.  Thank you all!

Also just to share, I've tried the following 3 workarounds and they all work in my case:
- Switching to iptables mode.  It works I think because the iptables NAT rule refers to the hostname as the destination, which gets resolved to the public node IP.
- Disable IPv6 on my OS.  This works too as it makes the code fall back to the single stack mode, which grabs the node IP also from looking up the hostname.
- Explicitly configure the `nodePortAddresses` field in the kube proxy config, that overrides the default get local addresses.

uablrek (Assginee) on (2023-01-13 16:30:47 UTC): @adr-xyt  I backed down to K8s v1.22.4 and I can't get it to work unless you have the master outside the cluster (i.e. not a node), or have a setup where the external-ip you set as loadBalancerIP is _not_ on the main K8s network, something like;

<img src=""https://raw.githubusercontent.com/Nordix/xcluster/master/ovl/network-topology/backend.svg"" width=""65%"" />

K8s is using the ""backend"" network, the ""external"" ips are on the ""frontend"" network.

The reason is when you assign a loadBalancerIP, that address will be assigned to the `kube-ipvs0` interface, even before v1.23. So, on an in-cluster master node you will see something like;

```
vm-001 ~ # ip addr show dev kube-ipvs0
15: kube-ipvs0: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN group default 
    link/ether 1a:8d:7f:b9:0a:95 brd ff:ff:ff:ff:ff:ff
    inet 12.0.0.1/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 12.0.253.214/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 12.0.117.25/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 192.168.1.2/32 scope global kube-ipvs0
```
The `192.168.1.2` address is the loadBalancerIP and is also the node-address of another node in the cluster, ""vm-002"". When the master receives ""normal"" packets from ""vm-002"" they will have src=192.168.1.2 but since that address is assigned to a local interface on the master the packets will be discarded as [martians](https://en.wikipedia.org/wiki/Martian_packet) or spoofed packets.

The result is that ""vm-002"" (e.g. `kubelet` on that node) loses contact with the master;
```
vm-002 ~ # kubectl version
Client Version: version.Info{Major:""1"", Minor:""22"", GitVersion:""v1.22.4"", GitCommit:""b695d79d4f967c403a96986f1750a35eb75e75f1"", GitTreeState:""clean"", BuildDate:""2021-11-17T15:48:33Z"", GoVersion:""go1.16.10"", Compiler:""gc"", Platform:""linux/amd64""}
Unable to connect to the server: dial tcp 192.168.1.1:6443: connect: no route to host
```

uablrek (Assginee) on (2023-01-13 16:46:36 UTC): I discovered this when testing PR https://github.com/kubernetes/kubernetes/pull/115019. I think I can restore the pre v1.23 behavior now, but I suffer from the problem described above.

uablrek (Assginee) on (2023-01-13 16:53:20 UTC): @chlam4 I don't think your problem is precisely the same. To disable ipv6 or set nodePortAddresses can't really be used as work-arounds for this issue.

chlam4 on (2023-01-14 19:15:08 UTC): Thank you, @uablrek. You're right about IPv6 - disabling it didn't really help.  I forgot to remove the `nodePortAddresses` I put in while testing with disabling IPv6.  It was the `nodePortAddresses` that makes the difference.  That said, it really appears to work if we explicitly put in the `nodePortAddresses`.  Maybe I'm still missing something in the code, but if I read it correctly, here is where we take out the load balancer IP:
```
func (r *realIPGetter) NodeIPs() (ips []net.IP, err error) {

	nodeAddress, err := r.nl.GetAllLocalAddresses()
	if err != nil {
		return nil, fmt.Errorf(""error listing LOCAL type addresses from host, error: %v"", err)
	}

	// We must exclude the addresses on the IPVS dummy interface
	bindedAddress, err := r.BindedIPs()
	if err != nil {
		return nil, err
	}
	ipset := nodeAddress.Difference(bindedAddress)

	// translate ip string to IP
	for _, ipStr := range ipset.UnsortedList() {
		a := netutils.ParseIPSloppy(ipStr)
		ips = append(ips, a)
	}
	return ips, nil
}
```

It is called from the folllowing, but only if `nodeAddrSet` contains any zero CIDRs.  If a node port address is explicitly provided and it is NOT a zero CIDR, then `ipGetter.NodeIPs()` will not be called.
```
	if hasNodePort {
		nodeAddrSet, err := utilproxy.GetNodeAddresses(proxier.nodePortAddresses, proxier.networkInterfacer)
		if err != nil {
			klog.ErrorS(err, ""Failed to get node IP address matching nodeport cidr"")
		} else {
			nodeAddresses = nodeAddrSet.List()
			for _, address := range nodeAddresses {
				a := netutils.ParseIPSloppy(address)
				if a.IsLoopback() {
					continue
				}
				if utilproxy.IsZeroCIDR(address) {
					nodeIPs, err = proxier.ipGetter.NodeIPs()
					if err != nil {
						klog.ErrorS(err, ""Failed to list all node IPs from host"")
					}
					break
				}
				nodeIPs = append(nodeIPs, a)
			}
		}
	}
```

We're planning to roll out this workaround before upgrading to a future Kubernetes release with your fix.  We would much appreciate it if you could shed more light why it wouldn't work.  We didn't want to change to iptables mode if this second workaround really works.  Thank you so much!

uablrek (Assginee) on (2023-01-16 10:13:13 UTC): @chlam4 You are right. To explicitly set `nodePortAddresses` in the kube-proxy config works as a work-around.

Still the loadBalancerIP (which is also an address on a node) can't be used by the main k8s network, or the master must not be a node in the cluster as described in https://github.com/kubernetes/kubernetes/issues/114815#issuecomment-1382090138. And...

## Warning

This will work up to and including K8s v1.25.x, but **_NOT v1.26.0!_** I haven't checked yet but I think it's because of https://github.com/kubernetes/kubernetes/pull/108460.

uablrek (Assginee) on (2023-01-16 10:24:15 UTC): Since the loadBalancerIP that is owned by a node is assigned to `kube-ipvs0` on _other_ nodes you must stop those other nodes from responding to ARP. For example with:
```
sysctl -w net.ipv4.conf.all.arp_ignore=1
```

adr-xyt (Issue Creator) on (2023-01-17 08:35:33 UTC): From my perspective, this is not a big [problem](https://github.com/kubernetes/kubernetes/issues/114815#issuecomment-1382090138), I have the whole architecture implemented in separate networks. I think it's worth adding this information somewhere in the documentation.

chlam4 on (2023-01-19 22:01:40 UTC): @uablrek Thank you so much for the response, epesically the info about v1.26.  We'll check it out for sure and will report back.  Our node port use case is for single-node deployments only, so luckily there are no other nodes.

"
1518847631,issue,closed,completed,"two masters not ready because of the below error Unable to register node ""node name"" with API server: nodes is forbidden: User ""system:anonymous"" cannot create resource ""nodes"" in API group """" at the cluster scope","### What happened?

we have setup with three masters node and we got suddenly two masters node went not ready and we can see some issues on the kubelet 
Unable to register node ""node name"" with API server: nodes is forbidden: User ""system:anonymous"" cannot create resource ""nodes"" in API group """" at the cluster scope


kubelet[11429]: E0104 14:08:35.176102   11429 kubelet.go:2243] node ""node name"" not found

### What did you expect to happen?

the master node be on ready state 

### How can we reproduce it (as minimally and precisely as possible)?

we restart the kubelet and the whole node and it same result 

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""20"", GitVersion:""v1.20.2"", GitCommit:""faecb196815e248d3ecfb03c680a4507229c2a56"", GitTreeState:""clean"", BuildDate:""2021-01-13T13:28:09Z"", GoVersion:""go1.15.5"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""20"", GitVersion:""v1.20.2"", GitCommit:""faecb196815e248d3ecfb03c680a4507229c2a56"", GitTreeState:""clean"", BuildDate:""2021-01-13T13:20:00Z"", GoVersion:""go1.15.5"", Compiler:""gc"", Platform:""linux/amd64""}
</details>


### Cloud provider

<details>
on primes setup 
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME=""Red Hat Enterprise Linux Server""
VERSION=""7.9 (Maipo)""
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",Mohamed2assem,2023-01-04 11:57:02+00:00,[],2023-01-05 19:31:47+00:00,2023-01-05 19:31:46+00:00,https://github.com/kubernetes/kubernetes/issues/114813,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('kind/support', 'Categorizes issue or PR as a support question.'), ('needs-sig', 'Indicates an issue or PR lacks a `sig/foo` label and requires one.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1370837897, 'issue_id': 1518847631, 'author': 'k8s-ci-robot', 'body': '@Mohamed2assem: There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\n- `/sig <group-name>`\n- `/wg <group-name>`\n- `/committee <group-name>`\n\nPlease see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 4, 11, 57, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370837917, 'issue_id': 1518847631, 'author': 'k8s-ci-robot', 'body': '@Mohamed2assem: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 4, 11, 57, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371678183, 'issue_id': 1518847631, 'author': 'pacoxu', 'body': 'The End Of Life Date of v1.20.2 is 2022-02-28. See https://kubernetes.io/releases/patch-releases/.', 'created_at': datetime.datetime(2023, 1, 5, 2, 17, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 1372643360, 'issue_id': 1518847631, 'author': 'neolit123', 'body': 'best to ask on support channels like slack  / discuss.k8s.io\r\n\r\n> ""system:anonymous""...\r\n\r\nthis might be due to missing kubelet client credentials\r\n""node not found"" happens until the node is registered.\r\n\r\n/close\r\n/kind support', 'created_at': datetime.datetime(2023, 1, 5, 19, 31, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 1372643452, 'issue_id': 1518847631, 'author': 'k8s-ci-robot', 'body': '@neolit123: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114813#issuecomment-1372643360):\n\n>best to ask on support channels like slack  / discuss.k8s.io\r\n>\r\n>> ""system:anonymous""...\r\n>\r\n>this might be due to missing kubelet client credentials\r\n>""node not found"" happens until the node is registered.\r\n>\r\n>/close\r\n>/kind support\r\n>\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 5, 19, 31, 47, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-04 11:57:09 UTC): @Mohamed2assem: There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:
- `/sig <group-name>`
- `/wg <group-name>`
- `/committee <group-name>`

Please see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

k8s-ci-robot on (2023-01-04 11:57:10 UTC): @Mohamed2assem: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

pacoxu on (2023-01-05 02:17:24 UTC): The End Of Life Date of v1.20.2 is 2022-02-28. See https://kubernetes.io/releases/patch-releases/.

neolit123 on (2023-01-05 19:31:40 UTC): best to ask on support channels like slack  / discuss.k8s.io


this might be due to missing kubelet client credentials
""node not found"" happens until the node is registered.

/close
/kind support

k8s-ci-robot on (2023-01-05 19:31:47 UTC): @neolit123: Closing this issue.

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114813#issuecomment-1372643360):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

"
1518658655,issue,closed,completed, Checkpoint Restoration Failing in kubeadm and minikube,"### What happened?

Unable to checkpoint a nginx container even after following all the prereq steps. 


Followed this blog [Forensic container checkpointing in Kubernetes | Kubernetes](https://kubernetes.io/blog/2022/12/05/forensic-container-checkpointing-alpha/)




### What did you expect to happen?

Steps taken for enabling checkpoint

Enabled feature gates in all three components of k8s
Checked manifests file .They are enabled
`root@kubemaster:/etc/kubernetes/manifests# grep -E 'feature-gates' *.yaml`

> kube-apiserver.yaml: - --feature-gates=ContainerCheckpoint=true
> kube-controller-manager.yaml: - --feature-gates=ContainerCheckpoint=true
> kube-scheduler.yaml: - --feature-gates=ContainerCheckpoint=true

2)Enabled enable_criu_support and drop_infra_ctr
`root@kubemaster:/etc/crio# grep -E 'enable_criu_support = true|drop_infra_ctr = false' *.conf
`
> drop_infra_ctr = false
> enable_criu_support = true

3)Ensure nodes are using crio 1.25 and k8s 1.25

`root@kubemaster:/# kubectl get no -o wide`

> NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME
> kubemaster Ready control-plane 28m v1.25.0 192.168.56.2 Ubuntu 22.04.1 LTS 5.15.0-56-generic cri-o://1.25.0







### How can we reproduce it (as minimally and precisely as possible)?

Steps to replicate

> Spinned up ubuntu 22.04 LTS on virtual box 

`sudo minikube start --force --feature-gates=ContainerCheckpoint=true --install-addons=true  --container-runtime=cri-o --network-plugin=cni --vm-driver=docker --apiserver-ips 127.0.0.1 --apiserver-name localhost`

updated crio.conf file in crio folder and verified


Created nginx pod
`kubectl run webserver --image=nginx -n default`

Tried to checkpoint it

Kubeadm 

```
curl -sk -X POST  ""https://localhost:10250/checkpoint/default/webserver/webserver"" \
   --key /etc/kubernetes/pki/apiserver-kubelet-client.key \
   --cacert /etc/kubernetes/pki/ca.crt \
   --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt

```

Minikube
`curl -sk -X POST  ""https://localhost:10250/checkpoint/default/webserver/webserver"" \  --cert ~/.minikube/profiles/minikube/client.crt --key ~/.minikube/profiles/minikube/client.key 
`
Getting the Same error

> checkpointing of default/webserver/webserver failed (rpc error: code = Unknown desc = checkpoint/restore support not available)

Tried similar setup in kubeadm and facing the exact same issue

### Anything else we need to know?

Are there any other pre req? I seem to have covered all of them.

### Kubernetes version

<details>

```console
$ kubectl version
Kubernetes version: 1.25

```

</details>


### Cloud provider

<details>
Cloud being used: bare-metal
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
22.04.1 LTS(ubuntu/jammy64)
```

</details>


### Install tools

<details>
Installation method: Kubeadm and minikube
</details>


### Container runtime (CRI) and version (if applicable)

<details>
CRI and version: cri-o v1.25.0
</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CNI and version: Calico v3.24.0. (for my Kubeadm setup)
</details>
",ContainerCheckpointfalse,2023-01-04 09:50:01+00:00,[],2023-04-04 10:11:10+00:00,2023-01-08 07:34:19+00:00,https://github.com/kubernetes/kubernetes/issues/114805,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('sig/node', 'Categorizes an issue or PR as relevant to SIG Node.'), ('triage/needs-information', 'Indicates an issue needs more information in order to work on it.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1370704629, 'issue_id': 1518658655, 'author': 'k8s-ci-robot', 'body': '@ContainerCheckpointfalse: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 4, 9, 50, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370712255, 'issue_id': 1518658655, 'author': 'ContainerCheckpointfalse', 'body': '/sig node', 'created_at': datetime.datetime(2023, 1, 4, 9, 57, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370715607, 'issue_id': 1518658655, 'author': 'ContainerCheckpointfalse', 'body': 'cc: @adrianreber Am i missing something here ?', 'created_at': datetime.datetime(2023, 1, 4, 10, 0, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370773795, 'issue_id': 1518658655, 'author': 'adrianreber', 'body': 'The error message comes from CRI-O. So it seems the CRIU support in CRI-O was not correctly enabled. Do you have a log from CRI-O? You should see a message like this:\r\n```\r\nDEBU[2023-01-04 10:54:05.655244620Z] Using criu from $PATH: /usr/local/sbin/criu   file=""config/config.go:1298""\r\nINFO[2023-01-04 10:54:05.655277912Z] Checkpoint/restore support enabled            file=""config/config.go:1068""\r\n```', 'created_at': datetime.datetime(2023, 1, 4, 10, 54, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371100777, 'issue_id': 1518658655, 'author': 'ContainerCheckpointfalse', 'body': '[criooutput.txt](https://github.com/kubernetes/kubernetes/files/10345441/criooutput.txt)\r\n\r\nTried to enable criu from  crio using the command\r\n`crio --enable-criu-support=true  --log-level = debug`\r\n\r\nseems to be enabled .\r\n\r\n> DEBU[2023-01-04 20:09:47.617217671+05:30] Using criu from $PATH: /usr/sbin/criu         file=""config/config.go:1294""\r\n> INFO[2023-01-04 20:09:47.617281811+05:30] Checkpoint/restore support enabled            file=""config/config.go:1064""\r\n> \r\n\r\n But still the same error output when i try to checkpoint my nginx container', 'created_at': datetime.datetime(2023, 1, 4, 15, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371119761, 'issue_id': 1518658655, 'author': 'adrianreber', 'body': 'That looks correct. Everything looks correct. Looking at the code the error message you get exist only at one location in the code and only happens if the CRIU binary is not found. Your log says something else. So it does not really make sense what is happening here. I never used minikube so maybe it uses another instance of CRI-O. Could that be? 1.25.0 is good enough.\r\n\r\nDo you see any other output in your crio log file once Kubernetes is running?\r\n\r\nThe next problem, if you use CRIU from Ubuntu, they packaged a broken version, so you need to update CRIU to a newer version once you are to solve this initial error.', 'created_at': datetime.datetime(2023, 1, 4, 16, 6, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371268803, 'issue_id': 1518658655, 'author': 'SergeyKanzhelev', 'body': '/triage needs-information\r\n/cc @haircommander \r\n\r\nCan you please try with non-broken version of CRI-O and report back. Thank you!', 'created_at': datetime.datetime(2023, 1, 4, 18, 21, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374531762, 'issue_id': 1518658655, 'author': 'crypticDAGGER', 'body': 'I was facing a similar issue when I tried checkpointing with kubeadm . Everything seemed to be installed and configured correctly .\r\nThe issue was \r\n[Failed to create existing container](https://github.com/cri-o/cri-o/issues/5490)\r\nEach time i requested the kubectl api to checkpoint my container the above error would be logged multiple times followed by\r\n\r\n> checkpointing of default/webserver/webserver failed (rpc error: code = Unknown desc = checkpoint/restore support not available)\r\n\r\nAfter this workaround [stop all pods and Restart](https://github.com/cri-o/cri-o/issues/5490#issuecomment-999835173), I was able to checkpoint without any issues.\r\nThe error isnt related to criu or its usage in k8s i presume.', 'created_at': datetime.datetime(2023, 1, 7, 16, 23, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374534062, 'issue_id': 1518658655, 'author': 'adrianreber', 'body': '@crypticDAGGER thanks for that information', 'created_at': datetime.datetime(2023, 1, 7, 16, 31, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 1374741864, 'issue_id': 1518658655, 'author': 'ContainerCheckpointfalse', 'body': '@crypticDAGGER Thanks this fixed the issue for me.', 'created_at': datetime.datetime(2023, 1, 8, 7, 34, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 1495676598, 'issue_id': 1518658655, 'author': 'Carol-code2222', 'body': 'I have a question. Can you answer it for me?\r\nWhen I run the conmmand `curl -sk -X POST ""https://localhost:10250/checkpoint/default/nginx/nginx""`  on the node, it needs to add `--key` and `--cert`, but where do I get this certificate?\r\n\r\n1. In the official document `https://kubernetes.io/blog/2022/12/05/forensic-container-checkpointing-alpha/`, the `key` and `cert` are `/var/run/kubernetes/client-admin.key` and `/var/run/kubernetes/client-admin.crt`, but the two files do not exist on the node of the Kubernetes cluster.\r\n\r\n2. According to your description, \r\n`curl -sk -X POST ""https://localhost:10250/checkpoint/default/webserver/webserver"" \\\r\n--key /etc/kubernetes/pki/apiserver-kubelet-client.key \\\r\n--cacert /etc/kubernetes/pki/ca.crt \\\r\n--cert /etc/kubernetes/pki/apiserver-kubelet-client.crt`\r\nOn the node, only `/etc/kubernetes/pki/ca.crt` exists, but `/etc/kubernetes/pki/apiserver-kubelet-client.crt` and `/etc/kubernetes/pki/apiserver-kubelet-client.key` do not exist. Correspondingly, `apiserver-kubelet-client.crt` and `apiserver-kubelet-client.key` exist only on the master node.\r\n\r\nnode2:\r\n![image](https://user-images.githubusercontent.com/31803337/229756026-0b2451a0-8565-449d-814b-bb7a66e74c80.png)\r\n\r\nmaster:\r\n![image](https://user-images.githubusercontent.com/31803337/229756315-7f592c20-cc95-46d1-92c8-d09f86da399b.png)\r\n\r\n\r\nThe Kubernetes version is 1.25.4. Where should I get the key and cert?', 'created_at': datetime.datetime(2023, 4, 4, 9, 52, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 1495702059, 'issue_id': 1518658655, 'author': 'adrianreber', 'body': '> I have a question. Can you answer it for me?\r\n\r\nNo, not really. This seems to be dependent on the type of your installation and where certificates and keys are stored', 'created_at': datetime.datetime(2023, 4, 4, 10, 11, 10, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-04 09:50:09 UTC): @ContainerCheckpointfalse: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

ContainerCheckpointfalse (Issue Creator) on (2023-01-04 09:57:39 UTC): /sig node

ContainerCheckpointfalse (Issue Creator) on (2023-01-04 10:00:05 UTC): cc: @adrianreber Am i missing something here ?

adrianreber on (2023-01-04 10:54:58 UTC): The error message comes from CRI-O. So it seems the CRIU support in CRI-O was not correctly enabled. Do you have a log from CRI-O? You should see a message like this:
```
DEBU[2023-01-04 10:54:05.655244620Z] Using criu from $PATH: /usr/local/sbin/criu   file=""config/config.go:1298""
INFO[2023-01-04 10:54:05.655277912Z] Checkpoint/restore support enabled            file=""config/config.go:1068""
```

ContainerCheckpointfalse (Issue Creator) on (2023-01-04 15:51:00 UTC): [criooutput.txt](https://github.com/kubernetes/kubernetes/files/10345441/criooutput.txt)

Tried to enable criu from  crio using the command
`crio --enable-criu-support=true  --log-level = debug`

seems to be enabled .


 But still the same error output when i try to checkpoint my nginx container

adrianreber on (2023-01-04 16:06:17 UTC): That looks correct. Everything looks correct. Looking at the code the error message you get exist only at one location in the code and only happens if the CRIU binary is not found. Your log says something else. So it does not really make sense what is happening here. I never used minikube so maybe it uses another instance of CRI-O. Could that be? 1.25.0 is good enough.

Do you see any other output in your crio log file once Kubernetes is running?

The next problem, if you use CRIU from Ubuntu, they packaged a broken version, so you need to update CRIU to a newer version once you are to solve this initial error.

SergeyKanzhelev on (2023-01-04 18:21:01 UTC): /triage needs-information
/cc @haircommander 

Can you please try with non-broken version of CRI-O and report back. Thank you!

crypticDAGGER on (2023-01-07 16:23:16 UTC): I was facing a similar issue when I tried checkpointing with kubeadm . Everything seemed to be installed and configured correctly .
The issue was 
[Failed to create existing container](https://github.com/cri-o/cri-o/issues/5490)
Each time i requested the kubectl api to checkpoint my container the above error would be logged multiple times followed by


After this workaround [stop all pods and Restart](https://github.com/cri-o/cri-o/issues/5490#issuecomment-999835173), I was able to checkpoint without any issues.
The error isnt related to criu or its usage in k8s i presume.

adrianreber on (2023-01-07 16:31:35 UTC): @crypticDAGGER thanks for that information

ContainerCheckpointfalse (Issue Creator) on (2023-01-08 07:34:19 UTC): @crypticDAGGER Thanks this fixed the issue for me.

Carol-code2222 on (2023-04-04 09:52:26 UTC): I have a question. Can you answer it for me?
When I run the conmmand `curl -sk -X POST ""https://localhost:10250/checkpoint/default/nginx/nginx""`  on the node, it needs to add `--key` and `--cert`, but where do I get this certificate?

1. In the official document `https://kubernetes.io/blog/2022/12/05/forensic-container-checkpointing-alpha/`, the `key` and `cert` are `/var/run/kubernetes/client-admin.key` and `/var/run/kubernetes/client-admin.crt`, but the two files do not exist on the node of the Kubernetes cluster.

2. According to your description, 
`curl -sk -X POST ""https://localhost:10250/checkpoint/default/webserver/webserver"" \
--key /etc/kubernetes/pki/apiserver-kubelet-client.key \
--cacert /etc/kubernetes/pki/ca.crt \
--cert /etc/kubernetes/pki/apiserver-kubelet-client.crt`
On the node, only `/etc/kubernetes/pki/ca.crt` exists, but `/etc/kubernetes/pki/apiserver-kubelet-client.crt` and `/etc/kubernetes/pki/apiserver-kubelet-client.key` do not exist. Correspondingly, `apiserver-kubelet-client.crt` and `apiserver-kubelet-client.key` exist only on the master node.

node2:
![image](https://user-images.githubusercontent.com/31803337/229756026-0b2451a0-8565-449d-814b-bb7a66e74c80.png)

master:
![image](https://user-images.githubusercontent.com/31803337/229756315-7f592c20-cc95-46d1-92c8-d09f86da399b.png)


The Kubernetes version is 1.25.4. Where should I get the key and cert?

adrianreber on (2023-04-04 10:11:10 UTC): No, not really. This seems to be dependent on the type of your installation and where certificates and keys are stored

"
1518326332,issue,closed,completed,ServiceAccount creating Secret Object by default for version greater than v1.22,"### What happened?

As per the change log mentioned [here](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#urgent-upgrade-notes), Secret API objects are no longer auto-generated for ServiceAccount. But, when I created ServiceAccount, it created secret object automatically.

Command Used (default namespace):

`kubectl create serviceaccount myapp-sa`


`kubectl get secret`
```Output:
myapp-sa-token-c779s                 kubernetes.io/service-account-token   3      15m
```

I am using kubernetes inside minikube in my ubuntu machine.


### What did you expect to happen?

As per the change logs, the secret object shouldn't have been created automatically.

### How can we reproduce it (as minimally and precisely as possible)?

With the same version as above, create the new service account.


### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:""1"", Minor:""24"", GitVersion:""v1.24.1"", GitCommit:""3ddd0f45aa91e2f30c70734b175631bec5b5825a"", GitTreeState:""clean"", BuildDate:""2022-05-24T12:26:19Z"", GoVersion:""go1.18.2"", Compiler:""gc"", Platform:""linux/amd64""}
Kustomize Version: v4.5.4
Server Version: version.Info{Major:""1"", Minor:""23"", GitVersion:""v1.23.3"", GitCommit:""816c97ab8cff8a1c72eccca1026f7820e93e0d25"", GitTreeState:""clean"", BuildDate:""2022-01-25T21:19:12Z"", GoVersion:""go1.17.6"", Compiler:""gc"", Platform:""linux/amd64""}
```

</details>


### Cloud provider

<details>
minikube in local
</details>


### OS version

<details>

```console

$ cat /etc/os-release
NAME=""Linux Mint""
VERSION=""20.3 (Una)""
ID=linuxmint
ID_LIKE=ubuntu
PRETTY_NAME=""Linux Mint 20.3""
VERSION_ID=""20.3""
HOME_URL=""https://www.linuxmint.com/""
SUPPORT_URL=""https://forums.linuxmint.com/""
BUG_REPORT_URL=""http://linuxmint-troubleshooting-guide.readthedocs.io/en/latest/""
PRIVACY_POLICY_URL=""https://www.linuxmint.com/""
VERSION_CODENAME=una
UBUNTU_CODENAME=focal
$ uname -a
Linux surajgautam 5.4.0-135-generic #152-Ubuntu SMP Wed Nov 23 20:19:22 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux


</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",surajgautam,2023-01-04 05:11:40+00:00,[],2023-01-19 03:45:21+00:00,2023-01-19 03:45:20+00:00,https://github.com/kubernetes/kubernetes/issues/114799,"[('sig/auth', 'Categorizes an issue or PR as relevant to SIG Auth.'), ('triage/not-reproducible', 'Indicates an issue can not be reproduced as described.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1370496800, 'issue_id': 1518326332, 'author': 'k8s-ci-robot', 'body': '@surajgautam: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 4, 5, 11, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370499616, 'issue_id': 1518326332, 'author': 'surajgautam', 'body': '/sig release', 'created_at': datetime.datetime(2023, 1, 4, 5, 17, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370500331, 'issue_id': 1518326332, 'author': 'surajgautam', 'body': '/sig architecture', 'created_at': datetime.datetime(2023, 1, 4, 5, 19, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370500827, 'issue_id': 1518326332, 'author': 'surajgautam', 'body': '/sig apps', 'created_at': datetime.datetime(2023, 1, 4, 5, 20, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370619369, 'issue_id': 1518326332, 'author': 'pacoxu', 'body': '/sig auth\r\n/remove-sig apps architecture release\r\nI think you refer to https://github.com/kubernetes/kubernetes/pull/108309 LegacyServiceAccountTokenNoAutoGeneration to beta.\r\n\r\n> Server Version: version.Info{Major:""1"", Minor:""23"", GitVersion:""v1.23.3"", GitCommit:""816c97ab8cff8a1c72eccca1026f7820e93e0d25"", GitTreeState:""clean"", BuildDate:""2022-01-25T21:19:12Z"", GoVersion:""go1.17.6"", Compiler:""gc"", Platform:""linux/amd64""}\r\n\r\nAs your information shows, you are using Kubernetes v1.23 and LegacyServiceAccountTokenNoAutoGeneration is beta in v1.24, the behavior is expected. You may have to upgrade to v1.24+ to get this feature. If I understand correctly, this is not a bug.', 'created_at': datetime.datetime(2023, 1, 4, 8, 34, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370695806, 'issue_id': 1518326332, 'author': 'sftim', 'body': '/triage not-reproducible\r\n\r\nLegacyServiceAccountTokenNoAutoGeneration was not available before Kubernetes v1.24', 'created_at': datetime.datetime(2023, 1, 4, 9, 41, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 1381321409, 'issue_id': 1518326332, 'author': 'Shubham82', 'body': '@surajgautam,  from k8s v1.24  `LegacyServiceAccountTokenNoAutoGeneration` feature gate, is enabled by default, so what @pacoxu said above is right. it is not a bug.\r\nPlease see the [Urgent Upgrade Notes](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#urgent-upgrade-notes) for CHANGELOG-1.24.', 'created_at': datetime.datetime(2023, 1, 13, 5, 7, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 1381327262, 'issue_id': 1518326332, 'author': 'Shubham82', 'body': '/remove-kind bug', 'created_at': datetime.datetime(2023, 1, 13, 5, 16, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 1396402134, 'issue_id': 1518326332, 'author': 'Shubham82', 'body': 'closing this issue, as functionality work as expected.\r\nPlease re-open this issue if you have any concerns regarding this.\r\n\r\n/close', 'created_at': datetime.datetime(2023, 1, 19, 3, 45, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 1396402176, 'issue_id': 1518326332, 'author': 'k8s-ci-robot', 'body': '@Shubham82: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114799#issuecomment-1396402134):\n\n>closing this issue, as functionality work as expected.\r\n>Please re-open this issue if you have any concerns regarding this.\r\n>\r\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 19, 3, 45, 20, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-04 05:11:48 UTC): @surajgautam: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

surajgautam (Issue Creator) on (2023-01-04 05:17:33 UTC): /sig release

surajgautam (Issue Creator) on (2023-01-04 05:19:08 UTC): /sig architecture

surajgautam (Issue Creator) on (2023-01-04 05:20:24 UTC): /sig apps

pacoxu on (2023-01-04 08:34:55 UTC): /sig auth
/remove-sig apps architecture release
I think you refer to https://github.com/kubernetes/kubernetes/pull/108309 LegacyServiceAccountTokenNoAutoGeneration to beta.


As your information shows, you are using Kubernetes v1.23 and LegacyServiceAccountTokenNoAutoGeneration is beta in v1.24, the behavior is expected. You may have to upgrade to v1.24+ to get this feature. If I understand correctly, this is not a bug.

sftim on (2023-01-04 09:41:33 UTC): /triage not-reproducible

LegacyServiceAccountTokenNoAutoGeneration was not available before Kubernetes v1.24

Shubham82 on (2023-01-13 05:07:21 UTC): @surajgautam,  from k8s v1.24  `LegacyServiceAccountTokenNoAutoGeneration` feature gate, is enabled by default, so what @pacoxu said above is right. it is not a bug.
Please see the [Urgent Upgrade Notes](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#urgent-upgrade-notes) for CHANGELOG-1.24.

Shubham82 on (2023-01-13 05:16:51 UTC): /remove-kind bug

Shubham82 on (2023-01-19 03:45:15 UTC): closing this issue, as functionality work as expected.
Please re-open this issue if you have any concerns regarding this.

/close

k8s-ci-robot on (2023-01-19 03:45:20 UTC): @Shubham82: Closing this issue.

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114799#issuecomment-1396402134):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

"
1518165064,issue,closed,completed,CPU manager does not seem to return proper topology hints for Pod  scope,"### What happened?

When the topology manager scope is set to `pod`, CPU manager returns a non-preferred hint that crosses NUMA nodes.

Testing environment:

- Node NUMA topology
  - Two NUMA nodes:
    - NUMA node 0 has two CPU cores
    - NUMA node 1 has two CPU cores

- Kubelet config

  All of the CPU manager, topology manager and memory manager are enabled. Topology manager is in ""Pod"" scope and using `restricted` policy.

```
""--cpu-manager-policy=static --topology-manager-scope=pod --topology-manager-policy=restricted --memory-manager-policy=Static --reserved-memory 0:memory=3864Mi""
```

- Pod spec

```
apiVersion: v1
kind: Pod
metadata:
  name: u0
spec:
  containers:
  - image: ubuntu
    command:
      - ""sleep""
      - ""604800""
    name: u0-a
    resources:
      limits:
        memory: ""200Mi""
        cpu: ""2""
      requests:
        memory: ""200Mi""
        cpu: ""2""
  - image: ubuntu
    command:
      - ""sleep""
      - ""604800""
    name: u0-b
    resources:
      limits:
        memory: ""200Mi""
        cpu: ""1""
      requests:
        memory: ""200Mi""
        cpu: ""1""
```

- Hint returned by CPU manager:

```
Jan 04 01:28:18 worker-1--82a66dec968226e-3697a2e796e7148a.lab.anthos kubelet[373797]: I0104 01:28:18.574223  373797 topology_manager.go:205] ""Topology Admit Handler""
Jan 04 01:28:18 worker-1--82a66dec968226e-3697a2e796e7148a.lab.anthos kubelet[373797]: I0104 01:28:18.574272  373797 scope_pod.go:77] ""TopologyHints"" hints=map[] pod=""default/u0""
Jan 04 01:28:18 worker-1--82a66dec968226e-3697a2e796e7148a.lab.anthos kubelet[373797]: I0104 01:28:18.574374  373797 policy_static.go:517] ""TopologyHints generated"" pod=""default/u0"" cpuHints=[{NUMANodeAffinity:11 Preferred:false}]
Jan 04 01:28:18 worker-1--82a66dec968226e-3697a2e796e7148a.lab.anthos kubelet[373797]: I0104 01:28:18.574385  373797 scope_pod.go:77] ""TopologyHints"" hints=map[cpu:[{NUMANodeAffinity:11 Preferred:false}]] pod=""default/u0""
Jan 04 01:28:18 worker-1--82a66dec968226e-3697a2e796e7148a.lab.anthos kubelet[373797]: I0104 01:28:18.574436  373797 scope_pod.go:77] ""TopologyHints"" hints=map[memory:[{NUMANodeAffinity:01 Preferred:true} {NUMANodeAffinity:10 Preferred:true} {NUMANodeAffinity:11 Preferred:false}]] pod=""default/u0""
Jan 04 01:28:18 worker-1--82a66dec968226e-3697a2e796e7148a.lab.anthos kubelet[373797]: I0104 01:28:18.574453  373797 policy.go:72] ""Hint Provider has no preference for NUMA affinity with any resource""
Jan 04 01:28:18 worker-1--82a66dec968226e-3697a2e796e7148a.lab.anthos kubelet[373797]: I0104 01:28:18.574464  373797 scope_pod.go:85] ""PodTopologyHint"" bestHint={NUMANodeAffinity:11 Preferred:false}
Jan 04 01:28:18 worker-1--82a66dec968226e-3697a2e796e7148a.lab.anthos kubelet[373797]: I0104 01:28:18.574472  373797 scope_pod.go:53] ""Best TopologyHint"" bestHint={NUMANodeAffinity:11 Preferred:false} pod=""default/u0""

```

As a result the Pod cannot be scheduled:

```
Events:
  Type     Reason                 Age    From     Message
  ----     ------                 ----   ----     -------
  Warning  TopologyAffinityError  2m40s  kubelet  Resources cannot be allocated with Topology locality
```

### What did you expect to happen?

I expect the CPU manager to return a ""preferred""  hint that crosses two NUMA nodes, as the topology manager scope is set to ""Pod"", and each NUMA node can alone satisfy each container of the Pod.

This behaivor is contradict to the memory manager. The Pod below can be scheduled, **even if one of its containers already requests hugepages across NUMA nodes**.

- Setup:
  - Two NUMA nodes:
    - NUMA node 0 has 2G hugepages-2Mi
    - NUMA node 1 has 2G hugepages-2Mi
  
  - Kubelet flags: same with before 

- Pod spec:

```
apiVersion: v1
kind: Pod
metadata:
  name: u0
spec:
  containers:
  - image: ubuntu
    command:
      - ""sleep""
      - ""604800""
    name: u0-a
    resources:
      limits:
        memory: ""200Mi""
        cpu: ""200m""
        hugepages-2Mi: 3Gi
      requests:
        memory: ""200Mi""
        cpu: ""200m""
        hugepages-2Mi: 3Gi
  - image: ubuntu
    command:
      - ""sleep""
      - ""604800""
    name: u0-b
    resources:
      limits:
        memory: ""200Mi""
        cpu: ""100m""
        hugepages-2Mi: 500Mi
      requests:
        memory: ""200Mi""
        cpu: ""100m""
        hugepages-2Mi: 500Mi
```

### How can we reproduce it (as minimally and precisely as possible)?

Please see the testing environment above.

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""20"", GitVersion:""v1.20.5"", GitCommit:""6b1d87acf3c8253c123756b9e61dac642678305f"", GitTreeState:""clean"", BuildDate:""2021-03-18T01:10:43Z"", GoVersion:""go1.15.8"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""25"", GitVersion:""v1.25.5-gke.100"", GitCommit:""ccb35b8b179fee26fa6f46c59fdb5f8ff569fdfd"", GitTreeState:""clean"", BuildDate:""2022-12-08T21:50:42Z"", GoVersion:""go1.19.4 X:boringcrypto"", Compiler:""gc"", Platform:""linux/amd64""}

```

</details>


### Cloud provider

GKE

### OS version

<details>

Ubuntu 20.04

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",pomodorox,2023-01-04 01:44:12+00:00,[],2023-01-04 18:18:34+00:00,2023-01-04 18:18:33+00:00,https://github.com/kubernetes/kubernetes/issues/114797,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('sig/node', 'Categorizes an issue or PR as relevant to SIG Node.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1370397389, 'issue_id': 1518165064, 'author': 'k8s-ci-robot', 'body': '@didovesei: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 4, 1, 44, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370597292, 'issue_id': 1518165064, 'author': 'pacoxu', 'body': '/sig node', 'created_at': datetime.datetime(2023, 1, 4, 8, 9, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371266163, 'issue_id': 1518165064, 'author': 'SergeyKanzhelev', 'body': 'As https://kubernetes.io/docs/tasks/administer-cluster/topology-manager/#pod-scope states:\r\n\r\n> That is, the Topology Manager treats a pod as a whole and attempts to allocate the entire pod (all containers) to either a single NUMA node or a common set of NUMA nodes. \r\n\r\nMeaning that this Pod cannot be scheduled. Am I missing something in the bug description? Please reopen if so\r\n\r\n/close', 'created_at': datetime.datetime(2023, 1, 4, 18, 18, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371266273, 'issue_id': 1518165064, 'author': 'k8s-ci-robot', 'body': '@SergeyKanzhelev: Closing this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114797#issuecomment-1371266163):\n\n>As https://kubernetes.io/docs/tasks/administer-cluster/topology-manager/#pod-scope states:\r\n>\r\n>> That is, the Topology Manager treats a pod as a whole and attempts to allocate the entire pod (all containers) to either a single NUMA node or a common set of NUMA nodes. \r\n>\r\n>Meaning that this Pod cannot be scheduled. Am I missing something in the bug description? Please reopen if so\r\n>\r\n>/close\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 4, 18, 18, 34, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-04 01:44:20 UTC): @didovesei: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

pacoxu on (2023-01-04 08:09:08 UTC): /sig node

SergeyKanzhelev on (2023-01-04 18:18:28 UTC): As https://kubernetes.io/docs/tasks/administer-cluster/topology-manager/#pod-scope states:


Meaning that this Pod cannot be scheduled. Am I missing something in the bug description? Please reopen if so

/close

k8s-ci-robot on (2023-01-04 18:18:34 UTC): @SergeyKanzhelev: Closing this issue.

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114797#issuecomment-1371266163):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

"
1518124851,issue,closed,completed,scheduler_perf: `SchedulingMigratedInTreePVs` is failing,"### What happened?

/sig scheduling

`SchedulingMigratedInTreePVs` in the `ci-benchmark-scheduler-perf-master` job is failing.

```
BenchmarkPerfScheduling/SchedulingMigratedInTreePVs/500Nodes
panic: feature ""CSIMigration"" is not registered in FeatureGate ""k8s.io/apiserver/pkg/util/feature/feature_gate.go:28""
```

https://github.com/kubernetes/kubernetes/issues/114509#issuecomment-1370367560

### What did you expect to happen?

success

### How can we reproduce it (as minimally and precisely as possible)?

run `scheduler_perf`

### Anything else we need to know?

Due to this failure, we cannot see any scheduler metrics results from the perf-dash.

### Kubernetes version

master

",sanposhiho,2023-01-04 00:50:56+00:00,['sanposhiho'],2023-01-04 22:30:34+00:00,2023-01-04 22:30:34+00:00,https://github.com/kubernetes/kubernetes/issues/114795,"[('sig/scheduling', 'Categorizes an issue or PR as relevant to SIG Scheduling.'), ('kind/failing-test', 'Categorizes issue or PR as related to a consistently or frequently failing test.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1370372326, 'issue_id': 1518124851, 'author': 'k8s-ci-robot', 'body': '@sanposhiho: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 4, 0, 51, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370372763, 'issue_id': 1518124851, 'author': 'sanposhiho', 'body': '/remove-kind bug\r\n/kind flake\r\n/kind failing-test', 'created_at': datetime.datetime(2023, 1, 4, 0, 52, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370375286, 'issue_id': 1518124851, 'author': 'sanposhiho', 'body': '/assign\r\n/remove-kind flake', 'created_at': datetime.datetime(2023, 1, 4, 0, 58, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370383245, 'issue_id': 1518124851, 'author': 'sanposhiho', 'body': 'will fixed by https://github.com/kubernetes/kubernetes/pull/114796', 'created_at': datetime.datetime(2023, 1, 4, 1, 11, 22, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-04 00:51:03 UTC): @sanposhiho: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

sanposhiho (Issue Creator) on (2023-01-04 00:52:02 UTC): /remove-kind bug
/kind flake
/kind failing-test

sanposhiho (Issue Creator) on (2023-01-04 00:58:07 UTC): /assign
/remove-kind flake

sanposhiho (Issue Creator) on (2023-01-04 01:11:22 UTC): will fixed by https://github.com/kubernetes/kubernetes/pull/114796

"
1517849651,issue,closed,not_planned,PodTopologyConstraints - DoNotSchedule not working as expected,"### What happened?

Hi,

I tried to use PodTopologySpreadConstraint to organize our deployment engine, and to spread replicas across zones. 

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 4 
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: nginx
      tolerations:
      - effect: NoSchedule
        key: platform
        operator: Equal
        value: ""true""
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nodes
                operator: In
                values:
                - platform
```

I expected that, if I have just one node labeled ""platform"", the scheduler needs to run just one replica of Nginx, and others need to be in a ""pending"" state because ""DoNotSchedule"" constraint is not satisfied.

But the deployment running the 4 replicas in the same node/zone, even that ""DoNotSchedule"" defined.

```
➜ kubectl get pods -o wide | grep nginx-deployment
nginx-deployment-6cbbb547c-6qvrg   1/1     Running            0               15m     192.168.33.28    ip-192-168-42-2.sa-east-1.compute.internal     <none>           <none>
nginx-deployment-6cbbb547c-cgbcf   1/1     Running            0               15m     192.168.37.251   ip-192-168-42-2.sa-east-1.compute.internal     <none>           <none>
nginx-deployment-6cbbb547c-dlqvd   1/1     Running            0               15m     192.168.47.173   ip-192-168-42-2.sa-east-1.compute.internal     <none>           <none>
nginx-deployment-6cbbb547c-hkgzc   1/1     Running            0               15m     192.168.39.27    ip-192-168-42-2.sa-east-1.compute.internal     <none>           <none>
```

and i just have one node labeled ""platform""

```
➜ kubectl get nodes -l app=platform
NAME                                         STATUS   ROLES    AGE     VERSION
ip-192-168-42-2.sa-east-1.compute.internal   Ready    <none>   4h45m   v1.22.15-eks-fb459a0
```

Does anyone have any idea what I made wrong?

### What did you expect to happen?

I expect that PodTopologySpreadConstraint with ""DoNotSchedule"" value, not scheduling pods in the same zone/node if I have just one node.

### How can we reproduce it (as minimally and precisely as possible)?

Apply the following manifest with one node labeled ""platform"" and following kubelet_extra_args: `-node-labels=app=platform,nodes=platform --register-with-taints=platform=true:NoSchedule`, or remove affinity and toleration from manifest and run in cluster with single node.

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 4 # tells deployment to run 2 pods matching the template
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: nginx
      tolerations:
      - effect: NoSchedule
        key: platform
        operator: Equal
        value: ""true""
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nodes
                operator: In
                values:
                - platform
```

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
➜ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:""1"", Minor:""25"", GitVersion:""v1.25.3"", GitCommit:""434bfd82814af038ad94d62ebe59b133fcb50506"", GitTreeState:""clean"", BuildDate:""2022-10-12T10:47:25Z"", GoVersion:""go1.19.2"", Compiler:""gc"", Platform:""darwin/arm64""}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:""1"", Minor:""22+"", GitVersion:""v1.22.16-eks-ffeb93d"", GitCommit:""52e500d139bdef42fbc4540c357f0565c7867a81"", GitTreeState:""clean"", BuildDate:""2022-11-29T18:41:42Z"", GoVersion:""go1.16.15"", Compiler:""gc"", Platform:""linux/amd64""}
WARNING: version difference between client (1.25) and server (1.22) exceeds the supported minor version skew of +/-1
```

</details>


### Cloud provider

<details>
AWS
</details>


### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",chmodrs,2023-01-03 19:55:00+00:00,[],2024-04-04 17:06:29+00:00,2024-04-04 17:06:27+00:00,https://github.com/kubernetes/kubernetes/issues/114788,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('sig/scheduling', 'Categorizes an issue or PR as relevant to SIG Scheduling.'), ('lifecycle/rotten', 'Denotes an issue or PR that has aged beyond stale and will be auto-closed.'), ('triage/needs-information', 'Indicates an issue needs more information in order to work on it.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1370167526, 'issue_id': 1517849651, 'author': 'k8s-ci-robot', 'body': '@chmodrs: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 3, 19, 55, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370598005, 'issue_id': 1517849651, 'author': 'pacoxu', 'body': '/sig scheduling', 'created_at': datetime.datetime(2023, 1, 4, 8, 9, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370821560, 'issue_id': 1517849651, 'author': 'songminglong', 'body': ""PodTopologyConstraints add new feature gate 'NodeInclusionPolicyInPodTopologySpread' to control the relationship between PodTopologyConstraints and nodeAffinity/nodeSelector in 1.25/alpha version. The default value of nodeAffinityPolicy is honor, means that only nodes matching nodeAffinity/nodeSelector are included in the calculations"", 'created_at': datetime.datetime(2023, 1, 4, 11, 38, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370981749, 'issue_id': 1517849651, 'author': 'chmodrs', 'body': '@songminglong perfect, I understand that PodTopologySpreadConstraint doesn\'t work very well with nodeAffinity/nodeSelector definition, right?\r\n\r\nBut in case that we don\'t use this flags? Like bellow:\r\n\r\n```\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: nginx-deployment\r\nspec:\r\n  selector:\r\n    matchLabels:\r\n      app: nginx\r\n  replicas: 8 # tells deployment to run 2 pods matching the template\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: nginx\r\n    spec:\r\n      containers:\r\n      - name: nginx\r\n        image: nginx:alpine\r\n        ports:\r\n        - containerPort: 80\r\n      topologySpreadConstraints:\r\n      - maxSkew: 1\r\n        topologyKey: topology.kubernetes.io/zone\r\n        whenUnsatisfiable: DoNotSchedule\r\n        labelSelector:\r\n          matchLabels:\r\n            app: nginx\r\n      #tolerations:\r\n      #- effect: NoSchedule\r\n      #  key: platform\r\n      #  operator: Equal\r\n      #  value: ""true""\r\n      #affinity:\r\n      #  nodeAffinity:\r\n      #    requiredDuringSchedulingIgnoredDuringExecution:\r\n      #      nodeSelectorTerms:\r\n      #      - matchExpressions:\r\n      #        - key: nodes\r\n      #          operator: In\r\n      #          values:\r\n      #          - platform\r\n```\r\nIn this case, I just have 3 zones in sa-east-1, I think that the scheduler just can run one pod per zone and others need to be in a ""pending state"", don\'t they?\r\n\r\nBut still running multiple pods in the same zone :(\r\n\r\n```\r\nnginx-deployment-56f498fc9b-4w7hn   1/1     Running     0                 44s     192.168.33.145   ip-192-168-43-89.sa-east-1.compute.internal    <none>           <none>\r\nnginx-deployment-56f498fc9b-9wqgz   1/1     Running     0                 11s     192.168.43.127   ip-192-168-43-89.sa-east-1.compute.internal    <none>           <none>\r\nnginx-deployment-56f498fc9b-dq966   1/1     Running     0                 44s     192.168.61.95    ip-192-168-63-116.sa-east-1.compute.internal   <none>           <none>\r\nnginx-deployment-56f498fc9b-fflc6   1/1     Running     0                 11s     192.168.70.36    ip-192-168-70-157.sa-east-1.compute.internal   <none>           <none>\r\nnginx-deployment-56f498fc9b-jgzg8   1/1     Running     0                 11s     192.168.63.240   ip-192-168-63-116.sa-east-1.compute.internal   <none>           <none>\r\nnginx-deployment-56f498fc9b-m5g6r   1/1     Running     0                 44s     192.168.78.10    ip-192-168-70-157.sa-east-1.compute.internal   <none>           <none>\r\nnginx-deployment-56f498fc9b-p6z2z   1/1     Running     0                 44s     192.168.45.204   ip-192-168-43-89.sa-east-1.compute.internal    <none>           <none>\r\nnginx-deployment-56f498fc9b-zxmr4   1/1     Running     0                 11s     192.168.53.57    ip-192-168-63-116.sa-east-1.compute.internal   <none>           <none>\r\n```', 'created_at': datetime.datetime(2023, 1, 4, 14, 14, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371976505, 'issue_id': 1517849651, 'author': 'kerthcet', 'body': '> I expected that, if I have just one node labeled ""platform"", the scheduler needs to run just one replica of Nginx, and others need to be in a ""pending"" state because ""DoNotSchedule"" constraint is not satisfied.\r\n\r\nzone is divided by `topologyKey: topology.kubernetes.io/zone`, not `platform`.', 'created_at': datetime.datetime(2023, 1, 5, 9, 29, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371977406, 'issue_id': 1517849651, 'author': 'kerthcet', 'body': ""Here're some examples: https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/#topology-spread-constraint-examples"", 'created_at': datetime.datetime(2023, 1, 5, 9, 30, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 1384128191, 'issue_id': 1517849651, 'author': 'sftim', 'body': 'Do we think this is a bug, or an opportunity to provide extra documentation?', 'created_at': datetime.datetime(2023, 1, 16, 14, 14, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 1384797766, 'issue_id': 1517849651, 'author': 'kerthcet', 'body': 'Yes, I think we should add the examples of new features like `NodeInclusionPolicyInPodTopologySpread ` to https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/#topology-spread-constraint-examples, and we also plan to post a new blog about them.', 'created_at': datetime.datetime(2023, 1, 17, 3, 49, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 1384801307, 'issue_id': 1517849651, 'author': 'kerthcet', 'body': ""From the output:\r\n```\r\nnginx-deployment-56f498fc9b-4w7hn   1/1     Running     0                 44s     192.168.33.145   ip-192-168-43-89.sa-east-1.compute.internal    <none>           <none>\r\nnginx-deployment-56f498fc9b-9wqgz   1/1     Running     0                 11s     192.168.43.127   ip-192-168-43-89.sa-east-1.compute.internal    <none>           <none>\r\nnginx-deployment-56f498fc9b-dq966   1/1     Running     0                 44s     192.168.61.95    ip-192-168-63-116.sa-east-1.compute.internal   <none>           <none>\r\nnginx-deployment-56f498fc9b-fflc6   1/1     Running     0                 11s     192.168.70.36    ip-192-168-70-157.sa-east-1.compute.internal   <none>           <none>\r\nnginx-deployment-56f498fc9b-jgzg8   1/1     Running     0                 11s     192.168.63.240   ip-192-168-63-116.sa-east-1.compute.internal   <none>           <none>\r\nnginx-deployment-56f498fc9b-m5g6r   1/1     Running     0                 44s     192.168.78.10    ip-192-168-70-157.sa-east-1.compute.internal   <none>           <none>\r\nnginx-deployment-56f498fc9b-p6z2z   1/1     Running     0                 44s     192.168.45.204   ip-192-168-43-89.sa-east-1.compute.internal    <none>           <none>\r\nnginx-deployment-56f498fc9b-zxmr4   1/1     Running     0                 11s     192.168.53.57    ip-192-168-63-116.sa-east-1.compute.internal   <none>           <none>\r\n```\r\n\r\nI guess you have three zones:\r\n - ip-192-168-63-116.sa-east-1.compute.internal  -- three pods matched\r\n - ip-192-168-43-89.sa-east-1.compute.internal   -- three pods matched\r\n - ip-192-168-43-157.sa-east-1.compute.internal  -- two pods matched\r\n\r\nThe maxMatchedPodsNumber - minMatchedPodsNumber = 3 - 2 = 1 <= maxSkew, so it's still satisfied and working as expected. @chmodrs Can you confirm this and I guess you misunderstood the usage."", 'created_at': datetime.datetime(2023, 1, 17, 3, 57, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 1384801684, 'issue_id': 1517849651, 'author': 'kerthcet', 'body': '/triage needs-information', 'created_at': datetime.datetime(2023, 1, 17, 3, 58, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 1384828076, 'issue_id': 1517849651, 'author': 'chmodrs', 'body': ""Hi @kerthcet \r\n\r\nSo maybe I made a confusion about PodTopologySpreadConstraint utilization, right?\r\n\r\nI think that configuring maxSkew = 1 corresponds to that I will have 1 pod in each zone, using `topologyKey: topology.kubernetes.io/zone` (its what I wanted), but I need to use node toleration + affinity to place my pods in correct nodegroup (platform) too.\r\n\r\nAn example of a scenario that I want to configure, with PodTopologySpreadConstraint is:\r\n\r\n- 3 zones (a, b, c)\r\n- All deployments with 4 replicas\r\n- Pods running in `platform` nodegroup\r\n\r\nGuarantee that I have at least 1 pod in each zone, and the last pod (fourth) in any zone.\r\n\r\nI can't configure this scenario with PodTopologySpreadConstraints?\r\n\r\nThankss!!"", 'created_at': datetime.datetime(2023, 1, 17, 4, 50, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 1384871700, 'issue_id': 1517849651, 'author': 'kerthcet', 'body': 'So I guess all the nodes in platform nodegroup scoped by the three zones. Then what you pasted above can work as your expected:\r\n```\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: nginx-deployment\r\nspec:\r\n  selector:\r\n    matchLabels:\r\n      app: nginx\r\n  replicas: 4 # tells deployment to run 2 pods matching the template\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: nginx\r\n    spec:\r\n      containers:\r\n      - name: nginx\r\n        image: nginx:alpine\r\n        ports:\r\n        - containerPort: 80\r\n      topologySpreadConstraints:\r\n      - maxSkew: 1\r\n        topologyKey: topology.kubernetes.io/zone\r\n        whenUnsatisfiable: DoNotSchedule\r\n        labelSelector:\r\n          matchLabels:\r\n            app: nginx\r\n      tolerations:\r\n      - effect: NoSchedule\r\n        key: platform\r\n        operator: Equal\r\n        value: ""true""\r\n      affinity:\r\n        nodeAffinity:\r\n          requiredDuringSchedulingIgnoredDuringExecution:\r\n            nodeSelectorTerms:\r\n            - matchExpressions:\r\n              - key: nodes\r\n                operator: In\r\n                values:\r\n                - platform\r\n```\r\n\r\nThe nodeAffinity will filter out all the nodes with label `nodes:platform`, then `topologySpreadConstraints` will divide the zones via `topology.kubernetes.io/zone`, and make sure not violating the maxSkew.', 'created_at': datetime.datetime(2023, 1, 17, 5, 58, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 1428239465, 'issue_id': 1517849651, 'author': 'chmodrs', 'body': 'Hi @kerthcet , sorry for my lateness.\r\n\r\nI tried to apply the following deployment:\r\n\r\n```\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: nginx-deployment\r\nspec:\r\n  selector:\r\n    matchLabels:\r\n      app: nginx\r\n  replicas: 5\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: nginx\r\n    spec:\r\n      containers:\r\n      - name: nginx\r\n        image: nginx:alpine\r\n        ports:\r\n        - containerPort: 80\r\n      topologySpreadConstraints:\r\n      - maxSkew: 1\r\n        topologyKey: topology.kubernetes.io/zone\r\n        whenUnsatisfiable: DoNotSchedule\r\n        labelSelector:\r\n          matchLabels:\r\n            app: nginx\r\n```\r\n\r\nI have a EKS cluster in sa-east-1 region with 3 availability zones.\r\n\r\n```\r\n➜ kubectl get pods | grep nginx-deployment\r\nnginx-deployment-56f498fc9b-d4hfr   1/1     Running            0                  9m56s\r\nnginx-deployment-56f498fc9b-d6hzd   1/1     Running            0                  9m56s\r\nnginx-deployment-56f498fc9b-nkgs5   1/1     Running            0                  9m56s\r\nnginx-deployment-56f498fc9b-q968x   1/1     Running            0                  9m53s\r\nnginx-deployment-56f498fc9b-vdwmq   1/1     Running            0                  9m54s\r\n```\r\n\r\nWith just 3 zones and maxSkew=1, we need to get two pods in ""Pending"" state, right? Because ""DotNotSchedule"" is defined.\r\n\r\nIn my case, even i use ""DoNotSchedule"" or ""ScheduleAnyway"" the result is the same.', 'created_at': datetime.datetime(2023, 2, 13, 16, 24, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 1432651613, 'issue_id': 1517849651, 'author': 'geniusjoe', 'body': ""@kerthcet \r\nI use EKS with kubernetes 1.20.x and it seems that I encountered the same problem. I am a newbee in kubernetes and I don't know if my opration is correct. Steps below:\r\n\r\n1. Create a cluster with two EKS nodes: node1, node2\r\n2. Add a label to node1: `testLabel: testValue`\r\n3. Create a StatefulSet with 3 replicas, and I set `topologySpreadConstraints` and `nodeAffinity` properties like:\r\n```\r\n    topologySpreadConstraints:\r\n    - maxSkew: 1\r\n      topologyKey: testLabel\r\n      whenUnsatisfiable: DoNotSchedule\r\n      labelSelector:\r\n        matchLabels:\r\n          app: my-app\r\n    affinity:\r\n      nodeAffinity:\r\n        requiredDuringSchedulingIgnoredDuringExecution:\r\n          nodeSelectorTerms:\r\n          - matchExpressions:\r\n            - key: testLabel\r\n              operator: In\r\n              values:\r\n                - testValue\r\n                - testValue1\r\n```\r\nThis time every created pods will be scheduled to node1, and it work fine.\r\n\r\n4. Add a label in node2: `testLabel: testValue1`\r\n5. Expand this StatefulSet to 4 replicas, I think this new pod will still hang in pending status because `maxSkew` violate in node1(3pods) and node2(0pod). *But the pod now is successfully scheduled to node2 and is in running status.*\r\n\r\nAre there any questions or advice? Please feel free to let me know."", 'created_at': datetime.datetime(2023, 2, 16, 7, 41, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 1551144661, 'issue_id': 1517849651, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 5, 17, 10, 31, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 1556178866, 'issue_id': 1517849651, 'author': 'kerthcet', 'body': '/remove-lifecycle stale', 'created_at': datetime.datetime(2023, 5, 21, 13, 20, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 1793738535, 'issue_id': 1517849651, 'author': 'dharmjit', 'body': '@chmodrs In your example above, The `maxSkew=1` is not violated, as the pod distribution across AZs might be 2:2:1. So it seems like the feature is working as expected.\r\n>With just 3 zones and maxSkew=1, we need to get two pods in ""Pending"" state, right? Because ""DotNotSchedule"" is defined.', 'created_at': datetime.datetime(2023, 11, 5, 13, 31, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 1795305815, 'issue_id': 1517849651, 'author': 'dharmjit', 'body': '@geniusjoe Not sure but it seems to me that when a new node is added, the skew would be 3 because of 3,3,0 for an incoming pod and scheduling the pod on new node will decrease the maxSkew to 2.', 'created_at': datetime.datetime(2023, 11, 6, 16, 9, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 1925822907, 'issue_id': 1517849651, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2024, 2, 4, 16, 34, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 1979192450, 'issue_id': 1517849651, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2024, 3, 5, 16, 42, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2037743215, 'issue_id': 1517849651, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2024, 4, 4, 17, 6, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2037743353, 'issue_id': 1517849651, 'author': 'k8s-ci-robot', 'body': '@k8s-triage-robot: Closing this issue, marking it as ""Not Planned"".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114788#issuecomment-2037743215):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2024, 4, 4, 17, 6, 27, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-03 19:55:08 UTC): @chmodrs: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

pacoxu on (2023-01-04 08:09:50 UTC): /sig scheduling

songminglong on (2023-01-04 11:38:47 UTC): PodTopologyConstraints add new feature gate 'NodeInclusionPolicyInPodTopologySpread' to control the relationship between PodTopologyConstraints and nodeAffinity/nodeSelector in 1.25/alpha version. The default value of nodeAffinityPolicy is honor, means that only nodes matching nodeAffinity/nodeSelector are included in the calculations

chmodrs (Issue Creator) on (2023-01-04 14:14:19 UTC): @songminglong perfect, I understand that PodTopologySpreadConstraint doesn't work very well with nodeAffinity/nodeSelector definition, right?

But in case that we don't use this flags? Like bellow:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 8 # tells deployment to run 2 pods matching the template
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: nginx
      #tolerations:
      #- effect: NoSchedule
      #  key: platform
      #  operator: Equal
      #  value: ""true""
      #affinity:
      #  nodeAffinity:
      #    requiredDuringSchedulingIgnoredDuringExecution:
      #      nodeSelectorTerms:
      #      - matchExpressions:
      #        - key: nodes
      #          operator: In
      #          values:
      #          - platform
```
In this case, I just have 3 zones in sa-east-1, I think that the scheduler just can run one pod per zone and others need to be in a ""pending state"", don't they?

But still running multiple pods in the same zone :(

```
nginx-deployment-56f498fc9b-4w7hn   1/1     Running     0                 44s     192.168.33.145   ip-192-168-43-89.sa-east-1.compute.internal    <none>           <none>
nginx-deployment-56f498fc9b-9wqgz   1/1     Running     0                 11s     192.168.43.127   ip-192-168-43-89.sa-east-1.compute.internal    <none>           <none>
nginx-deployment-56f498fc9b-dq966   1/1     Running     0                 44s     192.168.61.95    ip-192-168-63-116.sa-east-1.compute.internal   <none>           <none>
nginx-deployment-56f498fc9b-fflc6   1/1     Running     0                 11s     192.168.70.36    ip-192-168-70-157.sa-east-1.compute.internal   <none>           <none>
nginx-deployment-56f498fc9b-jgzg8   1/1     Running     0                 11s     192.168.63.240   ip-192-168-63-116.sa-east-1.compute.internal   <none>           <none>
nginx-deployment-56f498fc9b-m5g6r   1/1     Running     0                 44s     192.168.78.10    ip-192-168-70-157.sa-east-1.compute.internal   <none>           <none>
nginx-deployment-56f498fc9b-p6z2z   1/1     Running     0                 44s     192.168.45.204   ip-192-168-43-89.sa-east-1.compute.internal    <none>           <none>
nginx-deployment-56f498fc9b-zxmr4   1/1     Running     0                 11s     192.168.53.57    ip-192-168-63-116.sa-east-1.compute.internal   <none>           <none>
```

kerthcet on (2023-01-05 09:29:09 UTC): zone is divided by `topologyKey: topology.kubernetes.io/zone`, not `platform`.

kerthcet on (2023-01-05 09:30:03 UTC): Here're some examples: https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/#topology-spread-constraint-examples

sftim on (2023-01-16 14:14:46 UTC): Do we think this is a bug, or an opportunity to provide extra documentation?

kerthcet on (2023-01-17 03:49:22 UTC): Yes, I think we should add the examples of new features like `NodeInclusionPolicyInPodTopologySpread ` to https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/#topology-spread-constraint-examples, and we also plan to post a new blog about them.

kerthcet on (2023-01-17 03:57:21 UTC): From the output:
```
nginx-deployment-56f498fc9b-4w7hn   1/1     Running     0                 44s     192.168.33.145   ip-192-168-43-89.sa-east-1.compute.internal    <none>           <none>
nginx-deployment-56f498fc9b-9wqgz   1/1     Running     0                 11s     192.168.43.127   ip-192-168-43-89.sa-east-1.compute.internal    <none>           <none>
nginx-deployment-56f498fc9b-dq966   1/1     Running     0                 44s     192.168.61.95    ip-192-168-63-116.sa-east-1.compute.internal   <none>           <none>
nginx-deployment-56f498fc9b-fflc6   1/1     Running     0                 11s     192.168.70.36    ip-192-168-70-157.sa-east-1.compute.internal   <none>           <none>
nginx-deployment-56f498fc9b-jgzg8   1/1     Running     0                 11s     192.168.63.240   ip-192-168-63-116.sa-east-1.compute.internal   <none>           <none>
nginx-deployment-56f498fc9b-m5g6r   1/1     Running     0                 44s     192.168.78.10    ip-192-168-70-157.sa-east-1.compute.internal   <none>           <none>
nginx-deployment-56f498fc9b-p6z2z   1/1     Running     0                 44s     192.168.45.204   ip-192-168-43-89.sa-east-1.compute.internal    <none>           <none>
nginx-deployment-56f498fc9b-zxmr4   1/1     Running     0                 11s     192.168.53.57    ip-192-168-63-116.sa-east-1.compute.internal   <none>           <none>
```

I guess you have three zones:
 - ip-192-168-63-116.sa-east-1.compute.internal  -- three pods matched
 - ip-192-168-43-89.sa-east-1.compute.internal   -- three pods matched
 - ip-192-168-43-157.sa-east-1.compute.internal  -- two pods matched

The maxMatchedPodsNumber - minMatchedPodsNumber = 3 - 2 = 1 <= maxSkew, so it's still satisfied and working as expected. @chmodrs Can you confirm this and I guess you misunderstood the usage.

kerthcet on (2023-01-17 03:58:05 UTC): /triage needs-information

chmodrs (Issue Creator) on (2023-01-17 04:50:04 UTC): Hi @kerthcet 

So maybe I made a confusion about PodTopologySpreadConstraint utilization, right?

I think that configuring maxSkew = 1 corresponds to that I will have 1 pod in each zone, using `topologyKey: topology.kubernetes.io/zone` (its what I wanted), but I need to use node toleration + affinity to place my pods in correct nodegroup (platform) too.

An example of a scenario that I want to configure, with PodTopologySpreadConstraint is:

- 3 zones (a, b, c)
- All deployments with 4 replicas
- Pods running in `platform` nodegroup

Guarantee that I have at least 1 pod in each zone, and the last pod (fourth) in any zone.

I can't configure this scenario with PodTopologySpreadConstraints?

Thankss!!

kerthcet on (2023-01-17 05:58:29 UTC): So I guess all the nodes in platform nodegroup scoped by the three zones. Then what you pasted above can work as your expected:
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 4 # tells deployment to run 2 pods matching the template
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: nginx
      tolerations:
      - effect: NoSchedule
        key: platform
        operator: Equal
        value: ""true""
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nodes
                operator: In
                values:
                - platform
```

The nodeAffinity will filter out all the nodes with label `nodes:platform`, then `topologySpreadConstraints` will divide the zones via `topology.kubernetes.io/zone`, and make sure not violating the maxSkew.

chmodrs (Issue Creator) on (2023-02-13 16:24:51 UTC): Hi @kerthcet , sorry for my lateness.

I tried to apply the following deployment:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 5
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: nginx
```

I have a EKS cluster in sa-east-1 region with 3 availability zones.

```
➜ kubectl get pods | grep nginx-deployment
nginx-deployment-56f498fc9b-d4hfr   1/1     Running            0                  9m56s
nginx-deployment-56f498fc9b-d6hzd   1/1     Running            0                  9m56s
nginx-deployment-56f498fc9b-nkgs5   1/1     Running            0                  9m56s
nginx-deployment-56f498fc9b-q968x   1/1     Running            0                  9m53s
nginx-deployment-56f498fc9b-vdwmq   1/1     Running            0                  9m54s
```

With just 3 zones and maxSkew=1, we need to get two pods in ""Pending"" state, right? Because ""DotNotSchedule"" is defined.

In my case, even i use ""DoNotSchedule"" or ""ScheduleAnyway"" the result is the same.

geniusjoe on (2023-02-16 07:41:45 UTC): @kerthcet 
I use EKS with kubernetes 1.20.x and it seems that I encountered the same problem. I am a newbee in kubernetes and I don't know if my opration is correct. Steps below:

1. Create a cluster with two EKS nodes: node1, node2
2. Add a label to node1: `testLabel: testValue`
3. Create a StatefulSet with 3 replicas, and I set `topologySpreadConstraints` and `nodeAffinity` properties like:
```
    topologySpreadConstraints:
    - maxSkew: 1
      topologyKey: testLabel
      whenUnsatisfiable: DoNotSchedule
      labelSelector:
        matchLabels:
          app: my-app
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: testLabel
              operator: In
              values:
                - testValue
                - testValue1
```
This time every created pods will be scheduled to node1, and it work fine.

4. Add a label in node2: `testLabel: testValue1`
5. Expand this StatefulSet to 4 replicas, I think this new pod will still hang in pending status because `maxSkew` violate in node1(3pods) and node2(0pod). *But the pod now is successfully scheduled to node2 and is in running status.*

Are there any questions or advice? Please feel free to let me know.

k8s-triage-robot on (2023-05-17 10:31:09 UTC): The Kubernetes project currently lacks enough contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle stale`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle stale

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

kerthcet on (2023-05-21 13:20:22 UTC): /remove-lifecycle stale

dharmjit on (2023-11-05 13:31:01 UTC): @chmodrs In your example above, The `maxSkew=1` is not violated, as the pod distribution across AZs might be 2:2:1. So it seems like the feature is working as expected.

dharmjit on (2023-11-06 16:09:22 UTC): @geniusjoe Not sure but it seems to me that when a new node is added, the skew would be 3 because of 3,3,0 for an incoming pod and scheduling the pod on new node will decrease the maxSkew to 2.

k8s-triage-robot on (2024-02-04 16:34:24 UTC): The Kubernetes project currently lacks enough contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle stale`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle stale

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-triage-robot on (2024-03-05 16:42:31 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle rotten

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-triage-robot on (2024-04-04 17:06:23 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.

This bot triages issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Reopen this issue with `/reopen`
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/close not-planned

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-ci-robot on (2024-04-04 17:06:27 UTC): @k8s-triage-robot: Closing this issue, marking it as ""Not Planned"".

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114788#issuecomment-2037743215):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

"
1517589633,issue,closed,completed,update-vendor-licenses.sh doesn't handle submodules in some cases,"### What happened?

When updating go-oidc, running `update-vendor.sh` failed when looking for licenses; specifically, it was looking for `cloud.google.com/go`’s `LICENSE` file, but that was no longer vendored.

### What did you expect to happen?

Since no `cloud.google.com/go` packages are actually used, the license shouldn’t be needed.

### How can we reproduce it (as minimally and precisely as possible)?

On master,

```
hack/pin-dependency.sh github.com/coreos/go-oidc/v3 v3.4.0
sed -i sXgithub.com/coreos/go-oidcXgithub.com/coreos/go-oidc/v3/oidcX staging/src/k8s.io/apiserver/plugin/pkg/authenticator/token/oidc/oidc.go
hack/update-vendor.sh
```

### Anything else we need to know?

_No response_

### Kubernetes version

Tip of the master branch.

### Cloud provider

N/A

### OS version

N/A

### Install tools

N/A

### Container runtime (CRI) and version (if applicable)

N/A

### Related plugins (CNI, CSI, ...) and versions (if applicable)

N/A",skitt,2023-01-03 15:54:12+00:00,[],2023-01-03 18:35:48+00:00,2023-01-03 18:35:48+00:00,https://github.com/kubernetes/kubernetes/issues/114781,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.'), ('sig/k8s-infra', 'Categorizes an issue or PR as relevant to SIG K8s Infra.')]","[{'comment_id': 1369932084, 'issue_id': 1517589633, 'author': 'k8s-ci-robot', 'body': '@skitt: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 3, 15, 54, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 1369934755, 'issue_id': 1517589633, 'author': 'skitt', 'body': '/sig k8s-infra\r\n/cc @liggitt', 'created_at': datetime.datetime(2023, 1, 3, 15, 56, 48, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-03 15:54:21 UTC): @skitt: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

skitt (Issue Creator) on (2023-01-03 15:56:48 UTC): /sig k8s-infra
/cc @liggitt

"
1517478666,issue,closed,not_planned,Use `kubernetes.io/bootstrapping` label more,"### What would you like to be added?

- Label each built-in PriorityClass with `kubernetes.io/bootstrapping`
- Label the built-in FlowSchema _exempt_ with `kubernetes.io/bootstrapping`
- Label the built-in FlowSchema _catch-all_, and PriorityLevelConfiguration _catch-all_ with `kubernetes.io/bootstrapping`

Pick suitable values for these labels.

&hellip;and optionally:
- Label system namespaces with with `kubernetes.io/bootstrapping`
- Label _default_ namespace with with `kubernetes.io/bootstrapping` (only at creation time; don't manage that label)
- Label APIServices for built-in APIs with `kubernetes.io/bootstrapping` (as well as the existing `kube-aggregator.kubernetes.io/automanaged`  label)

Again, pick suitable values for these labels.

### Why is this needed?

https://github.com/kubernetes/website/issues/35602#issuecomment-1369825341 suggested this change",sftim,2023-01-03 14:36:26+00:00,"['indevi', 'Phixsura']",2024-05-17 12:34:25+00:00,2024-05-17 12:34:23+00:00,https://github.com/kubernetes/kubernetes/issues/114778,"[('kind/feature', 'Categorizes issue or PR as related to a new feature.'), ('sig/architecture', 'Categorizes an issue or PR as relevant to SIG Architecture.'), ('lifecycle/rotten', 'Denotes an issue or PR that has aged beyond stale and will be auto-closed.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1369840151, 'issue_id': 1517478666, 'author': 'k8s-ci-robot', 'body': '@sftim: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 3, 14, 36, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 1369840404, 'issue_id': 1517478666, 'author': 'sftim', 'body': 'If need be, this could become an umbrella issue.\r\n\r\n/sig architecture', 'created_at': datetime.datetime(2023, 1, 3, 14, 36, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 1494545095, 'issue_id': 1517478666, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 4, 3, 15, 33, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 1494593076, 'issue_id': 1517478666, 'author': 'indevi', 'body': '/assign', 'created_at': datetime.datetime(2023, 4, 3, 16, 3, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 1533318814, 'issue_id': 1517478666, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 5, 3, 16, 8, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 1546977708, 'issue_id': 1517478666, 'author': 'Phixsura', 'body': ""Hello, excuse me.  I'm a beginner. I'm trying to do this part in the source code. To my dismay, PriorityClass seem to be generated by gen, but I can't find the source of the generation. Can you give me some advice?\r\n@sftim"", 'created_at': datetime.datetime(2023, 5, 14, 19, 5, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 1547363142, 'issue_id': 1517478666, 'author': 'sftim', 'body': ""This isn't an area I can particularly advise on."", 'created_at': datetime.datetime(2023, 5, 15, 7, 53, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 1547363823, 'issue_id': 1517478666, 'author': 'sftim', 'body': 'Another side of the same thing: https://github.com/kubernetes/kubernetes/issues/117807', 'created_at': datetime.datetime(2023, 5, 15, 7, 53, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 1552788295, 'issue_id': 1517478666, 'author': 'Phixsura', 'body': '/assign', 'created_at': datetime.datetime(2023, 5, 18, 9, 29, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 1595699837, 'issue_id': 1517478666, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 6, 17, 10, 12, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 1595699852, 'issue_id': 1517478666, 'author': 'k8s-ci-robot', 'body': '@k8s-triage-robot: Closing this issue, marking it as ""Not Planned"".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114778#issuecomment-1595699837):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 6, 17, 10, 12, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2061137731, 'issue_id': 1517478666, 'author': 'sftim', 'body': ""/reopen\r\n\r\nI'd like us to triage this if we get capacity."", 'created_at': datetime.datetime(2024, 4, 17, 12, 23, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2061137896, 'issue_id': 1517478666, 'author': 'k8s-ci-robot', 'body': ""@sftim: Reopened this issue.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114778#issuecomment-2061137731):\n\n>/reopen\r\n>\r\n>I'd like us to triage this if we get capacity.\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"", 'created_at': datetime.datetime(2024, 4, 17, 12, 23, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2117500929, 'issue_id': 1517478666, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2024, 5, 17, 12, 34, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2117501147, 'issue_id': 1517478666, 'author': 'k8s-ci-robot', 'body': '@k8s-triage-robot: Closing this issue, marking it as ""Not Planned"".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114778#issuecomment-2117500929):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2024, 5, 17, 12, 34, 24, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-03 14:36:34 UTC): @sftim: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

sftim (Issue Creator) on (2023-01-03 14:36:47 UTC): If need be, this could become an umbrella issue.

/sig architecture

k8s-triage-robot on (2023-04-03 15:33:45 UTC): The Kubernetes project currently lacks enough contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle stale`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle stale

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

indevi (Assginee) on (2023-04-03 16:03:23 UTC): /assign

k8s-triage-robot on (2023-05-03 16:08:52 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle rotten

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

Phixsura (Assginee) on (2023-05-14 19:05:21 UTC): Hello, excuse me.  I'm a beginner. I'm trying to do this part in the source code. To my dismay, PriorityClass seem to be generated by gen, but I can't find the source of the generation. Can you give me some advice?
@sftim

sftim (Issue Creator) on (2023-05-15 07:53:29 UTC): This isn't an area I can particularly advise on.

sftim (Issue Creator) on (2023-05-15 07:53:59 UTC): Another side of the same thing: https://github.com/kubernetes/kubernetes/issues/117807

Phixsura (Assginee) on (2023-05-18 09:29:36 UTC): /assign

k8s-triage-robot on (2023-06-17 10:12:51 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.

This bot triages issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Reopen this issue with `/reopen`
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/close not-planned

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-ci-robot on (2023-06-17 10:12:56 UTC): @k8s-triage-robot: Closing this issue, marking it as ""Not Planned"".

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114778#issuecomment-1595699837):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

sftim (Issue Creator) on (2024-04-17 12:23:27 UTC): /reopen

I'd like us to triage this if we get capacity.

k8s-ci-robot on (2024-04-17 12:23:33 UTC): @sftim: Reopened this issue.

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114778#issuecomment-2061137731):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

k8s-triage-robot on (2024-05-17 12:34:19 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.

This bot triages issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Reopen this issue with `/reopen`
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/close not-planned

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-ci-robot on (2024-05-17 12:34:24 UTC): @k8s-triage-robot: Closing this issue, marking it as ""Not Planned"".

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114778#issuecomment-2117500929):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes-sigs/prow](https://github.com/kubernetes-sigs/prow/issues/new?title=Prow%20issue:) repository.
</details>

"
1517458136,issue,closed,not_planned,Empty string in env of container always causes kubectl apply command show configured.,"### What happened?

kubectl apply -f deploy.yaml always show configured even if there is no change.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dockerd
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: dockerd
  template:
    metadata:
      labels:
        app: dockerd
    spec:
      containers:
        - name: dockerd
          image: harbor.intra.ke.com/appcenter/proxy/docker:20.10.21-dind
          imagePullPolicy: IfNotPresent
          env:
            - name: DOCKER_TLS_CERTDIR
              value: """"
```

### What did you expect to happen?

deployment.apps/dockerd unchanged

### How can we reproduce it (as minimally and precisely as possible)?

kubectl apply -f the yaml > two times



### Kubernetes version

<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:""1"", Minor:""24"", GitVersion:""v1.24.6"", GitCommit:""b39bf148cd654599a52e867485c02c4f9d28b312"", GitTreeState:""clean"", BuildDate:""2022-09-21T13:19:24Z"", GoVersion:""go1.18.6"", Compiler:""gc"", Platform:""linux/amd64""}
Kustomize Version: v4.5.4
Server Version: version.Info{Major:""1"", Minor:""24"", GitVersion:""v1.24.6"", GitCommit:""b39bf148cd654599a52e867485c02c4f9d28b312"", GitTreeState:""clean"", BuildDate:""2022-09-21T13:12:04Z"", GoVersion:""go1.18.6"", Compiler:""gc"", Platform:""linux/amd64""}
```

</details>

",libinglong,2023-01-03 14:20:11+00:00,[],2023-06-14 16:38:54+00:00,2023-06-14 16:38:53+00:00,https://github.com/kubernetes/kubernetes/issues/114777,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('sig/cli', 'Categorizes an issue or PR as relevant to SIG CLI.'), ('lifecycle/rotten', 'Denotes an issue or PR that has aged beyond stale and will be auto-closed.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1369822870, 'issue_id': 1517458136, 'author': 'k8s-ci-robot', 'body': '@libinglong: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 3, 14, 20, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370510595, 'issue_id': 1517458136, 'author': 'libinglong', 'body': '> @libinglong: There are no sig labels on this issue. Please add an appropriate label by using one of the following commands:\r\n> \r\n> * `/sig <group-name>`\r\n> * `/wg <group-name>`\r\n> * `/committee <group-name>`\r\n> \r\n> Please see the [group list](https://git.k8s.io/community/sig-list.md) for a listing of the SIGs, working groups, and committees available.\r\n\r\n/sig cli', 'created_at': datetime.datetime(2023, 1, 4, 5, 43, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370602620, 'issue_id': 1517458136, 'author': 'ardaguclu', 'body': 'As far as I can recall this problem was fixed in newer versions. Could you please try with latest kubectl?', 'created_at': datetime.datetime(2023, 1, 4, 8, 14, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370604303, 'issue_id': 1517458136, 'author': 'pacoxu', 'body': '> As far as I can recall this problem was fixed in newer versions. Could you please try with the latest kubectl?\r\n\r\nJust tried in v1.26.0 env, and it still exists.\r\n\r\n\r\nThe request body is like `""env"":[{""name"":""DOCKER_TLS_CERTDIR"", ""value"": """"}`\r\n```\r\n          env:\r\n            - name: DOCKER_TLS_CERTDIR\r\n              value: """"\r\n```\r\nThe response body is like `""env"":[{""name"":""DOCKER_TLS_CERTDIR""}`\r\n```\r\n          env:\r\n            - name: DOCKER_TLS_CERTDIR\r\n```\r\nWithout `value: """"` in the deployment yaml, the `kubectl apply` will return `unchanged`.', 'created_at': datetime.datetime(2023, 1, 4, 8, 16, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 1373346156, 'issue_id': 1517458136, 'author': 'ardaguclu', 'body': ""`env` field in deployment spec is set as omit if empty https://github.com/kubernetes/kubernetes/blob/57b9656e2ba84f533006fb0536920b1777411d62/staging/src/k8s.io/api/core/v1/types.go#L2027. That's why, it ignores this value as @pacoxu said above ^. \r\n\r\nOn client side, strategic merge detects this as difference and sends this patch to apiserver. Just because of this reason, it shows `configured`. Especially considering the move towards server side apply, I think we can say this works as expected.\r\n\r\nWhat do you think @apelisse?"", 'created_at': datetime.datetime(2023, 1, 6, 8, 33, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 1373907639, 'issue_id': 1517458136, 'author': 'apelisse', 'body': ""We don't have a good way to do the same thing server-side so yeah, we're probably not going to fix that."", 'created_at': datetime.datetime(2023, 1, 6, 17, 11, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 1499434595, 'issue_id': 1517458136, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 4, 6, 18, 5, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 1537199983, 'issue_id': 1517458136, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 5, 6, 18, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 1548181635, 'issue_id': 1517458136, 'author': 'apelisse', 'body': 'This is more or less working as intended. Clients don\'t always know how the change is going to be applied by the apiserver, and if it\'s actually going to resolve to a change. With client-side apply, we could totally check if the resourceVersion has changed between the object we got before applying and the returned object. Is it worth the effort? This ""configured/unchanged"" shouldn\'t be construed as an API.', 'created_at': datetime.datetime(2023, 5, 15, 16, 31, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 1591624990, 'issue_id': 1517458136, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 6, 14, 16, 38, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 1591625199, 'issue_id': 1517458136, 'author': 'k8s-ci-robot', 'body': '@k8s-triage-robot: Closing this issue, marking it as ""Not Planned"".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114777#issuecomment-1591624990):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 6, 14, 16, 38, 54, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-03 14:20:19 UTC): @libinglong: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

libinglong (Issue Creator) on (2023-01-04 05:43:54 UTC): /sig cli

ardaguclu on (2023-01-04 08:14:48 UTC): As far as I can recall this problem was fixed in newer versions. Could you please try with latest kubectl?

pacoxu on (2023-01-04 08:16:43 UTC): Just tried in v1.26.0 env, and it still exists.


The request body is like `""env"":[{""name"":""DOCKER_TLS_CERTDIR"", ""value"": """"}`
```
          env:
            - name: DOCKER_TLS_CERTDIR
              value: """"
```
The response body is like `""env"":[{""name"":""DOCKER_TLS_CERTDIR""}`
```
          env:
            - name: DOCKER_TLS_CERTDIR
```
Without `value: """"` in the deployment yaml, the `kubectl apply` will return `unchanged`.

ardaguclu on (2023-01-06 08:33:10 UTC): `env` field in deployment spec is set as omit if empty https://github.com/kubernetes/kubernetes/blob/57b9656e2ba84f533006fb0536920b1777411d62/staging/src/k8s.io/api/core/v1/types.go#L2027. That's why, it ignores this value as @pacoxu said above ^. 

On client side, strategic merge detects this as difference and sends this patch to apiserver. Just because of this reason, it shows `configured`. Especially considering the move towards server side apply, I think we can say this works as expected.

What do you think @apelisse?

apelisse on (2023-01-06 17:11:49 UTC): We don't have a good way to do the same thing server-side so yeah, we're probably not going to fix that.

k8s-triage-robot on (2023-04-06 18:05:44 UTC): The Kubernetes project currently lacks enough contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle stale`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle stale

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-triage-robot on (2023-05-06 18:43:00 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle rotten

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

apelisse on (2023-05-15 16:31:24 UTC): This is more or less working as intended. Clients don't always know how the change is going to be applied by the apiserver, and if it's actually going to resolve to a change. With client-side apply, we could totally check if the resourceVersion has changed between the object we got before applying and the returned object. Is it worth the effort? This ""configured/unchanged"" shouldn't be construed as an API.

k8s-triage-robot on (2023-06-14 16:38:48 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.

This bot triages issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Reopen this issue with `/reopen`
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/close not-planned

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-ci-robot on (2023-06-14 16:38:54 UTC): @k8s-triage-robot: Closing this issue, marking it as ""Not Planned"".

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114777#issuecomment-1591624990):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

"
1516588114,issue,closed,completed,Event timestamps don't support sub-second precision,"### What happened?

We started exporting Kubernetes events into a monitoring system and noticed that some events seem to be out of order (e.g. container started event before container created event). Upon further investigation we concluded that the root cause is the insufficient precision of the event timestamps - the timestamps only support seconds precision - so the order of events which happen in the same second is lost.
This behavior turns out to be by design: 
* [staging/src/k8s.io/apimachinery/pkg/apis/meta/v1/time_proto.go#L45-L48](https://github.com/kubernetes/kubernetes/blob/ca858e0c961db6ef8b22ecc3e257a02757261ea1/staging/src/k8s.io/apimachinery/pkg/apis/meta/v1/time_proto.go#L45-L48)
* [staging/src/k8s.io/apimachinery/pkg/apis/meta/v1/time_proto.go#L70-L73](/kubernetes/kubernetes/blob/ca858e0c961db6ef8b22ecc3e257a02757261ea1/staging/src/k8s.io/apimachinery/pkg/apis/meta/v1/time_proto.go#L70-L73)

It was introduced in commit 5b8a483a9623998625fe970e78307b0c5e064d51.
Given the commit is 5 years old, has it perhaps become obsolete, could it be reverted so that the nanosecond precision is preserved?

### What did you expect to happen?

Order of events which happened in the same second is preserved. The relevant timestamps support sub-second precision.

### How can we reproduce it (as minimally and precisely as possible)?

Query the Kubernetes Events API using client supporting ProtoBuf content type (such [resmoio/kubernetes-event-exporter](/resmoio/kubernetes-event-exporter), which uses k8s client internally).

### Anything else we need to know?

_No response_

### Kubernetes version

```yaml
clientVersion:
  buildDate: ""2022-11-11T00:00:00Z""
  compiler: gc
  gitCommit: fdc77503e954d1ee641c0e350481f7528e8d068b
  gitTreeState: archive
  gitVersion: v1.24.8
  goVersion: go1.18.8
  major: ""1""
  minor: ""24""
  platform: linux/amd64
kustomizeVersion: v4.5.4
serverVersion:
  buildDate: ""2022-12-08T10:08:09Z""
  compiler: gc
  gitCommit: 804d6167111f6858541cef440ccc53887fbbc96a
  gitTreeState: clean
  gitVersion: v1.25.5
  goVersion: go1.19.4
  major: ""1""
  minor: ""25""
  platform: linux/amd64
```

### Cloud provider

N/A

### OS version

N/A

### Install tools

`kubeadm`

### Container runtime (CRI) and version (if applicable)

_No response_

### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_",mruzicka,2023-01-02 17:18:13+00:00,[],2023-01-02 21:05:34+00:00,2023-01-02 21:05:34+00:00,https://github.com/kubernetes/kubernetes/issues/114765,"[('kind/support', 'Categorizes issue or PR as a support question.'), ('sig/instrumentation', 'Categorizes an issue or PR as relevant to SIG Instrumentation.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1369105266, 'issue_id': 1516588114, 'author': 'k8s-ci-robot', 'body': '@mruzicka: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 1, 2, 17, 18, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 1369117397, 'issue_id': 1516588114, 'author': 'aojea', 'body': 'you should not rely on events timestamp https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/event-v1/#Event \r\n\r\n> Event consumers should not rely on the timing of an event with a given Reason reflecting a consistent underlying trigger, or the continued existence of events with that Reason. Events should be treated as informative, best-effort, supplemental data.', 'created_at': datetime.datetime(2023, 1, 2, 17, 48, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 1369117559, 'issue_id': 1516588114, 'author': 'aojea', 'body': '/sig instrumentation', 'created_at': datetime.datetime(2023, 1, 2, 17, 49, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 1369202837, 'issue_id': 1516588114, 'author': 'liggitt', 'body': 'also, reading/writing events via the events.k8s.io/v1 API include an `eventTime` field with microsecond-level precision', 'created_at': datetime.datetime(2023, 1, 2, 21, 4, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 1369203435, 'issue_id': 1516588114, 'author': 'liggitt', 'body': 'proto and json precision must match, the metav1.Time field only have second-level precision', 'created_at': datetime.datetime(2023, 1, 2, 21, 5, 31, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2023-01-02 17:18:21 UTC): @mruzicka: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

aojea on (2023-01-02 17:48:43 UTC): you should not rely on events timestamp https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/event-v1/#Event

aojea on (2023-01-02 17:49:06 UTC): /sig instrumentation

liggitt on (2023-01-02 21:04:40 UTC): also, reading/writing events via the events.k8s.io/v1 API include an `eventTime` field with microsecond-level precision

liggitt on (2023-01-02 21:05:31 UTC): proto and json precision must match, the metav1.Time field only have second-level precision

"
1515133462,issue,closed,not_planned,kubelet memory leaks for housekeeping goroutines keeps leakiness,"### What happened?

In my online kubernetes cluster, kubelet memory keeps growing, finally more than 50G, and kill many low-priority processes with memory. 
<img width=""1342"" alt=""image"" src=""https://user-images.githubusercontent.com/57479557/210139100-e406d2e5-5894-4648-8c60-2936b15db955.png"">
I observed goroutines of kubelet are also increasing synchronously with the memory：
<img width=""1344"" alt=""image"" src=""https://user-images.githubusercontent.com/57479557/210139168-54851a95-378f-4a86-a321-3fdd2e1281b0.png"">
I use golang pprof analysis and found that many goroutines stay in [the housekeeping logic](https://github.com/google/cadvisor/blob/92d49a1c4cd1b0c983883fa965406355fe96bcb6/manager/container.go#L446:~:text=if%20!c.housekeepingTick,%7D) 

<div align=""center"">
  <img src=""https://user-images.githubusercontent.com/57479557/210139290-38d32074-700a-4907-a7d0-623e719f8aad.png"" width=40% title=""pprof housekeeping"">
</div>

[housekeeping() goroutines](https://github.com/google/cadvisor/blob/92d49a1c4cd1b0c983883fa965406355fe96bcb6/manager/container.go#L108) have only one exit point, which is [read <-c.stop chan massage](https://github.com/google/cadvisor/blob/92d49a1c4cd1b0c983883fa965406355fe96bcb6/manager/container.go#L446).  But I didn't find anything unusual by checking the kebelet log, so why housekeeping() goroutines do keep growing?


### What did you expect to happen?

Housekeeping() goroutines exit normally and find out why it can't exit normally.

### How can we reproduce it (as minimally and precisely as possible)?

I don't know reproduce it, restart the kubelet and the problem will disappear

### Anything else we need to know?

_No response_

### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.5"", GitCommit:""---"", GitTreeState:""clean"", BuildDate:""2020-06-26T03:47:41Z"", GoVersion:""go1.13.9"", Compiler:""gc"", Platform:""linux/amd64""}
Server Version: version.Info{Major:""1"", Minor:""18"", GitVersion:""v1.18.5"", GitCommit:""---"", GitTreeState:""clean"", BuildDate:""2020-06-26T03:39:24Z"", GoVersion:""go1.13.9"", Compiler:""gc"", Platform:""linux/amd64""}
```

</details>


### Cloud provider

<details>
Internal private cloud platform
</details>


### OS version

<details>

```console
# On Linux:
root@:~# cat /etc/os-release
PRETTY_NAME=""Debian GNU/Linux bookworm/sid""
NAME=""Debian GNU/Linux""
VERSION_CODENAME=bookworm
ID=debian
HOME_URL=""https://www.debian.org/""
SUPPORT_URL=""https://www.debian.org/support""
BUG_REPORT_URL=""https://bugs.debian.org/""
```

</details>


### Install tools

<details>

</details>


### Container runtime (CRI) and version (if applicable)

<details>

root@:~# crictl version
Version:  0.1.0
RuntimeName:  containerd
RuntimeVersion:  v1.5.5-9
RuntimeApiVersion:  v1alpha2

</details>


### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>
",attlee-wang,2022-12-31 14:42:58+00:00,[],2023-06-16 18:07:59+00:00,2023-06-16 18:07:58+00:00,https://github.com/kubernetes/kubernetes/issues/114751,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('area/cadvisor', None), ('sig/node', 'Categorizes an issue or PR as relevant to SIG Node.'), ('lifecycle/rotten', 'Denotes an issue or PR that has aged beyond stale and will be auto-closed.'), ('triage/needs-information', 'Indicates an issue needs more information in order to work on it.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1368224466, 'issue_id': 1515133462, 'author': 'k8s-ci-robot', 'body': '@attlee-wang: This issue is currently awaiting triage.\n\nIf a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.\n\nThe `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.\n\n\n<details>\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2022, 12, 31, 14, 43, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 1368239557, 'issue_id': 1515133462, 'author': 'attlee-wang', 'body': '/sig kubelet', 'created_at': datetime.datetime(2022, 12, 31, 15, 9, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 1368239561, 'issue_id': 1515133462, 'author': 'k8s-ci-robot', 'body': ""@attlee-wang: The label(s) `sig/kubelet` cannot be applied, because the repository doesn't have them.\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114751#issuecomment-1368239557):\n\n>/sig kubelet \n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>"", 'created_at': datetime.datetime(2022, 12, 31, 15, 9, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 1368255075, 'issue_id': 1515133462, 'author': 'aojea', 'body': '1.18 is EOL, you should check if this is still an issue in any supported version', 'created_at': datetime.datetime(2022, 12, 31, 17, 21, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 1368351742, 'issue_id': 1515133462, 'author': 'pacoxu', 'body': '/sig node\r\n\r\n> 1.18 is EOL, you should check if this is still an issue in any supported version\r\n\r\nSee details at https://kubernetes.io/releases/patch-releases/#detailed-release-history-for-active-branches\r\n\r\n\r\nI did not look deep into the code and there is a memory leak issue of cadvisor in https://github.com/google/cadvisor/pull/3014 which is fixed in v1.22/v1.23. More in https://github.com/google/cadvisor/pulls?q=is%3Apr+%22memory+leak%22+is%3Aclosed.', 'created_at': datetime.datetime(2023, 1, 1, 4, 51, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371293413, 'issue_id': 1515133462, 'author': 'SergeyKanzhelev', 'body': '/triage needs-information\r\n\r\nplease try a fresh version and report back. Thank you!', 'created_at': datetime.datetime(2023, 1, 4, 18, 47, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 1371293548, 'issue_id': 1515133462, 'author': 'SergeyKanzhelev', 'body': '/area cadvisor', 'created_at': datetime.datetime(2023, 1, 4, 18, 47, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 1373623033, 'issue_id': 1515133462, 'author': 'attlee-wang', 'body': ""> /sig node\r\n> \r\n> > 1.18 is EOL, you should check if this is still an issue in any supported version\r\n> \r\n> See details at https://kubernetes.io/releases/patch-releases/#detailed-release-history-for-active-branches\r\n> \r\n> I did not look deep into the code and there is a memory leak issue of cadvisor in [google/cadvisor#3014](https://github.com/google/cadvisor/pull/3014) which is fixed in v1.22/v1.23. More in https://github.com/google/cadvisor/pulls?q=is%3Apr+%22memory+leak%22+is%3Aclosed.\r\n\r\n@pacoxu [pacoxu](https://github.com/pacoxu)  I also noticed issues  [#3014](https://github.com/google/cadvisor/pull/3014), but after my investigation, the reason is different.\r\n\r\nI add log to [housekeeping()](https://github.com/google/cadvisor/blob/92d49a1c4cd1b0c983883fa965406355fe96bcb6/manager/container.go#L446:~:text=if%20!c.housekeepingTick,%7D) found that add housekeeping() is much more than stop housekeeping(), but there are only a few containers on my node.\r\n\r\n![38D05xyX8S](https://user-images.githubusercontent.com/57479557/211016460-9ccec7b8-111b-444c-9381-3ccfdffccd38.jpg)\r\n\r\nAdding logs to the code, I found that the [lastWatched of the failed container is always false](https://github.com/google/cadvisor/blob/92d49a1c4cd1b0c983883fa965406355fe96bcb6/container/raw/watcher.go#L216:~:text=watching%20for%20it.-,lastWatched%2C%20err%20%3A%3D%20self.watcher.RemoveWatch(containerName,%7D,-default%3A)), so it is not sent stop housekeeping(). But I didn't find why [lastWatched is always false](https://github.com/google/cadvisor/blob/92d49a1c4cd1b0c983883fa965406355fe96bcb6/container/raw/watcher.go#L216:~:text=watching%20for%20it.-,lastWatched%2C%20err%20%3A%3D%20self.watcher.RemoveWatch(containerName,%7D,-default%3A)) of the failed container.  @[pacoxu](https://github.com/pacoxu)  @[iwankgb](https://github.com/iwankgb)  @bobbypage"", 'created_at': datetime.datetime(2023, 1, 6, 13, 26, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 1499221154, 'issue_id': 1515133462, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle stale`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle stale\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 4, 6, 15, 5, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 1537167536, 'issue_id': 1515133462, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.\n\nThis bot triages un-triaged issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Close this issue with `/close`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/lifecycle rotten\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 5, 6, 15, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 1551824776, 'issue_id': 1515133462, 'author': 'SergeyKanzhelev', 'body': '@attlee-wang have you had a chance to try a fresh version of k8s?', 'created_at': datetime.datetime(2023, 5, 17, 17, 52, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 1595070270, 'issue_id': 1515133462, 'author': 'k8s-triage-robot', 'body': 'The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n\nThis bot triages issues according to the following rules:\n- After 90d of inactivity, `lifecycle/stale` is applied\n- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n\nYou can:\n- Reopen this issue with `/reopen`\n- Mark this issue as fresh with `/remove-lifecycle rotten`\n- Offer to help out with [Issue Triage][1]\n\nPlease send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n\n/close not-planned\n\n[1]: https://www.kubernetes.dev/docs/guide/issue-triage/', 'created_at': datetime.datetime(2023, 6, 16, 18, 7, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 1595070736, 'issue_id': 1515133462, 'author': 'k8s-ci-robot', 'body': '@k8s-triage-robot: Closing this issue, marking it as ""Not Planned"".\n\n<details>\n\nIn response to [this](https://github.com/kubernetes/kubernetes/issues/114751#issuecomment-1595070270):\n\n>The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.\n>\n>This bot triages issues according to the following rules:\n>- After 90d of inactivity, `lifecycle/stale` is applied\n>- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied\n>- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed\n>\n>You can:\n>- Reopen this issue with `/reopen`\n>- Mark this issue as fresh with `/remove-lifecycle rotten`\n>- Offer to help out with [Issue Triage][1]\n>\n>Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).\n>\n>/close not-planned\n>\n>[1]: https://www.kubernetes.dev/docs/guide/issue-triage/\n\n\nInstructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.\n</details>', 'created_at': datetime.datetime(2023, 6, 16, 18, 7, 59, tzinfo=datetime.timezone.utc)}]","k8s-ci-robot on (2022-12-31 14:43:06 UTC): @attlee-wang: This issue is currently awaiting triage.

If a SIG or subproject determines this is a relevant issue, they will accept it by applying the `triage/accepted` label and provide further guidance.

The `triage/accepted` label can be added by org members by writing `/triage accepted` in a comment.


<details>

Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

attlee-wang (Issue Creator) on (2022-12-31 15:09:24 UTC): /sig kubelet

k8s-ci-robot on (2022-12-31 15:09:25 UTC): @attlee-wang: The label(s) `sig/kubelet` cannot be applied, because the repository doesn't have them.

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114751#issuecomment-1368239557):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

aojea on (2022-12-31 17:21:26 UTC): 1.18 is EOL, you should check if this is still an issue in any supported version

pacoxu on (2023-01-01 04:51:57 UTC): /sig node


See details at https://kubernetes.io/releases/patch-releases/#detailed-release-history-for-active-branches


I did not look deep into the code and there is a memory leak issue of cadvisor in https://github.com/google/cadvisor/pull/3014 which is fixed in v1.22/v1.23. More in https://github.com/google/cadvisor/pulls?q=is%3Apr+%22memory+leak%22+is%3Aclosed.

SergeyKanzhelev on (2023-01-04 18:47:42 UTC): /triage needs-information

please try a fresh version and report back. Thank you!

SergeyKanzhelev on (2023-01-04 18:47:50 UTC): /area cadvisor

attlee-wang (Issue Creator) on (2023-01-06 13:26:25 UTC): @pacoxu [pacoxu](https://github.com/pacoxu)  I also noticed issues  [#3014](https://github.com/google/cadvisor/pull/3014), but after my investigation, the reason is different.

I add log to [housekeeping()](https://github.com/google/cadvisor/blob/92d49a1c4cd1b0c983883fa965406355fe96bcb6/manager/container.go#L446:~:text=if%20!c.housekeepingTick,%7D) found that add housekeeping() is much more than stop housekeeping(), but there are only a few containers on my node.

![38D05xyX8S](https://user-images.githubusercontent.com/57479557/211016460-9ccec7b8-111b-444c-9381-3ccfdffccd38.jpg)

Adding logs to the code, I found that the [lastWatched of the failed container is always false](https://github.com/google/cadvisor/blob/92d49a1c4cd1b0c983883fa965406355fe96bcb6/container/raw/watcher.go#L216:~:text=watching%20for%20it.-,lastWatched%2C%20err%20%3A%3D%20self.watcher.RemoveWatch(containerName,%7D,-default%3A)), so it is not sent stop housekeeping(). But I didn't find why [lastWatched is always false](https://github.com/google/cadvisor/blob/92d49a1c4cd1b0c983883fa965406355fe96bcb6/container/raw/watcher.go#L216:~:text=watching%20for%20it.-,lastWatched%2C%20err%20%3A%3D%20self.watcher.RemoveWatch(containerName,%7D,-default%3A)) of the failed container.  @[pacoxu](https://github.com/pacoxu)  @[iwankgb](https://github.com/iwankgb)  @bobbypage

k8s-triage-robot on (2023-04-06 15:05:45 UTC): The Kubernetes project currently lacks enough contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle stale`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle stale

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-triage-robot on (2023-05-06 15:40:00 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues.

This bot triages un-triaged issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Close this issue with `/close`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/lifecycle rotten

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

SergeyKanzhelev on (2023-05-17 17:52:11 UTC): @attlee-wang have you had a chance to try a fresh version of k8s?

k8s-triage-robot on (2023-06-16 18:07:53 UTC): The Kubernetes project currently lacks enough active contributors to adequately respond to all issues and PRs.

This bot triages issues according to the following rules:
- After 90d of inactivity, `lifecycle/stale` is applied
- After 30d of inactivity since `lifecycle/stale` was applied, `lifecycle/rotten` is applied
- After 30d of inactivity since `lifecycle/rotten` was applied, the issue is closed

You can:
- Reopen this issue with `/reopen`
- Mark this issue as fresh with `/remove-lifecycle rotten`
- Offer to help out with [Issue Triage][1]

Please send feedback to sig-contributor-experience at [kubernetes/community](https://github.com/kubernetes/community).

/close not-planned

[1]: https://www.kubernetes.dev/docs/guide/issue-triage/

k8s-ci-robot on (2023-06-16 18:07:59 UTC): @k8s-triage-robot: Closing this issue, marking it as ""Not Planned"".

<details>

In response to [this](https://github.com/kubernetes/kubernetes/issues/114751#issuecomment-1595070270):



Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>

"
1514030211,issue,open,,Inconsistent behavior using multiple api-servers with different ServiceCIDRs ,"Kubernetes apiservers are responsible of handling multiple aspects of Services:
- apiservers run a reconcile loop to maintain the special `kubernetes.default` Service, that uses the first address of the configured primary Service CIDR
- apiservers assign `ClusterIP` to Services by sharing a bitmap that is snapshoted in `etcd`
- apiservers run a repair loop to keep the Services ClusterIPs and bitmap in sync

The Service CIDRs are configured by the following flag `--service-cluster-ip-range string` , and is it possible that mulitple apiservers use different configuration.

If this happens, the cluster will be in a totally inconsistent state, since each loop in each apiserver will try to reconcile their own parameters, 

```sh
    kube_apiserver_test.go:684: svcs from client 2: svc-apiserver2-9 192.168.0.22
    kube_apiserver_test.go:687: ------------
    kube_apiserver_test.go:658: Unexpected error: Internal error occurred: failed to allocate a serviceIP: the provided range does not match the current range
I1229 20:22:50.079653   56647 alloc.go:327] ""allocated clusterIPs"" service=""default/svc-apiserver2-17"" clusterIPs=map[IPv4:192.168.0.78]
    kube_apiserver_test.go:673: ------------
    kube_apiserver_test.go:676: svcs from client 1: kubernetes 10.0.0.1
    kube_apiserver_test.go:676: svcs from client 1: svc-apiserver2-0 192.168.0.114
    kube_apiserver_test.go:676: svcs from client 1: svc-apiserver2-1 192.168.0.248
```

easy to reproduce with the following integration test

```diff
diff --git a/test/integration/controlplane/kube_apiserver_test.go b/test/integration/controlplane/kube_apiserver_test.go
index 0658516aa70..f7b302636aa 100644
--- a/test/integration/controlplane/kube_apiserver_test.go
+++ b/test/integration/controlplane/kube_apiserver_test.go
@@ -42,6 +42,7 @@ import (
        ""k8s.io/client-go/kubernetes""
        ""k8s.io/kube-aggregator/pkg/apis/apiregistration""
        ""k8s.io/kube-openapi/pkg/validation/spec""
+       ""k8s.io/kubernetes/cmd/kube-apiserver/app/options""
        kubeapiservertesting ""k8s.io/kubernetes/cmd/kube-apiserver/app/testing""
        ""k8s.io/kubernetes/test/integration/etcd""
        ""k8s.io/kubernetes/test/integration/framework""
@@ -606,3 +607,84 @@ func TestMultiAPIServerNodePortAllocation(t *testing.T) {
        }

 }
+
+func TestMultiAPIServerDifferentServiceCIDRs(t *testing.T) {
+       serviceObject := &corev1.Service{
+               ObjectMeta: metav1.ObjectMeta{
+                       Labels: map[string]string{""foo"": ""bar""},
+                       Name:   ""test-svc"",
+               },
+               Spec: corev1.ServiceSpec{
+                       Ports: []corev1.ServicePort{
+                               {
+                                       Name:       ""test"",
+                                       Port:       443,
+                                       TargetPort: intstr.IntOrString{IntVal: 443},
+                                       Protocol:   ""TCP"",
+                               },
+                       },
+                       Type:     corev1.ServiceTypeNodePort,
+                       Selector: map[string]string{""foo"": ""bar""},
+               },
+       }
+       etcd := framework.SharedEtcd()
+
+       serviceCIDR1 := ""10.0.0.0/16""
+
+       client1, _, tearDownFn1 := framework.StartTestServer(t, framework.TestServerSetup{
+               ModifyServerRunOptions: func(opts *options.ServerRunOptions) {
+                       opts.ServiceClusterIPRanges = serviceCIDR1
+                       opts.Etcd.StorageConfig = *etcd
+               },
+       })
+       defer tearDownFn1()
+
+       serviceCIDR2 := ""192.168.0.0/24""
+
+       client2, _, tearDownFn2 := framework.StartTestServer(t, framework.TestServerSetup{
+               ModifyServerRunOptions: func(opts *options.ServerRunOptions) {
+                       opts.ServiceClusterIPRanges = serviceCIDR2
+                       opts.Etcd.StorageConfig = *etcd
+               },
+       })
+       defer tearDownFn2()
+
+       for i := 0; i < 2500; i++ {
+               // create a Service with first API server
+               svc1 := serviceObject.DeepCopy()
+               svc1.Name = fmt.Sprintf(""svc-apiserver1-%d"", i)
+               _, err := client1.CoreV1().Services(metav1.NamespaceDefault).Create(context.Background(), svc1, metav1.CreateOptions{})
+               if err != nil {
+                       t.Logf(""Unexpected error: %v"", err)
+               }
+
+               // create a Service with second API server
+               svc2 := serviceObject.DeepCopy()
+               svc2.Name = fmt.Sprintf(""svc-apiserver2-%d"", i)
+               _, err = client2.CoreV1().Services(metav1.NamespaceDefault).Create(context.Background(), svc2, metav1.CreateOptions{})
+               if err != nil {
+                       t.Logf(""Unexpected error: %v"", err)
+               }
+
+               svcs, err := client1.CoreV1().Services(metav1.NamespaceAll).List(context.TODO(), metav1.ListOptions{})
+               if err != nil {
+                       t.Logf(""Unexpected error: %v"", err)
+               }
+               t.Logf(""------------"")
+
+               for _, s := range svcs.Items {
+                       t.Logf(""svcs from client 1: %s %s\n"", s.Name, s.Spec.ClusterIP)
+               }
+
+               svcs, err = client1.CoreV1().Services(metav1.NamespaceAll).List(context.TODO(), metav1.ListOptions{})
+               if err != nil {
+                       t.Logf(""Unexpected error: %v"", err)
+               }
+               for _, s := range svcs.Items {
+                       t.Logf(""svcs from client 2: %s %s\n"", s.Name, s.Spec.ClusterIP)
+               }
+               time.Sleep(2 * time.Second)
+               t.Logf(""------------"")
+       }
+
+}
```

It is important to mention that the Service definition is not reconciled since 1.17 https://github.com/kubernetes/kubernetes/commit/2a9a9fa155e92d0bd7f4eb85d843cfa14b830cca
It means that first apiserver in create the `kubernetes.default` Service wins

Reporting it just for documenting it, initially described and fixed by https://github.com/kubernetes/enhancements/tree/master/keps/sig-network/1880-multiple-service-cidrs

/sig api-machinery
/sig network
/assign",aojea,2022-12-29 20:23:18+00:00,['aojea'],2025-02-07 17:32:07+00:00,,https://github.com/kubernetes/kubernetes/issues/114743,"[('kind/bug', 'Categorizes issue or PR as related to a bug.'), ('sig/network', 'Categorizes an issue or PR as relevant to SIG Network.'), ('sig/api-machinery', 'Categorizes an issue or PR as relevant to SIG API Machinery.'), ('needs-triage', 'Indicates an issue or PR lacks a `triage/foo` label and requires one.')]","[{'comment_id': 1367568958, 'issue_id': 1514030211, 'author': 'aojea', 'body': '/kind bug', 'created_at': datetime.datetime(2022, 12, 29, 20, 23, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 1370241782, 'issue_id': 1514030211, 'author': 'fedebongio', 'body': '/triage accepted', 'created_at': datetime.datetime(2023, 1, 3, 21, 27, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 1930734770, 'issue_id': 1514030211, 'author': 'k8s-triage-robot', 'body': 'This issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted', 'created_at': datetime.datetime(2024, 2, 6, 20, 54, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 1934610404, 'issue_id': 1514030211, 'author': 'seans3', 'body': '/triage accepted', 'created_at': datetime.datetime(2024, 2, 8, 17, 27, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2643561818, 'issue_id': 1514030211, 'author': 'k8s-triage-robot', 'body': 'This issue has not been updated in over 1 year, and should be re-triaged.\n\nYou can:\n- Confirm that this issue is still relevant with `/triage accepted` (org members only)\n- Close this issue with `/close`\n\nFor more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/\n\n/remove-triage accepted', 'created_at': datetime.datetime(2025, 2, 7, 17, 32, 4, tzinfo=datetime.timezone.utc)}]","aojea (Issue Creator) on (2022-12-29 20:23:33 UTC): /kind bug

fedebongio on (2023-01-03 21:27:43 UTC): /triage accepted

k8s-triage-robot on (2024-02-06 20:54:41 UTC): This issue has not been updated in over 1 year, and should be re-triaged.

You can:
- Confirm that this issue is still relevant with `/triage accepted` (org members only)
- Close this issue with `/close`

For more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/

/remove-triage accepted

seans3 on (2024-02-08 17:27:41 UTC): /triage accepted

k8s-triage-robot on (2025-02-07 17:32:04 UTC): This issue has not been updated in over 1 year, and should be re-triaged.

You can:
- Confirm that this issue is still relevant with `/triage accepted` (org members only)
- Close this issue with `/close`

For more details on the triage process, see https://www.kubernetes.dev/docs/guide/issue-triage/

/remove-triage accepted

"
