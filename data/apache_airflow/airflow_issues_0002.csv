id,type,state,state_reason,title,body,author,created_at,assignees,updated_at,closed_at,url,labels,comments_list,comment_thread
2741022067,issue,open,,Fix DatabricksPartitionSensor from immediately failing; return false in poke method for DatabricksPartitionSensor,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

DatabricksParitionSensor immediately fails when in ""poke"" mode. 

#44949 attempts a fix. Let me know any issues and I can add.

### What you think should happen instead?

It should poke. And fail after the timeout interval.

### How to reproduce

```
    wait_for_table_partition = DatabricksPartitionSensor(
        task_id=""wait_for_table_partition"",
        table_name=""table"",
        schema=""schema"",
        catalog=""catalog"",
        partitions={""snapshot_date"": ""cat""},
        databricks_conn_id=""databricks"",
        poke_interval=60,
        timeout=3600,
        mode=""poke"",
    )
```

### Operating System

Mac OS

### Versions of Apache Airflow Providers

Current

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Running in dev

### Anything else?

No

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",big-c-note,2024-12-16 00:21:12+00:00,[],2025-01-02 13:35:31+00:00,,https://github.com/apache/airflow/issues/44950,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('area:core', ''), ('provider:databricks', '')]","[{'comment_id': 2544228799, 'issue_id': 2741022067, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 16, 0, 21, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558246194, 'issue_id': 2741022067, 'author': 'big-c-note', 'body': ""Hi @elad-galili-ka , happy to work on this PR. Can you help me understand if the way I'm going about it is the right approach? Ie; returning `False` instead of raising an error from the `poke` method. \r\n\r\nIf so, I am guessing I need to test it and update the tests so that they pass."", 'created_at': datetime.datetime(2024, 12, 21, 21, 33, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567042787, 'issue_id': 2741022067, 'author': 'big-c-note', 'body': ""I won't work on this unless I have an understanding that it would be merged. Just hacking it with dag level retries for now.\r\n\r\nPlease let me know if there is any interest on a solution, and I will take it on"", 'created_at': datetime.datetime(2025, 1, 1, 15, 4, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567064113, 'issue_id': 2741022067, 'author': 'potiuk', 'body': 'DEfinitely if you add tests there and make it green and ping a bit more in the PR, there is a chance that now - after holidays there will be interest. Maybe @alexott  might help there. But in general one of the first things to do - before any reviews is to add unit tests and show that you understand the problem, error case and that you are providing solution to it. It\'s much easier for anyone doing the review (especially if they do not know much about databricks - Airflow maintainers are NOT databricks specialists - we just integrate with it) if they see that you can show the test case that they will be able to understand, see it running and see it in a bigger context.\r\n\r\nYou simply increase your chances of getting the review this way. \r\n\r\nNot mentioning that you submitted your change just before holidays and for many people holidays are still there and conssidering that most maintainers are pretty busy preparing to Airflow 3.\r\n\r\nAlso this is the first test for any author of any PR. Are you self-sufficient and self-driven? Can you come to the right conclusions, follow the contribution docs, look at other PRs and learn from them that unit tests are a prerequisite for reviewers to spend their time on that - generally red PR  with 2 line changes is an indication of heavily-work-in-progress that the author have not spend a lot of time on trying to understand the problem. solution, being able to reason about it and answer questions ""why this test"" (especially if there is no test).\r\n\r\nSo I would encourage you to continue working on it. \r\n\r\nAnd no- there is never guarantee that PR will be merged. Sometimes after a discussion, showing test and mutual understanding between author and reviewers the discussion might lead to different solutions - and this is good, because everyone learns along the way - even if code that you wrote is thrown away and replaced by something else.\r\n\r\nYou should treat such open PR as a learning opportunity, and by providing the reviewer more of an indication that you have not only spend a bit of time on providing fix but also understanding howt things work here and responding to the needs of reviewers who sometimes had to review 30 PRs a day and do it in their free time  as well as you do contributions - you increse your chances on getting your change (or whatever will result of it) merged.', 'created_at': datetime.datetime(2025, 1, 1, 16, 9, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567785656, 'issue_id': 2741022067, 'author': 'big-c-note', 'body': 'Sounds good. Thanks for the response', 'created_at': datetime.datetime(2025, 1, 2, 13, 35, 29, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-16 00:21:15 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

big-c-note (Issue Creator) on (2024-12-21 21:33:25 UTC): Hi @elad-galili-ka , happy to work on this PR. Can you help me understand if the way I'm going about it is the right approach? Ie; returning `False` instead of raising an error from the `poke` method. 

If so, I am guessing I need to test it and update the tests so that they pass.

big-c-note (Issue Creator) on (2025-01-01 15:04:35 UTC): I won't work on this unless I have an understanding that it would be merged. Just hacking it with dag level retries for now.

Please let me know if there is any interest on a solution, and I will take it on

potiuk on (2025-01-01 16:09:19 UTC): DEfinitely if you add tests there and make it green and ping a bit more in the PR, there is a chance that now - after holidays there will be interest. Maybe @alexott  might help there. But in general one of the first things to do - before any reviews is to add unit tests and show that you understand the problem, error case and that you are providing solution to it. It's much easier for anyone doing the review (especially if they do not know much about databricks - Airflow maintainers are NOT databricks specialists - we just integrate with it) if they see that you can show the test case that they will be able to understand, see it running and see it in a bigger context.

You simply increase your chances of getting the review this way. 

Not mentioning that you submitted your change just before holidays and for many people holidays are still there and conssidering that most maintainers are pretty busy preparing to Airflow 3.

Also this is the first test for any author of any PR. Are you self-sufficient and self-driven? Can you come to the right conclusions, follow the contribution docs, look at other PRs and learn from them that unit tests are a prerequisite for reviewers to spend their time on that - generally red PR  with 2 line changes is an indication of heavily-work-in-progress that the author have not spend a lot of time on trying to understand the problem. solution, being able to reason about it and answer questions ""why this test"" (especially if there is no test).

So I would encourage you to continue working on it. 

And no- there is never guarantee that PR will be merged. Sometimes after a discussion, showing test and mutual understanding between author and reviewers the discussion might lead to different solutions - and this is good, because everyone learns along the way - even if code that you wrote is thrown away and replaced by something else.

You should treat such open PR as a learning opportunity, and by providing the reviewer more of an indication that you have not only spend a bit of time on providing fix but also understanding howt things work here and responding to the needs of reviewers who sometimes had to review 30 PRs a day and do it in their free time  as well as you do contributions - you increse your chances on getting your change (or whatever will result of it) merged.

big-c-note (Issue Creator) on (2025-01-02 13:35:29 UTC): Sounds good. Thanks for the response

"
2740748244,issue,closed,completed,Add from_trigger field to AssetEvent extra for events from Trigger,"### Description

Currently when API is used to create an asset event `from_rest_api` is set as `True` in `extra` for the `AssetEvent` . This helps in identifying the source. Similarly `from_trigger` can be set for asset events created as part of event based scheduling. This will also help in UI where `Trigger` can be mentioned as the source. 

Another idea but slightly bigger one would be to pass the event payload as extra to the asset event, this will help in passing information from the trigger to the dag that consumes the event.

https://github.com/apache/airflow/blob/c77c7f003a2458698a1d5a440670b9728783ff78/airflow/api_connexion/endpoints/asset_endpoint.py#L350 

`extra = {""from_trigger"": True}` can be passed to `register_asset_change`

https://github.com/apache/airflow/blob/c77c7f003a2458698a1d5a440670b9728783ff78/airflow/models/trigger.py#L246-L249

Sample usage in `Asset Events` widget in dashboard UI that I am developing

![image](https://github.com/user-attachments/assets/a51cddf4-85bc-4b1b-b2c9-c83e4e039b6d)


### Use case/motivation

Helps in identification of the event source from trigger like how it's done through API now.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",tirkarthi,2024-12-15 16:28:07+00:00,[],2025-01-24 18:21:41+00:00,2025-01-24 18:21:41+00:00,https://github.com/apache/airflow/issues/44944,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2543938084, 'issue_id': 2740748244, 'author': 'tirkarthi', 'body': 'cc: @vincbeck', 'created_at': datetime.datetime(2024, 12, 15, 16, 28, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2543955741, 'issue_id': 2740748244, 'author': 'tirkarthi', 'body': ""The idea about passing trigger event to asset event is something like https://github.com/apache/airflow/issues/37810 but that trigger's submitted event payload is passed to dataset event. This could be an issue on it's own I guess. cc: @uranusjr @Lee-W"", 'created_at': datetime.datetime(2024, 12, 15, 17, 20, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2547202802, 'issue_id': 2740748244, 'author': 'Lee-W', 'body': ""I think it's ok to add an `from_trigger` field here. There's no fix format for extra so shouldn't be a problem"", 'created_at': datetime.datetime(2024, 12, 17, 0, 11, 13, tzinfo=datetime.timezone.utc)}]","tirkarthi (Issue Creator) on (2024-12-15 16:28:35 UTC): cc: @vincbeck

tirkarthi (Issue Creator) on (2024-12-15 17:20:53 UTC): The idea about passing trigger event to asset event is something like https://github.com/apache/airflow/issues/37810 but that trigger's submitted event payload is passed to dataset event. This could be an issue on it's own I guess. cc: @uranusjr @Lee-W

Lee-W on (2024-12-17 00:11:13 UTC): I think it's ok to add an `from_trigger` field here. There's no fix format for extra so shouldn't be a problem

"
2740638612,issue,closed,completed,Code in PR 43662 got removed in 2.10.3,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

One issue was fixed in PR https://github.com/apache/airflow/pull/43662 came up again.  The codes to fix the issue was removed somehow****

### What you think should happen instead?

code in PR 43662 should persist or Airflow has another way to fix the issue in this PR

### How to reproduce

Pls refer to https://github.com/apache/airflow/pull/43662

### Operating System

Debian 12.7

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",nicolasge,2024-12-15 13:18:37+00:00,[],2024-12-15 18:27:19+00:00,2024-12-15 18:27:19+00:00,https://github.com/apache/airflow/issues/44943,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2543991225, 'issue_id': 2740638612, 'author': 'potiuk', 'body': 'This code is in FAB provider, not in airflow. please refer to changelog there - and see if you have the right provider installed https://airflow.apache.org/docs/apache-airflow-providers-fab/stable/index.html - look at which FAB provider version you have installed \r\n\r\nIf you still have some kind of error - related to it - please open a new issue describing your problem and circumstances around it. \r\n\r\nYou can also - if you are not sure - check the 2.10.4rc1 release that is just being voted (see https://github.com/apache/airflow/issues/44811) - and look up the version 2.10.4 rc1 has.', 'created_at': datetime.datetime(2024, 12, 15, 18, 27, 15, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-12-15 18:27:15 UTC): This code is in FAB provider, not in airflow. please refer to changelog there - and see if you have the right provider installed https://airflow.apache.org/docs/apache-airflow-providers-fab/stable/index.html - look at which FAB provider version you have installed 

If you still have some kind of error - related to it - please open a new issue describing your problem and circumstances around it. 

You can also - if you are not sure - check the 2.10.4rc1 release that is just being voted (see https://github.com/apache/airflow/issues/44811) - and look up the version 2.10.4 rc1 has.

"
2739760998,issue,closed,completed,/airflow/settings.py raises TypeError: 'module' object is not callable,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.6.3

### What happened?

In pendulum 2 `pendulum.tz.timezone` refers to the function, in pendulum 3 refers to the module

```
Traceback (most recent call last):
  File ""/home/success/Desktop/airflow/.venv/2.6/bin/airflow"", line 5, in <module>
    from airflow.__main__ import main
  File ""/home/success/Desktop/airflow/airflow/__main__.py"", line 35, in <module>
    from airflow import configuration
  File ""/home/success/Desktop/airflow/airflow/configuration.py"", line 1814, in <module>
    secrets_backend_list = initialize_secrets_backends()
  File ""/home/success/Desktop/airflow/airflow/configuration.py"", line 1742, in initialize_secrets_backends
    secrets_backend_cls = import_string(class_name)
  File ""/home/success/Desktop/airflow/airflow/utils/module_loading.py"", line 36, in import_string
    module = import_module(module_path)
  File ""/usr/lib/python3.9/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/home/success/Desktop/airflow/airflow/secrets/metastore.py"", line 28, in <module>
    from airflow.utils.session import NEW_SESSION, provide_session
  File ""/home/success/Desktop/airflow/airflow/utils/session.py"", line 24, in <module>
    from airflow import settings
  File ""/home/success/Desktop/airflow/airflow/settings.py"", line 51, in <module>
    TIMEZONE = pendulum.tz.timezone(""UTC"")
TypeError: 'module' object is not callable
```

### What you think should happen instead?

_No response_

### How to reproduce

Install airflow
```
pip install -e .
```
`pip` requires pendulum>=2.0.0

### Operating System

ubuntu

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",SuccessMoses,2024-12-14 10:28:56+00:00,[],2024-12-14 18:07:58+00:00,2024-12-14 18:07:58+00:00,https://github.com/apache/airflow/issues/44933,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:dependencies', 'Issues related to dependencies problems'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2543225848, 'issue_id': 2739760998, 'author': 'tirkarthi', 'body': 'Related https://github.com/apache/airflow/issues/38714 . As noted in the linked issue please use proper constraints to install compatible versions of the dependencies .', 'created_at': datetime.datetime(2024, 12, 14, 18, 6, 23, tzinfo=datetime.timezone.utc)}]","tirkarthi on (2024-12-14 18:06:23 UTC): Related https://github.com/apache/airflow/issues/38714 . As noted in the linked issue please use proper constraints to install compatible versions of the dependencies .

"
2739265423,issue,closed,completed,Rate limited error on recent versions of SlackAPIFileOperator,"### Apache Airflow Provider(s)

slack

### Versions of Apache Airflow Providers

apache-airflow-providers-common-sql   1.16.0
apache-airflow-providers-slack      8.9.0
slack_sdk 3.32.0

### Apache Airflow version

AIRFLOW_VERSION=2.10.2

### Operating System

linux/macos

### Deployment

Docker-Compose

### Deployment details

Deploying airflow locally on docker with AIRFLOW_VERSION=2.10.2

### What happened

Referencing this operator `SlackAPIFileOperator` in version `apache-airflow-providers-slack==8.9.0`

We recently upgraded our deployment requirements an apache-airflow-providers-slack was one of them (8.0.0 -> 8.9.0)
After the upgrades, our airflow slack tasks started failing with ratelimited errors, like the one below:
```
slack_sdk.errors.SlackApiError: The request to the Slack API failed. (url: https://slack.com/api/conversations.list[)](https://slack.com/api/conversations.list))
The server responded with: {‘ok’: False, ‘error’: ‘ratelimited’}
```

Previously we were using apache-airflow-providers-slack 8.0.0 - which I believe was using SlackHook.send_file (v1) -- according to changelogs this has been [default bumped to v2 in ver. 8.7.0](https://airflow.apache.org/docs/apache-airflow-providers-slack/8.9.0/changelog.html#id6). 

However, if we switch to `method_version=""v1""`, it seems to still be working. 
Slack states that they will be deprecating the files.upload method in march 2025, but it looks like the [SlackAPIFileOperator class still uses files.upload here ](https://github.com/apache/airflow/blob/b69441dc27888012f218683b09241e5db12c37a5/providers/src/airflow/providers/slack/operators/slack.py#L230), I want to ensure that our code is prepared for the changes upcoming; can we still use `method_version=""v1""` beyond the point of deprecation? (is it using files.upload?)

I can't tell if it is or not, it just references self.hook.send_file which does seem to be explicitly in the repo?
    def _method_resolver(self):
        if self.method_version == ""v1"":
            return self.hook.send_file
        return self.hook.send_file_v1_to_v2


### What you think should happen instead

Ideally, we would be able to use the existing SlackAPIFileOperator with the send_file_v1_to_v2 (as it is default)

### How to reproduce

Reproduce with SlackAPIFileOperator using AIRFLOW_VERSION=2.10.2.
Create any DAG using the above operator and attempt to send a small .txt file to a slack channel your connection token has access to, getting this error:

slack_sdk.errors.SlackApiError: The request to the Slack API failed. (url: https://slack.com/api/conversations.list[)](https://slack.com/api/conversations.list))
The server responded with: {‘ok’: False, ‘error’: ‘ratelimited’}

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",hao-num,2024-12-13 21:53:23+00:00,[],2024-12-16 18:13:22+00:00,2024-12-16 18:13:22+00:00,https://github.com/apache/airflow/issues/44923,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:slack', '')]","[{'comment_id': 2542441185, 'issue_id': 2739265423, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 13, 21, 53, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2546319050, 'issue_id': 2739265423, 'author': 'hao-num', 'body': 'Hey turns out, with the new version of the package, if you use a channel name like ""#dwh-support"", instead of an channel id -- then the update operator will spend an API call(s) to retrieve the corresponding ID name which causes this rate limited issue.\r\n\r\nSolution here: convert channel names to channel_id -- this should resolve rate limited errors', 'created_at': datetime.datetime(2024, 12, 16, 18, 13, 22, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-13 21:53:27 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

hao-num (Issue Creator) on (2024-12-16 18:13:22 UTC): Hey turns out, with the new version of the package, if you use a channel name like ""#dwh-support"", instead of an channel id -- then the update operator will spend an API call(s) to retrieve the corresponding ID name which causes this rate limited issue.

Solution here: convert channel names to channel_id -- this should resolve rate limited errors

"
2738292464,issue,open,,TaskGroup ui_color ignored by the new graph view ,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.1

### What happened?

The new graph view seems to be ignoring set TaskGroup ui_color value, displays the default 
![image](https://github.com/user-attachments/assets/f2e67704-7df9-47af-9062-85d231b447d0)
![image](https://github.com/user-attachments/assets/9173f0de-c9df-4ddd-95da-b1375882580a)


### What you think should happen instead?

Display specified color, as in the old graph view

### How to reproduce

Recreate the following DAG and check its graph view in the UI

```
from datetime import datetime
from airflow.decorators import dag, task_group
from airflow.operators.empty import EmptyOperator

@dag(dag_id=""task_group_color"", start_date=datetime(2024, 12, 1), schedule=None)
def generate_dag():
    @task_group(ui_color=""orange"")
    def group():
        EmptyOperator(task_id=""task"")
    group()
generate_dag()
```

### Operating System

composer-2.8.8-airflow-2.9.1

### Versions of Apache Airflow Providers

_No response_

### Deployment

Google Cloud Composer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Voldurk,2024-12-13 12:31:31+00:00,[],2025-02-03 04:54:50+00:00,,https://github.com/apache/airflow/issues/44911,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2541347562, 'issue_id': 2738292464, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 13, 12, 31, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2613481597, 'issue_id': 2738292464, 'author': 'geraj1010', 'body': 'Interested if nobody is working on it. Should I apply this to both Airflow 2 and 3?', 'created_at': datetime.datetime(2025, 1, 24, 22, 10, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629928424, 'issue_id': 2738292464, 'author': 'geraj1010', 'body': 'Confirming this is also the case with Airflow 2.10.2. I tested with composer-2.10.2-2.10.2.', 'created_at': datetime.datetime(2025, 2, 3, 4, 54, 49, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-13 12:31:35 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

geraj1010 on (2025-01-24 22:10:20 UTC): Interested if nobody is working on it. Should I apply this to both Airflow 2 and 3?

geraj1010 on (2025-02-03 04:54:49 UTC): Confirming this is also the case with Airflow 2.10.2. I tested with composer-2.10.2-2.10.2.

"
2737533325,issue,closed,completed,providers-odbc in airflow:2.10.2 error UnicodeDecodeError: 'utf-8' codec can't decode byte 0x91.,"### Apache Airflow Provider(s)

odbc

### Versions of Apache Airflow Providers

apache-airflow-providers-odbc==4.7.0


### Apache Airflow version

2.7.1 and 2.10.2

### Operating System

PRETTY_NAME=""Debian GNU/Linux 12 (bookworm)"" NAME=""Debian GNU/Linux"" VERSION_ID=""12"" VERSION=""12 (bookworm)"" VERSION_CODENAME=bookworm ID=debian HOME_URL=""https://www.debian.org/"" SUPPORT_URL=""https://www.debian.org/support"" BUG_REPORT_URL=""https://bugs.debian.org/""

### Deployment

Docker-Compose

### Deployment details

I build arirflow from image tag 2.7.1 and 2.10.2. both image use the same Dockerfile just change the image tag. this Dockerfile will install IBM Netezza Datawarehouse ODBC Driver and copy unixODBC configulation files.

**Dockerfile**
```
FROM apache/airflow:2.10.2  #<---change tag.

USER root
RUN apt-get update
RUN apt install -y \
    nano \
    procps \
    unixodbc \
    build-essential \
    unzip
RUN rm -rf /var/lib/apt/lists/*

# ENV JAVA_HOME /usr/lib/jvm/java-17-openjdk-amd64
ENV LD_LIBRARY_PATH /usr/local/nz/lib64

COPY ./odbc/* /opt/nz_odbc

WORKDIR /opt/nz_odbc
RUN tar -xvzf nps-linuxclient-v11.2.1.10.tar.gz

WORKDIR /opt/nz_odbc/linux64
RUN ./unpack

COPY ./odbc/config/odbc.ini /etc/odbc.ini
COPY ./odbc/config/odbc.ini /usr/local/etc/odbc.ini
COPY ./odbc/config/odbcinst.ini /etc/odbcinst.ini
COPY ./odbc/config/odbcinst.ini /usr/local/etc/odbcinst.ini

USER airflow

RUN pip install apache-airflow apache-airflow-providers-odbc

WORKDIR /opt/
```

### What happened

After that I run compose up and then set connection name ""nz_odbc"" like this image below.
![image](https://github.com/user-attachments/assets/df3efb2a-20d8-4205-8e82-0ff4b542fb31)

and then use this python script to run DAG create table in Netezza.
```
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
from airflow.providers.odbc.hooks.odbc import OdbcHook

default_args = {
    'owner': 'MyDAG',
    'depends_on_past': False,
    'start_date': datetime(2024, 12, 13),
    'retries': 0,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    dag_id=""create_table"",
    default_args=default_args,
    schedule_interval=""@daily""
)

def get_data_from_system_i():
    
    odbc_hook = OdbcHook(odbc_conn_id='nz_odbc')
    
    connection = odbc_hook.get_conn()

    # Set encoding (optional, depends on your driver/database)
    connection.setdecoding(pyodbc.SQL_CHAR, encoding='utf-8')
    connection.setdecoding(pyodbc.SQL_WCHAR, encoding='utf-8')
    connection.setdecoding(pyodbc.SQL_WMETADATA, encoding='utf-8')
    connection.setencoding(encoding='utf-8')

    cursor = connection.cursor()
    
    cursor.execute(""""""
        CREATE TABLE EMPLOYEE (
            id int,
            name VARCHAR(50),
            department VARCHAR(50)
        )
    """""")
    
    connection.commit() 
    
    cursor.close()
    connection.close()

create_table = PythonOperator(
    task_id='get_customer_data',
    python_callable=get_data_from_system_i,
    dag=dag
)

create_table
```

then it have error when I use airflow version 2.10.2 but success in version 2.7.1.

the error of airflow:2.10.2
![image](https://github.com/user-attachments/assets/3154431a-c38d-4ec7-9232-6b82b3942f2b)


### What you think should happen instead

It's Should success like airflow:2.7.1

### How to reproduce

1. install netezzaODBC driver
2. test configulation is collect by **isql**
3. run airflow dag

### Anything else

- when I try to write python script by use pyodbc and then run script in container by command ""python3 main.py"" in airflow2.10.2 show same error that happen in airflow log. but airflow 2.7.1 success.
- I try to install everything in ubuntu20.04 (maybe same version of airflow:2.7.1) and 22.04 (maybe same version of airflow:2.10.2) and then run the pyodbc script. it's show the same error like above.
- that mean I think ubuntu version is not nessesary. maybe airflow 2.7.1 have some configulation or installation different from others.

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",RamesChan,2024-12-13 06:30:08+00:00,[],2024-12-19 16:18:04+00:00,2024-12-19 16:18:03+00:00,https://github.com/apache/airflow/issues/44902,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('pending-response', ''), ('provider:odbc', '')]","[{'comment_id': 2540650702, 'issue_id': 2737533325, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 13, 6, 30, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2541239141, 'issue_id': 2737533325, 'author': 'potiuk', 'body': 'Hard to say - looks like it might be a combination of some of your locale and how json gets translated - and likely some changes uncovered bad configuration of your machine. I know @dabla has been improving and changing it over the last couple of months and there were changes recently in retrieving the extras.\r\n\r\nAs a quick workaround @RamesChan  - can you please downgrade your ODBC provider following https://airflow.apache.org/docs/docker-stack/build.html#example-of-setting-own-airflow-provider-packages to the version that was installed in 2.7.1 (I think from https://github.com/apache/airflow/blob/main/generated/provider_metadata.json it looks like it was 4.0.0 where in 2.10.2 it was 4.7.1  - there was quite many changes between those versions.\r\n\r\nYou can also see the changelog https://airflow.apache.org/docs/apache-airflow-providers-odbc/stable/changelog.html and see that there were many changes between. \r\n\r\nIdeally - maybe that something you can help us with - you could pin-point in which version of the ODBC provider it has changed. The fastest way is ""bisecting"" -  if it works with 4.0.0 -> you could try in the ""half"" between 4.0.0 and 4.7.1 and then see if it is still working (if it does - the change was introduce later, if not - it was before). Rinse and repeat until you find the exact version that causes your problem.\r\n\r\nThat could enormously help in diagnosing the root cause of the problem.', 'created_at': datetime.datetime(2024, 12, 13, 11, 27, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2541242371, 'issue_id': 2737533325, 'author': 'potiuk', 'body': 'So if you could make such an exercise @RamesChan and bisect which odbc provider introduces the prblem - that woudl be fantastic - maybe we will also find it will **not** work with the old ODBC provider, but that would lead to maybe other conclusions.', 'created_at': datetime.datetime(2024, 12, 13, 11, 29, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2541833367, 'issue_id': 2737533325, 'author': 'dabla', 'body': ""Seems like an encoding issue, but looking at your connection details I don't see anything special, unless of course there would be a special character in the password."", 'created_at': datetime.datetime(2024, 12, 13, 16, 39, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2541841572, 'issue_id': 2737533325, 'author': 'dabla', 'body': 'Those where the changes I made, still can\'t figure out how this would have impacted the encoding issue:\r\n\r\n```\r\n    @property\r\n    def connection(self):\r\n        """"""The Connection object with ID ``odbc_conn_id``.""""""\r\n        if not self._connection:\r\n            self._connection = self.get_connection(self.get_conn_id())\r\n        return self._connection\r\n\r\n    @property\r\n    def connection_extra_lower(self) -> dict:\r\n        """"""\r\n        ``connection.extra_dejson`` but where keys are converted to lower case.\r\n\r\n        This is used internally for case-insensitive access of odbc params.\r\n        """"""\r\n        return {k.lower(): v for k, v in self.connection.extra_dejson.items()}\r\n\r\n    @property\r\n    def odbc_connection_string(self):\r\n        """"""\r\n        ODBC connection string.\r\n\r\n        We build connection string instead of using ``pyodbc.connect`` params\r\n        because, for example, there is no param representing\r\n        ``ApplicationIntent=ReadOnly``.  Any key-value pairs provided in\r\n        ``Connection.extra`` will be added to the connection string.\r\n        """"""\r\n        if not self._conn_str:\r\n            conn_str = """"\r\n            if self.driver:\r\n                conn_str += f""DRIVER={{{self.driver}}};""\r\n            if self.dsn:\r\n                conn_str += f""DSN={self.dsn};""\r\n            if self.connection.host:\r\n                conn_str += f""SERVER={self.connection.host};""\r\n            database = self.database or self.connection.schema\r\n            if database:\r\n                conn_str += f""DATABASE={database};""\r\n            if self.connection.login:\r\n                conn_str += f""UID={self.connection.login};""\r\n            if self.connection.password:\r\n                conn_str += f""PWD={self.connection.password};""\r\n            if self.connection.port:\r\n                conn_str += f""PORT={self.connection.port};""\r\n\r\n            extra_exclude = {""driver"", ""dsn"", ""connect_kwargs"", ""sqlalchemy_scheme"", ""placeholder""}\r\n            extra_params = {\r\n                k: v for k, v in self.connection.extra_dejson.items() if k.lower() not in extra_exclude\r\n            }\r\n            for k, v in extra_params.items():\r\n                conn_str += f""{k}={v};""\r\n\r\n            self._conn_str = conn_str\r\n        return self._conn_str\r\n```', 'created_at': datetime.datetime(2024, 12, 13, 16, 44, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2543022163, 'issue_id': 2737533325, 'author': 'RamesChan', 'body': 'Hello, everyone! I have some great news to share. I’ve managed to resolve the issue and identified its root cause.\r\n\r\nThe problem was related to the version of pyodbc used:\r\n\r\nIn `airflow:2.7.1`, the version of `pyodbc` is `4.0.39`.\r\nHowever, in `airflow:2.10.2`, the version of `pyodbc` is `5.1.0`.\r\nTo resolve the issue, I downgraded the Python version in `airflow:2.10.2` from `3.12.x` to `3.10.12`, allowing me to install `pyodbc version 4.0.39` on `airflow:2.10.2`. After making this change, everything works perfectly! I was able to run the script and successfully connect to Netezza.\r\n\r\nHowever, I have yet to verify exactly until which version of `pyodbc` my `Netezza` can support. Additionally, I haven’t checked which version of `airflow-provider-odbc` I need to use if I want to work with newer versions of `Airflow`.\r\n\r\nI’d like to extend my gratitude to both of you @dabla @potiuk  for helping troubleshoot and identify the cause of this problem. Thank you so much!', 'created_at': datetime.datetime(2024, 12, 14, 9, 26, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2543342088, 'issue_id': 2737533325, 'author': 'dabla', 'body': '> Hello, everyone! I have some great news to share. I’ve managed to resolve the issue and identified its root cause.\r\n> \r\n> The problem was related to the version of pyodbc used:\r\n> \r\n> In `airflow:2.7.1`, the version of `pyodbc` is `4.0.39`. However, in `airflow:2.10.2`, the version of `pyodbc` is `5.1.0`. To resolve the issue, I downgraded the Python version in `airflow:2.10.2` from `3.12.x` to `3.10.12`, allowing me to install `pyodbc version 4.0.39` on `airflow:2.10.2`. After making this change, everything works perfectly! I was able to run the script and successfully connect to Netezza.\r\n> \r\n> However, I have yet to verify exactly until which version of `pyodbc` my `Netezza` can support. Additionally, I haven’t checked which version of `airflow-provider-odbc` I need to use if I want to work with newer versions of `Airflow`.\r\n> \r\n> I’d like to extend my gratitude to both of you @dabla @potiuk for helping troubleshoot and identify the cause of this problem. Thank you so much!\r\n\r\nYou’re welcome, glad you found the issue and you found a workaround. We also experienced weird issues with odbc drivers in the past, not always easy to pin-point.', 'created_at': datetime.datetime(2024, 12, 14, 20, 55, 8, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-13 06:30:34 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-12-13 11:27:46 UTC): Hard to say - looks like it might be a combination of some of your locale and how json gets translated - and likely some changes uncovered bad configuration of your machine. I know @dabla has been improving and changing it over the last couple of months and there were changes recently in retrieving the extras.

As a quick workaround @RamesChan  - can you please downgrade your ODBC provider following https://airflow.apache.org/docs/docker-stack/build.html#example-of-setting-own-airflow-provider-packages to the version that was installed in 2.7.1 (I think from https://github.com/apache/airflow/blob/main/generated/provider_metadata.json it looks like it was 4.0.0 where in 2.10.2 it was 4.7.1  - there was quite many changes between those versions.

You can also see the changelog https://airflow.apache.org/docs/apache-airflow-providers-odbc/stable/changelog.html and see that there were many changes between. 

Ideally - maybe that something you can help us with - you could pin-point in which version of the ODBC provider it has changed. The fastest way is ""bisecting"" -  if it works with 4.0.0 -> you could try in the ""half"" between 4.0.0 and 4.7.1 and then see if it is still working (if it does - the change was introduce later, if not - it was before). Rinse and repeat until you find the exact version that causes your problem.

That could enormously help in diagnosing the root cause of the problem.

potiuk on (2024-12-13 11:29:49 UTC): So if you could make such an exercise @RamesChan and bisect which odbc provider introduces the prblem - that woudl be fantastic - maybe we will also find it will **not** work with the old ODBC provider, but that would lead to maybe other conclusions.

dabla on (2024-12-13 16:39:14 UTC): Seems like an encoding issue, but looking at your connection details I don't see anything special, unless of course there would be a special character in the password.

dabla on (2024-12-13 16:44:06 UTC): Those where the changes I made, still can't figure out how this would have impacted the encoding issue:

```
    @property
    def connection(self):
        """"""The Connection object with ID ``odbc_conn_id``.""""""
        if not self._connection:
            self._connection = self.get_connection(self.get_conn_id())
        return self._connection

    @property
    def connection_extra_lower(self) -> dict:
        """"""
        ``connection.extra_dejson`` but where keys are converted to lower case.

        This is used internally for case-insensitive access of odbc params.
        """"""
        return {k.lower(): v for k, v in self.connection.extra_dejson.items()}

    @property
    def odbc_connection_string(self):
        """"""
        ODBC connection string.

        We build connection string instead of using ``pyodbc.connect`` params
        because, for example, there is no param representing
        ``ApplicationIntent=ReadOnly``.  Any key-value pairs provided in
        ``Connection.extra`` will be added to the connection string.
        """"""
        if not self._conn_str:
            conn_str = """"
            if self.driver:
                conn_str += f""DRIVER={{{self.driver}}};""
            if self.dsn:
                conn_str += f""DSN={self.dsn};""
            if self.connection.host:
                conn_str += f""SERVER={self.connection.host};""
            database = self.database or self.connection.schema
            if database:
                conn_str += f""DATABASE={database};""
            if self.connection.login:
                conn_str += f""UID={self.connection.login};""
            if self.connection.password:
                conn_str += f""PWD={self.connection.password};""
            if self.connection.port:
                conn_str += f""PORT={self.connection.port};""

            extra_exclude = {""driver"", ""dsn"", ""connect_kwargs"", ""sqlalchemy_scheme"", ""placeholder""}
            extra_params = {
                k: v for k, v in self.connection.extra_dejson.items() if k.lower() not in extra_exclude
            }
            for k, v in extra_params.items():
                conn_str += f""{k}={v};""

            self._conn_str = conn_str
        return self._conn_str
```

RamesChan (Issue Creator) on (2024-12-14 09:26:39 UTC): Hello, everyone! I have some great news to share. I’ve managed to resolve the issue and identified its root cause.

The problem was related to the version of pyodbc used:

In `airflow:2.7.1`, the version of `pyodbc` is `4.0.39`.
However, in `airflow:2.10.2`, the version of `pyodbc` is `5.1.0`.
To resolve the issue, I downgraded the Python version in `airflow:2.10.2` from `3.12.x` to `3.10.12`, allowing me to install `pyodbc version 4.0.39` on `airflow:2.10.2`. After making this change, everything works perfectly! I was able to run the script and successfully connect to Netezza.

However, I have yet to verify exactly until which version of `pyodbc` my `Netezza` can support. Additionally, I haven’t checked which version of `airflow-provider-odbc` I need to use if I want to work with newer versions of `Airflow`.

I’d like to extend my gratitude to both of you @dabla @potiuk  for helping troubleshoot and identify the cause of this problem. Thank you so much!

dabla on (2024-12-14 20:55:08 UTC): You’re welcome, glad you found the issue and you found a workaround. We also experienced weird issues with odbc drivers in the past, not always easy to pin-point.

"
2737025845,issue,closed,completed,onboading issue: airflow users create does not work,"### What do you see as an issue?

_No response_

### Solving the problem

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",atomic,2024-12-12 22:51:41+00:00,[],2024-12-12 23:01:42+00:00,2024-12-12 23:01:42+00:00,https://github.com/apache/airflow/issues/44900,"[('kind:bug', 'This is a clearly a bug'), ('area:CLI', ''), ('kind:documentation', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2540167827, 'issue_id': 2737025845, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 12, 22, 51, 44, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-12 22:51:44 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2736933164,issue,open,,Allow check_fn of S3KeySensor to receive bucket key,"### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.28.0

### Apache Airflow version

2.9.3

### Operating System

macOS 15.1.1

### Deployment

Google Cloud Composer

### Deployment details

_No response_

### What happened

When using the `check_fn` function in `S3KeySensor`, there's no way for the function to check against a specific object name. The only available keys are what's provided in the S3 `head_object` API call, which doesn't include the prefix or object name itself. 

### What you think should happen instead

If `check_fn` takes in a list of file sizes, it should also map the S3 key to the file size so there's flexibility in how to filter the list.

### How to reproduce

If there is a bucket with the following objects:
```
$ aws s3 ls s3://test-bucket/path/to/some/files
2024-12-11 20:09:12   18348549 000000_0-hadoop_20241212010840_abcdef.gz
2024-12-11 20:09:14   16543931 000001_0-hadoop_20241212010840_sadfjwij.gz
2024-12-11 20:09:49          0 _SUCCESS
```
and S3KeySensor:
```python
def check_for_file_in_s3 = S3KeySensor(
        task_id=""check_for_file_in_s3"",
        soft_fail=True,
        mode=""reschedule"",
        poke_interval=0,
        timeout=0, 
        bucket_name=""test-bucket"",
        bucket_key=[
            ""path/to/some/files/_SUCCESS"", 
            ""path/to/some/files/000000_0-hadoop_*""
        ],
        aws_conn_id=""spend327_aws_connection"",
        retries=0,=
        wildcard_match=True,
        check_fn=check_fn
    )
```
then the following `check_fn` will never succeed:
```python
def check_fn(files: list, **kwargs: Any) -> bool:
    """"""
    Check that the data file is greater than 0.5 megabyte

    :param files: List of S3 object attributes.
    :return: true if the criteria is met
    """"""
    for file in files:
        if ""hadoop"" in file:
            return file.get(""Size"", 0) > 524288
        elif ""SUCCESS"" in file:
            return True
        else:
            return False
```

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",tuzonghua,2024-12-12 21:40:44+00:00,['tuzonghua'],2024-12-16 20:49:15+00:00,,https://github.com/apache/airflow/issues/44896,"[('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('kind:feature', 'Feature Requests'), ('good first issue', '')]","[{'comment_id': 2540068354, 'issue_id': 2736933164, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 12, 21, 40, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2540094370, 'issue_id': 2736933164, 'author': 'tuzonghua', 'body': '@vincbeck Before I start on any work, just double checking that we could, for example, restructure the [metadata dictionary](https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/amazon/aws/sensors/s3.py#L135) in the sensor to include the S3 Key as a key so that `head_object` metadata is associated with a specific item in the `files` list. So the default dictionary might be `{""Key"": str, ""Size"": int}`.', 'created_at': datetime.datetime(2024, 12, 12, 21, 58, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2546720696, 'issue_id': 2736933164, 'author': 'vincbeck', 'body': '> @vincbeck Before I start on any work, just double checking that we could, for example, restructure the [metadata dictionary](https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/amazon/aws/sensors/s3.py#L135) in the sensor to include the S3 Key as a key so that `head_object` metadata is associated with a specific item in the `files` list. So the default dictionary might be `{""Key"": str, ""Size"": int}`.\r\n\r\nYes that looks good. That\'d be a good and simple way to handle that', 'created_at': datetime.datetime(2024, 12, 16, 20, 49, 14, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-12 21:40:48 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

tuzonghua (Issue Creator) on (2024-12-12 21:58:23 UTC): @vincbeck Before I start on any work, just double checking that we could, for example, restructure the [metadata dictionary](https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/amazon/aws/sensors/s3.py#L135) in the sensor to include the S3 Key as a key so that `head_object` metadata is associated with a specific item in the `files` list. So the default dictionary might be `{""Key"": str, ""Size"": int}`.

vincbeck on (2024-12-16 20:49:14 UTC): Yes that looks good. That'd be a good and simple way to handle that

"
2736854659,issue,closed,completed,Task retries are not run with soft_fail=True,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Using `soft_fail=True` with sensors does not allow retries.

If a sensor is going to fail and `soft_fail=True`, the task will skip instead of failing. However, if retries are set for the task, those are not executed since an [AirflowSkipExecution is raised in the code](https://github.com/apache/airflow/blob/35087d7d10714130cc3e9e9730e34b07fc56938d/airflow/sensors/base.py#L293), and the task's status is marked as `skipped`, not `up_for_retry`.

### What you think should happen instead?

If the task has retries left, its status should be set to `up_for_retry`. The `AirflowSkipException` should only be raised if no retries are left.

### How to reproduce

I have seen a few reports of this and was also able to reproduce it with the below DAG code.

```
import datetime

from airflow.decorators import dag
from airflow.decorators import task
from airflow.sensors.external_task import ExternalTaskSensor


@dag(start_date=datetime.datetime(2024, 10, 1), schedule=""* * * * *"", catchup=False)
def dag_2():

    ets = ExternalTaskSensor(
        task_id=""ets"",
        external_dag_id=""non_existent_dag"",
        soft_fail=True,
        retries=3,
        poke_interval=10,
        timeout=5,
    )

    ets

dag_2()
```

### Operating System

Debian GNU/Linux 12

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",karenbraganz,2024-12-12 20:49:06+00:00,[],2024-12-23 08:33:58+00:00,2024-12-23 08:33:58+00:00,https://github.com/apache/airflow/issues/44893,"[('kind:bug', 'This is a clearly a bug'), ('pending-response', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2545486339, 'issue_id': 2736854659, 'author': 'nathadfield', 'body': ""@karenbraganz I think, in the scenario you describe, this is the desired behaviour.  The reason why the sensor task is not going into an `UP_FOR_RETRY` state is because the we have reached the [timeout](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/tasks.html#timeouts).\r\n\r\nWhen this occurs, the sensor task will immediately raise a `AirflowSensorTimeout` exception and fail without retrying.\r\n\r\n```\r\n[2024-12-16, 12:09:25 GMT] {external_task.py:274} INFO - Poking for DAG 'non_existent_dag' on 2024-12-16T12:07:00+00:00 ... \r\n[2024-12-16, 12:09:25 GMT] {taskinstance.py:301} INFO - Sensor has timed out; run duration of 10.012994088 seconds exceeds the specified timeout of 5.0.\r\n```\r\n\r\nIt makes no sense to retry a sensor just for a timeout; instead the timeout should be longer."", 'created_at': datetime.datetime(2024, 12, 16, 12, 21, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558306645, 'issue_id': 2736854659, 'author': 'karenbraganz', 'body': ""@nathadfield You're right- timeouts are probably not a good application for retries. I thought that all failures including intermittent failures would result in the task getting skipped even when retries are set. After reviewing the code and running a few tests to see if this applies to intermittent issues (by deliberately getting tasks stuck in queued and failing), it seems this isn't the case. When the task got stuck in queued and failed, it was able to retry."", 'created_at': datetime.datetime(2024, 12, 22, 2, 36, 51, tzinfo=datetime.timezone.utc)}]","nathadfield on (2024-12-16 12:21:50 UTC): @karenbraganz I think, in the scenario you describe, this is the desired behaviour.  The reason why the sensor task is not going into an `UP_FOR_RETRY` state is because the we have reached the [timeout](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/tasks.html#timeouts).

When this occurs, the sensor task will immediately raise a `AirflowSensorTimeout` exception and fail without retrying.

```
[2024-12-16, 12:09:25 GMT] {external_task.py:274} INFO - Poking for DAG 'non_existent_dag' on 2024-12-16T12:07:00+00:00 ... 
[2024-12-16, 12:09:25 GMT] {taskinstance.py:301} INFO - Sensor has timed out; run duration of 10.012994088 seconds exceeds the specified timeout of 5.0.
```

It makes no sense to retry a sensor just for a timeout; instead the timeout should be longer.

karenbraganz (Issue Creator) on (2024-12-22 02:36:51 UTC): @nathadfield You're right- timeouts are probably not a good application for retries. I thought that all failures including intermittent failures would result in the task getting skipped even when retries are set. After reviewing the code and running a few tests to see if this applies to intermittent issues (by deliberately getting tasks stuck in queued and failing), it seems this isn't the case. When the task got stuck in queued and failed, it was able to retry.

"
2736827350,issue,open,,Deferrable BeamRunPythonPipelineOperator fails in Cloud Composer,"### Apache Airflow Provider(s)

apache-beam

### Versions of Apache Airflow Providers

`apache-airflow-providers-apache-beam==5.7.1`

Others specified in the [Cloud Composer version list](https://cloud.google.com/composer/docs/concepts/versioning/composer-versions#:~:text=composer%2D2.9.1%2Dairflow%2D2.9.1)

### Apache Airflow version

2.9.1

### Operating System

composer-2.9.1-airflow-2.9.1

### Deployment

Google Cloud Composer

### Deployment details

Triggerer config: `1 triggerer with 1 vCPU, 1 GB memory, 1 GB storage`

### What happened

When triggering a Dataflow job using the `BeamRunPythonPipelineOperator` with `deferrable=True`, the task fails because the triggerer does not have access to the DAG folder and thus cannot access the Python file containing the Beam pipeline. 

Triggerer logs:
```
2024-12-12 11:02:18.997 MST airflow-triggerer trigger deferrable_dag/manual__2024-12-12T18:02:13.335160+00:00/deferrable_task/-1/1 (ID 16) starting
2024-12-12 11:04:18.382 MST airflow-triggerer 1 triggers currently running
2024-12-12 11:04:21.491 MST airflow-triggerer Beam version: 2.57.0
2024-12-12 11:04:21.492 MST airflow-triggerer Running command: /tmp/apache-beam-venv_ucd7ai0/bin/python /home/airflow/gcs/dags/include/beam/deferrable.py --runner=DataflowRunner
2024-12-12 11:04:21.495 MST airflow-triggerer Start waiting for Apache Beam process to complete.
2024-12-12 11:04:21.542 MST airflow-triggerer /tmp/apache-beam-venv_ucd7ai0/bin/python: can't open file '/home/airflow/gcs/dags/include/beam/deferrable.py': [Errno 2] No such file or directory
2024-12-12 11:04:21.547 MST airflow-triggerer Process exited with return code: 2
2024-12-12 11:04:22.384 MST airflow-triggerer trigger deferrable_dag/manual__2024-12-12T18:02:13.335160+00:00/deferrable_task/-1/1 (ID 16) complete
```

### What you think should happen instead

Apache Beam needs to run a command like `python local_pipeline_file.py` to start the job, so my understanding is that needs to happen on the worker before the task gets deferred to a Trigger

### How to reproduce

1. Create a Cloud Composer environment with at least 1 triggerer
2. 
```python
deferrable_test = BeamRunPythonPipelineOperator(
        py_file=f""{{{{ conf.get('core', 'dags_folder') }}}}/include/beam/deferrable.py"",
        runner=""DataflowRunner"",
        ...,
        deferrable=True,
    )
```

### Anything else

The issue only occurs when running the DAG on Cloud Composer. Using the same setup locally with Docker Compose, everything works as expected. My theory is this is because the DAGs folder is not synchronized with the Airflow triggerer in CC but it is mounted as a volume for the triggerer in `docker-compose.yaml`


### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",devinmnorris,2024-12-12 20:32:17+00:00,[],2024-12-30 12:55:35+00:00,,https://github.com/apache/airflow/issues/44892,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:apache-beam', '')]","[{'comment_id': 2565449667, 'issue_id': 2736827350, 'author': 'MaksYermak', 'body': 'Hello @devinmnorris\r\nThank you for creating an issue. The problem which you describe was fixed by this PR https://github.com/apache/airflow/pull/44386', 'created_at': datetime.datetime(2024, 12, 30, 12, 55, 34, tzinfo=datetime.timezone.utc)}]","MaksYermak on (2024-12-30 12:55:34 UTC): Hello @devinmnorris
Thank you for creating an issue. The problem which you describe was fixed by this PR https://github.com/apache/airflow/pull/44386

"
2736640154,issue,open,,Bring back use_airflow_context to Python operator.,"### Body

See #44552  - once context serialization is ready we should bring it back.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",potiuk,2024-12-12 18:44:28+00:00,['potiuk'],2024-12-12 18:46:42+00:00,,https://github.com/apache/airflow/issues/44889,"[('kind:feature', 'Feature Requests'), ('kind:meta', 'High-level information important to the community'), ('area:core-operators', 'Operators, Sensors and hooks within Core Airflow')]",[],
2736451071,issue,closed,completed,Handle login with FAB provider in new UI,"### Description

In Airflow 2 the login with the FAB auth manager is handled automatically by Flask. The login form and the logic behind is handled by Flask. The FAB auth manager will be available in Airflow 3 as well. How the login experience will look like?

There are multiple options:
- We need to implement everything ourselves (login and form and logic) in the FAB provider
- We embed the current login form in the minimal FAB application we have for Airflow 2 plugins in FAB provider

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vincbeck,2024-12-12 17:05:49+00:00,['vincbeck'],2025-01-21 20:17:53+00:00,2025-01-21 20:17:51+00:00,https://github.com/apache/airflow/issues/44885,"[('kind:feature', 'Feature Requests'), ('area:auth', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2543113200, 'issue_id': 2736451071, 'author': 'SAXENA117', 'body': '@rawwar', 'created_at': datetime.datetime(2024, 12, 14, 13, 36, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2605655202, 'issue_id': 2736451071, 'author': 'vincbeck', 'body': 'Completed as part of #45765', 'created_at': datetime.datetime(2025, 1, 21, 20, 17, 52, tzinfo=datetime.timezone.utc)}]","SAXENA117 on (2024-12-14 13:36:45 UTC): @rawwar

vincbeck (Issue Creator) on (2025-01-21 20:17:52 UTC): Completed as part of #45765

"
2736448620,issue,open,,AIP-38 JWT Flow,"Implement the JWT flow in the front-end:
- [ ] retrieve the JWT from the homepage URL after login (or any page actually)
- [ ] persist it in the local storage
- [ ] provide the Token in all requests
- [ ] eventually clear on 401 expiration (no refresh flow implemented yet)

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-12-12 17:04:46+00:00,[],2025-01-25 09:53:20+00:00,,https://github.com/apache/airflow/issues/44884,"[('area:auth', '')]","[{'comment_id': 2613872491, 'issue_id': 2736448620, 'author': 'tirkarthi', 'body': 'Related : https://github.com/apache/airflow/pull/45765', 'created_at': datetime.datetime(2025, 1, 25, 9, 53, 19, tzinfo=datetime.timezone.utc)}]","tirkarthi on (2025-01-25 09:53:19 UTC): Related : https://github.com/apache/airflow/pull/45765

"
2736385793,issue,closed,completed,Implement Fab provider views in airflow 3,"### Context
This is a front-end story.

Depends on: https://github.com/apache/airflow/issues/44881

FabAuthManager register admin views leveraging FAB, to enable this:

![Screenshot 2024-12-12 at 17 31 09](https://github.com/user-attachments/assets/c8ed1042-96da-427e-aaea-c7e490f8cd64)

The front end code need to be developed (most likely inside the provider), and we need to find a way to register `react` code from the provider into the main react application. The ideal way would be to leverage the new front-end plugin system.

That might be a two step process:
- **Part 1**: Leverage the new plugin system for the react UI from the providers (fab) to be able to register a new dummy front-end page (POC). (Requires the plugin system)
- **Part 2**: Develop all the admin pages necessary from the fab provider (user, role, permissions, etc...). Thanks to **Part 1** this code should live directly inside the provider.


### Fallback
If the plugin system if not ready or if this happen to be too complex (packing of providers with react code etc...) we could temporarily:
Code this front-end pages in the core UI, and depending on the auth manager used, display them or not. That's less scalable because if other providers need to register new views, we will have their code into core UI with conditional rendering, which isn't great.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-12-12 16:37:03+00:00,[],2025-01-21 21:09:28+00:00,2025-01-21 21:09:27+00:00,https://github.com/apache/airflow/issues/44883,"[('kind:feature', 'Feature Requests'), ('good first issue', ''), ('kind:meta', 'High-level information important to the community'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('provider:fab', '')]","[{'comment_id': 2539456580, 'issue_id': 2736385793, 'author': 'pierrejeambrun', 'body': 'cc: @vincbeck', 'created_at': datetime.datetime(2024, 12, 12, 16, 38, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2539550647, 'issue_id': 2736385793, 'author': 'pierrejeambrun', 'body': 'good first issue for the `Part 2`, to drag attention, a lot of interesting PRs to make there.', 'created_at': datetime.datetime(2024, 12, 12, 17, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2548626273, 'issue_id': 2736385793, 'author': 'pierrejeambrun', 'body': 'cc: @jedcunningham \r\n\r\nUpdate this will most likely not be done like this to avoid re-implementing admin views in react. We will embed the flask rendered pages in the new UI.', 'created_at': datetime.datetime(2024, 12, 17, 14, 34, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2548908094, 'issue_id': 2736385793, 'author': 'vincbeck', 'body': ""> cc: @jedcunningham\r\n> \r\n> Update this will most likely not be done like this to avoid re-implementing admin views in react. We will embed the flask rendered pages in the new UI.\r\n\r\nI am happy to discuss this with you but I'd be careful if we go down that path. Explanations.\r\n\r\nGoing down that path means that an auth manager needs to use Flask to create custom views. To me, this is a bit weird to explain to users that if they want to create an auth manager, they need to use Flask, which will then be rendered in Fast api. One solution could be to offer both mode (we keep the Flask option for legacy auth managers such as Fab and we also provide the option to create custom views with Fast api). I'd strongly encourage going with this option. But this option is not perfect either. Keeping Flask as an option for auth managers means:\r\n- Flask wont be removed from Airflow 3, we will still have Flask as part of dependencies\r\n- Some methods in the [base_auth_manager.py](https://github.com/apache/airflow/blob/main/airflow/auth/managers/base_auth_manager.py) such as `get_api_endpoints` (used for an auth manager to extend the Rest API) depend on Flask (it returns a Flask Blueprint). If we need to keep this method for legacy auth managers, we wont be able to remove it before Airflow 4 (breaking change)"", 'created_at': datetime.datetime(2024, 12, 17, 16, 12, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2548965639, 'issue_id': 2736385793, 'author': 'jedcunningham', 'body': ""Sorry for the delay here, I'd intended to get a discussion going on this but y'all beat me too it :)\r\n\r\nThe auth mananger interface should just have a way to accept fastapi subapps - that leaves the specifics to the auth managers themselves. Some could use fastapi native subapps, others could wrap flask/fab/etc with WSGIMiddleware. We could document how to mount flask, or not. But this removes the dependency on flask/fab if it's not actually used."", 'created_at': datetime.datetime(2024, 12, 17, 16, 32, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2549279611, 'issue_id': 2736385793, 'author': 'vincbeck', 'body': ""> Sorry for the delay here, I'd intended to get a discussion going on this but y'all beat me too it :)\r\n> \r\n> The auth mananger interface should just have a way to accept fastapi subapps - that leaves the specifics to the auth managers themselves. Some could use fastapi native subapps, others could wrap flask/fab/etc with WSGIMiddleware. We could document how to mount flask, or not. But this removes the dependency on flask/fab if it's not actually used.\r\n\r\nThat should work :)"", 'created_at': datetime.datetime(2024, 12, 17, 18, 27, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2549617441, 'issue_id': 2736385793, 'author': 'vincbeck', 'body': 'See #45009', 'created_at': datetime.datetime(2024, 12, 17, 20, 59, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2605658849, 'issue_id': 2736385793, 'author': 'vincbeck', 'body': 'I think we can close this one? As far as I know we will not do it?', 'created_at': datetime.datetime(2025, 1, 21, 20, 19, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2605744360, 'issue_id': 2736385793, 'author': 'jedcunningham', 'body': 'Yeah, think we are good on this one.', 'created_at': datetime.datetime(2025, 1, 21, 21, 9, 27, tzinfo=datetime.timezone.utc)}]","pierrejeambrun (Issue Creator) on (2024-12-12 16:38:03 UTC): cc: @vincbeck

pierrejeambrun (Issue Creator) on (2024-12-12 17:17:00 UTC): good first issue for the `Part 2`, to drag attention, a lot of interesting PRs to make there.

pierrejeambrun (Issue Creator) on (2024-12-17 14:34:49 UTC): cc: @jedcunningham 

Update this will most likely not be done like this to avoid re-implementing admin views in react. We will embed the flask rendered pages in the new UI.

vincbeck on (2024-12-17 16:12:24 UTC): I am happy to discuss this with you but I'd be careful if we go down that path. Explanations.

Going down that path means that an auth manager needs to use Flask to create custom views. To me, this is a bit weird to explain to users that if they want to create an auth manager, they need to use Flask, which will then be rendered in Fast api. One solution could be to offer both mode (we keep the Flask option for legacy auth managers such as Fab and we also provide the option to create custom views with Fast api). I'd strongly encourage going with this option. But this option is not perfect either. Keeping Flask as an option for auth managers means:
- Flask wont be removed from Airflow 3, we will still have Flask as part of dependencies
- Some methods in the [base_auth_manager.py](https://github.com/apache/airflow/blob/main/airflow/auth/managers/base_auth_manager.py) such as `get_api_endpoints` (used for an auth manager to extend the Rest API) depend on Flask (it returns a Flask Blueprint). If we need to keep this method for legacy auth managers, we wont be able to remove it before Airflow 4 (breaking change)

jedcunningham on (2024-12-17 16:32:45 UTC): Sorry for the delay here, I'd intended to get a discussion going on this but y'all beat me too it :)

The auth mananger interface should just have a way to accept fastapi subapps - that leaves the specifics to the auth managers themselves. Some could use fastapi native subapps, others could wrap flask/fab/etc with WSGIMiddleware. We could document how to mount flask, or not. But this removes the dependency on flask/fab if it's not actually used.

vincbeck on (2024-12-17 18:27:52 UTC): That should work :)

vincbeck on (2024-12-17 20:59:27 UTC): See #45009

vincbeck on (2025-01-21 20:19:55 UTC): I think we can close this one? As far as I know we will not do it?

jedcunningham on (2025-01-21 21:09:27 UTC): Yeah, think we are good on this one.

"
2736344254,issue,closed,completed,Register extra router from the authmanager into the FastAPI application,"### Body

Depends on https://github.com/apache/airflow/issues/44847

The extra router exposed by the auth manager should be registered into the core FastAPI application.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-12-12 16:18:14+00:00,['jason810496'],2024-12-20 23:19:46+00:00,2024-12-20 23:19:46+00:00,https://github.com/apache/airflow/issues/44882,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('good first issue', ''), ('area:auth', '')]","[{'comment_id': 2539411210, 'issue_id': 2736344254, 'author': 'pierrejeambrun', 'body': 'cc: @vincbeck', 'created_at': datetime.datetime(2024, 12, 12, 16, 18, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2541234986, 'issue_id': 2736344254, 'author': 'jason810496', 'body': 'Hi @pierrejeambrun, I can work on this issue, could you assign to me? Thanks !', 'created_at': datetime.datetime(2024, 12, 13, 11, 25, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2541596878, 'issue_id': 2736344254, 'author': 'pierrejeambrun', 'body': 'Hello @jason810496, of course, thanks for your involvement, assigned.', 'created_at': datetime.datetime(2024, 12, 13, 14, 35, 56, tzinfo=datetime.timezone.utc)}]","pierrejeambrun (Issue Creator) on (2024-12-12 16:18:29 UTC): cc: @vincbeck

jason810496 (Assginee) on (2024-12-13 11:25:14 UTC): Hi @pierrejeambrun, I can work on this issue, could you assign to me? Thanks !

pierrejeambrun (Issue Creator) on (2024-12-13 14:35:56 UTC): Hello @jason810496, of course, thanks for your involvement, assigned.

"
2736332395,issue,closed,completed,FABAuthManager migrate endpoints to FastAPI,"### Description
Depends on: https://github.com/apache/airflow/issues/44847

FABAuthManager defines extra flask endpoints.

As part of the migration to FastAPI these endpoints need to be converted to FastAPI.

Endpoints located in `airflow/providers/fab/auth_manager/api_endpoints` needs to be converted to FastAPI. (Duplicated for now, legacy flask endpoint will be removed later)

",pierrejeambrun,2024-12-12 16:12:44+00:00,['andreimek'],2025-01-21 21:14:05+00:00,2025-01-21 21:14:03+00:00,https://github.com/apache/airflow/issues/44881,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('good first issue', ''), ('area:auth', ''), ('provider:fab', '')]","[{'comment_id': 2539401597, 'issue_id': 2736332395, 'author': 'pierrejeambrun', 'body': 'cc: @vincbeck', 'created_at': datetime.datetime(2024, 12, 12, 16, 14, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2539811935, 'issue_id': 2736332395, 'author': 'andreimek', 'body': 'Hi @pierrejeambrun,\nI would like to work on this issue.', 'created_at': datetime.datetime(2024, 12, 12, 19, 11, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2541149469, 'issue_id': 2736332395, 'author': 'phanikumv', 'body': 'Assigned to you @andreimek , thanks', 'created_at': datetime.datetime(2024, 12, 13, 10, 45, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2548974874, 'issue_id': 2736332395, 'author': 'jedcunningham', 'body': ""Related to the discussion in #44883, we likely don't need to actually rewrite or convert these, just wrap them with WSGIMiddleware when auth managers start exposing fastapi subapps."", 'created_at': datetime.datetime(2024, 12, 17, 16, 35, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2594378440, 'issue_id': 2736332395, 'author': 'phanikumv', 'body': 'Hey @andreimek , did you get a chance to work on this issue? Please do let us know if you need any help.', 'created_at': datetime.datetime(2025, 1, 16, 3, 6, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2605661452, 'issue_id': 2736332395, 'author': 'vincbeck', 'body': 'Should we close this one? My understanding is we will not migrate the endpoints but just embed the Flask endpoint into the Fastapi?', 'created_at': datetime.datetime(2025, 1, 21, 20, 21, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2605752376, 'issue_id': 2736332395, 'author': 'jedcunningham', 'body': 'Yeah, and #45009 is already doing that mounting.', 'created_at': datetime.datetime(2025, 1, 21, 21, 14, 3, tzinfo=datetime.timezone.utc)}]","pierrejeambrun (Issue Creator) on (2024-12-12 16:14:14 UTC): cc: @vincbeck

andreimek (Assginee) on (2024-12-12 19:11:05 UTC): Hi @pierrejeambrun,
I would like to work on this issue.

phanikumv on (2024-12-13 10:45:44 UTC): Assigned to you @andreimek , thanks

jedcunningham on (2024-12-17 16:35:59 UTC): Related to the discussion in #44883, we likely don't need to actually rewrite or convert these, just wrap them with WSGIMiddleware when auth managers start exposing fastapi subapps.

phanikumv on (2025-01-16 03:06:14 UTC): Hey @andreimek , did you get a chance to work on this issue? Please do let us know if you need any help.

vincbeck on (2025-01-21 20:21:24 UTC): Should we close this one? My understanding is we will not migrate the endpoints but just embed the Flask endpoint into the Fastapi?

jedcunningham on (2025-01-21 21:14:03 UTC): Yeah, and #45009 is already doing that mounting.

"
2736115060,issue,open,,Serialize ExternalTaskMarker dag dependency,"### Body

Related to: https://github.com/apache/airflow/issues/42367

Final Goal:
`SerializedDagModel.get_dag_dependencies()` is not returning linked objects related to `ExternalTaskMarker`. This is useful for the front-end to be able to display that a specific dag /task depends on an external task.

Consideration:
- This impacts how we serialize dags, especially the `dag_dependencies` fields.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-12-12 14:44:58+00:00,[],2024-12-12 14:47:09+00:00,,https://github.com/apache/airflow/issues/44879,"[('area:serialization', ''), ('kind:meta', 'High-level information important to the community')]",[],
2735943417,issue,closed,completed,Add MessageDeduplicationId support to SqsPublishOperator,"### Body

AWS SQS FIFO queues supports MessageDeduplicationId for deduplication of sent messages.

Currently SqsPublishOperator doesnt support, this parameter. users has to completly relay on ContentBasedDeduplication. which is not a case for all the scenarios.

https://boto3.amazonaws.com/v1/documentation/api/1.35.6/reference/services/sqs/client/send_message.html



### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",gopidesupavan,2024-12-12 13:37:40+00:00,['Prab-27'],2025-01-07 10:06:28+00:00,2025-01-07 10:06:28+00:00,https://github.com/apache/airflow/issues/44876,"[('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('kind:feature', 'Feature Requests'), ('good first issue', '')]","[{'comment_id': 2543404106, 'issue_id': 2735943417, 'author': 'Prab-27', 'body': ""@gopidesupavan , I'd like to work on this issue"", 'created_at': datetime.datetime(2024, 12, 15, 1, 32, 57, tzinfo=datetime.timezone.utc)}]","Prab-27 (Assginee) on (2024-12-15 01:32:57 UTC): @gopidesupavan , I'd like to work on this issue

"
2735500941,issue,closed,not_planned,"Airflow UI ,Trigger DAG w/config option number validation breaks when using ""JSON Schema numeric type validation options""","### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.6.2.

### What happened?

When triggering DAGs manually in the Airflow UI, while using any validation options available, it only allows the default value to be changed by +-1. 
Normal triggering with no validation options:
DAG definition:
```
@dag(
   ***
    params={""keks"": Param(0.7, type=""number"")},
   ***
)
```
Airflow UI
![Screenshot 2024-12-12 at 11 12 06](https://github.com/user-attachments/assets/138cb6ed-c692-46ed-a1c1-85b78d7d48e7)
Works without problem

Trying to trigger with validation for minimum and maximum:
DAG definition:
```
@dag(
   ***
    params={""keks"": Param(0.7, type=""number"", minimum=0, maximum=1)},
   ***
)
```
Airflow UI
![Screenshot 2024-12-12 at 11 14 29](https://github.com/user-attachments/assets/03ac8ca3-cba3-4af9-860c-45f7e7713061)
![Screenshot 2024-12-12 at 11 14 34](https://github.com/user-attachments/assets/1334c757-496e-46e6-86fc-5733a6840843)
Validation Works as intended
But if we try to input any number that isn't +-1 of the original number, the validation breaks

![Screenshot 2024-12-12 at 11 17 42](https://github.com/user-attachments/assets/f4f70a04-ccc0-4e98-a3ee-35d1b7a1f28d)


### What you think should happen instead?

The UI should behave the same as when there are no validators when using validators, allowing inputing a float number regardless of it being default + 1 or default -1.

### How to reproduce

Make a DAG with a defined parameter which is a number and contains validation.
Something like this:
```
@dag(
   ***
    params={""keks"": Param(0.7, type=""number"", minimum=0, maximum=1)},
   ***
)
```
Run Airflow and try to trigger this DAG w/confige changing the parameter to anything else that should be valid, for example a float number around the default value (which is inside the min/max validation)


### Operating System

ProductName:            macOS ProductVersion:         14.6.1 BuildVersion:           23G93

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

Every time when validators are used for a number field in params for Airflow UI

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Dome404,2024-12-12 10:30:44+00:00,[],2025-02-06 00:15:25+00:00,2025-02-06 00:15:24+00:00,https://github.com/apache/airflow/issues/44870,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('pending-response', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('affected_version:2.6', 'Issues Reported for 2.6')]","[{'comment_id': 2538492265, 'issue_id': 2735500941, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 12, 10, 30, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2538923111, 'issue_id': 2735500941, 'author': 'gopidesupavan', 'body': 'Try the latest version of airflow, this has been fixed, https://airflow.apache.org/docs/apache-airflow/stable/release_notes.html#airflow-2-10-3-2024-11-04 . see the bigfixes: `Handle ENTER key correctly in trigger form and allow manual JSON` .\r\n\r\nAre you referring same one?', 'created_at': datetime.datetime(2024, 12, 12, 13, 28, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2540915067, 'issue_id': 2735500941, 'author': 'Dome404', 'body': 'No by the looks of it that bugfix handles another issue, will update this when we get to update to the latest version and see if the bug still exists.', 'created_at': datetime.datetime(2024, 12, 13, 8, 58, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585205649, 'issue_id': 2735500941, 'author': 'jscheffl', 'body': 'I can say this problem does not appear in Airflow 2.10.4, just tested. Can you check it on your side?\r\n\r\nIf yes, then can you tell us your browser and environment details?', 'created_at': datetime.datetime(2025, 1, 11, 11, 9, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2617240475, 'issue_id': 2735500941, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2025, 1, 28, 0, 15, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2638320963, 'issue_id': 2735500941, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2025, 2, 6, 0, 15, 24, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-12 10:30:47 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

gopidesupavan on (2024-12-12 13:28:15 UTC): Try the latest version of airflow, this has been fixed, https://airflow.apache.org/docs/apache-airflow/stable/release_notes.html#airflow-2-10-3-2024-11-04 . see the bigfixes: `Handle ENTER key correctly in trigger form and allow manual JSON` .

Are you referring same one?

Dome404 (Issue Creator) on (2024-12-13 08:58:23 UTC): No by the looks of it that bugfix handles another issue, will update this when we get to update to the latest version and see if the bug still exists.

jscheffl on (2025-01-11 11:09:22 UTC): I can say this problem does not appear in Airflow 2.10.4, just tested. Can you check it on your side?

If yes, then can you tell us your browser and environment details?

github-actions[bot] on (2025-01-28 00:15:14 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2025-02-06 00:15:24 UTC): This issue has been closed because it has not received response from the issue author.

"
2735319155,issue,closed,completed,Update taskinstance clear endpoint to support mapped tasks,"### Body

~We are missing clear endpoint in the[ taskinstance api](https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html#tag/TaskInstance)~

~We need the endpoint to have same functionality as clear button in the UI.
Note: The task needs to accommodate both clearing a task and clearing a map index (when using [dynamic task mapping](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/dynamic-task-mapping.html))~

~for reference you can see previous PRs that added functionality to the Rest API: https://github.com/apache/airflow/pull/41017, https://github.com/apache/airflow/pull/39138~

Update (after discussing in comments)
1. Move clear endpoint from Dag to TaskInstance
2. Add support for mapped index 

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2024-12-12 09:14:08+00:00,['vatsrahul1001'],2025-01-27 11:35:06+00:00,2025-01-27 11:35:05+00:00,https://github.com/apache/airflow/issues/44867,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('good first issue', ''), ('area:core', ''), ('area:dynamic-task-mapping', 'AIP-42')]","[{'comment_id': 2538544239, 'issue_id': 2735319155, 'author': 'vatsrahul1001', 'body': 'I can work on this. Assigning it to myself.', 'created_at': datetime.datetime(2024, 12, 12, 10, 53, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2538599826, 'issue_id': 2735319155, 'author': 'vatsrahul1001', 'body': '@eladkal Should I also create an Issue to migrate this to FastAPI as well?', 'created_at': datetime.datetime(2024, 12, 12, 11, 18, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2538932935, 'issue_id': 2735319155, 'author': 'eladkal', 'body': 'No need for another issue. You can simply start another PR directly', 'created_at': datetime.datetime(2024, 12, 12, 13, 31, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2548587582, 'issue_id': 2735319155, 'author': 'vatsrahul1001', 'body': '@eladkal We do have taskinstance clear endpoint implementation already [here](https://github.com/apache/airflow/blob/main/airflow/api_connexion/endpoints/task_instance_endpoint.py#L447). As per openapi spec its under DAG endpoint [here](http://localhost:28080/redoc#tag/DAG/operation/post_clear_task_instances).\r\n\r\n\r\n<img width=""1506"" alt=""image"" src=""https://github.com/user-attachments/assets/d560d407-a0db-479b-8d7b-25bed48943a8"" />\r\n\r\nIn FastAPI we have correctly added this under taskinstances.\r\n<img width=""1475"" alt=""image"" src=""https://github.com/user-attachments/assets/9dc2d9ca-2686-4153-aa86-0d6f04417786"" />\r\n\r\nDo let me know if this is what you wanted in implementation, if yes should I move this endpoint from DAG to TaskInstance in legacy API?', 'created_at': datetime.datetime(2024, 12, 17, 14, 19, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2548705297, 'issue_id': 2735319155, 'author': 'eladkal', 'body': ""> Do let me know if this is what you wanted in implementation, if yes should I move this endpoint from DAG to TaskInstance in legacy API?\r\n\r\nIn my perspective yes. I expect to find clear action on task instance endpoints but I think it's a good idea to track the PR added it and see if there was discussion around why it's placed there. We may be missing something.\r\n\r\nBy the way, it seems like with current endpoint you are not able to clear a specific map index in a specific task instance so I think we need to add support for this."", 'created_at': datetime.datetime(2024, 12, 17, 15, 8, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551525213, 'issue_id': 2735319155, 'author': 'pierrejeambrun', 'body': ""Maybe we should close this one, or update it in favor of a feature request for a more fined grained clearing capabilities on 'mapped task instances' ?\r\n\r\nAs I understand the endpoint exists, it just needs improvement ?"", 'created_at': datetime.datetime(2024, 12, 18, 14, 53, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551772437, 'issue_id': 2735319155, 'author': 'eladkal', 'body': '> Maybe we should close this one, or update it in favor of a feature request \r\n\r\n~In the wrong place. Its not intuitive to find it under DAGs. So this issue is about moving it and refactor it to support mapped tasks. @vatsrahul1001 do you need me to update issue description?~\r\n\r\nUpdated title and description', 'created_at': datetime.datetime(2024, 12, 18, 16, 29, 33, tzinfo=datetime.timezone.utc)}]","vatsrahul1001 (Assginee) on (2024-12-12 10:53:32 UTC): I can work on this. Assigning it to myself.

vatsrahul1001 (Assginee) on (2024-12-12 11:18:09 UTC): @eladkal Should I also create an Issue to migrate this to FastAPI as well?

eladkal (Issue Creator) on (2024-12-12 13:31:13 UTC): No need for another issue. You can simply start another PR directly

vatsrahul1001 (Assginee) on (2024-12-17 14:19:48 UTC): @eladkal We do have taskinstance clear endpoint implementation already [here](https://github.com/apache/airflow/blob/main/airflow/api_connexion/endpoints/task_instance_endpoint.py#L447). As per openapi spec its under DAG endpoint [here](http://localhost:28080/redoc#tag/DAG/operation/post_clear_task_instances).


<img width=""1506"" alt=""image"" src=""https://github.com/user-attachments/assets/d560d407-a0db-479b-8d7b-25bed48943a8"" />

In FastAPI we have correctly added this under taskinstances.
<img width=""1475"" alt=""image"" src=""https://github.com/user-attachments/assets/9dc2d9ca-2686-4153-aa86-0d6f04417786"" />

Do let me know if this is what you wanted in implementation, if yes should I move this endpoint from DAG to TaskInstance in legacy API?

eladkal (Issue Creator) on (2024-12-17 15:08:54 UTC): In my perspective yes. I expect to find clear action on task instance endpoints but I think it's a good idea to track the PR added it and see if there was discussion around why it's placed there. We may be missing something.

By the way, it seems like with current endpoint you are not able to clear a specific map index in a specific task instance so I think we need to add support for this.

pierrejeambrun on (2024-12-18 14:53:21 UTC): Maybe we should close this one, or update it in favor of a feature request for a more fined grained clearing capabilities on 'mapped task instances' ?

As I understand the endpoint exists, it just needs improvement ?

eladkal (Issue Creator) on (2024-12-18 16:29:33 UTC): ~In the wrong place. Its not intuitive to find it under DAGs. So this issue is about moving it and refactor it to support mapped tasks. @vatsrahul1001 do you need me to update issue description?~

Updated title and description

"
2734983326,issue,open,,Add support for default view in dag page,"### Description

In the legacy UI on selecting the dag from home page the grid view was open by default and the `dag_default_view` from airflow configuration can be used to set the default view. In the new UI it seems we need multiple clicks to get to the dagrun or taskinstance while legacy view was one click. The `dag_default_view` configuration should be used to display the configured view by default when dag page is selected probably by setting the `modal` query parameter with the `dag_default_view` value.

I am not sure if it's tracked but thought to create this issue to track this. Please close this if it's tracked elsewhere. 

### Use case/motivation

Users should have option to have a default view open to avoid multiple clicks to get to a dagrun or taskinstance.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",tirkarthi,2024-12-12 06:27:02+00:00,[],2024-12-12 06:28:51+00:00,,https://github.com/apache/airflow/issues/44864,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]","[{'comment_id': 2537923333, 'issue_id': 2734983326, 'author': 'tirkarthi', 'body': 'cc: @jscheffl as per comment https://github.com/apache/airflow/pull/44828#pullrequestreview-2493830331', 'created_at': datetime.datetime(2024, 12, 12, 6, 28, 22, tzinfo=datetime.timezone.utc)}]","tirkarthi (Issue Creator) on (2024-12-12 06:28:22 UTC): cc: @jscheffl as per comment https://github.com/apache/airflow/pull/44828#pullrequestreview-2493830331

"
2734221135,issue,closed,completed,AIP-38 | Clear Task Instance,Like #44859 We should add a button to clear a Task Instance on the Task Instance Details page with a modal to display the dry run before the user can confirm the change.,bbovenzi,2024-12-11 23:16:24+00:00,[],2025-01-11 23:56:18+00:00,2025-01-11 23:56:18+00:00,https://github.com/apache/airflow/issues/44860,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]",[],
2734216806,issue,closed,completed,AIP-38 | Add clear dag run action to Dag Run Details,"Like how the Dag Details page has a Trigger button in the top right of its header, the Dag Run Details page should have a button to Clear the Dag Run. When clicking on the button it should open a modal and fire off a dry run of clearing the run. Then the modal should display the task instances subject to change before the user can click submit.",bbovenzi,2024-12-11 23:13:04+00:00,[],2025-01-06 13:19:43+00:00,2025-01-06 13:19:43+00:00,https://github.com/apache/airflow/issues/44859,"[('area:UI', 'Related to UI/UX. For Frontend Developers.')]",[],
2734214641,issue,closed,completed,AIP-38 | Use Dag params to build an advanced Trigger Dag Run form,"To Do after #44857 

Update the trigger config from a free-form json editor by using the dag's params in `/dag/details`and dynamically generating form components.",bbovenzi,2024-12-11 23:11:05+00:00,[],2025-01-29 15:44:53+00:00,2025-01-29 15:44:53+00:00,https://github.com/apache/airflow/issues/44858,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2537391080, 'issue_id': 2734214641, 'author': 'bbovenzi', 'body': '@shubhamraj-git', 'created_at': datetime.datetime(2024, 12, 11, 23, 12, 29, tzinfo=datetime.timezone.utc)}]","bbovenzi (Issue Creator) on (2024-12-11 23:12:29 UTC): @shubhamraj-git

"
2734211732,issue,closed,completed,AIP-38 | Connect Trigger form to rest API,"We built a modal to trigger a dag, but the API endpoint wasn't ready yet. It now is. We should now wire the form up to the trigger dag run request.",bbovenzi,2024-12-11 23:08:23+00:00,[],2024-12-16 20:31:56+00:00,2024-12-16 20:31:56+00:00,https://github.com/apache/airflow/issues/44857,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]","[{'comment_id': 2537387904, 'issue_id': 2734211732, 'author': 'bbovenzi', 'body': '@shubhamraj-git', 'created_at': datetime.datetime(2024, 12, 11, 23, 9, 42, tzinfo=datetime.timezone.utc)}]","bbovenzi (Issue Creator) on (2024-12-11 23:09:42 UTC): @shubhamraj-git

"
2733610652,issue,closed,completed,Support JWT token authentication in FAB auth manager,"### Description

In Airflow 3, JWT token based authentication is used to authenticate front-end with APIs. FAB auth manager needs to support it so that it can be used in Airflow 3.

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vincbeck,2024-12-11 17:49:23+00:00,['vincbeck'],2025-01-21 20:20:37+00:00,2025-01-21 20:20:22+00:00,https://github.com/apache/airflow/issues/44849,"[('kind:feature', 'Feature Requests'), ('area:auth', '')]","[{'comment_id': 2536703256, 'issue_id': 2733610652, 'author': 'vincbeck', 'body': '@pierrejeambrun', 'created_at': datetime.datetime(2024, 12, 11, 17, 54, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2539369218, 'issue_id': 2733610652, 'author': 'pierrejeambrun', 'body': 'Nice', 'created_at': datetime.datetime(2024, 12, 12, 16, 1, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2605659647, 'issue_id': 2733610652, 'author': 'vincbeck', 'body': 'Done in #44885', 'created_at': datetime.datetime(2025, 1, 21, 20, 20, 23, tzinfo=datetime.timezone.utc)}]","vincbeck (Issue Creator) on (2024-12-11 17:54:11 UTC): @pierrejeambrun

pierrejeambrun on (2024-12-12 16:01:11 UTC): Nice

vincbeck (Issue Creator) on (2025-01-21 20:20:23 UTC): Done in #44885

"
2733599728,issue,closed,completed,Pass down the previous page (referrer) in the URL after logging in successfully with simple auth manager,"### Description

When using the FAB auth manager, when logging in, the user gets redirected automatically to the page they were before logging in. This same mechanism needs to be implemented in the simple auth manager. This is particularly important because this feature is needed to test the authentication and authorization flow with simple auth manager in Airflow 3 (more details below).

After logging in successfully with the simple auth manager, the JWT token is returned as part of the URL (e.g. `<airflow_endpoint>/home?token=....`). To start testing the authorization in Airflow 3, this redirection needs to happen to Airflow 3 UI homepage so that the new font-end can handle this token (and save it locally).

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vincbeck,2024-12-11 17:43:35+00:00,[],2024-12-12 14:59:58+00:00,2024-12-12 14:59:58+00:00,https://github.com/apache/airflow/issues/44848,"[('kind:feature', 'Feature Requests'), ('area:auth', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2536703080, 'issue_id': 2733599728, 'author': 'vincbeck', 'body': '@pierrejeambrun', 'created_at': datetime.datetime(2024, 12, 11, 17, 54, 5, tzinfo=datetime.timezone.utc)}]","vincbeck (Issue Creator) on (2024-12-11 17:54:05 UTC): @pierrejeambrun

"
2733568498,issue,closed,completed,Update auth manager interface to extend fast APIs,"### Description

Today, it is possible to extend the Flask Rest API using `get_api_endpoints` in the auth manager interface. In Airflow 3, we will no longer use Flask and use Fast API instead. The auth manager interface needs to be updated to add an option so that the auth manager can extend the Fast API defined in Airflow.

### Use case/motivation

Example of usage: the FAB auth manager needs to extend the Airflow API to add user and roles related API.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vincbeck,2024-12-11 17:29:57+00:00,['vincbeck'],2025-01-06 19:58:33+00:00,2025-01-06 19:58:33+00:00,https://github.com/apache/airflow/issues/44847,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('area:auth', '')]","[{'comment_id': 2536642549, 'issue_id': 2733568498, 'author': 'vincbeck', 'body': '@pierrejeambrun', 'created_at': datetime.datetime(2024, 12, 11, 17, 30, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2536681290, 'issue_id': 2733568498, 'author': 'pierrejeambrun', 'body': 'Thanks', 'created_at': datetime.datetime(2024, 12, 11, 17, 43, 1, tzinfo=datetime.timezone.utc)}]","vincbeck (Issue Creator) on (2024-12-11 17:30:10 UTC): @pierrejeambrun

pierrejeambrun on (2024-12-11 17:43:01 UTC): Thanks

"
2733451511,issue,closed,completed,Helm Chart: Add startupProbe to flower deployment,"### Description

We've run into the issue of the flower pod not being quite ready by the time the readinessProbe starts firing leading to a crashbackoffloop. A startupProbe using the same check would be fairly painless and without significant side effects.

### Use case/motivation

We would like to ensure that slow starting flower pods in our Airflow Helm deployments come into ready state cleanly. 

### Related issues

#33099 added startup probes to the scheduler and webserver. 

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",MWatter,2024-12-11 16:36:36+00:00,[],2025-01-27 13:39:39+00:00,2025-01-27 13:39:39+00:00,https://github.com/apache/airflow/issues/44846,"[('kind:feature', 'Feature Requests'), ('area:helm-chart', 'Airflow Helm Chart')]","[{'comment_id': 2536502852, 'issue_id': 2733451511, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 11, 16, 36, 40, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-11 16:36:40 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2733309167,issue,closed,completed,CVE-2024-49767,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

CVE-2024-49767 looks similar to CVE-2023-46136:

https://github.com/apache/airflow/issues/36915

Is it also true for this CVE that ""Airflow is not likely vulnerable""?


### What you think should happen instead?

_No response_

### How to reproduce

NA

### Operating System

All

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",lewijw,2024-12-11 15:36:01+00:00,[],2024-12-12 06:36:56+00:00,2024-12-12 06:36:56+00:00,https://github.com/apache/airflow/issues/44844,"[('kind:bug', 'This is a clearly a bug'), ('security', 'Security issues that must be fixed'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2536551039, 'issue_id': 2733309167, 'author': 'amoghrajesh', 'body': 'Please use the security policy to report CVEs and any security related issues: https://github.com/apache/airflow?tab=security-ov-file#readme', 'created_at': datetime.datetime(2024, 12, 11, 16, 57, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2536576623, 'issue_id': 2733309167, 'author': 'lewijw', 'body': 'The CVE is public.  Do you really want everyone that runs a scan on Airflow to contact the security email address to ask this question?', 'created_at': datetime.datetime(2024, 12, 11, 17, 6, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2537929787, 'issue_id': 2733309167, 'author': 'eladkal', 'body': ""> Do you really want everyone that runs a scan on Airflow to contact the security email address to ask this question?\r\n\r\nOur policy states that we do not accept reports of automated scans. If you believe Airflow is affected by any security issue you should report to the security email address with clear explnation of what the risk is and how it can be exploited. If you can't specify how it can be exploited the report will be automatically rejected. There are dozens of automated tools that generated many false report and there are many people who reports thoughts/concerns/questions. As open source project that is consistent mostly with volunteers we can not triage and handle such traffic volume so we expect the reporter to do the extra mile and verify that the problem being reported is real.\r\n\r\nYou are also very welcome to raise your thoughts on the poicy itself with the same email if you believe it should change and can offer reasoning for it."", 'created_at': datetime.datetime(2024, 12, 12, 6, 33, 14, tzinfo=datetime.timezone.utc)}]","amoghrajesh on (2024-12-11 16:57:04 UTC): Please use the security policy to report CVEs and any security related issues: https://github.com/apache/airflow?tab=security-ov-file#readme

lewijw (Issue Creator) on (2024-12-11 17:06:31 UTC): The CVE is public.  Do you really want everyone that runs a scan on Airflow to contact the security email address to ask this question?

eladkal on (2024-12-12 06:33:14 UTC): Our policy states that we do not accept reports of automated scans. If you believe Airflow is affected by any security issue you should report to the security email address with clear explnation of what the risk is and how it can be exploited. If you can't specify how it can be exploited the report will be automatically rejected. There are dozens of automated tools that generated many false report and there are many people who reports thoughts/concerns/questions. As open source project that is consistent mostly with volunteers we can not triage and handle such traffic volume so we expect the reporter to do the extra mile and verify that the problem being reported is real.

You are also very welcome to raise your thoughts on the poicy itself with the same email if you believe it should change and can offer reasoning for it.

"
2732375812,issue,closed,completed,MWAA: psycopg2.OperationalError: SSL connection has been closed unexpectedly,"### Apache Airflow version

2.10.1,  2.8.1

### If ""Other Airflow 2 version"" selected, which one?

2.8.1

### What happened?

```
[2024-10-08, 16:38:05 UTC] {{taskinstance.py:2698}} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1094, in _commit_impl
    self.engine.dialect.do_commit(self.connection)
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/default.py"", line 686, in do_commit
    dbapi_connection.commit()
psycopg2.OperationalError: SSL connection has been closed unexpectedly


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 434, in _execute_task
    with create_session() as session:
  File ""/usr/local/lib/python3.11/contextlib.py"", line 144, in __exit__
    next(self.gen)
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py"", line 39, in create_session
    session.commit()
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py"", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py"", line 839, in commit
    trans.commit()
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 2469, in commit
    self._do_commit()
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 2659, in _do_commit
    self._connection_commit_impl()
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 2630, in _connection_commit_impl
    self.connection._commit_impl()
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1096, in _commit_impl
    self._handle_dbapi_exception(e, None, None, None, None)
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 2134, in _handle_dbapi_exception
    util.raise_(
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/sqlalchemy/util/compat.py"", line 211, in raise_
    raise exception
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1094, in _commit_impl
    self.engine.dialect.do_commit(self.connection)
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/default.py"", line 686, in do_commit
    dbapi_connection.commit()
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) SSL connection has been closed unexpectedly

(Background on this error at: https://sqlalche.me/e/14/e3q8)
```


Airflow task itself was successful, but it failed on exit and only after ~30 minutes of runtime, if task runs lower than this time, there are no issues. **If i run same task on 2.4.3, all good.**

### What you think should happen instead?

Task finished without any errros.

### How to reproduce

MWAA: 2.8.1, 2.10.1.
Run successful task for 30+ minutes.
More details: https://repost.aws/questions/QU9VhJrACSSdy7zJku4pVO9Q/mwaa-psycopg2-operationalerror-ssl-connection-has-been-closed-unexpectedly

### Operating System

Aws MWAA

### Versions of Apache Airflow Providers

```
apache-airflow-providers-amazon	8.16.0
apache-airflow-providers-celery	3.5.1
apache-airflow-providers-common-io	1.2.0
apache-airflow-providers-common-sql	1.10.0
apache-airflow-providers-databricks	6.0.0
apache-airflow-providers-ftp	3.7.0
apache-airflow-providers-google	10.13.1
apache-airflow-providers-http	4.8.0
apache-airflow-providers-imap	3.5.0
apache-airflow-providers-postgres	5.10.0
apache-airflow-providers-sendgrid	3.4.0
apache-airflow-providers-sftp	4.8.1
apache-airflow-providers-slack	8.5.1
apache-airflow-providers-sqlite	3.7.0
apache-airflow-providers-ssh	3.10.0
apache-airflow-providers-trino	5.6.0
astronomer-cosmos	1.7.1
```

### Deployment

Amazon (AWS) MWAA

### Deployment details

Version: 2.8.1+
Size: mw1.large


### Anything else?

During the investigation, I checked:

1. new mwaa cluster.
2. latest mwaa version.
3. this options:
```
database.sql_alchemy_pool_size 20 
database.sql_alchemy_pool_recycle 300
database.sql_alchemy_max_overflow 20
database.sql_alchemy_pool_pre_ping true
```

sql-alchemy-connect-args and all others related to engine + keepalive timeouts, it looks like mwaa blocked this parameter, the issue reported in slack channel → https://apache-airflow.slack.com/archives/CCRR5EBA7/p1733820955101179
overall issue report → https://apache-airflow.slack.com/archives/CCRR5EBA7/p1732187849345499
repost question → https://repost.aws/questions/QU9VhJrACSSdy7zJku4pVO9Q/mwaa-psycopg2-operationalerror-ssl-connection-has-been-closed-unexpectedly

Here is a rds dump for keepalive 2.4.3 and 2.8.1(its the same  on both envs):
```
[2024-12-11, 09:23:19 UTC] {{logging_mixin.py:137}} INFO - tcp_keepalives_count: 2
[2024-12-11, 09:23:19 UTC] {{logging_mixin.py:137}} INFO - tcp_keepalives_idle: 300
[2024-12-11, 09:23:19 UTC] {{logging_mixin.py:137}} INFO - tcp_keepalives_interval: 30
```

     


### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",sweetpythoncode,2024-12-11 09:36:16+00:00,[],2024-12-11 13:14:21+00:00,2024-12-11 13:14:21+00:00,https://github.com/apache/airflow/issues/44837,"[('kind:bug', 'This is a clearly a bug'), ('area:MetaDB', 'Meta Database related issues.'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2535315013, 'issue_id': 2732375812, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 11, 9, 36, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2535721454, 'issue_id': 2732375812, 'author': 'eladkal', 'body': ""This seems to be MWAA specific issue. You are discussing about specific settings with specific environment.\r\nI'd suggest to contact MWAA support.\r\n\r\nIf you have reproduce steps from scratch on Airflow main branch with code example that worked on pervious version but isn't working now we can investigate, until then it's not clear if there is a bug or it's just something with MWAA or your deployment configuration."", 'created_at': datetime.datetime(2024, 12, 11, 11, 47, 30, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-11 09:36:20 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2024-12-11 11:47:30 UTC): This seems to be MWAA specific issue. You are discussing about specific settings with specific environment.
I'd suggest to contact MWAA support.

If you have reproduce steps from scratch on Airflow main branch with code example that worked on pervious version but isn't working now we can investigate, until then it's not clear if there is a bug or it's just something with MWAA or your deployment configuration.

"
2731964381,issue,closed,completed,Support AssetRef access to inlet/outlet events and int access to outlet event,"### Description

as title

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-12-11 06:40:21+00:00,['uranusjr'],2025-01-13 23:39:05+00:00,2025-01-13 23:39:04+00:00,https://github.com/apache/airflow/issues/44834,"[('kind:feature', 'Feature Requests'), ('area:datasets', 'Issues related to the datasets feature')]","[{'comment_id': 2588464460, 'issue_id': 2731964381, 'author': 'Lee-W', 'body': 'I think this one has been addressed by https://github.com/apache/airflow/pull/45028. Close this one', 'created_at': datetime.datetime(2025, 1, 13, 23, 39, 5, tzinfo=datetime.timezone.utc)}]","Lee-W (Issue Creator) on (2025-01-13 23:39:05 UTC): I think this one has been addressed by https://github.com/apache/airflow/pull/45028. Close this one

"
2730555037,issue,closed,not_planned,Allow use of callable for `template_fields` for TaskFlow API tasks,"### Description

PR #37028 in Airflow 2.10.0 introduced the ability to pass a Python callable to templateable fields, instead of a Jinja templated string:

https://www.astronomer.io/docs/learn/templating/#use-a-python-callable-for-template-fields
> In Airflow 2.10+ it is possible to pass a Python callable to templateable fields. This is especially useful when the parameter value is created using complex operations that might not be possible or are hard to read in Jinja.

However, this feature doesn't seem to work for `@task.*` decorated tasks.

IMO it doesn't make sense to introduce a new feature that only works with the old style of tasks.

This feature is to make the new Python callable templating work for `@task` decorated TaskFlow API tasks.

### Use case/motivation

I want to specify a Python function for a templated field, instead of a Jinja templated string, for the reasons mentioned in the linked PR. For complex logic, it's much easier to implement and read Jinja rendering functions than a Jinja templated string.

### Related issues

https://github.com/apache/airflow/pull/35844
https://github.com/apache/airflow/pull/37028

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",matthewblock,2024-12-10 16:10:14+00:00,[],2024-12-12 17:30:51+00:00,2024-12-11 10:52:37+00:00,https://github.com/apache/airflow/issues/44819,"[('kind:feature', 'Feature Requests'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2535223998, 'issue_id': 2730555037, 'author': 'topherinternational', 'body': ""Could you provide a code snippet of a @task-decorated function where the behavior isn't working as you expect?"", 'created_at': datetime.datetime(2024, 12, 11, 8, 54, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2535512406, 'issue_id': 2730555037, 'author': 'potiuk', 'body': 'Task flow operators do not have templateable field at all - so it does not make sense to add it there. And in task flow task you can simply call a Python function from inside the decorated function - there is no need to pass callable to the taskflow parameter if you can call them directly. This request makes no sense.', 'created_at': datetime.datetime(2024, 12, 11, 10, 52, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2536407180, 'issue_id': 2730555037, 'author': 'matthewblock', 'body': '> Task flow operators do not have templateable field at all\r\n\r\nMaybe my terminology is off, but if I decorate `PythonOperator`, the following are still templateable fields, because the user can specify a Jinja template string and it will get rendered by the time the task executes.\r\n\r\n- `op_kwargs`\r\n- `op_args`\r\n- `templates_dict`\r\n\r\nThe first two technically can\'t be specified in a decorated task, instead Airflow is rolling up any ""extra"" args or kwargs the user specifies when they instantiate a task, but they still get rendered.\r\n\r\nHere is a test DAG that demonstrates what I\'m talking about - See the last task, which prints the un-rendered function.\r\n\r\n```\r\nfrom datetime import datetime, timedelta\r\n\r\nfrom airflow.decorators import dag, task\r\n\r\ndefault_args = {\r\n    ""owner"": ""airflow"",\r\n    ""depends_on_past"": False,\r\n    ""retry_delay"": timedelta(minutes=1),\r\n    ""retries"": 0,\r\n}\r\n\r\n@dag(\r\n    default_args=default_args,\r\n    start_date=datetime(2000, 1, 1),\r\n    catchup=False,\r\n    schedule=None,\r\n)\r\ndef test_jinja_functions_dag():\r\n    def get_foo(context, jinja_env):\r\n        return ""bar""\r\n\r\n    @task\r\n    def args_kwargs_task(*args, **kwargs):\r\n        print(args) # Prints tuple containing current date (ds)\r\n        print(kwargs[""foo""]) # Prints Task Instance object (ti)\r\n\r\n    @task(templates_dict={""some_thing"": ""{{ ds }}""})\r\n    def templates_dict_class(**kwargs):\r\n        print(kwargs[""templates_dict""][""some_thing""]) # Prints current date (ds)\r\n\r\n    @task\r\n    def direct_kwarg_task(foo):\r\n        print(foo) # Prints Task Instance object (ti)\r\n\r\n    @task\r\n    def direct_kwarg_no_rendering_task(foo):\r\n        print(foo) # Prints function object get_foo, NOT ""bar""\r\n\r\n    (\r\n        args_kwargs_task(""{{ ds }}"", foo=""{{ ti }}""),\r\n        templates_dict_class(),\r\n        direct_kwarg_task(foo=""{{ ti }}""),\r\n        direct_kwarg_no_rendering_task(foo=get_foo),\r\n    )\r\n\r\ntest_jinja_functions_dag()\r\n```\r\n\r\n\r\n> And in task flow task you can simply call a Python function from inside the decorated function - there is no need to pass callable to the taskflow parameter if you can call them directly.\r\n\r\nYou can also do this in your `python_callable` in a non-decorated `PythonOperator` task 🤷\u200d♂️ but that\'s not the point. As a user, it doesn\'t make sense why I\'m encouraged to use TaskFlow API going forward, but I can only use a new syntax introduced in 2.10.0 using the old style PythonOperator.\r\n\r\nWhen I was developing my DAG, I was surprised that my template function didn\'t actually get called; my task didn\'t fail because my function outputted a string and instead the name of the function ```<function test_jinja_functions_dag.<***s>.get_foo at 0xffffacac0a40>``` was getting passed everywhere.\r\n\r\nAt the least I think the documentation should be updated.\r\n\r\nSee references:\r\n- https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/operators.html#jinja-templating\r\n- https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html#using-templates-in-decorated-tasks\r\n- https://stackoverflow.com/questions/70134140/how-can-i-pass-jinja2-templated-parameter-to-a-task-using-the-taskflow-api\r\n- https://stackoverflow.com/questions/79055995/airflow-python-operator-typeerror-got-multiple-values-for-keyword-argument-op', 'created_at': datetime.datetime(2024, 12, 11, 16, 2, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2539580141, 'issue_id': 2730555037, 'author': 'matthewblock', 'body': ""@potiuk can I open this as a discussion? Maybe this doesn't warrant a feature request but IMO this behavior is unexpected"", 'created_at': datetime.datetime(2024, 12, 12, 17, 30, 50, tzinfo=datetime.timezone.utc)}]","topherinternational on (2024-12-11 08:54:45 UTC): Could you provide a code snippet of a @task-decorated function where the behavior isn't working as you expect?

potiuk on (2024-12-11 10:52:29 UTC): Task flow operators do not have templateable field at all - so it does not make sense to add it there. And in task flow task you can simply call a Python function from inside the decorated function - there is no need to pass callable to the taskflow parameter if you can call them directly. This request makes no sense.

matthewblock (Issue Creator) on (2024-12-11 16:02:39 UTC): Maybe my terminology is off, but if I decorate `PythonOperator`, the following are still templateable fields, because the user can specify a Jinja template string and it will get rendered by the time the task executes.

- `op_kwargs`
- `op_args`
- `templates_dict`

The first two technically can't be specified in a decorated task, instead Airflow is rolling up any ""extra"" args or kwargs the user specifies when they instantiate a task, but they still get rendered.

Here is a test DAG that demonstrates what I'm talking about - See the last task, which prints the un-rendered function.

```
from datetime import datetime, timedelta

from airflow.decorators import dag, task

default_args = {
    ""owner"": ""airflow"",
    ""depends_on_past"": False,
    ""retry_delay"": timedelta(minutes=1),
    ""retries"": 0,
}

@dag(
    default_args=default_args,
    start_date=datetime(2000, 1, 1),
    catchup=False,
    schedule=None,
)
def test_jinja_functions_dag():
    def get_foo(context, jinja_env):
        return ""bar""

    @task
    def args_kwargs_task(*args, **kwargs):
        print(args) # Prints tuple containing current date (ds)
        print(kwargs[""foo""]) # Prints Task Instance object (ti)

    @task(templates_dict={""some_thing"": ""{{ ds }}""})
    def templates_dict_class(**kwargs):
        print(kwargs[""templates_dict""][""some_thing""]) # Prints current date (ds)

    @task
    def direct_kwarg_task(foo):
        print(foo) # Prints Task Instance object (ti)

    @task
    def direct_kwarg_no_rendering_task(foo):
        print(foo) # Prints function object get_foo, NOT ""bar""

    (
        args_kwargs_task(""{{ ds }}"", foo=""{{ ti }}""),
        templates_dict_class(),
        direct_kwarg_task(foo=""{{ ti }}""),
        direct_kwarg_no_rendering_task(foo=get_foo),
    )

test_jinja_functions_dag()
```



You can also do this in your `python_callable` in a non-decorated `PythonOperator` task 🤷‍♂️ but that's not the point. As a user, it doesn't make sense why I'm encouraged to use TaskFlow API going forward, but I can only use a new syntax introduced in 2.10.0 using the old style PythonOperator.

When I was developing my DAG, I was surprised that my template function didn't actually get called; my task didn't fail because my function outputted a string and instead the name of the function ```<function test_jinja_functions_dag.<***s>.get_foo at 0xffffacac0a40>``` was getting passed everywhere.

At the least I think the documentation should be updated.

See references:
- https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/operators.html#jinja-templating
- https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html#using-templates-in-decorated-tasks
- https://stackoverflow.com/questions/70134140/how-can-i-pass-jinja2-templated-parameter-to-a-task-using-the-taskflow-api
- https://stackoverflow.com/questions/79055995/airflow-python-operator-typeerror-got-multiple-values-for-keyword-argument-op

matthewblock (Issue Creator) on (2024-12-12 17:30:50 UTC): @potiuk can I open this as a discussion? Maybe this doesn't warrant a feature request but IMO this behavior is unexpected

"
2730069549,issue,closed,completed,Status of testing of Apache Airflow 2.10.4rc1,"### Body

We are kindly requesting that contributors to [Apache Airflow RC 2.10.4rc1](https://pypi.org/project/apache-airflow/2.10.4rc1/) help test the RC.

Please let us know by commenting if the issue is addressed in the latest RC.

- [ ] [Check pool_slots on partial task import instead of execution (#39724) (#42693)](https://github.com/apache/airflow/pull/42693): @karenbraganz
- [ ] [Double-check TaskInstance state if it differs from the Executor state. (#43063)](https://github.com/apache/airflow/pull/43063): @antonysouthworth-halter
- [x] [Allow Utkarsh to run dockerhub image release job (#43588) (#43589)](https://github.com/apache/airflow/pull/43589): @utkarsharma2
- [x] [ Fix Try Selector in Mapped Tasks also on Index 0 (#43590) Backport (#43591)](https://github.com/apache/airflow/pull/43591): @jscheffl
     Linked issues:
     - [Fix Try Selector in Mapped Tasks also on Index 0 (#43590)](https://github.com/apache/airflow/pull/43590)
- [x] [[BACKPORT] Complete automation of version replacement pre-commit for pip and uv … (#43623)](https://github.com/apache/airflow/pull/43623): @potiuk @kaxil
     Linked issues:
     - [Bump ``pip`` to ``24.2`` (#43197)](https://github.com/apache/airflow/pull/43197)
     - [Bump ``uv`` to ``0.4.24`` (#43135)](https://github.com/apache/airflow/pull/43135)
- [x] [[BACKPORT] Allow to switch breeze to use uv internally to create virtualenvs (#43587) (#43624)](https://github.com/apache/airflow/pull/43624): @potiuk
- [x] [[BACKPORT] Remove root warning in image used to build packages in CI (#43597)  (#43625)](https://github.com/apache/airflow/pull/43625): @potiuk
- [x] [ Fix venv numpy example which needs to be 1.26 at least to be working in Python 3.12 (#43653) (#43659)](https://github.com/apache/airflow/pull/43659): @jscheffl
     Linked issues:
     - [Fix venv numpy example which needs to be 1.26 at least to be working in Python 3.12 (#43653)](https://github.com/apache/airflow/pull/43653)
- [x] [[BACKPORT] Fix reproducibility of prepared provider packages (fix flit frontend)… (#43687)](https://github.com/apache/airflow/pull/43687): @potiuk
- [x] [[BACKPORT] Detect situation where Breeze is installed with both pipx and uv (#43… (#43695)](https://github.com/apache/airflow/pull/43695): @potiuk
- [ ] [Disable XCom list ordering by execution_date (#43680) (#43696)](https://github.com/apache/airflow/pull/43696): @pierrejeambrun
- [x] [[BACKPORT] Handle FileNotFound Error returned by missing uv or pipx (#43714) (#43715)](https://github.com/apache/airflow/pull/43715): @potiuk
- [x] [Remove note about MySQL 5 (#43729)](https://github.com/apache/airflow/pull/43729): @eladkal
- [x] [[BACKPORT (Modified)] Prevent using trigger_rule=""always"" in a dynamic mapped task (#43368) (#43810)](https://github.com/apache/airflow/pull/43810): @shahar1
     Linked issues:
     - [Prevent using `trigger_rule=TriggerRule.ALWAYS` in a task-generated mapping within mapped task groups (#43368)](https://github.com/apache/airflow/pull/43368)
- [x] [#43252 Disable extra links button if link is null or empty (#43844) (#43851)](https://github.com/apache/airflow/pull/43851): @jscheffl @enisnazif
     Linked issues:
     - [#43252 Disable extra links button if link is null or empty (#43844)](https://github.com/apache/airflow/pull/43844)
- [x] [ Correct mime-type in OpenAPI spec (#43879) (#43901)](https://github.com/apache/airflow/pull/43901): @jscheffl @xitep
     Linked issues:
     - [Correct mime-type in OpenAPI spec (#43879)](https://github.com/apache/airflow/pull/43879)
- [ ] [Fix duplication of Task tries in the UI (#43891) (#43950)](https://github.com/apache/airflow/pull/43950): @ephraimbuddy @tirkarthi
     Linked issues:
     - [Duplicate entries in API response when TaskInstanceHistory and TaskInstance have same maximum try number (#41765)](https://github.com/apache/airflow/issues/41765)
- [x] [ Ensure priority weight is capped at 32-bit integer to prevent roll-over (#43611) (#44045)](https://github.com/apache/airflow/pull/44045): @jscheffl
- [x] [ Log message source details are grouped (#43681) (#44070)](https://github.com/apache/airflow/pull/44070): @jscheffl
- [x] [[v2-10-test] Add .dockerignore to target workflow override (#43885) (#44103)](https://github.com/apache/airflow/pull/44103): @potiuk
- [ ] [get_task_instance_try_details API returns TaskInstanceHistory schema … (#44133)](https://github.com/apache/airflow/pull/44133): @pierrejeambrun
- [x] [[v2-10-test] Re-queue tassk when they are stuck in queued (#43520) (#44158)](https://github.com/apache/airflow/pull/44158): @jscheffl @dimberman
     Linked issues:
     - [Allow for retry when tasks are stuck in queued (#43520)](https://github.com/apache/airflow/pull/43520)
- [ ] [[v2-10-test] suppress the warnings where we check for sensitive values (#44148) (#44167)](https://github.com/apache/airflow/pull/44167): @zachliu
- [x] [[v2-10-test] Avoid grouping task instance stats by try_number for dynamic mapped tasks (#44300) (#44319)](https://github.com/apache/airflow/pull/44319): @shahar1
- [ ] [[v2-10-test] Fix problem with inability to remove fields from Connection form (#40421) (#44442)](https://github.com/apache/airflow/pull/44442): @MaksYermak
- [ ] [Fix wrong display of multiline messages in the log after filtering (#44457)](https://github.com/apache/airflow/pull/44457): @jason810496 @Pad71
     Linked issues:
     - [wrong display of multiline messages in the log after filtering by message level (#41265)](https://github.com/apache/airflow/issues/41265)
- [x] [[v2-10-test] Allow ""/"" in metrics validator (#42934) (#44515)](https://github.com/apache/airflow/pull/44515): @potiuk
- [ ] [[v2-10-test] fix gantt flickering #42215 (#44488) (#44517)](https://github.com/apache/airflow/pull/44517): @darkag
- [x] [[v2-10-test] Fix tests badge in README.md (#44505) (#44587)](https://github.com/apache/airflow/pull/44587): @shahar1
- [ ] [Fix test_deprecated_options_with_new_section (#44647)](https://github.com/apache/airflow/pull/44647): @utkarsharma2
- [x] [[BACKPORT (modified)] Prevent using `trigger_rule=TriggerRule.ALWAYS` in a task-generated mapping within bare tasks (#44751) (#44769)](https://github.com/apache/airflow/pull/44769): @shahar1
     Linked issues:
     - [Prevent using `trigger_rule=TriggerRule.ALWAYS` in a task-generated mapping within bare tasks (#44751)](https://github.com/apache/airflow/pull/44751)
- [ ] [[Backport] Fixing cli test failure in CI (#44679) (#44806)](https://github.com/apache/airflow/pull/44806): @utkarsharma2


Thanks to all who contributed to the release (probably not a complete list!):
@antonysouthworth-halter @Pad71 @karenbraganz @pierrejeambrun @kaxil @potiuk @shahar1 @utkarsharma2 @enisnazif @eladkal @xitep @ephraimbuddy @tirkarthi @jscheffl @jason810496 @dimberman

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",utkarsharma2,2024-12-10 13:03:54+00:00,[],2024-12-16 12:45:34+00:00,2024-12-16 12:45:33+00:00,https://github.com/apache/airflow/issues/44811,"[('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2531787914, 'issue_id': 2730069549, 'author': 'kaxil', 'body': '@utkarsharma2 Was https://github.com/apache/airflow/pull/44155 (Backport PR: https://github.com/apache/airflow/pull/44184) not included in 2.10.4?', 'created_at': datetime.datetime(2024, 12, 10, 14, 26, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2536962610, 'issue_id': 2730069549, 'author': 'potiuk', 'body': 'All my changes checked !', 'created_at': datetime.datetime(2024, 12, 11, 19, 43, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2537108470, 'issue_id': 2730069549, 'author': 'darkag', 'body': 'hello\r\n#44517 works as expected', 'created_at': datetime.datetime(2024, 12, 11, 20, 44, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2537335989, 'issue_id': 2730069549, 'author': 'jscheffl', 'body': 'Tested or checked code for #43591, #43659, #43851, #43901, #44045, #44158, #44070 and all looks good.\r\n\r\nHad a bit of challenges running with breeze (either from TAG 2.10.4rc1 or from main with --use-airflow-version)... but this is not blocking release in my view.', 'created_at': datetime.datetime(2024, 12, 11, 22, 31, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2537349341, 'issue_id': 2730069549, 'author': 'potiuk', 'body': '> Had a bit of challenges running with breeze (either from TAG 2.10.4rc1 or from main with --use-airflow-version)... but this is not blocking release in my view.\r\n\r\nWhat challenges :) ?', 'created_at': datetime.datetime(2024, 12, 11, 22, 40, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2537354703, 'issue_id': 2730069549, 'author': 'jscheffl', 'body': '> > Had a bit of challenges running with breeze (either from TAG 2.10.4rc1 or from main with --use-airflow-version)... but this is not blocking release in my view.\r\n> \r\n> What challenges :) ?\r\n\r\n(1) using tag 2.10.4rc1 and starting with breeze matching from the tag as well failed in DAG parsing for some examples as the tag / branch does not contani standard provider (providers noch back-ported). As providers are not updated on tag/v2-10-test branch did not further attempted to patch around.\r\n\r\n(2) using current main and `breeze down && breeze release-management prepare-provider-packages --include-not-ready-providers --skip-tag-check && rm -v dist/*fab* && breeze start-airflow --python 3.12 --load-example-dags --backend postgres --executor CeleryExecutor --answer y --use-airflow-version 2.10.4rc1 --use-packages-from-dist` failed to start Celery Worker. Needed to use LocalExecutor\r\n\r\nMaybe best would have been using the docker images directly :-D', 'created_at': datetime.datetime(2024, 12, 11, 22, 44, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2539559205, 'issue_id': 2730069549, 'author': 'shahar1', 'body': 'All of my changes were tested :)', 'created_at': datetime.datetime(2024, 12, 12, 17, 20, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2541322156, 'issue_id': 2730069549, 'author': 'jason810496', 'body': 'Hi, https://github.com/apache/airflow/pull/44457 ( Fix wrong display of multiline messages in the log after filtering ) works as expected, Thanks !\r\n![Fix wrong display of multiline messages in the log after filtering](https://github.com/user-attachments/assets/f2059af9-ebd7-4c0c-bedd-a199f8407cbd)', 'created_at': datetime.datetime(2024, 12, 13, 12, 16, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2545538215, 'issue_id': 2730069549, 'author': 'utkarsharma2', 'body': 'We\'ve just released Apache Airflow 2.10.4 🎉\r\n\r\n📦 PyPI: https://pypi.org/project/apache-airflow/2.10.4/\r\n📚 Docs: https://airflow.apache.org/docs/apache-airflow/2.10.4/\r\n🛠 Release Notes: https://airflow.apache.org/docs/apache-airflow/2.10.3/release_notes.html\r\n🐳 Docker Image: ""docker pull apache/airflow:2.10.4""\r\n🚏 Constraints: https://github.com/apache/airflow/tree/constraints-2.10.4\r\n\r\nThanks to all the contributors who made this possible.', 'created_at': datetime.datetime(2024, 12, 16, 12, 45, 33, tzinfo=datetime.timezone.utc)}]","kaxil on (2024-12-10 14:26:43 UTC): @utkarsharma2 Was https://github.com/apache/airflow/pull/44155 (Backport PR: https://github.com/apache/airflow/pull/44184) not included in 2.10.4?

potiuk on (2024-12-11 19:43:28 UTC): All my changes checked !

darkag on (2024-12-11 20:44:16 UTC): hello
#44517 works as expected

jscheffl on (2024-12-11 22:31:07 UTC): Tested or checked code for #43591, #43659, #43851, #43901, #44045, #44158, #44070 and all looks good.

Had a bit of challenges running with breeze (either from TAG 2.10.4rc1 or from main with --use-airflow-version)... but this is not blocking release in my view.

potiuk on (2024-12-11 22:40:47 UTC): What challenges :) ?

jscheffl on (2024-12-11 22:44:55 UTC): (1) using tag 2.10.4rc1 and starting with breeze matching from the tag as well failed in DAG parsing for some examples as the tag / branch does not contani standard provider (providers noch back-ported). As providers are not updated on tag/v2-10-test branch did not further attempted to patch around.

(2) using current main and `breeze down && breeze release-management prepare-provider-packages --include-not-ready-providers --skip-tag-check && rm -v dist/*fab* && breeze start-airflow --python 3.12 --load-example-dags --backend postgres --executor CeleryExecutor --answer y --use-airflow-version 2.10.4rc1 --use-packages-from-dist` failed to start Celery Worker. Needed to use LocalExecutor

Maybe best would have been using the docker images directly :-D

shahar1 on (2024-12-12 17:20:43 UTC): All of my changes were tested :)

jason810496 on (2024-12-13 12:16:32 UTC): Hi, https://github.com/apache/airflow/pull/44457 ( Fix wrong display of multiline messages in the log after filtering ) works as expected, Thanks !
![Fix wrong display of multiline messages in the log after filtering](https://github.com/user-attachments/assets/f2059af9-ebd7-4c0c-bedd-a199f8407cbd)

utkarsharma2 (Issue Creator) on (2024-12-16 12:45:33 UTC): We've just released Apache Airflow 2.10.4 🎉

📦 PyPI: https://pypi.org/project/apache-airflow/2.10.4/
📚 Docs: https://airflow.apache.org/docs/apache-airflow/2.10.4/
🛠 Release Notes: https://airflow.apache.org/docs/apache-airflow/2.10.3/release_notes.html
🐳 Docker Image: ""docker pull apache/airflow:2.10.4""
🚏 Constraints: https://github.com/apache/airflow/tree/constraints-2.10.4

Thanks to all the contributors who made this possible.

"
2729876535,issue,open,,"SparkSubmitOperator on kubernetes.  Due to task failure and retry, _spark_exit_code is not 0, which eventually leads to task status failure","### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I use airflow to schedule spark jobs on k8s using SparkSubmitOperator.The spark job succeeded, but the airflow status is failed

### What you think should happen instead?

Due to memory oom exceptions in some tasks, the exit code is generated, resulting in _spark_exit_code not being equal to 0. However, the task will retry itself, and the spark task is ultimately successful. Since _spark_exit_code is not 0, SparkSubmitOperator considers the task status to be a failure. Is it possible to not check _spark_exit_code? The status code returned by the child process shall prevail (returncode)

![95DF0514-E3BB-4B53-9F82-3948223F49B6](https://github.com/user-attachments/assets/ce0edfc5-2c94-4d3d-aefb-9d46aae28cb5)

```    
    def submit(self, application: str = """", **kwargs: Any) -> None:
        """"""
        Remote Popen to execute the spark-submit job.

        :param application: Submitted application, jar or py file
        :param kwargs: extra arguments to Popen (see subprocess.Popen)
        """"""
        spark_submit_cmd = self._build_spark_submit_command(application)

        if self._env:
            env = os.environ.copy()
            env.update(self._env)
            kwargs[""env""] = env

        self._submit_sp = subprocess.Popen(
            spark_submit_cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            bufsize=-1,
            universal_newlines=True,
            **kwargs,
        )

        self._process_spark_submit_log(iter(self._submit_sp.stdout))  # type: ignore
        returncode = self._submit_sp.wait()

        # Check spark-submit return code. In Kubernetes mode, also check the value
        # of exit code in the log, as it may differ.
        if returncode or (self._is_kubernetes and self._spark_exit_code != 0):
            if self._is_kubernetes:
                raise AirflowException(
                    f""Cannot execute: {self._mask_cmd(spark_submit_cmd)}. Error code is: {returncode}. ""
                    f""Kubernetes spark exit code is: {self._spark_exit_code}""
                )
            else:
                raise AirflowException(
                    f""Cannot execute: {self._mask_cmd(spark_submit_cmd)}. Error code is: {returncode}.""
                )

        self.log.debug(""Should track driver: %s"", self._should_track_driver_status)

        # We want the Airflow job to wait until the Spark driver is finished
        if self._should_track_driver_status:
            if self._driver_id is None:
                raise AirflowException(
                    ""No driver id is known: something went wrong when executing the spark submit command""
                )

            # We start with the SUBMITTED status as initial status
            self._driver_status = ""SUBMITTED""

            # Start tracking the driver status (blocking function)
            self._start_driver_status_tracking()

            if self._driver_status != ""FINISHED"":
                raise AirflowException(
                    f""ERROR : Driver {self._driver_id} badly exited with status {self._driver_status}""
                )
```
```   def _process_spark_submit_log(self, itr: Iterator[Any]) -> None:
        """"""
        Process the log files and extract useful information out of it.

        If the deploy-mode is 'client', log the output of the submit command as those
        are the output logs of the Spark worker directly.

        Remark: If the driver needs to be tracked for its status, the log-level of the
        spark deploy needs to be at least INFO (log4j.logger.org.apache.spark.deploy=INFO)

        :param itr: An iterator which iterates over the input of the subprocess
        """"""
        # Consume the iterator
        for line in itr:
            line = line.strip()
            # If we run yarn cluster mode, we want to extract the application id from
            # the logs so we can kill the application when we stop it unexpectedly
            if self._is_yarn and self._connection[""deploy_mode""] == ""cluster"":
                match = re.search(""application[0-9_]+"", line)
                if match:
                    self._yarn_application_id = match.group(0)
                    self.log.info(""Identified spark application id: %s"", self._yarn_application_id)

            # If we run Kubernetes cluster mode, we want to extract the driver pod id
            # from the logs so we can kill the application when we stop it unexpectedly
            elif self._is_kubernetes:
                match_driver_pod = re.search(r""\s*pod name: ((.+?)-([a-z0-9]+)-driver$)"", line)
                if match_driver_pod:
                    self._kubernetes_driver_pod = match_driver_pod.group(1)
                    self.log.info(""Identified spark driver pod: %s"", self._kubernetes_driver_pod)

                match_application_id = re.search(r""\s*spark-app-selector -> (spark-([a-z0-9]+)), "", line)
                if match_application_id:
                    self._kubernetes_application_id = match_application_id.group(1)
                    self.log.info(""Identified spark application id: %s"", self._kubernetes_application_id)

                # Store the Spark Exit code
                match_exit_code = re.search(r""\s*[eE]xit code: (\d+)"", line)
                if match_exit_code:
                    self._spark_exit_code = int(match_exit_code.group(1))

            # if we run in standalone cluster mode and we want to track the driver status
            # we need to extract the driver id from the logs. This allows us to poll for
            # the status using the driver id. Also, we can kill the driver when needed.
            elif self._should_track_driver_status and not self._driver_id:
                match_driver_id = re.search(r""driver-[0-9\-]+"", line)
                if match_driver_id:
                    self._driver_id = match_driver_id.group(0)
                    self.log.info(""identified spark driver id: %s"", self._driver_id)

            self.log.info(line)
```

### How to reproduce

You can make the internal task reproduce the problem due to partial OOM failure

### Operating System

PRETTY_NAME=""Debian GNU/Linux 11 (bullseye)"" NAME=""Debian GNU/Linux"" VERSION_ID=""11"" VERSION=""11 (bullseye)"" VERSION_CODENAME=bullseye ID=debian HOME_URL=""https://www.debian.org/"" SUPPORT_URL=""https://www.debian.org/support"" BUG_REPORT_URL=""https://bugs.debian.org/""

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bin-lian,2024-12-10 11:40:22+00:00,[],2025-01-28 09:06:51+00:00,,https://github.com/apache/airflow/issues/44810,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('provider:apache-spark', '')]","[{'comment_id': 2531331386, 'issue_id': 2729876535, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 10, 11, 40, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2618135224, 'issue_id': 2729876535, 'author': 'eladkal', 'body': 'If you have the fix can you open a PR?', 'created_at': datetime.datetime(2025, 1, 28, 7, 44, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2618365093, 'issue_id': 2729876535, 'author': 'nevcohen', 'body': 'So depending on the logs you get from the driver, how would you consider a spark run as a failed run if not based on the exit code?', 'created_at': datetime.datetime(2025, 1, 28, 9, 6, 49, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-10 11:40:25 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2025-01-28 07:44:39 UTC): If you have the fix can you open a PR?

nevcohen on (2025-01-28 09:06:49 UTC): So depending on the logs you get from the driver, how would you consider a spark run as a failed run if not based on the exit code?

"
2729420235,issue,open,,Improve example docs around `SQLExecuteQueryOperator` in all SQLrelated providers ,"### Body

Context: https://github.com/apache/airflow/pull/44707#discussion_r1872961329

In Sqlite provider we have `operators.rst` which explains how to leverage `SQLExecuteQueryOperator` from `common.sql `provider to be used with Sqlite. We should have similar doc and examples for all SQL related operators  that were removed as part of https://github.com/apache/airflow/issues/44559

**The Task:**
verify that we have `operators.rst` in each of the above providers that explain how to use `SQLExecuteQueryOperator` with the specific provider **See sqlite provider [operators.rst](https://github.com/apache/airflow/blob/c73becd1a7b5b38a5b0dfe0cb9d02b076bfe6f32/docs/apache-airflow-providers-sqlite/operators.rst#L25) for example template**:

```
.. _howto/operator:SqliteOperator:

SQLExecuteQueryOperator to connect to Sqlite
============================================

Use the :class:`SQLExecuteQueryOperator<airflow.providers.common.sql.operators.sql>` to execute
Sqlite commands in a `Sqlite <https://sqlite.org/lang.html>`__ database.

.. note::
    Previously, ``SqliteOperator`` was used to perform this kind of operation. After deprecation this has been removed. Please use ``SQLExecuteQueryOperator`` instead.

Using the Operator
^^^^^^^^^^^^^^^^^^

Use the ``conn_id`` argument to connect to your Sqlite instance where
the connection metadata is structured as follows:

.. list-table:: Sqlite Airflow Connection Metadata
   :widths: 25 25
   :header-rows: 1

   * - Parameter
     - Input
   * - Host: string
     - Sqlite database file

An example usage of the SQLExecuteQueryOperator to connect to Sqlite is as follows:

.. exampleinclude:: /../../providers/tests/system/sqlite/example_sqlite.py
    :language: python
    :start-after: [START howto_operator_sqlite]
    :end-before: [END howto_operator_sqlite]

Furthermore, you can use an external file to execute the SQL commands. Script folder must be at the same level as DAG.py file.

.. exampleinclude:: /../../providers/tests/system/sqlite/example_sqlite.py
    :language: python
    :start-after: [START howto_operator_sqlite_external_file]
    :end-before: [END howto_operator_sqlite_external_file]

```


If possible accommodate for the specific parameters that can be passed from the provider to the hook.
For example in Postgres we can pass `hook_params` for enabling logging so we explain about this explicitly with [example](https://airflow.apache.org/docs/apache-airflow-providers-postgres/stable/operators/postgres_operator_howto_guide.html#enable-logging-of-database-messages-sent-to-the-client)

![Screenshot 2024-12-10 at 10 42 52](https://github.com/user-attachments/assets/6cfb53df-6eab-4522-974d-67bb82aa1606)

The items:

- [ ] Exasol (Previously had `ExasolOperator`)
- [ ] apache.drill (Previously had `DrillOperator`)
- [ ] jdbc (Previously had `JdbcOperator`)
- [ ] microsoft.mssql (Previously had `MsSqlOperator`)
- [ ] mysql (Previously had `MySQLOperator`)
- [x] oracle (Previously had `OracleOperator`)
- [x] presto Previously had `PrestoOperator`)
- [ ] snowflake (Previously had `SnowflakeOperator`)
- [ ] trino (Previously had `TrinoOperator`)
- [x] vertica (Previously had `VerticaOperator`)
- [x] postgres (Previously had `PostgresOperator`)  Docs already [exists](https://airflow.apache.org/docs/apache-airflow-providers-postgres/stable/operators/postgres_operator_howto_guide.html) but requires tweaks to align the style

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2024-12-10 08:45:08+00:00,['nailo2c'],2025-02-09 06:13:49+00:00,,https://github.com/apache/airflow/issues/44807,"[('area:providers', ''), ('good first issue', ''), ('kind:documentation', ''), ('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2533537320, 'issue_id': 2729420235, 'author': 'geraj1010', 'body': 'To confirm, this is for all operators that were deprecated (and merged) in [#44559](https://github.com/apache/airflow/issues/44559)? Are you looking for additional `HOW-TO` guides for the `The Items`?', 'created_at': datetime.datetime(2024, 12, 11, 3, 19, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2533760678, 'issue_id': 2729420235, 'author': 'eladkal', 'body': ""> To confirm, this is for all operators that were deprecated (and merged) in [#44559](https://github.com/apache/airflow/issues/44559)? Are you looking for additional `HOW-TO` guides for the `The Items`?\r\n\r\nYes, get what we have for sqlite to the other providers. I assume it's mostly the same with difference of `conn_id`"", 'created_at': datetime.datetime(2024, 12, 11, 6, 39, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2544041741, 'issue_id': 2729420235, 'author': 'geraj1010', 'body': 'For Amazon, I did find mention of `SQLExecuteQueryOperator` for Redshift and Athena, but not RDS.\r\n\r\nRedshift: https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/redshift/redshift_sql.html\r\nAthena: https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/athena/athena_sql.html\r\n\r\nShould I replace the existing files with `operators.rst` and create a new one for RDS (if needed)?', 'created_at': datetime.datetime(2024, 12, 15, 20, 21, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2544052087, 'issue_id': 2729420235, 'author': 'eladkal', 'body': 'Amazon is not on the list because the docs there are OK (I looked only on Redshift)\r\n\r\nIf RDS can be read with this operator then you are welcome to add it', 'created_at': datetime.datetime(2024, 12, 15, 20, 39, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614227834, 'issue_id': 2729420235, 'author': 'nailo2c', 'body': ""Hi @eladkal could you assign this issue to me? I'd like to give it a try."", 'created_at': datetime.datetime(2025, 1, 26, 6, 3, 45, tzinfo=datetime.timezone.utc)}]","geraj1010 on (2024-12-11 03:19:18 UTC): To confirm, this is for all operators that were deprecated (and merged) in [#44559](https://github.com/apache/airflow/issues/44559)? Are you looking for additional `HOW-TO` guides for the `The Items`?

eladkal (Issue Creator) on (2024-12-11 06:39:15 UTC): Yes, get what we have for sqlite to the other providers. I assume it's mostly the same with difference of `conn_id`

geraj1010 on (2024-12-15 20:21:05 UTC): For Amazon, I did find mention of `SQLExecuteQueryOperator` for Redshift and Athena, but not RDS.

Redshift: https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/redshift/redshift_sql.html
Athena: https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/athena/athena_sql.html

Should I replace the existing files with `operators.rst` and create a new one for RDS (if needed)?

eladkal (Issue Creator) on (2024-12-15 20:39:57 UTC): Amazon is not on the list because the docs there are OK (I looked only on Redshift)

If RDS can be read with this operator then you are welcome to add it

nailo2c (Assginee) on (2025-01-26 06:03:45 UTC): Hi @eladkal could you assign this issue to me? I'd like to give it a try.

"
2729166632,issue,closed,completed,Create mock capability to define DAGs in line for task runner tests,"### Body

Ref https://github.com/apache/airflow/pull/44786#discussion_r1875807589
The idea is so that we do not have to keep expanding the `task_sdk/tests/dags` folder by adding new DAGs all the time. If we can define in line DAGs, and then stub the parse function, it should do the job.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",amoghrajesh,2024-12-10 06:39:01+00:00,['amoghrajesh'],2024-12-10 20:05:30+00:00,2024-12-10 20:05:30+00:00,https://github.com/apache/airflow/issues/44805,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2728105846,issue,closed,completed,Keda does not work properly when using gitSync,"### Official Helm Chart version

1.15.0 (latest released)

### Apache Airflow version

2.10.3

### Kubernetes Version

1.31.2

### Helm Chart configuration

I have the below section of workers form my values.yaml:

workers:  
  replicas: 1
  keda:
    enabled: true
    minReplicaCount: 1   # each worker can has 16 celery concurencies
    maxReplicaCount: 20
  resources:
    requests:
      cpu: 100m
      memory: 1750Mi

### Docker Image customizations

_No response_

### What happened

I try to use keda to autoscale my airflow workers which use CeleryKubernetesExecutor.

I followed instruction in https://airflow.apache.org/docs/helm-chart/stable/keda.html to install Keda and change my airflow values.yaml accordingly.

After applying the changes, I observed lots of warning from keda-operator with error message:
""error parsing postgreSQL metadata: error parsing postgresql metadata: no host given""

It seems to me that Keda ScaledObject doesnt work properly.

I checked that:
1. ScaledObject created on correct namespace, same as my airflow namespace
2. It has correct trigger type: postgresql  (I use postgres for airflow metadata)
3. it has connectionFromEnv: AIRFLOW_CONN_AIRFLOW_DB

After further investigation, I found that ScaledObject scaleTargetRef has other property called ""envSourceContainerName""
as explain in https://keda.sh/docs/2.16/reference/scaledobject-spec/#scaletargetref

The purpose of that property is to tell Keda name of container on which AIRFLOW_CONN_AIRFLOW_DB defined.

If I modify airflow-worker ScaledObject by adding envSourceContainerName: worker into scaleTargetRef, it start working properly.

When I check my airflow-worker StatefulSet, I can see that it has 3 containers: git-sync, worker-log-groomer, and worker.

I'm aware that on the chart template workers/worker-deployment.yaml, worker container is defined as 1st container, but for some reason when I apply the template, worker container is 3rd container in the StatefulSet. I tried deleting airflow-worker StatefulSet and do another helm install with same result.

In my opinion, we cant rely on the position of worker container within the StatefulSet nor Deployment, but we should specify worker container name in ScaledObject scaleTargetRef


### What you think should happen instead

Keda autoscaller should work regardless of position of worker container in StatefulSet or Deployment.

### How to reproduce

1. Deploy aiflow with git-sync enable
2. Deploy Keda into Kubernetes cluster
3. Configure worker to use keda
4. Observe that ScaledObj has no warning

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",yovio-rca,2024-12-09 19:46:45+00:00,['jx2lee'],2025-01-27 02:21:29+00:00,2025-01-27 02:21:29+00:00,https://github.com/apache/airflow/issues/44798,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:helm-chart', 'Airflow Helm Chart')]","[{'comment_id': 2529264184, 'issue_id': 2728105846, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 9, 19, 46, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2529282861, 'issue_id': 2728105846, 'author': 'yovio-rca', 'body': 'Reopen as it was mistakenly close', 'created_at': datetime.datetime(2024, 12, 9, 19, 52, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2543151637, 'issue_id': 2728105846, 'author': 'jx2lee', 'body': 'HI, Can I take this?', 'created_at': datetime.datetime(2024, 12, 14, 15, 21, 22, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-09 19:46:48 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

yovio-rca (Issue Creator) on (2024-12-09 19:52:01 UTC): Reopen as it was mistakenly close

jx2lee (Assginee) on (2024-12-14 15:21:22 UTC): HI, Can I take this?

"
2727854305,issue,closed,completed,Inconsistent Database Resolution Order `sql_alchemy_conn` vs `sql_alchemy_conn_secret`,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

The Airflow scheduler does not choose consistently between `sql_alchemy_conn` and `sql_alchemy_conn_secret`. If both are set and point to different databases, this can cause errors.

For example, DAGs populated into the `sql_alchemy_conn` connection, but when trying to run a TriggerDagRunOperator, it would run into a bug because it would apparently connect through `sql_alchemy_conn_secret` and run into database errors (DAG not found etc).

These settings were misconfigured on my side, but a consistent resolution order would help to surface these misconfigurations.

### What you think should happen instead?

There should be a consistent resolution order. Probably `sql_alchemy_conn` should be preferred, then the other options `_cmd`, then `_secret`, but the exact order would be less important than having it be consistent.

### How to reproduce

Set `sql_alchemy_conn` and `sql_alchemy_conn_secret` to point to different databases.

In my case, `sql_alchemy_conn_secret` database was upgraded to a lower version (`2.8.1`) and the schema difference caused errors.

Try to trigger a DAG through `TriggerDagRunOperator`, eg:

```
import datetime
from airflow import DAG
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.operators.empty import EmptyOperator

with DAG(
    ""my_automation_dag"",
    start_date=datetime.datetime(year=2024, day=1, month=1),
) as dag:
    task = TriggerDagRunOperator(
        task_id=""trigger_new_task"",
        trigger_dag_id=""my_task_dag"",
    )

with DAG(
    ""my_task_dag"",
    start_date=datetime.datetime(year=2024, day=1, month=1),
) as task_dag:
    empty_task = EmptyOperator(
        task_id=""my_empty_task""
    )
```

The DAGs will populate to the `sql_alchemy_conn` database but TriggerDagRunOperator should fail.

You might get the same error as I did, about a missing column, `task_instance.task_display_name`.






### Operating System

Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

```
apache-airflow-providers-amazon==8.23.0
apache-airflow-providers-cncf-kubernetes==7.14.0
apache-airflow-providers-common-compat==1.2.2
apache-airflow-providers-common-io==1.4.2
apache-airflow-providers-common-sql==1.20.0
apache-airflow-providers-fab==1.5.1
apache-airflow-providers-ftp==3.11.1
apache-airflow-providers-http==4.13.3
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-postgres==5.14.0
apache-airflow-providers-slack==8.9.2
apache-airflow-providers-smtp==1.8.1
apache-airflow-providers-sqlite==3.9.1
```

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",theis188,2024-12-09 17:54:46+00:00,[],2024-12-09 19:50:59+00:00,2024-12-09 19:50:59+00:00,https://github.com/apache/airflow/issues/44793,"[('kind:bug', 'This is a clearly a bug'), ('area:MetaDB', 'Meta Database related issues.'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2529278535, 'issue_id': 2727854305, 'author': 'potiuk', 'body': 'I seriously doubt this is a problem Airflow uses the same resolution mechanism in consistent order.\r\n\r\nMost likely what was different is that you had DIFFERENT configuration in two places where things were running. For example in worker you did not have SECRET configured. This is impossible expectation to use a configuration that you have no access to.\r\n\r\nGenerally speaking the expectation is that it\'s on yoy (deployment manager) to make sure your configuratoin is consistently set on all your nodes and in all processes of Airflow. That\'s very basic assumption of airflow environment documented in https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#configuration-reference\r\n\r\n> Use the same configuration across all the Airflow components. While each component does not require all, some configurations need to be same otherwise they would not work as expected. A good example for that is [secret_key](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#config-webserver-secret-key) which should be same on the Webserver and Worker to allow Webserver to fetch logs from Worker.\r\n\r\n\r\nNote that this is changing in Airflow 3, because the ""workers"" will no longer have database access and they wil not require most of configuration so the configuration for sql_alchemy_conn will only be needed in scheduler, webserver and triggerer (but it will still need to be consistent across all three components)\r\n\r\nConverting it into discussion in case more concrete evidences of the error and inconsistency is on airflow side (but it\'s practically impossible).', 'created_at': datetime.datetime(2024, 12, 9, 19, 50, 53, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-12-09 19:50:53 UTC): I seriously doubt this is a problem Airflow uses the same resolution mechanism in consistent order.

Most likely what was different is that you had DIFFERENT configuration in two places where things were running. For example in worker you did not have SECRET configured. This is impossible expectation to use a configuration that you have no access to.

Generally speaking the expectation is that it's on yoy (deployment manager) to make sure your configuratoin is consistently set on all your nodes and in all processes of Airflow. That's very basic assumption of airflow environment documented in https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#configuration-reference



Note that this is changing in Airflow 3, because the ""workers"" will no longer have database access and they wil not require most of configuration so the configuration for sql_alchemy_conn will only be needed in scheduler, webserver and triggerer (but it will still need to be consistent across all three components)

Converting it into discussion in case more concrete evidences of the error and inconsistency is on airflow side (but it's practically impossible).

"
2726342905,issue,closed,completed,Add docs and example for `SQLThresholdCheckOperator`,"### Body

The [SQLThresholdCheckOperator](https://github.com/apache/airflow/blob/1275fec92fd7cd7135b100d66d41bdcb79ade29d/providers/src/airflow/providers/common/sql/operators/sql.py#L1033) isn't mentioned in our docs
https://airflow.apache.org/docs/apache-airflow-providers-common-sql/stable/operators.html

The task is to add information about it with example

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2024-12-09 08:26:11+00:00,[],2024-12-09 19:52:11+00:00,2024-12-09 19:52:11+00:00,https://github.com/apache/airflow/issues/44782,"[('area:providers', ''), ('good first issue', ''), ('kind:documentation', ''), ('provider:common-sql', '')]",[],
2726100821,issue,open,,Allow @task.kubernetes to derive pod name based on decorated function (or more flexibility to override the behaviour),"### Description

I'd like for `@task.kubernetes` to derive the task_id / pod_name based on the decorated function's name. To achieve this now, I would need to pass it into the decorator args, which is a bit repetitive:
```python
@task.kubernetes(task_id=""some_func"", name=f""{some_prefix}_some_func"")
def some_func():
    pass
```

We managed to do something like the above but it was more involved than expected:
```python
# inheriting from a private class, not ideal
class DefaultKubernetesDecoratedOperator(_KubernetesDecoratedOperator):
    custom_operator_name = ""@default_kubernetes_task"" # the operator removes the decorator so this is required

    def __init__(self, **kwargs: Any) -> None:
        if ""task_id"" not in kwargs and ""python_callable"" in kwargs:
            task_id = kwargs[""python_callable""].__name__
            kwargs[""task_id""] = task_id

        if ""task_id"" in kwargs and ""name"" not in kwargs:
            name = f""{get_prefix()}-{kwargs['task_id']}""
            kwargs[""name""] = name

        super().__init__(**kwargs)


def default_kubernetes_task(
    python_callable: Callable[[], None] | None = None,
    multiple_outputs: bool | None = None,
    **kwargs: Any,
) -> TaskDecorator:
    return task_decorator_factory(
        python_callable=python_callable,
        multiple_outputs=multiple_outputs,
        decorated_operator_class=DefaultKubernetesDecoratedOperator,
        **kwargs,
    )
```

Initially I tried wrapping `task.kubernetes` itself, but the [source code scrubbing](https://github.com/apache/airflow/blob/d43052e53bcf9bd8772484b8be4590f869932330/providers/src/airflow/providers/cncf/kubernetes/python_kubernetes_script.py#L42) relies on a [hardcoded decorator name](https://github.com/apache/airflow/blob/d43052e53bcf9bd8772484b8be4590f869932330/providers/src/airflow/providers/cncf/kubernetes/decorators/kubernetes.py#L57), so it doesn't scrub the decorator and the pod fails with `NameError: name 'default_kubernetes_task' is not defined`:
```python
def default_kubernetes_task(
    **task_kwargs: Any,
) -> Callable[[Callable[..., Any]], Callable[..., Any]]:
    def decorator(func: Callable[..., Any]) -> Any:
        if ""task_id"" not in task_kwargs:
            task_kwargs[""task_id""] = func.__name__

        if ""name"" not in task_kwargs:
            name = f""{get_[prefix()}-{task_kwargs['task_id']}""
            task_kwargs[""name""] = name

        return task.kubernetes(**task_kwargs)(func)

    return decorator
```

It'd be nice if there were a more official way to achieve the above (or something similar). Perhaps:
- `@task.kubernetes` could accept an override for the `decorated_operator_class`?
- a list of custom operator names to remove could be set in the configuration?

### Use case/motivation

I'd like for `@task.kubernetes` to derive the task_id / pod_name based on the decorated function's name, but more generally it could be useful to allow `task.kubernetes` to be overridden?

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",wasabigeek,2024-12-09 06:15:48+00:00,[],2025-01-27 01:03:15+00:00,,https://github.com/apache/airflow/issues/44779,"[('kind:feature', 'Feature Requests'), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2527031679, 'issue_id': 2726100821, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 9, 6, 15, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614680731, 'issue_id': 2726100821, 'author': 'kyleburke-meq', 'body': 'I would also like this to be more flexible. Thank you for pointing out that i need to override the custom_operator_name so it can strip the decorator name.', 'created_at': datetime.datetime(2025, 1, 27, 1, 3, 14, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-09 06:15:50 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

kyleburke-meq on (2025-01-27 01:03:14 UTC): I would also like this to be more flexible. Thank you for pointing out that i need to override the custom_operator_name so it can strip the decorator name.

"
2725373666,issue,closed,completed,Cannot schedule a DAG on a DatasetAlias when using a clean Airflow docker image (for CI),"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

Observed also on 2.10.2

### What happened?

One of the tests in our CI is creating a DagBag. After adding a DAG that's scheduled on a DatasetAlias, the bag cannot be created. Confusingly, this issue does not occur in other settings/environments (e.g. dag.test, local pytest, non-CI Airflow run in Docker).

<details><summary>Relevant stack trace</summary>

```none
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: dataset_alias
[SQL: SELECT dataset_alias.id, dataset_alias.name 
FROM dataset_alias 
WHERE dataset_alias.name = ?
 LIMIT ? OFFSET ?]
[parameters: ('bar', 1, 0)]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2024-12-08T16:08:22.227+0000] {variable.py:357} ERROR - Unable to retrieve variable from secrets backend (MetastoreBackend). Checking subsequent secrets backend.
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context
    self.dialect.do_execute(
  File ""/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/default.py"", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: no such table: variable
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/models/variable.py"", line 353, in get_variable_from_secrets
    var_val = secrets_backend.get_variable(key=key)
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/secrets/metastore.py"", line 66, in get_variable
    return MetastoreBackend._fetch_variable(key=key, session=session)
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/api_internal/internal_api_call.py"", line 139, in wrapper
    return func(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py"", line 94, in wrapper
    return func(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/secrets/metastore.py"", line 84, in _fetch_variable
    var_value = session.scalar(select(Variable).where(Variable.key == key).limit(1))
  File ""/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 1747, in scalar
    return self.execute(
  File ""/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File ""/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File ""/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/sql/elements.py"", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File ""/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File ""/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File ""/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 2134, in _handle_dbapi_exception
    util.raise_(
  File ""/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/util/compat.py"", line 211, in raise_
    raise exception
  File ""/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context
    self.dialect.do_execute(
  File ""/home/airflow/.local/lib/python3.9/site-packages/sqlalchemy/engine/default.py"", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: variable
[SQL: SELECT variable.val, variable.id, variable.""key"", variable.description, variable.is_encrypted 
FROM variable 
WHERE variable.""key"" = ?
 LIMIT ? OFFSET ?]
[parameters: ('latest_processed_instrument_file_path', 1, 0)]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
```
</details> 

### What you think should happen instead?

DagBag creation should not fail on a one-shot parse pass of the dags folder regardless of order.

### How to reproduce

1. Obtain a recent docker image, such as: `apache/airflow:2.10.4-python3.11`
2. Spin up a container and open a docker shell.
3. Add the following DAG to the `dags` folder:
```python
from pendulum import datetime

from airflow import DAG
from airflow.datasets import DatasetAlias

with DAG(
    dag_id=""foo"",
    start_date=datetime(2000, 1, 1),
    schedule=[
        DatasetAlias(""bar""),
    ],
    catchup=False,
):
    pass
```
4. Open a python shell, and run:
```python
from airflow.models import DagBag
DagBag(include_examples=False)
```

### Operating System

Rocky Linux 9.3

### Versions of Apache Airflow Providers

(Irrelevant)

### Deployment

Other Docker-based deployment

### Deployment details

Dockerfile-ci:
```
FROM apache/airflow:2.10.2-python3.9

# Install system packages
USER root
RUN apt-get update \
    && apt-get install -y --no-install-recommends build-essential vim strace iproute2 git \
       pkg-config libxml2-dev libxmlsec1-dev libxmlsec1-openssl \
    && apt-get autoremove -yqq --purge \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

USER airflow
```

ci script:
```yaml
  script:
    - git config --global --add safe.directory $PWD
    - pip install uv pre-commit-uv --upgrade
    - uv pip install -e .[dev] --constraint ""https://raw.githubusercontent.com/apache/airflow/constraints-2.10.2/constraints-3.9.txt""
    - uv tool install pre-commit --force --with pre-commit-uv --force-reinstall
    - export PIP_USER=false && pre-commit install --install-hooks
    - pre-commit run --all-files --show-diff-on-failure
```

### Anything else?

Presumably, this issue isn't present in a ""running"" Airflow instance where there is at least one DAG that outputs a DatasetAlias, which causes the necessary tables to be created, and then the 2nd parse of the alias-scheduled DAG succeeds.

I believe this happens because the docker image doesn't come with an initialized sqlite database, an issue that surfaces when the DagBag is built. 

As a workaround, I tried adding an `airflow db migrate` step to the image creation, and while this did help with the manual test explained above, the CI kept failing. Adding `airflow db migrate` to the CI script itself similarly did not help.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Dev-iL,2024-12-08 16:39:29+00:00,[],2024-12-27 09:10:14+00:00,2024-12-27 09:10:14+00:00,https://github.com/apache/airflow/issues/44775,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('area:datasets', 'Issues related to the datasets feature')]","[{'comment_id': 2526399875, 'issue_id': 2725373666, 'author': 'Dev-iL', 'body': 'Looks like this was fixed in 2.10.3.', 'created_at': datetime.datetime(2024, 12, 8, 21, 54, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2548030996, 'issue_id': 2725373666, 'author': 'Dev-iL', 'body': ""Reopening because this issue is still present on 2.10.4. The root cause seems to be the fact the sqlite db that comes with the docker image is uninitialized and can't handle a DatasetAlias schedule in this state."", 'created_at': datetime.datetime(2024, 12, 17, 10, 11, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2548344196, 'issue_id': 2725373666, 'author': 'Dev-iL', 'body': '## Possible solution:\r\n\r\nModify `airflow/datasets/__init__.py` as follows:\r\n\r\n```diff\r\n@@ 26 @@\r\n- from sqlalchemy import select\r\n+ from sqlalchemy import exc, select\r\n\r\n@@ 142 @@\r\n@internal_api_call\r\n@provide_session\r\ndef expand_alias_to_datasets(\r\n    alias: str | DatasetAlias, *, session: Session = NEW_SESSION\r\n) -> list[BaseDataset]:\r\n    """"""Expand dataset alias to resolved datasets.""""""\r\n    from airflow.models.dataset import DatasetAliasModel\r\n\r\n    alias_name = alias.name if isinstance(alias, DatasetAlias) else alias\r\n\r\n+    try:\r\n        dataset_alias_obj = session.scalar(\r\n            select(DatasetAliasModel).where(DatasetAliasModel.name == alias_name).limit(1)\r\n        )\r\n+    except exc.OperationalError:\r\n+       return []\r\n    if dataset_alias_obj:\r\n        return [Dataset(uri=dataset.uri, extra=dataset.extra) for dataset in dataset_alias_obj.datasets]\r\n    return []\r\n```', 'created_at': datetime.datetime(2024, 12, 17, 12, 35, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561495022, 'issue_id': 2725373666, 'author': 'Lee-W', 'body': ""> Reopening because this issue is still present on 2.10.4. The root cause seems to be the fact the sqlite db that comes with the docker image is uninitialized and can't handle a DatasetAlias schedule in this state.\r\n\r\nInteresting 🤔  I'll try to repro and see whether I can do it on my end."", 'created_at': datetime.datetime(2024, 12, 24, 23, 52, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561512539, 'issue_id': 2725373666, 'author': 'Dev-iL', 'body': '@Lee-W Thank you for taking a look! Even if I\'m wrong about the root cause, this issue needs a better error message. When it throws an sqla message about a missing ""variable"" or ""dataset-alias"" tables, when that isn\'t the case, it doesn\'t really help with debugging.\r\n\r\nBtw, I didn\'t mention it, but on CI we\'re running unit tests using a pytest pre-commit hook.\r\n\r\nIf you need any more details, I\'m also available on slack.', 'created_at': datetime.datetime(2024, 12, 25, 0, 45, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561698447, 'issue_id': 2725373666, 'author': 'uranusjr', 'body': 'What is the exact error you are getting? Catching an OperationalError at runtime is generally the wrong thing to do. Even in a blank database, the database structure should still be there and not throw an OperationalError. If the official Docker image does, it may mean we are not cxreating the image correctly, and that should be fixed instead.', 'created_at': datetime.datetime(2024, 12, 25, 8, 1, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561705393, 'issue_id': 2725373666, 'author': 'Dev-iL', 'body': '@uranusjr I included the stack trace in the issue - please see the collapsed section under ""what happened?"".', 'created_at': datetime.datetime(2024, 12, 25, 8, 12, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561709707, 'issue_id': 2725373666, 'author': 'Dev-iL', 'body': ""I just had another thought - maybe when running airflow and AIRFLOW_HOME cannot be determined, the new db created is broken.\r\n\r\nSo when running pytest, it somehow (perhaps running from the wrong folder or missing some environment variables) misses the correct db and creates one that's empty or at least has some tables missing."", 'created_at': datetime.datetime(2024, 12, 25, 8, 19, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563489445, 'issue_id': 2725373666, 'author': 'potiuk', 'body': 'While I cannot help you to answer your question, I might try to guide you with looking at the reasoning.\r\n\r\nI think you need to describe what you are exactly doing and how you are initializing the databse of Airflow and which tests you are talking about. If your tests are accessing the DB, you have to make sure in your test setup that the database is created. This is what various fixtures are doing usually.\r\n\r\nAirflow DB tests in Airflow CI do this by auto-use fixture that creates and initializes the DB: https://github.com/apache/airflow/blob/main/tests_common/pytest_plugin.py#L317  when it has not been initialized (which is generally the first time it runs in a clear environment - say in a new docker container). And it creates a file  "".airflow_db_initialised"" in HOME DIR of airlow when it does so, so it does not attempt to do it again. This file does not survive container restart usually so the intialization happens every time breeze container is started for example. This behaviour can be overwritten with `--with-db-init` flag that is added by our pytest plugin - when this flag is passed, database initialization happens at the beginning of pytest session.\r\n\r\nBut this is how ""airflow"" test suite works - we have no idea what test suite you are talking about and how you run it, and what kind of assertions your containers have (which files are preserved between runs - for example being mounted, and which are not). This is all the question of how your CI and test suite is organized.\r\n\r\nGenerally speaking - you have to make sure that your fixtures (if you use pytest) are doing the right thing and setting up the datebase for you. One of the difficulties you might have is that this also might depend on import sequence of things. Unfortunately airflow import does a lot of implicit things, some lazy loading of various components - because we are sort of trying to initialize everything when we import airflow, but we also try to avoid that initialization and do some magic with lazy loading to sometimes not to complete that intitialization to speed up things in some cases. This is a bit of duality we have - because we do ""import airflow"" pretty much with every possible command, but some of the commands, tests cases or direct imports should be way faster than completely importing and initializing everything that airflow needs to import (configuraiton, settings, database, plugins, providers and so on).\r\n\r\nI hope we will do it differently in Airlfow 3 - and do much more explicit initialization of whatever we need, rather  than do that half-initialization and lazy-loading dance (which also causes occassional resursive import errors when modules are importing each other while not being fully imported - depending on sequence of imports). But that\'s something that we will likely discuss in coming weeks when we will be discussing Airflow 3 packaging and initialization.\r\n\r\nThe consequence of that implicit loading is that we also attempt to make sure that all the models are imported before the database is reset. Most of this happens here:\r\n\r\nhttps://github.com/apache/airflow/blob/main/airflow/models/__init__.py\r\n\r\nAnd there are basically three ways how models are imported: \r\n\r\n* TYPE_CHECKING - loading models for mypy type hint verification\r\n* lazy-loading some old models when they are accessed using ""from airlfow.models import Model""  rather than importing models from sub-packages (this is done lazily because otherwise it will cause recursive imports and unnecessary loading the models\r\n* `import_all_models` method - that is supposed to make sure all models are loaded - for example when we run ""init db"" command - because sqlalchemy will only create the tables, when corresponding ORM models are imported and registered in SQL Alchemy engine. Generally you should make sure you run this method before you run ""initdb"" with sqlite when you create a new database. Maybe your test fixture does not do it.\r\n\r\nSo in general it\'s not too explicit and there are few places/paths where creation of database of Airflow might go wrong. If you have any path where you are runnning db init but do not import/load the models in mamory - you might simply create an empty database, or maybe you have similar ""sentinel file"" or some kind of guard that prevents the database from being initialized when needed and this file is not removed in your CI ? But I am only guessing.\r\n\r\nMy guess is that your tests do not do proper initialization, or maybe delete/remove the tables created at some point in time or maybe some combination of these.\r\n\r\nBTW. I am converting this into a discussion. This is not really ""issue"" in Airlfow - it\'s more troubleshooting of what your test suite does or does not.', 'created_at': datetime.datetime(2024, 12, 27, 9, 10, 5, tzinfo=datetime.timezone.utc)}]","Dev-iL (Issue Creator) on (2024-12-08 21:54:57 UTC): Looks like this was fixed in 2.10.3.

Dev-iL (Issue Creator) on (2024-12-17 10:11:24 UTC): Reopening because this issue is still present on 2.10.4. The root cause seems to be the fact the sqlite db that comes with the docker image is uninitialized and can't handle a DatasetAlias schedule in this state.

Dev-iL (Issue Creator) on (2024-12-17 12:35:07 UTC): ## Possible solution:

Modify `airflow/datasets/__init__.py` as follows:

```diff
@@ 26 @@
- from sqlalchemy import select
+ from sqlalchemy import exc, select

@@ 142 @@
@internal_api_call
@provide_session
def expand_alias_to_datasets(
    alias: str | DatasetAlias, *, session: Session = NEW_SESSION
) -> list[BaseDataset]:
    """"""Expand dataset alias to resolved datasets.""""""
    from airflow.models.dataset import DatasetAliasModel

    alias_name = alias.name if isinstance(alias, DatasetAlias) else alias

+    try:
        dataset_alias_obj = session.scalar(
            select(DatasetAliasModel).where(DatasetAliasModel.name == alias_name).limit(1)
        )
+    except exc.OperationalError:
+       return []
    if dataset_alias_obj:
        return [Dataset(uri=dataset.uri, extra=dataset.extra) for dataset in dataset_alias_obj.datasets]
    return []
```

Lee-W on (2024-12-24 23:52:47 UTC): Interesting 🤔  I'll try to repro and see whether I can do it on my end.

Dev-iL (Issue Creator) on (2024-12-25 00:45:19 UTC): @Lee-W Thank you for taking a look! Even if I'm wrong about the root cause, this issue needs a better error message. When it throws an sqla message about a missing ""variable"" or ""dataset-alias"" tables, when that isn't the case, it doesn't really help with debugging.

Btw, I didn't mention it, but on CI we're running unit tests using a pytest pre-commit hook.

If you need any more details, I'm also available on slack.

uranusjr on (2024-12-25 08:01:51 UTC): What is the exact error you are getting? Catching an OperationalError at runtime is generally the wrong thing to do. Even in a blank database, the database structure should still be there and not throw an OperationalError. If the official Docker image does, it may mean we are not cxreating the image correctly, and that should be fixed instead.

Dev-iL (Issue Creator) on (2024-12-25 08:12:25 UTC): @uranusjr I included the stack trace in the issue - please see the collapsed section under ""what happened?"".

Dev-iL (Issue Creator) on (2024-12-25 08:19:41 UTC): I just had another thought - maybe when running airflow and AIRFLOW_HOME cannot be determined, the new db created is broken.

So when running pytest, it somehow (perhaps running from the wrong folder or missing some environment variables) misses the correct db and creates one that's empty or at least has some tables missing.

potiuk on (2024-12-27 09:10:05 UTC): While I cannot help you to answer your question, I might try to guide you with looking at the reasoning.

I think you need to describe what you are exactly doing and how you are initializing the databse of Airflow and which tests you are talking about. If your tests are accessing the DB, you have to make sure in your test setup that the database is created. This is what various fixtures are doing usually.

Airflow DB tests in Airflow CI do this by auto-use fixture that creates and initializes the DB: https://github.com/apache/airflow/blob/main/tests_common/pytest_plugin.py#L317  when it has not been initialized (which is generally the first time it runs in a clear environment - say in a new docker container). And it creates a file  "".airflow_db_initialised"" in HOME DIR of airlow when it does so, so it does not attempt to do it again. This file does not survive container restart usually so the intialization happens every time breeze container is started for example. This behaviour can be overwritten with `--with-db-init` flag that is added by our pytest plugin - when this flag is passed, database initialization happens at the beginning of pytest session.

But this is how ""airflow"" test suite works - we have no idea what test suite you are talking about and how you run it, and what kind of assertions your containers have (which files are preserved between runs - for example being mounted, and which are not). This is all the question of how your CI and test suite is organized.

Generally speaking - you have to make sure that your fixtures (if you use pytest) are doing the right thing and setting up the datebase for you. One of the difficulties you might have is that this also might depend on import sequence of things. Unfortunately airflow import does a lot of implicit things, some lazy loading of various components - because we are sort of trying to initialize everything when we import airflow, but we also try to avoid that initialization and do some magic with lazy loading to sometimes not to complete that intitialization to speed up things in some cases. This is a bit of duality we have - because we do ""import airflow"" pretty much with every possible command, but some of the commands, tests cases or direct imports should be way faster than completely importing and initializing everything that airflow needs to import (configuraiton, settings, database, plugins, providers and so on).

I hope we will do it differently in Airlfow 3 - and do much more explicit initialization of whatever we need, rather  than do that half-initialization and lazy-loading dance (which also causes occassional resursive import errors when modules are importing each other while not being fully imported - depending on sequence of imports). But that's something that we will likely discuss in coming weeks when we will be discussing Airflow 3 packaging and initialization.

The consequence of that implicit loading is that we also attempt to make sure that all the models are imported before the database is reset. Most of this happens here:

https://github.com/apache/airflow/blob/main/airflow/models/__init__.py

And there are basically three ways how models are imported: 

* TYPE_CHECKING - loading models for mypy type hint verification
* lazy-loading some old models when they are accessed using ""from airlfow.models import Model""  rather than importing models from sub-packages (this is done lazily because otherwise it will cause recursive imports and unnecessary loading the models
* `import_all_models` method - that is supposed to make sure all models are loaded - for example when we run ""init db"" command - because sqlalchemy will only create the tables, when corresponding ORM models are imported and registered in SQL Alchemy engine. Generally you should make sure you run this method before you run ""initdb"" with sqlite when you create a new database. Maybe your test fixture does not do it.

So in general it's not too explicit and there are few places/paths where creation of database of Airflow might go wrong. If you have any path where you are runnning db init but do not import/load the models in mamory - you might simply create an empty database, or maybe you have similar ""sentinel file"" or some kind of guard that prevents the database from being initialized when needed and this file is not removed in your CI ? But I am only guessing.

My guess is that your tests do not do proper initialization, or maybe delete/remove the tables created at some point in time or maybe some combination of these.

BTW. I am converting this into a discussion. This is not really ""issue"" in Airlfow - it's more troubleshooting of what your test suite does or does not.

"
2725073906,issue,closed,completed,"On WSL2/Windows, constantly getting ""Heartbeat failed with Exception: PID of job runner does not match"" warnings","### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

This has happened on a few machines where we have a fresh installation of Airflow/Postgres 16. 
The console keeps reporting the following 2 lines:
Heartbeat failed with Exception: PID of job runner does not match
Recorded pid 40205 does not match the current pid 40236


### What you think should happen instead?

More information / no warning / how to fix

### How to reproduce

Installed fresh Airflow on WSL 2, Ubuntu 24.04, inititalized database

### Operating System

Windows 11 Professional / WSL 2

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",yarong-lifemap,2024-12-08 09:34:07+00:00,[],2024-12-11 21:23:03+00:00,2024-12-11 21:23:03+00:00,https://github.com/apache/airflow/issues/44771,"[('kind:bug', 'This is a clearly a bug'), ('pending-response', ''), ('area:core', '')]","[{'comment_id': 2537219979, 'issue_id': 2725073906, 'author': 'potiuk', 'body': 'I think you need to provide a bit more information. Windows/WSL2 is not really a target platform for airflow runtime for ""production"" and we do not run any tests with it.\r\n\r\nBut maybe - if you can do some analysis and provide more information we can attempt to help you in your problem. \r\n\r\nYou can try to gather some more logs and information - i.e. gather more detailed logs from example tasks - to see which processes have been created - if you look and trace it in the logs and send us the info, maybe we can help you to figure out some workarounds. Track what processes are being created. Also if you are using `run_as_user` - feature, there might be a problem with WSL2 or your impersonation user settings.\r\n\r\nThere few parameters you can try - https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#execute-tasks-new-python-interpreter might be something worth trying. \r\n\r\nConverting to a discussion, as Windows/WSL2 is not a target runtime platform - but there might be a discussion coming out of it.', 'created_at': datetime.datetime(2024, 12, 11, 21, 22, 58, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-12-11 21:22:58 UTC): I think you need to provide a bit more information. Windows/WSL2 is not really a target platform for airflow runtime for ""production"" and we do not run any tests with it.

But maybe - if you can do some analysis and provide more information we can attempt to help you in your problem. 

You can try to gather some more logs and information - i.e. gather more detailed logs from example tasks - to see which processes have been created - if you look and trace it in the logs and send us the info, maybe we can help you to figure out some workarounds. Track what processes are being created. Also if you are using `run_as_user` - feature, there might be a problem with WSL2 or your impersonation user settings.

There few parameters you can try - https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#execute-tasks-new-python-interpreter might be something worth trying. 

Converting to a discussion, as Windows/WSL2 is not a target runtime platform - but there might be a discussion coming out of it.

"
2724506616,issue,open,,"Trigger event from deferred task does not get scheduled immediately, leading to timeout.","### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.8.1

### What happened?

We are using deferred operators to execute jobs in databricks. These jobs utlize a common database so we use task pools to limit the concurrency to 1 task. This pool includes deferred operators. In some cases we see task timeouts, even though the deferred task successfully finished. You can see 1.5h passing between trigger event and scheduling:

```
[2024-12-06, 14:01:10 CET] {{taskinstance.py:2344}} INFO - Pausing task as DEFERRED. dag_id=my-dag, task_id=my-task, execution_date=20241205T130000, start_date=20241206T130108
[2024-12-06, 14:01:10 CET] {{local_task_job_runner.py:231}} INFO - Task exited with return code 100 (task deferral)
[2024-12-06, 14:01:11 CET] {{base.py:83}} INFO - Using connection ID 'databricks' for task execution.
[2024-12-06, 14:01:11 CET] {{databricks.py:94}} INFO - run-id 847717920033451 in run state {'life_cycle_state': 'PENDING', 'result_state': '', 'state_message': 'Waiting for cluster'}. sleeping for 30 seconds
...
[2024-12-06, 14:09:42 CET] {{databricks.py:94}} INFO - run-id 847717920033451 in run state {'life_cycle_state': 'RUNNING', 'result_state': '', 'state_message': 'In run'}. sleeping for 30 seconds
[2024-12-06, 14:10:12 CET] {{databricks.py:94}} INFO - run-id 847717920033451 in run state {'life_cycle_state': 'RUNNING', 'result_state': '', 'state_message': 'In run'}. sleeping for 30 seconds
[2024-12-06, 14:10:42 CET] {{triggerer_job_runner.py:602}} INFO - Trigger my-dag/scheduled__2024-12-05T13:00:00+00:00/my-task/-1/1 (ID 10030) fired: TriggerEvent<{'run_id': 847717920033451, 'run_page_url': '...', 'run_state': '{""life_cycle_state"": ""TERMINATED"", ""result_state"": ""SUCCESS"", ""state_message"": """"}'}>
[2024-12-06, 15:38:27 CET] {{taskinstance.py:1956}} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: my-dag.my-task scheduled__2024-12-05T13:00:00+00:00 [queued]>
...
[2024-12-06, 15:38:27 CET] {{taskinstance.py:2698}} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 425, in _execute_task
    raise AirflowTaskTimeout()
airflow.exceptions.AirflowTaskTimeout
```
Our assumption of what happens in the following:

- Many tasks are waiting to be executed but are limited by the pool
- Task starts running and is deferred (pool slot is consumed)
- Deferred task is running in the triggerer (pool slot is consumed)
- Deferred task emits trigger event and stops (pool slot is released)
- As the pool slot is released, another task starts running (pool slot is consumed again)
- The post-deferral task for our previous task is scheduled, but cannot run due to unavailable pool slots.
- After the task that got scheduled in between finishes and the pool is released, the post-deferral task runs and times out immediately.

### What you think should happen instead?

I see multiple things that could improve this behavior:

- Tasks waking up after deferral do not consume slots within task pools.
- Tasks waking up have priority over other tasks when making scheduling decisions.
- Tasks waking up have their own timeout for the post-deferral trigger.


### How to reproduce

- Create a DAG with many deferrable tasks sharing a single task pool.
- Reduce pool capacity to 1 and enable `Include Deferred`.
- Observe that sometimes a new task is scheduled before the post-deferral task is being scheduled.

### Operating System

Amazon Linux 2

### Versions of Apache Airflow Providers

_No response_

### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Birne94,2024-12-07 09:33:45+00:00,[],2024-12-22 17:45:40+00:00,,https://github.com/apache/airflow/issues/44759,"[('good first issue', ''), ('kind:documentation', ''), ('area:core', ''), ('area:Triggerer', '')]","[{'comment_id': 2528079187, 'issue_id': 2724506616, 'author': 'tirkarthi', 'body': 'If you are not doing anything on `execute_complete` and on track to upgrade to 2.10.0 maybe exit task directly from trigger could help.\r\n\r\nhttps://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/deferring.html#exiting-deferred-task-from-triggers', 'created_at': datetime.datetime(2024, 12, 9, 14, 14, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2528089078, 'issue_id': 2724506616, 'author': 'Birne94', 'body': '> If you are not doing anything on `execute_complete` and on track to upgrade to 2.10.0 maybe exit task directly from trigger could help.\r\n> \r\n> https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/deferring.html#exiting-deferred-task-from-triggers\r\n\r\nThank you for that hint, that sounds interesting. We need to apply a bit of logic before exiting and publish task results to xcom. From [the code](https://github.com/apache/airflow/blob/cf4f2caac93c3482c4812ce0f2e81f822f323762/airflow/triggers/base.py#L187) I see that this case would be supported, hwoever it is not mentioned in the docs. Is that feature stable already?', 'created_at': datetime.datetime(2024, 12, 9, 14, 18, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2528268226, 'issue_id': 2724506616, 'author': 'tirkarthi', 'body': ""It's added and stable in 2.10 and you can set `self.xcoms` which will get pushed.\r\n\r\nhttps://github.com/apache/airflow/blob/cf4f2caac93c3482c4812ce0f2e81f822f323762/airflow/triggers/base.py#L229\r\n\r\n> TaskSuccessEvent and TaskFailureEvent are the two events that can be used to end the task instance directly. This marks the task with the state task_instance_state and optionally pushes xcom if applicable. Here’s an example of how to use these events:\r\n\r\nMaybe the doc could have some examples or explain this feature better"", 'created_at': datetime.datetime(2024, 12, 9, 15, 6, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2530733055, 'issue_id': 2724506616, 'author': 'Birne94', 'body': 'Thank you @tirkarthi, we will see if we can prioritize upgrading our MWAA environment to 2.10 and testing this approach.\r\n\r\nI assume that the described behavior (post-deferral task execution being delayed) is expected in this case and changing it would be more of a QoL change rather than a bug, right?', 'created_at': datetime.datetime(2024, 12, 10, 8, 6, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2530776921, 'issue_id': 2724506616, 'author': 'eladkal', 'body': 'Given what @tirkarthi mentioned - changing this to doc only issue.\r\nPR to improve the doc are welcome', 'created_at': datetime.datetime(2024, 12, 10, 8, 22, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558123389, 'issue_id': 2724506616, 'author': 'avyuktsoni0731', 'body': ""Hey @eladkal\r\nI want to contribute to the improved documentation regarding this issue. Although I'm a newbie to Apache Airflow, could you guide me on how I can get started with it? Where can I find the appropriate documentation to contribute?\r\n\r\nHelp would be much appreciated.\r\nThanks"", 'created_at': datetime.datetime(2024, 12, 21, 13, 30, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558222567, 'issue_id': 2724506616, 'author': 'eladkal', 'body': '@avyuktsoni0731 check https://github.com/apache/airflow/blob/main/contributing-docs/README.rst', 'created_at': datetime.datetime(2024, 12, 21, 19, 57, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558534518, 'issue_id': 2724506616, 'author': 'avyuktsoni0731', 'body': ""@eladkal I've created a PR. I tried to understand the issue that has been addressed, but do let me know if any other changes are required, will look into it."", 'created_at': datetime.datetime(2024, 12, 22, 17, 45, 38, tzinfo=datetime.timezone.utc)}]","tirkarthi on (2024-12-09 14:14:29 UTC): If you are not doing anything on `execute_complete` and on track to upgrade to 2.10.0 maybe exit task directly from trigger could help.

https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/deferring.html#exiting-deferred-task-from-triggers

Birne94 (Issue Creator) on (2024-12-09 14:18:12 UTC): Thank you for that hint, that sounds interesting. We need to apply a bit of logic before exiting and publish task results to xcom. From [the code](https://github.com/apache/airflow/blob/cf4f2caac93c3482c4812ce0f2e81f822f323762/airflow/triggers/base.py#L187) I see that this case would be supported, hwoever it is not mentioned in the docs. Is that feature stable already?

tirkarthi on (2024-12-09 15:06:17 UTC): It's added and stable in 2.10 and you can set `self.xcoms` which will get pushed.

https://github.com/apache/airflow/blob/cf4f2caac93c3482c4812ce0f2e81f822f323762/airflow/triggers/base.py#L229


Maybe the doc could have some examples or explain this feature better

Birne94 (Issue Creator) on (2024-12-10 08:06:36 UTC): Thank you @tirkarthi, we will see if we can prioritize upgrading our MWAA environment to 2.10 and testing this approach.

I assume that the described behavior (post-deferral task execution being delayed) is expected in this case and changing it would be more of a QoL change rather than a bug, right?

eladkal on (2024-12-10 08:22:58 UTC): Given what @tirkarthi mentioned - changing this to doc only issue.
PR to improve the doc are welcome

avyuktsoni0731 on (2024-12-21 13:30:57 UTC): Hey @eladkal
I want to contribute to the improved documentation regarding this issue. Although I'm a newbie to Apache Airflow, could you guide me on how I can get started with it? Where can I find the appropriate documentation to contribute?

Help would be much appreciated.
Thanks

eladkal on (2024-12-21 19:57:06 UTC): @avyuktsoni0731 check https://github.com/apache/airflow/blob/main/contributing-docs/README.rst

avyuktsoni0731 on (2024-12-22 17:45:38 UTC): @eladkal I've created a PR. I tried to understand the issue that has been addressed, but do let me know if any other changes are required, will look into it.

"
2724227365,issue,open,,Add ability to generate temporary downloadable link for task logs(stored on cloud storage) on UI,"### Description

When a task generates many logs, the web server usually gets OOM Killed while trying to render the task logs. For this situation, we can give the user a downloadable link to the file on UI. If the task logs are located on cloud storage, we can generate the following:

1. AWS - [Presigned URLs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html)
2. GCP - [Signed URL's](https://cloud.google.com/storage/docs/access-control/signed-urls)
3. Azure - [Shared Access Signature](https://learn.microsoft.com/en-us/azure/storage/common/storage-sas-overview)


### Use case/motivation

To render large task log files

### Related issues

https://github.com/apache/airflow/issues/33625

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rawwar,2024-12-07 01:13:18+00:00,[],2025-01-27 03:06:43+00:00,,https://github.com/apache/airflow/issues/44753,"[('area:logging', ''), ('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2525071649, 'issue_id': 2724227365, 'author': 'jscheffl', 'body': 'I have also espcially seen that the webserver gets OOM even if you download because the FileTaskHandler tries to sort+merge different log sources. One important thing is not only the download but also the FileTaskHandler must 1:1 stream the logs from the backend, else it will go OOM as well.\r\n\r\nThis included when I took a look last time... why actually log sorting and merging is implemented in general I doubt the usefulness because I also had situations where log sorting by timestamp gave ""strange"" results. In my view logs should not be modified/sorted/merged when served from webserver. This would also remove the OOM problem in general... whereas I have seen situations as well where the browser/client ""dies"" in rendering. So a limit in logs provided to browser might be a important feature as well (but please not like Github is doing it)', 'created_at': datetime.datetime(2024, 12, 7, 10, 59, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2525193224, 'issue_id': 2724227365, 'author': 'potiuk', 'body': 'Mostly agree with @jscheffl -> but I still think merging logs **might** be useful in some cases, though the ""naive"" version it is done now should be either limited to certain log size that should be able to fit in memory or fixed to support arbitrary log size. Loading whole log to memory is generally bad idea (but OK if we can confirm they will fit in memory).\r\n\r\nThere are some algorithms that could be used to do it ""well"" even when the logs are huge, but they require much more sophisticated behaviour and local file storage and likely are not suitable to run in an ""API"" call.  So I am not sure if it at all worth doing it (airflow is NOT a sophisticated logging solution) - but if we have this ""download full log"" (and even there we could download zipped logs from several sources without merging them) - that could be a useful counterpart for merging for big files.\r\n\r\nThere are two options how to do it, I think:\r\n\r\n1) if the files are huge (and we could set arbitrary value here), only show the original task log (streaming it) and add a link to ""download"" the .zip file where you see the missing logs as well\r\n2) if the files are huge just download the ""max"" part of the files - perform merge and add a note that the logs are incomplete and that you should download the whole .zip content of the several logs\r\n\r\nGenerally, yes, I think we should implement it (and prevent the OOM from happening).', 'created_at': datetime.datetime(2024, 12, 7, 14, 45, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2526209568, 'issue_id': 2724227365, 'author': 'jason810496', 'body': ""Hi, after tracing down related GitHub issues and attempting to reproduce the OOM issue, I have some questions and ideas about related improvements.  \r\n\r\n### Related Issues and Context\r\n\r\n- https://github.com/apache/airflow/pull/29390 \r\n    - Limits UI rendering to 10,000 lines, but the webserver still reads the full logs.\r\n    - The browser tab will get stuck also.\r\n- https://github.com/apache/airflow/issues/29405\r\n    - https://github.com/apache/airflow/pull/30729\r\n    - Only adds a description of `log_pos` and `end_of_log` metadata using `URLSafeSerializer`, the `get_log` API remain unchanged.\r\n- https://github.com/apache/airflow/issues/33625\r\n    - Pagination on UI side, haven't been resolved yet. \r\n    - Suggests exposing `log_pos` and `end_of_log` as query parameters for better UI pagination.  \r\n  As noted by @bbovenzi, this remains unresolved.\r\n\r\n### Ideas for Related Improvements\r\n\r\n> I’d be happy to take on these improvements if there are no objections!\r\n\r\n1. **Expose `log_pos` and `end_of_log` as Query Parameters**:\r\n   - Implement in Flask and backport to `v2.10.x`.\r\n   - Implement in FastAPI. ( new UI will have to implement the pagination logic as well )\r\n\r\n2. **Pagination for Legacy UI**:\r\n   - Implement pagination for logs in the legacy UI after addressing the above.\r\n   - https://github.com/apache/airflow/issues/33625\r\n\r\n3. **Refactor `FileTaskHandler`**:\r\n   - Address the root cause of OOM directly at the handler level.\r\n\r\nThough I couldn't reliably reproduce OOM on the webserver, the browser tab crashing due to large logs remains an issue.\r\n\r\n### Question\r\n\r\nHow can I reliably reproduce the OOM issue when fetching large task logs?  \r\nI’ve tried observing memory usage with [Memray](https://bloomberg.github.io/memray/run.html) but found it challenging to reproduce.  \r\n- **Steps Taken**:\r\n  - Used the DAG mentioned in [PR #29390](https://github.com/apache/airflow/pull/29390).\r\n  - Triggered runs multiple times and managed to reproduce OOM only once.\r\n  - Used `breeze start-airflow --python 3.9 --backend sqlite` as the environment.\r\n\r\nAny tips on how to consistently reproduce the issue? Or is there a specific environment setup recommended for this?"", 'created_at': datetime.datetime(2024, 12, 8, 16, 32, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2526210485, 'issue_id': 2724227365, 'author': 'rawwar', 'body': '@jason810496 , I think you need to put resource limits on each airflow component. On managed services, the webserver is usually limited, On Astro its about 1 vcpu and 2GiB memory.', 'created_at': datetime.datetime(2024, 12, 8, 16, 35, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2526214142, 'issue_id': 2724227365, 'author': 'jason810496', 'body': '> @jason810496 , I think you need to put resource limits on each airflow component. On managed services, the webserver is usually limited, On Astro its about 1 vcpu and 2GiB memory.\r\n\r\nThanks, I will try it with docker compose !', 'created_at': datetime.datetime(2024, 12, 8, 16, 45, 34, tzinfo=datetime.timezone.utc)}]","jscheffl on (2024-12-07 10:59:31 UTC): I have also espcially seen that the webserver gets OOM even if you download because the FileTaskHandler tries to sort+merge different log sources. One important thing is not only the download but also the FileTaskHandler must 1:1 stream the logs from the backend, else it will go OOM as well.

This included when I took a look last time... why actually log sorting and merging is implemented in general I doubt the usefulness because I also had situations where log sorting by timestamp gave ""strange"" results. In my view logs should not be modified/sorted/merged when served from webserver. This would also remove the OOM problem in general... whereas I have seen situations as well where the browser/client ""dies"" in rendering. So a limit in logs provided to browser might be a important feature as well (but please not like Github is doing it)

potiuk on (2024-12-07 14:45:14 UTC): Mostly agree with @jscheffl -> but I still think merging logs **might** be useful in some cases, though the ""naive"" version it is done now should be either limited to certain log size that should be able to fit in memory or fixed to support arbitrary log size. Loading whole log to memory is generally bad idea (but OK if we can confirm they will fit in memory).

There are some algorithms that could be used to do it ""well"" even when the logs are huge, but they require much more sophisticated behaviour and local file storage and likely are not suitable to run in an ""API"" call.  So I am not sure if it at all worth doing it (airflow is NOT a sophisticated logging solution) - but if we have this ""download full log"" (and even there we could download zipped logs from several sources without merging them) - that could be a useful counterpart for merging for big files.

There are two options how to do it, I think:

1) if the files are huge (and we could set arbitrary value here), only show the original task log (streaming it) and add a link to ""download"" the .zip file where you see the missing logs as well
2) if the files are huge just download the ""max"" part of the files - perform merge and add a note that the logs are incomplete and that you should download the whole .zip content of the several logs

Generally, yes, I think we should implement it (and prevent the OOM from happening).

jason810496 on (2024-12-08 16:32:14 UTC): Hi, after tracing down related GitHub issues and attempting to reproduce the OOM issue, I have some questions and ideas about related improvements.  

### Related Issues and Context

- https://github.com/apache/airflow/pull/29390 
    - Limits UI rendering to 10,000 lines, but the webserver still reads the full logs.
    - The browser tab will get stuck also.
- https://github.com/apache/airflow/issues/29405
    - https://github.com/apache/airflow/pull/30729
    - Only adds a description of `log_pos` and `end_of_log` metadata using `URLSafeSerializer`, the `get_log` API remain unchanged.
- https://github.com/apache/airflow/issues/33625
    - Pagination on UI side, haven't been resolved yet. 
    - Suggests exposing `log_pos` and `end_of_log` as query parameters for better UI pagination.  
  As noted by @bbovenzi, this remains unresolved.

### Ideas for Related Improvements


1. **Expose `log_pos` and `end_of_log` as Query Parameters**:
   - Implement in Flask and backport to `v2.10.x`.
   - Implement in FastAPI. ( new UI will have to implement the pagination logic as well )

2. **Pagination for Legacy UI**:
   - Implement pagination for logs in the legacy UI after addressing the above.
   - https://github.com/apache/airflow/issues/33625

3. **Refactor `FileTaskHandler`**:
   - Address the root cause of OOM directly at the handler level.

Though I couldn't reliably reproduce OOM on the webserver, the browser tab crashing due to large logs remains an issue.

### Question

How can I reliably reproduce the OOM issue when fetching large task logs?  
I’ve tried observing memory usage with [Memray](https://bloomberg.github.io/memray/run.html) but found it challenging to reproduce.  
- **Steps Taken**:
  - Used the DAG mentioned in [PR #29390](https://github.com/apache/airflow/pull/29390).
  - Triggered runs multiple times and managed to reproduce OOM only once.
  - Used `breeze start-airflow --python 3.9 --backend sqlite` as the environment.

Any tips on how to consistently reproduce the issue? Or is there a specific environment setup recommended for this?

rawwar (Issue Creator) on (2024-12-08 16:35:17 UTC): @jason810496 , I think you need to put resource limits on each airflow component. On managed services, the webserver is usually limited, On Astro its about 1 vcpu and 2GiB memory.

jason810496 on (2024-12-08 16:45:34 UTC): Thanks, I will try it with docker compose !

"
2723739690,issue,closed,completed,DAG parameters form updates only on blur,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.2

### What happened?

When triggerring a DAG and updating some parameter, the DAG ran with the old value that param had.

### What you think should happen instead?

If I change the value of some textbox in the DAG parameters and press Enter, the DAG should use the value I filled in.

### How to reproduce

- Create a DAG with some param
- In the web UI click ""Trigger DAG""
- in the ""DAG conf Parameters"" form, fill in some value in some textbox and press Enter (without first unfocusing the textbox)
- The DAG will run with the parameter set to the value it had before, and ignore the value you tried to set

It's easiest to see when you set the param to be required but have no default value, then pressing Enter will error and let you refill the form.

### Operating System

Ubuntu 22.04

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",NotWearingPants,2024-12-06 19:06:30+00:00,[],2024-12-06 22:13:08+00:00,2024-12-06 22:13:07+00:00,https://github.com/apache/airflow/issues/44739,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2523975084, 'issue_id': 2723739690, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 6, 19, 6, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2524281320, 'issue_id': 2723739690, 'author': 'NotWearingPants', 'body': 'Duplicate of #42157 (which was fixed in the next minor)', 'created_at': datetime.datetime(2024, 12, 6, 22, 13, 7, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-06 19:06:33 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

NotWearingPants (Issue Creator) on (2024-12-06 22:13:07 UTC): Duplicate of #42157 (which was fixed in the next minor)

"
2723670909,issue,closed,not_planned,Scheduler crash loop due to invalid task_id persisted in DagBag without pre-validation,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.6.3

### What happened?

We inadvertently committed a change which introduced some airflow task names that were longer than 250 characters. This caused `validate_key` check ([code: search for validate_key](https://airflow.apache.org/docs/apache-airflow/1.10.3/_modules/airflow/utils/helpers.html)) to fail in the Airflow scheduler that required all task names to be less than 250 characters. 

A corrupted version of the serialised DAG was cached in the airflow DB. The scheduler was stuck in a loop of continuously hitting this exception even when the long task names were reverted and scheduler was restarted. This was a very confusing state for us to be in as it didn't make sense immediately why reverting change and restarting wouldn't fix it.

The issue was eventually resolved by running cmd cmd `airflow dags reserialize -v -S <path-to-dags>`  to force reserialization outside of scheduler process. 
https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html#reserialize


### What you think should happen instead?

There are three main problems here:
1. Scheduler should never get caught in a crash loop - problems associated with bad serialization should go away when bad changes are reverted and scheduler restarted.
2. The issue stems from a bug in Airflow's task validation process. Specifically, the validation check for task integrity was not performed before the task was serialized and persisted into the DagBag. As a result, an invalid task was written to the DagBag, creating a corrupted state.
3. Having issues in one of the DAGs shouldn't take the whole scheduler down. The validation exceptions should be caught somewhere in such a way that it just skips that DAG, appropriately notifies user about it and still proceeds with rest of DAGs.

Minor: 
1. Improve error messages, the error message from `validate_keys` that we were seeing was `airflow.exceptions.AirflowException: The key has to be less than 250 characters`. It should also print the name of offending key for easy debugging for user.

### How to reproduce

* Create task with task_id greator than 250 characters.
* Let scheduler pick this up
* Notice crash in scheduler logs that shows validation errors from `validate_keys` function
* Revert original change to restore task_id to less than 250 characters
* Scheduler still keeps crashing with same errors
* Run `airflow dags reserialize -v -S <path-to-dags>` to fix this 

### Operating System

Debian GNU/Linux 11.7 (Bullseye)

### Versions of Apache Airflow Providers

```
>>> pip freeze  | g apache-airflow-providers
apache-airflow-providers-celery==3.5.1
apache-airflow-providers-common-sql==1.10.0
apache-airflow-providers-elasticsearch==4.4.0
apache-airflow-providers-ftp==3.7.0
apache-airflow-providers-http==4.8.0
apache-airflow-providers-imap==3.5.0
apache-airflow-providers-sftp==4.8.1
apache-airflow-providers-sqlite==3.7.0
apache-airflow-providers-ssh==3.10.0
```

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

I will be happy to send a PR, let me know if this sounds good:
> 1. Scheduler should never get caught in a crash loop - problems associated with bad serialization should go away when bad changes are reverted and scheduler restarted.

Possibly requires a big refactor. Not sure if I can do this with a quick PR? Happy to be guided more on this.

> 2. The issue stems from a bug in Airflow's task validation process. Specifically, the validation check for task integrity was not performed before the task was serialized and persisted into the DagBag. As a result, an invalid task was written to the DagBag, creating a corrupted state.

Add a call to `validate_key` before serialization happens and is persisted. Any other validation to add along with this?

> 3. Having issues in one of the DAGs shouldn't take the whole scheduler down. The validation exceptions should be caught somewhere in such a way that it just skips that DAG, appropriately notifies user about it and still proceeds with rest of DAGs.

Again possibly major refacor needed for this? Something to consider in design for future versions?

> 1. Improve error messages, the error message from `validate_keys` that we were seeing was `airflow.exceptions.AirflowException: The key has to be less than 250 characters`. It should also print the name of offending key for easy debugging for user.

Going to start printing invalid task_id in this exception.

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",patniharshit,2024-12-06 18:24:00+00:00,[],2025-01-06 00:16:22+00:00,2025-01-06 00:16:22+00:00,https://github.com/apache/airflow/issues/44738,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('area:serialization', ''), ('pending-response', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('affected_version:2.6', 'Issues Reported for 2.6')]","[{'comment_id': 2523906841, 'issue_id': 2723670909, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 6, 18, 24, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2534445852, 'issue_id': 2723670909, 'author': 'gopidesupavan', 'body': 'have you validated on the latest airflow versions? if not can you please try? it seems your using very old version.', 'created_at': datetime.datetime(2024, 12, 11, 7, 56, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2543013976, 'issue_id': 2723670909, 'author': 'ttippinyu', 'body': 'Hi, we have reproduced on the latest airflow version 2.10.3. So the issue seems to be happen when the task id itself is less than 250, but the group id prefix + task id is greater than 250. The reproduction steps are:\r\n\r\n1. start `airflow standalone`\r\n2. create a dag\r\n```\r\nwith DAG(\r\n    dag_id=""foo"",\r\n    schedule=None,\r\n    start_date=datetime.datetime(2022, 3, 4),\r\n    catchup=False,\r\n    tags=[""example"", ""params""],\r\n) as dag:\r\n    with TaskGroup(""A"" * 20):\r\n        EmptyOperator(task_id=""1"" * 20)\r\n```\r\n3. trigger the dag \r\n4. disable the dag\r\n5. clear the task\r\n6. update the dag so that the task_id is now `""1"" * 240` (so that the full task id will now be length 260 - with the `""A"" * 20` group prefix\r\n7. if we check the airflow metadb now, we will find that the task id in `serialized_dag` is `AAAAAAAAAAAAAAAAAAAA.111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111`, which has length 261\r\n8. enable the DAG, the scheduler will crash [at this line](https://sourcegraph.com/github.com/apache/airflow@2.10.3/-/blob/airflow/models/baseoperator.py?L960)\r\n9. fix the task id so it is short again\r\n10. try to start scheduler by running `airflow scheduler` and the scheduler will also have an exception when it calls `_run_scheduler_loop`\r\n11. after a few attempts then `airflow scheduler` will succeed and the short task id is serialized in the DB \r\n\r\nFrom a glance, this seems to happen because in `BaseOperator` we validate the task id without the group id prefix. However, in the metadb we store the ""full"" task id, which includes the group id prefix. Hence, when `_run_scheduler_loop` runs, it throws an exception. \r\n\r\nHaven\'t been able to test on latest `main` but looking at the call of `validate_key` in `BaseOperator` [here](https://sourcegraph.com/github.com/apache/airflow/-/blob/task_sdk/src/airflow/sdk/definitions/baseoperator.py?L735), seems like it only validates individual task id and not the full task id (group id + task id).', 'created_at': datetime.datetime(2024, 12, 14, 9, 1, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564879924, 'issue_id': 2723670909, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 12, 30, 0, 16, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2571807394, 'issue_id': 2723670909, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2025, 1, 6, 0, 16, 22, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-06 18:24:04 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

gopidesupavan on (2024-12-11 07:56:24 UTC): have you validated on the latest airflow versions? if not can you please try? it seems your using very old version.

ttippinyu on (2024-12-14 09:01:57 UTC): Hi, we have reproduced on the latest airflow version 2.10.3. So the issue seems to be happen when the task id itself is less than 250, but the group id prefix + task id is greater than 250. The reproduction steps are:

1. start `airflow standalone`
2. create a dag
```
with DAG(
    dag_id=""foo"",
    schedule=None,
    start_date=datetime.datetime(2022, 3, 4),
    catchup=False,
    tags=[""example"", ""params""],
) as dag:
    with TaskGroup(""A"" * 20):
        EmptyOperator(task_id=""1"" * 20)
```
3. trigger the dag 
4. disable the dag
5. clear the task
6. update the dag so that the task_id is now `""1"" * 240` (so that the full task id will now be length 260 - with the `""A"" * 20` group prefix
7. if we check the airflow metadb now, we will find that the task id in `serialized_dag` is `AAAAAAAAAAAAAAAAAAAA.111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111`, which has length 261
8. enable the DAG, the scheduler will crash [at this line](https://sourcegraph.com/github.com/apache/airflow@2.10.3/-/blob/airflow/models/baseoperator.py?L960)
9. fix the task id so it is short again
10. try to start scheduler by running `airflow scheduler` and the scheduler will also have an exception when it calls `_run_scheduler_loop`
11. after a few attempts then `airflow scheduler` will succeed and the short task id is serialized in the DB 

From a glance, this seems to happen because in `BaseOperator` we validate the task id without the group id prefix. However, in the metadb we store the ""full"" task id, which includes the group id prefix. Hence, when `_run_scheduler_loop` runs, it throws an exception. 

Haven't been able to test on latest `main` but looking at the call of `validate_key` in `BaseOperator` [here](https://sourcegraph.com/github.com/apache/airflow/-/blob/task_sdk/src/airflow/sdk/definitions/baseoperator.py?L735), seems like it only validates individual task id and not the full task id (group id + task id).

github-actions[bot] on (2024-12-30 00:16:12 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2025-01-06 00:16:22 UTC): This issue has been closed because it has not received response from the issue author.

"
2723221408,issue,open,,Support IAM auth when using a managed PostgreSQL from AWS or others,"### Description

Existing database setup seems to only suggest username/password auth for Postgres https://airflow.apache.org/docs/apache-airflow/stable/howto/set-up-database.html#setting-up-a-postgresql-database. It would be great to support IAM auth as well.

### Use case/motivation

Modern security practices include moving away from username/password auth, e.g. to avoid stale credentials and potential credentials leaks. Some companies may not even support connecting with username/password anymore.

### Related issues

https://github.com/apache/airflow/issues/31759

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",andrii-korotkov-verkada,2024-12-06 14:41:54+00:00,[],2024-12-06 15:28:49+00:00,,https://github.com/apache/airflow/issues/44735,"[('area:MetaDB', 'Meta Database related issues.'), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2523497154, 'issue_id': 2723221408, 'author': 'andrii-korotkov-verkada', 'body': ""I'm trying to learn what AWS does for their MWAA (not sure they'll share though)."", 'created_at': datetime.datetime(2024, 12, 6, 15, 28, 48, tzinfo=datetime.timezone.utc)}]","andrii-korotkov-verkada (Issue Creator) on (2024-12-06 15:28:48 UTC): I'm trying to learn what AWS does for their MWAA (not sure they'll share though).

"
2723192700,issue,closed,completed,Support IAM auth for a database when connecting via CLI,"### Description

Right now it looks like a username and password is needed to connect, e.g. https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html#connection-cli. It would be great to support IAM auth with auto-refreshing credentials.

### Use case/motivation

Modern security practices include moving away from username/password to IAM auth, e.g. to avoid stale unrotated credentials and risks of leaking them. In some companies, access to the database via username/password might not even be supported anymore. To have CLI usable, IAM auth needs to be supported. Technically, people can get a temporary password for 15 min, but refreshing it manually would be pretty frustrating.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",andrii-korotkov-verkada,2024-12-06 14:28:27+00:00,[],2025-01-03 22:02:12+00:00,2025-01-03 22:02:12+00:00,https://github.com/apache/airflow/issues/44734,"[('area:CLI', ''), ('kind:feature', 'Feature Requests'), ('pending-response', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2569436059, 'issue_id': 2723192700, 'author': 'nathadfield', 'body': '@andrii-korotkov-verkada Airflow supports connections for many databases and services with a wide variety of authentication methods.  Are you referring to something specific?', 'created_at': datetime.datetime(2025, 1, 3, 15, 41, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569869563, 'issue_id': 2723192700, 'author': 'potiuk', 'body': 'Indeed.  There is nothing to prevent using those mechanisms you mention @andrii-korotkov-verkada - for example Google And AWS provider already   support what you advocate for. Please double check all the authentication mechanism. What exactly you think is not possible or problematic in the current setup ?', 'created_at': datetime.datetime(2025, 1, 3, 21, 58, 29, tzinfo=datetime.timezone.utc)}]","nathadfield on (2025-01-03 15:41:36 UTC): @andrii-korotkov-verkada Airflow supports connections for many databases and services with a wide variety of authentication methods.  Are you referring to something specific?

potiuk on (2025-01-03 21:58:29 UTC): Indeed.  There is nothing to prevent using those mechanisms you mention @andrii-korotkov-verkada - for example Google And AWS provider already   support what you advocate for. Please double check all the authentication mechanism. What exactly you think is not possible or problematic in the current setup ?

"
2723168020,issue,closed,completed,Upgrade Flask to 2.3.3,"### Description

Right now Flask is pinned to be <2.3 https://github.com/apache/airflow/blob/3d421f78d7046474c5684580a744f87160378935/hatch_build.py#L387-L390. Update this to be <2.4 or <3.

### Use case/motivation

This pin makes it pretty hard to use in some environments, such as Bazel-built monorepo with a Flask version higher than that. Also, an upgrade probably needs to happen eventually anyways.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",andrii-korotkov-verkada,2024-12-06 14:16:57+00:00,[],2024-12-06 15:13:11+00:00,2024-12-06 15:06:12+00:00,https://github.com/apache/airflow/issues/44733,"[('kind:feature', 'Feature Requests'), ('area:dependencies', 'Issues related to dependencies problems'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2523347267, 'issue_id': 2723168020, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 6, 14, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2523450705, 'issue_id': 2723168020, 'author': 'potiuk', 'body': 'I am afraid it will not happen.\r\n\r\nWe are getting rid of flask altogether anyway in Airlfow 3. This limit comes from connexion 2 - latest version 2.14.2 that we are getting rid as well in Airflow 3. One of the reasons is that upgrading to Connexion 3 is non-trivial (https://github.com/apache/airflow/pull/39055 was a massive green PR that nobody had the courage to continue working on), actually it\'s very hard to migrate because they changed architecture completely, so we are switching to FastAPi. Because it turned out to be easier.\r\n\r\nSo ... this issue is ""won\'t do""', 'created_at': datetime.datetime(2024, 12, 6, 15, 6, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2523465267, 'issue_id': 2723168020, 'author': 'andrii-korotkov-verkada', 'body': 'Ah, okay.\r\n\r\n> We are getting rid of flask altogether anyway in Airlfow 3.\r\n\r\nThis would work too.', 'created_at': datetime.datetime(2024, 12, 6, 15, 13, 9, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-06 14:17:00 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-12-06 15:06:12 UTC): I am afraid it will not happen.

We are getting rid of flask altogether anyway in Airlfow 3. This limit comes from connexion 2 - latest version 2.14.2 that we are getting rid as well in Airflow 3. One of the reasons is that upgrading to Connexion 3 is non-trivial (https://github.com/apache/airflow/pull/39055 was a massive green PR that nobody had the courage to continue working on), actually it's very hard to migrate because they changed architecture completely, so we are switching to FastAPi. Because it turned out to be easier.

So ... this issue is ""won't do""

andrii-korotkov-verkada (Issue Creator) on (2024-12-06 15:13:09 UTC): Ah, okay.


This would work too.

"
2722839513,issue,closed,completed,Error while trying to send logs threw elasticsearch,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Hello all, hope you doing well.

While trying to send logs to elasticsearch directly threw the elastic adapter (inside airflow conf), it does not work. 
--> today we pass threw file share (azure) mounted as PV inside K8S, then logstash pipeline, but it costs a lots per years ...

I think the adapter is broken, even when trying to test a connection directly inside the webserver, we have an error :

`'ESConnection' object has no attribute 'close'`

Then when trying to send logs to elasticsearch, it does not try to send logs and it cannot connect to the elasticsearch when trying to get back logs.

Anyway, while trying to look for logs inside a DAG we also have this error :

`elasticsearch.AuthenticationException: AuthenticationException(401, 'security_exception', 'missing authentication credentials for REST request [/airflow-logs-*/_count]')    
airflow-webserver-1  | 172.18.0.1 - - [06/Dec/2024:11:26:46 +0000] ""GET /api/v1/dags/debug_airflow_to_elastic/dagRuns/manual__2024-12-06T11:26:37.042642+00:00/taskInstances/print_debug_message/logs/1?full_content=false HTTP/1.1"" 500 1588 ""http://localhost:8080/dags/debug_airflow_to_elastic/grid?dag_run_id=manual__2024-12-06T11%3A26%3A37.042642%2B00%3A00&task_id=print_debug_message&tab=logs"" ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36""`

But the I tried with an api key, user password and even both but cannot get rid of it.

I think there is a bug around it or did we do something wrong.

Thousands thanks !

Benjamin

### What you think should happen instead?

_No response_

### How to reproduce

Just add a connection to elasticsearch, try to connect to it.

Then add remote logging inside conf.

It does not try to send logs and it cannot connect to the elasticsearch.

### Operating System

Kubernetes and docker compose. (both d'ont work)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

`
[core]
sensitive_var_conn_names = key,login,secret,pass,auth
hide_sensitive_var_conn_fields = True
max_map_length = 16396
expose_config = non-sensitive-only
load_examples = False
test_connection = Enabled
[webserver]
show_trigger_form_if_no_params = True
allow_testing_connections = Enabled
[logging]
remote_logging = True
remote_log_conn_id = elasticsearch_default
logging_level = INFO
[elasticsearch]
host = ************************************
write_stdout = True
json_format = True
index_patterns = airflow-logs-*
[elasticsearch_config]
verify_certs=False
`

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",benjamindinh-loreal,2024-12-06 11:42:17+00:00,[],2024-12-29 23:16:30+00:00,2024-12-29 23:16:30+00:00,https://github.com/apache/airflow/issues/44724,"[('kind:bug', 'This is a clearly a bug'), ('area:logging', ''), ('pending-response', ''), ('area:core', '')]","[{'comment_id': 2522996002, 'issue_id': 2722839513, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 6, 11, 42, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2544034598, 'issue_id': 2722839513, 'author': 'eladkal', 'body': 'cc @Owen-CH-Leung wdyt?', 'created_at': datetime.datetime(2024, 12, 15, 20, 4, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2548412080, 'issue_id': 2722839513, 'author': 'Owen-CH-Leung', 'body': ""From your error log, it seems that the elasticsearch cluster has a security setup to prevent unauthorised access from your k8s cluster. The `AuthenticationException` is a clear indication.\r\n\r\nI'd advise to start by testing connectivity outside of Airflow to narrow down the root cause. For example, try running a standalone Python script inside the same Kubernetes cluster that hosts your Airflow environment. In that script, use the official [elasticsearch-py] (https://github.com/elastic/elasticsearch-py) client library to connect to your Elasticsearch cluster and try to do sth like `es_client.ping()`. Make sure to experiment with SSL-related parameters such as ssl_verify and ca_certs until you can reliably connect.\r\n\r\nOnce you've confirmed that your Python script can successfully interact with Elasticsearch, you can mirror those working configurations in your airflow.cfg (e.g., adjusting the Elasticsearch configuration sections) and restart Airflow"", 'created_at': datetime.datetime(2024, 12, 17, 13, 5, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2550888117, 'issue_id': 2722839513, 'author': 'benjamindinh-loreal', 'body': 'Hello @Owen-CH-Leung,\r\n\r\nThank you for your response.\r\n\r\nThe cluster is accessible from the outside, and some pipelines already successfully send data to Elasticsearch using elasticsearch-py.\r\n\r\nHowever, the error logs suggest that Airflow is not transmitting the authentication parameters:\r\n\r\n`missing authentication credentials for REST request`\r\n\r\nThe elasticsearch_default connection has already been created, so I’m wondering if there might be a workaround to ensure Airflow sends the authentication details to Elasticsearch properly?\r\n\r\nThank you', 'created_at': datetime.datetime(2024, 12, 18, 9, 59, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551328540, 'issue_id': 2722839513, 'author': 'Owen-CH-Leung', 'body': 'You can define the credentials in `elasticsearch_configs` session in your airflow cfg.\r\n\r\nhttps://airflow.apache.org/docs/apache-airflow-providers-elasticsearch/stable/configurations-ref.html#elasticsearch-configs\r\n\r\nIn the `elasticsearch_configs` session, you can pass in any parameters that elasticsearch client accepts. Example: \r\n\r\n```\r\n[elasticsearch_configs] \r\nhttp_compress = True \r\nca_certs = /root/ca.pem \r\napi_key = ""SOMEAPIKEY"" \r\nverify_certs = True\r\n```\r\n\r\nAll the params you define will be passed into the elasticsearch python library like `elasticsearch.Elasticsearch(**kwargs)`', 'created_at': datetime.datetime(2024, 12, 18, 13, 29, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2554997515, 'issue_id': 2722839513, 'author': 'julienlagorsse-loreal', 'body': ""Thanks, it solved the issue, but we are still blocked after that, the data isn't push, however we can see pull of logs. We will create another issue for that. Thank you"", 'created_at': datetime.datetime(2024, 12, 19, 16, 39, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2557006395, 'issue_id': 2722839513, 'author': 'julienlagorsse-loreal', 'body': ""The doc seems to be missleading, the title is Writing logs to Elasticsearch but it doesn't write anything to Elasticsearch, only read."", 'created_at': datetime.datetime(2024, 12, 20, 13, 22, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2557632100, 'issue_id': 2722839513, 'author': 'potiuk', 'body': ""> The doc seems to be missleading, the title is Writing logs to Elasticsearch but it doesn't write anything to Elasticsearch, only read.\r\n\r\nActually not really - It's about both writing (and then reading the logs. If you read the first paragraph (that's the first time I see the docs). The docs say that you can get the logs from stdout and forward them (write) to elasticsearch by `fluentd, logstash or others`.\r\n\r\n> Airflow can be configured to read task logs from Elasticsearch and optionally write logs to stdout in standard or json format. These logs can later be collected and forwarded to the Elasticsearch cluster using tools like fluentd, logstash or others.\r\n\r\n\r\nAre you doing it @julienlagorsse-loreal ? Maybe that is the problem that you are not forwarding the stdout logs to elasticsearch?"", 'created_at': datetime.datetime(2024, 12, 20, 19, 51, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2557694496, 'issue_id': 2722839513, 'author': 'julienlagorsse-loreal', 'body': ""We have issues on file share with k8s, anyway it's not related to the bug, but the title is clearly misleading, it say write logs to elastic, not read logs from elastic or write logs to stdout ... But I agree the doc paragraph is not.\r\n\r\nEnvoyé à partir de Outlook pour Android<https://aka.ms/AAb9ysg>\r\n\r\n\r\nC1 - Internal use\r\n\r\n________________________________\r\nFrom: Jarek Potiuk ***@***.***>\r\nSent: Friday, December 20, 2024 8:52:13 PM\r\nTo: apache/airflow ***@***.***>\r\nCc: LAGORSSE Julien - FREELANCE.COM ***@***.***>; Mention ***@***.***>\r\nSubject: Re: [apache/airflow] Error while trying to send logs threw elasticsearch (Issue #44724)\r\n\r\nEXTERNAL EMAIL: BE VIGILANT\r\n\r\n\r\nThe doc seems to be missleading, the title is Writing logs to Elasticsearch but it doesn't write anything to Elasticsearch, only read.\r\n\r\nActually not really - It's about both writing (and then reading the logs. If you read the first paragraph (that's the first time I see the docs). The docs say that you can get the logs from stdout and forward them (write) to elasticsearch by fluentd, logstash or others.\r\n\r\nAirflow can be configured to read task logs from Elasticsearch and optionally write logs to stdout in standard or json format. These logs can later be collected and forwarded to the Elasticsearch cluster using tools like fluentd, logstash or others.\r\n\r\nAre you doing it @julienlagorsse-loreal<https://urldefense.com/v3/__https://github.com/julienlagorsse-loreal__;!!IY5JXqZAIQ!8kh68iXFJWWysNnClYw4Xu_NkAOtn7hDyAbnZrfeNTfefGic2TZ9O8JeDyr_z95KgouWMaCt4-3lybhoK1CwzEEJBf3B$> ? Maybe that is the problem that you are not forwarding the stdout logs to elasticsearch?\r\n\r\n—\r\nReply to this email directly, view it on GitHub<https://urldefense.com/v3/__https://github.com/apache/airflow/issues/44724*issuecomment-2557632100__;Iw!!IY5JXqZAIQ!8kh68iXFJWWysNnClYw4Xu_NkAOtn7hDyAbnZrfeNTfefGic2TZ9O8JeDyr_z95KgouWMaCt4-3lybhoK1CwzE5zm7Lm$>, or unsubscribe<https://urldefense.com/v3/__https://github.com/notifications/unsubscribe-auth/ATGMVPQYF2MI7T3QPTBPEW32GRYO3AVCNFSM6AAAAABTEPZO3OVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKNJXGYZTEMJQGA__;!!IY5JXqZAIQ!8kh68iXFJWWysNnClYw4Xu_NkAOtn7hDyAbnZrfeNTfefGic2TZ9O8JeDyr_z95KgouWMaCt4-3lybhoK1CwzKpGjsyx$>.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***>\r\n\r\n\r\nThis message and any attachments are confidential and intended solely for the addressees.\r\nIf you receive this message in error, please delete it and immediately notify the sender. If the reader of this message is not the intended recipient, you are hereby notified that any unauthorized use, copying or dissemination is prohibited. E-mails are susceptible to alteration. Neither LOREAL nor any of its subsidiaries or affiliates shall be liable for the message if altered, changed or falsified."", 'created_at': datetime.datetime(2024, 12, 20, 20, 45, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2557748142, 'issue_id': 2722839513, 'author': 'potiuk', 'body': '> We have issues on file share with k8s, anyway it\'s not related to the bug, but the title is clearly misleading, it say write logs to elastic, not read logs from elastic or write logs to stdout ... But I agree the doc paragraph is not.\r\n\r\n\r\nCan you please propose an update to the page. It\'s as simple as clickign ""Suggest a change on this page"" and it will open a Pull Request where you can propose a change tha will remove the confusion.\r\n\r\nCan we count on it @julienlagorsse-loreal ?', 'created_at': datetime.datetime(2024, 12, 20, 21, 37, 1, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-06 11:42:20 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2024-12-15 20:04:58 UTC): cc @Owen-CH-Leung wdyt?

Owen-CH-Leung on (2024-12-17 13:05:32 UTC): From your error log, it seems that the elasticsearch cluster has a security setup to prevent unauthorised access from your k8s cluster. The `AuthenticationException` is a clear indication.

I'd advise to start by testing connectivity outside of Airflow to narrow down the root cause. For example, try running a standalone Python script inside the same Kubernetes cluster that hosts your Airflow environment. In that script, use the official [elasticsearch-py] (https://github.com/elastic/elasticsearch-py) client library to connect to your Elasticsearch cluster and try to do sth like `es_client.ping()`. Make sure to experiment with SSL-related parameters such as ssl_verify and ca_certs until you can reliably connect.

Once you've confirmed that your Python script can successfully interact with Elasticsearch, you can mirror those working configurations in your airflow.cfg (e.g., adjusting the Elasticsearch configuration sections) and restart Airflow

benjamindinh-loreal (Issue Creator) on (2024-12-18 09:59:35 UTC): Hello @Owen-CH-Leung,

Thank you for your response.

The cluster is accessible from the outside, and some pipelines already successfully send data to Elasticsearch using elasticsearch-py.

However, the error logs suggest that Airflow is not transmitting the authentication parameters:

`missing authentication credentials for REST request`

The elasticsearch_default connection has already been created, so I’m wondering if there might be a workaround to ensure Airflow sends the authentication details to Elasticsearch properly?

Thank you

Owen-CH-Leung on (2024-12-18 13:29:21 UTC): You can define the credentials in `elasticsearch_configs` session in your airflow cfg.

https://airflow.apache.org/docs/apache-airflow-providers-elasticsearch/stable/configurations-ref.html#elasticsearch-configs

In the `elasticsearch_configs` session, you can pass in any parameters that elasticsearch client accepts. Example: 

```
[elasticsearch_configs] 
http_compress = True 
ca_certs = /root/ca.pem 
api_key = ""SOMEAPIKEY"" 
verify_certs = True
```

All the params you define will be passed into the elasticsearch python library like `elasticsearch.Elasticsearch(**kwargs)`

julienlagorsse-loreal on (2024-12-19 16:39:11 UTC): Thanks, it solved the issue, but we are still blocked after that, the data isn't push, however we can see pull of logs. We will create another issue for that. Thank you

julienlagorsse-loreal on (2024-12-20 13:22:41 UTC): The doc seems to be missleading, the title is Writing logs to Elasticsearch but it doesn't write anything to Elasticsearch, only read.

potiuk on (2024-12-20 19:51:50 UTC): Actually not really - It's about both writing (and then reading the logs. If you read the first paragraph (that's the first time I see the docs). The docs say that you can get the logs from stdout and forward them (write) to elasticsearch by `fluentd, logstash or others`.



Are you doing it @julienlagorsse-loreal ? Maybe that is the problem that you are not forwarding the stdout logs to elasticsearch?

julienlagorsse-loreal on (2024-12-20 20:45:26 UTC): We have issues on file share with k8s, anyway it's not related to the bug, but the title is clearly misleading, it say write logs to elastic, not read logs from elastic or write logs to stdout ... But I agree the doc paragraph is not.

Envoyé à partir de Outlook pour Android<https://aka.ms/AAb9ysg>


C1 - Internal use

________________________________
From: Jarek Potiuk ***@***.***>
Sent: Friday, December 20, 2024 8:52:13 PM
To: apache/airflow ***@***.***>
Cc: LAGORSSE Julien - FREELANCE.COM ***@***.***>; Mention ***@***.***>
Subject: Re: [apache/airflow] Error while trying to send logs threw elasticsearch (Issue #44724)

EXTERNAL EMAIL: BE VIGILANT


The doc seems to be missleading, the title is Writing logs to Elasticsearch but it doesn't write anything to Elasticsearch, only read.

Actually not really - It's about both writing (and then reading the logs. If you read the first paragraph (that's the first time I see the docs). The docs say that you can get the logs from stdout and forward them (write) to elasticsearch by fluentd, logstash or others.

Airflow can be configured to read task logs from Elasticsearch and optionally write logs to stdout in standard or json format. These logs can later be collected and forwarded to the Elasticsearch cluster using tools like fluentd, logstash or others.

Are you doing it @julienlagorsse-loreal<https://urldefense.com/v3/__https://github.com/julienlagorsse-loreal__;!!IY5JXqZAIQ!8kh68iXFJWWysNnClYw4Xu_NkAOtn7hDyAbnZrfeNTfefGic2TZ9O8JeDyr_z95KgouWMaCt4-3lybhoK1CwzEEJBf3B$> ? Maybe that is the problem that you are not forwarding the stdout logs to elasticsearch?

—
Reply to this email directly, view it on GitHub<https://urldefense.com/v3/__https://github.com/apache/airflow/issues/44724*issuecomment-2557632100__;Iw!!IY5JXqZAIQ!8kh68iXFJWWysNnClYw4Xu_NkAOtn7hDyAbnZrfeNTfefGic2TZ9O8JeDyr_z95KgouWMaCt4-3lybhoK1CwzE5zm7Lm$>, or unsubscribe<https://urldefense.com/v3/__https://github.com/notifications/unsubscribe-auth/ATGMVPQYF2MI7T3QPTBPEW32GRYO3AVCNFSM6AAAAABTEPZO3OVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKNJXGYZTEMJQGA__;!!IY5JXqZAIQ!8kh68iXFJWWysNnClYw4Xu_NkAOtn7hDyAbnZrfeNTfefGic2TZ9O8JeDyr_z95KgouWMaCt4-3lybhoK1CwzKpGjsyx$>.
You are receiving this because you were mentioned.Message ID: ***@***.***>


This message and any attachments are confidential and intended solely for the addressees.
If you receive this message in error, please delete it and immediately notify the sender. If the reader of this message is not the intended recipient, you are hereby notified that any unauthorized use, copying or dissemination is prohibited. E-mails are susceptible to alteration. Neither LOREAL nor any of its subsidiaries or affiliates shall be liable for the message if altered, changed or falsified.

potiuk on (2024-12-20 21:37:01 UTC): Can you please propose an update to the page. It's as simple as clickign ""Suggest a change on this page"" and it will open a Pull Request where you can propose a change tha will remove the confusion.

Can we count on it @julienlagorsse-loreal ?

"
2721242444,issue,closed,not_planned,Installing sqoop provider generates error with airflow cmd,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Testing new installation of Airflow 2.10.3. If I install **apache-airflow-providers-apache-sqoop** invocations of airflow command produce the error:
 
> $ airflow info
> Traceback (most recent call last):
>   File ""/usr/local/bin/airflow"", line 8, in <module>
>     sys.exit(main())
>   File ""/airflow/git/airflow/__main__.py"", line 62, in main
>     args.func(args)
>   File ""/airflow/git/airflow/cli/cli_config.py"", line 49, in command
>     return func(*args, **kwargs)
>   File ""/airflow/git/airflow/utils/cli.py"", line 387, in _wrapper
>     f(*args, **kwargs)
>   File ""/airflow/git/airflow/utils/providers_configuration_loader.py"", line 54, in wrapped_function
>     ProvidersManager().initialize_providers_configuration()
>   File ""/airflow/git/airflow/providers_manager.py"", line 384, in wrapped_function
>     func(*args, **kwargs)
>   File ""/airflow/git/airflow/providers_manager.py"", line 580, in initialize_providers_configuration
>     self._initialize_providers_configuration()
>   File ""/airflow/git/airflow/providers_manager.py"", line 593, in _initialize_providers_configuration
>     self.initialize_providers_list()
>   File ""/airflow/git/airflow/providers_manager.py"", line 384, in wrapped_function
>     func(*args, **kwargs)
>   File ""/airflow/git/airflow/providers_manager.py"", line 495, in initialize_providers_list
>     self._discover_all_providers_from_packages()
>   File ""/airflow/git/airflow/providers_manager.py"", line 628, in _discover_all_providers_from_packages
>     provider_info = entry_point.load()()
>   File ""/usr/local/lib/python3.9/site-packages/importlib_metadata/__init__.py"", line 211, in load
>     module = import_module(match.group('module'))
>   File ""/usr/lib64/python3.9/importlib/__init__.py"", line 127, in import_module
>     return _bootstrap._gcd_import(name[level:], package, level)
>   File ""<frozen importlib._bootstrap>"", line 1030, in _gcd_import
>   File ""<frozen importlib._bootstrap>"", line 1007, in _find_and_load
>   File ""<frozen importlib._bootstrap>"", line 972, in _find_and_load_unlocked
>   File ""<frozen importlib._bootstrap>"", line 228, in _call_with_frames_removed
>   File ""<frozen importlib._bootstrap>"", line 1030, in _gcd_import
>   File ""<frozen importlib._bootstrap>"", line 1007, in _find_and_load
>   File ""<frozen importlib._bootstrap>"", line 984, in _find_and_load_unlocked
> ModuleNotFoundError: No module named 'airflow.providers.apache.sqoop'


### What you think should happen instead?

I assume this error should not appear. No issues with a few other providers.

### How to reproduce

I'm installing from git branch 2.10.3 using command:
`pip install -e . -c https://raw.githubusercontent.com/apache/airflow/constraints-2.10.3/constraints-3.9.txt`

### Operating System

Rocky Linux release 8.10 (Green Obsidian)

### Versions of Apache Airflow Providers

apache-airflow-providers-apache-sqoop==4.2.1

I also tried earlier versions

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",b3c0bb,2024-12-05 19:37:17+00:00,[],2024-12-11 19:00:08+00:00,2024-12-11 10:47:50+00:00,https://github.com/apache/airflow/issues/44703,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:apache-sqoop', '')]","[{'comment_id': 2521243198, 'issue_id': 2721242444, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 5, 19, 37, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2534409910, 'issue_id': 2721242444, 'author': 'gopidesupavan', 'body': 'Sqoop provider was terminated from apache-airflow. https://airflow.apache.org/docs/apache-airflow-providers-apache-sqoop/4.2.1/changelog.html.', 'created_at': datetime.datetime(2024, 12, 11, 7, 54, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2535502986, 'issue_id': 2721242444, 'author': 'potiuk', 'body': 'Indeed', 'created_at': datetime.datetime(2024, 12, 11, 10, 47, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2536267658, 'issue_id': 2721242444, 'author': 'b3c0bb', 'body': 'Ok, thanks. I thought the provider could still be used despite being deprecated, but I guess that is not the case.', 'created_at': datetime.datetime(2024, 12, 11, 15, 10, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2536486501, 'issue_id': 2721242444, 'author': 'potiuk', 'body': 'Hard to say - old providers, yes can be used. but you are somehow mixing installing old airflow in editable way with installing released provider. Those two have nothing to do with each other. Generally providers are never released from version branches, they are always released from main branch and editable installation just uses providers that were available in sources at the moment airflow was released. So yes - released provided are supported, but you are trying to use sources zat the moment where the provider has already been removed and install it from there. That is not going to work', 'created_at': datetime.datetime(2024, 12, 11, 16, 29, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2536876076, 'issue_id': 2721242444, 'author': 'b3c0bb', 'body': 'Makes sense, thanks for the info @potiuk', 'created_at': datetime.datetime(2024, 12, 11, 19, 0, 7, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-05 19:37:20 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

gopidesupavan on (2024-12-11 07:54:19 UTC): Sqoop provider was terminated from apache-airflow. https://airflow.apache.org/docs/apache-airflow-providers-apache-sqoop/4.2.1/changelog.html.

potiuk on (2024-12-11 10:47:58 UTC): Indeed

b3c0bb (Issue Creator) on (2024-12-11 15:10:18 UTC): Ok, thanks. I thought the provider could still be used despite being deprecated, but I guess that is not the case.

potiuk on (2024-12-11 16:29:50 UTC): Hard to say - old providers, yes can be used. but you are somehow mixing installing old airflow in editable way with installing released provider. Those two have nothing to do with each other. Generally providers are never released from version branches, they are always released from main branch and editable installation just uses providers that were available in sources at the moment airflow was released. So yes - released provided are supported, but you are trying to use sources zat the moment where the provider has already been removed and install it from there. That is not going to work

b3c0bb (Issue Creator) on (2024-12-11 19:00:07 UTC): Makes sense, thanks for the info @potiuk

"
2720276349,issue,open,,"GlueJobOperator stuck in running state, even when the job is completed on AWS, when Verbose=True","### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==9.1.0

### Apache Airflow version

2.10.3

### Operating System

ubuntu-22.04

### Deployment

Astronomer

### Deployment details

_No response_

### What happened

GlueJobOperator was stuck in running for a long time, while the actual Glue job on AWS took a minute to complete. This is only happening when Verbose is set to Truehappens

### What you think should happen instead

It should not get stuck for a long time when Verbose is set to True

### How to reproduce

I used the following DAG Code:

```
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.operators.glue import GlueJobOperator
from datetime import timedelta

from datetime import datetime


def _start():
    print(""Hi"")


def _end():
    print(""Job end"")


with DAG(""dag_glue_script_python"", catchup=False) as dag:
    start = PythonOperator(task_id=""start"", python_callable=_start)
    
    start_glue_job = GlueJobOperator(
        job_name='sleep2',
        task_id='run',
        aws_conn_id='aws_cre',
        create_job_kwargs={""NumberOfWorkers"": 1, ""WorkerType"": ""G.1X""},
        stop_job_run_on_kill=True,
        verbose=True,
        wait_for_completion=True,
        deferrable=True,
        job_poll_interval=15
    )

    end = PythonOperator(task_id=""end"", python_callable=_end)

start >> start_glue_job >> end

```


### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rawwar,2024-12-05 12:24:43+00:00,[],2025-02-07 18:00:04+00:00,,https://github.com/apache/airflow/issues/44694,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('good first issue', '')]","[{'comment_id': 2520184962, 'issue_id': 2720276349, 'author': 'rawwar', 'body': 'I modified the glue.py(in aws provider 9.1.0) hook\'s `print_job_logs` method and added a few print statements. Here\'s the updated glue.py and the task logs:\r\n\r\n```\r\ndef print_job_logs(\r\n            self,\r\n            job_name: str,\r\n            run_id: str,\r\n            continuation_tokens: LogContinuationTokens,\r\n        ):\r\n            """"""\r\n            Print the latest job logs to the Airflow task log and updates the continuation tokens.\r\n\r\n            :param continuation_tokens: the tokens where to resume from when reading logs.\r\n                The object gets updated with the new tokens by this method.\r\n            """"""\r\n            log_client = self.logs_hook.get_conn()\r\n            paginator = log_client.get_paginator(""filter_log_events"")\r\n            \r\n\r\n            def display_logs_from(log_group: str, continuation_token: str | None) -> str | None:\r\n                """"""Mutualize iteration over the 2 different log streams glue jobs write to.""""""\r\n                print(f""display_logs_from start with log_group={log_group}, continuation_token={continuation_token}"")\r\n                fetched_logs = []\r\n                next_token = continuation_token\r\n                try:\r\n                    for response in paginator.paginate(\r\n                        logGroupName=log_group,\r\n                        logStreamNames=[run_id],\r\n                        PaginationConfig={""StartingToken"": continuation_token},\r\n                    ):\r\n                        print(""paginator response"", response)\r\n                        fetched_logs.extend([event[""message""] for event in response[""events""]])\r\n                        # if the response is empty there is no nextToken in it\r\n                        next_token = response.get(""nextToken"") or next_token\r\n                        print(""fetched_logs"", fetched_logs)\r\n                        print(""next_token"", next_token)\r\n                except ClientError as e:\r\n                    if e.response[""Error""][""Code""] == ""ResourceNotFoundException"":\r\n                        # we land here when the log groups/streams don\'t exist yet\r\n                        self.log.warning(\r\n                            ""No new Glue driver logs so far.\\n""\r\n                            ""If this persists, check the CloudWatch dashboard at: %r."",\r\n                            f""https://{self.conn_region_name}.console.aws.amazon.com/cloudwatch/home"",\r\n                        )\r\n                    else:\r\n                        print(""error"", e)\r\n                        raise\r\n                print(""finished paginator"")\r\n                if len(fetched_logs):\r\n                    # Add a tab to indent those logs and distinguish them from airflow logs.\r\n                    # Log lines returned already contain a newline character at the end.\r\n                    messages = ""\\t"".join(fetched_logs)\r\n                    self.log.info(""Glue Job Run %s Logs:\\n\\t%s"", log_group, messages)\r\n                else:\r\n                    self.log.info(""No new log from the Glue Job in %s"", log_group)\r\n                return next_token\r\n\r\n            log_group_prefix = self.conn.get_job_run(JobName=job_name, RunId=run_id)[""JobRun""][""LogGroupName""]\r\n            log_group_default = f""{log_group_prefix}/{DEFAULT_LOG_SUFFIX}""\r\n            log_group_error = f""{log_group_prefix}/{ERROR_LOG_SUFFIX}""\r\n            print(f""log_group_prefix={log_group_prefix}, log_group_default={log_group_default}, log_group_error={log_group_error}"")\r\n            # one would think that the error log group would contain only errors, but it actually contains\r\n            # a lot of interesting logs too, so it\'s valuable to have both\r\n            print(""before display_logs_from"")\r\n            continuation_tokens.output_stream_continuation = display_logs_from(\r\n                log_group_default, continuation_tokens.output_stream_continuation\r\n            )\r\n            print(""After"")\r\n            continuation_tokens.error_stream_continuation = display_logs_from(\r\n                log_group_error, continuation_tokens.error_stream_continuation\r\n            )\r\n            print(""Done"")\r\n```\r\n\r\n\r\nTask logs are attached\r\n[task logs.log](https://github.com/user-attachments/files/18023154/task.logs.log)', 'created_at': datetime.datetime(2024, 12, 5, 12, 27, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2520187362, 'issue_id': 2720276349, 'author': 'rawwar', 'body': 'Issue seems to be that, `paginate.paginate` kept going in a loop until it ultimately failed', 'created_at': datetime.datetime(2024, 12, 5, 12, 28, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2520670561, 'issue_id': 2720276349, 'author': 'eladkal', 'body': 'Intresting timing. there was a fix about verbose for GlueJobTrigger https://github.com/apache/airflow/pull/43622', 'created_at': datetime.datetime(2024, 12, 5, 15, 44, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2521311385, 'issue_id': 2720276349, 'author': 'rawwar', 'body': ""> Intresting timing. there was a fix about verbose for GlueJobTrigger #43622\r\n\r\n@eladkal, I don't think the fix in #43622 is relevant here(Or maybe you were just mentioning about it). Issue happens in both deferrable and non-deferrable mode. Also, I noticed that this issue happens only with one of our customers' AWS accounts. I checked with my personal AWS account with exact permissions, and I can't replicate it. This makes me think there's some edge case in Boto3's paginate, which we need to handle in the provider code."", 'created_at': datetime.datetime(2024, 12, 5, 20, 17, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569984392, 'issue_id': 2720276349, 'author': 'ferruzzi', 'body': 'I can\'t seem to reproduce this on my laptop either, but would adding a call to get_job_state [here](https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/amazon/aws/hooks/glue.py#L269) do the trick?  Something along the lines of:\r\n\r\n```\r\nif len(fetched_logs):\r\n    # Add a tab to indent those logs and distinguish them from airflow logs.\r\n    # Log lines returned already contain a newline character at the end.\r\n    messages = ""\\t"".join(fetched_logs)\r\n    self.log.info(""Glue Job Run %s Logs:\\n\\t%s"", log_group, messages)\r\n\r\nelif self.get_job_state(job_name, run_id) in [""FAILED"", ""TIMEOUT"", ""SUCCEEDED"", ""STOPPED""]:  \r\n   # no new logs and the job has terminated\r\n    return\r\n\r\nelse:\r\n  # no new logs but job isn\'t finished, print a ""waiting..."" message\r\n  self.log.info(""No new log from the Glue Job in %s"", log_group)\r\n```', 'created_at': datetime.datetime(2025, 1, 4, 1, 18, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569990204, 'issue_id': 2720276349, 'author': 'rawwar', 'body': ""@ferruzzi , I'll give this a try and give you an update."", 'created_at': datetime.datetime(2025, 1, 4, 1, 31, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2606924847, 'issue_id': 2720276349, 'author': 'rawwar', 'body': ""[logs.log](https://github.com/user-attachments/files/18555784/logs.log)\n@ferruzzi, the Job state check did not help. It's the same issue. It keeps fetching new tokens repeatedly. I'll share the logs in some time."", 'created_at': datetime.datetime(2025, 1, 22, 10, 58, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2643623736, 'issue_id': 2720276349, 'author': 'ferruzzi', 'body': ""That's a lot of empty paginator replies.   Perhaps add a log message to check that the job state check you added is being hit and enforced?  Maybe that would give some insight here?"", 'created_at': datetime.datetime(2025, 2, 7, 18, 0, 3, tzinfo=datetime.timezone.utc)}]","rawwar (Issue Creator) on (2024-12-05 12:27:42 UTC): I modified the glue.py(in aws provider 9.1.0) hook's `print_job_logs` method and added a few print statements. Here's the updated glue.py and the task logs:

```
def print_job_logs(
            self,
            job_name: str,
            run_id: str,
            continuation_tokens: LogContinuationTokens,
        ):
            """"""
            Print the latest job logs to the Airflow task log and updates the continuation tokens.

            :param continuation_tokens: the tokens where to resume from when reading logs.
                The object gets updated with the new tokens by this method.
            """"""
            log_client = self.logs_hook.get_conn()
            paginator = log_client.get_paginator(""filter_log_events"")
            

            def display_logs_from(log_group: str, continuation_token: str | None) -> str | None:
                """"""Mutualize iteration over the 2 different log streams glue jobs write to.""""""
                print(f""display_logs_from start with log_group={log_group}, continuation_token={continuation_token}"")
                fetched_logs = []
                next_token = continuation_token
                try:
                    for response in paginator.paginate(
                        logGroupName=log_group,
                        logStreamNames=[run_id],
                        PaginationConfig={""StartingToken"": continuation_token},
                    ):
                        print(""paginator response"", response)
                        fetched_logs.extend([event[""message""] for event in response[""events""]])
                        # if the response is empty there is no nextToken in it
                        next_token = response.get(""nextToken"") or next_token
                        print(""fetched_logs"", fetched_logs)
                        print(""next_token"", next_token)
                except ClientError as e:
                    if e.response[""Error""][""Code""] == ""ResourceNotFoundException"":
                        # we land here when the log groups/streams don't exist yet
                        self.log.warning(
                            ""No new Glue driver logs so far.\n""
                            ""If this persists, check the CloudWatch dashboard at: %r."",
                            f""https://{self.conn_region_name}.console.aws.amazon.com/cloudwatch/home"",
                        )
                    else:
                        print(""error"", e)
                        raise
                print(""finished paginator"")
                if len(fetched_logs):
                    # Add a tab to indent those logs and distinguish them from airflow logs.
                    # Log lines returned already contain a newline character at the end.
                    messages = ""\t"".join(fetched_logs)
                    self.log.info(""Glue Job Run %s Logs:\n\t%s"", log_group, messages)
                else:
                    self.log.info(""No new log from the Glue Job in %s"", log_group)
                return next_token

            log_group_prefix = self.conn.get_job_run(JobName=job_name, RunId=run_id)[""JobRun""][""LogGroupName""]
            log_group_default = f""{log_group_prefix}/{DEFAULT_LOG_SUFFIX}""
            log_group_error = f""{log_group_prefix}/{ERROR_LOG_SUFFIX}""
            print(f""log_group_prefix={log_group_prefix}, log_group_default={log_group_default}, log_group_error={log_group_error}"")
            # one would think that the error log group would contain only errors, but it actually contains
            # a lot of interesting logs too, so it's valuable to have both
            print(""before display_logs_from"")
            continuation_tokens.output_stream_continuation = display_logs_from(
                log_group_default, continuation_tokens.output_stream_continuation
            )
            print(""After"")
            continuation_tokens.error_stream_continuation = display_logs_from(
                log_group_error, continuation_tokens.error_stream_continuation
            )
            print(""Done"")
```


Task logs are attached
[task logs.log](https://github.com/user-attachments/files/18023154/task.logs.log)

rawwar (Issue Creator) on (2024-12-05 12:28:33 UTC): Issue seems to be that, `paginate.paginate` kept going in a loop until it ultimately failed

eladkal on (2024-12-05 15:44:08 UTC): Intresting timing. there was a fix about verbose for GlueJobTrigger https://github.com/apache/airflow/pull/43622

rawwar (Issue Creator) on (2024-12-05 20:17:57 UTC): @eladkal, I don't think the fix in #43622 is relevant here(Or maybe you were just mentioning about it). Issue happens in both deferrable and non-deferrable mode. Also, I noticed that this issue happens only with one of our customers' AWS accounts. I checked with my personal AWS account with exact permissions, and I can't replicate it. This makes me think there's some edge case in Boto3's paginate, which we need to handle in the provider code.

ferruzzi on (2025-01-04 01:18:28 UTC): I can't seem to reproduce this on my laptop either, but would adding a call to get_job_state [here](https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/amazon/aws/hooks/glue.py#L269) do the trick?  Something along the lines of:

```
if len(fetched_logs):
    # Add a tab to indent those logs and distinguish them from airflow logs.
    # Log lines returned already contain a newline character at the end.
    messages = ""\t"".join(fetched_logs)
    self.log.info(""Glue Job Run %s Logs:\n\t%s"", log_group, messages)

elif self.get_job_state(job_name, run_id) in [""FAILED"", ""TIMEOUT"", ""SUCCEEDED"", ""STOPPED""]:  
   # no new logs and the job has terminated
    return

else:
  # no new logs but job isn't finished, print a ""waiting..."" message
  self.log.info(""No new log from the Glue Job in %s"", log_group)
```

rawwar (Issue Creator) on (2025-01-04 01:31:59 UTC): @ferruzzi , I'll give this a try and give you an update.

rawwar (Issue Creator) on (2025-01-22 10:58:46 UTC): [logs.log](https://github.com/user-attachments/files/18555784/logs.log)
@ferruzzi, the Job state check did not help. It's the same issue. It keeps fetching new tokens repeatedly. I'll share the logs in some time.

ferruzzi on (2025-02-07 18:00:03 UTC): That's a lot of empty paginator replies.   Perhaps add a log message to check that the job state check you added is being hit and enforced?  Maybe that would give some insight here?

"
2719975807,issue,closed,completed,Updating Airflow Helm with ArgoCD,"### Official Helm Chart version

1.15.0 (latest released)

### Apache Airflow version

2.7.1

### Kubernetes Version

v1.25.12+26bab08

### Helm Chart configuration

values.yaml:
``` 
-Home: &-home /opt/- -:
  conn_-: '{{ .Values.data.metadataConnection.protocol }}://{{ .Values.data.metadataConnection.user
    }}:{{ .Values.data.metadataConnection.pass }}@{{ .Values.data.metadataConnection.host 
    }}:{{ .Values.data.metadataConnection.port }}/-_db'
  conn_xcom: '{{ .Values.data.metadataConnection.protocol }}://{{ .Values.data.metadataConnection.user
    }}:{{ .Values.data.metadataConnection.pass }}@{{ .Values.data.metadataConnection.host
    }}:{{ .Values.data.metadataConnection.port }}/{{ .Values.data.metadataConnection.db
    }}'
  
developSourcesVolume:
  enabled: false

jobs:
    enabled: &jobs_enabled true

create-DBJob:
  enabled: true
    ttlSecondsAfterFinished: ~
    command: ~
    args: [""bash"", ""-c"", ""exec ./scripts/-.sh""]
    annotations: {}
    jobAnnotations: {}
  securityContext: {}
    securityContexts:
    pod: {}
    container: {}
  containerLifecycleHooks: {}
  serviceAccount:
    automountServiceAccountToken: true
    create: true
    name: ~
        annotations: {}
  resources: {}
    extraContainers: []
  extraVolumeMounts:
    ---
        defaultMode: 0777
  nodeSelector:
    role: apps
  affinity: {}
  tolerations: []
  topologySpreadConstraints: []
      useHelmHooks: false
  applyCustomEnv: false

createImporterPoolsJob:
  enabled: true
    ttlSecondsAfterFinished: ~
    command: ~

    args:
    - bash
    - -c
    - exec airflow pools import /opt/-/config/importer_pools.json

    annotations: {}
    jobAnnotations: {}
    securityContext: {}
        securityContexts:
    pod: {}
    container: {}
    containerLifecycleHooks: {}
    serviceAccount:
            automountServiceAccountToken: true
        create: true
            name: ~
        annotations: {}
  resources: {}
    extraContainers: []
  extraVolumes: []
  extraVolumeMounts: []
  nodeSelector:
    role: apps
  affinity: {}
  tolerations: []
  topologySpreadConstraints: []
      useHelmHooks: false
  applyCustomEnv: false

createContentPoolsJob:
  enabled: true
    ttlSecondsAfterFinished: ~
    command: ~

    args:
    - bash
    - -c
    - exec airflow pools import /opt/-/config/content_importer_pools.json

    annotations: {}
    jobAnnotations: {}
    securityContext: {}
        securityContexts:
    pod: {}
    container: {}
    containerLifecycleHooks: {}
    serviceAccount:
            automountServiceAccountToken: true
        create: true
            name: ~
        annotations: {}
  resources: {}
    extraContainers: []
  extraVolumes: []
  extraVolumeMounts: []
  nodeSelector:
    role: apps
  affinity: {}
  tolerations: []
  topologySpreadConstraints: []
      useHelmHooks: false
  applyCustomEnv: false

fullnameOverride: """"

nameOverride: """"

useStandardNaming: true

revisionHistoryLimit: ~

uid: 10000
gid: 0

securityContext: {}
securityContexts:
  pod: {}
  containers: {}

containerLifecycleHooks: {}

airflowHome: /opt/airflow

defaultAirflowRepository: <repo>

defaultAirflowTag: ""1.0""

defaultAirflowDigest: ~

airflowVersion: ""2.8.3""

images:
  airflow:
    repository: <repo>
    tag: ~
        digest: ~
    pullPolicy: IfNotPresent
  useDefaultImageForMigration: false
    migrationsWaitTimeout: 60
  pod_template:
    repository: <repo>
    tag: ""1.0""
    pullPolicy: IfNotPresent
  flower:
    repository: ~
    tag: ~
    pullPolicy: IfNotPresent
  statsd:
    repository: quay.io/prometheus/statsd-exporter
    tag: v0.22.8
    pullPolicy: IfNotPresent
  redis:
    repository: dockerhub.hi.inet/library/redis
    tag: 7-bullseye
    pullPolicy: IfNotPresent
  pgbouncer:
    repository: dockerhub.hi.inet/apache/airflow
    tag: airflow-pgbouncer-2023.02.24-1.16.1
    pullPolicy: IfNotPresent
  pgbouncerExporter:
    repository: dockerhub.hi.inet/apache/airflow
    tag: airflow-pgbouncer-exporter-2023.02.21-0.14.0
    pullPolicy: IfNotPresent
  gitSync:
    repository: registry.k8s.io/git-sync/git-sync
    tag: v3.6.9
    pullPolicy: IfNotPresent

nodeSelector: {}
affinity: {}
tolerations: []
topologySpreadConstraints: []
schedulerName: ~

labels: {}
ingress:
    enabled: ~
  web:
        enabled: true

        annotations: {}

        path: ""/""

        pathType: ""ImplementationSpecific""

        host: """"

        hosts: [""host""]

        ingressClassName: """"

        tls:
            enabled: false
            secretName: """"

        precedingPaths: []

        succeedingPaths: []

    flower:
        enabled: true

        annotations: {}

        path: ""/""

        pathType: ""ImplementationSpecific""

        host: """"

        hosts: [""host""]
        ingressClassName: """"

        tls:
            enabled: false
            secretName: """"

networkPolicies:
    enabled: false

airflowPodAnnotations: {}

airflowConfigAnnotations: {}

airflowLocalSettings: |-
  {{- if semverCompare "">=2.2.0"" .Values.airflowVersion }}
  {{- if not (or .Values.webserverSecretKey .Values.webserverSecretKeySecretName) }}
  from airflow.www.utils import UIAlert

  DASHBOARD_UIALERTS = [
    UIAlert(
      'Usage of a dynamic webserver secret key detected. We recommend a static webserver secret key instead.'
      ' See the <a href='
      '""https://airflow.apache.org/docs/helm-chart/stable/production-guide.html      'Helm Chart Production Guide</a> for more details.',
      category=""warning"",
      roles=[""Admin""],
      html=True,
    )
  ]
  {{- end }}
  {{- end }}

rbac:
    create: true
  createSCCRoleBinding: true

executor: ""CeleryExecutor""

allowPodLaunching: false
  

env: 
  - name: ""AIRFLOW__API__AUTH_BACKEND""
    value: ""airflow.api.auth.backend.basic_auth""
  - name: ""AIRFLOW__LOGGING__LOGGING_LEVEL""
    value: ""DEBUG""
  - name: ""AIRFLOW__LOGGING__LOGGING_CONFIG_CLASS""
    value: ""utils.local_logging.log_config.NEW_LOGGING_CONFIG""
  - name:  ""ENV_VAR_LOCAL_LOGGING_LEVEL""
    value: ""DEBUG""
  - name: ""ENV_VAR_TASK_LOG_LEVEL""
    value: ""WARNING""
  - name : ""ENV_VAR_COMPONENT_NAME""
    value: ""top.enablers.-.airflow""
  - name : ""AIRFLOW__LOGGING__CELERY_LOGGING_LEVEL""
    value: ""ERROR""
  - name : ""AIRFLOW__CORE__PARALLELISM""
    value: ""0""
  - name : ""AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG""
    value: ""32""
  - name : ""AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG""
    value: ""60""


volumes: []

volumeMounts: []

secret: []
enableBuiltInSecretEnvVars:
  AIRFLOW__CORE__FERNET_KEY: false
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: true
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: true
  AIRFLOW_CONN_AIRFLOW_DB: true
  AIRFLOW__WEBSERVER__SECRET_KEY: false
  AIRFLOW__CELERY__CELERY_RESULT_BACKEND: true
  AIRFLOW__CELERY__RESULT_BACKEND: true
  AIRFLOW__CELERY__BROKER_URL: true
  AIRFLOW__ELASTICSEARCH__HOST: true
  AIRFLOW__ELASTICSEARCH__ELASTICSEARCH_HOST: true

priorityClasses: []
extraSecrets: {}
extraConfigMaps: {}
extraEnv: ~
extraEnvFrom: ~
data:
  metadataSecretName: ~
  resultBackendSecretName: ~
  brokerUrlSecretName: ~

    metadataConnection:
    <data>
    resultBackendConnection: ~
  brokerUrl: ~

fernetKey: airflow-fernet-secret
fernetKeySecretName: ~

webserverSecretKey: airflow-webserver-secret
webserverSecretKeySecretName: ~
kerberos:
  enabled: false
  ccacheMountPath: /var/kerberos-ccache
  ccacheFileName: cache
  configPath: /etc/krb5.conf
  keytabBase64Content: ~
  keytabPath: /etc/airflow.keytab
  principal: airflow@FOO.COM
  reinitFrequency: 3600
  config: |
                            [logging]
    default = ""FILE:{{ template ""airflow_logs_no_quote"" . }}/kerberos_libs.log""
    kdc = ""FILE:{{ template ""airflow_logs_no_quote"" . }}/kerberos_kdc.log""
    admin_server = ""FILE:{{ template ""airflow_logs_no_quote"" . }}/kadmind.log""

    [libdefaults]
    default_realm = FOO.COM
    ticket_lifetime = 10h
    renew_lifetime = 7d
    forwardable = true

    [realms]
    FOO.COM = {
      kdc = kdc-server.foo.com
      admin_server = admin_server.foo.com
    }

workers:
    replicas: 4
    revisionHistoryLimit: ~

    command: ~
    args:
    - ""bash""
    - ""-c""
        - |-
      exec \
      airflow {{ semverCompare "">=2.0.0"" .Values.airflowVersion | ternary ""celery worker"" ""worker"" }}

      livenessProbe:
    enabled: false
    initialDelaySeconds: 10
    timeoutSeconds: 20
    failureThreshold: 5
    periodSeconds: 60
    command: ~

    updateStrategy: ~
    strategy:
    rollingUpdate:
      maxSurge: ""100%""
      maxUnavailable: ""50%""

    securityContext: {}
          securityContexts:
    pod: {}
    container: {}

    containerLifecycleHooks: {}

    serviceAccount:
            automountServiceAccountToken: true
        create: true
            name: ~

        annotations: {}

    keda:
    enabled: false
    namespaceLabels: {}

        pollingInterval: 5

            cooldownPeriod: 30

        minReplicaCount: 0

        maxReplicaCount: 10

        advanced: {}
    query: >-
      SELECT ceil(COUNT(*)::decimal / {{ .Values.config.celery.worker_concurrency }})
      FROM task_instance
      WHERE (state='running' OR state='queued')
      {{- if eq .Values.executor ""CeleryKubernetesExecutor"" }}
      AND queue != '{{ .Values.config.celery_kubernetes_executor.kubernetes_queue }}'
      {{- end }}

            usePgbouncer: false

  persistence:
        enabled: true
        size: 100Gi
        storageClassName:
                fixPermissions: false
        annotations: {}
        securityContexts:
      container: {}
        containerLifecycleHooks: {}

  kerberosSidecar:
        enabled: false
    resources: {}
    securityContexts:
      container: {}
        containerLifecycleHooks: {}

  resources:
    limits:
     cpu: 1500m
     memory: 16Gi
    requests:
     cpu: 500m
     memory: 4Gi

    terminationGracePeriodSeconds: 600

      safeToEvict: true
  extraContainers: []
    extraInitContainers: []

  extraVolumeMounts:
  ---
  extraVolumes:
  ---

    nodeSelector:
    role: apps
  runtimeClassName: ~
  priorityClassName: ~
  affinity: {}
  tolerations: []
  topologySpreadConstraints: []
        hostAliases: []

    annotations: {}

  podAnnotations: {}

    labels: {}

  logGroomerSidecar:
        enabled: true
        command: ~
        args: [""bash"", ""/clean-logs""]
        retentionDays: 15
    resources: {}
    securityContexts:
      container: {}

  waitForMigrations:
        enabled: true
    env: []
        securityContexts:
      container: {}

  env: 
    - name: ""AIRFLOW__CELERY__WORKER_CONCURRENCY""
      value: ""16""

scheduler:
    hostAliases: []
  livenessProbe:
    initialDelaySeconds: 10
    timeoutSeconds: 20
    failureThreshold: 5
    periodSeconds: 60
    command: ~

      startupProbe:
    failureThreshold: 6
    periodSeconds: 10
    timeoutSeconds: 20
    command: ~
      replicas: 1
    revisionHistoryLimit: ~

    command: ~
    args: [""bash"", ""-c"", ""exec airflow scheduler""]
  updateStrategy: ~
  strategy: ~

      securityContext: {}
          securityContexts:
    pod: {}
    container: {}

    containerLifecycleHooks: {}

    serviceAccount:
            automountServiceAccountToken: true
        create: true
            name: ~

        annotations: {}

    podDisruptionBudget:
    enabled: false

        config:
            maxUnavailable: 1
        resources:
    limits:
     cpu: 600m
     memory: 8Gi
    requests:
     cpu: 300m
     memory: 4Gi

      safeToEvict: true

    extraContainers: []
    extraInitContainers: []
  extraVolumeMounts:
    ---
  extraVolumes:
    ---

    nodeSelector:
    role: apps
  affinity: {}
  tolerations: []
  topologySpreadConstraints: []

  priorityClassName: ~

    annotations: {}

  podAnnotations: {}

    labels: {}

  logGroomerSidecar:
        enabled: true
        command: ~
        args: [""bash"", ""/clean-logs""]
        retentionDays: 15
    resources: {}
                                securityContexts:
      container: {}
        containerLifecycleHooks: {}

  waitForMigrations:
        enabled: *jobs_enabled
    env: []
        securityContexts:
      container: {}

  env: []

createUserJob:
    ttlSecondsAfterFinished: ~
    command: ~
    args:
    - ""bash""
    - ""-c""
        - |-
      exec \
      airflow {{ semverCompare "">=2.0.0"" .Values.airflowVersion | ternary ""users create"" ""create_user"" }} ""$@""
    - --
    - ""-r""
    - ""{{ .Values.webserver.defaultUser.role }}""
    - ""-u""
    - ""{{ .Values.webserver.defaultUser.username }}""
    - ""-e""
    - ""{{ .Values.webserver.defaultUser.email }}""
    - ""-f""
    - ""{{ .Values.webserver.defaultUser.firstName }}""
    - ""-l""
    - ""{{ .Values.webserver.defaultUser.lastName }}""
    - ""-p""
    - ""{{ .Values.webserver.defaultUser.password }}""

    annotations: {}
    jobAnnotations: {}

    labels: {}

    securityContext: {}
          securityContexts:
    pod: {}
    container: {}

    containerLifecycleHooks: {}

    serviceAccount:
            automountServiceAccountToken: true
        create: true
            name: ~

        annotations: {}

    extraContainers: []

  extraVolumes: []
  extraVolumeMounts: []

  nodeSelector:
    role: apps
  affinity: {}
  tolerations: []
  topologySpreadConstraints: []
      useHelmHooks: false
  applyCustomEnv: false

  env: []

  resources:
    limits:
     cpu: 300m
     memory: 8Gi
    requests:
     cpu: 100m
     memory: 4Gi

migrateDatabaseJob:
  enabled: *jobs_enabled
    ttlSecondsAfterFinished: ~
    command: ~
    args:
    - ""bash""
    - ""-c""
    - >-
      exec \

      airflow {{ semverCompare "">=2.7.0"" .Values.airflowVersion
      | ternary ""db migrate"" (semverCompare "">=2.0.0"" .Values.airflowVersion
      | ternary ""db upgrade"" ""upgradedb"") }}

    annotations: {}
    jobAnnotations:
        ""argocd.argoproj.io/hook"": Sync

    securityContext: {}
          securityContexts:
    pod: {}
    container: {}

    containerLifecycleHooks: {}

    serviceAccount:
            automountServiceAccountToken: true
        create: true
            name: ~

        annotations: {}

  resources:
    limits:
     cpu: 300m
     memory: 8Gi
    requests:
     cpu: 100m
     memory: 4Gi

    extraContainers: []

  extraVolumeMounts:
    ---
  extraVolumes:
    ---

  nodeSelector:
    role: apps
  affinity: {}
  tolerations: []
  topologySpreadConstraints: []
      useHelmHooks: false
  applyCustomEnv: false

webserver:
    configMapAnnotations: {}
    hostAliases: []
              allowPodLogReading: true
  livenessProbe:
    initialDelaySeconds: 15
    timeoutSeconds: 5
    failureThreshold: 5
    periodSeconds: 10
    scheme: HTTP

  readinessProbe:
    initialDelaySeconds: 15
    timeoutSeconds: 5
    failureThreshold: 5
    periodSeconds: 10
    scheme: HTTP

  startupProbe:
    timeoutSeconds: 20
    failureThreshold: 6
    periodSeconds: 10
    scheme: HTTP

    replicas: 1
    revisionHistoryLimit: ~

    command: ~
    args: [""bash"", ""-c"", ""exec airflow webserver""]

    serviceAccount:
            automountServiceAccountToken: true
        create: true
            name: ~

        annotations: {}

    podDisruptionBudget:
    enabled: false

        config:
            maxUnavailable: 1
          strategy: ~

      securityContext: {}
          securityContexts:
    pod: {}
    container: {}

    containerLifecycleHooks: {}

    extraNetworkPolicies: []
  networkPolicy:
    ingress:
            from: []
            ports:
        - port: ""{{ .Values.ports.airflowUI }}""

  resources:
    limits:
     cpu: 1000m
     memory: 8Gi
    requests:
     cpu: 300m
     memory: 4Gi

    defaultUser:
    enabled: *jobs_enabled
    role: Admin
    username: admin
    email: admin@example.com
    firstName: admin
    lastName: user
    password: admin

    extraContainers: []
    extraInitContainers: []-
  extraVolumes: []
  extraVolumeMounts: []
  webserverConfig: ~
              webserverConfigConfigMapName: ~

  service:
    type: ClusterIP
        annotations: {}
    ports:
      - name: airflow-ui
        port: ""{{ .Values.ports.airflowUI }}""
    loadBalancerIP: ~
                loadBalancerSourceRanges: []

    nodeSelector:
    role: apps
  priorityClassName: ~
  affinity: {}
  tolerations: []
  topologySpreadConstraints: []

    annotations: {}

  podAnnotations: {}

    labels: {}

  waitForMigrations:
        enabled: *jobs_enabled
    env: []
        securityContexts:
      container: {}

  env: []

triggerer:
  enabled: true
    replicas: 1
    revisionHistoryLimit: ~

    command: ~
    args: [""bash"", ""-c"", ""exec airflow triggerer""]

    updateStrategy: ~
    strategy:
    rollingUpdate:
      maxSurge: ""100%""
      maxUnavailable: ""50%""

      livenessProbe:
    initialDelaySeconds: 10
    timeoutSeconds: 20
    failureThreshold: 5
    periodSeconds: 60
    command: ~

    serviceAccount:
            automountServiceAccountToken: true
        create: true
            name: ~

        annotations: {}

    securityContext: {}
          securityContexts:
    pod: {}
    container: {}

    containerLifecycleHooks: {}

  persistence:
        enabled: true
        size: 100Gi
        storageClassName:
                fixPermissions: false
        annotations: {}

  resources:
    limits:
     cpu: 300m
     memory: 8Gi
    requests:
     cpu: 100m
     memory: 4Gi

    terminationGracePeriodSeconds: 60

      safeToEvict: true

    extraContainers: []
    extraInitContainers: []
  extraVolumes: []
  extraVolumeMounts: []

    nodeSelector:
    role: apps
  affinity: {}
  tolerations: []
  topologySpreadConstraints: []

  priorityClassName: ~

    annotations: {}

  podAnnotations: {}

    labels: {}

  logGroomerSidecar:
        enabled: true
        command: ~
        args: [""bash"", ""/clean-logs""]
        retentionDays: 15
    resources: {}
                                securityContexts:
      container: {}

        containerLifecycleHooks: {}

  waitForMigrations:
        enabled: *jobs_enabled
    env: []
        securityContexts:
      container: {}

  env: []

    keda:
    enabled: false
    namespaceLabels: {}

        pollingInterval: 5

            cooldownPeriod: 30

        minReplicaCount: 0

        maxReplicaCount: 10

        advanced: {}
    query: >-
      SELECT ceil(COUNT(*)::decimal / {{ .Values.config.triggerer.default_capacity }})
      FROM trigger

dagProcessor:
  enabled: false
    replicas: 1
    revisionHistoryLimit: ~

    command: ~
    args: [""bash"", ""-c"", ""exec airflow dag-processor""]

    strategy:
    rollingUpdate:
      maxSurge: ""100%""
      maxUnavailable: ""50%""

      livenessProbe:
    initialDelaySeconds: 10
    timeoutSeconds: 20
    failureThreshold: 5
    periodSeconds: 60
    command: ~

    serviceAccount:
            automountServiceAccountToken: true
        create: true
            name: ~

        annotations: {}

    securityContext: {}
          securityContexts:
    pod: {}
    container: {}

    containerLifecycleHooks: {}

  resources:
    limits:
     cpu: 300m
     memory: 8Gi
    requests:
     cpu: 100m
     memory: 4Gi

    terminationGracePeriodSeconds: 60

      safeToEvict: true

    extraContainers: []
    extraInitContainers: []

  extraVolumes: []
  extraVolumeMounts: []

    nodeSelector:
    role: apps
  affinity: {}
  tolerations: []
  topologySpreadConstraints: []

  priorityClassName: ~

    annotations: {}

  podAnnotations: {}

  logGroomerSidecar:
        enabled: true
        command: ~
        args: [""bash"", ""/clean-logs""]
        retentionDays: 15
    resources: {}
                          waitForMigrations:
        enabled: true
    env: []

  env: []

flower:
      enabled: true
    revisionHistoryLimit: ~

    command: ~
    args:
    - ""bash""
    - ""-c""
        - |-
      exec \
      airflow {{ semverCompare "">=2.0.0"" .Values.airflowVersion | ternary ""celery flower"" ""flower"" }}

    extraNetworkPolicies: []
  networkPolicy:
    ingress:
            from: []
            ports:
        - port: ""{{ .Values.ports.flowerUI }}""

  resources: {}
                securityContext: {}
          securityContexts:
    pod: {}
    container: {}

    containerLifecycleHooks: {}

    serviceAccount:
            automountServiceAccountToken: true
        create: true
            name: ~

        annotations: {}

    secretName: ~

    username: ~
  password: ~

  service:
    type: ClusterIP
        annotations: {}
    ports:
      - name: flower-ui
        port: ""{{ .Values.ports.flowerUI }}""
                        loadBalancerIP: ~
                loadBalancerSourceRanges: []

    extraContainers: []
  extraVolumes: []
  extraVolumeMounts: []

    nodeSelector:
    role: apps
  affinity: {}
  tolerations: []
  topologySpreadConstraints: []

  priorityClassName: ~

    annotations: {}

  podAnnotations: {}

    labels: {}
  env: []

statsd:
    configMapAnnotations: {}

  enabled: true
    revisionHistoryLimit: ~

    args: [""--statsd.mapping-config=/etc/statsd-exporter/mappings.yml""]

    annotations: {}

    serviceAccount:
            automountServiceAccountToken: true
        create: true
            name: ~

        annotations: {}

  uid: 65534
      securityContext: {}
          securityContexts:
    pod: {}
    container: {}

    containerLifecycleHooks: {}

    extraNetworkPolicies: []
  resources: {}
              service:
    extraAnnotations: {}

    nodeSelector:
    role: apps
  affinity: {}
  tolerations: []
  topologySpreadConstraints: []

  priorityClassName: ~

        extraMappings: []

        overrideMappings: []

  podAnnotations: {}
  env: []

pgbouncer:
    enabled: false
    replicas: 1
    revisionHistoryLimit: ~
    command: [""pgbouncer"", ""-u"", ""nobody"", ""/etc/pgbouncer/pgbouncer.ini""]
    args: ~
  auth_type: md5
  auth_file: /etc/pgbouncer/users.txt

    annotations: {}

  podAnnotations: {}

    serviceAccount:
            automountServiceAccountToken: true
        create: true
            name: ~

        annotations: {}

    extraNetworkPolicies: []

    metadataPoolSize: 10
  resultBackendPoolSize: 5

    maxClientConn: 100

  configSecretName: ~

    podDisruptionBudget:
    enabled: false

        config:
            maxUnavailable: 1
        resources: {}

  service:
    extraAnnotations: {}

    verbose: 2
  logDisconnections: 1
  logConnections: 1

  sslmode: ""prefer""
  ciphers: ""normal""

  ssl:
    ca: ~
    cert: ~
    key: ~

      extraIniMetadata: ~
  extraIniResultBackend: ~
    extraIni: ~

  extraVolumes: []
  extraVolumeMounts: []

    extraContainers: []

    nodeSelector:
    role: apps
  affinity: {}
  tolerations: []
  topologySpreadConstraints: []

  priorityClassName: ~

  uid: 65534

    securityContexts:
    pod: {}
    container: {}

    containerLifecycleHooks:
    preStop:
      exec:
                command: [""/bin/sh"", ""-c"", ""killall -INT pgbouncer && sleep 120""]

  metricsExporterSidecar:
    resources: {}
                            sslmode: ""disable""

        statsSecretName: ~

        statsSecretKey: ~

        securityContexts:
      container: {}

        containerLifecycleHooks: {}

    livenessProbe:
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 1

    readinessProbe:
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 1

    env: []

redis:
  enabled: true
  terminationGracePeriodSeconds: 600

    serviceAccount:
            automountServiceAccountToken: true
        create: true
            name: ~

        annotations: {}

  persistence:
        enabled: true
        size: 1Gi
        storageClassName:
        annotations: {}

  resources:
    limits:
     cpu: 300m
     memory: 8Gi
    requests:
     cpu: 100m
     memory: 4Gi

    passwordSecretName: ~

        password: ~

      safeToEvict: true

    nodeSelector:
    role: apps
  affinity: {}
  tolerations: []
  topologySpreadConstraints: []

    uid: 0
    securityContext: {}
        securityContexts:
    pod:
      fsGroup: 0
      runAsGroup: 0
      runAsUser: 999
    container:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL

    containerLifecycleHooks: {}

  podAnnotations: {}
registry:
  secretName: crvpprevrm

            elasticsearch:
    enabled: false
    secretName: ~
                connection: {}

ports:
  flowerUI: 5555
  airflowUI: 8080
  workerLogs: 8793
  triggererLogs: 8794
  redisDB: 6379
  statsdIngest: 9125
  statsdScrape: 9102
  pgbouncer: 6543
  pgbouncerScrape: 9127

quotas: {}

limits: []

cleanup:
  enabled: false
    schedule: ""*/15 * * * *""
  command: ~
    args: [""bash"", ""-c"", ""exec airflow kubernetes cleanup-pods --namespace={{ .Release.Namespace }}""]

    jobAnnotations: {}

    nodeSelector:
    role: apps
  affinity: {}
  tolerations: []
  topologySpreadConstraints: []

  podAnnotations: {}

    labels: {}

  resources: {}
                serviceAccount:
            automountServiceAccountToken: true
        create: true
            name: ~

        annotations: {}

    securityContext: {}
      env: []

    securityContexts:
    pod: {}
    container: {}

    containerLifecycleHooks: {}

      failedJobsHistoryLimit: ~
  successfulJobsHistoryLimit: ~

postgresql:
  enabled: false
  image:
    tag: ""11""
  auth:
    enablePostgresUser: true
    postgresPassword: postgres
    username: """"
    password: """"

config:
  core:
    dags_folder: '/opt/-/dags'
    dags_are_paused_at_creation: 'False'
        load_examples: 'False'
    executor: '{{ .Values.executor }}'
        colored_console_log: 'False'
    remote_logging: '{{- ternary ""True"" ""False"" .Values.elasticsearch.enabled }}'
  logging:
    remote_logging: '{{- ternary ""True"" ""False"" .Values.elasticsearch.enabled }}'
    colored_console_log: 'False'
  metrics:
    statsd_on: '{{ ternary ""True"" ""False"" .Values.statsd.enabled }}'
    statsd_port: 9125
    statsd_prefix: airflow
    statsd_host: '{{ printf ""%s-statsd"" .Release.Name }}'
  webserver:
    enable_proxy_fix: 'True'
        rbac: 'True'
  celery:
    flower_url_prefix: '{{ ternary """" .Values.ingress.flower.path (eq .Values.ingress.flower.path ""/"") }}'
    worker_concurrency: 16
  scheduler:
    standalone_dag_processor: '{{ ternary ""True"" ""False"" .Values.dagProcessor.enabled }}'
        statsd_on: '{{ ternary ""True"" ""False"" .Values.statsd.enabled }}'
    statsd_port: 9125
    statsd_prefix: airflow
    statsd_host: '{{ printf ""%s-statsd"" .Release.Name }}'
        run_duration: 41460
  elasticsearch:
    json_format: 'True'
    log_id_template: ""{dag_id}_{task_id}_{execution_date}_{try_number}""
  elasticsearch_configs:
    max_retries: 3
    timeout: 30
    retry_timeout: 'True'
  kerberos:
    keytab: '{{ .Values.kerberos.keytabPath }}'
    reinit_frequency: '{{ .Values.kerberos.reinitFrequency }}'
    principal: '{{ .Values.kerberos.principal }}'
    ccache: '{{ .Values.kerberos.ccacheMountPath }}/{{ .Values.kerberos.ccacheFileName }}'
  celery_kubernetes_executor:
    kubernetes_queue: 'kubernetes'
      kubernetes:
    namespace: '{{ .Release.Namespace }}'
        airflow_configmap: '{{ include ""airflow_config"" . }}'
    airflow_local_settings_configmap: '{{ include ""airflow_config"" . }}'
    pod_template_file: '{{ include ""airflow_pod_template_file"" . }}/pod_template_file.yaml'
    worker_container_repository: <repo>
        multi_namespace_mode: '{{ ternary ""True"" ""False"" .Values.multiNamespaceMode }}'
    kubernetes_executor:
    namespace: '{{ .Release.Namespace }}'
    pod_template_file: '{{ include ""airflow_pod_template_file"" . }}/pod_template_file.yaml'
        worker_container_repository: <repo>
    multi_namespace_mode: '{{ ternary ""True"" ""False"" .Values.multiNamespaceMode }}'
  triggerer:
    default_capacity: 1000
multiNamespaceMode: false
podTemplate: ~
dags:
  persistence:
        annotations: {}
        enabled: false
        size: 1Gi
        storageClassName:
        accessMode: ReadWriteOnce
        existingClaim:
        subPath: ~
  gitSync:
    enabled: false

                repo: https://github.com/apache/airflow.git
    branch: v2-2-stable
    rev: HEAD
    depth: 1
        maxFailures: 0
            subPath: ""tests/dags""
    wait: 5
    containerName: git-sync
    uid: 65533

        securityContext: {}
            securityContexts:
      container: {}

        containerLifecycleHooks: {}

                        extraVolumeMounts: []
    env: []
                resources: {}
                        logs:
  useDevelopPVC: false
  persistence:
        enabled: true
        size: 100Gi
        annotations: {}
        storageClassName: ""-""
        existingClaim: airflow-logs

```

### Docker Image customizations

_No response_

### What happened

I'm deploying Apache Airflow Helm Chart on our corporate environment with ArgoCD and I would love to get some help on something that I'm not being able to solve. Thanks in advance.

So far it's working pretty good, I've made some customizations to the deployment, pv's, added some jobs, etc. The problem is, when upgrading with ArgoCD, some barriers show up. 

Firstly, fernet-key didn't allow upgrade on objects claiming error ""missing fernet-key"", so I disabled that temporarily for debug purposes. Now, everytime we commit into the branch that ArgoCD is deploying, the deploy breaks, being unable to fully sync the app, claiming errors of ""Inmutable fields"". I think the app is not trying to create a new updated object when we try to update, and instead is trying to modify the already existing objects. The troubleshoting I've found is just manually deleting the app and letting ArgoCD deploy it entirely again, but this way we are loosing ArgoCD automatization point.

I’m sure I’m not the only one deploying this Helm chart with ArgoCD, so I’d like to know if anyone can spot any visible errors or missing configurations. So far, I’ve followed the documentation at https://airflow.apache.org/docs/helm-chart/1.8.0/ to deploy with ArgoCD, but I might be overlooking something.

### What you think should happen instead

ArgoCD should be able to update the deployment configuration and images when we push into the deployed branch. 

### How to reproduce

In an OpenShift cluster with ArgoCD, we deployed the official Helm chart via ArgoCD, using the configurations previously described in the values.yaml.

The deployment is successful, and the application works correctly. ArgoCD is able to deploy all objects and identify the synchronization status of the app.

We then attempt to make a change to the deployment, either by committing to the deployed branch or switching branches. The change could involve modifying a configuration or updating the image version.

Synchronization via ArgoCD fails. The only solution is to delete the deployment and let ArgoCD recreate the app from scratch.

### Anything else

Thanks in advance!

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",juansgr,2024-12-05 10:17:28+00:00,[],2024-12-06 11:01:25+00:00,2024-12-06 11:01:25+00:00,https://github.com/apache/airflow/issues/44688,"[('kind:bug', 'This is a clearly a bug'), ('area:helm-chart', 'Airflow Helm Chart'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2519863875, 'issue_id': 2719975807, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 5, 10, 17, 32, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-05 10:17:32 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2719801304,issue,open,reopened,Visualizing a mapped task from the grid view causes a UI crash,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When I'm inspecting a DAG grid, by going to `/dag/<dag_id>/grid`, and I click on a grid square representing a DAG mapped task, then an XHR call is performed to `/api/v1/dags/<dag_id>/dagRuns/<run_id>/taskInstances/<task_id>`, which returns a 404 with the following error:
```json
{
  ""detail"": ""Task instance is mapped, add the map_index value to the URL"",
  ""status"": 404,
  ""title"": ""Task instance not found"",
  ""type"": ""https://airflow.apache.org/docs/apache-airflow/2.10.3/stable-rest-api-ref.html#section/Errors/NotFound""
}
```

When that happens, the page turns white and empty, with a console error linked to Axios crashing to the 404 response.
![image](https://github.com/user-attachments/assets/66d37cf8-a42b-4dd0-b02c-1d20ae5c94d4)

However, when performing the exact same behavior when located on URL `/dags/<dag_id>/grid?dag_run_id=<run_id>&task_id=<task_id>` (for example by refreshing the page after the crash) then no such XHR call is being sent, and the task panel loads just fine.

<img width=""1495"" alt=""Screenshot 2024-12-05 at 10 00 15"" src=""https://github.com/user-attachments/assets/a3b14d94-8426-4972-ba45-5fb6df33fb9d"">

### What you think should happen instead?

I'm _guessing_ that when located on `/dags/<dag_id>/grid` with no prior query arg, the UI does not ""know"" that the task is itself composed of mapped tasks, and thus is calling the wrong API path to inspect it. 

### How to reproduce

Here's a screen recording of the crash.

https://github.com/user-attachments/assets/bba93885-765b-40ee-a62f-b4704bbddf9e

The webserver logs associated with the crash are
```
x.x.x.x - - [05/Dec/2024:08:58:05 +0000] ""GET /api/v1/dags/refine_to_hive_hourly/dagRuns/scheduled__2024-12-05T04:00:00+00:00/taskInstances/refine_hive_dataset.wait_for_gobblin_export/dependencies HTTP/1.1"" 404 323 ""https://airflow-analytics.wikimedia.org/dags/refine_to_hive_hourly/grid?dag_run_id=scheduled__2024-12-05T04%3A00%3A00%2B00%3A00&task_id=refine_hive_dataset.wait_for_gobblin_export"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:133.0) Gecko/20100101 Firefox/133.0""
x.x.x.x - - [05/Dec/2024:08:58:05 +0000] ""GET /api/v1/dags/refine_to_hive_hourly/tasks/refine_hive_dataset.wait_for_gobblin_export HTTP/1.1"" 200 1117 ""https://airflow-analytics.wikimedia.org/dags/refine_to_hive_hourly/grid?dag_run_id=scheduled__2024-12-05T04%3A00%3A00%2B00%3A00&task_id=refine_hive_dataset.wait_for_gobblin_export"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:133.0) Gecko/20100101 Firefox/133.0""
x.x.x.x - - [05/Dec/2024:08:58:05 +0000] ""GET /api/v1/dags/refine_to_hive_hourly/dagRuns/scheduled__2024-12-05T04:00:00+00:00/taskInstances/refine_hive_dataset.wait_for_gobblin_export HTTP/1.1"" 404 249 ""https://airflow-analytics.wikimedia.org/dags/refine_to_hive_hourly/grid?dag_run_id=scheduled__2024-12-05T04%3A00%3A00%2B00%3A00&task_id=refine_hive_dataset.wait_for_gobblin_export"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:133.0) Gecko/20100101 Firefox/133.0""
x.x.x.x - - [05/Dec/2024:08:58:06 +0000] ""GET /object/grid_data?dag_id=refine_to_hive_hourly&num_runs=25 HTTP/1.1"" 200 105027 ""https://airflow-analytics.wikimedia.org/dags/refine_to_hive_hourly/grid?dag_run_id=scheduled__2024-12-05T04%3A00%3A00%2B00%3A00&task_id=refine_hive_dataset.wait_for_gobblin_export"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:133.0) Gecko/20100101 Firefox/133.0""
x.x.x.x - - [05/Dec/2024:08:58:06 +0000] ""GET /api/v1/dags/refine_to_hive_hourly/dagRuns/scheduled__2024-12-05T04:00:00+00:00/taskInstances/refine_hive_dataset.wait_for_gobblin_export/dependencies HTTP/1.1"" 404 323 ""https://airflow-analytics.wikimedia.org/dags/refine_to_hive_hourly/grid?dag_run_id=scheduled__2024-12-05T04%3A00%3A00%2B00%3A00&task_id=refine_hive_dataset.wait_for_gobblin_export"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:133.0) Gecko/20100101 Firefox/133.0""
x.x.x.x - - [05/Dec/2024:08:58:06 +0000] ""GET /api/v1/dags/refine_to_hive_hourly/dagRuns/scheduled__2024-12-05T04:00:00+00:00/taskInstances/refine_hive_dataset.wait_for_gobblin_export HTTP/1.1"" 404 249 ""https://airflow-analytics.wikimedia.org/dags/refine_to_hive_hourly/grid?dag_run_id=scheduled__2024-12-05T04%3A00%3A00%2B00%3A00&task_id=refine_hive_dataset.wait_for_gobblin_export"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:133.0) Gecko/20100101 Firefox/133.0""
```

### Operating System

Debian Bullseye

### Versions of Apache Airflow Providers

```
apache-airflow-providers-amazon==9.0.0
apache-airflow-providers-apache-hdfs==4.6.0
apache-airflow-providers-apache-hive==8.2.0
apache-airflow-providers-apache-spark==4.8.1
apache-airflow-providers-cncf-kubernetes==9.0.1
apache-airflow-providers-common-compat==1.2.1
apache-airflow-providers-common-io==1.4.2
apache-airflow-providers-common-sql==1.19.0
apache-airflow-providers-fab==1.5.0
apache-airflow-providers-ftp==3.11.1
apache-airflow-providers-http==4.13.2
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-postgres==5.13.1
apache-airflow-providers-smtp==1.8.0
apache-airflow-providers-sqlite==3.9.0
```

### Deployment

Other

### Deployment details

We're seeing this bug with 2 types of deployments:
- airflow components running as systemd services on VMs/physical hosts
- airflow components running as Kubernetes Pods, deployed via a custom chart

### Anything else?

This bug is reproducible every time. There does not seem to be a random element to ot.

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",brouberol,2024-12-05 09:04:59+00:00,['brouberol'],2025-01-31 05:54:02+00:00,,https://github.com/apache/airflow/issues/44685,"[('kind:bug', 'This is a clearly a bug'), ('area:API', ""Airflow's REST/HTTP API""), ('area:helm-chart', 'Airflow Helm Chart'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('area:dynamic-task-mapping', 'AIP-42'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2519684390, 'issue_id': 2719801304, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 5, 9, 5, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2525185773, 'issue_id': 2719801304, 'author': 'shahar1', 'body': ""Welcome to Apache Airflow and thanks for reporting!\r\nI assigned you with this task as you checked the box for submitting a PR - please let us know if you want to be unassigned, so others might be able to resolve the issue.\r\nI've tried to reproduce the issue locally using [breeze](https://github.com/apache/airflow/tree/main/dev/breeze) (Airflow's dev. env.), without much success.\r\nIf you don't have a clear idea how to resolve the issue, it might be helpful for others if you could provide minimal yet exact steps for local reproduction (inc. DAG file, helm chart etc.).\r\nGood luck!"", 'created_at': datetime.datetime(2024, 12, 7, 14, 20, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2527408486, 'issue_id': 2719801304, 'author': 'brouberol', 'body': 'I\'m trying to reproduce the issue locally, in a breeze environment. \r\n\r\nI\'ve defined the following DAG with mapped tasks:\r\n\r\n```python\r\nfrom datetime import datetime\r\nimport time\r\nfrom airflow.decorators import task\r\nfrom airflow.models.dag import DAG\r\n\r\nwith DAG(dag_id=""example_dynamic_task_mapping"", schedule=None, start_date=datetime(2022, 3, 4)) as dag:\r\n\r\n    @task\r\n    def add_one(x: int):\r\n        time.sleep(x)  # to give the DAG time to change status.\r\n        return x + 1\r\n\r\n    @task\r\n    def sum_it(values):\r\n        total = sum(values)\r\n        print(f""Total was {total}"")\r\n\r\n    added_values = add_one.expand(x=[10, 20, 30])\r\n    sum_it(added_values)\r\n```\r\n\r\nI\'m not seemingly able to reproduce with that setup.\r\n\r\nOnce I trigger the DAG, I hard refresh the page with the URL set to `http://127.0.0.1:28080/dags/example_dynamic_task_mapping/grid`. I do observe 3 of the 4 `404` responses, mentioning the lack of a `map_index` URL fragment. However, the UI does not crash, and correctly loads the task details.\r\n\r\n<img width=""1495"" alt=""Screenshot 2024-12-09 at 09 46 14"" src=""https://github.com/user-attachments/assets/0703f460-eefc-4208-942e-5b0407ffb6d5"">\r\n\r\nHowever, as visible on the screenshots, our production DAG is formed of an initial task, followed by a mapped task group.\r\n\r\nSo, to reflect that, I changed the DAG to the following:\r\n```python\r\nfrom datetime import datetime\r\nimport time\r\nfrom airflow.decorators import task, task_group\r\nfrom airflow.models.dag import DAG\r\n\r\n\r\nwith DAG(dag_id=""example_dynamic_task_mapping"", schedule=None, start_date=datetime(2022, 3, 4)) as dag:\r\n\r\n    @task\r\n    def expensive_operation():\r\n        time.sleep(60)\r\n        return [1, 2, 3]\r\n\r\n    @task\r\n    def add_one(x: int):\r\n        time.sleep(x)\r\n        return x + 1\r\n\r\n    @task\r\n    def add_two(x: int):\r\n        return x + 2\r\n\r\n    @task_group\r\n    def complex_computation(value):\r\n        return add_two(add_one(value))\r\n\r\n    complex_computation.expand(value=expensive_operation())\r\n```\r\n\r\nI then go to `http://127.0.0.1:28080/dags/example_dynamic_task_mapping/grid` and hard refresh the page. While the task group is not being executed, I click on the `add_one` task. I\'m not seeing any 404 response then, as the map index is still set to `-1`.\r\n<img width=""1497"" alt=""Screenshot 2024-12-09 at 10 34 22"" src=""https://github.com/user-attachments/assets/3d3950ba-f012-45a1-9a3b-5a798776e5fa"">\r\n\r\nOnce the task group is being executed, I then click on `add_one` again (after having hard refreshed the page to  `http://127.0.0.1:28080/dags/example_dynamic_task_mapping/grid`), and I still can\'t observe any crash.\r\n\r\n<img width=""1493"" alt=""Screenshot 2024-12-09 at 10 38 21"" src=""https://github.com/user-attachments/assets/518726d8-64da-40c8-8663-1ce89c33f1bc"">', 'created_at': datetime.datetime(2024, 12, 9, 9, 38, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2560459412, 'issue_id': 2719801304, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 12, 24, 0, 15, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2581488641, 'issue_id': 2719801304, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2025, 1, 10, 0, 15, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2617240588, 'issue_id': 2719801304, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2025, 1, 28, 0, 15, 19, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-05 09:05:03 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

shahar1 on (2024-12-07 14:20:37 UTC): Welcome to Apache Airflow and thanks for reporting!
I assigned you with this task as you checked the box for submitting a PR - please let us know if you want to be unassigned, so others might be able to resolve the issue.
I've tried to reproduce the issue locally using [breeze](https://github.com/apache/airflow/tree/main/dev/breeze) (Airflow's dev. env.), without much success.
If you don't have a clear idea how to resolve the issue, it might be helpful for others if you could provide minimal yet exact steps for local reproduction (inc. DAG file, helm chart etc.).
Good luck!

brouberol (Issue Creator) on (2024-12-09 09:38:56 UTC): I'm trying to reproduce the issue locally, in a breeze environment. 

I've defined the following DAG with mapped tasks:

```python
from datetime import datetime
import time
from airflow.decorators import task
from airflow.models.dag import DAG

with DAG(dag_id=""example_dynamic_task_mapping"", schedule=None, start_date=datetime(2022, 3, 4)) as dag:

    @task
    def add_one(x: int):
        time.sleep(x)  # to give the DAG time to change status.
        return x + 1

    @task
    def sum_it(values):
        total = sum(values)
        print(f""Total was {total}"")

    added_values = add_one.expand(x=[10, 20, 30])
    sum_it(added_values)
```

I'm not seemingly able to reproduce with that setup.

Once I trigger the DAG, I hard refresh the page with the URL set to `http://127.0.0.1:28080/dags/example_dynamic_task_mapping/grid`. I do observe 3 of the 4 `404` responses, mentioning the lack of a `map_index` URL fragment. However, the UI does not crash, and correctly loads the task details.

<img width=""1495"" alt=""Screenshot 2024-12-09 at 09 46 14"" src=""https://github.com/user-attachments/assets/0703f460-eefc-4208-942e-5b0407ffb6d5"">

However, as visible on the screenshots, our production DAG is formed of an initial task, followed by a mapped task group.

So, to reflect that, I changed the DAG to the following:
```python
from datetime import datetime
import time
from airflow.decorators import task, task_group
from airflow.models.dag import DAG


with DAG(dag_id=""example_dynamic_task_mapping"", schedule=None, start_date=datetime(2022, 3, 4)) as dag:

    @task
    def expensive_operation():
        time.sleep(60)
        return [1, 2, 3]

    @task
    def add_one(x: int):
        time.sleep(x)
        return x + 1

    @task
    def add_two(x: int):
        return x + 2

    @task_group
    def complex_computation(value):
        return add_two(add_one(value))

    complex_computation.expand(value=expensive_operation())
```

I then go to `http://127.0.0.1:28080/dags/example_dynamic_task_mapping/grid` and hard refresh the page. While the task group is not being executed, I click on the `add_one` task. I'm not seeing any 404 response then, as the map index is still set to `-1`.
<img width=""1497"" alt=""Screenshot 2024-12-09 at 10 34 22"" src=""https://github.com/user-attachments/assets/3d3950ba-f012-45a1-9a3b-5a798776e5fa"">

Once the task group is being executed, I then click on `add_one` again (after having hard refreshed the page to  `http://127.0.0.1:28080/dags/example_dynamic_task_mapping/grid`), and I still can't observe any crash.

<img width=""1493"" alt=""Screenshot 2024-12-09 at 10 38 21"" src=""https://github.com/user-attachments/assets/518726d8-64da-40c8-8663-1ce89c33f1bc"">

github-actions[bot] on (2024-12-24 00:15:31 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2025-01-10 00:15:44 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2025-01-28 00:15:19 UTC): This issue has been closed because it has not received response from the issue author.

"
2719772800,issue,closed,completed,Unable to view trigger log in real-time in Airflow UI when task is deferred,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.6.3

### What happened?

when task is deferred, the webserver UI is unable to read trigger log:
`*** Could not read served logs: Client error '404 NOT FOUND' for url 'http://<triggerer hostname>:8794/log/dag_id=xxx/run_id=xxx/task_id=xxx/attempt=18.log.trigger.3509.log'
`

after task is completed, the above error message will disappear, it says the trigger log is found on s3 (we enable remote logging):

`*** Found logs in s3:
***   * s3://<bucket>/path_to_log/attempt=18.log
***   * s3://<bucket>/path_to_log/attempt=18.log.trigger.3509.log`

and the content of the trigger log is displayed with the task log in the UI
`[2024-12-05, 07:34:00 UTC] {triggerer_job_runner.py:616} INFO - Trigger xxx/-1/18 (ID 160) fired: TriggerEvent<{'state': 'SUCCESS', 'message': None, 'extra_data': {...}}>
`

When task is deferred I exec into the triggerer pod, I can't find the expected `xxx/attempt=18.log.trigger.3509.log` file in log folder; while after the task completes, I can find this log file in the folder.

### What you think should happen instead?

while task is deferred, I expect to view the corresponding trigger log in real-time in the UI, like the video [here](https://github.com/apache/airflow/pull/27758)

### How to reproduce

run a DAG with deferrable task, and view the task log when it is deferred

### Operating System

Ubuntu

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.1.0
apache-airflow-providers-celery==3.2.0
apache-airflow-providers-cncf-kubernetes==7.0.0
apache-airflow-providers-common-sql==1.5.1
apache-airflow-providers-docker==3.7.0

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

triggerer pod has open 8794 port, and has a service listening on that port
`triggerer      ClusterIP   None             <none>        8794/TCP            22d
`
and webserver secret key has configured properly.

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",lanxinxs,2024-12-05 08:51:51+00:00,[],2024-12-06 10:55:39+00:00,2024-12-06 10:55:34+00:00,https://github.com/apache/airflow/issues/44683,"[('kind:bug', 'This is a clearly a bug'), ('duplicate', 'Issue that is duplicated'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('area:Triggerer', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2519654096, 'issue_id': 2719772800, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 5, 8, 51, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2522844988, 'issue_id': 2719772800, 'author': 'potiuk', 'body': 'Likely duplicate of https://github.com/apache/airflow/pull/41272 -> please upgrade to latest 2.10 (2.10.3 now) to check if it is fixed there. Closing as duplicate - if it appears still after upgrade, we can always re-open it.', 'created_at': datetime.datetime(2024, 12, 6, 10, 55, 34, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-05 08:51:55 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-12-06 10:55:34 UTC): Likely duplicate of https://github.com/apache/airflow/pull/41272 -> please upgrade to latest 2.10 (2.10.3 now) to check if it is fixed there. Closing as duplicate - if it appears still after upgrade, we can always re-open it.

"
2718902829,issue,open,,AIP-38 | List Dag Runs across all dags,,bbovenzi,2024-12-04 22:20:39+00:00,[],2024-12-05 14:45:49+00:00,,https://github.com/apache/airflow/issues/44674,"[('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2519472898, 'issue_id': 2718902829, 'author': 'tirkarthi', 'body': 'Does this come under `browse` page similar to current `Events` page?', 'created_at': datetime.datetime(2024, 12, 5, 7, 39, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2520517138, 'issue_id': 2718902829, 'author': 'bbovenzi', 'body': 'Actually the idea is to make it a tabbed view under the Dags page. I\'m still playing with it. Which Is why I didn\'t finish the description yet.\r\n\r\n<img width=""523"" alt=""Screenshot 2024-12-05 at 9 43 33\u202fAM"" src=""https://github.com/user-attachments/assets/f7a312ac-9379-4a5d-81b8-e32895f077a9"">', 'created_at': datetime.datetime(2024, 12, 5, 14, 44, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2520520778, 'issue_id': 2718902829, 'author': 'tirkarthi', 'body': 'Thanks, looking forward to the designs.', 'created_at': datetime.datetime(2024, 12, 5, 14, 45, 47, tzinfo=datetime.timezone.utc)}]","tirkarthi on (2024-12-05 07:39:47 UTC): Does this come under `browse` page similar to current `Events` page?

bbovenzi (Issue Creator) on (2024-12-05 14:44:20 UTC): Actually the idea is to make it a tabbed view under the Dags page. I'm still playing with it. Which Is why I didn't finish the description yet.

<img width=""523"" alt=""Screenshot 2024-12-05 at 9 43 33 AM"" src=""https://github.com/user-attachments/assets/f7a312ac-9379-4a5d-81b8-e32895f077a9"">

tirkarthi on (2024-12-05 14:45:47 UTC): Thanks, looking forward to the designs.

"
2718887226,issue,closed,completed,AIP-38 | Dashboard - Quick Links to filter Dags List,"Next to the Dag Import Errors button, add other buttons to link to the Dags List but with certain filters enabled.

- Last dag run failed
- Has a running dag run (let's use the regular running color vs what the design has)
- Dag is unpaused

![Image](https://github.com/user-attachments/assets/7a145c6e-caae-4a0c-a5a3-8b911d81d99f)


Let's skip Stalled Dags since there is no such concept yet. The counts in each button can be in a later PR because that should be calculated by a custom `ui` API endpoint.
",bbovenzi,2024-12-04 22:10:17+00:00,[],2024-12-05 14:41:11+00:00,2024-12-05 14:41:11+00:00,https://github.com/apache/airflow/issues/44673,"[('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]",[],
2718882716,issue,open,,AIP-38 | Add Gantt View,,bbovenzi,2024-12-04 22:07:08+00:00,['bbovenzi'],2024-12-04 22:09:21+00:00,,https://github.com/apache/airflow/issues/44672,"[('area:UI', 'Related to UI/UX. For Frontend Developers.')]",[],
2718881137,issue,closed,completed,AIP-38 | Add Grid view,,bbovenzi,2024-12-04 22:05:57+00:00,['bbovenzi'],2025-01-27 14:07:43+00:00,2025-01-27 14:07:43+00:00,https://github.com/apache/airflow/issues/44671,"[('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2519470654, 'issue_id': 2718881137, 'author': 'tirkarthi', 'body': 'Dependent API PR : https://github.com/apache/airflow/pull/44332', 'created_at': datetime.datetime(2024, 12, 5, 7, 38, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2615463918, 'issue_id': 2718881137, 'author': 'phanikumv', 'body': 'PR which added the Grid view https://github.com/apache/airflow/pull/45497', 'created_at': datetime.datetime(2025, 1, 27, 11, 10, 9, tzinfo=datetime.timezone.utc)}]","tirkarthi on (2024-12-05 07:38:38 UTC): Dependent API PR : https://github.com/apache/airflow/pull/44332

phanikumv on (2025-01-27 11:10:09 UTC): PR which added the Grid view https://github.com/apache/airflow/pull/45497

"
2718871926,issue,closed,completed,AIP-84 Filter task instances list by task id and display name,"Add two new filters to list task instances at `/public/dags/{dag_id}/dagRuns/{dag_run_id}/taskInstances`:


`task_id` to only return Tis with that task_id
`task_display_name_pattern` to allow a user to pass a string to search for a TI by `task_display_name` or `task_id`

",bbovenzi,2024-12-04 21:59:23+00:00,['jason810496'],2024-12-10 21:27:35+00:00,2024-12-10 21:27:35+00:00,https://github.com/apache/airflow/issues/44670,"[('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2518962219, 'issue_id': 2718871926, 'author': 'jason810496', 'body': 'Hi @bbovenzi I can work on this issue, could you assign to me ? Thanks !', 'created_at': datetime.datetime(2024, 12, 5, 2, 38, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2518969514, 'issue_id': 2718871926, 'author': 'bbovenzi', 'body': '@jason810496 go for it!', 'created_at': datetime.datetime(2024, 12, 5, 2, 45, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2519217557, 'issue_id': 2718871926, 'author': 'tirkarthi', 'body': 'There is list task instances batch to pass task_id, dag_id and other parameters as POST call.\r\n\r\nhttps://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html#operation/get_task_instances_batch', 'created_at': datetime.datetime(2024, 12, 5, 5, 30, 18, tzinfo=datetime.timezone.utc)}]","jason810496 (Assginee) on (2024-12-05 02:38:18 UTC): Hi @bbovenzi I can work on this issue, could you assign to me ? Thanks !

bbovenzi (Issue Creator) on (2024-12-05 02:45:47 UTC): @jason810496 go for it!

tirkarthi on (2024-12-05 05:30:18 UTC): There is list task instances batch to pass task_id, dag_id and other parameters as POST call.

https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html#operation/get_task_instances_batch

"
2718866078,issue,closed,completed,AIP-38 | Task Details Page,"Similar to the Dag, Dag Run and Task Instance Details pages. We need a Task Details pages that lists details of a task and can summarize its Task Instances.

- Have the [Task Card](https://github.com/apache/airflow/pull/44604) in the Dag Details page link to this new Task Detail page
- Create a Task Instances list that uses the public rest API but filtered by Task_id. [Backend issue]([44670](https://github.com/apache/airflow/issues/44670))
- Add an overview page with the same TrendCountButton as the Dag Details overview page to filter the list of task instances.

",bbovenzi,2024-12-04 21:55:41+00:00,[],2024-12-11 18:05:35+00:00,2024-12-11 18:05:35+00:00,https://github.com/apache/airflow/issues/44669,"[('AIP-38', 'Modern Web Application')]","[{'comment_id': 2519466432, 'issue_id': 2718866078, 'author': 'tirkarthi', 'body': 'Tasks can also have docs as `doc_md` attribute which can be added similar to `Dag Docs` button to show the docs as modal in task details page.', 'created_at': datetime.datetime(2024, 12, 5, 7, 36, 18, tzinfo=datetime.timezone.utc)}]","tirkarthi on (2024-12-05 07:36:18 UTC): Tasks can also have docs as `doc_md` attribute which can be added similar to `Dag Docs` button to show the docs as modal in task details page.

"
2718844455,issue,closed,completed,AIP-38 | Add Task Instance Details,"After https://github.com/apache/airflow/pull/44656 is merged. We should fill in the Task Instance ""advanced"" Details tab. This should list all task instance details which we don't already display in the page header.

We can show these details in a simple key/value table.

TI Details can change from try to try so it should also use a Try number selector like #44663.",bbovenzi,2024-12-04 21:41:45+00:00,['dauinh'],2025-01-06 11:21:11+00:00,2025-01-06 11:20:09+00:00,https://github.com/apache/airflow/issues/44668,"[('AIP-38', 'Modern Web Application')]","[{'comment_id': 2518627577, 'issue_id': 2718844455, 'author': 'bbovenzi', 'body': 'Example from the old UI: ![Image](https://github.com/user-attachments/assets/5389dce0-1479-4c64-aa15-cc652837e0dc)', 'created_at': datetime.datetime(2024, 12, 4, 21, 47, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2524256808, 'issue_id': 2718844455, 'author': 'dauinh', 'body': 'hi @bbovenzi! Can I try this issue?', 'created_at': datetime.datetime(2024, 12, 6, 22, 4, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2528485355, 'issue_id': 2718844455, 'author': 'bbovenzi', 'body': 'Yeah, once https://github.com/apache/airflow/pull/44656 is merged you should be good to go.', 'created_at': datetime.datetime(2024, 12, 9, 15, 55, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2537882727, 'issue_id': 2718844455, 'author': 'dauinh', 'body': 'hi @bbovenzi, I\'m having trouble implementing components like `Status` or `Time` into the data table. The code below is my current implementation. I have also tried modifying columns definition, but stuck at how to implement different components for one column. Please let me know if you have any suggestion or insight. Thank you!\r\n\r\nCode:\r\n```\r\nconst formattedData = [\r\n    {key: ""Status"", value: task?.state && <Status state={task.state}>{task.state}</Status>},\r\n    ...\r\n]\r\n```\r\nExpected:\r\n<img width=""356"" alt=""Screenshot 2024-12-11 at 8 40 29\u202fPM"" src=""https://github.com/user-attachments/assets/5ad3d552-28ab-4315-add9-1b4022b62028"" />\r\nCurrent:\r\n<img width=""398"" alt=""Screenshot 2024-12-11 at 9 56 18\u202fPM"" src=""https://github.com/user-attachments/assets/040cc032-7b0c-4894-a457-54aeee96d2cf"" />\r\n\r\n\r\nAlso, how do I remove header title without causing error?', 'created_at': datetime.datetime(2024, 12, 12, 5, 57, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2541836354, 'issue_id': 2718844455, 'author': 'bbovenzi', 'body': ""@dauinh We don't need the full DataTable component, just a simple Table is fine. Check out the [chakra docs](https://www.chakra-ui.com/docs/components/table) for examples. I believe we can simply not define `Table.Header` and just write each detail row as jsx instead of creating a `formattedData` array."", 'created_at': datetime.datetime(2024, 12, 13, 16, 40, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2543394975, 'issue_id': 2718844455, 'author': 'dauinh', 'body': 'got it, thank you!', 'created_at': datetime.datetime(2024, 12, 15, 0, 49, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2572906528, 'issue_id': 2718844455, 'author': 'pierrejeambrun', 'body': 'Done in #45273', 'created_at': datetime.datetime(2025, 1, 6, 11, 21, 10, tzinfo=datetime.timezone.utc)}]","bbovenzi (Issue Creator) on (2024-12-04 21:47:20 UTC): Example from the old UI: ![Image](https://github.com/user-attachments/assets/5389dce0-1479-4c64-aa15-cc652837e0dc)

dauinh (Assginee) on (2024-12-06 22:04:33 UTC): hi @bbovenzi! Can I try this issue?

bbovenzi (Issue Creator) on (2024-12-09 15:55:41 UTC): Yeah, once https://github.com/apache/airflow/pull/44656 is merged you should be good to go.

dauinh (Assginee) on (2024-12-12 05:57:42 UTC): hi @bbovenzi, I'm having trouble implementing components like `Status` or `Time` into the data table. The code below is my current implementation. I have also tried modifying columns definition, but stuck at how to implement different components for one column. Please let me know if you have any suggestion or insight. Thank you!

Code:
```
const formattedData = [
    {key: ""Status"", value: task?.state && <Status state={task.state}>{task.state}</Status>},
    ...
]
```
Expected:
<img width=""356"" alt=""Screenshot 2024-12-11 at 8 40 29 PM"" src=""https://github.com/user-attachments/assets/5ad3d552-28ab-4315-add9-1b4022b62028"" />
Current:
<img width=""398"" alt=""Screenshot 2024-12-11 at 9 56 18 PM"" src=""https://github.com/user-attachments/assets/040cc032-7b0c-4894-a457-54aeee96d2cf"" />


Also, how do I remove header title without causing error?

bbovenzi (Issue Creator) on (2024-12-13 16:40:59 UTC): @dauinh We don't need the full DataTable component, just a simple Table is fine. Check out the [chakra docs](https://www.chakra-ui.com/docs/components/table) for examples. I believe we can simply not define `Table.Header` and just write each detail row as jsx instead of creating a `formattedData` array.

dauinh (Assginee) on (2024-12-15 00:49:31 UTC): got it, thank you!

pierrejeambrun on (2025-01-06 11:21:10 UTC): Done in #45273

"
2718840965,issue,closed,completed,AIP-38 | Add Task Instance XCom,Once https://github.com/apache/airflow/pull/44656 we will have a tab on the task instance details page to show XComs. We should use the public API to render a table of all xcoms for that task instance.,bbovenzi,2024-12-04 21:39:21+00:00,['tirkarthi'],2025-01-06 11:28:12+00:00,2025-01-06 11:27:54+00:00,https://github.com/apache/airflow/issues/44667,"[('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]","[{'comment_id': 2538260773, 'issue_id': 2718840965, 'author': 'tirkarthi', 'body': '@bbovenzi I can take this up if you are okay. The API returns only XCom keys and there needs to be an API call per XCom entry to get the value as string. There could also be a clipboard button to copy the contents of the xcom entry.', 'created_at': datetime.datetime(2024, 12, 12, 9, 0, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2538413006, 'issue_id': 2718840965, 'author': 'tirkarthi', 'body': 'Created https://github.com/apache/airflow/pull/44869 as a first attempt.', 'created_at': datetime.datetime(2024, 12, 12, 9, 56, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2572917468, 'issue_id': 2718840965, 'author': 'pierrejeambrun', 'body': 'Closing, done in https://github.com/apache/airflow/pull/44869.', 'created_at': datetime.datetime(2025, 1, 6, 11, 27, 54, tzinfo=datetime.timezone.utc)}]","tirkarthi (Assginee) on (2024-12-12 09:00:26 UTC): @bbovenzi I can take this up if you are okay. The API returns only XCom keys and there needs to be an API call per XCom entry to get the value as string. There could also be a clipboard button to copy the contents of the xcom entry.

tirkarthi (Assginee) on (2024-12-12 09:56:24 UTC): Created https://github.com/apache/airflow/pull/44869 as a first attempt.

pierrejeambrun on (2025-01-06 11:27:54 UTC): Closing, done in https://github.com/apache/airflow/pull/44869.

"
2718826791,issue,closed,completed,AIP-38 | Add Task Instance Logs,"After https://github.com/apache/airflow/pull/44656 is merged we will have a placeholder page for task logs. We should reimplement the task logs page here.

- check if the TI try_number > 1. Then use the public rest api to fetch all previous tries and render a TryNumber selector. The selector should default to the latest try
- use the public rest api endpoint to fetch logs by task instance try
- get `default_wrap`  from `useConfig` to have the wrap checkbox checked or not

We can leave file source and log level alone for now since that should be filters done by the backend. Also there will be no ""see more"" button.

Optional to include colored log lines in this PR or in a subsequent one. https://github.com/apache/airflow/issues/43541

Legacy UI view
![Image](https://github.com/user-attachments/assets/6a171eb5-c5c8-4196-8a97-e7a8c82e5d7e)
",bbovenzi,2024-12-04 21:31:09+00:00,['bbovenzi'],2025-01-07 18:02:55+00:00,2025-01-07 18:02:55+00:00,https://github.com/apache/airflow/issues/44663,"[('AIP-38', 'Modern Web Application')]","[{'comment_id': 2519455632, 'issue_id': 2718826791, 'author': 'tirkarthi', 'body': 'It will be good to have a full screen view of the logs at least without the filters like graph modal which was one of the requested features as legacy logs UI was relatively full screen.', 'created_at': datetime.datetime(2024, 12, 5, 7, 30, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2520512667, 'issue_id': 2718826791, 'author': 'bbovenzi', 'body': 'Perhaps an expand icon button which will open the same logs but in a full page modal?', 'created_at': datetime.datetime(2024, 12, 5, 14, 42, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2555982264, 'issue_id': 2718826791, 'author': 'set92', 'body': 'I was searching for the exact Issue, but I couldn\'t find it. I was testing today AF 2.10.4, and I found that when you have more than 10 fails in a task, the buttons are replaced by a dropdown list?\r\n\r\n<img width=""487"" alt=""image"" src=""https://github.com/user-attachments/assets/48b292e3-ef5d-4e82-9364-dc7bbbc9ee91"" />\r\n\r\nIf you get to that number of tries is most probably because you are debugging, so you don\'t want more steps to go to your last log error. So, from this point of view I see 2 improvements\r\n- Make the order in reverse. Probably will be weird, but if you are with an error, you want to select the last error log as soon as possible, and when you have to select the dropdown list, and then go to select the last element is not very nice or comfortable. Other option would be to put a limit or make the list smaller to only shows 5 elements, and have a scrollbar.\r\n- The other option would be to have both, the dropdown, and the last or the last 3 log error. Which will appear on the right side of the dropdown. This way it is easier to see error logs, that\'s most people are interested in. But, if you want to check previous ones, you still have the dropdown.', 'created_at': datetime.datetime(2024, 12, 20, 0, 2, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2557233787, 'issue_id': 2718826791, 'author': 'bbovenzi', 'body': '@set92 good call. Updated for 3.0: https://github.com/apache/airflow/pull/45117', 'created_at': datetime.datetime(2024, 12, 20, 15, 35, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558097249, 'issue_id': 2718826791, 'author': 'set92', 'body': '@bbovenzi Great, thanks! Although I feel the other option would have been better in functionality. Although more complex in terms of UI, and therefore harder to maintain.\r\n\r\nIn case it was not clear what I meant, I did a quick draw. We would have the dropdown with all the task tries. But at a side, also the latest tries (they can be the last 3, 5 or customizable by config up to a number), which will allow to quickly select them, without having to open the dropdown.\r\n<img width=""464"" alt=""image"" src=""https://github.com/user-attachments/assets/3e852ed0-9c28-4a6b-8077-1eb40d50d71e"" />', 'created_at': datetime.datetime(2024, 12, 21, 12, 5, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2572916879, 'issue_id': 2718826791, 'author': 'pierrejeambrun', 'body': 'I believe we are only missing the colors on this one before closing ?', 'created_at': datetime.datetime(2025, 1, 6, 11, 27, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2575920445, 'issue_id': 2718826791, 'author': 'bbovenzi', 'body': 'I think we can close and make other issues for colors, groups, and filters', 'created_at': datetime.datetime(2025, 1, 7, 18, 2, 55, tzinfo=datetime.timezone.utc)}]","tirkarthi on (2024-12-05 07:30:40 UTC): It will be good to have a full screen view of the logs at least without the filters like graph modal which was one of the requested features as legacy logs UI was relatively full screen.

bbovenzi (Issue Creator) on (2024-12-05 14:42:35 UTC): Perhaps an expand icon button which will open the same logs but in a full page modal?

set92 on (2024-12-20 00:02:07 UTC): I was searching for the exact Issue, but I couldn't find it. I was testing today AF 2.10.4, and I found that when you have more than 10 fails in a task, the buttons are replaced by a dropdown list?

<img width=""487"" alt=""image"" src=""https://github.com/user-attachments/assets/48b292e3-ef5d-4e82-9364-dc7bbbc9ee91"" />

If you get to that number of tries is most probably because you are debugging, so you don't want more steps to go to your last log error. So, from this point of view I see 2 improvements
- Make the order in reverse. Probably will be weird, but if you are with an error, you want to select the last error log as soon as possible, and when you have to select the dropdown list, and then go to select the last element is not very nice or comfortable. Other option would be to put a limit or make the list smaller to only shows 5 elements, and have a scrollbar.
- The other option would be to have both, the dropdown, and the last or the last 3 log error. Which will appear on the right side of the dropdown. This way it is easier to see error logs, that's most people are interested in. But, if you want to check previous ones, you still have the dropdown.

bbovenzi (Issue Creator) on (2024-12-20 15:35:17 UTC): @set92 good call. Updated for 3.0: https://github.com/apache/airflow/pull/45117

set92 on (2024-12-21 12:05:07 UTC): @bbovenzi Great, thanks! Although I feel the other option would have been better in functionality. Although more complex in terms of UI, and therefore harder to maintain.

In case it was not clear what I meant, I did a quick draw. We would have the dropdown with all the task tries. But at a side, also the latest tries (they can be the last 3, 5 or customizable by config up to a number), which will allow to quickly select them, without having to open the dropdown.
<img width=""464"" alt=""image"" src=""https://github.com/user-attachments/assets/3e852ed0-9c28-4a6b-8077-1eb40d50d71e"" />

pierrejeambrun on (2025-01-06 11:27:34 UTC): I believe we are only missing the colors on this one before closing ?

bbovenzi (Issue Creator) on (2025-01-07 18:02:55 UTC): I think we can close and make other issues for colors, groups, and filters

"
2718228169,issue,open,,Standalone DAG Processor Causes DAGs to Appear and Disappear Frequently in Production,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

**Description:**

I have Airflow 2.10.3 deployed in AKS using the Helm chart, and everything works fine. I tried to deploy the standalone DAG processor to run as a standalone process. Here is my configuration:

```yaml
dagProcessor:
  enabled: true
  replicas: 2
  revisionHistoryLimit: 5
  resources:
    requests:
        cpu: 2500m
        ephemeral-storage: 200Mi
        memory: 2500Mi
    limits:
        ephemeral-storage: 200Mi
        memory: 2500Mi
  podAnnotations: *podAnnotations
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: ""airflow.workload""
            operator: In
            values:
            - dagprocessor
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchLabels:
              component: dagprocessor
          topologyKey: kubernetes.io/hostname
        weight: 100
  tolerations:
    - key: ""airflow.workload""
      value: ""dagprocessor""
      operator: ""Equal""
      effect: ""NoSchedule""

[core]
  standalone_dag_processor: ""True""
```

I managed to separate the DAG processor pods and the worker node pool. However, I started encountering issues where DAGs appear and disappear frequently when the DAG bag size is large. In my QA environment with only 500 DAGs, I don't have this issue, but in production with more than 2000 DAGs, this happens frequently.

In the cluster activity, I see the DAG processor state turning red (unhealthy) for a few seconds and then healthy again, in a non-ending cycle. The error in the logs is:

```
sqlalchemy.exc.PendingRollbackError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint ""serialized_dag_pkey""
DETAIL:  Key (dag_id)=(demo-dag) already exists.
(Background on this error at: https://sqlalche.me/e/14/gkpj) (Background on this error at: https://sqlalche.me/e/14/7s2a)
```

Any help or guidance on resolving this issue would be greatly appreciated.

### What you think should happen instead?

The DAG processor should handle a large number of DAGs without causing them to appear and disappear frequently. The state of the DAG processor should remain stable and not fluctuate between healthy and unhealthy.

### How to reproduce


- Run the DAG processor as a subprocess of a scheduler job.
- Migrate the DAG processor to run as a standalone process deployment.
- Deploy in an environment with a large number of DAGs (e.g., more than 2000 DAGs).

### Operating System

AKS 1.29, AzureLinux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

And lots of error lines:

```
<unknown>:298 SyntaxWarning: invalid escape sequence xxx
INFO - Heartbeat recovered after 319.20 seconds
```
### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",seifrajhi,2024-12-04 16:17:03+00:00,[],2024-12-10 08:24:46+00:00,,https://github.com/apache/airflow/issues/44652,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:DAG-processing', ''), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2519518871, 'issue_id': 2718228169, 'author': 'seifrajhi', 'body': 'What makes things weirder, is that it shows yesterday\'s date for the last heartbeat\r\n\r\n<img width=""371"" alt=""image"" src=""https://github.com/user-attachments/assets/1008ad7e-f7f1-48a1-9c33-61d7c91ee8c9"">\r\n\r\nFew seconds later goes back to healthy with current date', 'created_at': datetime.datetime(2024, 12, 5, 8, 4, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2523329898, 'issue_id': 2718228169, 'author': 'seifrajhi', 'body': 'After reverting back to dagProcessor subprocess it works fine with more than 5000 DAGs\r\n\r\n<img width=""1698"" alt=""image"" src=""https://github.com/user-attachments/assets/df6c59d5-e3fc-4039-b47b-089ed8a841ad"">', 'created_at': datetime.datetime(2024, 12, 6, 14, 8, 3, tzinfo=datetime.timezone.utc)}]","seifrajhi (Issue Creator) on (2024-12-05 08:04:45 UTC): What makes things weirder, is that it shows yesterday's date for the last heartbeat

<img width=""371"" alt=""image"" src=""https://github.com/user-attachments/assets/1008ad7e-f7f1-48a1-9c33-61d7c91ee8c9"">

Few seconds later goes back to healthy with current date

seifrajhi (Issue Creator) on (2024-12-06 14:08:03 UTC): After reverting back to dagProcessor subprocess it works fine with more than 5000 DAGs

<img width=""1698"" alt=""image"" src=""https://github.com/user-attachments/assets/df6c59d5-e3fc-4039-b47b-089ed8a841ad"">

"
2717612285,issue,closed,completed,AttributeError: '_thread._local' object has no attribute 'callers',"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Airflow error message
[2024-12-04, 15:49:18 IST] {taskinstance.py:3311} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/operator_helpers.py"", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py"", line 408, in wrapper
    cls._sentinel.callers[sentinel_key] = sentinel
    ^^^^^^^^^^^^^^^^^^^^^
AttributeError: '_thread._local' object has no attribute 'callers'


### What you think should happen instead?

_No response_

### How to reproduce

Create airflow KubernetesPodOperator in airflow version 2.10.3

### Operating System

Airflow official helm chart

### Versions of Apache Airflow Providers

NA

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

NA

### Anything else?

NA

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rahulgoyal2987,2024-12-04 12:35:35+00:00,[],2025-01-28 07:40:36+00:00,2025-01-28 07:40:35+00:00,https://github.com/apache/airflow/issues/44648,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2517234891, 'issue_id': 2717612285, 'author': 'rahulgoyal2987', 'body': 'Please find PR related to defect fix\r\nhttps://github.com/apache/airflow/pull/44646', 'created_at': datetime.datetime(2024, 12, 4, 12, 38, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2618128903, 'issue_id': 2717612285, 'author': 'eladkal', 'body': 'fixed in https://github.com/apache/airflow/pull/44646', 'created_at': datetime.datetime(2025, 1, 28, 7, 40, 35, tzinfo=datetime.timezone.utc)}]","rahulgoyal2987 (Issue Creator) on (2024-12-04 12:38:05 UTC): Please find PR related to defect fix
https://github.com/apache/airflow/pull/44646

eladkal on (2025-01-28 07:40:35 UTC): fixed in https://github.com/apache/airflow/pull/44646

"
2717379956,issue,open,,Some statsd metrics are not prefixed correctly,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When configuring a statsd prefix (with AIRFLOW__METRICS__STATSD_PREFIX for example), some metrics are not prefixed correctly and use the prefix ""airflow"" instead.

Here is a list of such metrics in my case: 

* `airflow.triggers.running.{triggerer pod name}`
* `airflow.triggers.running`
* `airflow.triggerer_heartbeat`
* `airflow.serde.load_serializers`

Those seem to be linked to PR #33320.

### What you think should happen instead?

All metrics should implement the correct configured prefix.

### How to reproduce

* Configure your Airflow instance to export statsd metrics and set a AIRFLOW__METRICS__STATSD_PREFIX=mytest
* Enable a statsd-exporter sidecar
* Run a dummy DAG to generate metrics
* See that most metrics are correctly prefixed with ""mytest"" but some are prefixed with ""airflow""

### Operating System

Containerized (Kubernetes)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",NBardelot,2024-12-04 11:10:41+00:00,[],2024-12-04 11:13:00+00:00,,https://github.com/apache/airflow/issues/44643,"[('kind:bug', 'This is a clearly a bug'), ('area:metrics', ''), ('area:core', ''), ('area:Triggerer', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2716258814,issue,closed,not_planned,SQLToGoogleSheetOperator unintuitive handling of nulls,"### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

apache-airflow-providers-google==10.22.0

### Apache Airflow version

2.10.1

### Operating System

Linux

### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### What happened

When using the `SQLToGoogleSheets` operator data that is null is incorrectly updated in the Google Sheet; primarily if pre-existing data is in place. 

### What you think should happen instead

The Range of cells updated by the function should be cleared before uploading the new values, so that null values are updated correctly. For example the following Operator resolves the issue.

```python
class SqlToGoogleSheetWithClear(SQLToGoogleSheetsOperator):
    def execute(self, context: Any) -> None:
        sheet_hook = GSheetsHook(
            gcp_conn_id=self.gcp_conn_id,
            delegate_to=self.delegate_to,
            impersonation_chain=self.impersonation_chain,
        )
        sheet_hook.clear(
            spreadsheet_id=self.spreadsheet_id,
            range_=f""{self.spreadsheet_range}!A1:ZZ"", #More complex logic needed if not the raw sheet
        )

        super().execute(context)
```

### How to reproduce

To recreate: Populate a google sheet's First row so all columns have the value `1`. In the operator add the query 
```sql
select 2::int as id, null as name, 'William' as last_name
```

The google sheet update process sees the 'null' name column as empty and doesn't update the existing cell populated with the value `1` resulting in:
```
id, name, last_name
2, 1, William
```

### Anything else

I'm willing to submit a PR but unsure the best approach to handle backwards compatibility. An additional parameter could be included to truncate before load, but I do believe this feels like a bug to me. If my SQL returns Null I expect the cell's value to be empty on a successful run of the operator. 

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",FridayPush,2024-12-04 00:09:20+00:00,[],2025-02-07 00:15:19+00:00,2025-02-07 00:15:18+00:00,https://github.com/apache/airflow/issues/44628,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('provider:google', 'Google (including GCP) related issues'), ('area:providers', ''), ('pending-response', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2515844918, 'issue_id': 2716258814, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 4, 0, 9, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2544037861, 'issue_id': 2716258814, 'author': 'eladkal', 'body': ""> The google sheet update process sees the 'null' name column as empty\r\n\r\nUnless I am missing something I am not convinced this is a bug in the operator.\r\nThis feels like the data in the Sheet is not so clean to begin with. I would suggest to subclass the operator and implement `pre_execute()` to clean the data.\r\n\r\nOr did you mean that NULL is always read as 'null' from Google Sheet?\r\n\r\nWDYT?"", 'created_at': datetime.datetime(2024, 12, 15, 20, 12, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2617240662, 'issue_id': 2716258814, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2025, 1, 28, 0, 15, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641466103, 'issue_id': 2716258814, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2025, 2, 7, 0, 15, 17, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-04 00:09:22 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2024-12-15 20:12:04 UTC): Unless I am missing something I am not convinced this is a bug in the operator.
This feels like the data in the Sheet is not so clean to begin with. I would suggest to subclass the operator and implement `pre_execute()` to clean the data.

Or did you mean that NULL is always read as 'null' from Google Sheet?

WDYT?

github-actions[bot] on (2025-01-28 00:15:23 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2025-02-07 00:15:17 UTC): This issue has been closed because it has not received response from the issue author.

"
2715753871,issue,open,,Log groomer sidecars do propagate secrets configurations causing failures,"### Official Helm Chart version

1.15.0 (latest released)

### Apache Airflow version

2.10.3

### Kubernetes Version

1.30

### Helm Chart configuration

```
config:
  secrets:
    backend: ""custom_providers.vault_secret.VaultSecret""
    backend_kwargs: '{""connections_path"": ""airflow/connections"", ""mount_point"": null, ""url"": ""vault.vault.svc.cluster.local"", ""auth_type"": ""aws_iam""}'
env:
  - name: ""AIRFLOW__SECRETS__BACKEND""
     value: ""custom_providers.vault_secret.VaultSecret""
  - name: ""AIRFLOW__SECRETS__BACKEND_KWARGS""
    value: ""{\""connections_path\"": \""airflow/connections\"", \""mount_point\"": null, \""url\"": \""vault.vault.svc.cluster.local\"", \""auth_type\"": \""aws_iam\""}""
```

### Docker Image customizations

_No response_

### What happened

All log groomer sidecars (both on triggerer and scheduler) fail w/ the error:

> ERROR! Maximum number of retries (20) reached.
> Last check result:
> $ airflow db check
> Traceback (most recent call last):
>   File ""/home/airflow/.local/bin/airflow"", line 5, in <module>
>     from airflow.__main__ import main
>   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/__init__.py"", line 53, in <module>
>     from airflow import configuration, settings
>   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py"", line 2371, in <module>
>     secrets_backend_list = initialize_secrets_backends()
>                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
>   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py"", line 2279, in initialize_secrets_backends
>     custom_secret_backend = get_custom_secret_backend()
>                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
>   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py"", line 2267, in get_custom_secret_backend
>     return secrets_backend_cls(**backend_kwargs)
>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
>   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/hashicorp/secrets/vault.py"", line 141, in __init__
>     self.vault_client = _VaultClient(
>                         ^^^^^^^^^^^^^
>   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/hashicorp/_internal_client/vault_client.py"", line 134, in __init__
>     raise VaultError(""The 'token' authentication type requires 'token' or 'token_path'"")
> hvac.exceptions.VaultError: The 'token' authentication type requires 'token' or 'token_path', on None None

When I ssh to the container and echo for the env variables in question the following outputs occur:

> airflow@ci-airflow-triggerer-0:/opt/airflow$ echo $AIRFLOW__SECRETS__BACKEND
> custom_providers.vault_secret.VaultSecret
> airflow@ci-airflow-triggerer-0:/opt/airflow$ echo $AIRFLOW__SECRETS__BACKEND_KWARGS
> 
> airflow@ci-airflow-triggerer-0:/opt/airflow$

I can also see that the env section is properly filled out for other sidecars / init containers in the live manifest but not for groomers - running echos from the main containers have the expected results.

### What you think should happen instead

The groomers should populate configuration properly and be able to function when using a Vault secret backend.

### How to reproduce

Attempt to use groomers while supplying a Vault secret backend

### Anything else

Initially we were supplying these arguments only via env variables - at which point the Jobs created by the chart (create-user and run-airflow-migration) were also failing with the same logs / the env variables are not populated properly for these either.

Changing to hard coding the Values.config section fixed this issue. We attempted only env variables, only Values.config, and both, and none populate properly.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",JDKnobloch,2024-12-03 18:41:35+00:00,[],2024-12-03 22:44:12+00:00,,https://github.com/apache/airflow/issues/44621,"[('kind:bug', 'This is a clearly a bug'), ('area:secrets', ''), ('area:helm-chart', 'Airflow Helm Chart'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2515322524, 'issue_id': 2715753871, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 3, 18, 41, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2515703340, 'issue_id': 2715753871, 'author': 'gbazad93', 'body': 'It seems the log groomer sidecars aren\'t picking up the necessary environment variables for the Vault secret backend, particularly AIRFLOW__SECRETS__BACKEND_KWARGS. Since sidecars run in separate containers, they might not inherit the env vars you\'ve set elsewhere.\r\n\r\nI believe you can fix this by adding the required environment variables directly to the log groomer sidecar configuration in your Helm chart. Try adding the following to your values.yaml:\r\n\r\n```bash\r\nlogGroomerSidecar:\r\n  enabled: true\r\n  extraEnv:\r\n    - name: AIRFLOW__SECRETS__BACKEND\r\n      value: custom_providers.vault_secret.VaultSecret\r\n    - name: AIRFLOW__SECRETS__BACKEND_KWARGS\r\n      value: \'{""connections_path"": ""airflow/connections"", ""mount_point"": null, ""url"": ""vault.vault.svc.cluster.local"", ""auth_type"": ""aws_iam""}\'\r\n```', 'created_at': datetime.datetime(2024, 12, 3, 22, 44, 10, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-03 18:41:38 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

gbazad93 on (2024-12-03 22:44:10 UTC): It seems the log groomer sidecars aren't picking up the necessary environment variables for the Vault secret backend, particularly AIRFLOW__SECRETS__BACKEND_KWARGS. Since sidecars run in separate containers, they might not inherit the env vars you've set elsewhere.

I believe you can fix this by adding the required environment variables directly to the log groomer sidecar configuration in your Helm chart. Try adding the following to your values.yaml:

```bash
logGroomerSidecar:
  enabled: true
  extraEnv:
    - name: AIRFLOW__SECRETS__BACKEND
      value: custom_providers.vault_secret.VaultSecret
    - name: AIRFLOW__SECRETS__BACKEND_KWARGS
      value: '{""connections_path"": ""airflow/connections"", ""mount_point"": null, ""url"": ""vault.vault.svc.cluster.local"", ""auth_type"": ""aws_iam""}'
```

"
2715717102,issue,open,,Unsafe dag_id and task_id serialization in statsd metrics,"### Apache Airflow version

2.10.3

### What happened?

Currently, the ""safe_dag_id"" function is not used when Airflow sends statsd metrics. The task_id is not safe either.

Thus, if you have a DAG with dag_id=`my.dag` and an Operator with task_id=`my.task` (which is OK per `KEY_REGEX = re.compile(r""^[\w.-]+$"")` in `utils/helper.py` used both for DAG and BaseOperator), some metrics that involve both fields will look like:

```
some.prefix.ti.start.my.dag.my.task
```

And the statsd mapping cannot be correctly done, as it will map ""my.dag.my"" as the dag_id (the capture group being greedy), and ""task"" as the remaining task_id.

### What you think should happen instead?

At least the dag_id should be safe, to that dots are replaced with `__dot__` which can be captured in a dot-separated string that contains both a dag_id and task_id with dots.

```
some.prefix.ti.start.my__dot__dag.my.task
```

Can corretly be mapped using `([\w-]+)` (important, no dot) as a capture group for the dag_id, and `([\w.-]+)` as a capture group for the task_id.

But, in an ideal situation, both dag_id and task_id should have their respective dots replaced with `__dot__`.

### How to reproduce

* Create a DAG with a dag_id=`my.dag` and a DummyOperator with task_id=`my.task` and export metrics using a statsd-exporter.
* Add an extraMappings in the values.yaml to configure a mapping for dag_id and task_id as labels.
* In the statsd-exporter, query the metric after mapping.

### Operating System

Containerized (Kubernetes)

### Deployment

Official Apache Airflow Helm Chart

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",NBardelot,2024-12-03 18:22:11+00:00,[],2024-12-03 18:28:01+00:00,,https://github.com/apache/airflow/issues/44620,"[('kind:bug', 'This is a clearly a bug'), ('area:metrics', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2715483775,issue,closed,completed,PowerBIDatasetRefreshOperator task fails whereas dataset refresh succeeds,"### Apache Airflow Provider(s)

microsoft-azure

### Versions of Apache Airflow Providers

apache-airflow-providers-microsoft-azure==11.1.0

### Apache Airflow version

2.10.2

### Operating System

linux

### Deployment

Google Cloud Composer

### Deployment details

_No response_

### What happened

We use the operator `PowerBIDatasetRefreshOperator` to refresh our PowerBI datasets. Sometimes, the task quickly fails with the following error:
```
airflow.exceptions.AirflowException: An error occurred: Unable to fetch the details of dataset refresh with Request Id: <request id>
```
<img width=""751"" alt=""image"" src=""https://github.com/user-attachments/assets/ddcff823-c2c4-456f-861e-f7a54eabd991"">


However, on PowerBI side, the dataset is refreshed (timezone GMT+1, both the time in the refresh and the log are about the same):
<img width=""171"" alt=""image"" src=""https://github.com/user-attachments/assets/47f263b0-cf20-4b4e-803a-aee25a248d25"">


If I understand correctly, this corresponds to an error in [this function](https://github.com/apache/airflow/blob/d059d4a84b526e5f975d17c6846f5c8e29adbc3a/providers/src/airflow/providers/microsoft/azure/hooks/powerbi.py#L148), which may mean that there was an error when trying to fetch the refresh status, even though the refresh was running. Maybe the error comes from the request to the Microsoft API, which could be improved with a retry (but we did not test for the moment).

### What you think should happen instead

The task should succeed. (since the dataset refresh suceeded)

### How to reproduce

On our side, we use the following configuration.

```python
refresh_powerbi_dataset_success = PowerBIDatasetRefreshOperator(
        task_id=""refresh_powerbi_dataset_success"",
        conn_id=""our-conn-id"",
        dataset_id=""our-dataset-id"",
        group_id=""our-group-id"",
    )
```

The dataset ID and group ID correspond to valid PowerBI dataset and workspace.

### Anything else

This bug occurs about once every 3 task runs (the task fails but the refresh succeeds). Sometimes if fails several times in a row then works again, other times it works on the first run. I think this should mean that the configuration is good (since the operator often works fine).
I haven't been able to identify any specific pattern.

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Ohashiro,2024-12-03 16:31:58+00:00,[],2025-01-26 17:42:44+00:00,2025-01-26 17:42:44+00:00,https://github.com/apache/airflow/issues/44618,"[('kind:bug', 'This is a clearly a bug'), ('provider:microsoft-azure', 'Azure-related issues'), ('area:providers', '')]","[{'comment_id': 2551445833, 'issue_id': 2715483775, 'author': 'dabla', 'body': 'Can you just specify a retry with following parameters on the PowerBIDatasetRefreshOperator?\r\n\r\n```\r\nretry_exponential_backoff=True,  # to increase the delay after each failed attempt\r\nretry_delay=60,\r\nretries=5,\r\n```', 'created_at': datetime.datetime(2024, 12, 18, 14, 20, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2553142811, 'issue_id': 2715483775, 'author': 'Ohashiro', 'body': 'Thank you for the suggestion! (sorry I didn\'t know these fields were available)\r\nI just tried it and still get some task fails (with the same error). I will try tweaking the parameters a little to see if it fixes the issue.\r\nDo you know to what (which request) do these ""retries"" apply?', 'created_at': datetime.datetime(2024, 12, 19, 9, 9, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2554006776, 'issue_id': 2715483775, 'author': 'dabla', 'body': 'Those parameters apply on the whole operator, you could also try to specify the check_interval  and timeout parameter, which by default both are 60 seconds, I think timeout should be greater then then check_interval, otherwise it will stop polling due to timeout.', 'created_at': datetime.datetime(2024, 12, 19, 13, 33, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2554040707, 'issue_id': 2715483775, 'author': 'Ohashiro', 'body': 'OK I see, so the retry is not specific to the ""get status"" request. \r\nFrom what I understand, the issue seems to be that the very first request (to get the refresh status) fails (maybe because the refresh id is not yet available for this endpoint) and the task exits. \r\nIf I understand correctly, the retry on the operator allows us to re-run the whole process (so re-trigger a refresh), instead of retrying the request to get the refresh status already running.', 'created_at': datetime.datetime(2024, 12, 19, 13, 39, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563538301, 'issue_id': 2715483775, 'author': 'dabla', 'body': '> OK I see, so the retry is not specific to the ""get status"" request. From what I understand, the issue seems to be that the very first request (to get the refresh status) fails (maybe because the refresh id is not yet available for this endpoint) and the task exits. If I understand correctly, the retry on the operator allows us to re-run the whole process (so re-trigger a refresh), instead of retrying the request to get the refresh status already running.\r\n\r\nIndeed, you\'re correct about it. I still don\'t understand your issue completely, as from what I see in the code of the \r\nPowerBIDatasetRefreshOperator, it first triggers the refresh datasets and then polls and wait for the refresh to complete (which by default has a timeout of 60 seconds).  So indeed it could happen that the refresh isn\'t ready or started yet, but still I would expect it to retry within the timeout interval, unless of course the timeout has expired, but that\'s something you can parametrize.', 'created_at': datetime.datetime(2024, 12, 27, 10, 8, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563544190, 'issue_id': 2715483775, 'author': 'dabla', 'body': ""I understand the issue, I've checked the code of the PowerBITrigger and the check_interval is set to 60 seconds, but the timeout is also 60 seconds by default, which means the status is only fetched once and then it fails like in your example log.  So you should reduce the check_interval value to like 10 seconds for example, or even a bit less.  So try setting the check_interval to 10 seconds.  Maybe we should do a PR and add a check that the check_interval must be smaller than the timeout and set it to 10 seconds by default."", 'created_at': datetime.datetime(2024, 12, 27, 10, 14, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567785221, 'issue_id': 2715483775, 'author': 'Ohashiro', 'body': 'Hello!\r\nThank you very much for taking the time to look at the code and making this suggestion.\r\nI tried to reduce the `check_interval` to 5, unfortunately it did not seem to fix the issue on our side.\r\n\r\nAlso, I thought that the default timeout was 1 week because of this line: https://github.com/apache/airflow/blob/608de6b97ef04307559d79b2044899adfb9b2a93/providers/src/airflow/providers/microsoft/azure/triggers/powerbi.py#L57\r\n\r\nBut maybe I got this wrong. Am I mistaken?\r\n\r\nRegarding the bug, I think the issue may come from the very first request to get the refresh status, at this line: https://github.com/apache/airflow/blob/f7da5e48a2740a2bfb1c3dd57bb463ae29599a26/providers/src/airflow/providers/microsoft/azure/triggers/powerbi.py#L116\r\n\r\nThis request is made by the PowerBi hook (using the method `get_refresh_details_by_refresh_id`) and I think that, sometimes, the API is not yet ""up-to-date"" with the latest refresh IDs and can\'t find the status of our refresh ID. So, we fall in this case: https://github.com/apache/airflow/blob/f7da5e48a2740a2bfb1c3dd57bb463ae29599a26/providers/src/airflow/providers/microsoft/azure/hooks/powerbi.py#L159\r\n\r\nand the hook raises the error ""Unable to fetch the details of dataset refresh with Request Id"" (which is also what I find in my Airflow logs). Do you think it makes sense?\r\n\r\nI did try to add a retry on this request (and it seemed to fix the bug), but I understand that you don\'t want to introduce this type of custom retry mechanism. Maybe another option could be to wait ""check_interval"" seconds BEFORE sending the first request. What do you think?', 'created_at': datetime.datetime(2025, 1, 2, 13, 35, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568104948, 'issue_id': 2715483775, 'author': 'dabla', 'body': '> Hello! Thank you very much for taking the time to look at the code and making this suggestion. I tried to reduce the `check_interval` to 5, unfortunately it did not seem to fix the issue on our side.\r\n> \r\n> Also, I thought that the default timeout was 1 week because of this line:\r\n> \r\n> https://github.com/apache/airflow/blob/608de6b97ef04307559d79b2044899adfb9b2a93/providers/src/airflow/providers/microsoft/azure/triggers/powerbi.py#L57\r\n> \r\n> But maybe I got this wrong. Am I mistaken?\r\n> \r\n> Regarding the bug, I think the issue may come from the very first request to get the refresh status, at this line:\r\n> \r\n> https://github.com/apache/airflow/blob/f7da5e48a2740a2bfb1c3dd57bb463ae29599a26/providers/src/airflow/providers/microsoft/azure/triggers/powerbi.py#L116\r\n> \r\n> This request is made by the PowerBi hook (using the method `get_refresh_details_by_refresh_id`) and I think that, sometimes, the API is not yet ""up-to-date"" with the latest refresh IDs and can\'t find the status of our refresh ID. So, we fall in this case:\r\n> \r\n> https://github.com/apache/airflow/blob/f7da5e48a2740a2bfb1c3dd57bb463ae29599a26/providers/src/airflow/providers/microsoft/azure/hooks/powerbi.py#L159\r\n> \r\n> and the hook raises the error ""Unable to fetch the details of dataset refresh with Request Id"" (which is also what I find in my Airflow logs). Do you think it makes sense?\r\n> \r\n> I did try to add a retry on this request (and it seemed to fix the bug), but I understand that you don\'t want to introduce this type of custom retry mechanism. Maybe another option could be to wait ""check_interval"" seconds BEFORE sending the first request. What do you think?\r\n\r\n@Ohashiro from what I can see from my smartphone (no laptop atm) your explanation does totally make sense.  Waiting before doing the actual invocation could make sense, but I would suggest checking the PowerBIDatasetRefreshException and reinvoke the method after waiting for the the interval when this error occurs within the code.', 'created_at': datetime.datetime(2025, 1, 2, 17, 14, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568113154, 'issue_id': 2715483775, 'author': 'dabla', 'body': '@Ohashiro also thank you for investigating this thouroughly. It would also be nice to have a unit test which reproduces this behaviour, so on fist invocation raising an PowerBIDatasetRefreshException, then the code waits for the interval on on second invocation the call would succeed. This should be doable by mocking the PowerBI hook.', 'created_at': datetime.datetime(2025, 1, 2, 17, 21, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568830715, 'issue_id': 2715483775, 'author': 'Ohashiro', 'body': '@dabla \r\nThank you for your feedback! I\'ll be working on it today.\r\n\r\nI have a quick question regarding the best practices. Given that the PowerBi Hook can raise `PowerBIDatasetRefreshException` for different scenarios (including when it didn\'t find the refresh Id, but not only), do you think it would be best to create a new dedicated exception for our case (ex: `PowerBIDatasetRefreshIdNotFoundException`) or would you rather check the exception message to see if it matches ""Unable to fetch the details of dataset refresh with Request Id""?', 'created_at': datetime.datetime(2025, 1, 3, 8, 12, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569028004, 'issue_id': 2715483775, 'author': 'Ohashiro', 'body': ""I opened a PR with both the retry in case of exception on the first request, and the corresponding unit test. I'd be happy to have your feedbacks on this when you have some time!"", 'created_at': datetime.datetime(2025, 1, 3, 10, 44, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2572273229, 'issue_id': 2715483775, 'author': 'ambika-garg', 'body': 'Hi @Ohashiro, thank you for bringing this up and creating the PR to address it. After reviewing the conversation, I see the issue lies in `get refresh history` function in hook class, as sometimes it fails to return the ""dataset refresh histories"" leading the dataset refresh to be marked as fail, even if it actually succeeds. Please correct me if I’ve misunderstood.\r\n\r\nSo, I suggest, we should add the retry mechanism to the get refresh history function only as below, it will retry to fetch histories, if it still fails, then we just throw the exception. That would also mean we don\'t need any extra exception class as you created ""PowerBIDatasetRefreshStatusExecption""\r\n\r\n```\r\n@tenacity.retry(\r\n        stop=tenacity.stop_after_attempt(3),\r\n        wait=tenacity.wait_random_exponential(),\r\n        reraise=True,\r\n        retry=tenacity.retry_if_exception(should_retry_creation),\r\n    )\r\n    async def get_refresh_history(\r\n        self,\r\n        dataset_id: str,\r\n        group_id: str,\r\n    ) -> list[dict[str, str]]:\r\n        """"""\r\n        Retrieve the refresh history of the specified dataset from the given group ID.\r\n\r\n        :param dataset_id: The dataset ID.\r\n        :param group_id: The workspace ID.\r\n\r\n        :return: Dictionary containing all the refresh histories of the dataset.\r\n        """"""\r\n        try:\r\n            response = await self.run(\r\n                url=""myorg/groups/{group_id}/datasets/{dataset_id}/refreshes"",\r\n                path_parameters={\r\n                    ""group_id"": group_id,\r\n                    ""dataset_id"": dataset_id,\r\n                },\r\n            )\r\n\r\n            refresh_histories = response.get(""value"")\r\n            if not refresh_histories:  # Retry if refresh_histories is None or empty\r\n                raise PowerBIDatasetRefreshException(\r\n                    ""Refresh histories are empty; retrying...""\r\n                )\r\n                \r\n            return [self.raw_to_refresh_details(refresh_history) for refresh_history in refresh_histories]\r\n\r\n        except Exception as error:\r\n            raise PowerBIDatasetRefreshException(f""Failed to retrieve refresh history due to error: {error}"")\r\n```', 'created_at': datetime.datetime(2025, 1, 6, 4, 53, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2572497803, 'issue_id': 2715483775, 'author': 'dabla', 'body': '> Hi @Ohashiro, thank you for bringing this up and creating the PR to address it. After reviewing the conversation, I see the issue lies in `get refresh history` function in hook class, as sometimes it fails to return the ""dataset refresh histories"" leading the dataset refresh to be marked as fail, even if it actually succeeds. Please correct me if I’ve misunderstood.\r\n> \r\n> So, I suggest, we should add the retry mechanism to the get refresh history function only as below, it will retry to fetch histories, if it still fails, then we just throw the exception. That would also mean we don\'t need any extra exception class as you created ""PowerBIDatasetRefreshStatusExecption""\r\n> \r\n> ```\r\n> @tenacity.retry(\r\n>         stop=tenacity.stop_after_attempt(3),\r\n>         wait=tenacity.wait_random_exponential(),\r\n>         reraise=True,\r\n>         retry=tenacity.retry_if_exception(should_retry_creation),\r\n>     )\r\n>     async def get_refresh_history(\r\n>         self,\r\n>         dataset_id: str,\r\n>         group_id: str,\r\n>     ) -> list[dict[str, str]]:\r\n>         """"""\r\n>         Retrieve the refresh history of the specified dataset from the given group ID.\r\n> \r\n>         :param dataset_id: The dataset ID.\r\n>         :param group_id: The workspace ID.\r\n> \r\n>         :return: Dictionary containing all the refresh histories of the dataset.\r\n>         """"""\r\n>         try:\r\n>             response = await self.run(\r\n>                 url=""myorg/groups/{group_id}/datasets/{dataset_id}/refreshes"",\r\n>                 path_parameters={\r\n>                     ""group_id"": group_id,\r\n>                     ""dataset_id"": dataset_id,\r\n>                 },\r\n>             )\r\n> \r\n>             refresh_histories = response.get(""value"")\r\n>             if not refresh_histories:  # Retry if refresh_histories is None or empty\r\n>                 raise PowerBIDatasetRefreshException(\r\n>                     ""Refresh histories are empty; retrying...""\r\n>                 )\r\n>                 \r\n>             return [self.raw_to_refresh_details(refresh_history) for refresh_history in refresh_histories]\r\n> \r\n>         except Exception as error:\r\n>             raise PowerBIDatasetRefreshException(f""Failed to retrieve refresh history due to error: {error}"")\r\n> ```\r\n\r\nI don\'t think it\'s a good practice to use tenacity retry mechanism while operators (e.g. task instances) have there own retry mechanism managed by Airflow tasks, but I could be wrong.  I also went back to the code and I still don\'t understand why the call would still fail when the task instance for that operator is retried the second time by Airflow, as some time would have passed between the first and the second attempt, I would expect the second call to succeed but apparently it still doesn\'t.\r\n\r\nMaybe it because of this part:\r\n\r\n```\r\n    async def run(self) -> AsyncIterator[TriggerEvent]:\r\n        """"""Make async connection to the PowerBI and polls for the dataset refresh status.""""""\r\n        # this is always called, even if it succeeded during first attempt but is retried because the second one below fails, maybe something has to be done to avoid that when a retry is being executed\r\n        self.dataset_refresh_id = await self.hook.trigger_dataset_refresh(\r\n            dataset_id=self.dataset_id,\r\n            group_id=self.group_id,\r\n        )\r\n\r\n        async def fetch_refresh_status_and_error() -> tuple[str, str]:\r\n            """"""Fetch the current status and error of the dataset refresh.""""""\r\n            # this is the call that fails, because it probably happens to fast after the above one\r\n            refresh_details = await self.hook.get_refresh_details_by_refresh_id(\r\n                dataset_id=self.dataset_id,\r\n                group_id=self.group_id,\r\n                refresh_id=self.dataset_refresh_id,\r\n            )\r\n            return refresh_details[""status""], refresh_details[""error""]\r\n```\r\n\r\nSo maybe @Ohashiro a delay before calling the get_refresh_details_by_refresh_id would be a possible easy fix to avoid the issue on the second call, even though I don\'t like it that much and also doesn\'t ensure it will eventually succeed.\r\n\r\nWhat I suggest is to refactor the PowerBITrigger and PowerBIDatasetRefreshOperator.  The PowerBITrigger should get an extra dataset_refresh_id parameter in the constructor, so the run method can be called without and with the dataset_refresh_id  parameter, that way it can handle both scenario\'s and return corresponding TriggerEvents regarding the executed flow (e.g. is a dataset refresh being triggered or do we want to get the dataset refresh details), code modifications could possibly look like this in PowerBITrigger :\r\n\r\n```\r\n    def __init__(\r\n        self,\r\n        conn_id: str,\r\n        dataset_id: str,\r\n        group_id: str,\r\n        timeout: float = 60 * 60 * 24 * 7,\r\n        proxies: dict | None = None,\r\n        api_version: APIVersion | str | None = None,\r\n        check_interval: int = 60,\r\n        wait_for_termination: bool = True,\r\n        dataset_refresh_id: str | None = None,  # add dataset_refresh_id parameter\r\n    ):\r\n        super().__init__()\r\n        self.hook = PowerBIHook(conn_id=conn_id, proxies=proxies, api_version=api_version, timeout=timeout)\r\n        self.dataset_id = dataset_id\r\n        self.timeout = timeout\r\n        self.group_id = group_id\r\n        self.check_interval = check_interval\r\n        self.wait_for_termination = wait_for_termination\r\n        self.dataset_refresh_id = dataset_refresh_id\r\n        \r\n        def serialize(self):\r\n              """"""Serialize the trigger instance.""""""\r\n              return (\r\n                  ""airflow.providers.microsoft.azure.triggers.powerbi.PowerBITrigger"",\r\n                  {\r\n                      ""conn_id"": self.conn_id,\r\n                      ""proxies"": self.proxies,\r\n                      ""api_version"": self.api_version,\r\n                      ""dataset_id"": self.dataset_id,\r\n                      ""group_id"": self.group_id,\r\n                      ""timeout"": self.timeout,\r\n                      ""check_interval"": self.check_interval,\r\n                      ""wait_for_termination"": self.wait_for_termination,\r\n                      ""dataset_refresh_id "": self.dataset_refresh_id ,  # IMPORTANT: do not forget to add parameter in serialize method also\r\n                  },\r\n              )\r\n\r\n    async def run(self) -> AsyncIterator[TriggerEvent]:\r\n        """"""Make async connection to the PowerBI and polls for the dataset refresh status.""""""\r\nsomething has to be done to avoid that when a retry is being executed\r\n        if not self.dataset_refresh_id:\r\n            dataset_refresh_id = await self.hook.trigger_dataset_refresh(\r\n                dataset_id=self.dataset_id,\r\n                group_id=self.group_id,\r\n            )\r\n            \r\n            # Just yield a TriggerEvent with the dataset_refresh_id, that will then be used by the operator to retrigger it with the corresponding dataset_refresh_id so the PowerBITrigger knows it has to only get the refresh details in case of failure, then the refresh details would then be executed.\r\n            yield TriggerEvent(\r\n                {\r\n                    ""status"": ""success"",\r\n                    ""message"": f""The dataset refresh {self.dataset_refresh_id} has been triggered."",\r\n                    ""dataset_refresh_id"": dataset_refresh_id,\r\n                }\r\n            )\r\n\r\n          async def fetch_refresh_status_and_error() -> tuple[str, str]:\r\n              """"""Fetch the current status and error of the dataset refresh.""""""\r\n              refresh_details = await self.hook.get_refresh_details_by_refresh_id(\r\n                  dataset_id=self.dataset_id,\r\n                  group_id=self.group_id,\r\n                  refresh_id=self.dataset_refresh_id,\r\n              )\r\n              return refresh_details[""status""], refresh_details[""error""]\r\n```\r\n\r\nThen in the PowerBIDatasetRefreshOperator, both TriggerEvents should be handled accordingly, which means the PowerBITrigger will be called (e.g. deferred) twice, once to trigger the dataset_refresh_id and once to poll for the get_refresh_details_by_refresh_id.  On the first attempt, if the trigger succeeds, the dataset_refresh_id should be persisted as an XCom within the operator, so that when the seconds calls fails the operator can directly retry the second Trigger call.\r\n\r\nThis is of course a more complex approach due to the deferrable aspect, but would be more in line with how operators should work imho instead of hiding failures and doing retries outside Airflow flow using the tenacity library.  From what I see in the code base, tenacity is only used in the retries module of Airflow and the cli commands, not in the operators/hooks/triggerers.', 'created_at': datetime.datetime(2025, 1, 6, 7, 54, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2572631048, 'issue_id': 2715483775, 'author': 'Ohashiro', 'body': 'Hello! @dabla thank you for your investigation and suggestion!\r\n\r\nRegarding your first point:\r\n\r\n> I also went back to the code and I still don\'t understand why the call would still fail when the task instance for that operator is retried the second time by Airflow, as some time would have passed between the first and the second attempt, I would expect the second call to succeed but apparently it still doesn\'t.\r\n\r\nFrom what I understand, when the task fails (for example, in our case, because the refreshId was not found), the operator cancels the refresh (using `cancel_dataset_refresh` hook method). So when the task is retried by Airflow, a new refresh is triggered with a new refreshId. \r\n\r\nhttps://github.com/apache/airflow/blob/413a1833c302e2409d2ac96c6521f97e6589a594/providers/src/airflow/providers/microsoft/azure/hooks/powerbi.py#L202\r\n\r\nRegarding the fix implementation, the delay solution we discussed seems to work well (which, imho, confirm the bug root cause), but I agree that this fix is more of a ""quick fix"" than a clean one.\r\n\r\nI can work on the refactor you suggested to check, I just have a few questions:\r\n\r\n> so that when the seconds calls fails the operator can directly retry the second Trigger call\r\n\r\nRegarding the retry mechanism you are suggesting in the operator, how would you do it? \r\nI just wrote the following draft, do you confirm this is what you expected? However I\'m not sure how you\'d handle the retry part... Would you let the operator fail and when the task is retried, it tried to fetch the refreshId directly? Or would you make the operator retry the Trigger in the same task?\r\n\r\nRegarding the trigger, if I understand correctly, the `run` method would look like that:\r\n```python\r\nasync def run(self) -> AsyncIterator[TriggerEvent]:\r\n\r\n   if not self.dataset_refresh_id:\r\n      # Just yield a TriggerEvent with the dataset_refresh_id, that will then be used by the operator to retrigger it with the corresponding dataset_refresh_id so the PowerBITrigger knows it has to only get the refresh details in case of failure, then the refresh details would then be executed.\r\n      yield TriggerEvent(\r\n         ...\r\n      )\r\n\r\n   else:\r\n      # Handle the ""while"" loop looking for the refresh status\r\n```\r\n\r\nAnd regarding the operator, if I understand correctly, it would look like that:\r\n```python\r\nclass PowerBIDatasetRefreshOperator(BaseOperator):\r\n   def execute(self, context: Context):\r\n        """"""Refresh the Power BI Dataset.""""""\r\n        if self.wait_for_termination:\r\n            self.defer(\r\n                trigger=PowerBITrigger(...),\r\n                method_name=self.push_refreshId.__name__,\r\n            )\r\n\r\n   def push_refreshId():\r\n      # push the refresh Id to xcom\r\n      self.xcom_push(\r\n                context=context, key=""powerbi_dataset_refresh_Id"", value=event[""dataset_refresh_id""]\r\n            )\r\n      self.defer(\r\n                trigger=PowerBITrigger(...),\r\n                method_name=self.execute.__name__,\r\n            )\r\n\r\n   def execute():\r\n      # exit the operator as currently done\r\n```', 'created_at': datetime.datetime(2025, 1, 6, 9, 6, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2572797731, 'issue_id': 2715483775, 'author': 'dabla', 'body': '> Hello! @dabla thank you for your investigation and suggestion!\r\n> \r\n> Regarding your first point:\r\n> \r\n> > I also went back to the code and I still don\'t understand why the call would still fail when the task instance for that operator is retried the second time by Airflow, as some time would have passed between the first and the second attempt, I would expect the second call to succeed but apparently it still doesn\'t.\r\n> \r\n> From what I understand, when the task fails (for example, in our case, because the refreshId was not found), the operator cancels the refresh (using `cancel_dataset_refresh` hook method). So when the task is retried by Airflow, a new refresh is triggered with a new refreshId.\r\n> \r\n> https://github.com/apache/airflow/blob/413a1833c302e2409d2ac96c6521f97e6589a594/providers/src/airflow/providers/microsoft/azure/hooks/powerbi.py#L202\r\n> \r\n> Regarding the fix implementation, the delay solution we discussed seems to work well (which, imho, confirm the bug root cause), but I agree that this fix is more of a ""quick fix"" than a clean one.\r\n> \r\n> I can work on the refactor you suggested to check, I just have a few questions:\r\n> \r\n> > so that when the seconds calls fails the operator can directly retry the second Trigger call\r\n> \r\n> Regarding the retry mechanism you are suggesting in the operator, how would you do it? I just wrote the following draft, do you confirm this is what you expected? However I\'m not sure how you\'d handle the retry part... Would you let the operator fail and when the task is retried, it tried to fetch the refreshId directly? Or would you make the operator retry the Trigger in the same task?\r\n> \r\n> Regarding the trigger, if I understand correctly, the `run` method would look like that:\r\n> \r\n> ```python\r\n> async def run(self) -> AsyncIterator[TriggerEvent]:\r\n> \r\n>    if not self.dataset_refresh_id:\r\n>       # Just yield a TriggerEvent with the dataset_refresh_id, that will then be used by the operator to retrigger it with the corresponding dataset_refresh_id so the PowerBITrigger knows it has to only get the refresh details in case of failure, then the refresh details would then be executed.\r\n>       yield TriggerEvent(\r\n>          ...\r\n>       )\r\n> \r\n>    else:\r\n>       # Handle the ""while"" loop looking for the refresh status\r\n> ```\r\n> \r\n> And regarding the operator, if I understand correctly, it would look like that:\r\n> \r\n> ```python\r\n> class PowerBIDatasetRefreshOperator(BaseOperator):\r\n>    def execute(self, context: Context):\r\n>         """"""Refresh the Power BI Dataset.""""""\r\n>         if self.wait_for_termination:\r\n>             self.defer(\r\n>                 trigger=PowerBITrigger(...),\r\n>                 method_name=self.push_refreshId.__name__,\r\n>             )\r\n> \r\n>    def push_refreshId():\r\n>       # push the refresh Id to xcom\r\n>       self.xcom_push(\r\n>                 context=context, key=""powerbi_dataset_refresh_Id"", value=event[""dataset_refresh_id""]\r\n>             )\r\n>       self.defer(\r\n>                 trigger=PowerBITrigger(...),\r\n>                 method_name=self.execute.__name__,\r\n>             )\r\n> \r\n>    def execute():\r\n>       # exit the operator as currently done\r\n> ```\r\n\r\nIndeed, that\'s the main issue, because the fact that the refresh details fails and how the operator is implemented today, instead of trying to directly get the refresh details on the second attempt, it will again trigger a new dataset refresh and then again try to get its refresh details, which of course like you mentioned, will fail again, thus the main problem persists.\r\n\r\nThat\'s why I suggested the refactor, so that the PowerBITrigger can handle both cases separately, and that the operator can then retry the second case directly instead of redoing the whole flow.\r\n\r\nI also saw after a remark made by my colleague @joffreybienvenu-infrabel that the tenacity is actually also used in the KubernetesPodOperator, so I was wrong there that it\'s not being used by operators.  But still I think it\'s better to use the retry mechanism implemented by the TaskInstance instead of bypassing it and doing it directly within the hook/operator, as imho that\'s not the purpose/ good practise but again I could be wrong.\r\n\r\nAlso regarding you retry question, the retry mechanism is implemented by default by the task handling in Airflow, that\'s why I want to avoid the usage of tenacity (I see this more as a hack or quick fix than an actual good solution) as there is already a solution for that in Airflow.  So I you do the refactor as I suggested, the retry mechanism will work fine if the xcom_push will work when a task fails afterwards, so that something to be tested.', 'created_at': datetime.datetime(2025, 1, 6, 10, 16, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2572812009, 'issue_id': 2715483775, 'author': 'Ohashiro', 'body': 'Btw, I just (locally) implemented your refactor and the separation between the 2 flows (refresh trigger and get refresh status) in 2 deferrable triggers seems to let enough time between the refresh creation and the first ""get refresh status"" request. It seems that I don\'t get the error anymore (not that the bug is really fixed, but in my environment, the duration between both events is enough to prevent the error).\r\n\r\n> the operator can then retry the second case directly instead of redoing the whole flow\r\n\r\nRegarding this, how/where would you retry the second case?', 'created_at': datetime.datetime(2025, 1, 6, 10, 24, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2572818531, 'issue_id': 2715483775, 'author': 'dabla', 'body': '> Btw, I just (locally) implemented your refactor and the separation between the 2 flows (refresh trigger and get refresh status) in 2 deferrable triggers seems to let enough time between the refresh creation and the first ""get refresh status"" request. It seems that I don\'t get the error anymore (not that the bug is really fixed, but in my environment, the duration between both events is enough to prevent the error).\r\n> \r\n> > the operator can then retry the second case directly instead of redoing the whole flow\r\n> \r\n> Regarding this, how/where would you retry the second case?\r\n\r\nIndeed, that\'s also a consequence of using the trigger twice, as more time will pass automatically between both invocations, the error will probably not occurs anymore and everything will succeed in one attempt.\r\n\r\nFor the second case, in the operator, you should do in the execute method an xcom_pull to see if there is an existing dataset_refresh_id or not, and if not you know you have to execute the whole flow, if it\'s there then you know you should only trigger the second part.', 'created_at': datetime.datetime(2025, 1, 6, 10, 28, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2573201358, 'issue_id': 2715483775, 'author': 'Ohashiro', 'body': '> For the second case, in the operator, you should do in the execute method an xcom_pull to see if there is an existing dataset_refresh_id or not, and if not you know you have to execute the whole flow, if it\'s there then you know you should only trigger the second part.\r\n\r\nAfter a bit a investigation, I think this won\'t be possible to pass XCom messages between different task executions of the same operator (if this was what you meant). According to [Airflow XComs documentation](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/xcoms.html), ""_If the first task run is not succeeded then on every retry task XComs will be cleared to make the task run idempotent._"". From what I understand, it is not possible for a 2nd operator (in a task retry) to access the 1st task XCom message.\r\nHowever, it should be possible to retry the operator ""execute"" function without retrying the task, as done in `DmsStartReplicationOperator`: \r\nhttps://github.com/apache/airflow/blob/9ba279d15f26088400b15281b3cc346e2d7a0e30/providers/src/airflow/providers/amazon/aws/operators/dms.py#L720-L723\r\n\r\nThough I\'m not sure if this is what you\'d prefer.\r\n\r\n\r\n\r\nAfter looking at the codebase, I see retry mechanisms in some hooks (ex:`BaseDatabricksHook`, `LivyHook`, `GlueJobHook`, `EmrContainerHook`...), handlers (ex: `S3TaskHandler`) and in some operators as well (ex: the Kubernetes operators as your colleague pointed out, or some AWS operators such as `BedrockCustomizeModelOperator`, `DmsDeleteReplicationConfigOperator`...). I didn\'t look at all the files, there must be others. \r\nThe retry can be used to wait for a status (ex: hook `ElastiCacheReplicationGroupHook`, method `wait_for_availability`, [link](https://github.com/apache/airflow/blob/9ba279d15f26088400b15281b3cc346e2d7a0e30/providers/src/airflow/providers/amazon/aws/hooks/elasticache_replication_group.py#L120)) or to wait for an API request to succeed. In our case, I think that we can fall in these cases: We could add a retry in  `PowerBIHook.get_refresh_details_by_refresh_id` before raising the PowerBIDatasetRefreshException?\r\n\r\nNot sure which solution is best to be honest. What do you think? \r\n- Should I set the retry within the operator? with a `retry_execution` method? or a `while` loop in the `execute` method? (or else?)\r\n- Should I handle the retry in the existing `while` loop from the trigger?\r\n- Or should I set a retry in the hook directly?', 'created_at': datetime.datetime(2025, 1, 6, 14, 15, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2574683765, 'issue_id': 2715483775, 'author': 'dabla', 'body': '> However, it should be possible to retry the operator ""execute"" function without retrying the task, as done in DmsStartReplicationOperator\r\n\r\nYou\'re right about the Xcoms being flushed when a task fails, I like the idea/above of approach, by define a dedicated retry method which is passed as argument of the next_method of the Trigger, that could be an option.  If it\'s too difficult, then I would do what @ambika-garg proposed and I think you also proposed at the beginning of this issue.', 'created_at': datetime.datetime(2025, 1, 7, 8, 35, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2575249301, 'issue_id': 2715483775, 'author': 'Ohashiro', 'body': 'Hi @dabla \r\nI quickly tested what we said, so that the triggers return to `execute_complete` method, which itself checks the message and if it detects that there was an issue fetching the refresh Id, it redirects to a `retry_execution` method that counts the retries and re-executes the `execute` method. Of course the code is not very clean, but here is an overview:\r\n\r\n```python\r\ndef execute(self, context: Context):\r\n    """"""Refresh the Power BI Dataset.""""""\r\n    if self.wait_for_termination:\r\n        self.defer(\r\n            trigger=PowerBITrigger(\r\n                conn_id=self.conn_id,\r\n                group_id=self.group_id,\r\n                dataset_id=self.dataset_id,\r\n                timeout=self.timeout,\r\n                proxies=self.proxies,\r\n                api_version=self.api_version,\r\n                check_interval=self.check_interval,\r\n                wait_for_termination=self.wait_for_termination,\r\n            ),\r\n            method_name=self.get_refresh_status.__name__,\r\n        )\r\n\r\ndef get_refresh_status(self, context: Context, event: dict[str, str] | None = None):\r\n    """"""Push the refresh Id to XCom then runs the Triggers to wait for refresh completion.""""""\r\n\r\n    if event:\r\n        if event[""status""] == ""error"" and ""Unable to fetch the details of dataset refresh with Request Id"" not in event[""message""] and ""not found"" not in event[""message""]:\r\n            raise AirflowException(event[""message""])\r\n\r\n        self.xcom_push(context=context, key=""powerbi_dataset_refresh_Id"", value=event[""dataset_refresh_id""])\r\n\r\n    dataset_refresh_id = self.xcom_pull(context=context, key=""powerbi_dataset_refresh_Id"")\r\n    if dataset_refresh_id:\r\n        self.defer(\r\n            trigger=PowerBITrigger(\r\n                conn_id=self.conn_id,\r\n                group_id=self.group_id,\r\n                dataset_id=self.dataset_id,\r\n                dataset_refresh_id=dataset_refresh_id,\r\n                timeout=self.timeout,\r\n                proxies=self.proxies,\r\n                api_version=self.api_version,\r\n                check_interval=self.check_interval,\r\n                wait_for_termination=self.execute_complete,\r\n            ),\r\n            method_name=self.execute_complete.__name__,\r\n        )\r\n\r\ndef retry_execution(self, context: Context):\r\n    retries = self.xcom_pull(context=context, key=""retries"")\r\n    if retries and retries >= self.max_retries:\r\n        raise AirflowException(""Max number of retries reached!"")\r\n\r\n    if not retries:\r\n        retries = 0\r\n    self.xcom_push(context=context, key=""retries"", value=retries+1)\r\n\r\n    self.get_refresh_status(context)\r\n\r\ndef execute_complete(self, context: Context, event: dict[str, str]) -> Any:\r\n    """"""\r\n    Return immediately - callback for when the trigger fires.\r\n\r\n    Relies on trigger to throw an exception, otherwise it assumes execution was successful.\r\n    """"""\r\n    if event:\r\n        if event[""status""] == ""error"":\r\n            if ""Unable to fetch the details of dataset refresh with Request Id"" in event[""message""] or ""not found"" in event[""message""]:\r\n                self.retry_execution(context)\r\n            else:\r\n                raise AirflowException(event[""message""])\r\n\r\n        self.xcom_push(context=context, key=""powerbi_dataset_refresh_status"", value=event[""status""])\r\n\r\n```\r\n\r\nNote: in addition to these changes, we should add a new way to handle the refresh cancellation. By default, if the trigger encounters an exception, it cancels the refresh (which is not compatible with the retry made by the operator). If we keep this solution, we have to change this behavior.\r\n\r\nI think this solution can work but might add a little too much complexity to the operator compared to a simple retry, though I think that this separation between the trigger refresh and the status fetch is nice.\r\nWhat\'s your opinion?', 'created_at': datetime.datetime(2025, 1, 7, 13, 3, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2577233080, 'issue_id': 2715483775, 'author': 'dabla', 'body': ""The separation of the 2 flows handled by the PowerBITriggerer is a good thing indeed, so the effort there is not lost and is in fact a cleaner design.  Now if it's to complex to handle the retry within the operator, then I propose you use the tenacity on that refresh_history method of the PowerBIHook."", 'created_at': datetime.datetime(2025, 1, 8, 9, 45, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2580851281, 'issue_id': 2715483775, 'author': 'Ohashiro', 'body': ""Hi @dabla\r\nI opened a PR with the following changes: the separation between the 2 flows and the retry on the func `fetch_refresh_status_and_error`. \r\nI'd be happy to have your feedbacks on this PR when you have some time !"", 'created_at': datetime.datetime(2025, 1, 9, 17, 14, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2582098823, 'issue_id': 2715483775, 'author': 'dabla', 'body': ""> Hi @dabla I opened a PR with the following changes: the separation between the 2 flows and the retry on the func `fetch_refresh_status_and_error`. I'd be happy to have your feedbacks on this PR when you have some time !\r\n\r\n@Ohashiro Just reviewed it, looks good to me, nice job!"", 'created_at': datetime.datetime(2025, 1, 10, 8, 56, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2594029844, 'issue_id': 2715483775, 'author': 'harryvgiunta', 'body': 'looking forward to this fix, running into same issue', 'created_at': datetime.datetime(2025, 1, 15, 22, 3, 25, tzinfo=datetime.timezone.utc)}]","dabla on (2024-12-18 14:20:54 UTC): Can you just specify a retry with following parameters on the PowerBIDatasetRefreshOperator?

```
retry_exponential_backoff=True,  # to increase the delay after each failed attempt
retry_delay=60,
retries=5,
```

Ohashiro (Issue Creator) on (2024-12-19 09:09:08 UTC): Thank you for the suggestion! (sorry I didn't know these fields were available)
I just tried it and still get some task fails (with the same error). I will try tweaking the parameters a little to see if it fixes the issue.
Do you know to what (which request) do these ""retries"" apply?

dabla on (2024-12-19 13:33:50 UTC): Those parameters apply on the whole operator, you could also try to specify the check_interval  and timeout parameter, which by default both are 60 seconds, I think timeout should be greater then then check_interval, otherwise it will stop polling due to timeout.

Ohashiro (Issue Creator) on (2024-12-19 13:39:57 UTC): OK I see, so the retry is not specific to the ""get status"" request. 
From what I understand, the issue seems to be that the very first request (to get the refresh status) fails (maybe because the refresh id is not yet available for this endpoint) and the task exits. 
If I understand correctly, the retry on the operator allows us to re-run the whole process (so re-trigger a refresh), instead of retrying the request to get the refresh status already running.

dabla on (2024-12-27 10:08:58 UTC): Indeed, you're correct about it. I still don't understand your issue completely, as from what I see in the code of the 
PowerBIDatasetRefreshOperator, it first triggers the refresh datasets and then polls and wait for the refresh to complete (which by default has a timeout of 60 seconds).  So indeed it could happen that the refresh isn't ready or started yet, but still I would expect it to retry within the timeout interval, unless of course the timeout has expired, but that's something you can parametrize.

dabla on (2024-12-27 10:14:54 UTC): I understand the issue, I've checked the code of the PowerBITrigger and the check_interval is set to 60 seconds, but the timeout is also 60 seconds by default, which means the status is only fetched once and then it fails like in your example log.  So you should reduce the check_interval value to like 10 seconds for example, or even a bit less.  So try setting the check_interval to 10 seconds.  Maybe we should do a PR and add a check that the check_interval must be smaller than the timeout and set it to 10 seconds by default.

Ohashiro (Issue Creator) on (2025-01-02 13:35:10 UTC): Hello!
Thank you very much for taking the time to look at the code and making this suggestion.
I tried to reduce the `check_interval` to 5, unfortunately it did not seem to fix the issue on our side.

Also, I thought that the default timeout was 1 week because of this line: https://github.com/apache/airflow/blob/608de6b97ef04307559d79b2044899adfb9b2a93/providers/src/airflow/providers/microsoft/azure/triggers/powerbi.py#L57

But maybe I got this wrong. Am I mistaken?

Regarding the bug, I think the issue may come from the very first request to get the refresh status, at this line: https://github.com/apache/airflow/blob/f7da5e48a2740a2bfb1c3dd57bb463ae29599a26/providers/src/airflow/providers/microsoft/azure/triggers/powerbi.py#L116

This request is made by the PowerBi hook (using the method `get_refresh_details_by_refresh_id`) and I think that, sometimes, the API is not yet ""up-to-date"" with the latest refresh IDs and can't find the status of our refresh ID. So, we fall in this case: https://github.com/apache/airflow/blob/f7da5e48a2740a2bfb1c3dd57bb463ae29599a26/providers/src/airflow/providers/microsoft/azure/hooks/powerbi.py#L159

and the hook raises the error ""Unable to fetch the details of dataset refresh with Request Id"" (which is also what I find in my Airflow logs). Do you think it makes sense?

I did try to add a retry on this request (and it seemed to fix the bug), but I understand that you don't want to introduce this type of custom retry mechanism. Maybe another option could be to wait ""check_interval"" seconds BEFORE sending the first request. What do you think?

dabla on (2025-01-02 17:14:55 UTC): @Ohashiro from what I can see from my smartphone (no laptop atm) your explanation does totally make sense.  Waiting before doing the actual invocation could make sense, but I would suggest checking the PowerBIDatasetRefreshException and reinvoke the method after waiting for the the interval when this error occurs within the code.

dabla on (2025-01-02 17:21:26 UTC): @Ohashiro also thank you for investigating this thouroughly. It would also be nice to have a unit test which reproduces this behaviour, so on fist invocation raising an PowerBIDatasetRefreshException, then the code waits for the interval on on second invocation the call would succeed. This should be doable by mocking the PowerBI hook.

Ohashiro (Issue Creator) on (2025-01-03 08:12:07 UTC): @dabla 
Thank you for your feedback! I'll be working on it today.

I have a quick question regarding the best practices. Given that the PowerBi Hook can raise `PowerBIDatasetRefreshException` for different scenarios (including when it didn't find the refresh Id, but not only), do you think it would be best to create a new dedicated exception for our case (ex: `PowerBIDatasetRefreshIdNotFoundException`) or would you rather check the exception message to see if it matches ""Unable to fetch the details of dataset refresh with Request Id""?

Ohashiro (Issue Creator) on (2025-01-03 10:44:15 UTC): I opened a PR with both the retry in case of exception on the first request, and the corresponding unit test. I'd be happy to have your feedbacks on this when you have some time!

ambika-garg on (2025-01-06 04:53:17 UTC): Hi @Ohashiro, thank you for bringing this up and creating the PR to address it. After reviewing the conversation, I see the issue lies in `get refresh history` function in hook class, as sometimes it fails to return the ""dataset refresh histories"" leading the dataset refresh to be marked as fail, even if it actually succeeds. Please correct me if I’ve misunderstood.

So, I suggest, we should add the retry mechanism to the get refresh history function only as below, it will retry to fetch histories, if it still fails, then we just throw the exception. That would also mean we don't need any extra exception class as you created ""PowerBIDatasetRefreshStatusExecption""

```
@tenacity.retry(
        stop=tenacity.stop_after_attempt(3),
        wait=tenacity.wait_random_exponential(),
        reraise=True,
        retry=tenacity.retry_if_exception(should_retry_creation),
    )
    async def get_refresh_history(
        self,
        dataset_id: str,
        group_id: str,
    ) -> list[dict[str, str]]:
        """"""
        Retrieve the refresh history of the specified dataset from the given group ID.

        :param dataset_id: The dataset ID.
        :param group_id: The workspace ID.

        :return: Dictionary containing all the refresh histories of the dataset.
        """"""
        try:
            response = await self.run(
                url=""myorg/groups/{group_id}/datasets/{dataset_id}/refreshes"",
                path_parameters={
                    ""group_id"": group_id,
                    ""dataset_id"": dataset_id,
                },
            )

            refresh_histories = response.get(""value"")
            if not refresh_histories:  # Retry if refresh_histories is None or empty
                raise PowerBIDatasetRefreshException(
                    ""Refresh histories are empty; retrying...""
                )
                
            return [self.raw_to_refresh_details(refresh_history) for refresh_history in refresh_histories]

        except Exception as error:
            raise PowerBIDatasetRefreshException(f""Failed to retrieve refresh history due to error: {error}"")
```

dabla on (2025-01-06 07:54:26 UTC): I don't think it's a good practice to use tenacity retry mechanism while operators (e.g. task instances) have there own retry mechanism managed by Airflow tasks, but I could be wrong.  I also went back to the code and I still don't understand why the call would still fail when the task instance for that operator is retried the second time by Airflow, as some time would have passed between the first and the second attempt, I would expect the second call to succeed but apparently it still doesn't.

Maybe it because of this part:

```
    async def run(self) -> AsyncIterator[TriggerEvent]:
        """"""Make async connection to the PowerBI and polls for the dataset refresh status.""""""
        # this is always called, even if it succeeded during first attempt but is retried because the second one below fails, maybe something has to be done to avoid that when a retry is being executed
        self.dataset_refresh_id = await self.hook.trigger_dataset_refresh(
            dataset_id=self.dataset_id,
            group_id=self.group_id,
        )

        async def fetch_refresh_status_and_error() -> tuple[str, str]:
            """"""Fetch the current status and error of the dataset refresh.""""""
            # this is the call that fails, because it probably happens to fast after the above one
            refresh_details = await self.hook.get_refresh_details_by_refresh_id(
                dataset_id=self.dataset_id,
                group_id=self.group_id,
                refresh_id=self.dataset_refresh_id,
            )
            return refresh_details[""status""], refresh_details[""error""]
```

So maybe @Ohashiro a delay before calling the get_refresh_details_by_refresh_id would be a possible easy fix to avoid the issue on the second call, even though I don't like it that much and also doesn't ensure it will eventually succeed.

What I suggest is to refactor the PowerBITrigger and PowerBIDatasetRefreshOperator.  The PowerBITrigger should get an extra dataset_refresh_id parameter in the constructor, so the run method can be called without and with the dataset_refresh_id  parameter, that way it can handle both scenario's and return corresponding TriggerEvents regarding the executed flow (e.g. is a dataset refresh being triggered or do we want to get the dataset refresh details), code modifications could possibly look like this in PowerBITrigger :

```
    def __init__(
        self,
        conn_id: str,
        dataset_id: str,
        group_id: str,
        timeout: float = 60 * 60 * 24 * 7,
        proxies: dict | None = None,
        api_version: APIVersion | str | None = None,
        check_interval: int = 60,
        wait_for_termination: bool = True,
        dataset_refresh_id: str | None = None,  # add dataset_refresh_id parameter
    ):
        super().__init__()
        self.hook = PowerBIHook(conn_id=conn_id, proxies=proxies, api_version=api_version, timeout=timeout)
        self.dataset_id = dataset_id
        self.timeout = timeout
        self.group_id = group_id
        self.check_interval = check_interval
        self.wait_for_termination = wait_for_termination
        self.dataset_refresh_id = dataset_refresh_id
        
        def serialize(self):
              """"""Serialize the trigger instance.""""""
              return (
                  ""airflow.providers.microsoft.azure.triggers.powerbi.PowerBITrigger"",
                  {
                      ""conn_id"": self.conn_id,
                      ""proxies"": self.proxies,
                      ""api_version"": self.api_version,
                      ""dataset_id"": self.dataset_id,
                      ""group_id"": self.group_id,
                      ""timeout"": self.timeout,
                      ""check_interval"": self.check_interval,
                      ""wait_for_termination"": self.wait_for_termination,
                      ""dataset_refresh_id "": self.dataset_refresh_id ,  # IMPORTANT: do not forget to add parameter in serialize method also
                  },
              )

    async def run(self) -> AsyncIterator[TriggerEvent]:
        """"""Make async connection to the PowerBI and polls for the dataset refresh status.""""""
something has to be done to avoid that when a retry is being executed
        if not self.dataset_refresh_id:
            dataset_refresh_id = await self.hook.trigger_dataset_refresh(
                dataset_id=self.dataset_id,
                group_id=self.group_id,
            )
            
            # Just yield a TriggerEvent with the dataset_refresh_id, that will then be used by the operator to retrigger it with the corresponding dataset_refresh_id so the PowerBITrigger knows it has to only get the refresh details in case of failure, then the refresh details would then be executed.
            yield TriggerEvent(
                {
                    ""status"": ""success"",
                    ""message"": f""The dataset refresh {self.dataset_refresh_id} has been triggered."",
                    ""dataset_refresh_id"": dataset_refresh_id,
                }
            )

          async def fetch_refresh_status_and_error() -> tuple[str, str]:
              """"""Fetch the current status and error of the dataset refresh.""""""
              refresh_details = await self.hook.get_refresh_details_by_refresh_id(
                  dataset_id=self.dataset_id,
                  group_id=self.group_id,
                  refresh_id=self.dataset_refresh_id,
              )
              return refresh_details[""status""], refresh_details[""error""]
```

Then in the PowerBIDatasetRefreshOperator, both TriggerEvents should be handled accordingly, which means the PowerBITrigger will be called (e.g. deferred) twice, once to trigger the dataset_refresh_id and once to poll for the get_refresh_details_by_refresh_id.  On the first attempt, if the trigger succeeds, the dataset_refresh_id should be persisted as an XCom within the operator, so that when the seconds calls fails the operator can directly retry the second Trigger call.

This is of course a more complex approach due to the deferrable aspect, but would be more in line with how operators should work imho instead of hiding failures and doing retries outside Airflow flow using the tenacity library.  From what I see in the code base, tenacity is only used in the retries module of Airflow and the cli commands, not in the operators/hooks/triggerers.

Ohashiro (Issue Creator) on (2025-01-06 09:06:39 UTC): Hello! @dabla thank you for your investigation and suggestion!

Regarding your first point:


From what I understand, when the task fails (for example, in our case, because the refreshId was not found), the operator cancels the refresh (using `cancel_dataset_refresh` hook method). So when the task is retried by Airflow, a new refresh is triggered with a new refreshId. 

https://github.com/apache/airflow/blob/413a1833c302e2409d2ac96c6521f97e6589a594/providers/src/airflow/providers/microsoft/azure/hooks/powerbi.py#L202

Regarding the fix implementation, the delay solution we discussed seems to work well (which, imho, confirm the bug root cause), but I agree that this fix is more of a ""quick fix"" than a clean one.

I can work on the refactor you suggested to check, I just have a few questions:


Regarding the retry mechanism you are suggesting in the operator, how would you do it? 
I just wrote the following draft, do you confirm this is what you expected? However I'm not sure how you'd handle the retry part... Would you let the operator fail and when the task is retried, it tried to fetch the refreshId directly? Or would you make the operator retry the Trigger in the same task?

Regarding the trigger, if I understand correctly, the `run` method would look like that:
```python
async def run(self) -> AsyncIterator[TriggerEvent]:

   if not self.dataset_refresh_id:
      # Just yield a TriggerEvent with the dataset_refresh_id, that will then be used by the operator to retrigger it with the corresponding dataset_refresh_id so the PowerBITrigger knows it has to only get the refresh details in case of failure, then the refresh details would then be executed.
      yield TriggerEvent(
         ...
      )

   else:
      # Handle the ""while"" loop looking for the refresh status
```

And regarding the operator, if I understand correctly, it would look like that:
```python
class PowerBIDatasetRefreshOperator(BaseOperator):
   def execute(self, context: Context):
        """"""Refresh the Power BI Dataset.""""""
        if self.wait_for_termination:
            self.defer(
                trigger=PowerBITrigger(...),
                method_name=self.push_refreshId.__name__,
            )

   def push_refreshId():
      # push the refresh Id to xcom
      self.xcom_push(
                context=context, key=""powerbi_dataset_refresh_Id"", value=event[""dataset_refresh_id""]
            )
      self.defer(
                trigger=PowerBITrigger(...),
                method_name=self.execute.__name__,
            )

   def execute():
      # exit the operator as currently done
```

dabla on (2025-01-06 10:16:43 UTC): Indeed, that's the main issue, because the fact that the refresh details fails and how the operator is implemented today, instead of trying to directly get the refresh details on the second attempt, it will again trigger a new dataset refresh and then again try to get its refresh details, which of course like you mentioned, will fail again, thus the main problem persists.

That's why I suggested the refactor, so that the PowerBITrigger can handle both cases separately, and that the operator can then retry the second case directly instead of redoing the whole flow.

I also saw after a remark made by my colleague @joffreybienvenu-infrabel that the tenacity is actually also used in the KubernetesPodOperator, so I was wrong there that it's not being used by operators.  But still I think it's better to use the retry mechanism implemented by the TaskInstance instead of bypassing it and doing it directly within the hook/operator, as imho that's not the purpose/ good practise but again I could be wrong.

Also regarding you retry question, the retry mechanism is implemented by default by the task handling in Airflow, that's why I want to avoid the usage of tenacity (I see this more as a hack or quick fix than an actual good solution) as there is already a solution for that in Airflow.  So I you do the refactor as I suggested, the retry mechanism will work fine if the xcom_push will work when a task fails afterwards, so that something to be tested.

Ohashiro (Issue Creator) on (2025-01-06 10:24:35 UTC): Btw, I just (locally) implemented your refactor and the separation between the 2 flows (refresh trigger and get refresh status) in 2 deferrable triggers seems to let enough time between the refresh creation and the first ""get refresh status"" request. It seems that I don't get the error anymore (not that the bug is really fixed, but in my environment, the duration between both events is enough to prevent the error).


Regarding this, how/where would you retry the second case?

dabla on (2025-01-06 10:28:25 UTC): Indeed, that's also a consequence of using the trigger twice, as more time will pass automatically between both invocations, the error will probably not occurs anymore and everything will succeed in one attempt.

For the second case, in the operator, you should do in the execute method an xcom_pull to see if there is an existing dataset_refresh_id or not, and if not you know you have to execute the whole flow, if it's there then you know you should only trigger the second part.

Ohashiro (Issue Creator) on (2025-01-06 14:15:42 UTC): After a bit a investigation, I think this won't be possible to pass XCom messages between different task executions of the same operator (if this was what you meant). According to [Airflow XComs documentation](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/xcoms.html), ""_If the first task run is not succeeded then on every retry task XComs will be cleared to make the task run idempotent._"". From what I understand, it is not possible for a 2nd operator (in a task retry) to access the 1st task XCom message.
However, it should be possible to retry the operator ""execute"" function without retrying the task, as done in `DmsStartReplicationOperator`: 
https://github.com/apache/airflow/blob/9ba279d15f26088400b15281b3cc346e2d7a0e30/providers/src/airflow/providers/amazon/aws/operators/dms.py#L720-L723

Though I'm not sure if this is what you'd prefer.



After looking at the codebase, I see retry mechanisms in some hooks (ex:`BaseDatabricksHook`, `LivyHook`, `GlueJobHook`, `EmrContainerHook`...), handlers (ex: `S3TaskHandler`) and in some operators as well (ex: the Kubernetes operators as your colleague pointed out, or some AWS operators such as `BedrockCustomizeModelOperator`, `DmsDeleteReplicationConfigOperator`...). I didn't look at all the files, there must be others. 
The retry can be used to wait for a status (ex: hook `ElastiCacheReplicationGroupHook`, method `wait_for_availability`, [link](https://github.com/apache/airflow/blob/9ba279d15f26088400b15281b3cc346e2d7a0e30/providers/src/airflow/providers/amazon/aws/hooks/elasticache_replication_group.py#L120)) or to wait for an API request to succeed. In our case, I think that we can fall in these cases: We could add a retry in  `PowerBIHook.get_refresh_details_by_refresh_id` before raising the PowerBIDatasetRefreshException?

Not sure which solution is best to be honest. What do you think? 
- Should I set the retry within the operator? with a `retry_execution` method? or a `while` loop in the `execute` method? (or else?)
- Should I handle the retry in the existing `while` loop from the trigger?
- Or should I set a retry in the hook directly?

dabla on (2025-01-07 08:35:18 UTC): You're right about the Xcoms being flushed when a task fails, I like the idea/above of approach, by define a dedicated retry method which is passed as argument of the next_method of the Trigger, that could be an option.  If it's too difficult, then I would do what @ambika-garg proposed and I think you also proposed at the beginning of this issue.

Ohashiro (Issue Creator) on (2025-01-07 13:03:22 UTC): Hi @dabla 
I quickly tested what we said, so that the triggers return to `execute_complete` method, which itself checks the message and if it detects that there was an issue fetching the refresh Id, it redirects to a `retry_execution` method that counts the retries and re-executes the `execute` method. Of course the code is not very clean, but here is an overview:

```python
def execute(self, context: Context):
    """"""Refresh the Power BI Dataset.""""""
    if self.wait_for_termination:
        self.defer(
            trigger=PowerBITrigger(
                conn_id=self.conn_id,
                group_id=self.group_id,
                dataset_id=self.dataset_id,
                timeout=self.timeout,
                proxies=self.proxies,
                api_version=self.api_version,
                check_interval=self.check_interval,
                wait_for_termination=self.wait_for_termination,
            ),
            method_name=self.get_refresh_status.__name__,
        )

def get_refresh_status(self, context: Context, event: dict[str, str] | None = None):
    """"""Push the refresh Id to XCom then runs the Triggers to wait for refresh completion.""""""

    if event:
        if event[""status""] == ""error"" and ""Unable to fetch the details of dataset refresh with Request Id"" not in event[""message""] and ""not found"" not in event[""message""]:
            raise AirflowException(event[""message""])

        self.xcom_push(context=context, key=""powerbi_dataset_refresh_Id"", value=event[""dataset_refresh_id""])

    dataset_refresh_id = self.xcom_pull(context=context, key=""powerbi_dataset_refresh_Id"")
    if dataset_refresh_id:
        self.defer(
            trigger=PowerBITrigger(
                conn_id=self.conn_id,
                group_id=self.group_id,
                dataset_id=self.dataset_id,
                dataset_refresh_id=dataset_refresh_id,
                timeout=self.timeout,
                proxies=self.proxies,
                api_version=self.api_version,
                check_interval=self.check_interval,
                wait_for_termination=self.execute_complete,
            ),
            method_name=self.execute_complete.__name__,
        )

def retry_execution(self, context: Context):
    retries = self.xcom_pull(context=context, key=""retries"")
    if retries and retries >= self.max_retries:
        raise AirflowException(""Max number of retries reached!"")

    if not retries:
        retries = 0
    self.xcom_push(context=context, key=""retries"", value=retries+1)

    self.get_refresh_status(context)

def execute_complete(self, context: Context, event: dict[str, str]) -> Any:
    """"""
    Return immediately - callback for when the trigger fires.

    Relies on trigger to throw an exception, otherwise it assumes execution was successful.
    """"""
    if event:
        if event[""status""] == ""error"":
            if ""Unable to fetch the details of dataset refresh with Request Id"" in event[""message""] or ""not found"" in event[""message""]:
                self.retry_execution(context)
            else:
                raise AirflowException(event[""message""])

        self.xcom_push(context=context, key=""powerbi_dataset_refresh_status"", value=event[""status""])

```

Note: in addition to these changes, we should add a new way to handle the refresh cancellation. By default, if the trigger encounters an exception, it cancels the refresh (which is not compatible with the retry made by the operator). If we keep this solution, we have to change this behavior.

I think this solution can work but might add a little too much complexity to the operator compared to a simple retry, though I think that this separation between the trigger refresh and the status fetch is nice.
What's your opinion?

dabla on (2025-01-08 09:45:25 UTC): The separation of the 2 flows handled by the PowerBITriggerer is a good thing indeed, so the effort there is not lost and is in fact a cleaner design.  Now if it's to complex to handle the retry within the operator, then I propose you use the tenacity on that refresh_history method of the PowerBIHook.

Ohashiro (Issue Creator) on (2025-01-09 17:14:57 UTC): Hi @dabla
I opened a PR with the following changes: the separation between the 2 flows and the retry on the func `fetch_refresh_status_and_error`. 
I'd be happy to have your feedbacks on this PR when you have some time !

dabla on (2025-01-10 08:56:46 UTC): @Ohashiro Just reviewed it, looks good to me, nice job!

harryvgiunta on (2025-01-15 22:03:25 UTC): looking forward to this fix, running into same issue

"
2715380686,issue,open,,Rework `SUPERVISOR_COMMS.send_request` class/method to have `log` stored as a attribute,"              This reminds me: we should rework this class/method to have `log` stored as a attribute so we don't need to pass log in to this class.

(Future work; doing this will make it easier to use this from within XCom or Variable class.)

_Originally posted by @ashb in https://github.com/apache/airflow/pull/44590#discussion_r1867707822_
            ",kaxil,2024-12-03 15:46:17+00:00,[],2025-01-17 15:24:18+00:00,,https://github.com/apache/airflow/issues/44615,"[('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2715327440,issue,closed,completed,schedule_interval do not work the cron expression for week days with day of month interval,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When I try using the cron expression ""0 10 1-7 * 1"". Must execute the first monday at 10am for each month but it executes the first seven days of each month at 10am.

### What you think should happen instead?

Must execute the first monday at 10am for each month.

### How to reproduce

Try set the cron expression ""0 10 1-7 * 1"" in schedule_interval in a dag definition.

### Operating System

Oracle Linux Server 7.9

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==9.0.0
apache-airflow-providers-celery==3.8.3
apache-airflow-providers-cncf-kubernetes==9.0.1
apache-airflow-providers-common-compat==1.2.1
apache-airflow-providers-common-io==1.4.2
apache-airflow-providers-common-sql==1.19.0
apache-airflow-providers-docker==3.14.0
apache-airflow-providers-elasticsearch==5.5.2
apache-airflow-providers-fab==1.5.0
apache-airflow-providers-ftp==3.11.1
apache-airflow-providers-google==10.25.0
apache-airflow-providers-grpc==3.6.0
apache-airflow-providers-hashicorp==3.8.0
apache-airflow-providers-http==4.13.2
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-microsoft-azure==11.0.0
apache-airflow-providers-mysql==5.7.3
apache-airflow-providers-odbc==4.8.0
apache-airflow-providers-openlineage==1.13.0
apache-airflow-providers-postgres==5.13.1
apache-airflow-providers-redis==3.8.0
apache-airflow-providers-sendgrid==3.6.0
apache-airflow-providers-sftp==4.11.1
apache-airflow-providers-slack==8.9.1
apache-airflow-providers-smtp==1.8.0
apache-airflow-providers-snowflake==5.8.0
apache-airflow-providers-sqlite==3.9.0
apache-airflow-providers-ssh==3.14.0

### Deployment

Docker-Compose

### Deployment details

Docker Compose version v2.18.1

Docker version 24.0.2, build cb74dfc

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jenergm,2024-12-03 15:23:36+00:00,[],2025-01-10 14:47:42+00:00,2025-01-10 14:47:42+00:00,https://github.com/apache/airflow/issues/44614,"[('kind:bug', 'This is a clearly a bug'), ('pending-response', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2514871086, 'issue_id': 2715327440, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 3, 15, 23, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2514881523, 'issue_id': 2715327440, 'author': 'ES00660463', 'body': '+1', 'created_at': datetime.datetime(2024, 12, 3, 15, 27, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2514901902, 'issue_id': 2715327440, 'author': 'eladkal', 'body': '> When I try using the cron expression ""0 10 1-7 * 1"". Must execute the first monday at 10am for each month but it executes the first seven days of each month at 10am.\r\n\r\n> Must execute the first monday at 10am for each month.\r\n\r\n\r\nThis cron expression doesn\'t say what you think it say.\r\n\r\nhttps://crontab.guru/#0_10_1-7_*_1\r\n\r\n![Screenshot 2024-12-03 at 17 33 09](https://github.com/user-attachments/assets/d43288b2-346c-41d8-bc80-03498e338aa7)\r\n\r\n\r\nEither find a cron expresion that says what you wish or use [Timetable](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/timetable.html#timetables) if you need customize behavior', 'created_at': datetime.datetime(2024, 12, 3, 15, 35, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2515254561, 'issue_id': 2715327440, 'author': 'jenergm', 'body': '> > When I try using the cron expression ""0 10 1-7 * 1"". Must execute the first monday at 10am for each month but it executes the first seven days of each month at 10am.\r\n> \r\n> > Must execute the first monday at 10am for each month.\r\n> \r\n> This cron expression doesn\'t say what you think it say.\r\n> \r\n> https://crontab.guru/#0_10_1-7_*_1\r\n> \r\n> ![Screenshot 2024-12-03 at 17 33 09](https://private-user-images.githubusercontent.com/45845474/392030135-d43288b2-346c-41d8-bc80-03498e338aa7.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzMyNDc3OTEsIm5iZiI6MTczMzI0NzQ5MSwicGF0aCI6Ii80NTg0NTQ3NC8zOTIwMzAxMzUtZDQzMjg4YjItMzQ2Yy00MWQ4LWJjODAtMDM0OThlMzM4YWE3LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDEyMDMlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQxMjAzVDE3MzgxMVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTJhZjgyODhiZjZiYjlmMzNkOGRiMTJkNDdmYWE0YzA5ZjQwMjI1YzFhMGJmMTEzZjcxOGU5ZWQxN2UyODVhYzQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.K39F-f4mQNfvfDvxE5o-fYlbikRHThD91fyFYqQ_hQ0)\r\n> \r\n> Either find a cron expresion that says what you wish or use [Timetable](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/timetable.html#timetables) if you need customize behavior\r\n\r\nHi Elad,\r\n\r\nWhat I need is it must run once a month in the first monday of month at 10AM.\r\n\r\nThe problem is that cron must be right, but it seams doing an OR condition instead of AND. It\'s running on other week days.', 'created_at': datetime.datetime(2024, 12, 3, 18, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2515802625, 'issue_id': 2715327440, 'author': 'Pavan-talluri', 'body': 'Hi @jenergm,\r\n\r\nWe are also dealing with this  and we found that its just that Unix cron functions this way.  it is not programming and operator its just plain english and\r\n\r\nairflow scheduler uses default \r\n**CronDataIntervalTimetable**\r\n[https://github.com/apache/airflow/blob/4ffa6afc721ab99a974bb8bbda74b1d9300ae456/airflow/timetables/interval.py#L119]\r\n\r\n for processing crons which underlying uses croniter library for processing crons. You can this as reference and write timetable class', 'created_at': datetime.datetime(2024, 12, 3, 23, 32, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2578930290, 'issue_id': 2715327440, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2025, 1, 9, 0, 15, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2578965790, 'issue_id': 2715327440, 'author': 'jenergm', 'body': '> Hi @jenergm,\r\n> \r\n> We are also dealing with this and we found that its just that Unix cron functions this way. it is not programming and operator its just plain english and\r\n> \r\n> airflow scheduler uses default **CronDataIntervalTimetable** [https://github.com/apache/airflow/blob/4ffa6afc721ab99a974bb8bbda74b1d9300ae456/airflow/timetables/interval.py#L119]\r\n> \r\n> for processing crons which underlying uses croniter library for processing crons. You can this as reference and write timetable class\r\n\r\nHi @Pavan-talluri,\r\n\r\nHave you some example creating and using timeline classes?\r\n\r\nBetter if it works just using the correct cron expression.\r\n\r\nThanks in advance!', 'created_at': datetime.datetime(2025, 1, 9, 0, 47, 29, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-03 15:23:39 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

ES00660463 on (2024-12-03 15:27:39 UTC): +1

eladkal on (2024-12-03 15:35:47 UTC): This cron expression doesn't say what you think it say.

https://crontab.guru/#0_10_1-7_*_1

![Screenshot 2024-12-03 at 17 33 09](https://github.com/user-attachments/assets/d43288b2-346c-41d8-bc80-03498e338aa7)


Either find a cron expresion that says what you wish or use [Timetable](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/timetable.html#timetables) if you need customize behavior

jenergm (Issue Creator) on (2024-12-03 18:06:00 UTC): Hi Elad,

What I need is it must run once a month in the first monday of month at 10AM.

The problem is that cron must be right, but it seams doing an OR condition instead of AND. It's running on other week days.

Pavan-talluri on (2024-12-03 23:32:12 UTC): Hi @jenergm,

We are also dealing with this  and we found that its just that Unix cron functions this way.  it is not programming and operator its just plain english and

airflow scheduler uses default 
**CronDataIntervalTimetable**
[https://github.com/apache/airflow/blob/4ffa6afc721ab99a974bb8bbda74b1d9300ae456/airflow/timetables/interval.py#L119]

 for processing crons which underlying uses croniter library for processing crons. You can this as reference and write timetable class

github-actions[bot] on (2025-01-09 00:15:31 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

jenergm (Issue Creator) on (2025-01-09 00:47:29 UTC): Hi @Pavan-talluri,

Have you some example creating and using timeline classes?

Better if it works just using the correct cron expression.

Thanks in advance!

"
2715310441,issue,closed,completed,PowerBIDatasetRefreshOperator task succeeds whereas dataset refresh fails,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.2

### What happened?

We use the operator `PowerBIDatasetRefreshOperator` to refresh our PowerBI datasets. For one of the datasets, the refresh fails (and the refresh status received by the task is ""Failed"") but the task is marked as SUCCESS.

<img width=""998"" alt=""image"" src=""https://github.com/user-attachments/assets/888d3068-5ecd-433d-93ca-ecdb9b108b81"">


### What you think should happen instead?

The Microsoft API documentation [here](https://learn.microsoft.com/en-us/rest/api/power-bi/datasets/get-refresh-history#refresh) lists the following status: Unknown, Completed, Failed, Disabled. I think that the task should fail if the refresh status is not ""Completed"".

In the code [here](https://github.com/apache/airflow/blob/40821bfd5c54f3a39b3ff6e8352a4e3a20323e24/providers/src/airflow/providers/microsoft/azure/operators/powerbi.py#L129), the operator checks only if there is an error in the request to get the status.

### How to reproduce

This bug appears when a Dataset can't be refreshed due to an error in the Dataset. Example: duplicate value in a column supposed to be ""Unique""

### Operating System

linux

### Versions of Apache Airflow Providers

apache-airflow-providers-microsoft-azure==11.1.0

### Deployment

Google Cloud Composer

### Deployment details

_No response_

### Anything else?

This bug occurs every time we run the operator.

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Ohashiro,2024-12-03 15:17:19+00:00,['Ohashiro'],2024-12-19 20:45:42+00:00,2024-12-19 20:45:42+00:00,https://github.com/apache/airflow/issues/44613,"[('kind:bug', 'This is a clearly a bug'), ('provider:microsoft-azure', 'Azure-related issues'), ('area:providers', ''), ('good first issue', ''), ('area:core', '')]","[{'comment_id': 2514854950, 'issue_id': 2715310441, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 3, 15, 17, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2514944944, 'issue_id': 2715310441, 'author': 'eladkal', 'body': 'Feel free to raise PR.\r\nI recommend having the statuses defined in the hook. Similar to:\r\n\r\nhttps://github.com/apache/airflow/blob/1275fec92fd7cd7135b100d66d41bdcb79ade29d/providers/src/airflow/providers/amazon/aws/hooks/athena.py#L69-L82', 'created_at': datetime.datetime(2024, 12, 3, 15, 52, 54, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-03 15:17:22 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2024-12-03 15:52:54 UTC): Feel free to raise PR.
I recommend having the statuses defined in the hook. Similar to:

https://github.com/apache/airflow/blob/1275fec92fd7cd7135b100d66d41bdcb79ade29d/providers/src/airflow/providers/amazon/aws/hooks/athena.py#L69-L82

"
2715119229,issue,closed,completed,AIP-84 Rework Asset API URLs to solve conflicting routes,"The way that the Asset API was implemented (flask) and then migrated over to FastAPI contains conflicting routes:
- GET `assets/events`
- GET /assets/queuedEvents/{uri:path}
- GET `/assets/{uri:path}`
- Maybe more

**First approach** to solve this would most probably be to swap the use of `uri` as an identifier for the backend to instead use the `id` (autoincremented int) therefore resolving the possible conflicts.

**Another Approach 1**: de-nest endpoints. `GET queuedEvents/{uri:path}`, `GET `/events`

**Another Approach 2**: forbid certain keywords for asset uri creation, i.e `events`, `queuedEvents`, etc.. that are internally used by airflow.


### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-12-03 14:02:40+00:00,['Prab-27'],2024-12-11 17:41:39+00:00,2024-12-11 12:51:37+00:00,https://github.com/apache/airflow/issues/44609,"[('area:API', ""Airflow's REST/HTTP API""), ('good first issue', ''), ('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2514799593, 'issue_id': 2715119229, 'author': 'Prab-27', 'body': ""Hello @pierrejeambrun,I am a newbie and I'd like to work on this issue"", 'created_at': datetime.datetime(2024, 12, 3, 14, 55, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2514857696, 'issue_id': 2715119229, 'author': 'pierrejeambrun', 'body': 'Hello @Prab-27,\r\n\r\nGreat! You are assigned. You can find the file for the `Asset` API in `airflow/api_fastapi/core_api/routes/public/assets.py`', 'created_at': datetime.datetime(2024, 12, 3, 15, 18, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2535887040, 'issue_id': 2715119229, 'author': 'Prab-27', 'body': '@pierrejeambrun , There is one PR merged #44801 that changed uri: str to id: int.\r\n\r\nWould you please tell me if I still need to de-nest the endpoints, or if this has already been resolved', 'created_at': datetime.datetime(2024, 12, 11, 12, 41, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2535907934, 'issue_id': 2715119229, 'author': 'pierrejeambrun', 'body': 'Thanks @Prab-27, indeed this is solved by https://github.com/apache/airflow/pull/44801. \r\n\r\nClosing.\r\n\r\nAdopted solution was:\r\n> First approach to solve this would most probably be to swap the use of uri as an identifier for the backend to instead use the id (autoincremented int) therefore resolving the possible conflicts.', 'created_at': datetime.datetime(2024, 12, 11, 12, 51, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2536016064, 'issue_id': 2715119229, 'author': 'Prab-27', 'body': '@pierrejeambrun , Could you please check if `get_asset_events` and ` create_asset_event` have the same path `/assets/events`,  and if they conflict?', 'created_at': datetime.datetime(2024, 12, 11, 13, 37, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2536677327, 'issue_id': 2715119229, 'author': 'pierrejeambrun', 'body': '> @pierrejeambrun , Could you please check if `get_asset_events` and ` create_asset_event` have the same path `/assets/events`, and if they conflict?\r\n\r\nThe method is not the same ‘GET’ vs ‘POST’. That shouldn’t be a problem.', 'created_at': datetime.datetime(2024, 12, 11, 17, 41, 38, tzinfo=datetime.timezone.utc)}]","Prab-27 (Assginee) on (2024-12-03 14:55:39 UTC): Hello @pierrejeambrun,I am a newbie and I'd like to work on this issue

pierrejeambrun (Issue Creator) on (2024-12-03 15:18:27 UTC): Hello @Prab-27,

Great! You are assigned. You can find the file for the `Asset` API in `airflow/api_fastapi/core_api/routes/public/assets.py`

Prab-27 (Assginee) on (2024-12-11 12:41:49 UTC): @pierrejeambrun , There is one PR merged #44801 that changed uri: str to id: int.

Would you please tell me if I still need to de-nest the endpoints, or if this has already been resolved

pierrejeambrun (Issue Creator) on (2024-12-11 12:51:37 UTC): Thanks @Prab-27, indeed this is solved by https://github.com/apache/airflow/pull/44801. 

Closing.

Adopted solution was:

Prab-27 (Assginee) on (2024-12-11 13:37:45 UTC): @pierrejeambrun , Could you please check if `get_asset_events` and ` create_asset_event` have the same path `/assets/events`,  and if they conflict?

pierrejeambrun (Issue Creator) on (2024-12-11 17:41:38 UTC): The method is not the same ‘GET’ vs ‘POST’. That shouldn’t be a problem.

"
2715020441,issue,closed,completed,Add support to filter by dag_id in recent_dag_runs endpoint,"              Also this just makes me realize that `http://localhost:29091/ui/dags/recent_dag_runs?dag_id_pattern=xxxxx` endpoint should most certainly also support a `dag_id` query params or `dag_ids`.

I am affraid that using a `pattern` like this could end up matching multiple dags while we are really trying to get the dag_runs from a specific dag like here.

_Originally posted by @pierrejeambrun in https://github.com/apache/airflow/issues/44074#issuecomment-2482613859_
            ",tirkarthi,2024-12-03 13:21:12+00:00,[],2024-12-11 11:06:49+00:00,2024-12-11 11:06:49+00:00,https://github.com/apache/airflow/issues/44606,"[('area:webserver', 'Webserver related Issues'), ('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('good first issue', ''), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2514563829, 'issue_id': 2715020441, 'author': 'tirkarthi', 'body': ""Opened this issue for tracking as it got lost in the PR review. It seems there is `QueryDagIdsFilter` in `get_dag_stats` but it's a GET endpoint and multiple `dag_ids` query parameter can be passed and there is no standard for this.\r\n\r\nhttps://stackoverflow.com/questions/24059773/correct-way-to-pass-multiple-values-for-same-parameter-name-in-get-request\r\n\r\nhttps://github.com/apache/airflow/blob/edf3e3376811129a9fad278ae396b3b5644c69d8/airflow/api_fastapi/core_api/routes/public/dag_stats.py#L40-L52"", 'created_at': datetime.datetime(2024, 12, 3, 13, 30, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2514580657, 'issue_id': 2715020441, 'author': 'pierrejeambrun', 'body': 'Nice thanks', 'created_at': datetime.datetime(2024, 12, 3, 13, 37, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2518993256, 'issue_id': 2715020441, 'author': 'jason810496', 'body': ""Hi @tirkarthi , I think this issue is resolve after https://github.com/apache/airflow/pull/44345 is merged:\r\n\r\nhttps://github.com/apache/airflow/blob/main/airflow/api_fastapi/core_api/routes/public/dag_stats.py#L56-L61\r\n\r\nAfter the refactor, we can leverage `FilterParam` and `filter_param_factory` for query parameters with simple logic. By passing `list[str]` to `_type`, we can standardize the format of request query parameters using FastAPI's native features."", 'created_at': datetime.datetime(2024, 12, 5, 3, 9, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2519858054, 'issue_id': 2715020441, 'author': 'pierrejeambrun', 'body': ""Indeed, now with the refactoring we have tools to achieve that. We still need to add this capability though, so we can close it. \r\n\r\nIt's quite straightforward with the new parameter system."", 'created_at': datetime.datetime(2024, 12, 5, 10, 15, 26, tzinfo=datetime.timezone.utc)}]","tirkarthi (Issue Creator) on (2024-12-03 13:30:22 UTC): Opened this issue for tracking as it got lost in the PR review. It seems there is `QueryDagIdsFilter` in `get_dag_stats` but it's a GET endpoint and multiple `dag_ids` query parameter can be passed and there is no standard for this.

https://stackoverflow.com/questions/24059773/correct-way-to-pass-multiple-values-for-same-parameter-name-in-get-request

https://github.com/apache/airflow/blob/edf3e3376811129a9fad278ae396b3b5644c69d8/airflow/api_fastapi/core_api/routes/public/dag_stats.py#L40-L52

pierrejeambrun on (2024-12-03 13:37:37 UTC): Nice thanks

jason810496 on (2024-12-05 03:09:27 UTC): Hi @tirkarthi , I think this issue is resolve after https://github.com/apache/airflow/pull/44345 is merged:

https://github.com/apache/airflow/blob/main/airflow/api_fastapi/core_api/routes/public/dag_stats.py#L56-L61

After the refactor, we can leverage `FilterParam` and `filter_param_factory` for query parameters with simple logic. By passing `list[str]` to `_type`, we can standardize the format of request query parameters using FastAPI's native features.

pierrejeambrun on (2024-12-05 10:15:26 UTC): Indeed, now with the refactoring we have tools to achieve that. We still need to add this capability though, so we can close it. 

It's quite straightforward with the new parameter system.

"
2714630238,issue,closed,completed,Respect asset name attribute when accessing inlet and outlet events,"### Description

AIP-74 introduces the name attribute to the asset, while the outlet_events and inlet_events only respect the URI attribute. This needs to be fixed 

* additional todo
    * [x] Mark `AssetAliasEvent` as private and move it to `airflow/utils/context.py`

### Use case/motivation

_No response_

### Related issues

https://github.com/apache/airflow/issues/43956

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-12-03 10:35:39+00:00,['Lee-W'],2024-12-11 08:19:19+00:00,2024-12-11 08:19:19+00:00,https://github.com/apache/airflow/issues/44601,"[('kind:feature', 'Feature Requests'), ('area:datasets', 'Issues related to the datasets feature'), ('AIP-74', 'Dataset -> Asset')]","[{'comment_id': 2516821879, 'issue_id': 2714630238, 'author': 'Lee-W', 'body': 'https://github.com/apache/airflow/pull/44639', 'created_at': datetime.datetime(2024, 12, 4, 10, 11, 58, tzinfo=datetime.timezone.utc)}]","Lee-W (Issue Creator) on (2024-12-04 10:11:58 UTC): https://github.com/apache/airflow/pull/44639

"
2714620145,issue,closed,completed,Fail a task if the outlet is set to an inactive asset,"### Description

A task references an inactive asset should raise an error

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-12-03 10:31:22+00:00,['Lee-W'],2024-12-25 07:52:32+00:00,2024-12-25 07:52:32+00:00,https://github.com/apache/airflow/issues/44600,"[('kind:feature', 'Feature Requests'), ('area:datasets', 'Issues related to the datasets feature'), ('AIP-74', 'Dataset -> Asset')]",[],
2713891317,issue,closed,completed,Callbacks issue,"### What do you see as an issue?

There's are some new parameter for DAG callback setting. E.g. `on_skipped_callback` added since version 2.9, but still can not use this parameter and not found in this repo.
https://airflow.apache.org/docs/apache-airflow/2.10.2/administration-and-deployment/logging-monitoring/callbacks.html

### Solving the problem

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",SWeiCHEN,2024-12-03 03:23:44+00:00,[],2024-12-03 07:05:38+00:00,2024-12-03 07:05:38+00:00,https://github.com/apache/airflow/issues/44592,"[('kind:bug', 'This is a clearly a bug'), ('kind:documentation', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2513467056, 'issue_id': 2713891317, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 3, 3, 23, 46, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-03 03:23:46 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2712330831,issue,closed,completed,apacheairflow[.]gateway[.]scarf[.]sh,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.2

### What happened?

Y'all need to rethink having this callout to apacheairflow[.gateway[.]scarf[.]sh from the Dashboard page.  This network call is not allowed in all corporate environments, and pretty much ruins the airflow UI performance.  Why are you collecting all this meta data?

### What you think should happen instead?

You shouldn't be making that network call.  At a minimum, we need to be able to disable it.

### How to reproduce

Operate airflow behind a corporate proxy that blocks all callouts to .sh TLD

### Operating System

linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",carnak,2024-12-02 15:30:32+00:00,[],2024-12-02 15:32:55+00:00,2024-12-02 15:32:55+00:00,https://github.com/apache/airflow/issues/44571,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('telemetry', 'Telemetry-related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2511855919, 'issue_id': 2712330831, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 2, 15, 30, 35, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-02 15:30:35 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2712292716,issue,closed,completed,Give the possibility to define a display name for task_group,"### Description

We currently have the possibility to define a ""display name"" for dags and tasks but not for task_group. When you define tasks/dag display name when having task_group, it's sad that you can't keep same naming convention for task_group

### Use case/motivation

Having a new task_group_display_name property that works like task_display_name (=replace the group_id in UI when defined)

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",darkag,2024-12-02 15:16:36+00:00,['hardeybisey'],2025-01-04 17:14:24+00:00,2025-01-04 16:07:14+00:00,https://github.com/apache/airflow/issues/44569,"[('kind:feature', 'Feature Requests'), ('good first issue', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2513243782, 'issue_id': 2712292716, 'author': 'potiuk', 'body': 'Good idea.', 'created_at': datetime.datetime(2024, 12, 3, 0, 16, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2547038834, 'issue_id': 2712292716, 'author': 'hardeybisey', 'body': 'I would like to work on this. Can it be assigned to me? Thanks.', 'created_at': datetime.datetime(2024, 12, 16, 22, 51, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2549334680, 'issue_id': 2712292716, 'author': 'potiuk', 'body': 'Did', 'created_at': datetime.datetime(2024, 12, 17, 18, 53, 16, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-12-03 00:16:07 UTC): Good idea.

hardeybisey (Assginee) on (2024-12-16 22:51:53 UTC): I would like to work on this. Can it be assigned to me? Thanks.

potiuk on (2024-12-17 18:53:16 UTC): Did

"
2710911014,issue,closed,completed,Remove deprecated code from providers before Airflow 2.11.0 release,"### Body

Following [[LAZY CONSENSUS] Remove deprecations from providers prior to Airflow 2.11 release](https://lists.apache.org/thread/lhy7zhz8yxo3jjpln0ds8ogszgb9b469) this issue will track the progress of the removal status.

**How to remove?**
1. PRs should be easy to review, You can handle more than 1 provider in the same PR but please use judgment about how many files the PR changes. Please use meaningful commit message.
2. Find all deprecated code in a provider and accommodate for the needed change (normally the code has deprecation warning that indicates what needs to be removed). Make sure you are not removing code that was just deprecated in recent PR and was never released in stable version. Do not bulk remove!
3. Add entry to CHANGELOG.rst with information about what was removed and how to mitigate (normally it's copy/paste with minor adjustments to the deprecation warning). Use the following blockright after the `Changelog` line:

```
main
....

.. warning::
  All deprecated classes, parameters and features have been removed from the {provider_name} provider package.
  The following breaking changes were introduced:

  * what was removed and how to mitigate

```

4. If provider was already handled and introduce new deprecation do not remove again (at least not for now).

List of providers:

- [x] airbyte https://github.com/apache/airflow/pull/44577
- [x] alibaba https://github.com/apache/airflow/pull/44576
- [x] amazon https://github.com/apache/airflow/pull/42450
- [x] apache.beam #44700
- [x] apache.cassandra no deprecations
- [x] apache.drill https://github.com/apache/airflow/pull/44575
- [x] apache.druid https://github.com/apache/airflow/pull/44765
- [x] apache.flink. no deprecations
- [x] apache.hdfs no deprecations
- [x] apache.hive #44715
- [x] apache.iceberg no deprecations
- [x] apache.impala no deprecations
- [x] apache.kafka no deprecations
- [x] apache.kylin no deprecations
- [x] apache.livy #44631
- [x] apache.pig no deprecations
- [x] apache.pinot no deprecations
- [x] apache.spark #44567
- [x] apprise #44764
- [x] arangodb *No deprecations*
- [x] asana *No deprecations*
- [x] atlassian.jira #44644
- [x] celery *Assume deprecations do not affect user and will stay*
- [x] cloudant *No deprecations*
- [x] cncf.kubernetes https://github.com/apache/airflow/pull/43689
- [x] cohere *No deprecations*
- [x] common.compat *No deprecations*
- [x] common.io *No deprecations*
- [x] common.sql #44645
- [x] databricks https://github.com/apache/airflow/pull/44566
- [x] datadog *No deprecations*
- [x] dbt #44638
- [x] dingding *No deprecations*
- [x] discord *No deprecations*
- [x] docker #44583
- [x] edge *New provider, this is sooo fresh and cool it does not have deprecations (yet!)*
- [x] elasticsearch https://github.com/apache/airflow/pull/44629
- [x] exasol *No deprecations*
- [x] fab https://github.com/apache/airflow/pull/44198
- [x] facebook *No deprecations*
- [x] ftp *No deprecations*
- [x] github *No deprecations*
- [x] google **excluded from policy**
- [x] grpc *No deprecations*
- [x] hashicorp https://github.com/apache/airflow/pull/44598
- [x] http #44542
- [x] imap *No deprecations*
- [x] influxdb *No deprecations*
- [x] jdbc #44662
- [x] jenkins *No deprecations*
- [x] microsoft.azure https://github.com/apache/airflow/pull/44763
- [x] microsoft.mssql #44762
- [x] microsoft.psrp #44761
- [x] microsoft.winrm *No deprecations*
- [x] mongo *No deprecations*
- [x] mysql #44665
- [x] neo4j *No deprecations*
- [x] odbc *No deprecations*
- [x] openai *No deprecations*
- [x] openfaas *No deprecations*
- [x] openlineage #44636
- [x] opensearch *No deprecations*
- [x] opsgenie *No deprecations*
- [x] oracle #44704
- [x] pagerduty https://github.com/apache/airflow/pull/44653
- [x] papermill *No deprecations*
- [x] pgvector *No deprecations*
- [x] pinecone *No deprecations*
- [x] postgres #44705
- [x] presto *No deprecations*
- [x] qdrant *No deprecations*
- [x] redis  #44633
- [x] salesforce *No deprecations*
- [x] samba *No deprecations*
- [x] segment *No deprecations*
- [x] sendgrid #44637
- [x] sftp https://github.com/apache/airflow/pull/44740
- [x] singularity *No deprecations*
- [x] slack #44693
- [x] smtp *No deprecations* 
- [x] snowflake  #44756
- [x] sqlite #44707
- [x] ssh #44544
- [x] standard https://github.com/apache/airflow/pull/44541
- [x] tableau https://github.com/apache/airflow/pull/44757
- [x] telegram *No deprecations*
- [x] teradata https://github.com/apache/airflow/pull/44746
- [x] trino https://github.com/apache/airflow/pull/44717
- [x] vertica #44748
- [x] weaviate https://github.com/apache/airflow/pull/44745
- [x] yandex https://github.com/apache/airflow/pull/44754
- [x] ydb *No deprecations*
- [x] zendesk *No deprecations*


**Maintainers please update the list with the relevant PRs/status**


### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2024-12-02 07:43:49+00:00,[],2024-12-16 18:14:41+00:00,2024-12-16 18:14:40+00:00,https://github.com/apache/airflow/issues/44559,"[('area:providers', ''), ('good first issue', ''), ('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2511618086, 'issue_id': 2710911014, 'author': 'Prab-27', 'body': ""Hello @potiuk I'm a newbie \r\nI would like to confirm whether I should use the `v2-10-test` branch for this issue since it is related to version 2"", 'created_at': datetime.datetime(2024, 12, 2, 13, 58, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2511678268, 'issue_id': 2710911014, 'author': 'kunaljubce', 'body': ""@Prab-27 If you're talking about which branch to raise the PR on, that should be `main`. Refer to the tagged PRs for inspiration."", 'created_at': datetime.datetime(2024, 12, 2, 14, 22, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2512213254, 'issue_id': 2710911014, 'author': 'kunaljubce', 'body': '@eladkal So in the context of this change, are we good to deprecate the Apache Drill provider completely? https://github.com/apache/airflow/blob/3747c91afdcd0470ca29e911c589b334b357b778/providers/src/airflow/providers/apache/drill/operators/drill.py#L33', 'created_at': datetime.datetime(2024, 12, 2, 17, 20, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2512251827, 'issue_id': 2710911014, 'author': 'ajitg25', 'body': 'Hello @eladkal @jedcunningham ,\r\nI have raised a [PR](https://github.com/apache/airflow/pull/44577) to remove deprecated code from Airbyte providers.\r\nPlease have a look and let me know if anychanges are required.\r\n\r\nThank you for your time!!', 'created_at': datetime.datetime(2024, 12, 2, 17, 37, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2512265819, 'issue_id': 2710911014, 'author': 'Prab-27', 'body': '@eladkal I’m working on the hashicorp provider and will soon raise a PR.', 'created_at': datetime.datetime(2024, 12, 2, 17, 44, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2512841629, 'issue_id': 2710911014, 'author': 'jscheffl', 'body': ""@eladkal I just checked for the noted deprecations in Celery - these are purely for executor interface, not DAG author specific - and the change was just added and require Airflow 2.10.4 as minimum. If we add a breaking change, Celery can not be upgraded for Airflow 2.10.3 and below.\r\nTherefore I'd recommend to keep the Celery deprecations for a moment. No harm for the intend to help users to migrate.\r\n\r\n--> FYI marked as to keep it. Hope it is OK?"", 'created_at': datetime.datetime(2024, 12, 2, 21, 6, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2513491904, 'issue_id': 2710911014, 'author': 'Prab-27', 'body': '@jscheffl - How can I determine the version number like 5.0.0 in CHANGELOG.rst? \r\n\r\nI believe the version number in CHANGELOG.rst is derived from the next versions mentioned in provider.yaml .', 'created_at': datetime.datetime(2024, 12, 3, 3, 53, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2513590038, 'issue_id': 2710911014, 'author': 'eladkal', 'body': ""> @jscheffl - How can I determine the version number like 5.0.0 in CHANGELOG.rst?\r\n> \r\n> I believe the version number in CHANGELOG.rst is derived from the next versions mentioned in provider.yaml .\r\n\r\nYou don't, see the instructions in step 3. You set `main` not version. The version number is chosen by release manager during release time."", 'created_at': datetime.datetime(2024, 12, 3, 5, 27, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2515989333, 'issue_id': 2710911014, 'author': 'jason810496', 'body': 'Working on \r\n- Elasticsearch\r\n- Jekins\r\n- Apache Livy\r\n- Mongo\r\n- Redis\r\n- SendGrid\r\n- DBT\r\n- Atlassian Jira\r\n- Common SQL', 'created_at': datetime.datetime(2024, 12, 4, 1, 53, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2525198671, 'issue_id': 2710911014, 'author': 'kunaljubce', 'body': '@eladkal @jscheffl Below have no deprecations. Please update the description -\r\n* apache.hive\r\n* apache.flink\r\n* apache.iceberg\r\n* apache.impala\r\n* apache.kafka\r\n* apache.kylin', 'created_at': datetime.datetime(2024, 12, 7, 15, 4, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2525199624, 'issue_id': 2710911014, 'author': 'potiuk', 'body': 'Updatiing', 'created_at': datetime.datetime(2024, 12, 7, 15, 8, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2525200066, 'issue_id': 2710911014, 'author': 'jscheffl', 'body': '> Updatiing\r\n\r\nUpdating descriptions in Github is a pain. I did it already ~10 times and it seems if others update as well then sometimes (at least 10 times already) my changes are lost. Also notes down a few multiple times which have no deprecations. Tracking a set of PRs on a bug ticket is something Github is not made for :-(', 'created_at': datetime.datetime(2024, 12, 7, 15, 9, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2525200240, 'issue_id': 2710911014, 'author': 'kunaljubce', 'body': 'FYI - Working on Apache Drill and Apache Druid deprecations.', 'created_at': datetime.datetime(2024, 12, 7, 15, 10, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2525200519, 'issue_id': 2710911014, 'author': 'potiuk', 'body': '> Updating descriptions in Github is a pain. I did it already ~10 times and it seems if others update as well then sometimes (at least 10 times already) my changes are lost. Also notes down a few multiple times which have no deprecations. Tracking a set of PRs on a bug ticket is something Github is not made for :-(\r\n\r\nWhat I did it in the past is re-generating those issues automatically from time to time ...  Another thing-> I did not link/update the PRs in the list just ""clicked"" the checkmark - it was good enough usually.', 'created_at': datetime.datetime(2024, 12, 7, 15, 11, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2525201299, 'issue_id': 2710911014, 'author': 'potiuk', 'body': 'But maybe a good idea is to do (or find?) a small tool that should do such update via API?\r\n\r\nsay:\r\n\r\n```\r\nbreeze check-issue #12345 --item abc --with-pr #27456\r\n``` \r\n\r\n😱 \r\n\r\nThat would make it far more usable', 'created_at': datetime.datetime(2024, 12, 7, 15, 14, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2525204298, 'issue_id': 2710911014, 'author': 'kunaljubce', 'body': ""@potiuk @jscheffl While we're at it, below also don't have any deprecations - \r\n\r\n* apache.pig\r\n* apache.pinot\r\n* apache.cassandra\r\n\r\nWith the `apache.druid` PR up, looks like we're done with the Apache series of deprecations :)"", 'created_at': datetime.datetime(2024, 12, 7, 15, 20, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2525307140, 'issue_id': 2710911014, 'author': 'potiuk', 'body': '<img width=""267"" alt=""Screenshot 2024-12-07 at 21 51 20"" src=""https://github.com/user-attachments/assets/3d32bfa9-51a3-4270-82bc-609b86210045"">\r\n\r\n🚀', 'created_at': datetime.datetime(2024, 12, 7, 20, 51, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2546313975, 'issue_id': 2710911014, 'author': 'potiuk', 'body': 'Is it done ?????!   ??? 🚀', 'created_at': datetime.datetime(2024, 12, 16, 18, 10, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2546314879, 'issue_id': 2710911014, 'author': 'potiuk', 'body': 'Shall you do the honors @eladkal and close it ?', 'created_at': datetime.datetime(2024, 12, 16, 18, 11, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2546321456, 'issue_id': 2710911014, 'author': 'eladkal', 'body': 'Created a small followup https://github.com/apache/airflow/issues/44807 to make docs better :)', 'created_at': datetime.datetime(2024, 12, 16, 18, 14, 40, tzinfo=datetime.timezone.utc)}]","Prab-27 on (2024-12-02 13:58:09 UTC): Hello @potiuk I'm a newbie 
I would like to confirm whether I should use the `v2-10-test` branch for this issue since it is related to version 2

kunaljubce on (2024-12-02 14:22:49 UTC): @Prab-27 If you're talking about which branch to raise the PR on, that should be `main`. Refer to the tagged PRs for inspiration.

kunaljubce on (2024-12-02 17:20:10 UTC): @eladkal So in the context of this change, are we good to deprecate the Apache Drill provider completely? https://github.com/apache/airflow/blob/3747c91afdcd0470ca29e911c589b334b357b778/providers/src/airflow/providers/apache/drill/operators/drill.py#L33

ajitg25 on (2024-12-02 17:37:21 UTC): Hello @eladkal @jedcunningham ,
I have raised a [PR](https://github.com/apache/airflow/pull/44577) to remove deprecated code from Airbyte providers.
Please have a look and let me know if anychanges are required.

Thank you for your time!!

Prab-27 on (2024-12-02 17:44:28 UTC): @eladkal I’m working on the hashicorp provider and will soon raise a PR.

jscheffl on (2024-12-02 21:06:51 UTC): @eladkal I just checked for the noted deprecations in Celery - these are purely for executor interface, not DAG author specific - and the change was just added and require Airflow 2.10.4 as minimum. If we add a breaking change, Celery can not be upgraded for Airflow 2.10.3 and below.
Therefore I'd recommend to keep the Celery deprecations for a moment. No harm for the intend to help users to migrate.

--> FYI marked as to keep it. Hope it is OK?

Prab-27 on (2024-12-03 03:53:33 UTC): @jscheffl - How can I determine the version number like 5.0.0 in CHANGELOG.rst? 

I believe the version number in CHANGELOG.rst is derived from the next versions mentioned in provider.yaml .

eladkal (Issue Creator) on (2024-12-03 05:27:33 UTC): You don't, see the instructions in step 3. You set `main` not version. The version number is chosen by release manager during release time.

jason810496 on (2024-12-04 01:53:30 UTC): Working on 
- Elasticsearch
- Jekins
- Apache Livy
- Mongo
- Redis
- SendGrid
- DBT
- Atlassian Jira
- Common SQL

kunaljubce on (2024-12-07 15:04:24 UTC): @eladkal @jscheffl Below have no deprecations. Please update the description -
* apache.hive
* apache.flink
* apache.iceberg
* apache.impala
* apache.kafka
* apache.kylin

potiuk on (2024-12-07 15:08:03 UTC): Updatiing

jscheffl on (2024-12-07 15:09:45 UTC): Updating descriptions in Github is a pain. I did it already ~10 times and it seems if others update as well then sometimes (at least 10 times already) my changes are lost. Also notes down a few multiple times which have no deprecations. Tracking a set of PRs on a bug ticket is something Github is not made for :-(

kunaljubce on (2024-12-07 15:10:26 UTC): FYI - Working on Apache Drill and Apache Druid deprecations.

potiuk on (2024-12-07 15:11:32 UTC): What I did it in the past is re-generating those issues automatically from time to time ...  Another thing-> I did not link/update the PRs in the list just ""clicked"" the checkmark - it was good enough usually.

potiuk on (2024-12-07 15:14:15 UTC): But maybe a good idea is to do (or find?) a small tool that should do such update via API?

say:

```
breeze check-issue #12345 --item abc --with-pr #27456
``` 

😱 

That would make it far more usable

kunaljubce on (2024-12-07 15:20:33 UTC): @potiuk @jscheffl While we're at it, below also don't have any deprecations - 

* apache.pig
* apache.pinot
* apache.cassandra

With the `apache.druid` PR up, looks like we're done with the Apache series of deprecations :)

potiuk on (2024-12-07 20:51:53 UTC): <img width=""267"" alt=""Screenshot 2024-12-07 at 21 51 20"" src=""https://github.com/user-attachments/assets/3d32bfa9-51a3-4270-82bc-609b86210045"">

🚀

potiuk on (2024-12-16 18:10:40 UTC): Is it done ?????!   ??? 🚀

potiuk on (2024-12-16 18:11:09 UTC): Shall you do the honors @eladkal and close it ?

eladkal (Issue Creator) on (2024-12-16 18:14:40 UTC): Created a small followup https://github.com/apache/airflow/issues/44807 to make docs better :)

"
2710686484,issue,open,, Airflow 2 to 3 auto migration rules - ruff (~44080),"### Description

Parent issue: #41641

## Ruff

### AIR302

#### context key
* [ ] `triggering_dataset_events` → `triggering_asset_events` (from #41348)

#### ~~resource key (not sure whether we can do it through ruff, probably not) (@Lee-W)~~
* ~~[ ] `dataset-uris` → `asset-uris` (for providers amazon, common.io, mysql, fab, postgres, trino)  (from #41348)~~ Just checked the 2.10 code. Don't think we can do anything

### AIR310: models related changes (AIP-72) not going to do it
<details>
    <summary>Rules to blocked by AIP-72 </summary>

* [ ] #40931
* [ ] #41440
* [ ] #41761
    * [ ] `airflow.models.baseoperator.BaseOperatorLink` → `airflow.models.baseoperatorlink.BaseOperatorLink`
* [ ] #41762
    * [ ] `airflow.models.connection.parse_netloc_to_hostname`
    * [ ] `airflow.models.connection.Connection.parse_from_uri`
    * [ ] `airflow.models.connection.Connection.log_info`
    * [ ] `airflow.models.connection.Connection.debug_info`
* [ ] #41774
* [ ] #41776
* [ ] #41778
* [ ] #41779
* [ ] #41780
* [ ] #41784
* [ ] #41808
* [ ] #41964
* [ ] #42023
* [ ] #42548
* [ ] #42739
* [ ] #42776
* [ ] #43067
* [ ] #43490
* [ ] `airflow.models.ImportError` → `airflow.models.errors.ParseImportError` (from #41367)
* [ ] `airflow.models.taskMixin.TaskMixin` → `airflow.models.taskMixin.DependencyMixin` (from #41394)

</details>

<details>
    <summary><h2>In review rules</h2></summary>

### AIR302

#### context key (@sunank200)
* [ ] `execution_date` (from #43902 https://github.com/apache/airflow/issues/44409)
* [ ] `next_ds` (from #43902 https://github.com/apache/airflow/issues/44409)
* [ ] `next_ds_nodash` (from #43902 https://github.com/apache/airflow/issues/44409)
* [ ] `next_execution_date` (from #43902 https://github.com/apache/airflow/issues/44409)
* [ ] `prev_ds` (from #43902 https://github.com/apache/airflow/issues/44409)
* [ ] `prev_ds_nodash` (from #43902 https://github.com/apache/airflow/issues/44409)
* [ ] `prev_execution_date` (from #43902 https://github.com/apache/airflow/issues/44409)
* [ ] `prev_execution_date_success` (from #43902 https://github.com/apache/airflow/issues/44409)
* [ ] `tomorrow_ds` (from #43902 https://github.com/apache/airflow/issues/44409)
* [ ] `yesterday_ds` (from #43902 https://github.com/apache/airflow/issues/44409)
* [ ] `yesterday_ds_nodash` (from #43902 https://github.com/apache/airflow/issues/44409)


### AIR303: moved to provider (@Lee-W)
* [ ] `airflow.kubernetes.kubernetes_helper_functions.add_pod_suffix` → `airflow.providers.cncf.kubernetes.kubernetes_helper_functions.add_pod_suffix` (from #41735)
* [ ] `airflow.kubernetes.kubernetes_helper_functions.annotations_for_logging_task_metadata` → `airflow.providers.cncf.kubernetes.kubernetes_helper_functions.annotations_for_logging_task_metadata` (from #41735)
* [ ] `airflow.kubernetes.kubernetes_helper_functions.annotations_to_key` → `airflow.providers.cncf.kubernetes.kubernetes_helper_functions.annotations_to_key` (from #41735)
* [ ] `airflow.kubernetes.kubernetes_helper_functions.create_pod_id` → `airflow.providers.cncf.kubernetes.kubernetes_helper_functions.create_pod_id` (from #41735)
* [ ] `airflow.kubernetes.kubernetes_helper_functions.get_logs_task_metadata` → `airflow.providers.cncf.kubernetes.kubernetes_helper_functions.get_logs_task_metadata`
* [ ] `airflow.kubernetes.kubernetes_helper_functions.rand_str` → `airflow.providers.cncf.kubernetes.kubernetes_helper_functions.rand_str` (from #41735)
* [ ] `airflow.kubernetes.pod.Port` → `kubernetes.client.models.V1ContainerPort` (from #41735)
* [ ] `airflow.kubernetes.pod.Resources` → `kubernetes.client.models.V1ResourceRequirements` (from #41735)
* [ ] `airflow.kubernetes.pod_launcher.PodLauncher` → `airflow.providers.cncf.kubernetes.pod_launcher.PodLauncher` (from #41735)
* [ ] `airflow.kubernetes.pod_launcher.PodStatus` → `airflow.providers.cncf.kubernetes.pod_launcher.PodStatus` (from #41735)
* [ ] `airflow.kubernetes.pod_launcher_deprecated.PodLauncher` → `airflow.providers.cncf.kubernetes.pod_launcher_deprecated.PodLauncher` (from #41735)
* [ ] `airflow.kubernetes.pod_launcher_deprecated.PodStatus` → `airflow.providers.cncf.kubernetes.pod_launcher_deprecated.PodStatus` (from #41735)
* [ ] `airflow.kubernetes.pod_launcher_deprecated.get_kube_client` → `airflow.providers.cncf.kubernetes.kube_client.get_kube_client` (from #41735)
* [ ] `airflow.kubernetes.pod_launcher_deprecated.PodDefaults` → `airflow.providers.cncf.kubernetes.pod_generator_deprecated.PodDefaults` (from #41735)
* [ ] `airflow.kubernetes.pod_runtime_info_env.PodRuntimeInfoEnv` → `kubernetes.client.models.V1EnvVar` (from #41735)
* [ ] `airflow.kubernetes.volume.Volume` → `kubernetes.client.models.V1Volume` (from #41735)
* [ ] `airflow.kubernetes.volume_mount.VolumeMount` → `kubernetes.client.models.V1VolumeMount` (from #41735)
* [ ] `airflow.kubernetes.k8s_model.K8SModel` → `airflow.providers.cncf.kubernetes.k8s_model.K8SModel` (from #41735)
* [ ] `airflow.kubernetes.k8s_model.append_to_pod` → `airflow.providers.cncf.kubernetes.k8s_model.append_to_pod` (from #41735)
* [ ] `airflow.kubernetes.kube_client._disable_verify_ssl` → `airflow.kubernetes.airflow.providers.cncf.kubernetes.kube_client._disable_verify_ssl` (from #41735)
* [ ] `airflow.kubernetes.kube_client._enable_tcp_keepalive` → `airflow.kubernetes.airflow.providers.cncf.kubernetes.kube_client._enable_tcp_keepalive` (from #41735)
* [ ] `airflow.kubernetes.kube_client.get_kube_client` → `airflow.kubernetes.airflow.providers.cncf.kubernetes.kube_client.get_kube_client` (from #41735)
* [ ] `airflow.kubernetes.pod_generator.datetime_to_label_safe_datestring` → `airflow.providers.cncf.kubernetes.pod_generator.datetime_to_label_safe_datestring` (from #41735)
* [ ] `airflow.kubernetes.pod_generator.extend_object_field` → `airflow.kubernetes.airflow.providers.cncf.kubernetes.pod_generator.extend_object_field` (from #41735)
* [ ] `airflow.kubernetes.pod_generator.label_safe_datestring_to_datetime` → `airflow.providers.cncf.kubernetes.pod_generator.label_safe_datestring_to_datetime` (from #41735)
* [ ] `airflow.kubernetes.pod_generator.make_safe_label_value` → `airflow.providers.cncf.kubernetes.pod_generator.make_safe_label_value` (from #41735)
* [ ] `airflow.kubernetes.pod_generator.merge_objects` → `airflow.providers.cncf.kubernetes.pod_generator.merge_objects` (from #41735)
* [ ] `airflow.kubernetes.pod_generator.PodGenerator` → `airflow.providers.cncf.kubernetes.pod_generator.PodGenerator` (from #41735)
* [ ] `airflow.kubernetes.pod_generator.PodGeneratorDeprecated` → `airflow.providers.cncf.kubernetes.pod_generator.PodGenerator` (from #41735)
* [ ] `airflow.kubernetes.pod_generator.PodDefaults` → `airflow.providers.cncf.kubernetes.pod_generator_deprecated.PodDefaults` (from #41735)
* [ ] `airflow.kubernetes.pod_generator.add_pod_suffix` → `airflow.providers.cncf.kubernetes.kubernetes_helper_functions.add_pod_suffix` (from #41735)
* [ ] `airflow.kubernetes.pod_generator.rand_str` → `airflow.providers.cncf.kubernetes.kubernetes_helper_functions.rand_str` (from #41735)
* [ ] `airflow.kubernetes.pod_generator_deprecated.make_safe_label_value` → `airflow.providers.cncf.kubernetes.pod_generator_deprecated.make_safe_label_value` (from #41735)
* [ ] `airflow.kubernetes.pod_generator_deprecated.PodDefaults` → `airflow.providers.cncf.kubernetes.pod_generator_deprecated.PodDefaults` (from #41735)
* [ ] `airflow.kubernetes.pod_generator_deprecated.PodGenerator` → `airflow.providers.cncf.kubernetes.pod_generator_deprecated.PodGenerator` (from #41735)
* [ ] `airflow.kubernetes.secret.Secret` → `airflow.providers.cncf.kubernetes.secret.Secret` (from #41735)
* [ ] `airflow.kubernetes.secret.K8SModel` → `airflow.providers.cncf.kubernetes.k8s_model.K8SModel` (from #41735)

</details>

<details>
    <summary><h2>Merged rules</h2></summary>

### AIR302

## args
* in `DAG`
    * [x] `schedule_interval` (from #41453)
* in `airflow.utils.log.file_task_handler.FileTaskHandler` and its subclassses
    * [x] `filename_template` (#41552)
* in `BaseOperator` and its subclassses
    * [x] `sla` (from #42285)
    * [x] `task_concurrency` → `max_active_tis_per_dag`  (from #41761) https://github.com/astral-sh/ruff/pull/14616

#### args (@Lee-W)
* in `DAG`
    * [x] `timetable` (from #41453)
    * [x] `sla_miss_callback` (from #42285)
* in `airflow.operators.trigger_dagrun.TriggerDagRunOperator`
    * [x] `execution_date` (from #41736)
* in `airflow.operators.weekday.DayOfWeekSensor`
    * [x] `use_task_execution_day` → `use_task_logical_date` (from #41393)
* in `airflow.operators.datetime.BranchDateTimeOperator`
    * [x] `use_task_execution_day` → `use_task_logical_date` (from #41736)
* in `airflow.operators.weekday.BranchDayOfWeekOperator`
    * [x] `use_task_execution_day` → `use_task_logical_date` (from #41736)

#### names (@Lee-W)
* [x] `airflow.triggers.external_task.TaskStateTrigger` (from #41737)
* [x] `airflow.metrics.validators.AllowListValidator` (from #41975) → use `airflow.metrics.validators.PatternAllowListValidator`
* [x] `airflow.metrics.validators.BlockListValidator` (from #41975) → use `airflow.metrics.validators.PatternBlockListValidator`
* [x] `airflow.utils.dates.parse_execution_date` (from #43533)
* [x] `airflow.utils.dates.round_time` (from #43533)
* [x] `airflow.utils.dates.scale_time_units` (from #43533)
* [x] `airflow.utils.dates.infer_time_unit` (from #43533)
* [x] `airflow.utils.dates.date_range` (from #41496) use `airflow.timetables.`
* [x] `airflow.utils.dates.days_ago` (from #41496) → use `pendulum.today('UTC').add(days=-N, ...)`
* [x] `airflow.utils.file.TemporaryDirectory` (from #41395) → use `tempfile.TemporaryDirectory`
* [x] `airflow.utils.file.mkdirs` (from #41395) → use `pathlib.Path({path}).mkdir`
* [x] `airflow.www.auth.has_access` (from #41758) → use `airflow.www.auth.has_access_*`
* [x] `airflow.api_connexion.security.requires_access` (from #41910) → use `requires_access_*`
* [x] `airflow.utils.dag_cycle_tester.test_cycle` (from #41395)
* [x] `airflow.utils.state.SHUTDOWN` (from #41395)
* [x] `airflow.utils.state.terminating_states` (from #41395)
* [x] `airflow.utils.decorators.apply_defaults` (from #41579) (auto applied)
* [x] `airflow.www.utils.get_sensitive_variables_fields` → `airflow.utils.log.secrets_masker.get_sensitive_variables_fields` (from #41758)
* [x] `airflow.www.utils.should_hide_value_for_key` → `airflow.utils.log.secrets_masker.should_hide_value_for_key` (from #41758)
* [x] `airflow.configuration.get` → `airflow.configuration.conf.get` (from #43530)
* [x] `airflow.configuration.getboolean` → `airflow.configuration.conf.getboolean` (from #43530)
* [x] `airflow.configuration.getfloat` → `airflow.configuration.conf.getfloat` (from #43530)
* [x] `airflow.configuration.getint` → `airflow.configuration.conf.getint` (from #43530)
* [x] `airflow.configuration.has_option` → `airflow.configuration.conf.has_option` (from #43530)
* [x] `airflow.configuration.remove_option` → `airflow.configuration.conf.remove_option` (from #43530)
* [x] `airflow.configuration.as_dict` → `airflow.configuration.conf.as_dict` (from #43530)
* [x] `airflow.configuration.set` → `airflow.configuration.conf.set` (from #43530)
* [x] `airflow.secrets.local_filesystem.load_connections` → `airflow.secrets.local_filesystem.load_connections_dict` (from #41533)
* [x] `airflow.secrets.local_filesystem.get_connection` → `airflow.secrets.local_filesystem.load_connections_dict` (from #41533)
* [x] `airflow.utils.helpers.chain` → `airflow.models.baseoperator.chain` (from #41520)
* [x] `airflow.utils.helpers.cross_downstream` → `airflow.models.baseoperator.cross_downstream` (from #41520)
* [x] `airflow.contrib.*` (from #41366)
* [x] `airflow.operators.subdag.*` (from #41390)
* [x] `airflow.sensors.external_task.ExternalTaskSensorLink` → `airflow.sensors.external_task.ExternalDagLin` (from #41391)
* [x] `airflow.operators.bash_operator.BashOperator` → `airflow.operators.bash.BashOperator` (from #41368)
* [x] `airflow.operators.branch_operator.BaseBranchOperator` → `airflow.operators.branch.BaseBranchOperator` (from #41368)
* [x] `airflow.operators.dummy.EmptyOperator` → `airflow.operators.empty.EmptyOperator` (from #41368)
* [x] `airflow.operators.dummy.DummyOperator` → `airflow.operators.empty.EmptyOperator` (from #41368)
* [x] `airflow.operators.dummy_operator.EmptyOperator` → `airflow.operators.empty.EmptyOperator` (from #41368)
* [x] `airflow.operators.dummy_operator.DummyOperator` → `airflow.operators.empty.EmptyOperator` (from #41368)
* [x]`airflow.operators.email_operator.EmailOperator` → `airflow.operators.email.EmailOperator` (from #41368)
* [x] `airflow.sensors.base_sensor_operator.BaseSensorOperator` → `airflow.sensors.base.BaseSensorOperator` (from #41368)
* [x] `airflow.sensors.date_time_sensor.DateTimeSensor` → `airflow.sensors.date_time.DateTimeSensor` (from #41368)
* [x] `airflow.sensors.external_task_sensor.ExternalTaskMarker` → `airflow.sensors.external_task.ExternalTaskMarker` (from #41368)
* [x] `airflow.sensors.external_task_sensor.ExternalTaskSensor` → `airflow.sensors.external_task.ExternalTaskSensor` (from #41368)
* [x] `airflow.sensors.external_task_sensor.ExternalTaskSensorLink` → `airflow.sensors.external_task.ExternalTaskSensorLink` (from #41368)
* [x] `airflow.sensors.time_delta_sensor.TimeDeltaSensor` → `airflow.sensors.time_delta.TimeDeltaSensor` (from #41368)
* [x] `airflow.utils.trigger_rule.TriggerRule.DUMMY` (from #41761)
* [x] `airflow.utils.trigger_rule.TriggerRule.NONE_FAILED_OR_SKIPPED` (from #41761)
* [x] `airflow.PY\d\d` (from #43562)
* [x] `airflow.api_connexion.security.requires_access_dataset` → `airflow.api_connexion.security.requires_access_asset` (from #41348)
* [x] `airflow.auth.managers.base_auth_manager.is_authorized_dataset` → `airflow.auth.managers.base_auth_manager.is_authorized_asset` (from #41348)
* [x] `airflow.auth.managers.models.resource_details.DatasetDetails` → `airflow.auth.managers.models.resource_details.AssetDetails` (from #41348)
* [x] `airflow.lineage.hook.DatasetLineageInfo` → `airflow.lineage.hook.AssetLineageInfo` (from #41348)
* [x] `airflow.security.permissions.RESOURCE_DATASET` → `airflow.security.permissions.RESOURCE_ASSET` (from #41348)
* [x] `airflow.www.auth.has_access_dataset` → `airflow.www.auth.has_access_dataset.has_access_asset` (from #41348)
* [x] `airflow.datasets.DatasetAliasEvent` (from #41348)
* [x] `airflow.datasets.Dataset` → `airflow.sdk.definitions.asset.Asset` (from #41348)
* [x] `airflow.Dataset` → `airflow.sdk.definitions.asset.Asset` (from #41348)
* [x] `airflow.datasets.DatasetAlias` → `airflow.sdk.definitions.asset.AssetAlias` (from #41348)
* [x] `airflow.datasets.DatasetAll` → `airflow.sdk.definitions.asset.AssetAll` (from #41348)
* [x] `airflow.datasets.DatasetAny` → `airflow.sdk.definitions.asset.AssetAny` (from #41348)
* [x] `airflow.datasets.metadata` → `airflow.sdk.definitions.asset.metadata` (from #41348)
* [x] `airflow.datasets.expand_alias_to_datasets` → `airflow.sdk.definitions.asset.expand_alias_to_assets` (from #41348)
* [x] `airflow.datasets.manager.dataset_manager` → `airflow.assets.manager` (from #41348)
* [x] `airflow.datasets.manager.resolve_dataset_manager` → `airflow.assets.resolve_asset_manager` (from #41348)
* [x] `airflow.datasets.manager.DatasetManager` → `airflow.assets.AssetManager` (from #41348)
* [x] `airflow.listeners.spec.dataset.on_dataset_created` → `airflow.listeners.spec.asset.on_asset_created` (from #41348)
* [x] `airflow.listeners.spec.dataset.on_dataset_changed` → `airflow.listeners.spec.asset.on_asset_changed` (from #41348)
* [x] `airflow.timetables.simple.DatasetTriggeredTimetable` → `airflow.timetables.simple.AssetTriggeredTimetable` (from #41348)
* [x] `airflow.timetables.datasets.DatasetOrTimeSchedule` → `airflow.timetables.assets.AssetOrTimeSchedule` (from #41348)
* [x] `airflow.providers.amazon.auth_manager.avp.entities.AvpEntities.DATASET` → `airflow.providers.amazon.auth_manager.avp.entities.AvpEntities.ASSET` (from #41348)
* [x] `airflow.providers.amazon.aws.datasets.s3.create_dataset` → `airflow.providers.amazon.aws.assets.s3.create_asset` (from #41348)
* [x] `airflow.providers.amazon.aws.datasets.s3.convert_dataset_to_openlineage` → `airflow.providers.amazon.aws.datasets.s3.convert_dataset_to_openlineage` (from #41348)
* [x] `airflow.providers.amazon.aws.datasets.s3.sanitize_uri` → `airflow.providers.amazon.aws.assets.s3.sanitize_uri` (from #41348)
* [x] `airflow.providers.common.io.datasets.file.convert_dataset_to_openlineage` → `airflow.providers.common.io.assets.file.convert_asset_to_openlineage` (from #41348)
* [x] `airflow.providers.common.io.datasets.file.sanitize_uri` → `airflow.providers.common.io.assets.file.sanitize_uri` (from #41348)
* [x] `airflow.providers.common.io.datasets.file.create_dataset` → `airflow.providers.common.io.assets.file.create_asset` (from #41348)
* [x] `airflow.providers.google.datasets.bigquery.sanitize_uri` → `airflow.providers.google.assets.bigquery.sanitize_uri` (from #41348)
* [x] `airflow.providers.google.datasets.gcs.create_dataset` → `airflow.providers.google.assets.gcs.create_asset` (from #41348)
* [x] `airflow.providers.google.datasets.gcs.sanitize_uri` → `airflow.providers.google.assets.gcs.sanitize_uri` (from #41348)
* [x] `airflow.providers.google.datasets.gcs.convert_dataset_to_openlineage` → `airflow.providers.google.assets.gcs.convert_asset_to_openlineage` (from #41348)
* [x] `airflow.providers.fab.auth_manager.fab_auth_manager.is_authorized_dataset` → `airflow.providers.fab.auth_manager.fab_auth_manager.is_authorized_asset` (from #41348)
* [x] `airflow.providers.openlineage.utils.utils.DatasetInfo` → `airflow.providers.openlineage.utils.utils.AssetInfo` (from #41348)
* [x] `airflow.providers.openlineage.utils.utils.translate_airflow_dataset` → `airflow.providers.openlineage.utils.utils.translate_airflow_asset` (from #41348)
* [x] `airflow.providers.postgres.datasets.postgres.sanitize_uri` → `airflow.providers.postgres.assets.postgres.sanitize_uri` (from #41348)
* [x] `airflow.providers.mysql.datasets.mysql.sanitize_uri` → `airflow.providers.mysql.assets.mysql.sanitize_uri` (from #41348)
* [x] `airflow.providers.trino.datasets.trino.sanitize_uri` → `airflow.providers.trino.assets.trino.sanitize_uri` (from #41348)
* [x] `airflow.hooks.base_hook.BaseHook` → `airflow.hooks.base.BaseHook` (from #41368)
* [x] `airflow.operators.dagrun_operator.TriggerDagRunLink` → `airflow.operators.trigger_dagrun.TriggerDagRunLink` (from #41368)
* [x] `airflow.operators.dagrun_operator.TriggerDagRunOperator` → `airflow.operators.trigger_dagrun.TriggerDagRunOperator` (from #41368)
* [x] `airflow.operators.python_operator.BranchPythonOperator` → `airflow.operators.python.BranchPythonOperator` (from #41368)
* [x] `airflow.operators.python_operator.PythonOperator` → `airflow.operators.python.PythonOperator` (from #41368)
* [x] `airflow.operators.python_operator.PythonVirtualenvOperator` → `airflow.operators.python.PythonVirtualenvOperator` (from #41368)
* [x] `airflow.operators.python_operator.ShortCircuitOperator` → `airflow.operators.python.ShortCircuitOperator` (from #41368)
* [x] `airflow.operators.latest_only_operator.LatestOnlyOperator` → `airflow.operators.latest_only.LatestOnlyOperator` (from #41368)

#### method call (@Lee-W)
* [x] `airflow.datasets.manager.DatasetManager.register_dataset_change` → `airflow.assets.manager.AssetManager.register_asset_change` (from #41348)
* [x] `airflow.datasets.manager.DatasetManager.create_datasets` → `airflow.assets.manager.AssetManager.create_assets` (from #41348)
* [x] `airflow.datasets.manager.DatasetManager.notify_dataset_created` → `airflow.assets.manager.AssetManager.notify_asset_created` (from #41348)
* [x] `airflow.datasets.manager.DatasetManager.notify_dataset_changed` → `airflow.assets.manager.AssetManager.notify_asset_changed` (from #41348)
* [x] `airflow.datasets.manager.DatasetManager.notify_dataset_alias_created` → `airflow.assets.manager.AssetManager.notify_asset_alias_created` (from #41348)
* [x] `airflow.providers.amazon.auth_manager.aws_auth_manager.AwsAuthManager.is_authorized_dataset` → `airflow.providers.amazon.auth_manager.aws_auth_manager.AwsAuthManager.is_authorized_asset` (from #41348)
* [x] `airflow.lineage.hook.HookLineageCollector.create_dataset` → `airflow.lineage.hook.HookLineageCollector.create_asset` (from #41348)
* [x] `airflow.lineage.hook.HookLineageCollector.add_input_dataset` → `airflow.lineage.hook.HookLineageCollector.add_input_asset` (from #41348)
* [x] `airflow.lineage.hook.HookLineageCollector.add_output_dataset` → `airflow.lineage.hook.HookLineageCollector.dd_output_asset` (from #41348)
* [x] `airflow.lineage.hook.HookLineageCollector.collected_datasets` → `airflow.lineage.hook.HookLineageCollector.collected_assets` (from #41348)
* [x] `airflow.providers_manager.ProvidersManager.initialize_providers_dataset_uri_resources` → `airflow.providers_manager.ProvidersManager.initialize_providers_asset_uri_resources` (from #41348)
* [x] `airflow.secrets.base_secrets.BaseSecretsBackend.get_conn_uri` → `airflow.secrets.base_secrets.BaseSecretsBackend.get_conn_value` (from #41348)
* [x] `airflow.secrets.base_secrets.BaseSecretsBackend.get_connections` → `airflow.secrets.base_secrets.BaseSecretsBackend.get_connection` (from #41348)
* [x] `airflow.hooks.base.BaseHook.get_connections` → use `get_connection` (from #41348)
* [x] `airflow.datasets.BaseDataset.iter_datasets` → `airflow.sdk.definitions.asset.BaseAsset.iter_assets`
* [x] `airflow.datasets.BaseDataset.iter_dataset_aliases` → `airflow.sdk.definitions.asset.BaseAsset.iter_asset_aliases` (from #41348)

#### property (@Lee-W)
* [x] `airflow.providers_manager.ProvidersManager.dataset_factories` → `airflow.providers_manager.ProvidersManager.asset_factories` (from #41348)
* [x] `airflow.providers_manager.ProvidersManager.dataset_uri_handlers` → `airflow.providers_manager.ProvidersManager.asset_uri_handlers` (from #41348)
* [x] `airflow.providers_manager.ProvidersManager.dataset_to_openlineage_converters` → `airflow.providers_manager.ProvidersManager.asset_to_openlineage_converters` (from #41348)

### class attrubite (@Lee-W)
* [x] `airflow.lineage.hook.DatasetLineageInfo.dataset`  → `airflow.lineage.hook.AssetLineageInfo.asset` (from #41348)
* inherit from `airflow.plugins_manager.AirflowPlugin`
    * [x] `executors` (from #43289)
    * [x] `hooks` (from #43291)
    * [x] `operators`
    * [x] `sensors`

### AIR303 (@Lee-W)
* [x] `airflow.www.security.FabAirflowSecurityManagerOverride` → `airflow.providers.fab.auth_manager.security_manager.override.FabAirflowSecurityManagerOverride` (from #41758)
* [x] `airflow.api.auth.backend.basic_auth` → `airflow.providers.fab.auth_manager.api.auth.backend.basic_auth` (from #41663)
* [x] `airflow.api.auth.backend.kerberos_auth` → airflow.executors.`airflow.providers.fab.auth_manager.api.auth.backend.kerberos_auth` (from #41693)
* [x] `airflow.auth.managers.fab.api.auth.backend.kerberos_auth` → `airflow.providers.fab.auth_manager.api.auth.backend.kerberos_auth` (from #41693)
* [x] `airflow.auth.managers.fab.fab_auth_manager` → `airflow.providers.fab.auth_manager.security_manager.override` (from #41708)
* [x] `airflow.auth.managers.fab.security_manager.override` → `airflow.providers.fab.auth_manager.security_manager.override` (from #41708)
* [x] `airflow.hooks.dbapi` → `airflow.providers.common.sql.hooks.sql` (from #41748)
* [x] `airflow.api.auth.backend.default` → ` airflow.providers.fab.auth_manager.api.auth.backend.session` (from #43096)
* [x] `airflow.executors.celery_executor.CeleryExecutor` → `airflow.providers.celery.executors.celery_executor.CeleryExecutor` (from #41368)
* [x] `airflow.executors.celery_kubernetes_executor.CeleryKubernetesExecutor` → `airflow.providers.celery.executors.celery_kubernetes_executor.CeleryKubernetesExecutor` (from #41368)
* [x] `airflow.executors.dask_executor.DaskExecutor` → `airflow.providers.daskexecutor.executors.dask_executor.DaskExecutor` (from #41368)
* [x] `airflow.executors.kubernetes_executor.KubernetesExecutor` → `airflow.providers.cncf.kubernetes.executors.kubernetes_executor.KubernetesExecutor` (from #41368)
* [x] `airflow.executors.kubernetes_executor_utils.AirflowKubernetesScheduler` → `airflow.providers.cncf.kubernetes.executors.kubernetes_executor_utils.AirflowKubernetesScheduler` (from #41368)
* [x] `airflow.executors.kubernetes_executor_utils.KubernetesJobWatcher` → `airflow.providers.cncf.kubernetes.executors.kubernetes_executor_utils.KubernetesJobWatcher` (from #41368)
* [x] `airflow.executors.kubernetes_executor_utils.ResourceVersion` → `airflow.providers.cncf.kubernetes.executors.kubernetes_executor_utils.ResourceVersion` (from #41368)
* [x] `airflow.executors.local_kubernetes_executor.LocalKubernetesExecutor` → `airflow.providers.cncf.kubernetes.executors.LocalKubernetesExecutor` (from #41368)
* [x] `airflow.hooks.S3_hook.S3Hook` → `airflow.providers.amazon.aws.hooks.s3.S3Hook` (from #41368)
* [x] `airflow.hooks.S3_hook.provide_bucket_name` → `airflow.providers.amazon.aws.hooks.s3.provide_bucket_name` (from #41368)
* [x] `airflow.hooks.base_hook.BaseHook` → `airflow.hooks.base.BaseHook` (from #41368)
* [x] `airflow.hooks.dbapi_hook.DbApiHook` → `airflow.providers.common.sql.hooks.sql.DbApiHook` (from #41368)
* [x] `airflow.hooks.docker_hook.DockerHook` → `airflow.providers.docker.hooks.docker.DockerHook` (from #41368)
* [x] `airflow.hooks.druid_hook.DruidDbApiHook` → `airflow.providers.apache.druid.hooks.druid.DruidDbApiHook` (from #41368)
* [x] `airflow.hooks.druid_hook.DruidHook` → `airflow.providers.apache.druid.hooks.druid.DruidHook` (from #41368)
* [x] `airflow.hooks.hive_hooks.HIVE_QUEUE_PRIORITIES` → `airflow.providers.apache.hive.hooks.hive.HIVE_QUEUE_PRIORITIES` (from #41368)
* [x] `airflow.hooks.hive_hooks.HiveCliHook` → `airflow.providers.apache.hive.hooks.hive.HiveCliHook` (from #41368)
* [x] `airflow.hooks.hive_hooks.HiveMetastoreHook` → `airflow.providers.apache.hive.hooks.hive.HiveMetastoreHook` (from #41368)
* [x] `airflow.hooks.hive_hooks.HiveServer2Hook` → `airflow.providers.apache.hive.hooks.hive.HiveServer2Hook` (from #41368)
* [x] `airflow.hooks.http_hook.HttpHook` → `airflow.providers.http.hooks.http.HttpHook` (from #41368)
* [x] `airflow.hooks.jdbc_hook.JdbcHook` → `airflow.providers.jdbc.hooks.jdbc.JdbcHook` (from #41368)
* [x] `airflow.hooks.jdbc_hook.jaydebeapi` → `airflow.providers.jdbc.hooks.jdbc.jaydebeapi` (from #41368)
* [x] `airflow.hooks.mssql_hook.MsSqlHook` → `airflow.providers.microsoft.mssql.hooks.mssql.MsSqlHook` (from #41368)
* [x] `airflow.hooks.mysql_hook.MySqlHook` → `airflow.providers.mysql.hooks.mysql.MySqlHook` (from #41368)
* [x] `airflow.hooks.oracle_hook.OracleHook` → `airflow.providers.oracle.hooks.oracle.OracleHook` (from #41368)
* [x] `airflow.hooks.pig_hook.PigCliHook` → `airflow.providers.apache.pig.hooks.pig.PigCliHook` (from #41368)
* [x] `airflow.hooks.postgres_hook.PostgresHook` → `airflow.providers.postgres.hooks.postgres.PostgresHook` (from #41368)
* [x] `airflow.hooks.presto_hook.PrestoHook` → `airflow.providers.presto.hooks.presto.PrestoHook` (from #41368)
* [x] `airflow.hooks.samba_hook.SambaHook` → `airflow.providers.samba.hooks.samba.SambaHook` (from #41368)
* [x] `airflow.hooks.slack_hook.SlackHook` → `airflow.providers.slack.hooks.slack.SlackHook` (from #41368)
* [x] `airflow.hooks.sqlite_hook.SqliteHook` → `airflow.providers.sqlite.hooks.sqlite.SqliteHook` (from #41368)
* [x] `airflow.hooks.webhdfs_hook.WebHDFSHook` → `airflow.providers.apache.hdfs.hooks.webhdfs.WebHDFSHook` (from #41368)
* [x] `airflow.hooks.zendesk_hook.ZendeskHook` → `airflow.providers.zendesk.hooks.zendesk.ZendeskHook` (from #41368)
 (from #41368)
* [x] `airflow.operators.check_operator.SQLCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLCheckOperator` (from #41368)
* [x] `airflow.operators.check_operator.SQLIntervalCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLIntervalCheckOperator` (from #41368)
* [x] `airflow.operators.check_operator.SQLThresholdCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLThresholdCheckOperator` (from #41368)
* [x] `airflow.operators.check_operator.SQLValueCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLValueCheckOperator` (from #41368)
* [x] `airflow.operators.check_operator.CheckOperator` → `airflow.providers.common.sql.operators.sql.SQLCheckOperator` (from #41368)
* [x] `airflow.operators.check_operator.IntervalCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLIntervalCheckOperator` (from #41368)
* [x] `airflow.operators.check_operator.ThresholdCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLThresholdCheckOperator` (from #41368)
* [x] `airflow.operators.check_operator.ValueCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLValueCheckOperator` (from #41368)
* [x] `airflow.operators.dagrun_operator.TriggerDagRunLink` → `airflow.operators.trigger_dagrun.TriggerDagRunLink` (from #41368)
* [x] `airflow.operators.dagrun_operator.TriggerDagRunOperator` → `airflow.operators.trigger_dagrun.TriggerDagRunOperator` (from #41368)
* [x] `airflow.operators.docker_operator.DockerOperator` → `airflow.providers.docker.operators.docker.DockerOperator` (from #41368)
* [x] `airflow.operators.druid_check_operator.DruidCheckOperator` → `airflow.providers.apache.druid.operators.druid_check.DruidCheckOperator` (from #41368)
* [x] `airflow.operators.gcs_to_s3.GCSToS3Operator` → `airflow.providers.amazon.aws.transfers.gcs_to_s3.GCSToS3Operator` (from #41368)
* [x] `airflow.operators.google_api_to_s3_transfer.GoogleApiToS3Operator` → `airflow.providers.amazon.aws.transfers.google_api_to_s3.GoogleApiToS3Operator` (from #41368)
* [x] `airflow.operators.google_api_to_s3_transfer.GoogleApiToS3Transfer` → `airflow.providers.amazon.aws.transfers.google_api_to_s3.GoogleApiToS3Operator` (from #41368)
* [x] `airflow.operators.hive_operator.HiveOperator` → `airflow.providers.apache.hive.operators.hive.HiveOperator` (from #41368)
* [x] `airflow.operators.hive_stats_operator.HiveStatsCollectionOperator` → `airflow.providers.apache.hive.operators.hive_stats.HiveStatsCollectionOperator` (from #41368)
* [x] `airflow.operators.hive_to_druid.HiveToDruidOperator` → `airflow.providers.apache.druid.transfers.hive_to_druid.HiveToDruidOperator` (from #41368)
* [x] `airflow.operators.hive_to_druid.HiveToDruidTransfer` → `airflow.providers.apache.druid.transfers.hive_to_druid.HiveToDruidOperator` (from #41368)
* [x] `airflow.operators.hive_to_mysql.HiveToMySqlOperator` → `airflow.providers.apache.hive.transfers.hive_to_mysql.HiveToMySqlOperator` (from #41368)
* [x] `airflow.operators.hive_to_mysql.HiveToMySqlTransfer` → `airflow.providers.apache.hive.transfers.hive_to_mysql.HiveToMySqlOperator` (from #41368)
* [x] `airflow.operators.local_kubernetes_executor.HiveToSambaOperator` → `airflow.providers.apache.hive.transfers.hive_to_samba.HiveToSambaOperator` (from #41368)
* [x] `airflow.operators.hive_to_samba_operator.SimpleHttpOperator` → `airflow.providers.http.operators.http.SimpleHttpOperator` (from #41368)
* [x] `airflow.operators.jdbc_operator.JdbcOperator` → `airflow.providers.jdbc.operators.jdbc.JdbcOperator` (from #41368)
* [x] `airflow.operators.latest_only_operator.LatestOnlyOperator` → `airflow.operators.latest_only.LatestOnlyOperator` (from #41368)
* [x] `airflow.operators.mssql_operator.MsSqlOperator` → `airflow.providers.microsoft.mssql.operators.mssql.MsSqlOperator` (from #41368)
* [x] `airflow.operators.mssql_to_hive.MsSqlToHiveOperator` → `airflow.providers.apache.hive.transfers.mssql_to_hive.MsSqlToHiveOperator` (from #41368)
* [x] `airflow.operators.mssql_to_hive.MsSqlToHiveTransfer` → `airflow.providers.apache.hive.transfers.mssql_to_hive.MsSqlToHiveOperator` (from #41368)
* [x] `airflow.operators.mysql_operator.MySqlOperator` → `airflow.providers.mysql.operators.mysql.MySqlOperator` (from #41368)
* [x] `airflow.operators.mysql_to_hive.MySqlToHiveOperator` → `airflow.providers.apache.hive.transfers.mysql_to_hive.MySqlToHiveOperator` (from #41368)
* [x] `airflow.operators.mysql_to_hive.MySqlToHiveTransfer` → `airflow.providers.apache.hive.transfers.mysql_to_hive.MySqlToHiveOperator` (from #41368)
* [x] `airflow.operators.oracle_operator.OracleOperator` → `airflow.providers.oracle.operators.oracle.OracleOperator` (from #41368)
* [x] `airflow.operators.papermill_operator.PapermillOperator` → `airflow.providers.papermill.operators.papermill.PapermillOperator` (from #41368)
* [x] `airflow.operators.pig_operator.PigOperator` → `airflow.providers.apache.pig.operators.pig.PigOperator` (from #41368)
* [x] `airflow.operators.postgres_operator.Mapping` → `airflow.providers.postgres.operators.postgres.Mapping` (from #41368)
* [x] `airflow.operators.postgres_operator.PostgresOperator` → `airflow.providers.postgres.operators.postgres.PostgresOperator` (from #41368)
* [x] `airflow.operators.presto_check_operator.SQLCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLCheckOperator` (from #41368)
* [x] `airflow.operators.presto_check_operator.SQLIntervalCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLIntervalCheckOperator` (from #41368)
* [x] `airflow.operators.presto_check_operator.SQLValueCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLValueCheckOperator` (from #41368)
* [x] `airflow.operators.presto_check_operator.PrestoCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLCheckOperator` (from #41368)
* [x] `airflow.operators.presto_check_operator.PrestoIntervalCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLIntervalCheckOperator` (from #41368)
* [x] `airflow.operators.presto_check_operator.PrestoValueCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLValueCheckOperator` (from #41368)
* [x] `airflow.operators.presto_to_mysql.PrestoToMySqlOperator` → `airflow.providers.mysql.transfers.presto_to_mysql.PrestoToMySqlOperator` (from #41368)
* [x] `airflow.operators.presto_to_mysql.PrestoToMySqlTransfer` → `airflow.providers.mysql.transfers.presto_to_mysql.PrestoToMySqlOperator` (from #41368)
* [x] `airflow.operators.python_operator.BranchPythonOperator` → `airflow.operators.python.BranchPythonOperator` (from #41368)
* [x] `airflow.operators.python_operator.PythonOperator` → `airflow.operators.python.PythonOperator` (from #41368)
* [x] `airflow.operators.python_operator.PythonVirtualenvOperator` → `airflow.operators.python.PythonVirtualenvOperator` (from #41368)
* [x] `airflow.operators.python_operator.ShortCircuitOperator` → `airflow.operators.python.ShortCircuitOperator` (from #41368)
* [x] `airflow.operators.redshift_to_s3_operator.RedshiftToS3Operator` → `airflow.providers.amazon.aws.transfers.redshift_to_s3.RedshiftToS3Operator` (from #41368)
* [x] `airflow.operators.redshift_to_s3_operator.RedshiftToS3Transfer` → `airflow.providers.amazon.aws.transfers.redshift_to_s3.RedshiftToS3Operator` (from #41368)
* [x] `airflow.operators.s3_file_transform_operator.S3FileTransformOperator` → `airflow.providers.amazon.aws.operators.s3_file_transform.S3FileTransformOperator` (from #41368)
* [x] `airflow.operators.s3_to_hive_operator.S3ToHiveOperator` → `airflow.providers.apache.hive.transfers.s3_to_hive.S3ToHiveOperator` (from #41368)
* [x] `airflow.operators.s3_to_hive_operator.S3ToHiveTransfer` → `airflow.providers.apache.hive.transfers.s3_to_hive.S3ToHiveOperator` (from #41368)
* [x] `airflow.operators.s3_to_redshift_operator.S3ToRedshiftOperator` → `airflow.providers.amazon.aws.transfers.s3_to_redshift.S3ToRedshiftOperator` (from #41368)
* [x] `airflow.operators.s3_to_redshift_operator.S3ToRedshiftTransfer` → `airflow.providers.amazon.aws.transfers.s3_to_redshift.S3ToRedshiftOperator` (from #41368)
* [x] `airflow.operators.slack_operator.SlackAPIOperator` → `airflow.providers.slack.operators.slack.SlackAPIOperator` (from #41368)
* [x] `airflow.operators.slack_operator.SlackAPIPostOperator` → `airflow.providers.slack.operators.slack.SlackAPIPostOperator` (from #41368)
* [x] `airflow.operators.sql.BaseSQLOperator` → `airflow.providers.common.sql.operators.sql.BaseSQLOperator` (from #41368)
* [x] `airflow.operators.sql.BranchSQLOperator` → `airflow.providers.common.sql.operators.sql.BranchSQLOperator` (from #41368)
* [x] `airflow.operators.sql.SQLCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLCheckOperator` (from #41368)
* [x] `airflow.operators.sql.SQLColumnCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLColumnCheckOperator` (from #41368)
* [x] `airflow.operators.sql.SQLIntervalCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLIntervalCheckOperator` (from #41368)
* [x] `airflow.operators.sql.SQLTableCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLTableCheckOperator` (from #41368)
* [x] `airflow.operators.sql.SQLThresholdCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLThresholdCheckOperator` (from #41368)
* [x] `airflow.operators.sql.SQLValueCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLValueCheckOperator` (from #41368)
* [x] `airflow.operators.sql._convert_to_float_if_possible` → `airflow.providers.common.sql.operators.sql._convert_to_float_if_possible` (from #41368)
* [x] `airflow.operators.sql.parse_boolean` → `airflow.providers.common.sql.operators.sql.parse_boolean` (from #41368)
* [x] `airflow.operators.sql_branch_operator.BranchSQLOperator` → `airflow.providers.common.sql.operators.sql.BranchSQLOperator` (from #41368)
* [x] `airflow.operators.sql_branch_operator.BranchSqlOperator` → `airflow.providers.common.sql.operators.sql.BranchSQLOperator` (from #41368)
* [x] `airflow.operators.sqlite_operator.SqliteOperator` → `airflow.providers.sqlite.operators.sqlite.SqliteOperator` (from #41368)
* [x] `airflow.sensors.hive_partition_sensor.HivePartitionSensor` → `airflow.providers.apache.hive.sensors.hive_partition.HivePartitionSensor` (from #41368)
* [x] `airflow.sensors.http_sensor.HttpSensor` → `airflow.providers.http.sensors.http.HttpSensor` (from #41368)
* [x] `airflow.sensors.metastore_partition_sensor.MetastorePartitionSensor` → `airflow.providers.apache.hive.sensors.metastore_partition.MetastorePartitionSensor` (from #41368)
* [x] `airflow.sensors.named_hive_partition_sensor.NamedHivePartitionSensor` → `airflow.providers.apache.hive.sensors.named_hive_partition.NamedHivePartitionSensor` (from #41368)
* [x] `airflow.sensors.s3_key_sensor.S3KeySensor` → `airflow.providers.amazon.aws.sensors.s3.S3KeySensor` (from #41368)
* [x] `airflow.sensors.sql.SqlSensor` → `airflow.providers.common.sql.sensors.sql.SqlSensor` (from #41368)
* [x] `airflow.sensors.sql_sensor.SqlSensor` → `airflow.providers.common.sql.sensors.sql.SqlSensor` (from #41368)
* [x] `airflow.sensors.web_hdfs_sensor.WebHdfsSensor` → `airflow.providers.apache.hdfs.sensors.web_hdfs.WebHdfsSensor` (from #41368)
* [x] `airflow.executors.kubernetes_executor_types.ALL_NAMESPACES` → `airflow.providers.cncf.kubernetes.executors.kubernetes_executor_types.ALL_NAMESPACES` (from #41368)
* [x] `airflow.executors.kubernetes_executor_types.POD_EXECUTOR_DONE_KEY` → `airflow.providers.cncf.kubernetes.executors.kubernetes_executor_types.POD_EXECUTOR_DONE_KEY` (from #41368)
* [x] `airflow.hooks.hive_hooks.HIVE_QUEUE_PRIORITIES` → `airflow.providers.apache.hive.hooks.hive.HIVE_QUEUE_PRIORITIES` (from #41368)
* [x] `airflow.executors.celery_executor.app` → `airflow.providers.celery.executors.celery_executor_utils.app` (from #41368)
* [x] `airflow.macros.hive.closest_ds_partition` → `airflow.providers.apache.hive.macros.hive.closest_ds_partition` (from #41368)
* [x] `airflow.macros.hive.max_partition` → `airflow.providers.apache.hive.macros.hive.max_partition` (from #41368)

</details>

### Use case/motivation

_No response_

### Related issues

https://github.com/apache/airflow/issues/41641

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-12-02 06:31:19+00:00,"['Lee-W', 'sunank200']",2025-01-02 12:45:53+00:00,,https://github.com/apache/airflow/issues/44556,"[('area:dev-tools', ''), ('kind:feature', 'Feature Requests'), ('area:upgrade', 'Facilitating migration to a newer version of Airflow'), ('area:dependencies', 'Issues related to dependencies problems')]","[{'comment_id': 2514720328, 'issue_id': 2710686484, 'author': 'Lee-W', 'body': '## Current progress\r\n\r\nUpdated: 1/2\r\n\r\n* remaining work\r\n    * rest of the context key removal (blocked by https://github.com/astral-sh/ruff/pull/15144) \r\n* in review PRs\r\n    1. https://github.com/astral-sh/ruff/pull/15144\r\n    1. https://github.com/astral-sh/ruff/pull/15220\r\n* merged PRs\r\n    1. https://github.com/astral-sh/ruff/pull/15196\r\n    1. https://github.com/astral-sh/ruff/pull/15211\r\n    1. https://github.com/astral-sh/ruff/pull/15159\r\n    1. https://github.com/astral-sh/ruff/pull/15145\r\n    1. https://github.com/astral-sh/ruff/pull/15083\r\n    1. https://github.com/astral-sh/ruff/pull/15054  \r\n    1. https://github.com/astral-sh/ruff/pull/15015\r\n    1. https://github.com/astral-sh/ruff/pull/14804\r\n    1. https://github.com/astral-sh/ruff/pull/14804\r\n    1. https://github.com/astral-sh/ruff/pull/14734\r\n    1. https://github.com/astral-sh/ruff/pull/14764 \r\n    1. https://github.com/astral-sh/ruff/pull/15216', 'created_at': datetime.datetime(2024, 12, 3, 14, 28, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2544713799, 'issue_id': 2710686484, 'author': 'prabhusneha', 'body': ""I'd like to work on AIR303 tasks"", 'created_at': datetime.datetime(2024, 12, 16, 6, 29, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2544720136, 'issue_id': 2710686484, 'author': 'Lee-W', 'body': ""> I'd like to work on AIR303 tasks\r\n\r\nThat would be really nice! Thanks for helping out! I tried to put some guideline in the first AIR303 PR I created https://github.com/astral-sh/ruff/pull/14764. Please also tag me when you create the PR. I think the ruff team need our help for confirming the rule part (they'll check the rust part 👍 )"", 'created_at': datetime.datetime(2024, 12, 16, 6, 33, 40, tzinfo=datetime.timezone.utc)}]","Lee-W (Issue Creator) on (2024-12-03 14:28:15 UTC): ## Current progress

Updated: 1/2

* remaining work
    * rest of the context key removal (blocked by https://github.com/astral-sh/ruff/pull/15144) 
* in review PRs
    1. https://github.com/astral-sh/ruff/pull/15144
    1. https://github.com/astral-sh/ruff/pull/15220
* merged PRs
    1. https://github.com/astral-sh/ruff/pull/15196
    1. https://github.com/astral-sh/ruff/pull/15211
    1. https://github.com/astral-sh/ruff/pull/15159
    1. https://github.com/astral-sh/ruff/pull/15145
    1. https://github.com/astral-sh/ruff/pull/15083
    1. https://github.com/astral-sh/ruff/pull/15054  
    1. https://github.com/astral-sh/ruff/pull/15015
    1. https://github.com/astral-sh/ruff/pull/14804
    1. https://github.com/astral-sh/ruff/pull/14804
    1. https://github.com/astral-sh/ruff/pull/14734
    1. https://github.com/astral-sh/ruff/pull/14764 
    1. https://github.com/astral-sh/ruff/pull/15216

prabhusneha on (2024-12-16 06:29:31 UTC): I'd like to work on AIR303 tasks

Lee-W (Issue Creator) on (2024-12-16 06:33:40 UTC): That would be really nice! Thanks for helping out! I tried to put some guideline in the first AIR303 PR I created https://github.com/astral-sh/ruff/pull/14764. Please also tag me when you create the PR. I think the ruff team need our help for confirming the rule part (they'll check the rust part 👍 )

"
2710682243,issue,closed,completed, Airflow 2 to 3 auto migration rules - airflow config (~44080),"### Description

Parent issue: #41641

## airflow config

### Removal
* `smtp`
    * `smtp_user` (from #41539)
    * `smtp_password` (from #41539)
* `webserver`
    * `allow_raw_html_descriptions` (from #40029)
    * `session_lifetime_days` (from #41550) → use `session_lifetime_minutes`
    * `force_log_out_after` (from #41550) → use `session_lifetime_minutes`
* `scheduler`
    * `dependency_detector` (from #41609)
* `operators`
    * `ALLOW_ILLEGAL_ARGUMENTS` (from #41761)
* `metrics`
    * `metrics_use_pattern_match` (from #41975)
    * `timer_unit_consistency` (from #43975)
* `celery`
    * `stalled_task_timeout` (from #42060)
* `core`
    * `check_slas` (from #42285)
    * `strict_dataset_uri_validation` (from #43915)
* `logging`
    * `enable_task_context_logger` (from #43183)
* `traces`
    * `otel_task_log_event` (from #43943)

### Rename
* `scheduler`
    * `processor_poll_interval` →  `cheduler_idle_sleep_time` (from #41096)
* `metrics`
    * `statsd_allow_list` → `metrics_allow_list` (from #42088)
    * `statsd_block_list` → `metrics_block_list` (from #42088)
* cross section
    * `scheduler.statsd_on` → `metrics.statsd_on` (from #42088)
    * `scheduler.statsd_host` → `metrics.statsd_host` (from #42088)
    * `scheduler.statsd_port` → `metrics.statsd_port` (from #42088)
    * `scheduler.statsd_prefix` → `metrics.statsd_prefix` (from #42088)
    * `scheduler.statsd_allow_list` → `metrics.statsd_allow_list` (from #42088)
    * `scheduler.stat_name_handler` → `metrics.stat_name_handler` (from #42088)
    * `scheduler.statsd_datadog_enabled` → `metrics.statsd_datadog_enabled` (from #42088)
    * `scheduler.statsd_datadog_tags` → `metrics.statsd_datadog_tags` (from #42088)
    * `scheduler.statsd_datadog_metrics_tags` → `metrics.statsd_datadog_metrics_tags` (from #42088)
    * `scheduler.statsd_custom_client_path` → 
    * `core.sql_alchemy_conn` → `database.sql_alchemy_conn` (from #42126)
    * `core.sql_engine_encoding` → `database.sql_engine_encoding` (from #42126)
    * `core.sql_engine_collation_for_ids` → `database.sql_engine_collation_for_ids` (from #42126)
    * `core.sql_alchemy_pool_enabled` → `database.sql_alchemy_pool_enabled` (from #42126)
    * `core.sql_alchemy_pool_size` → `database.sql_alchemy_pool_size` (from #42126)
    * `core.sql_alchemy_max_overflow` → `database.sql_alchemy_max_overflow` (from #42126)
    * `core.sql_alchemy_pool_recycle` → `database.sql_alchemy_pool_recycle` (from #42126)
    * `core.sql_alchemy_pool_pre_ping` → `database.sql_alchemy_pool_pre_ping` (from #42126)
    * `core.sql_alchemy_schema` → `database.sql_alchemy_schema` (from #42126)
    * `core.sql_alchemy_connect_args` → `database.sql_alchemy_connect_args` (from #42126)
    * `core.load_default_connections` → `database.load_default_connections` (from #42126)
    *  `core.max_db_retries` → `database.max_db_retries` (from #42126)
    * `core.worker_precheck` → `celery.worker_precheck` (from #42129)
    * `scheduler.max_threads` → `scheduler.parsing_processes` (from #42129)
    * `celery.default_queue` → `operators.default_queue` (from #42129)
    * `admin.hide_sensitive_variable_fields` → `core.hide_sensitive_var_conn_fields` (from #42129)
    * `admin.sensitive_variable_fields` → `core.sensitive_var_conn_names` (from #42129)
    * `core.non_pooled_task_slot_count` → `core.default_pool_task_slot_count` (from #42129)
    * `core.dag_concurrency` → `core.max_active_tasks_per_dag` (from #42129)
    * `api.access_control_allow_origin` → `api.access_control_allow_origins` (from #42129)
    * `api.auth_backend` → `api.auth_backends` (from #42129)
    * `scheduler.deactivate_stale_dags_interval` → `scheduler.parsing_cleanup_interval` (from #42129)
    * `kubernetes_executor.worker_pods_pending_timeout_check_interval` → `scheduler.task_queued_timeout_check_interval` (from #42129)
    * `webserver.update_fab_perms` → `fab.update_fab_perms` (from #42129)
    * `webserver.auth_rate_limited` → `fab.auth_rate_limited` (from #42129)
    * `webserver.auth_rate_limit` → `fab.auth_rate_limit` (from #42129)
* `policy` → `task_policy` (from #41550) 
* `kubernetes` → `kubernetes_executor` (from #42129)

### Use case/motivation

_No response_

### Related issues

https://github.com/apache/airflow/issues/41641

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-12-02 06:28:59+00:00,['sunank200'],2024-12-26 06:54:33+00:00,2024-12-19 08:32:54+00:00,https://github.com/apache/airflow/issues/44555,"[('area:dev-tools', ''), ('kind:feature', 'Feature Requests'), ('area:upgrade', 'Facilitating migration to a newer version of Airflow')]","[{'comment_id': 2541138012, 'issue_id': 2710682243, 'author': 'sunank200', 'body': 'Created the PR for `airflow config lint` : https://github.com/apache/airflow/pull/44908', 'created_at': datetime.datetime(2024, 12, 13, 10, 41, 8, tzinfo=datetime.timezone.utc)}]","sunank200 (Assginee) on (2024-12-13 10:41:08 UTC): Created the PR for `airflow config lint` : https://github.com/apache/airflow/pull/44908

"
2709515233,issue,open,,test_cli_run_time is Quaranitined,test_cli_run_time in `tests/cli/test_cli_parser.py` is quarantined. We need to fix it.,potiuk,2024-12-01 17:06:17+00:00,[],2024-12-01 17:08:32+00:00,,https://github.com/apache/airflow/issues/44539,"[('area:CLI', ''), ('Quarantine', 'Issues that are occasionally failing and are quarantined')]",[],
2709195229,issue,closed,completed,AIP-81 Implement GET Airflow Version in REST API (FastAPI),"### Description

Implement an endpoint `GET` that returns the Apache Airflow version that is installed and running.
i.e. 2.10.3

### Use case/motivation

[AIP-81](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-81+Enhanced+Security+in+CLI+via+Integration+of+API)

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-12-01 13:24:25+00:00,['Ajay-Satish-01'],2024-12-04 20:52:24+00:00,2024-12-04 20:50:48+00:00,https://github.com/apache/airflow/issues/44534,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('good first issue', ''), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]","[{'comment_id': 2510300317, 'issue_id': 2709195229, 'author': 'Ajay-Satish-01', 'body': 'I can work on this. Please assign it to me. Thanks', 'created_at': datetime.datetime(2024, 12, 1, 23, 24, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2518494562, 'issue_id': 2709195229, 'author': 'Ajay-Satish-01', 'body': '@bugraoz93  I would like to know if my thought process is right.\r\n\r\n1. `airflow/api_fastapi/core_api/routes/public/version.py` This path already has the function of returning the version number of airflow.\r\n2. The swagger UI for the route public/version returns  `{ ""version"": ""3.0.0.dev0"",  ""git_version"": null }`\r\n\r\nSo, should I remove the git_version and return version?\r\n\r\nOn the AIP-81 site, it is mentioned that\r\nMissing: Get Airflow version\r\nAdditional Note: This API will only return the Airflow version in the running environment. The CLI version will be locally displayed.', 'created_at': datetime.datetime(2024, 12, 4, 20, 28, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2518533933, 'issue_id': 2709195229, 'author': 'bugraoz93', 'body': '@Ajay-Satish-01 Yes, the endpoint has been implemented. I must have overlooked this—my apologies!\r\nI am closing this one because it already exists. This should be enough, indeed the CLI version will print its own from the system. This endpoint will give us the running environment version Thanks! :) \r\n\r\nNote: I will create more stories on the CLI side when the integration is completed, if you would like to contribute, follow the project board :eyes:', 'created_at': datetime.datetime(2024, 12, 4, 20, 50, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2518536557, 'issue_id': 2709195229, 'author': 'Ajay-Satish-01', 'body': 'Sure. Thanks', 'created_at': datetime.datetime(2024, 12, 4, 20, 52, 22, tzinfo=datetime.timezone.utc)}]","Ajay-Satish-01 (Assginee) on (2024-12-01 23:24:52 UTC): I can work on this. Please assign it to me. Thanks

Ajay-Satish-01 (Assginee) on (2024-12-04 20:28:09 UTC): @bugraoz93  I would like to know if my thought process is right.

1. `airflow/api_fastapi/core_api/routes/public/version.py` This path already has the function of returning the version number of airflow.
2. The swagger UI for the route public/version returns  `{ ""version"": ""3.0.0.dev0"",  ""git_version"": null }`

So, should I remove the git_version and return version?

On the AIP-81 site, it is mentioned that
Missing: Get Airflow version
Additional Note: This API will only return the Airflow version in the running environment. The CLI version will be locally displayed.

bugraoz93 (Issue Creator) on (2024-12-04 20:50:48 UTC): @Ajay-Satish-01 Yes, the endpoint has been implemented. I must have overlooked this—my apologies!
I am closing this one because it already exists. This should be enough, indeed the CLI version will print its own from the system. This endpoint will give us the running environment version Thanks! :) 

Note: I will create more stories on the CLI side when the integration is completed, if you would like to contribute, follow the project board :eyes:

Ajay-Satish-01 (Assginee) on (2024-12-04 20:52:22 UTC): Sure. Thanks

"
2707642504,issue,closed,completed,The test_long_stalled_task_is_killed_by_listener_overtime_if_ol_timeout_long_enough TestOpenLineageExecution fails,"The TestOpenLineageExecution.test_long_stalled_task_is_killed_by_listener_overtime_if_ol_timeout_long_enough
test in providers/tests/openlineage/plugins/test_execution.py is failing after recent changes with local executor job.

It's hard to see what caused it - likely local executor changes for Airlfow 3. Needs an expertise of OpenLineage folks to fix",potiuk,2024-11-30 16:47:49+00:00,[],2024-12-01 16:55:42+00:00,2024-12-01 16:55:42+00:00,https://github.com/apache/airflow/issues/44513,"[('area:providers', ''), ('Quarantine', 'Issues that are occasionally failing and are quarantined'), ('provider:openlineage', 'AIP-53')]","[{'comment_id': 2509035849, 'issue_id': 2707642504, 'author': 'potiuk', 'body': 'cc: @mobuchowski @kacpermuda', 'created_at': datetime.datetime(2024, 11, 30, 16, 49, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2509035951, 'issue_id': 2707642504, 'author': 'potiuk', 'body': 'Easy to repro. Fail consistently every time with latest airflow.', 'created_at': datetime.datetime(2024, 11, 30, 16, 50, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2510016486, 'issue_id': 2707642504, 'author': 'potiuk', 'body': 'Fixed by #44537', 'created_at': datetime.datetime(2024, 12, 1, 16, 55, 37, tzinfo=datetime.timezone.utc)}]","potiuk (Issue Creator) on (2024-11-30 16:49:45 UTC): cc: @mobuchowski @kacpermuda

potiuk (Issue Creator) on (2024-11-30 16:50:06 UTC): Easy to repro. Fail consistently every time with latest airflow.

potiuk (Issue Creator) on (2024-12-01 16:55:37 UTC): Fixed by #44537

"
2707400585,issue,open,,Make ARC / self-hosted runners work,"We should make our ARC/ self-hosted runners infrastructure work back.

We have an almost complete work by @hussein-awala to run self-hosted runners in our AWS account and we have 40K credits graciously donated by Amazon, so we should finally start making use of it and speed up our builds. 

Using huge self-hosted runner and parallelism will allow us to overcome the limitations in the FTR (number of parallel jobs we can run) for the Apache Software Foundation account, and plugging the new runners in with more CPU and memory could help us to handle some of our CI jobs 4x or 8x times faster.

There are a few of the tasks (#41935, #4932, #41934) that depend on ARC being available",potiuk,2024-11-30 14:21:43+00:00,['hussein-awala'],2024-12-24 08:28:56+00:00,,https://github.com/apache/airflow/issues/44512,"[('area:self-hosted-runners', '')]",[],
2707381976,issue,open,,Separate providers into sub-projects,"Each provider should have it's own independed sub-project in our mono-repo.

They should be fully ""standalone"" so that you can not only develop them completely independently from airflow core, but also that all the dependencies of thoseshould be stored in their own pyproject.toml. And `uv workspace` feature should be used to bind together all the provider sub-projects so that you can continue the development where you have airflow, task-sdk, providers and all the other sub-projects of Airlflow monorepo together in a single editable environment.

This move has been attempted earlier in https://github.com/apache/airflow/pull/28292 and https://github.com/apache/airflow/pull/28291 - but we did not have a good ""workspace"" solution and Airflow 2 namespace approach prevented us from making it good environment for provider development. With Airflow 3 and `uv workspace` feature that has been added - largely with our input so that Airlfow's provider structure could benefit from the `uv workspace` functionality, it's now entirely possible to do.

This means that dependencies should be moved from `providrer.yaml` files to `pyproject.toml`

The ideal setup there is to have this kind of structure (details to be worked out):

```
providers/
         providers-amazon/
                        src
                        tests-integration
                        tests-system
                        tests
                        docs
                        pyproject.toml
                        ...
```

Some important properties of the solution:

* Airflow ""core"" projects should not rely on providers being installed
* It should be possible to install all airflow core packages and providers and synchronize/resolves wit `uv sync
* it should be possible to install provider in `--editable` mode treating it as separate project from the workspace
* It should be possible to install provider  with GitHub URL 
* docs/ all kinds of tests, images etc. should all work independently (though thanks to monorepo, we can keep the code to run those in `breeze` as we do currently. Some of the current `doc` code will need to be moved to breeze as well for that likely
* we should be able to apply pyproject.toml changes for all providers automatically (might be semi-automated or with pre-commit). Quite often we make ""global"" changes there that affect all providers - and currently it is done via modifying breeze and templates for dynamically generated pyproject.toml file
* we need to keep reproducibility of provider packages intact - which likely means that they should be still generated with breeze - with all the ""extra"" stuff such as making sure we have controlled package build environment.
* we will need to change building of packages in CI in `docker container` environment - while currently we use `flit` as build backend and this comes from generated `pyproject.toml` that is placed inside breeze, if we keep pyproject.toml files in the repository, incoming PRs from forks might change build backends and thus inject any code in our build process

The most likely way to implement it is to:

1. manually convert one / few representative but not biggest providers first (POC) - and make a few releases with those - while updating our breeze automation to work in both cases - that will allow to iron out some teething problems
2. develop automation for converting the providers - similar to https://github.com/apache/airflow/pull/28291
3. perform the test if the `uv workspace` feature is usable at scale of 100+ projects bound together (and work with `uv` team to fix it if not)
4. apply - rather quickly, but incrementally - the automation to all the providers of ours - while letting all the in-progress contributors about the changes upfront and explaing what needs to be done
",potiuk,2024-11-30 13:50:15+00:00,['potiuk'],2024-12-24 08:26:03+00:00,,https://github.com/apache/airflow/issues/44511,"[('area:providers', '')]","[{'comment_id': 2526337959, 'issue_id': 2707381976, 'author': 'SuccessMoses', 'body': 'sounds like a good idea', 'created_at': datetime.datetime(2024, 12, 8, 19, 11, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2527024353, 'issue_id': 2707381976, 'author': 'uranusjr', 'body': 'Sounds good to me to. At this point I wonder if we should also move core Airflow into a subdirectory.', 'created_at': datetime.datetime(2024, 12, 9, 6, 9, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2527270466, 'issue_id': 2707381976, 'author': 'potiuk', 'body': '> Sounds good to me to. At this point I wonder if we should also move core Airflow into a subdirectory.\r\n\r\nAbsolutely.', 'created_at': datetime.datetime(2024, 12, 9, 8, 37, 53, tzinfo=datetime.timezone.utc)}]","SuccessMoses on (2024-12-08 19:11:26 UTC): sounds like a good idea

uranusjr on (2024-12-09 06:09:51 UTC): Sounds good to me to. At this point I wonder if we should also move core Airflow into a subdirectory.

potiuk (Issue Creator) on (2024-12-09 08:37:53 UTC): Absolutely.

"
2707307986,issue,open,,Improve K8S tests iteration experience,"As also experienced by @ashb in https://github.com/apache/airflow/pull/44463, our K8S tests are rather slow to iterate on and fix locally. 

The way how to run those tests locally is explained in https://github.com/apache/airflow/blob/main/contributing-docs/testing/k8s_tests.rst#typical-testing-pattern-for-kubernetes-tests in detail - but there are a fewproblems to solve with them:

* The k8s image needs to use PROD base image to be build first and - especially when dependencies change - sometimes, but not always the PROD image needs to be rebuilt with `--rebuild-base-image` flag of `breeze k8s build-k8s-image`. This might cause the Helm chart to not work at all (for example it failed when `asyncpg` was not installed in the base image

* Currently when you modify the `./kubernetest_tests` - i.e. tests that you run in the local k8s-dedicated virtulenv environment specially prepared for those tests, they can be re-run right after modification - which is great to iterate on the tests. However, this is NOT the case for ""airflow"" code that you modify and iterate on. While thisis understandable that if you modify helm chart, you need to redeploy the chart, you also need to rebuild the ""k8s image"" and re-build and re-upload the image to the kind cluster in order to iterate on your changes. This is SLOW LIKE HELL. Rebuilding the image, re-uploading it and re-deploying chart to reload airflow takes minutes. 

So in order to fix it we can do several things:

* better detection when we need to re-build the image because of dependency change
 
* using uv to install dependencies in PROD image as well (currently it is only used by default in CI image) - this way we could likely even rebuild the image more often as it wil be faster
 
* add extra layer to add missing dependencies (with UV) in the K8S image: https://github.com/apache/airflow/blob/0d6d5f23a2e3861e49b3ea5effeeeb42db4ded94/dev/breeze/src/airflow_breeze/commands/kubernetes_commands.py#L602 - which means that we could continue to only rebuilding the k8s image (without `--rebuild-base-image` flag even if dependencies change)
 
* we could integrate solutions (or build our own) like Telepresence: https://www.telepresence.io/ where local sources of airflow would be synced (via ssh) with the sources in the k8s cluster (for example via local folder mounting or SSH) - this is a bit difficult to implement it to work in all circumstances (Linux/ MacOS/ WSL2) and all kind of docker engines our users might have (Docker Desktop, Orbstack, Podman  etc.) - so doing it **righ** is a bit of a challenge. But that should be an ultimate way of iteration where changes to airflow and kuberntes providers could be followed by just pod restart and no need to build an upload new image.
",potiuk,2024-11-30 13:03:31+00:00,[],2024-11-30 13:05:43+00:00,,https://github.com/apache/airflow/issues/44508,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:CI', ""Airflow's tests and continious integration"")]",[],
2707104861,issue,open,,Implement OpenLineage support for DataprocSubmitJobOperator,"### Description

_No response_

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",spapi17,2024-11-30 09:54:35+00:00,[],2024-11-30 09:56:48+00:00,,https://github.com/apache/airflow/issues/44506,"[('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:openlineage', 'AIP-53')]","[{'comment_id': 2508906471, 'issue_id': 2707104861, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 30, 9, 54, 37, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-30 09:54:37 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2705862243,issue,closed,completed,"Port the DAG parsing code over to use SDK ""properly""","Port the DAG parsing code over to use SDK ""properly"" including getting Connections, Variables etc",kaxil,2024-11-29 18:33:19+00:00,['ashb'],2024-12-20 10:16:22+00:00,2024-12-20 10:16:22+00:00,https://github.com/apache/airflow/issues/44487,"[('area:DAG-processing', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]","[{'comment_id': 2527324273, 'issue_id': 2705862243, 'author': 'kaxil', 'body': 'WIP code: https://github.com/apache/airflow/compare/main...astronomer:airflow:dag-parsing-uses-task-sdk', 'created_at': datetime.datetime(2024, 12, 9, 9, 3, 3, tzinfo=datetime.timezone.utc)}]","kaxil (Issue Creator) on (2024-12-09 09:03:03 UTC): WIP code: https://github.com/apache/airflow/compare/main...astronomer:airflow:dag-parsing-uses-task-sdk

"
2705591627,issue,closed,completed,Missing significant newsfragment for standard provider,"### What do you see as an issue?

I think this could be considered a breaking change as users won't be able to import these operators in the old way. These are the prs I notice

https://github.com/apache/airflow/pull/41564
https://github.com/apache/airflow/pull/42506
https://github.com/apache/airflow/pull/42794
https://github.com/apache/airflow/pull/43608
https://github.com/apache/airflow/pull/43890
https://github.com/apache/airflow/pull/44053
https://github.com/apache/airflow/pull/44288 

### Solving the problem

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-11-29 16:32:23+00:00,['Lee-W'],2025-01-20 09:26:53+00:00,2025-01-20 09:26:53+00:00,https://github.com/apache/airflow/issues/44482,"[('kind:bug', 'This is a clearly a bug'), ('kind:documentation', '')]","[{'comment_id': 2508373491, 'issue_id': 2705591627, 'author': 'potiuk', 'body': 'BTW. The idea is that we redirect old imports to new standard provider automatically when we complete the provider move, but yes, newsfragment for that one will be needed.', 'created_at': datetime.datetime(2024, 11, 29, 19, 1, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2588466091, 'issue_id': 2705591627, 'author': 'Lee-W', 'body': '>  The idea is that we redirect old imports to new standard provider automatically when we complete the provider move\n\nI guess it means we do not need a migration rule for this then?', 'created_at': datetime.datetime(2025, 1, 13, 23, 42, 5, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-11-29 19:01:43 UTC): BTW. The idea is that we redirect old imports to new standard provider automatically when we complete the provider move, but yes, newsfragment for that one will be needed.

Lee-W (Issue Creator) on (2025-01-13 23:42:05 UTC): I guess it means we do not need a migration rule for this then?

"
2705579708,issue,closed,completed,"Pass ""real"" TaskInstance object with xcom methods in context to task execute functions","The K8s system tests are running https://github.com/apache/airflow/blob/cd5ccf0abf5027cf457b36a8a2bee397dcb538b4/airflow/example_dags/example_xcom.py and this is currently failing as we don't pass `ti` to the operator `execute()` function.

(We currently pass `task_isntance,` not `ti`, but even passing it as `ti` wouldn't work as the simple class we have for TI in TaskSDK doesn't have `xcom_pull` or `xcom_push` methods.

Sub-tasks:

- https://github.com/apache/airflow/pull/44894
- https://github.com/apache/airflow/pull/44899
- https://github.com/apache/airflow/pull/45043
- https://github.com/apache/airflow/pull/45075
- https://github.com/apache/airflow/pull/45112
- [x] #45230
- [x] #45232
- [x] #45234
- [x] #45243",ashb,2024-11-29 16:25:24+00:00,['kaxil'],2025-01-16 19:04:44+00:00,2025-01-16 19:04:44+00:00,https://github.com/apache/airflow/issues/44481,"[('area:core', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK'), ('area:task-sdk', None)]","[{'comment_id': 2532826658, 'issue_id': 2705579708, 'author': 'kaxil', 'body': 'The current values in the Context dict:\n\n- https://github.com/apache/airflow/blob/b3f36493aec69e34d68a248ec8257bff498665a5/airflow/models/taskinstance.py#L1009-L1048\n- Template ref: https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html\n\nThe plan is to:\n\n- Get certain `key:value` pairs from the API Server and pass it as a response to start the request to the API server (with `TIEnterRunningPayload`)\n- Augment it with `key:value` pairs from the Task Execution Interface (execution side of the Task SDK)\n\nAs a goal: I am planning to get as much info as possible on the Execution side itself as opposed to getting it from the API server', 'created_at': datetime.datetime(2024, 12, 10, 20, 33, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2537364046, 'issue_id': 2705579708, 'author': 'kaxil', 'body': 'WIP branch: https://github.com/apache/airflow/compare/main...astronomer:airflow:task-context?expand=1', 'created_at': datetime.datetime(2024, 12, 11, 22, 52, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563440240, 'issue_id': 2705579708, 'author': 'kaxil', 'body': 'Next set of sub-tasks:\r\n\r\n- #45230\r\n- #45231\r\n- #45232\r\n- #45233\r\n- #45234\r\n\r\nWIP Branch: https://github.com/apache/airflow/compare/main...astronomer:airflow:get-xcom-arg-working?expand=1', 'created_at': datetime.datetime(2024, 12, 27, 8, 10, 21, tzinfo=datetime.timezone.utc)}]","kaxil (Assginee) on (2024-12-10 20:33:17 UTC): The current values in the Context dict:

- https://github.com/apache/airflow/blob/b3f36493aec69e34d68a248ec8257bff498665a5/airflow/models/taskinstance.py#L1009-L1048
- Template ref: https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html

The plan is to:

- Get certain `key:value` pairs from the API Server and pass it as a response to start the request to the API server (with `TIEnterRunningPayload`)
- Augment it with `key:value` pairs from the Task Execution Interface (execution side of the Task SDK)

As a goal: I am planning to get as much info as possible on the Execution side itself as opposed to getting it from the API server

kaxil (Assginee) on (2024-12-11 22:52:18 UTC): WIP branch: https://github.com/apache/airflow/compare/main...astronomer:airflow:task-context?expand=1

kaxil (Assginee) on (2024-12-27 08:10:21 UTC): Next set of sub-tasks:

- #45230
- #45231
- #45232
- #45233
- #45234

WIP Branch: https://github.com/apache/airflow/compare/main...astronomer:airflow:get-xcom-arg-working?expand=1

"
2704439463,issue,closed,completed,airflow UI failed to read correct worker host in logs after multiple retries.,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I have a latest (2.10.3) airflow installed in k8s. 
Executors are celeryExecutors.
I have two workers deployed. 
- airflow-worker-0
- airflow-worker-1

I find out that if a task failed and retry, the log address `host` part will be overwritten by the last attemp.
Thus if the task retired on different workers, the log on machines different from the last worker will be unavaliable.
But if I go to the work node, I can find the log in the local path.

This attemp failed to get the log. Note that this actually run on airflow-worker-1 and logs are avaliable on local path
![image](https://github.com/user-attachments/assets/e43a3ce8-6e8f-4690-9fda-aaba125f0ac8)
airflow-worker-1 local path
![image](https://github.com/user-attachments/assets/be76ffb4-1afe-4983-a6e9-54fd88a66579)

This is because the host is over written by the last attempt:
![image](https://github.com/user-attachments/assets/c25469e9-7c8d-494e-a44f-424e619c8d9b)

I think the code here in `airflow/utils/log/file_task_handler.py:461` should be changed as it uses the last task_instance for all the attempts.
```
 def read(self, task_instance, try_number=None, metadata=None):
        """"""
        Read logs of given task instance from local machine.

        :param task_instance: task instance object
        :param try_number: task instance try_number to read logs from. If None
                           it returns all logs separated by try_number
        :param metadata: log metadata, can be used for steaming log reading and auto-tailing.
        :return: a list of listed tuples which order log string by host
        """"""
        # Task instance increments its try number when it starts to run.
        # So the log for a particular task try will only show up when
        # try number gets incremented in DB, i.e logs produced the time
        # after cli run and before try_number + 1 in DB will not be displayed.
        if try_number is None:
            next_try = task_instance.next_try_number
            try_numbers = list(range(1, next_try))
        elif try_number < 1:
            logs = [
                [(""default_host"", f""Error fetching the logs. Try number {try_number} is invalid."")],
            ]
            return logs, [{""end_of_log"": True}]
        else:
            try_numbers = [try_number]

        logs = [""""] * len(try_numbers)
        metadata_array = [{}] * len(try_numbers)

        # subclasses implement _read and may not have log_type, which was added recently
        for i, try_number_element in enumerate(try_numbers):
            log, out_metadata = self._read(task_instance, try_number_element, metadata)
            # es_task_handler return logs grouped by host. wrap other handler returning log string
            # with default/ empty host so that UI can render the response in the same way
            logs[i] = log if self._read_grouped_logs() else [(task_instance.hostname, log)]
            metadata_array[i] = out_metadata

        return logs, metadata_array
```

### What you think should happen instead?

Each attempt should has its own host.

### How to reproduce

- latest (2.10.3) airflow installed in k8s. 
- Executors are celeryExecutors.
- Multiple workers deployed. 
- Task failed and retied on different wokers


### Operating System

Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",ranmx,2024-11-29 09:03:02+00:00,[],2024-11-29 14:25:52+00:00,2024-11-29 14:25:52+00:00,https://github.com/apache/airflow/issues/44474,"[('kind:bug', 'This is a clearly a bug'), ('area:logging', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2507367283, 'issue_id': 2704439463, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 29, 9, 3, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507922072, 'issue_id': 2704439463, 'author': 'potiuk', 'body': 'This can be easily solved (and is recommended now) by having a shared volume where you keep logs. We are not likely to address it because we are completely changing it for Airflow 3 - and logs will be retrieved via Task API server, not via workers most likely, so we can convert that to a discussion now.', 'created_at': datetime.datetime(2024, 11, 29, 14, 25, 46, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-29 09:03:06 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-11-29 14:25:46 UTC): This can be easily solved (and is recommended now) by having a shared volume where you keep logs. We are not likely to address it because we are completely changing it for Airflow 3 - and logs will be retrieved via Task API server, not via workers most likely, so we can convert that to a discussion now.

"
2704432671,issue,closed,completed,Add Ruff Rule to Prevent Dynamic Dates in DAG Params,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

During AIP-65 testing, we observed that using dynamic dates in DAG parameters leads to the continuous creation of multiple DAG versions. This behavior can cause clutter dag_version table.



### What you think should happen instead?

Ideally, dynamic dates should not be used in DAG parameters. To address this, we can add a Ruff rule to identify and prevent the use of dynamic dates in DAG params, ensuring better practices and avoiding unnecessary version proliferation.

### How to reproduce

1. Use the below DAG.

```
from airflow.decorators import dag, task
from datetime import datetime, timedelta
import pendulum
from airflow.models.param import Param

default_args = {
    ""owner"": ""airflow"",

    ""retries"": 1,
    ""retry_delay"": timedelta(minutes=1),
}

@dag(description=""Create and read a file"", schedule='@daily', catchup=False, tags=['Atul_Practice11'], start_date=datetime(2024, 1, 1),default_args=default_args,  params={
        ""clean_before_timestamp"": Param(
            default=(datetime.now(tz=pendulum.UTC) - timedelta(days=90)).isoformat(),
            type=""string"",
            format=""date-time"",
            description=""Delete records older than this timestamp. Default is 90 days ago."",
        )})
def check_dag_retry():

    @task
    def read_file():
        pass
    read_file()
check_dag_retry()
```
2. Check DAG Version table 

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vatsrahul1001,2024-11-29 09:00:12+00:00,['vatsrahul1001'],2025-02-08 05:45:42+00:00,2025-02-08 05:45:42+00:00,https://github.com/apache/airflow/issues/44473,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('affected_version:3.0.0alpha', 'For all 3.0.0 alpha releases')]","[{'comment_id': 2618132899, 'issue_id': 2704432671, 'author': 'eladkal', 'body': '@vatsrahul1001 are you still working on it?', 'created_at': datetime.datetime(2025, 1, 28, 7, 43, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641791169, 'issue_id': 2704432671, 'author': 'vatsrahul1001', 'body': '> [@vatsrahul1001](https://github.com/vatsrahul1001) are you still working on it?\n\n@eladkal missed a notification somehow on this. Yes, I am working on this.', 'created_at': datetime.datetime(2025, 2, 7, 2, 39, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2644521274, 'issue_id': 2704432671, 'author': 'vatsrahul1001', 'body': 'closing this in favour of https://github.com/apache/airflow/issues/45524 as it will be a better solution for the issue as discussed with @jedcunningham.', 'created_at': datetime.datetime(2025, 2, 8, 5, 43, 34, tzinfo=datetime.timezone.utc)}]","eladkal on (2025-01-28 07:43:07 UTC): @vatsrahul1001 are you still working on it?

vatsrahul1001 (Issue Creator) on (2025-02-07 02:39:18 UTC): @eladkal missed a notification somehow on this. Yes, I am working on this.

vatsrahul1001 (Issue Creator) on (2025-02-08 05:43:34 UTC): closing this in favour of https://github.com/apache/airflow/issues/45524 as it will be a better solution for the issue as discussed with @jedcunningham.

"
2704298360,issue,closed,completed,`airflow.secrets.base_secrets.BaseSecretsBackend.get_conn_uri` → `airflow.secrets.base_secrets.BaseSecretsBackend.get_conn_value`,,Lee-W,2024-11-29 08:12:35+00:00,[],2024-11-29 08:12:45+00:00,2024-11-29 08:12:45+00:00,https://github.com/apache/airflow/issues/44472,[],[],
2701639579,issue,closed,completed,Using psycopg3 in PostgresHook,"### Description

Hi, I want to ask is there any plan to use psycopg3 instead of psycopg2 in PostgresHook? 

### Use case/motivation

There are a lot of performance improvements in psycopg3

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",mfatemipour,2024-11-28 10:24:29+00:00,[],2024-11-28 12:46:43+00:00,2024-11-28 12:46:43+00:00,https://github.com/apache/airflow/issues/44452,"[('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:postgres', '')]","[{'comment_id': 2506040392, 'issue_id': 2701639579, 'author': 'potiuk', 'body': 'No plans as far as I am concerned but we have no ""plans"" for that. In Open Source things are done when someone does it. Ant you are absolutely free to propose a PR. Airflow has > 3000 contributors and you can become one of them easily.\r\n\r\nBTW. When you ask a question, you should not crete an issue -> as we explain in the template which you got when you attempt to create an issue, you should create a discussion instead. I convert it to a discussion now in case more discussion is neded.', 'created_at': datetime.datetime(2024, 11, 28, 12, 46, 39, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-11-28 12:46:39 UTC): No plans as far as I am concerned but we have no ""plans"" for that. In Open Source things are done when someone does it. Ant you are absolutely free to propose a PR. Airflow has > 3000 contributors and you can become one of them easily.

BTW. When you ask a question, you should not crete an issue -> as we explain in the template which you got when you attempt to create an issue, you should create a discussion instead. I convert it to a discussion now in case more discussion is neded.

"
2701622884,issue,open,,AIP-65: || Previous DAG_VERSION_ID is displayed after clearing task on task_instance detail page.,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When a task is cleared after changes in DAG code It takes the latest DAG version code, however, on task details page its still showing previous DAG_VERSION_ID,

### What you think should happen instead?

Task details page should show the latest DAG_VERSION_ID.

### How to reproduce

1. Create a DAG RUN .
2. Make some changes in DAG after #1 run is completed.
3. Now clear the existing run task.
4. Check DAG Version on task details page is still showing  pervious version.

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vatsrahul1001,2024-11-28 10:17:55+00:00,['ephraimbuddy'],2025-02-06 22:30:52+00:00,,https://github.com/apache/airflow/issues/44451,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('affected_version:3.0.0alpha', 'For all 3.0.0 alpha releases')]",[],
2701196085,issue,closed,completed,"Add ""Post variable"" endpoint for execution API","### Body

Similar to https://github.com/apache/airflow/pull/43832 but for setting a variable

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",amoghrajesh,2024-11-28 07:48:05+00:00,['amoghrajesh'],2024-11-29 06:37:46+00:00,2024-11-29 06:37:46+00:00,https://github.com/apache/airflow/issues/44446,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2700639254,issue,open,reopened,Airflow keeps on increasing duration of dag if it is paused after it went to running state.,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

 v2.10.1

### What happened?

If dags are paused after they start running their duration keeps on increasing.
Due to this the Cluster Activity shows top 5 longest dags which were paused and are not even active.

### What you think should happen instead?

Pausing a dag should reset the status from running to scheduled or queued or no state.
Cluster Activity longest dags should exclude pause dags.

### How to reproduce

1. Trigger a dag
2. Once it starts running pause the dag.
3. Check the duration it will keep on increasing even though it is paused.
4. After few days check the cluster activity and paused dags will eventually come to the top as the long running dag.

### Operating System

AKS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

AKS
Airflow 2.10.1

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Raul824,2024-11-28 03:32:05+00:00,[],2025-01-07 19:40:42+00:00,,https://github.com/apache/airflow/issues/44443,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2506424640, 'issue_id': 2700639254, 'author': 'xionams', 'body': '@potiuk assign me ✨', 'created_at': datetime.datetime(2024, 11, 28, 15, 59, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2506503998, 'issue_id': 2700639254, 'author': 'eladkal', 'body': 'There is a product desicion to make here. I am not convinced current behavior considered a bug.\r\n\r\nI think we need to resolve https://github.com/apache/airflow/issues/22006 first', 'created_at': datetime.datetime(2024, 11, 28, 16, 49, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2506619376, 'issue_id': 2700639254, 'author': 'potiuk', 'body': 'I think those are unrelated - that one seems clearly as a bug - because paused DAGS that are not running are shown on cluster duration as long running. The #22006 is a completely new feature (""draining"" running DAGs) that has very little to do with the cluster activity displaying misleading information (or at least this is how I read it).', 'created_at': datetime.datetime(2024, 11, 28, 18, 20, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2506688505, 'issue_id': 2700639254, 'author': 'eladkal', 'body': '> I think those are unrelated - that one seems clearly as a bug - because paused DAGS that are not running are shown on cluster duration as long running.\r\n\r\nIs this accurate?\r\nWhile dag is paused task can in fact continue to run. Since pausing doesnt invoke `on_kill()` remote jobs continue to run.\r\nThere are use cases of levaring pause for temporary drain. I used it several times and it was great that metrics reflected it.\r\n\r\nMy point is that Cluster Activity is the symptom not the real problem. We need to decide what is the right overall behavior of paused (which is why I linked to #22006), as a result cluster activity will be fixed accordingly.', 'created_at': datetime.datetime(2024, 11, 28, 19, 37, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2506750308, 'issue_id': 2700639254, 'author': 'potiuk', 'body': '> My point is that Cluster Activity is the symptom not the real problem. We need to decide what is the right overall behavior of paused (which is why I linked to https://github.com/apache/airflow/issues/22006), as a result cluster activity will be fixed accordingly.\r\n\r\nI think this is not the case at all in this issue. What I understand is that cluster activity shows running time for paused event that are not running any more. This is at least what description of the issue is about.', 'created_at': datetime.datetime(2024, 11, 28, 20, 57, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2506750561, 'issue_id': 2700639254, 'author': 'potiuk', 'body': 'But maybe @Raul824 -> maybe you can clarify that?', 'created_at': datetime.datetime(2024, 11, 28, 20, 57, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507299827, 'issue_id': 2700639254, 'author': 'eladkal', 'body': '> I think this is not the case at all in this issue. What I understand is that cluster activity shows running time for paused event that are not running any more. This is at least what description of the issue is about.\r\n\r\nI consider this to be a symptom of a larger issue: what it means running + pausing but I get your point. For this specific issue we can just exclude paused runs from the Cluster Activity Top Running view @xionams would you like to raise a PR for this?', 'created_at': datetime.datetime(2024, 11, 29, 8, 19, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507860594, 'issue_id': 2700639254, 'author': 'potiuk', 'body': '> I consider this to be a symptom of a larger issue: what it means running + pausing but I get your point. For this specific issue we can just exclude paused runs from the Cluster Activity Top Running view @xionams would you like to raise a PR for this?\r\n\r\nYeah. I think those two are related but different.', 'created_at': datetime.datetime(2024, 11, 29, 13, 50, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507879440, 'issue_id': 2700639254, 'author': 'Raul824', 'body': ""Well I noticed this while looking at cluster activity, but isn't paused dags which went to running also using the resources of updating the duration as well.\r\n\r\nAnd pausing the dag doesn't actually kills the process which has started that I have noticed.\r\n\r\nSo I think, if pausing functionality can be fixed to reset the status of the dag from running to queued or no state that would solve these issues.\r\n\r\nAs for killing of already running tasks that is provided by marking a task as failed, the functionality of pause is fine at the moment of pausing the dag to not run further which is happening partially as tasks are not starting but dag state is also stuck on running in these cases which I think is a bug."", 'created_at': datetime.datetime(2024, 11, 29, 14, 1, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2513805076, 'issue_id': 2700639254, 'author': 'xionams', 'body': ""> > I think this is not the case at all in this issue. What I understand is that cluster activity shows running time for paused event that are not running any more. This is at least what description of the issue is about.\r\n> \r\n> I consider this to be a symptom of a larger issue: what it means running + pausing but I get your point. For this specific issue we can just exclude paused runs from the Cluster Activity Top Running view @xionams would you like to raise a PR for this?\r\n\r\nI'm just into something, if we no need to rush, I will happily raise PR"", 'created_at': datetime.datetime(2024, 12, 3, 8, 3, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2513833985, 'issue_id': 2700639254, 'author': 'eladkal', 'body': ""> I'm just into something, if we no need to rush, I will happily raise PR\r\n\r\nRaising a PR to fix the cluster activity (Top 5 running dags) is step forward."", 'created_at': datetime.datetime(2024, 12, 3, 8, 19, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2576089139, 'issue_id': 2700639254, 'author': 'willdanckwerts', 'body': ""I tested this via the web UI, running a DAG that simply uses time.sleep for 60 seconds. When I paused the DAG, not only did its time continue to increment in the Cluster Activity screen, once it ran for a bit beyond 60 seconds, its actually changed status to 'success' e.g. the run finished.\r\n\r\nIf appropriate I would like to try and work on this, although it would be my first issue so I might not be too reliable on timescales."", 'created_at': datetime.datetime(2025, 1, 7, 19, 40, 41, tzinfo=datetime.timezone.utc)}]","xionams on (2024-11-28 15:59:01 UTC): @potiuk assign me ✨

eladkal on (2024-11-28 16:49:05 UTC): There is a product desicion to make here. I am not convinced current behavior considered a bug.

I think we need to resolve https://github.com/apache/airflow/issues/22006 first

potiuk on (2024-11-28 18:20:06 UTC): I think those are unrelated - that one seems clearly as a bug - because paused DAGS that are not running are shown on cluster duration as long running. The #22006 is a completely new feature (""draining"" running DAGs) that has very little to do with the cluster activity displaying misleading information (or at least this is how I read it).

eladkal on (2024-11-28 19:37:28 UTC): Is this accurate?
While dag is paused task can in fact continue to run. Since pausing doesnt invoke `on_kill()` remote jobs continue to run.
There are use cases of levaring pause for temporary drain. I used it several times and it was great that metrics reflected it.

My point is that Cluster Activity is the symptom not the real problem. We need to decide what is the right overall behavior of paused (which is why I linked to #22006), as a result cluster activity will be fixed accordingly.

potiuk on (2024-11-28 20:57:23 UTC): I think this is not the case at all in this issue. What I understand is that cluster activity shows running time for paused event that are not running any more. This is at least what description of the issue is about.

potiuk on (2024-11-28 20:57:45 UTC): But maybe @Raul824 -> maybe you can clarify that?

eladkal on (2024-11-29 08:19:21 UTC): I consider this to be a symptom of a larger issue: what it means running + pausing but I get your point. For this specific issue we can just exclude paused runs from the Cluster Activity Top Running view @xionams would you like to raise a PR for this?

potiuk on (2024-11-29 13:50:19 UTC): Yeah. I think those two are related but different.

Raul824 (Issue Creator) on (2024-11-29 14:01:22 UTC): Well I noticed this while looking at cluster activity, but isn't paused dags which went to running also using the resources of updating the duration as well.

And pausing the dag doesn't actually kills the process which has started that I have noticed.

So I think, if pausing functionality can be fixed to reset the status of the dag from running to queued or no state that would solve these issues.

As for killing of already running tasks that is provided by marking a task as failed, the functionality of pause is fine at the moment of pausing the dag to not run further which is happening partially as tasks are not starting but dag state is also stuck on running in these cases which I think is a bug.

xionams on (2024-12-03 08:03:41 UTC): I'm just into something, if we no need to rush, I will happily raise PR

eladkal on (2024-12-03 08:19:07 UTC): Raising a PR to fix the cluster activity (Top 5 running dags) is step forward.

willdanckwerts on (2025-01-07 19:40:41 UTC): I tested this via the web UI, running a DAG that simply uses time.sleep for 60 seconds. When I paused the DAG, not only did its time continue to increment in the Cluster Activity screen, once it ran for a bit beyond 60 seconds, its actually changed status to 'success' e.g. the run finished.

If appropriate I would like to try and work on this, although it would be my first issue so I might not be too reliable on timescales.

"
2700155015,issue,closed,completed,deleted issue,,npatel44,2024-11-27 23:17:13+00:00,[],2024-11-27 23:37:23+00:00,2024-11-27 23:36:46+00:00,https://github.com/apache/airflow/issues/44438,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('area:logging', ''), ('pending-response', ''), ('area:core', ''), ('provider:common-sql', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2504965949, 'issue_id': 2700155015, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 27, 23, 17, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2504980237, 'issue_id': 2700155015, 'author': 'potiuk', 'body': 'First of all - please report such issues following https://github.com/apache/airflow/security/policy not via public issue. Reporting them in public is unresponsibe disclosure. Can you please send details of your disovery to the email address specified in the policy and provide there examples of your code that leads to this including showing exactly logs that you saw it.', 'created_at': datetime.datetime(2024, 11, 27, 23, 36, 47, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-27 23:17:16 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-11-27 23:36:47 UTC): First of all - please report such issues following https://github.com/apache/airflow/security/policy not via public issue. Reporting them in public is unresponsibe disclosure. Can you please send details of your disovery to the email address specified in the policy and provide there examples of your code that leads to this including showing exactly logs that you saw it.

"
2700126376,issue,closed,completed,Removal of AIP-44 code,"### Body

We eventually decided to drop AIP-44. We are about to remove it and here is the list of things that should be removed:


-  [x] removal of ""enble-aip-44"" controlled parts of CI workflows and breeze 
-  [x] removal of `--database-isolation` Breeze flag (and corresponding scripts/test shennigans
-  [x] removal of `AIRFLOW_ENABLE_AIP_44` occurrences (ci/code) and `_ENABLE_AIP_44` , `InternalApiConfig`, `database_access_isolation`
- [x] https://github.com/apache/airflow/pull/44463
-  [x] removal of ""internal_api_v1.yaml"" and ""internal-api"" CLI
-  [x] removal of @internal_api_call decorators (82 methods)  - re-join back methods that were separated out from the main code - when methods start with `_`:
   - [x] ./task_sdk/src/airflow/sdk/definitions/asset/__init__.py: expand_alias_to_assets
   - [x] ./airflow/secrets/metastore.py:     _fetch_connection
   - [x] ./airflow/secrets/metastore.py:     _fetch_variable
   - [x] ./airflow/jobs/job.py:     _kill
   - [x] ./airflow/jobs/job.py:     _fetch_from_db
   - [x] ./airflow/jobs/job.py:     _add_to_db
   - [x] ./airflow/jobs/job.py:     _update_in_db
   - [x] ./airflow/jobs/job.py:     _update_heartbeat
   - [x] ./airflow/jobs/job.py: most_recent_job
   - [x] ./airflow/api/common/trigger_dag.py: trigger_dag
   - [x] ./airflow/assets/manager.py:     register_asset_change
   - [x] ./airflow/dag_processing/processor.py:     update_import_errors
   - [x] ./airflow/dag_processing/processor.py:     _validate_task_pools_and_update_dag_warnings
   - [x] ./airflow/dag_processing/processor.py:     execute_callbacks
   - [x] ./airflow/dag_processing/processor.py:     execute_callbacks_without_dag
   - [x] ./airflow/dag_processing/processor.py:     _execute_task_callbacks
   - [x] ./airflow/dag_processing/processor.py:     save_dag_to_db
   - [x] ./airflow/dag_processing/manager.py:     deactivate_stale_dags
   - [x] ./airflow/dag_processing/manager.py:     _fetch_callbacks
   - [x] ./airflow/dag_processing/manager.py:     _get_priority_filelocs
   - [x] ./airflow/dag_processing/manager.py:     clear_nonexistent_import_errors
   - [x] ./airflow/models/dagrun.py:     fetch_task_instances
   - [x] ./airflow/models/dagrun.py:     _check_last_n_dagruns_failed
   - [x] ./airflow/models/dagrun.py:     fetch_task_instance
   - [x] ./airflow/models/dagrun.py:     get_previous_dagrun
   - [x] ./airflow/models/dagrun.py:     get_previous_scheduled_dagrun
   - [x] ./airflow/models/dagrun.py:     _get_log_template
   - [x] ./airflow/models/trigger.py:     from_object
   - [x] ./airflow/models/trigger.py:     bulk_fetch
   - [x] ./airflow/models/trigger.py:     clean_unused
   - [x] ./airflow/models/trigger.py:     submit_event
   - [x] ./airflow/models/trigger.py:     submit_failure
   - [x] ./airflow/models/trigger.py:     ids_for_triggerer
   - [x] ./airflow/models/trigger.py:     assign_unassigned
   - [x] ./airflow/models/xcom_arg.py: _get_task_map_length
   - [x] ./airflow/models/renderedtifields.py:     _update_runtime_evaluated_template_fields
   - [x] ./airflow/models/serialized_dag.py:     get_serialized_dag
   - [x] ./airflow/models/dag.py:     fetch_callback
   - [x] ./airflow/models/dag.py:     fetch_dagrun
   - [x] ./airflow/models/dag.py:     get_current
   - [x] ./airflow/models/dag.py:     get_paused_dag_ids
   - [x] ./airflow/models/dag.py:     deactivate_deleted_dags
   - [x] ./airflow/models/skipmixin.py:     _skip
   - [x] ./airflow/models/skipmixin.py:     _skip_all_except
   - [x] ./airflow/models/xcom.py:     set
   - [x] ./airflow/models/xcom.py:     get_value
   - [x] ./airflow/models/xcom.py:     get_one
   - [x] ./airflow/models/xcom.py:     clear
   - [x] ./airflow/models/taskinstance.py: _merge_ti
   - [x] ./airflow/models/taskinstance.py: _add_log
   - [x] ./airflow/models/taskinstance.py: _update_ti_heartbeat
   - [x] ./airflow/models/taskinstance.py: _xcom_pull
   - [x] ./airflow/models/taskinstance.py: _get_template_context
   - [x] ./airflow/models/taskinstance.py: _handle_failure
   - [x] ./airflow/models/taskinstance.py: _record_task_map_for_downstreams
   - [x] ./airflow/models/taskinstance.py: _update_rtif
   - [x] ./airflow/models/taskinstance.py: _defer_task
   - [x] ./airflow/models/taskinstance.py: _handle_reschedule
   - [x] ./airflow/models/taskinstance.py:     get_task_instance
   - [x] ./airflow/models/taskinstance.py:     _clear_xcom_data
   - [x] ./airflow/models/taskinstance.py:     _set_state
   - [x] ./airflow/models/taskinstance.py:     _get_dagrun
   - [x] ./airflow/models/taskinstance.py:     _check_and_change_state_before_execution
   - [x] ./airflow/models/taskinstance.py:     _register_asset_changes_int
   - [x] ./airflow/models/taskinstance.py:     save_to_db
   - [x] ./airflow/models/dagwarning.py:     purge_inactive_dag_warnings
   - [x] ./airflow/models/variable.py:     _set
   - [x] ./airflow/models/variable.py:     _update
   - [x] ./airflow/models/variable.py:     _delete
   - [x] ./airflow/cli/commands/task_command.py: _get_ti_db_access
   - [x] ./airflow/utils/log/file_task_handler.py:     _render_filename_db_access
   - [x] ./airflow/utils/cli_action_loggers.py: _default_action_log_internal
   - [x] ./airflow/sensors/base.py: _orig_start_date
   - [x] ./providers/src/airflow/providers/edge/models/edge_logs.py:     push_logs
   - [x] ./providers/src/airflow/providers/edge/models/edge_job.py:     reserve_task
   - [x] ./providers/src/airflow/providers/edge/models/edge_job.py:     set_state
   - [x] ./providers/src/airflow/providers/edge/models/edge_worker.py:     register_worker
   - [x] ./providers/src/airflow/providers/edge/models/edge_worker.py:     set_state
   - [x] ./tests/api_internal/test_internal_api_call.py:     fake_method
   - [x] ./tests/api_internal/test_internal_api_call.py:     fake_method_with_params
   - [x] ./tests/api_internal/test_internal_api_call.py:     fake_class_method_with_params
   - [x] ./tests/api_internal/test_internal_api_call.py:     fake_class_method_with_serialized_params
- [ ] removal of `*Pydantic` models 


BTW. Command to generate list of methods:

```bash
find . -name '*.py' | xargs grep -A 4 '@internal_api_call' | grep -v ""@internal_api_call"" | grep ""def "" | sed 's/(.*//' | sed ""s/-/: /"" | sed ""s/^/   - [ ] /"" | sed ""s/def //""
```



### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",potiuk,2024-11-27 22:57:54+00:00,[],2024-12-12 21:39:51+00:00,2024-12-12 21:36:45+00:00,https://github.com/apache/airflow/issues/44436,"[('area:CI', ""Airflow's tests and continious integration""), ('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2504949996, 'issue_id': 2700126376, 'author': 'potiuk', 'body': 'cc: @jscheffl @rawwar @kaxil @ashb', 'created_at': datetime.datetime(2024, 11, 27, 22, 58, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2506781410, 'issue_id': 2700126376, 'author': 'ashb', 'body': ""I'm currently re-working all the dag processing stuff to use TaskSDK code etc (but don't know when I'll have a PR for that -- likely Monday or Tuesday) so it _might_ be worth skipping over those. But on the other hand the changes are quite simple to do so it maybe makes sense to do it anyway."", 'created_at': datetime.datetime(2024, 11, 28, 21, 44, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2506786666, 'issue_id': 2700126376, 'author': 'potiuk', 'body': 'I think we can progress on those regardless. One-by-one.', 'created_at': datetime.datetime(2024, 11, 28, 21, 52, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2506829174, 'issue_id': 2700126376, 'author': 'jscheffl', 'body': ""I'd support - but probably I'm only having time on the weekend. As I was also preparing to move Edge-Worker off the internal API, I have 4 PR's (4 slices instead of one elephant) and asked @kaxil already for review. If somebody else has time... these 4 PRs implicitly also remove parts (5 hits) listed from the work and lift them to FastAPI.\r\n\r\nhttps://github.com/apache/airflow/pulls?q=is%3Aopen+is%3Apr+author%3Ajscheffl+label%3Aprovider%3Aedge"", 'created_at': datetime.datetime(2024, 11, 28, 22, 44, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508635280, 'issue_id': 2700126376, 'author': 'potiuk', 'body': 'I take ""airflow/secrets/"" to get the founding removal PR :).', 'created_at': datetime.datetime(2024, 11, 29, 20, 36, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508659634, 'issue_id': 2700126376, 'author': 'potiuk', 'body': 'First small, exemplary PR: https://github.com/apache/airflow/pull/44489', 'created_at': datetime.datetime(2024, 11, 29, 20, 59, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508725424, 'issue_id': 2700126376, 'author': 'potiuk', 'body': 'Looking at jobs.', 'created_at': datetime.datetime(2024, 11, 29, 23, 9, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508807640, 'issue_id': 2700126376, 'author': 'rawwar', 'body': 'working on `airflow/models/dagrun`', 'created_at': datetime.datetime(2024, 11, 30, 3, 16, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508846366, 'issue_id': 2700126376, 'author': 'jason810496', 'body': ""working on\r\n- `airflow/models/trigger`\r\n- `airflow/models/xcom_arg`\r\n    - Also related with Edge Worker, remain starting with `_`.\r\n- `airflow/models/renderedtifields`\r\n    - Starts with `_`, but I think for this method we don't need to rejoin the methods, since it is only used by Edge Worker.\r\n- `airflow/models/serialized_dag`\r\n- `airflow/models/dag`\r\n- `airflow/models/skipmixin`\r\n    - Starts with `_`, rejoined the methods and the test passed.\r\n- `airflow/models/xcom`"", 'created_at': datetime.datetime(2024, 11, 30, 6, 0, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508949883, 'issue_id': 2700126376, 'author': 'potiuk', 'body': 'One more thing: \r\n\r\nWhenever we see `| ....Pydantic` we should also remove it. We can remove it also later if we forget it (there is the last point about removing all `...Pydantic` classes, but if we are modifying a file already for AIP-44 removal, we can as well remove those condidtional Pydantic type hints.', 'created_at': datetime.datetime(2024, 11, 30, 12, 42, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508958100, 'issue_id': 2700126376, 'author': 'rawwar', 'body': 'Working on\r\n\r\n- [ ] /airflow/models/taskinstance.py - #44510\r\n- [ ] /airflow/models/variable.py - https://github.com/apache/airflow/pull/44525\r\n- [ ] ./providers/src/airflow/providers/edge/models/edge_logs.py - \r\n- [ ] ./providers/src/airflow/providers/edge/models/edge_job.py -', 'created_at': datetime.datetime(2024, 11, 30, 13, 11, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508960080, 'issue_id': 2700126376, 'author': 'potiuk', 'body': ""> I'm currently re-working all the dag processing stuff to use TaskSDK code etc (but don't know when I'll have a PR for that -- likely Monday or Tuesday) so it _might_ be worth skipping over those. But on the other hand the changes are quite simple to do so it maybe makes sense to do it anyway.\r\n\r\nYeah. Let's use the momentum and fix all of them :)"", 'created_at': datetime.datetime(2024, 11, 30, 13, 17, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2509182168, 'issue_id': 2700126376, 'author': 'jscheffl', 'body': 'All the providers/edge dependencies should be gone with #44434', 'created_at': datetime.datetime(2024, 11, 30, 20, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2509195468, 'issue_id': 2700126376, 'author': 'jscheffl', 'body': ""I'm going for\r\n\r\n- ./airflow/sensors/base.py: _orig_start_date --> #44518\r\n- ./airflow/utils/cli_action_loggers.py: _default_action_log_internal --> #44519\r\n- ./airflow/utils/log/file_task_handler.py: _render_filename_db_access --> #44520\r\n- ./airflow/cli/commands/task_command.py: _get_ti_db_access --> #44521\r\n- ./airflow/models/dagwarning.py: purge_inactive_dag_warnings -->  #44523\r\n- ./task_sdk/src/airflow/sdk/definitions/asset/init.py: expand_alias_to_assets --> #44530\r\n- ./airflow/dag_processing/processor.py: *** --> #44532"", 'created_at': datetime.datetime(2024, 11, 30, 20, 14, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2509886359, 'issue_id': 2700126376, 'author': 'potiuk', 'body': 'I am getting taskinstance 😱 😱 😱 😱 😱 😱', 'created_at': datetime.datetime(2024, 12, 1, 16, 5, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2509892721, 'issue_id': 2700126376, 'author': 'jscheffl', 'body': '> I am getting taskinstance 😱 😱 😱 😱 😱 😱\r\n\r\nI thought this is with @rawwar ? --> https://github.com/apache/airflow/pull/44510', 'created_at': datetime.datetime(2024, 12, 1, 16, 7, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2509920321, 'issue_id': 2700126376, 'author': 'rawwar', 'body': ""> I am getting taskinstance 😱 😱 😱 😱 😱 😱 \n\nPlease take it. I am stuck at airport and can't get to it until tomorrow"", 'created_at': datetime.datetime(2024, 12, 1, 16, 17, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2509946610, 'issue_id': 2700126376, 'author': 'potiuk', 'body': ""> > I am getting taskinstance 😱 😱 😱 😱 😱 😱\r\n> \r\n>  I thought this is with @rawwar ? --> https://github.com/apache/airflow/pull/44510\r\n\r\nAAAAH.... MISSED IT ....\r\n\r\n> Please take it. I am stuck at airport and can't get to it until tomorrow\r\n\r\nI looked at it, I am not sure if I want it ;) ... Sure .. will do :)"", 'created_at': datetime.datetime(2024, 12, 1, 16, 27, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2540062735, 'issue_id': 2700126376, 'author': 'potiuk', 'body': 'CLOSED !!!!', 'created_at': datetime.datetime(2024, 12, 12, 21, 37, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2540066946, 'issue_id': 2700126376, 'author': 'kaxil', 'body': 'Nice work 👏', 'created_at': datetime.datetime(2024, 12, 12, 21, 39, 51, tzinfo=datetime.timezone.utc)}]","potiuk (Issue Creator) on (2024-11-27 22:58:29 UTC): cc: @jscheffl @rawwar @kaxil @ashb

ashb on (2024-11-28 21:44:45 UTC): I'm currently re-working all the dag processing stuff to use TaskSDK code etc (but don't know when I'll have a PR for that -- likely Monday or Tuesday) so it _might_ be worth skipping over those. But on the other hand the changes are quite simple to do so it maybe makes sense to do it anyway.

potiuk (Issue Creator) on (2024-11-28 21:52:29 UTC): I think we can progress on those regardless. One-by-one.

jscheffl on (2024-11-28 22:44:52 UTC): I'd support - but probably I'm only having time on the weekend. As I was also preparing to move Edge-Worker off the internal API, I have 4 PR's (4 slices instead of one elephant) and asked @kaxil already for review. If somebody else has time... these 4 PRs implicitly also remove parts (5 hits) listed from the work and lift them to FastAPI.

https://github.com/apache/airflow/pulls?q=is%3Aopen+is%3Apr+author%3Ajscheffl+label%3Aprovider%3Aedge

potiuk (Issue Creator) on (2024-11-29 20:36:41 UTC): I take ""airflow/secrets/"" to get the founding removal PR :).

potiuk (Issue Creator) on (2024-11-29 20:59:05 UTC): First small, exemplary PR: https://github.com/apache/airflow/pull/44489

potiuk (Issue Creator) on (2024-11-29 23:09:01 UTC): Looking at jobs.

rawwar on (2024-11-30 03:16:42 UTC): working on `airflow/models/dagrun`

jason810496 on (2024-11-30 06:00:02 UTC): working on
- `airflow/models/trigger`
- `airflow/models/xcom_arg`
    - Also related with Edge Worker, remain starting with `_`.
- `airflow/models/renderedtifields`
    - Starts with `_`, but I think for this method we don't need to rejoin the methods, since it is only used by Edge Worker.
- `airflow/models/serialized_dag`
- `airflow/models/dag`
- `airflow/models/skipmixin`
    - Starts with `_`, rejoined the methods and the test passed.
- `airflow/models/xcom`

potiuk (Issue Creator) on (2024-11-30 12:42:19 UTC): One more thing: 

Whenever we see `| ....Pydantic` we should also remove it. We can remove it also later if we forget it (there is the last point about removing all `...Pydantic` classes, but if we are modifying a file already for AIP-44 removal, we can as well remove those condidtional Pydantic type hints.

rawwar on (2024-11-30 13:11:20 UTC): Working on

- [ ] /airflow/models/taskinstance.py - #44510
- [ ] /airflow/models/variable.py - https://github.com/apache/airflow/pull/44525
- [ ] ./providers/src/airflow/providers/edge/models/edge_logs.py - 
- [ ] ./providers/src/airflow/providers/edge/models/edge_job.py -

potiuk (Issue Creator) on (2024-11-30 13:17:59 UTC): Yeah. Let's use the momentum and fix all of them :)

jscheffl on (2024-11-30 20:05:00 UTC): All the providers/edge dependencies should be gone with #44434

jscheffl on (2024-11-30 20:14:01 UTC): I'm going for

- ./airflow/sensors/base.py: _orig_start_date --> #44518
- ./airflow/utils/cli_action_loggers.py: _default_action_log_internal --> #44519
- ./airflow/utils/log/file_task_handler.py: _render_filename_db_access --> #44520
- ./airflow/cli/commands/task_command.py: _get_ti_db_access --> #44521
- ./airflow/models/dagwarning.py: purge_inactive_dag_warnings -->  #44523
- ./task_sdk/src/airflow/sdk/definitions/asset/init.py: expand_alias_to_assets --> #44530
- ./airflow/dag_processing/processor.py: *** --> #44532

potiuk (Issue Creator) on (2024-12-01 16:05:09 UTC): I am getting taskinstance 😱 😱 😱 😱 😱 😱

jscheffl on (2024-12-01 16:07:37 UTC): I thought this is with @rawwar ? --> https://github.com/apache/airflow/pull/44510

rawwar on (2024-12-01 16:17:51 UTC): Please take it. I am stuck at airport and can't get to it until tomorrow

potiuk (Issue Creator) on (2024-12-01 16:27:53 UTC): AAAAH.... MISSED IT ....


I looked at it, I am not sure if I want it ;) ... Sure .. will do :)

potiuk (Issue Creator) on (2024-12-12 21:37:06 UTC): CLOSED !!!!

kaxil on (2024-12-12 21:39:51 UTC): Nice work 👏

"
2698831542,issue,closed,completed,Migrate `TI.trigger_timeout` to `UtcDateTime`,"Since we are working on Airflow 3, let's use this time to resolve this comment

https://github.com/apache/airflow/blob/6f0d731350674bedc320f213d55694c2df43017c/airflow/models/taskinstance.py#L1774-L1778

",kaxil,2024-11-27 14:52:31+00:00,['vatsrahul1001'],2024-12-12 11:34:30+00:00,2024-12-12 11:34:29+00:00,https://github.com/apache/airflow/issues/44421,"[('area:core', '')]","[{'comment_id': 2504412125, 'issue_id': 2698831542, 'author': 'rawwar', 'body': ""I'm working on converting dag_run.conf from BYTEA to JSON. I think, this will be a good next issue for me. I'll pick this if no one picks by the time I finish https://github.com/apache/airflow/pull/44333"", 'created_at': datetime.datetime(2024, 11, 27, 17, 21, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507848719, 'issue_id': 2698831542, 'author': 'rawwar', 'body': '@vatsrahul1001 will be working on this one.', 'created_at': datetime.datetime(2024, 11, 29, 13, 43, 11, tzinfo=datetime.timezone.utc)}]","rawwar on (2024-11-27 17:21:23 UTC): I'm working on converting dag_run.conf from BYTEA to JSON. I think, this will be a good next issue for me. I'll pick this if no one picks by the time I finish https://github.com/apache/airflow/pull/44333

rawwar on (2024-11-29 13:43:11 UTC): @vatsrahul1001 will be working on this one.

"
2698713505,issue,open,,Diagnose Cluster Task,"### What do you see as an issue?

https://airflow.apache.org/docs/apache-airflow-providers-google/stable/operators/cloud/dataproc.html

Here in the diagnose the cluster task the parameters listed has missing gcp_conn_id parameter, this parameter should also be there otherwise the task fails with an error of invalid gcp_conn_id

### Solving the problem

We can solve this by adding the parameter gcp_conn_id = GCP_CONN_ID

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",payalsaraljain,2024-11-27 14:24:48+00:00,[],2024-11-27 14:27:04+00:00,,https://github.com/apache/airflow/issues/44420,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('area:providers', ''), ('kind:documentation', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2504013093, 'issue_id': 2698713505, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 27, 14, 24, 52, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-27 14:24:52 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2698045377,issue,closed,completed,AIP-72: Handling task instance state exceptions from task sdk and execution API,"### Body

Similar to https://github.com/apache/airflow/issues/44137, there are various state exceptions raised while running `ti.task.execute`. This issue tracks handling them gracefully.

- [x] Deferral Exception: `TaskDeferred` (PR: https://github.com/apache/airflow/pull/44241 @amoghrajesh)
- [x] Skip Exception: `AirflowSkipException` (PR https://github.com/apache/airflow/pull/44786 @amoghrajesh)
- [x] Reschedule exception: `AirflowRescheduleException` (PR https://github.com/apache/airflow/pull/44907 @amoghrajesh) 
- [x] Fail Exception: `AirflowFailException` (PR https://github.com/apache/airflow/pull/44954 @amoghrajesh)
- [x] Sensor timeout exception: `AirflowSensorTimeout` (PR https://github.com/apache/airflow/pull/44954 @amoghrajesh)
- [x] Task terminated exception: `AirflowTaskTerminated` (PR https://github.com/apache/airflow/pull/44977 @amoghrajesh)
- [x] Task timeout (up_for_retry state) `AirflowTaskTimeout` (PR https://github.com/apache/airflow/pull/45310 @amoghrajesh)
- [x] Airflow Exception (up_for_retry state) (PR https://github.com/apache/airflow/pull/45308 @amoghrajesh)
- [x] System Exit: `SystemExit`(PR: https://github.com/apache/airflow/pull/45282 @amoghrajesh)
- [x] Base Exception: `BaseException` (PR https://github.com/apache/airflow/pull/45106 @amoghrajesh)

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",amoghrajesh,2024-11-27 10:42:15+00:00,"['ashb', 'kaxil', 'amoghrajesh']",2025-01-01 12:31:41+00:00,2025-01-01 12:31:41+00:00,https://github.com/apache/airflow/issues/44414,"[('kind:meta', 'High-level information important to the community'), ('area:core', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]","[{'comment_id': 2503565945, 'issue_id': 2698045377, 'author': 'ashb', 'body': 'System exit should be handled already I thought?', 'created_at': datetime.datetime(2024, 11, 27, 10, 57, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2503659001, 'issue_id': 2698045377, 'author': 'amoghrajesh', 'body': 'I do not think we do it:\r\n```\r\n    except SystemExit:\r\n        ...\r\n    except BaseException:\r\n        # TODO: Handle TI handle failure\r\n        raise\r\n```', 'created_at': datetime.datetime(2024, 11, 27, 11, 41, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2566989595, 'issue_id': 2698045377, 'author': 'amoghrajesh', 'body': 'All the TI states have been handled in various PRs as linked in the description. Closing this issue as complete.', 'created_at': datetime.datetime(2025, 1, 1, 12, 31, 41, tzinfo=datetime.timezone.utc)}]","ashb (Assginee) on (2024-11-27 10:57:05 UTC): System exit should be handled already I thought?

amoghrajesh (Issue Creator) on (2024-11-27 11:41:52 UTC): I do not think we do it:
```
    except SystemExit:
        ...
    except BaseException:
        # TODO: Handle TI handle failure
        raise
```

amoghrajesh (Issue Creator) on (2025-01-01 12:31:41 UTC): All the TI states have been handled in various PRs as linked in the description. Closing this issue as complete.

"
2697882281,issue,closed,completed,Asset related REST API endpoints rework,"### Description

We used to have `/api/v1/datasets/{uri}` in airflow 2.x and changed into `/api/v1/assets/{uri}` for airflow 3.0, but this could raise the issue as assets now have a new attribute `name.`

After discussion with @uranusjr , it should be changed into `/api/v1/assets/{name}` instead. As `Asset(""s3://..."")` generates an asset with name and URI set to `s3://...`, it should still make sense.

But there would be two more things to consider.

1. Whether we want to filter assets by URI 
2. Whehter we want to list the assets that are not active

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-11-27 09:40:04+00:00,"['uranusjr', 'Lee-W']",2024-12-26 04:14:04+00:00,2024-12-26 04:14:04+00:00,https://github.com/apache/airflow/issues/44412,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('area:datasets', 'Issues related to the datasets feature')]","[{'comment_id': 2503394437, 'issue_id': 2697882281, 'author': 'Lee-W', 'body': ""@jedcunningham @ephraimbuddy would like to know whether we're still returning a DAG that is deleted after DAG versioning. If not, we'll probably also not return non-active assets in the API"", 'created_at': datetime.datetime(2024, 11, 27, 9, 41, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2503426967, 'issue_id': 2697882281, 'author': 'ephraimbuddy', 'body': ""> @jedcunningham @ephraimbuddy would like to know whether we're still returning a DAG that is deleted after DAG versioning. If not, we'll probably also not return non-active assets in the API\r\n\r\nI think you mean a DAG whose file is deleted. If so, all we do is mark the dag as inactive, just as before. It didn't change. The only change is that the serdag for such dag won't be deleted, but previously, it would have been deleted."", 'created_at': datetime.datetime(2024, 11, 27, 9, 54, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2503432438, 'issue_id': 2697882281, 'author': 'Lee-W', 'body': ""> I think you mean a DAG whose file is deleted. If so, all we do is mark the dag as inactive, just as before. It didn't change. The only change is that the serdag for such dag won't be deleted, but previously, it would have been deleted.\r\n\r\nwill we still be able to get it through REST API?"", 'created_at': datetime.datetime(2024, 11, 27, 9, 57, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2503537216, 'issue_id': 2697882281, 'author': 'ephraimbuddy', 'body': ""> > I think you mean a DAG whose file is deleted. If so, all we do is mark the dag as inactive, just as before. It didn't change. The only change is that the serdag for such dag won't be deleted, but previously, it would have been deleted.\r\n> \r\n> will we still be able to get it through REST API?\r\n\r\n\r\nYes"", 'created_at': datetime.datetime(2024, 11, 27, 10, 43, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562044477, 'issue_id': 2697882281, 'author': 'Lee-W', 'body': ""@uranusjr If my memory serves correct, we're waiting for the UI folks for this?"", 'created_at': datetime.datetime(2024, 12, 26, 0, 43, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562095754, 'issue_id': 2697882281, 'author': 'uranusjr', 'body': 'We already merged #44801 to solve the URI part. I’m not sure what we should do for the activeness part since we don’t have a real use case yet.', 'created_at': datetime.datetime(2024, 12, 26, 2, 43, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562136116, 'issue_id': 2697882281, 'author': 'Lee-W', 'body': ""> We already merged #44801 to solve the URI part. I’m not sure what we should do for the activeness part since we don’t have a real use case yet.\r\n\r\nah yes, forget about that one. then I'll close this one till we have a concrete idea on what needs to be done"", 'created_at': datetime.datetime(2024, 12, 26, 4, 14, 4, tzinfo=datetime.timezone.utc)}]","Lee-W (Issue Creator) on (2024-11-27 09:41:23 UTC): @jedcunningham @ephraimbuddy would like to know whether we're still returning a DAG that is deleted after DAG versioning. If not, we'll probably also not return non-active assets in the API

ephraimbuddy on (2024-11-27 09:54:54 UTC): I think you mean a DAG whose file is deleted. If so, all we do is mark the dag as inactive, just as before. It didn't change. The only change is that the serdag for such dag won't be deleted, but previously, it would have been deleted.

Lee-W (Issue Creator) on (2024-11-27 09:57:04 UTC): will we still be able to get it through REST API?

ephraimbuddy on (2024-11-27 10:43:37 UTC): Yes

Lee-W (Issue Creator) on (2024-12-26 00:43:15 UTC): @uranusjr If my memory serves correct, we're waiting for the UI folks for this?

uranusjr (Assginee) on (2024-12-26 02:43:03 UTC): We already merged #44801 to solve the URI part. I’m not sure what we should do for the activeness part since we don’t have a real use case yet.

Lee-W (Issue Creator) on (2024-12-26 04:14:04 UTC): ah yes, forget about that one. then I'll close this one till we have a concrete idea on what needs to be done

"
2697772223,issue,closed,completed,Add backward compat to airflow.datasets.metadata.Metadata,"### Description

Allow import `Metadata` from `airflow.datasets.metadata` to keep the existing DAGs work as it used to be. But this behavior will be removed after 3.1

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-11-27 09:14:32+00:00,['Lee-W'],2024-12-02 13:12:31+00:00,2024-12-02 13:12:31+00:00,https://github.com/apache/airflow/issues/44411,"[('kind:feature', 'Feature Requests'), ('area:datasets', 'Issues related to the datasets feature'), ('airflow3.0:candidate', 'Potential candidates for Airflow 3.0')]",[],
2697009336,issue,open,, Check whether `context[key]`has variables that are no longer available,"### Description

The following context variables are no longer available:
```
execution_date
next_ds
next_ds_nodash
next_execution_date
prev_ds
prev_ds_nodash
prev_execution_date
prev_execution_date_success
tomorrow_ds
yesterday_ds
yesterday_ds_nodash
```

This change is part of [#43902](https://github.com/apache/airflow/pull/43902).

### Use case/motivation

Its been removed in [#43902](https://github.com/apache/airflow/pull/43902).

### Related issues

[41641](https://github.com/apache/airflow/issues/41641)

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",sunank200,2024-11-27 04:34:53+00:00,['sunank200'],2024-12-26 08:29:23+00:00,,https://github.com/apache/airflow/issues/44409,"[('area:dev-tools', ''), ('kind:feature', 'Feature Requests')]","[{'comment_id': 2562309327, 'issue_id': 2697009336, 'author': 'sunank200', 'body': 'Created the PR: [15144](https://github.com/astral-sh/ruff/pull/15144)', 'created_at': datetime.datetime(2024, 12, 26, 8, 29, 22, tzinfo=datetime.timezone.utc)}]","sunank200 (Issue Creator) on (2024-12-26 08:29:22 UTC): Created the PR: [15144](https://github.com/astral-sh/ruff/pull/15144)

"
2696500644,issue,closed,completed,Scheduler to associate dag runs with a bundle and version,"### Body

Once there's a bundle id / version in serdag, the scheduler will need to ensure that, when it creates a dag run, we mark the bundle / version for that dag run.  This way at execution time, we'll be able to pull up the right code.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2024-11-26 23:57:14+00:00,[],2025-01-28 04:25:38+00:00,2025-01-28 04:25:37+00:00,https://github.com/apache/airflow/issues/44403,"[('area:Scheduler', 'including HA (high availability) scheduler'), ('kind:meta', 'High-level information important to the community'), ('area:core', '')]","[{'comment_id': 2617816675, 'issue_id': 2696500644, 'author': 'jedcunningham', 'body': 'This was done.', 'created_at': datetime.datetime(2025, 1, 28, 4, 25, 37, tzinfo=datetime.timezone.utc)}]","jedcunningham on (2025-01-28 04:25:37 UTC): This was done.

"
2695691542,issue,closed,completed,"Add ability to list ""active"" backfills both across cluster and per-dag (UI-only API -- not public)","### Body

child of parent issue https://github.com/apache/airflow/issues/43970

UI needs the ability to fetch the active backfill for a dag, and the list of all active backfills across the cluster. 

Add this _only_ in the UI part of the core API, so that it is not considered ""public"" and therefore we can change it as desired without having to wait until airflow 4.0

i.e. here:
<img width=""241"" alt=""image"" src=""https://github.com/user-attachments/assets/3e06f066-0571-4027-9a8f-eddf0383fba6"">

cc @jedcunningham @bbovenzi @phanikumv @pierrejeambrun 




### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2024-11-26 18:38:06+00:00,['prabhusneha'],2024-12-11 11:46:52+00:00,2024-12-11 11:46:52+00:00,https://github.com/apache/airflow/issues/44396,"[('area:API', ""Airflow's REST/HTTP API""), ('kind:meta', 'High-level information important to the community'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('area:backfill', 'Specifically for backfill related')]","[{'comment_id': 2503004236, 'issue_id': 2695691542, 'author': 'vatsrahul1001', 'body': 'cc: @prabhusneha assigning this to you.', 'created_at': datetime.datetime(2024, 11, 27, 6, 21, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2503009828, 'issue_id': 2695691542, 'author': 'prabhusneha', 'body': '> cc: @prabhusneha assigning this to you.\r\n\r\nSure, will take this up.', 'created_at': datetime.datetime(2024, 11, 27, 6, 26, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2503020508, 'issue_id': 2695691542, 'author': 'phanikumv', 'body': 'Weird that @prabhusneha  is getting unassigned automatically, and that is showing up as @vatsrahul1001 unassigned her, seems to be a problem in GitHub. However it seems to be sorted now', 'created_at': datetime.datetime(2024, 11, 27, 6, 35, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2516291508, 'issue_id': 2695691542, 'author': 'prabhusneha', 'body': 'PR: https://github.com/apache/airflow/pull/44624', 'created_at': datetime.datetime(2024, 12, 4, 6, 14, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2527379324, 'issue_id': 2695691542, 'author': 'prabhusneha', 'body': 'Addressing the PR comments. \nWorking on the test that are failing.', 'created_at': datetime.datetime(2024, 12, 9, 9, 25, 42, tzinfo=datetime.timezone.utc)}]","vatsrahul1001 on (2024-11-27 06:21:24 UTC): cc: @prabhusneha assigning this to you.

prabhusneha (Assginee) on (2024-11-27 06:26:19 UTC): Sure, will take this up.

phanikumv on (2024-11-27 06:35:38 UTC): Weird that @prabhusneha  is getting unassigned automatically, and that is showing up as @vatsrahul1001 unassigned her, seems to be a problem in GitHub. However it seems to be sorted now

prabhusneha (Assginee) on (2024-12-04 06:14:37 UTC): PR: https://github.com/apache/airflow/pull/44624

prabhusneha (Assginee) on (2024-12-09 09:25:42 UTC): Addressing the PR comments. 
Working on the test that are failing.

"
2695645712,issue,closed,completed,Add robust dry run capability for backfill,"### Body

Child of parent issue https://github.com/apache/airflow/issues/43970

As a user, you want to be able to dry run the backfill creation process from the UI.  E.g. i click ""create backfill"" and give it a range, then I want, in the UI, to be able to see the runs that will be created if I click ""submit"".

In order to do this, we'll have to refactor the backfill creation process a bit.  Right now, we just submit a range, and the backfill endpoint will just create the backfill object and all of the runs.

One of the problems with the idea of implementing dry run is, suppose we return ""these runs will be created; proceed?"".  Well what if the scheduler schedules, or a user clears or deletes, a run in the range.  Then we would not end up doing exactly what we said we were going to do.

So what we need to do is somehow, implement in the API the ability to get some representation of the entirety of the backfill -- the object and its runs -- and then the user could submit that back to another endpoint which would just receive this payload and attempt to create it.  In this second endpoint which is essentially ""take the payload and create"", we wolud first lock the dag and then attempt to insert all the rows.  And if we find a conflict, we should abandon the whole try and tell the user, sorry, something changed, we got a conflict, please try again.  There's a 409 Conflict API response that would seem to be appropriate here.

cc @phanikumv @jedcunningham @bbovenzi @pierrejeambrun 



### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2024-11-26 18:13:25+00:00,['prabhusneha'],2025-01-12 16:48:53+00:00,2025-01-12 16:48:53+00:00,https://github.com/apache/airflow/issues/44395,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('kind:meta', 'High-level information important to the community'), ('area:backfill', 'Specifically for backfill related')]","[{'comment_id': 2515654828, 'issue_id': 2695645712, 'author': 'bbovenzi', 'body': ""This makes sense to me. I think it's very important for users to know exactly what they're about to change.\r\n\r\nWe can make sure the UI specifically handles the 409 response in the create backfill flow."", 'created_at': datetime.datetime(2024, 12, 3, 22, 11, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2516429994, 'issue_id': 2695645712, 'author': 'vatsrahul1001', 'body': 'Assigning to @prabhusneha', 'created_at': datetime.datetime(2024, 12, 4, 7, 44, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2516430856, 'issue_id': 2695645712, 'author': 'prabhusneha', 'body': '> Assigning to [@prabhusneha](https://github.com/prabhusneha)\n\nI will take this up', 'created_at': datetime.datetime(2024, 12, 4, 7, 45, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2550420896, 'issue_id': 2695645712, 'author': 'prabhusneha', 'body': 'After further discussions with @dstandish, we decided to adopt an approach aligned with the current CLI dry run functionality, rather than implementing a two stage process. Specifically, when a user requests a dry run of the backfill, the response will include only the DAG runs that will actually be created. Any DAG runs that would not be created based on the specified `reprocess_behavior` will be excluded from the dry run response.', 'created_at': datetime.datetime(2024, 12, 18, 6, 0, 36, tzinfo=datetime.timezone.utc)}]","bbovenzi on (2024-12-03 22:11:13 UTC): This makes sense to me. I think it's very important for users to know exactly what they're about to change.

We can make sure the UI specifically handles the 409 response in the create backfill flow.

vatsrahul1001 on (2024-12-04 07:44:43 UTC): Assigning to @prabhusneha

prabhusneha (Assginee) on (2024-12-04 07:45:13 UTC): I will take this up

prabhusneha (Assginee) on (2024-12-18 06:00:36 UTC): After further discussions with @dstandish, we decided to adopt an approach aligned with the current CLI dry run functionality, rather than implementing a two stage process. Specifically, when a user requests a dry run of the backfill, the response will include only the DAG runs that will actually be created. Any DAG runs that would not be created based on the specified `reprocess_behavior` will be excluded from the dry run response.

"
2694411106,issue,closed,completed,Avoid `airflow.contrib.*` import,"### Description

Implement a ruff rule to check whether there's `airflow.contribute.*`

### Use case/motivation

it has been removed in https://github.com/apache/airflow/pull/41366

### Related issues

https://github.com/apache/airflow/issues/41641

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-11-26 11:56:39+00:00,['Lee-W'],2024-12-03 14:16:52+00:00,2024-12-03 14:16:52+00:00,https://github.com/apache/airflow/issues/44385,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:dev-tools', ''), ('kind:feature', 'Feature Requests')]","[{'comment_id': 2514692420, 'issue_id': 2694411106, 'author': 'Lee-W', 'body': 'close this one in favor of https://github.com/apache/airflow/issues/44556', 'created_at': datetime.datetime(2024, 12, 3, 14, 16, 52, tzinfo=datetime.timezone.utc)}]","Lee-W (Issue Creator) on (2024-12-03 14:16:52 UTC): close this one in favor of https://github.com/apache/airflow/issues/44556

"
2694099228,issue,open,,"Add alerts on slack for new ""Security"" issues","There is a ""security"" tab in the airflow repository where code scanning produces new issues discovered in our code. 

In order to drag attention to it, we should have an automation to post slack messages in a private ""security"" channel - this, similarly as in case of main failures - might help us with more ""group"" handling of noticing and handling such security reports. ",potiuk,2024-11-26 10:34:17+00:00,['amoghrajesh'],2024-11-27 14:00:45+00:00,,https://github.com/apache/airflow/issues/44382,"[('security', 'Security issues that must be fixed'), ('provider:slack', '')]","[{'comment_id': 2500253428, 'issue_id': 2694099228, 'author': 'potiuk', 'body': 'cc: @amoghrajesh - when you have time :)', 'created_at': datetime.datetime(2024, 11, 26, 10, 35, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2500351347, 'issue_id': 2694099228, 'author': 'eladkal', 'body': 'Maybe we can find a way for the scanner to run on the PR directly before we merge it?', 'created_at': datetime.datetime(2024, 11, 26, 11, 17, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2502382030, 'issue_id': 2694099228, 'author': 'potiuk', 'body': '> Maybe we can find a way for the scanner to run on the PR directly before we merge it?\r\n\r\nYes. good point https://github.com/apache/airflow/blob/main/.github/workflows/codeql-analysis.yml likely need to be updated to work on `pull request` as well. I am not sure if it is going to work on pull requests from forks - but there does not seem to be a limitation for it mention and it needs only \'read-content"" permission: https://docs.github.com/en/code-security/code-scanning/managing-code-scanning-alerts/triaging-code-scanning-alerts-in-pull-requests', 'created_at': datetime.datetime(2024, 11, 27, 1, 5, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2502389382, 'issue_id': 2694099228, 'author': 'potiuk', 'body': 'https://github.com/apache/airflow/pull/44404 -> seems to work out of the box.', 'created_at': datetime.datetime(2024, 11, 27, 1, 9, 56, tzinfo=datetime.timezone.utc)}]","potiuk (Issue Creator) on (2024-11-26 10:35:02 UTC): cc: @amoghrajesh - when you have time :)

eladkal on (2024-11-26 11:17:01 UTC): Maybe we can find a way for the scanner to run on the PR directly before we merge it?

potiuk (Issue Creator) on (2024-11-27 01:05:30 UTC): Yes. good point https://github.com/apache/airflow/blob/main/.github/workflows/codeql-analysis.yml likely need to be updated to work on `pull request` as well. I am not sure if it is going to work on pull requests from forks - but there does not seem to be a limitation for it mention and it needs only 'read-content"" permission: https://docs.github.com/en/code-security/code-scanning/managing-code-scanning-alerts/triaging-code-scanning-alerts-in-pull-requests

potiuk (Issue Creator) on (2024-11-27 01:09:56 UTC): https://github.com/apache/airflow/pull/44404 -> seems to work out of the box.

"
2693880554,issue,open,,Add precommit to detect and deny core airflow imports in task sdk,"### Body

Right now, the reviewers have to be extremely careful about the AIP 72 PRs on task SDK not importing from airflow core or it would introduce coupling between core and sdk again.

Add a precommit script to detect and deny such commits.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",amoghrajesh,2024-11-26 09:33:08+00:00,['amoghrajesh'],2025-01-17 15:24:18+00:00,,https://github.com/apache/airflow/issues/44379,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:dev-tools', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2693847358,issue,open,,Better dev db auto-maintenance,"Sometimes our test fail when you develop airlfow inlocal venv with ""missing"" or ""wrong"" database objects - such as tables, sequences. 

For local venv (which almost exclusively uses `sqlite` until we implement `test-containers` produces failures related to sqlalchemy . Specifically when running individual tests using commands like

`pytest tests/api_fastapi/core_api/routes/public/test_dag_run.py::TestTriggerDagRun::test_raises_validation_error_for_invalid_params` 

It happens to one of the fastapi tests
 They just randomly happen for different tables and the error is something like this:
```
E   sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: dag_run_note
E   [SQL: SELECT dag_run_note.user_id AS dag_run_note_user_id, dag_run_note.dag_run_id AS dag_run_note_dag_run_id, dag_run_note.content AS dag_run_note_content, dag_run_note.created_at AS dag_run_note_created_at, dag_run_note.updated_at AS dag_run_note_updated_at 
E   FROM dag_run_note 
E   WHERE dag_run_note.dag_run_id = ?]
E   [parameters: (1,)]
E   (Background on this error at: https://sqlalche.me/e/14/e3q8)
```

if this happens, you need to  likely reset your database

A lot of tests in Airflow rely on database being available (so called ""db_tests"" and they expect it to be in the ""right"" state - i.e. tables created (edited) 

This will happen automatically if you run tests for the first time - and you can also force this is initialization with --with-db-init flag, We have actually quite a number of those airflow-specific pytest flags defined (they are printed with pytest --help:

```
airflow:
  --with-db-init        Forces database initialization before tests
  --integration=INTEGRATIONS
                        only run tests matching integration specified: [cassandra,kerberos,mongo,celery,statsd,trino].
  --keep-env-variables  do not clear environment variables that might have side effect while running tests
  --skip-db-tests       skip tests that require database
  --run-db-tests-only   only run tests requiring database
  --backend=BACKEND     only run tests matching the backend: [sqlite,postgres,mysql].
  --system              run system tests
  --include-long-running
                        Includes long running tests (marked with long_running marker). They are skipped by default.
  --include-quarantined
                        Includes quarantined tests (marked with quarantined marker). They are skipped by default.
  --trace-sql=COLUMNS   Trace SQL statements. As an argument, you must specify the columns to be displayed as a comma-separated list. Supported values: [fnum,time,trace,sql,parameters,count]
  --no-db-cleanup       Disable DB clear before each test module.
  --disable-forbidden-warnings
                        Disable raising an error if forbidden warnings detected.
  --disable-capture-warnings
                        Disable internal capture warnings.
  --warning-output-path=PATH
                        Path for resulting captured warnings. Absolute or relative to the `tests` directory. If not provided or environment variable `CAPTURE_WARNINGS_OUTPUT` not set then 'warnings.txt' will be used.
```

By default  the database is stored in ~/airflow/  folder - it's an sqlite database.

And if this happens you have to delete it manually while developing - especially after our recent agreement that we might squash some migrations while developing the DB - because it might mean that a database created at some point in time during your development, might not be ""upgrade'able"" any more - i.e. neither --with-db-init pytest flag nor airflow db reset will work (both will crash - and --with-db-init is I think crashing silently then) 

So sometimes you might have to manually wipe-out the local database and start from scratch. This is what `breeze down` command is doing for you - it deletes all the files that store created database - but breeze does it not only for `sqlite` but also for `postgres` and `mysql`.


The `initialize_airflow_tests` fixture in `tests/common/ptyest_plugin.py` performs this initialization automatically when you first time run tests, buet then it does not di maintenance automatically and does not handle all cases - especially when database changes got reverted or squashed while developing the db.

We could likely improve the code in that fixture to make it more resilient and ""self-healing"" the sqlite database, or maybe at the very least have better error messages when it happens and instruct the contributors what to do.
",potiuk,2024-11-26 09:19:37+00:00,[],2024-11-26 09:22:30+00:00,,https://github.com/apache/airflow/issues/44377,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code')]","[{'comment_id': 2500084725, 'issue_id': 2693847358, 'author': 'potiuk', 'body': 'This can likely be done together with https://github.com/apache/airflow/issues/43514 (test containers)', 'created_at': datetime.datetime(2024, 11, 26, 9, 22, 16, tzinfo=datetime.timezone.utc)}]","potiuk (Issue Creator) on (2024-11-26 09:22:16 UTC): This can likely be done together with https://github.com/apache/airflow/issues/43514 (test containers)

"
2693652830,issue,open,,"Provide users choice of timezone to `FileTaskHandler`, for log timestamp formatting","### Description

As a user, i want the logs presented in the Airflow UI to match the choice of timezone, set in the top right corner of the UI. So that timestamps from the logs are consistent with other times presented in the UI, such as task start and end time.

Normally, with logs stored in files, the timestamps are stuck in the file, as it was materialized during write. [*]
With modern remote logging solutions, that is no longer the case.

In our case, we have overridden the task logger to save all logs in a remote logging solution; Loki.
Loki internally stores all its logs in UTC time format, and leaves choice of time zone to the presentation layer.

In the airflow webserver, we set `logging_config_class`  to a class inheriting `(FileTaskHandler, ExternalLoggingMixin)` to fetch the logs from Loki, format them and present to the UI.

The problem is that the users choice of timezone is not exposed to the `FileTaskHandler.read` function, so we don't know which timezone to select.
What i'm asking for is either:
* A: Forward the users choice of time zone to  `FileTaskHandler.read`, to allow that function to format the log in the desired time zone.
* B: Rehaul the interface of sending logs between the backend and the frontend UI, so the backend sends logs as `(utc-time, entry)`-tuples instead of raw text, then leave the formatting of time to the UI layer. This second option has more benefits in that the user can choose to hide the timestamps entirely, if desired.

----

[*] *(Even with timestamps written to files, you can solve this, if the logfile has a structured format and something converts the timestamps during read of that file)*

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",hterik,2024-11-26 08:16:40+00:00,[],2024-11-27 00:59:17+00:00,,https://github.com/apache/airflow/issues/44376,"[('area:logging', ''), ('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2502374144, 'issue_id': 2693652830, 'author': 'potiuk', 'body': 'Comment - I believe in Airflow 3 we are going to always use structured logging - so I think that one will be possible to implement - at least theorethically. @ashb @kaxil - but for remote logs it might also need some changes to the APIs.', 'created_at': datetime.datetime(2024, 11, 27, 0, 59, 17, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-11-27 00:59:17 UTC): Comment - I believe in Airflow 3 we are going to always use structured logging - so I think that one will be possible to implement - at least theorethically. @ashb @kaxil - but for remote logs it might also need some changes to the APIs.

"
2693406248,issue,closed,completed,Compatibility for AIP-74,"### Body

We renamed dataset to asset for AIP-74, but it’s supposed to be completely backward compatible (DAGs on 2.x should work unchanged). We did the bulk of the work already, but missed some small ones (e.g. in the execute context). We should fix those.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",uranusjr,2024-11-26 06:49:48+00:00,"['uranusjr', 'Lee-W']",2024-12-07 07:43:51+00:00,2024-12-07 07:43:51+00:00,https://github.com/apache/airflow/issues/44375,"[('kind:meta', 'High-level information important to the community'), ('area:datasets', 'Issues related to the datasets feature')]","[{'comment_id': 2514159441, 'issue_id': 2693406248, 'author': 'Lee-W', 'body': 'At least, `DatasetAll` and `DatasetAny` will need to be backported as well', 'created_at': datetime.datetime(2024, 12, 3, 10, 32, 12, tzinfo=datetime.timezone.utc)}]","Lee-W (Assginee) on (2024-12-03 10:32:12 UTC): At least, `DatasetAll` and `DatasetAny` will need to be backported as well

"
2693013063,issue,closed,completed,Create news fragments template,"### Description

as title

### Use case/motivation

for easy compiling migration rules in https://github.com/apache/airflow/issues/41641 and breaking change list https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+3+breaking+changes

### Related issues

https://github.com/apache/airflow/issues/41641

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-11-26 04:05:57+00:00,['Lee-W'],2024-11-27 07:48:21+00:00,2024-11-27 07:48:21+00:00,https://github.com/apache/airflow/issues/44374,"[('kind:feature', 'Feature Requests')]",[],
2692705884,issue,open,,Modernize Docs Building workflows,"Our Docs building workflow are BAD. VERY BAD

a) airflow-site contains history of all versions of all released airflows and providers
b) because of that .git repo for airlfow-site is HUGE
c) just git commit takes seconds now, any other git command takes longer and longer
d) building the site takes a long time - more than half an hour and a lot of space
e) the versions of software used to build the site are out-dated (Sphinx, node, hugo - are all very old and have many unhandled security fixes)

This whole process should be updated:

* we should store old versions of docs in some archive and use it from there rather than from git
* we should update the software and process of generation to use modern tools (newer Sphinx, Hugo and more) 
* likely we should be able to split our documentation per provider using ""subdir"" feature https://issues.apache.org/jira/projects/INFRA/issues/INFRA-25389

",potiuk,2024-11-26 00:44:30+00:00,[],2025-01-17 00:40:10+00:00,,https://github.com/apache/airflow/issues/44373,"[('kind:documentation', '')]","[{'comment_id': 2508970781, 'issue_id': 2692705884, 'author': 'potiuk', 'body': 'This might also be easier/more natural to do as a follow up after #44511', 'created_at': datetime.datetime(2024, 11, 30, 13, 57, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2580877699, 'issue_id': 2692705884, 'author': 'RNHTTR', 'body': ""Here's an [Old dev list thread on the matter](https://lists.apache.org/list?dev@airflow.apache.org:2023-10:%22The%20GitHub%20Action%20for%20building%20docs%22). Maybe a good starting place is first developing all Airflow 3.0 (and new 2.x release) documentation using a static site generator like [Pelican](https://getpelican.com/). We actually have an [issue on airflow-site for this](https://github.com/apache/airflow-site/issues/719)."", 'created_at': datetime.datetime(2025, 1, 9, 17, 27, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2582747341, 'issue_id': 2692705884, 'author': 'shahar1', 'body': ""> Here's an [Old dev list thread on the matter](https://lists.apache.org/list?dev@airflow.apache.org:2023-10:%22The%20GitHub%20Action%20for%20building%20docs%22). Maybe a good starting place is first developing all Airflow 3.0 (and new 2.x release) documentation using a static site generator like [Pelican](https://getpelican.com/). We actually have an [issue on airflow-site for this](https://getpelican.com/).\r\n\r\nI assume that you meant https://github.com/apache/airflow-site/issues/719 in the second link?"", 'created_at': datetime.datetime(2025, 1, 10, 13, 45, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2597094187, 'issue_id': 2692705884, 'author': 'RNHTTR', 'body': ""@potiuk Do you know how breeze serves the container when running `breeze build-docs`? I'm trying to use [sphinx-autobuild](https://github.com/sphinx-doc/sphinx-autobuild/tree/main) and port forward the docs page, but I can't quite figure out how."", 'created_at': datetime.datetime(2025, 1, 16, 23, 12, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2597193486, 'issue_id': 2692705884, 'author': 'potiuk', 'body': '> [@potiuk](https://github.com/potiuk) Do you know how breeze serves the container when running `breeze build-docs`? I\'m trying to use [sphinx-autobuild](https://github.com/sphinx-doc/sphinx-autobuild/tree/main) and port forward the docs page, but I can\'t quite figure out how.\n\nThat\'s quite unlikely to be easy without  quite a redesign. Note I am not the author of it, I merely suffled around the scripts and code that was there from the beginning, so I do not know more than just by looking and reverse engineering of how it works, And if someone would like to redesign this (on top of what is already planned in fixing the infrastructuce pieces described here - they are absolutely welcome.\n\nIn short (you can see the sourcesin links:\n\n1) `breeze build docs` takes the parameters passed via breeze CLI and convert them into Build Params that are then used as arguments of a shell script /opt/airflow/scripts/in_container/run_docs_build.sh that is run inside the container.  \n\nhttps://github.com/apache/airflow/blob/main/dev/breeze/src/airflow_breeze/commands/developer_commands.py#L700\n\nAlso that breeze command makes sure that image is ready and rebuild. But essentially it calls this shell script:\n\n```python\n    cmd = ""/opt/airflow/scripts/in_container/run_docs_build.sh "" + "" "".join(\n        [shlex.quote(arg) for arg in doc_builder.args_doc_builder]\n```\n\n2) This script does some housekeeping and cleanup/removal of stuff at completion but essentially it calls (in container):\n\n```\npython -m docs.build_docs ""${@}"" \n```\n\nSo passing the parameters passed to `build_docs` python script defined in https://github.com/apache/airflow/blob/main/docs/build_docs.py\n\n3) This one can even be run directly with `--help` and show the parameters it can take (defined in argparse here https://github.com/apache/airflow/blob/main/docs/build_docs.py#L465) . That includes potential selection of the packages for which the documentation should be built\n\nThis script does quite a few more things:\n\n* it fetches inventories from public inter-sphinx inventories so that links to source code referred from libraries can be automatically linked from sphinx\n\n* it also fetches ""our"" package inventories (prepared in canary run and published in Airflow\'s amazon s3 bucket)  - so that for example if you only build one provider and refer to another or to airlfow, sphinx can properly build-inter-sphinx links to those ""external"" documents. Each provider, airflow, helm , ""proiders index"" are separate ""sphinx packages"" linked between each other via inter-sphinx inventories. When you build a package locally, the inventoy is regenerated and produced as part of the build, so when you build several packages locally they can refer to each-other\'s new APIs and pages added. This for example allows us to see that some links are missing when some pages or links are moved and we need to refer to them - with intersphinx we will see warnings (and error out) when such links are wrong\n\n* It then selects packages to build - prioritising those that do not have inventories - because those should be build first - so that other packages can use the inventories when they are built together\n\n* then the packages are built - the build is parallelised to allow to use multiple processors - each package is build by one of the N = CPU processors - this way building whole documentation on a 16 core machine will take less than 10 minutes rather than 1.5h - if they were run sequentially\n \n* then there is interesting mechanism to attempt to retry building packages to allow to link to other locally built packages - sometimes when a package is being build it has a new page that other packages refer to (refactors and such) then such packages will fail until inventories are build for the source package. As packages are build in parallel it might mean that some packages might fail in the first pass, and they need 2nd or 3rd pass in order to succeed (depends how much ""circular"" or transitive package dependencies we have . This happens up to 3 times. See here: https://github.com/apache/airflow/blob/main/docs/build_docs.py#L534\n \n* Buidling itself happens in the document builder class: https://github.com/apache/airflow/blob/main/docs/exts/docs_build/docs_builder.py - that clas prepares all the parameters needed to run sphinx command to run build for such a package. This command is derived here (for each package separately):\n\nhttps://github.com/apache/airflow/blob/main/docs/exts/docs_build/docs_builder.py#L237\n\nEssentially this:\n\n```python\n        build_cmd = [\n            ""sphinx-build"",\n            ""-T"",  # show full traceback on exception\n            ""--color"",  # do emit colored output\n            ""-b"",  # builder to use\n            ""html"",\n            ""-d"",  # path for the cached environment and doctree files\n            self._doctree_dir,\n            ""-c"",\n            DOCS_DIR,\n            ""-w"",  # write warnings (and errors) to given file\n            self.log_build_warning_filename,\n            self._src_dir,\n            self._build_dir,  # path to output directory\n        ]\n```\n\n\n* But then, each individual package is build via pretty complex sphinx extensions (and here I think where the complexity lies about making autobuild to work - as we have  a lot of extensions build and complex build configuration that might make it rather difficult and cumbersome to run such autobuild.\n\nEssentially the extenasions are configured here:\n\nhttps://github.com/apache/airflow/blob/main/docs/conf.py\n\nAnd this conf.py is comples conference retrieving piece that sphinx loads when it is invoked above - and it determines what exactly will be configured, which internal parameters are used an which extensions will be used when a concrete package is build. There are lots of ifs, exclusions config params etc. that are dynamicallly calculated based on which package is built. \n\nFinally, all the extensions used by sphinx are here https://github.com/apache/airflow/tree/main/docs/exts - there are quite a few of those - loading data from provider.yaml, exampleincludes, intersphinx extensions, operators and hooks references, handling .txt redirects, templates to generate some summary or overview pages and so on.\n\n\nI think that\'s about it (though every time I look i find something new and interesting so I likely did not cover everything).', 'created_at': datetime.datetime(2025, 1, 17, 0, 40, 9, tzinfo=datetime.timezone.utc)}]","potiuk (Issue Creator) on (2024-11-30 13:57:03 UTC): This might also be easier/more natural to do as a follow up after #44511

RNHTTR on (2025-01-09 17:27:15 UTC): Here's an [Old dev list thread on the matter](https://lists.apache.org/list?dev@airflow.apache.org:2023-10:%22The%20GitHub%20Action%20for%20building%20docs%22). Maybe a good starting place is first developing all Airflow 3.0 (and new 2.x release) documentation using a static site generator like [Pelican](https://getpelican.com/). We actually have an [issue on airflow-site for this](https://github.com/apache/airflow-site/issues/719).

shahar1 on (2025-01-10 13:45:51 UTC): I assume that you meant https://github.com/apache/airflow-site/issues/719 in the second link?

RNHTTR on (2025-01-16 23:12:06 UTC): @potiuk Do you know how breeze serves the container when running `breeze build-docs`? I'm trying to use [sphinx-autobuild](https://github.com/sphinx-doc/sphinx-autobuild/tree/main) and port forward the docs page, but I can't quite figure out how.

potiuk (Issue Creator) on (2025-01-17 00:40:09 UTC): That's quite unlikely to be easy without  quite a redesign. Note I am not the author of it, I merely suffled around the scripts and code that was there from the beginning, so I do not know more than just by looking and reverse engineering of how it works, And if someone would like to redesign this (on top of what is already planned in fixing the infrastructuce pieces described here - they are absolutely welcome.

In short (you can see the sourcesin links:

1) `breeze build docs` takes the parameters passed via breeze CLI and convert them into Build Params that are then used as arguments of a shell script /opt/airflow/scripts/in_container/run_docs_build.sh that is run inside the container.  

https://github.com/apache/airflow/blob/main/dev/breeze/src/airflow_breeze/commands/developer_commands.py#L700

Also that breeze command makes sure that image is ready and rebuild. But essentially it calls this shell script:

```python
    cmd = ""/opt/airflow/scripts/in_container/run_docs_build.sh "" + "" "".join(
        [shlex.quote(arg) for arg in doc_builder.args_doc_builder]
```

2) This script does some housekeeping and cleanup/removal of stuff at completion but essentially it calls (in container):

```
python -m docs.build_docs ""${@}"" 
```

So passing the parameters passed to `build_docs` python script defined in https://github.com/apache/airflow/blob/main/docs/build_docs.py

3) This one can even be run directly with `--help` and show the parameters it can take (defined in argparse here https://github.com/apache/airflow/blob/main/docs/build_docs.py#L465) . That includes potential selection of the packages for which the documentation should be built

This script does quite a few more things:

* it fetches inventories from public inter-sphinx inventories so that links to source code referred from libraries can be automatically linked from sphinx

* it also fetches ""our"" package inventories (prepared in canary run and published in Airflow's amazon s3 bucket)  - so that for example if you only build one provider and refer to another or to airlfow, sphinx can properly build-inter-sphinx links to those ""external"" documents. Each provider, airflow, helm , ""proiders index"" are separate ""sphinx packages"" linked between each other via inter-sphinx inventories. When you build a package locally, the inventoy is regenerated and produced as part of the build, so when you build several packages locally they can refer to each-other's new APIs and pages added. This for example allows us to see that some links are missing when some pages or links are moved and we need to refer to them - with intersphinx we will see warnings (and error out) when such links are wrong

* It then selects packages to build - prioritising those that do not have inventories - because those should be build first - so that other packages can use the inventories when they are built together

* then the packages are built - the build is parallelised to allow to use multiple processors - each package is build by one of the N = CPU processors - this way building whole documentation on a 16 core machine will take less than 10 minutes rather than 1.5h - if they were run sequentially
 
* then there is interesting mechanism to attempt to retry building packages to allow to link to other locally built packages - sometimes when a package is being build it has a new page that other packages refer to (refactors and such) then such packages will fail until inventories are build for the source package. As packages are build in parallel it might mean that some packages might fail in the first pass, and they need 2nd or 3rd pass in order to succeed (depends how much ""circular"" or transitive package dependencies we have . This happens up to 3 times. See here: https://github.com/apache/airflow/blob/main/docs/build_docs.py#L534
 
* Buidling itself happens in the document builder class: https://github.com/apache/airflow/blob/main/docs/exts/docs_build/docs_builder.py - that clas prepares all the parameters needed to run sphinx command to run build for such a package. This command is derived here (for each package separately):

https://github.com/apache/airflow/blob/main/docs/exts/docs_build/docs_builder.py#L237

Essentially this:

```python
        build_cmd = [
            ""sphinx-build"",
            ""-T"",  # show full traceback on exception
            ""--color"",  # do emit colored output
            ""-b"",  # builder to use
            ""html"",
            ""-d"",  # path for the cached environment and doctree files
            self._doctree_dir,
            ""-c"",
            DOCS_DIR,
            ""-w"",  # write warnings (and errors) to given file
            self.log_build_warning_filename,
            self._src_dir,
            self._build_dir,  # path to output directory
        ]
```


* But then, each individual package is build via pretty complex sphinx extensions (and here I think where the complexity lies about making autobuild to work - as we have  a lot of extensions build and complex build configuration that might make it rather difficult and cumbersome to run such autobuild.

Essentially the extenasions are configured here:

https://github.com/apache/airflow/blob/main/docs/conf.py

And this conf.py is comples conference retrieving piece that sphinx loads when it is invoked above - and it determines what exactly will be configured, which internal parameters are used an which extensions will be used when a concrete package is build. There are lots of ifs, exclusions config params etc. that are dynamicallly calculated based on which package is built. 

Finally, all the extensions used by sphinx are here https://github.com/apache/airflow/tree/main/docs/exts - there are quite a few of those - loading data from provider.yaml, exampleincludes, intersphinx extensions, operators and hooks references, handling .txt redirects, templates to generate some summary or overview pages and so on.


I think that's about it (though every time I look i find something new and interesting so I likely did not cover everything).

"
2692671980,issue,closed,completed,"Dagbag import is taking longer time than an older version, which makes some tasks get timeout exceed errors","### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.3

### What happened?

We're using cloud composer (2.9.5) and airflow (2.9.3) on python(3.11) and, after making an upgrade from composer(2.5.4) airflow(2.5.3) python(3.8), we started  to face an issue concerning our Dagbag import process. We use python scripts to generate dags. one of those script, generate a specific type of dags called ""global"", this dags are made and scheduled to control/orchestrate all the other subdags using TriggerDagRunOperators. When executing the triggering tasks in the workers, after the worker process fork, it started to self._process_file() and that starts by making the import of the corresponding python file. Well when the worker forked process starts ""Filling up the dagbag [dag_file_folder]"" as the logs say, it lasts more than 60 secs and it gets terminated with an error Process timeout (PID:xxxxx). In our configs we have dagbag_import_timeout at 60 and dag_file_processor_timeout at 300. Increasing the dagbag_import_timeout to 180 has solved the problem, but we need to understand when this delay is coming from in order to find the pattern that will help to avoid this issues in the future. Especially that it happens only in the newer versiosn mentionned above. **Note that scaling up are worker by doubling the amount of memory and cpu, or decrease the worker_concurrency, didn't have any significant impact on the process timeout issue**.

### What you think should happen instead?

If we consider that python 3.11 is faster ([check_this_article](https://www.phoronix.com/review/python-311-performance)) than 3.8, we are supposed to have more efficient and faster import and file processing.

### How to reproduce

Run a dag with a relative large amount of libs dependencies, set the dagbag_impot_timeout just above the actual import delay in an composer(2.5.4) airflow(2.5.3) python(3.8), and then do run the same dag with the same timeout configs in composer (2.9.5) and airflow (2.9.3) on python(3.11). use small sized composer cluster.

### Operating System

Embedded in cloud composer 2.9.5

### Versions of Apache Airflow Providers

_No response_

### Deployment

Google Cloud Composer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",ksidata,2024-11-26 00:09:43+00:00,[],2024-11-26 23:52:26+00:00,2024-11-26 23:52:26+00:00,https://github.com/apache/airflow/issues/44372,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('area:DAG-processing', '')]","[{'comment_id': 2499288426, 'issue_id': 2692671980, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 26, 0, 9, 45, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-26 00:09:45 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2691259822,issue,open,,Change 'None' to created state & make `TI.State` not nullable,"The Task Instance starts could have a `None` state, so instead of starting from a `None` state, lets have an explicit `Created` state (similar to https://github.com/apache/airflow/pull/16401).

We should then make the `state` not-nullable",kaxil,2024-11-25 15:52:54+00:00,[],2024-11-25 16:04:55+00:00,,https://github.com/apache/airflow/issues/44362,"[('area:core', '')]",[],
2691257699,issue,closed,completed,Move DAG Params to Task SDK,Move [`DAG Param`](https://github.com/apache/airflow/blob/f03b1d4d996cf2b3f62c21f8c6a38aa53ba1e3be/airflow/models/param.py#L264) to Task SDK,kaxil,2024-11-25 15:52:07+00:00,['amoghrajesh'],2025-01-31 06:32:43+00:00,2025-01-31 06:32:42+00:00,https://github.com/apache/airflow/issues/44361,"[('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2691247291,issue,closed,completed,"Handling ""Task Mapping"" in the Task SDK","We need to port the logic to handle ""Mapped Tasks"" in the Task SDK.",kaxil,2024-11-25 15:48:00+00:00,['ashb'],2025-02-06 17:58:15+00:00,2025-02-06 17:58:14+00:00,https://github.com/apache/airflow/issues/44360,"[('area:dynamic-task-mapping', 'AIP-42'), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]","[{'comment_id': 2566410514, 'issue_id': 2691247291, 'author': 'amoghrajesh', 'body': 'Assigning the issue to @ashb. He will be picking it up after holidays.', 'created_at': datetime.datetime(2024, 12, 31, 12, 30, 27, tzinfo=datetime.timezone.utc)}]","amoghrajesh on (2024-12-31 12:30:27 UTC): Assigning the issue to @ashb. He will be picking it up after holidays.

"
2691239905,issue,closed,completed,Add Endpoint to set TI Rendered Fields,"Create Endpoint to get TI Rendered fields on the worker and save it to DB

https://github.com/apache/airflow/blob/acf106b5a174aa8a0b27542c34e355a3ac0ef39e/airflow/models/taskinstance.py#L2915-L2916",kaxil,2024-11-25 15:45:34+00:00,['amoghrajesh'],2024-12-09 06:05:09+00:00,2024-12-09 06:05:09+00:00,https://github.com/apache/airflow/issues/44359,"[('area:MetaDB', 'Meta Database related issues.'), ('area:API', ""Airflow's REST/HTTP API""), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]","[{'comment_id': 2518569395, 'issue_id': 2691239905, 'author': 'SuccessMoses', 'body': 'Hi @amoghrajesh, are you working on this?', 'created_at': datetime.datetime(2024, 12, 4, 21, 10, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2518623930, 'issue_id': 2691239905, 'author': 'amoghrajesh', 'body': 'Hello! @SuccessMoses \nYes, I am working on this one', 'created_at': datetime.datetime(2024, 12, 4, 21, 45, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2520026607, 'issue_id': 2691239905, 'author': 'amoghrajesh', 'body': 'The fix for this issue will cover the execution api side changes,  which expects a `dict[str, str]` to store into the RTIF table. `dict[str, str]` denotes the `field`: `<rendered value>`', 'created_at': datetime.datetime(2024, 12, 5, 11, 19, 46, tzinfo=datetime.timezone.utc)}]","SuccessMoses on (2024-12-04 21:10:57 UTC): Hi @amoghrajesh, are you working on this?

amoghrajesh (Assginee) on (2024-12-04 21:45:01 UTC): Hello! @SuccessMoses 
Yes, I am working on this one

amoghrajesh (Assginee) on (2024-12-05 11:19:46 UTC): The fix for this issue will cover the execution api side changes,  which expects a `dict[str, str]` to store into the RTIF table. `dict[str, str]` denotes the `field`: `<rendered value>`

"
2691228129,issue,closed,completed,Add support to Set an Airflow Variable from Task SDK,"Follow-up of https://github.com/apache/airflow/pull/44229 to set a Variable from TASK SDK via supervisor
",kaxil,2024-11-25 15:41:45+00:00,['amoghrajesh'],2024-12-05 07:29:37+00:00,2024-12-05 07:29:37+00:00,https://github.com/apache/airflow/issues/44358,"[('area:API', ""Airflow's REST/HTTP API""), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]","[{'comment_id': 2505453833, 'issue_id': 2691228129, 'author': 'amoghrajesh', 'body': 'I am breaking this one into two parts, one for the execution API side changes and the other for the task SDK side changes. This issue will track the task sdk side changes', 'created_at': datetime.datetime(2024, 11, 28, 7, 46, 46, tzinfo=datetime.timezone.utc)}]","amoghrajesh (Assginee) on (2024-11-28 07:46:46 UTC): I am breaking this one into two parts, one for the execution API side changes and the other for the task SDK side changes. This issue will track the task sdk side changes

"
2691155673,issue,closed,completed,Port LocalTaskJob tests to Task SDK,"To get more confidence in the new Supervisor process in the Task SDK, we  should investigate and port over tests from `LocalTaskJob` and some from `models.TI` to Task SDK's superisor's test",kaxil,2024-11-25 15:31:20+00:00,['kaxil'],2024-12-03 15:47:44+00:00,2024-12-03 15:47:44+00:00,https://github.com/apache/airflow/issues/44356,"[('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]","[{'comment_id': 2499353803, 'issue_id': 2691155673, 'author': 'kaxil', 'body': 'WIP: https://github.com/apache/airflow/compare/main...astronomer:airflow:ltj-tests?expand=1', 'created_at': datetime.datetime(2024, 11, 26, 1, 13, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508295412, 'issue_id': 2691155673, 'author': 'kaxil', 'body': 'https://github.com/apache/airflow/pull/44405 & https://github.com/apache/airflow/pull/44405 got merged\n\nPRs:\n- https://github.com/apache/airflow/pull/44465', 'created_at': datetime.datetime(2024, 11, 29, 18, 34, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508751449, 'issue_id': 2691155673, 'author': 'kaxil', 'body': 'Next Up:\n- Port `overtime` to handle OL -- https://github.com/apache/airflow/pull/39890 -- to Supervisor', 'created_at': datetime.datetime(2024, 11, 30, 0, 23, 13, tzinfo=datetime.timezone.utc)}]","kaxil (Issue Creator) on (2024-11-26 01:13:47 UTC): WIP: https://github.com/apache/airflow/compare/main...astronomer:airflow:ltj-tests?expand=1

kaxil (Issue Creator) on (2024-11-29 18:34:30 UTC): https://github.com/apache/airflow/pull/44405 & https://github.com/apache/airflow/pull/44405 got merged

PRs:
- https://github.com/apache/airflow/pull/44465

kaxil (Issue Creator) on (2024-11-30 00:23:13 UTC): Next Up:
- Port `overtime` to handle OL -- https://github.com/apache/airflow/pull/39890 -- to Supervisor

"
2691153157,issue,closed,completed,Add HTTP retry handling into task sdk's api.client,https://github.com/apache/airflow/pull/43893#discussion_r1837271237,kaxil,2024-11-25 15:30:23+00:00,['jscheffl'],2024-12-30 07:46:25+00:00,2024-12-27 22:51:47+00:00,https://github.com/apache/airflow/issues/44355,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API"")]","[{'comment_id': 2544876929, 'issue_id': 2691153157, 'author': 'jscheffl', 'body': ""Might be related to proposal in #44536 - I'd propose to have (at least parameters) in common"", 'created_at': datetime.datetime(2024, 12, 16, 8, 12, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2565131447, 'issue_id': 2691153157, 'author': 'kaxil', 'body': 'Thanks @jscheffl', 'created_at': datetime.datetime(2024, 12, 30, 7, 46, 24, tzinfo=datetime.timezone.utc)}]","jscheffl (Assginee) on (2024-12-16 08:12:58 UTC): Might be related to proposal in #44536 - I'd propose to have (at least parameters) in common

kaxil (Issue Creator) on (2024-12-30 07:46:24 UTC): Thanks @jscheffl

"
2691148575,issue,open,,Move callbacks to worker,"We will move callbacks to workers. TBD on “how”.

Options:

1. Moving callbacks to workers as a separate activity.
2. Running callbacks as teardown task.
3. Deprecate (not remove) concept of callback and pushing users to use teardown. If we do this and deprecate/not remove this option will happen with one of the above two options.
4. Leave them in dag-processor

I am leaning towards (2) — and (3) is my next preference. This is because imo callbacks and teardowns are sort of redundant concepts, and if we keep them as is, we will have an explosion of callbacks, we already have: `on_{success,failure,skipped,retry,execute}_callback` .",kaxil,2024-11-25 15:28:32+00:00,[],2025-01-17 15:24:17+00:00,,https://github.com/apache/airflow/issues/44354,"[('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]","[{'comment_id': 2500818701, 'issue_id': 2691148575, 'author': 'potiuk', 'body': 'The (2) sounds like a really good idea (not for all callbacks though). It would be great to see callbacks as implicit ""tasks"" in Airflow UI. Though not all callbacks can be done this way I am afraid. ""on_retry"" and ""on execute"" are particularly not fitting the ""teardown"" concept. The ""on_execute"" might be an implicit ""setup"" task, but ""on_retry"" is a bit problematic', 'created_at': datetime.datetime(2024, 11, 26, 13, 34, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2509628817, 'issue_id': 2691148575, 'author': 'eladkal', 'body': ""we need to consider what happens when task succeeded but callback wasn't. We need retry for callbacks but if for example worker is terminated between task success to callback execution how would it retry?\r\nAlso `on_sla_callback()` and o`n_execute_callback()` can't be part of teardown. We need to verify with AIP-86 what is the plan for sla callback as part of the deadline interface and if it conflicts with the plan we choose to take here cc @ferruzzi @romsharon98"", 'created_at': datetime.datetime(2024, 12, 1, 8, 17, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2515212561, 'issue_id': 2691148575, 'author': 'ferruzzi', 'body': ""The plan for Deadlines is to add a simple check in the scheduler loop, essentially (obviously pseudocode): `if (select min(deadline) from DeadlinesTable) < utcnow(): handle_deadline_misses()`  My intention was for the handler to spin up a new process to run the callbacks so it isn't holding anything else up, but let me know if that plan needs to change with this initiative."", 'created_at': datetime.datetime(2024, 12, 3, 17, 44, 18, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-11-26 13:34:13 UTC): The (2) sounds like a really good idea (not for all callbacks though). It would be great to see callbacks as implicit ""tasks"" in Airflow UI. Though not all callbacks can be done this way I am afraid. ""on_retry"" and ""on execute"" are particularly not fitting the ""teardown"" concept. The ""on_execute"" might be an implicit ""setup"" task, but ""on_retry"" is a bit problematic

eladkal on (2024-12-01 08:17:22 UTC): we need to consider what happens when task succeeded but callback wasn't. We need retry for callbacks but if for example worker is terminated between task success to callback execution how would it retry?
Also `on_sla_callback()` and o`n_execute_callback()` can't be part of teardown. We need to verify with AIP-86 what is the plan for sla callback as part of the deadline interface and if it conflicts with the plan we choose to take here cc @ferruzzi @romsharon98

ferruzzi on (2024-12-03 17:44:18 UTC): The plan for Deadlines is to add a simple check in the scheduler loop, essentially (obviously pseudocode): `if (select min(deadline) from DeadlinesTable) < utcnow(): handle_deadline_misses()`  My intention was for the handler to spin up a new process to run the callbacks so it isn't holding anything else up, but let me know if that plan needs to change with this initiative.

"
2691147677,issue,open,,"SPIKE: Using ""Airflow Exceptions"" in Task SDK without importing core","Similar to https://github.com/apache/airflow/issues/44352, we need to figure a way to use Airflow Exception in the Task SDK without importing Core.",kaxil,2024-11-25 15:28:12+00:00,[],2025-01-17 15:24:16+00:00,,https://github.com/apache/airflow/issues/44353,"[('area:core', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]","[{'comment_id': 2499517214, 'issue_id': 2691147677, 'author': 'potiuk', 'body': 'Comment and proposal:\r\n\r\nI think we won\'t avoid some ""airflow.common"" eventually for thos and #44352 reason. It falls into packaging decisions that we deferred, but I think the cleanest way is to have some `common` package.\r\n\r\nWe have all the experiences from the provider\'s there and we should learn from it.\r\n\r\nThis package could be **potentially** embedded into TaskSDK and Core (with changed package name for example), but then there is a whole set of problems what happens if a different version of same package is embedded in different packages installed at the same time (at least for Local Executor and standalone case we will have to have both Task SDK and ""core"" installed together). And at the same time we want to update Airlfow code independently from Workers core - which makes it tricky as we should be able to handle the case where workers ""common"" code is older than ""core"" common code.\r\n\r\nIMHO - cleanest and simplest solution for this package is to have a separate package that comes with Airlfow and it will have to have a very strict compatibility policy - something we already went through with `common.sql` for example:\r\n\r\na) it should never have a breaking change\r\nb) it always be back-compatible (which is another way of saying a) \r\nc) we should have some tests that should make sure that breaking change did not happen accidentally.\r\nd) TaskSDK package should have `>= SOME_3_X_0` requirement for the common code  - and should work with any ""future"" version of the common code\r\n\r\nThe (somewhat successful but likely could be improved) attempt of doing it was in https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/common/sql/hooks/sql.pyi which is automatcally genereed by `update-common-sql-api-stubs` pre-commit. Which then delegated the check if interface changed to `mypy`. But IMHO it is far too little (and unnecessary if we attempt to actually test the common code for compatibility).\r\n\r\nWhat really works (and we have it in providers) is running current tests against older versions of packages we want to tests - - this has much more ""compatibility"" properties than checking if the Python API changed  (because it also tests the behaviour). \r\n\r\nSo we could potentially used similar techniques we use for providers now, where we determine the ""lowest direct"" set of dependencies and run ""compatibility"" tests of new providers with older airflow versions  - only here it would have to be more of a ""forward"" testing - we should get tests for the *OLDER* released Task SDK tests and run them against NEWER common code. This is all doable I think - Task SDK and set of tests it runs should be relatvely small, and we should be able to add CI job to checkout 3.0, 3.1, 3.2, .... Task SDK tests and run them against latest ""common"" code to see if there are any compatibility issues.\r\n\r\nI think it would be good to agree on some rules here - how long we can expect the old ""common"" to work with new ""core"" - and allow to remove some old, deprecated code. Also that will allow us to improve and evolve tests code, because it\'s a bit trickly to run test code checkout from one branch/version and run it with code from another version (this is the set of test compatibities we already have in provider\'s compatibility tests - and we will only be able to maintain it for a long time because we have the rule of 6 months support for old Airflow versions, it would be next to impossible to take main `tests` and run them on 2.0 for example - we are going to have very similar situation with running ""old task sdk tests"" with ""new common code""  and we should limit the number of versions we should run it with.\r\n\r\nThat would be my proposal how to solve it - and providers could be use as a showcase on kinds of issues we will need to handle and show that it can work.', 'created_at': datetime.datetime(2024, 11, 26, 2, 27, 57, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-11-26 02:27:57 UTC): Comment and proposal:

I think we won't avoid some ""airflow.common"" eventually for thos and #44352 reason. It falls into packaging decisions that we deferred, but I think the cleanest way is to have some `common` package.

We have all the experiences from the provider's there and we should learn from it.

This package could be **potentially** embedded into TaskSDK and Core (with changed package name for example), but then there is a whole set of problems what happens if a different version of same package is embedded in different packages installed at the same time (at least for Local Executor and standalone case we will have to have both Task SDK and ""core"" installed together). And at the same time we want to update Airlfow code independently from Workers core - which makes it tricky as we should be able to handle the case where workers ""common"" code is older than ""core"" common code.

IMHO - cleanest and simplest solution for this package is to have a separate package that comes with Airlfow and it will have to have a very strict compatibility policy - something we already went through with `common.sql` for example:

a) it should never have a breaking change
b) it always be back-compatible (which is another way of saying a) 
c) we should have some tests that should make sure that breaking change did not happen accidentally.
d) TaskSDK package should have `>= SOME_3_X_0` requirement for the common code  - and should work with any ""future"" version of the common code

The (somewhat successful but likely could be improved) attempt of doing it was in https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/common/sql/hooks/sql.pyi which is automatcally genereed by `update-common-sql-api-stubs` pre-commit. Which then delegated the check if interface changed to `mypy`. But IMHO it is far too little (and unnecessary if we attempt to actually test the common code for compatibility).

What really works (and we have it in providers) is running current tests against older versions of packages we want to tests - - this has much more ""compatibility"" properties than checking if the Python API changed  (because it also tests the behaviour). 

So we could potentially used similar techniques we use for providers now, where we determine the ""lowest direct"" set of dependencies and run ""compatibility"" tests of new providers with older airflow versions  - only here it would have to be more of a ""forward"" testing - we should get tests for the *OLDER* released Task SDK tests and run them against NEWER common code. This is all doable I think - Task SDK and set of tests it runs should be relatvely small, and we should be able to add CI job to checkout 3.0, 3.1, 3.2, .... Task SDK tests and run them against latest ""common"" code to see if there are any compatibility issues.

I think it would be good to agree on some rules here - how long we can expect the old ""common"" to work with new ""core"" - and allow to remove some old, deprecated code. Also that will allow us to improve and evolve tests code, because it's a bit trickly to run test code checkout from one branch/version and run it with code from another version (this is the set of test compatibities we already have in provider's compatibility tests - and we will only be able to maintain it for a long time because we have the rule of 6 months support for old Airflow versions, it would be next to impossible to take main `tests` and run them on 2.0 for example - we are going to have very similar situation with running ""old task sdk tests"" with ""new common code""  and we should limit the number of versions we should run it with.

That would be my proposal how to solve it - and providers could be use as a showcase on kinds of issues we will need to handle and show that it can work.

"
2691140320,issue,open,,Handling “Airflow confs” in Task SDK,Figure out how to retrieve Airflow configurations (from `airflow.cfg`) in the Task SDK without importing Airflow core.,kaxil,2024-11-25 15:25:57+00:00,[],2025-01-17 15:24:16+00:00,,https://github.com/apache/airflow/issues/44352,"[('priority:medium', 'Bug that should be fixed before next release but would not block a release'), ('area:core', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]","[{'comment_id': 2532577200, 'issue_id': 2691140320, 'author': 'kaxil', 'body': '@potiuk \'s options here: https://lists.apache.org/thread/8jtgr1m072lgsy4tl3km9ptb216vqjds\n\n> we have three\n> possible options to tell our users:.\n> \n> 1) either copy to workers only the configuration that is needed (but which\n> ones are needed and how to separate those config entries in the docs?)\n> 2) or NO configuration should be present in the worker - and all\n> configuration that is needed should be passed by Task SDK to the worker\n> 3) or we assess that task and dag file processors do not need any\n> configuration at all\n> \n> I think if we assess that 3) is the case - there is no other option but to\n> remove conf. That would also be really nice as we would not have to have\n> ""conf"" reading code in TaskSDK at all.\n> But if we think that tasks and dag parsing will actually need some\n> configuration, then we can go either 1) or 2) route\n> \n> I think option 2) is much better from less confusion, security and ""single\n> source of truth"" point of view. And if we go for 2) then `namespace =\n> conf.get(""kubernetes"", ""NAMESPACE"")` will not work because configuration\n> will not be present there and in this case what would make more sense is to\n> leave ""conf"" as part of the context but only pass the ""needed""\n> configuration there.\n> If we go for option 1) then likely yes we could remove it from context.', 'created_at': datetime.datetime(2024, 12, 10, 18, 32, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2532606098, 'issue_id': 2691140320, 'author': 'potiuk', 'body': ""Thanks @kaxil - yeah, that's the right place to discuss it :)"", 'created_at': datetime.datetime(2024, 12, 10, 18, 48, 34, tzinfo=datetime.timezone.utc)}]","kaxil (Issue Creator) on (2024-12-10 18:32:41 UTC): @potiuk 's options here: https://lists.apache.org/thread/8jtgr1m072lgsy4tl3km9ptb216vqjds

potiuk on (2024-12-10 18:48:34 UTC): Thanks @kaxil - yeah, that's the right place to discuss it :)

"
2691129538,issue,closed,completed,Handle Task retries in Task SDK,"Follow-up of https://github.com/apache/airflow/pull/44241 to handle Task retries i.e. checking max retries, setting correct state etc",kaxil,2024-11-25 15:22:11+00:00,['amoghrajesh'],2024-12-30 08:33:56+00:00,2024-12-30 08:33:56+00:00,https://github.com/apache/airflow/issues/44351,"[('area:core', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2691117631,issue,closed,completed,Add Endpoint to Push XCom from Task SDK,"Similar to the XCom get endpoint in the Task Execution API (added in [this PR](https://github.com/apache/airflow/pull/44101)), we should add a way to push XCom from a Task to API server (via Supervisor)",kaxil,2024-11-25 15:17:47+00:00,['amoghrajesh'],2024-12-06 17:05:51+00:00,2024-12-06 17:05:51+00:00,https://github.com/apache/airflow/issues/44350,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API"")]",[],
2690255653,issue,open,,'clear task' on backfilled tasks does not reschedule them without rerunning backfill command,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Pressing 'clear task' on any tasks from a backfill will not reschedule them without rerunning the backfill command by CLI.

### What you think should happen instead?

Cleared tasks should get queued again even if they originate from a backfill.

### How to reproduce

- backfill a run
- clear task any of its completed tasks
- find it will not reschedule


### Operating System

ubuntu 24.04

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",kiaradlf,2024-11-25 10:46:53+00:00,[],2024-11-25 10:49:06+00:00,,https://github.com/apache/airflow/issues/44343,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:backfill', 'Specifically for backfill related'), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2689711512,issue,open,,Log URL redirecting to No XCom error,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.1

### What happened?

The Log URL (`ti.log_url`) from the triggered retry email doesn't redirect to the Logs when clicked but instead shows 'No XCom' as the error in the Airflow UI. The Logs are shown when a time in future is selected as the `base_date` in the UI.

Before changing the base_date in the UI,

![image](https://github.com/user-attachments/assets/6a4a2edd-a2c6-42ca-95d2-6315ba4acc48)

After adding 1 second to the base_date in UI,

![image](https://github.com/user-attachments/assets/7cc6af14-92f9-46e0-8ac8-7abc63abaf54)




### What you think should happen instead?

I think the issue might be due to milli seconds getting truncated which causes the difference between the execution datetime (untruncated) and the execution datetime rendered - `base_date` (truncated) in the `ti.log_url`

### How to reproduce

Here is the DAG to reproduce the issue,

```
from airflow.decorators import dag, task
from datetime import datetime, timedelta

@dag(
    dag_id='default_email_on_retry_example',
    schedule_interval='@once',
    start_date=datetime(2023, 1, 1),
    catchup=False,
    default_args={
        'email': ['your@email.com'],  
        'email_on_retry': True,  
        'email_on_failure': False,  
    },
)
def default_email_on_retry_dag():

    @task
    def success_task():
        print(""This task will always succeed!"")

    @task(task_id='retry_task', retries=3, retry_delay=timedelta(seconds=15))
    def retry_task():
        print(""This task will retry and fail."")
        raise Exception(""Simulated task failure for retry."")

    success = success_task()
    retry = retry_task()

    success >> retry

default_email_on_retry_dag()
```

![image](https://github.com/user-attachments/assets/b6394374-69cf-4d88-bc6d-6154e8ef9ecb)

Click the Log `Link` from the email triggered by the DAG

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",sam-gen-cop,2024-11-25 08:19:23+00:00,['pranav-cs-1'],2025-02-01 17:26:24+00:00,,https://github.com/apache/airflow/issues/44337,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2497202895, 'issue_id': 2689711512, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 25, 8, 19, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507954068, 'issue_id': 2689711512, 'author': 'dada-engineer', 'body': '+1', 'created_at': datetime.datetime(2024, 11, 29, 14, 45, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2547176366, 'issue_id': 2689711512, 'author': 'moz3', 'body': '+1', 'created_at': datetime.datetime(2024, 12, 16, 23, 58, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2552834438, 'issue_id': 2689711512, 'author': 'pranav-cs-1', 'body': 'Hi @potiuk could you please assign the issue to me?', 'created_at': datetime.datetime(2024, 12, 19, 5, 42, 26, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-25 08:19:26 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

dada-engineer on (2024-11-29 14:45:23 UTC): +1

moz3 on (2024-12-16 23:58:01 UTC): +1

pranav-cs-1 (Assginee) on (2024-12-19 05:42:26 UTC): Hi @potiuk could you please assign the issue to me?

"
2689534635,issue,open,,Different `max_active_runs` for scheduled vs manual runs,"### Description

Hi
We have a use case where a DAG is scheduled similar to `ContinuousTimetable`, where it should run as often as possible, but it shouldn't be scheduled to run more than one at a time. 
The same DAG can be started manually, with a set of parameters to override the default behaviour. When someone decides to start manually, it is OK to have 2 or more of them running concurrently.

I can't find any way to implement this, setting `max_active_runs=1`, constrains also the manual runs.

I would like a new parameter (`max_active_scheduled_runs`?), or some new extension point in the `TimeTable` API, that can take all ongoing runs into consideration and use that to withhold scheduling new runs.

### Use case/motivation

Some example use cases

* The scheduled runs are running a complete test suite that takes many hours. Using Params, a user can select one specific test to run, that will only take minutes to run.
* Shortening lead times. Imagine a run taking 1 hour. Half an hour in you realize you need to do a new run with some new data that became available. Instead of waiting for the current run to finish or aborting the current run, you can manually override to start a second run in parallel.  Always having a half-hour cadence active is not desirable due to cost.


### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",hterik,2024-11-25 07:30:35+00:00,[],2024-11-25 07:32:48+00:00,,https://github.com/apache/airflow/issues/44336,"[('area:Scheduler', 'including HA (high availability) scheduler'), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2687501418,issue,closed,completed,KPO async -  kube_config_path not supported,"### Apache Airflow version

2.10.3


### What happened?



```log
[2024-11-24, 13:59:18 UTC] {base.py:84} INFO - Retrieving connection 'kubernetes_default'
[2024-11-24, 13:59:18 UTC] {pod_manager.py:622} INFO - Pod task-two-qli8k1x6 has phase Pending
[2024-11-24, 13:59:20 UTC] {pod.py:988} INFO - Deleting pod: task-two-qli8k1x6
[2024-11-24, 13:59:20 UTC] {taskinstance.py:3311} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 1804, in resume_execution
    return execute_callable(context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py"", line 781, in trigger_reentry
    raise AirflowException(message)
airflow.exceptions.AirflowException: Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/triggers/pod.py"", line 162, in run
    state = await self._wait_for_pod_start()
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/triggers/pod.py"", line 223, in _wait_for_pod_start
    pod = await self.hook.get_pod(self.pod_name, self.pod_namespace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/hooks/kubernetes.py"", line 754, in get_pod
    async with self.get_conn() as connection:
               ^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/contextlib.py"", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/hooks/kubernetes.py"", line 741, in get_conn
    kube_client = await self._load_config() or async_client.ApiClient()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/hooks/kubernetes.py"", line 711, in _load_config
    await async_config.load_kube_config(
  File ""/home/airflow/.local/lib/python3.12/site-packages/kubernetes_asyncio/config/kube_config.py"", line 603, in load_kube_config
    loader = _get_kube_config_loader_for_yaml_file(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/kubernetes_asyncio/config/kube_config.py"", line 567, in _get_kube_config_loader_for_yaml_file
    return KubeConfigLoader(
           ^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/kubernetes_asyncio/config/kube_config.py"", line 150, in __init__
    self.set_active_context(active_context)
  File ""/home/airflow/.local/lib/python3.12/site-packages/kubernetes_asyncio/config/kube_config.py"", line 162, in set_active_context
    self._current_context = self._config['contexts'].get_with_name(
                            ~~~~~~~~~~~~^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/kubernetes_asyncio/config/kube_config.py"", line 448, in __getitem__
    raise ConfigException(
kubernetes_asyncio.config.config_exception.ConfigException: Invalid kube-config file. Expected key contexts in kube-config
```

### How to reproduce

[kind](https://github.com/kubernetes-sigs/kind/) 0.24.0

```
kind create cluster
kind get kubeconfig --internal > conf/kube_conf
```

->

```yaml
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: L...
    server: https://kind-control-plane:6443
  name: kind-kind
contexts:
- context:
    cluster: kind-kind
    user: kind-kind
  name: kind-kind
current-context: kind-kind
kind: Config
preferences: {}
users:
- name: kind-kind
  user:
    client-certificate-data: L...
    client-key-data: L....


```


airflow connection

```json
  ""kubernetes_default"": {
    ""conn_type"": ""kubernetes"",
    ""extra"": ""{\""extra__kubernetes__in_cluster\"": false, \""extra__kubernetes__kube_config_path\"": \""/opt/airflow/include/.kube/config\"", \""extra__kubernetes__namespace\"": \""default\"", \""extra__kubernetes__cluster_context\"": \""kind-kind\"", \""extra__kubernetes__disable_verify_ssl\"": false, \""extra__kubernetes__disable_tcp_keepalive\"": false, \""xcom_sidecar_container_image\"": \""alpine:3.16.2\""}""
  }
```


```python
from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator

dag = DAG(
    dag_id=""kubernetes_dag"",
    schedule_interval=None,
    start_date=days_ago(1),
)

with dag:
    cmd = ""echo toto && sleep 4 && echo finish""

    KubernetesPodOperator(
        task_id=""task-one"",
        namespace=""default"",
        kubernetes_conn_id=""kubernetes_default"",
        image=""alpine:3.16.2"",
        cmds=[""sh"", ""-c"", cmd],
        deferrable=True,
    )

```

### Operating System

ubuntu 22.04

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes==10.0.0

### Deployment

Docker-Compose


### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",raphaelauv,2024-11-24 10:30:02+00:00,[],2025-02-07 13:50:17+00:00,2025-02-07 13:50:16+00:00,https://github.com/apache/airflow/issues/44325,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2546072263, 'issue_id': 2687501418, 'author': 'yovio-rca', 'body': 'I get the same issue with KubernetesPodOperator with deferrable=True and in_cluster = True (default)\r\n\r\nMy Airflow runs on EKS cluster. I saw similar issue with EKSPodOperator as well in this issue: https://github.com/apache/airflow/issues/39685', 'created_at': datetime.datetime(2024, 12, 16, 16, 23, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2546175883, 'issue_id': 2687501418, 'author': 'yovio-rca', 'body': 'I managed to resolve the issue by making sure to set in_cluster attribute in KubernetesPodOperator to true.\r\n\r\nIf unset the default is None and I think the subsequent logic will treat it as False.\r\n\r\nIMHO, the behavior need to change by looking at the in_cluster value from Configuration is attribute is None.\r\n\r\nThe reason because by default in official helm chart, if we do not set in_cluster in config, it will be set to true.\r\n\r\nWhen you wrote a dag, most people probably will just leave it default or unset.\r\n\r\nBtw, the dag was working fine in ""sync"" mode, meaning I dont have to set in_cluster attribute and it will work fine.', 'created_at': datetime.datetime(2024, 12, 16, 17, 6, 34, tzinfo=datetime.timezone.utc)}]","yovio-rca on (2024-12-16 16:23:26 UTC): I get the same issue with KubernetesPodOperator with deferrable=True and in_cluster = True (default)

My Airflow runs on EKS cluster. I saw similar issue with EKSPodOperator as well in this issue: https://github.com/apache/airflow/issues/39685

yovio-rca on (2024-12-16 17:06:34 UTC): I managed to resolve the issue by making sure to set in_cluster attribute in KubernetesPodOperator to true.

If unset the default is None and I think the subsequent logic will treat it as False.

IMHO, the behavior need to change by looking at the in_cluster value from Configuration is attribute is None.

The reason because by default in official helm chart, if we do not set in_cluster in config, it will be set to true.

When you wrote a dag, most people probably will just leave it default or unset.

Btw, the dag was working fine in ""sync"" mode, meaning I dont have to set in_cluster attribute and it will work fine.

"
2687473464,issue,closed,completed,"Status of testing Providers that were prepared on November 24, 2024","### Body


I have a kind request for all the contributors to the latest provider packages release.
Could you please help us to test the RC versions of the providers?

The guidelines on how to test providers can be found in

[Verify providers by contributors](https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDER_PACKAGES.md#verify-the-release-candidate-by-contributors)

Let us know in the comment, whether the issue is addressed.

Those are providers that require testing as there were some substantial changes introduced:


## Provider [celery: 3.8.5rc1](https://pypi.org/project/apache-airflow-providers-celery/3.8.5rc1)
   - [ ] [Allow for retry when tasks are stuck in queued (#43520)](https://github.com/apache/airflow/pull/43520): @dimberman
## Provider [cncf.kubernetes: 10.0.1rc1](https://pypi.org/project/apache-airflow-providers-cncf-kubernetes/10.0.1rc1)
   - [x] [Bugfix KubernetesJobOperator.on_kill() (#44131)](https://github.com/apache/airflow/pull/44131): @moiseenkov
   - [ ] [Allow for retry when tasks are stuck in queued (#43520)](https://github.com/apache/airflow/pull/43520): @dimberman
   - [ ] [Rename execution_date to logical_date across codebase (#43902)](https://github.com/apache/airflow/pull/43902): @sunank200
## Provider [google: 11.0.0rc1](https://pypi.org/project/apache-airflow-providers-google/11.0.0rc1)
   - [x] [Remove deprecated functionality from Google provider (#43953)](https://github.com/apache/airflow/pull/43953): @moiseenkov
   - [x] [feat: add OpenLineage support for BigQueryToBigQueryOperator (#44214)](https://github.com/apache/airflow/pull/44214): @kacpermuda
   - [ ] [Introduce gcp advance (V3) API translate native datasets operators (#44271)](https://github.com/apache/airflow/pull/44271): @olegkachur-e
   - [ ] [Introduce new gcp TranslateText and TranslateTextBatch operators (#43860)](https://github.com/apache/airflow/pull/43860): @olegkachur-e
   - [ ] [Add gcloud command to DataprocCreateClusterOperator to be able to create dataproc on GKE cluster (#44185)](https://github.com/apache/airflow/pull/44185): @MaksYermak
   - [x] [Fix incorrect query in `BigQueryAsyncHook.create_job_for_partition_get` (#44225)](https://github.com/apache/airflow/pull/44225): @sean-rose
     Linked issues:
       - [x] [Linked Issue #37655](https://github.com/apache/airflow/pull/37655): @Silviu-Surcica
   - [x] [Fix Dataplex Data Quality partial update (#44262)](https://github.com/apache/airflow/pull/44262): @amirmor1
   - [ ] [Bump `google-cloud-translate` to `3.16` (#44297)](https://github.com/apache/airflow/pull/44297): @kaxil
   - [ ] [fix google datacatalog operator tests (#44281)](https://github.com/apache/airflow/pull/44281): @gopidesupavan
   - [ ] [Rename execution_date to logical_date across codebase (#43902)](https://github.com/apache/airflow/pull/43902): @sunank200
## Provider [standard: 0.0.2rc1](https://pypi.org/project/apache-airflow-providers-standard/0.0.2rc1)
   - [x] [Fix TriggerDagRunOperator extra_link when trigger_dag_id is templated (#42810)](https://github.com/apache/airflow/pull/42810): @fredthomsen
   - [x] [Move `TriggerDagRunOperator` to standard provider (#44053)](https://github.com/apache/airflow/pull/44053): @hardeybisey
     Linked issues:
       - [x] [Linked Issue #43641](https://github.com/apache/airflow/issues/43641): @gopidesupavan
   - [x] [Move filesystem sensor to standard provider (#43890)](https://github.com/apache/airflow/pull/43890): @kunaljubce
     Linked issues:
       - [x] [Linked Issue #43641](https://github.com/apache/airflow/issues/43641): @gopidesupavan
   - [ ] [Rename execution_date to logical_date across codebase (#43902)](https://github.com/apache/airflow/pull/43902): @sunank200

<!--

NOTE TO RELEASE MANAGER:

You can move here the providers that have doc-only changes or for which changes are trivial, and
you could assess that they are OK.

-->
All users involved in the PRs:
@amirmor1 @dimberman @hardeybisey @fredthomsen @MaksYermak @sunank200 @kunaljubce @kacpermuda @sean-rose @kaxil @olegkachur-e @moiseenkov @gopidesupavan


### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2024-11-24 09:44:50+00:00,[],2024-11-27 13:20:36+00:00,2024-11-27 13:20:36+00:00,https://github.com/apache/airflow/issues/44324,"[('provider:google', 'Google (including GCP) related issues'), ('area:providers', ''), ('kind:meta', 'High-level information important to the community'), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('testing status', 'Status of testing releases'), ('provider:celery', '')]","[{'comment_id': 2497077968, 'issue_id': 2687473464, 'author': 'moiseenkov', 'body': 'Hi, \r\n#44131 and #43953 look good', 'created_at': datetime.datetime(2024, 11, 25, 7, 31, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2497138358, 'issue_id': 2687473464, 'author': 'hardeybisey', 'body': 'Hi, [#44053](https://github.com/apache/airflow/pull/44053) works as expected.', 'created_at': datetime.datetime(2024, 11, 25, 7, 54, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2497785682, 'issue_id': 2687473464, 'author': 'fredthomsen', 'body': '#42810 is good.', 'created_at': datetime.datetime(2024, 11, 25, 11, 44, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2497877266, 'issue_id': 2687473464, 'author': 'kacpermuda', 'body': '#44214 is good', 'created_at': datetime.datetime(2024, 11, 25, 12, 24, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2498875886, 'issue_id': 2687473464, 'author': 'amirmor1', 'body': '#44262 works as expected', 'created_at': datetime.datetime(2024, 11, 25, 19, 36, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2501638111, 'issue_id': 2687473464, 'author': 'sean-rose', 'body': 'I verified that #44225 works.', 'created_at': datetime.datetime(2024, 11, 26, 18, 19, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2503738448, 'issue_id': 2687473464, 'author': 'kunaljubce', 'body': '@eladkal So I verified that #43890 works with `apache-airflow>=2.10.0` - \r\n\r\n<img width=""746"" alt=""image"" src=""https://github.com/user-attachments/assets/e6fe96ac-45d5-4b56-9778-d6441f4a6265"">\r\n\r\nHowever, there\'s an error when I do the same with `apache-airflow==2.9.3` - \r\n\r\n<img width=""743"" alt=""image"" src=""https://github.com/user-attachments/assets/025af1ed-0aa9-410a-9a72-5534765db73c"">\r\n\r\nUnless I am doing something wrong, in which case please let me know, it seems that this is happening because of #41021 which seems to be linked to the airflow 2.10.0 milestone. If that\'s the case, maybe we should update the documentation for it on PyPi?\r\n\r\n<img width=""795"" alt=""image"" src=""https://github.com/user-attachments/assets/77548938-973d-4a03-8cc0-90d8d556ec70"">', 'created_at': datetime.datetime(2024, 11, 27, 12, 21, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2503768422, 'issue_id': 2687473464, 'author': 'potiuk', 'body': '> However, there\'s an error when I do the same with apache-airflow==2.9.3 \r\n\r\nGood catch. Apparently we miss compatibility class for that in ""common.compat"" and the sensor has no corresponding tests that would detect it in ""providers/tests/standard/sensors""  - the test for filesystem sensor is still in ""tests/sensors"" that\'s why our test suite did not catch it.\r\n\r\n> If that\'s the case, maybe we should update the documentation for it on PyPi?\r\n\r\nNope. We want to keep 2.8+ compatibility.  We should add compatibilty `StartTriggerArgs` class in ""common.compat"" similarly to other ""after 2.8"" features, and import the class from there in the sensor.\r\n\r\nI guess @eladkal  - since it\'s not a regression and standard provider is still 0.* it\'s not a blocker for the release and can be fixed as a follow-up.', 'created_at': datetime.datetime(2024, 11, 27, 12, 36, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2503836282, 'issue_id': 2687473464, 'author': 'eladkal', 'body': '@potiuk is correct, we don\'t consider this as a regression as standard provider is not yet ""stable"".\r\nSpecifically for 2.9.x the sensor is reachable by importing from core path. The PR in question just moved it from core to providers so while it doesn\'t work as intended there is simple workaround for 2.9', 'created_at': datetime.datetime(2024, 11, 27, 13, 7, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2503837462, 'issue_id': 2687473464, 'author': 'eladkal', 'body': '@kunaljubce if you can raise a PR with a fix that would be much appreciated', 'created_at': datetime.datetime(2024, 11, 27, 13, 8, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2503864119, 'issue_id': 2687473464, 'author': 'eladkal', 'body': 'Thank you everyone. Providers are released.\r\n\r\nI invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).', 'created_at': datetime.datetime(2024, 11, 27, 13, 20, 36, tzinfo=datetime.timezone.utc)}]","moiseenkov on (2024-11-25 07:31:23 UTC): Hi, 
#44131 and #43953 look good

hardeybisey on (2024-11-25 07:54:27 UTC): Hi, [#44053](https://github.com/apache/airflow/pull/44053) works as expected.

fredthomsen on (2024-11-25 11:44:12 UTC): #42810 is good.

kacpermuda on (2024-11-25 12:24:35 UTC): #44214 is good

amirmor1 on (2024-11-25 19:36:36 UTC): #44262 works as expected

sean-rose on (2024-11-26 18:19:31 UTC): I verified that #44225 works.

kunaljubce on (2024-11-27 12:21:53 UTC): @eladkal So I verified that #43890 works with `apache-airflow>=2.10.0` - 

<img width=""746"" alt=""image"" src=""https://github.com/user-attachments/assets/e6fe96ac-45d5-4b56-9778-d6441f4a6265"">

However, there's an error when I do the same with `apache-airflow==2.9.3` - 

<img width=""743"" alt=""image"" src=""https://github.com/user-attachments/assets/025af1ed-0aa9-410a-9a72-5534765db73c"">

Unless I am doing something wrong, in which case please let me know, it seems that this is happening because of #41021 which seems to be linked to the airflow 2.10.0 milestone. If that's the case, maybe we should update the documentation for it on PyPi?

<img width=""795"" alt=""image"" src=""https://github.com/user-attachments/assets/77548938-973d-4a03-8cc0-90d8d556ec70"">

potiuk on (2024-11-27 12:36:12 UTC): Good catch. Apparently we miss compatibility class for that in ""common.compat"" and the sensor has no corresponding tests that would detect it in ""providers/tests/standard/sensors""  - the test for filesystem sensor is still in ""tests/sensors"" that's why our test suite did not catch it.


Nope. We want to keep 2.8+ compatibility.  We should add compatibilty `StartTriggerArgs` class in ""common.compat"" similarly to other ""after 2.8"" features, and import the class from there in the sensor.

I guess @eladkal  - since it's not a regression and standard provider is still 0.* it's not a blocker for the release and can be fixed as a follow-up.

eladkal (Issue Creator) on (2024-11-27 13:07:48 UTC): @potiuk is correct, we don't consider this as a regression as standard provider is not yet ""stable"".
Specifically for 2.9.x the sensor is reachable by importing from core path. The PR in question just moved it from core to providers so while it doesn't work as intended there is simple workaround for 2.9

eladkal (Issue Creator) on (2024-11-27 13:08:21 UTC): @kunaljubce if you can raise a PR with a fix that would be much appreciated

eladkal (Issue Creator) on (2024-11-27 13:20:36 UTC): Thank you everyone. Providers are released.

I invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).

"
2683645033,issue,open,,xCom Templating for init_containers in KubernetesPodOperator,"### Description

We should be able to specify the `image`, `cmd`, `args`, etc. fields of init containers as xCom templates. Currently this is not supported.

### Use case/motivation

In our dags, we need a pod to have init containers with the image and command of those containers determined dynamically at runtime. Having the ability to pass these from upstream xComs and be able to template them into the PodOperator would be useful here.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",dimohammed328,2024-11-22 15:41:44+00:00,[],2024-11-22 15:44:04+00:00,,https://github.com/apache/airflow/issues/44289,"[('area:providers', ''), ('kind:feature', 'Feature Requests'), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2494054288, 'issue_id': 2683645033, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 22, 15, 41, 48, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-22 15:41:48 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2683253708,issue,closed,not_planned,Unable to create Base Azure connection using Azure SPN credentials,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I am trying to create a Basic Azure Connection with SPN credentials, but getting this error - 
**AzureBaseHook.__init__() got an unexpected keyword argument 'azure_conn_id'**
![image](https://github.com/user-attachments/assets/ead961ee-994a-45e8-bebd-57b2b61ea963)

I checked the library present already **apache-airflow-providers-microsoft-azure**
airflow@97aa33b3fa4c:/opt/airflow$ pip show apache-airflow-providers-microsoft-azure
Name: apache-airflow-providers-microsoft-azure
Version: 11.0.0

Can some one please help? 

### What you think should happen instead?

_No response_

### How to reproduce

Try creating a new Azure Basic connection with Airflow 2.10.3.

### Operating System

linux

### Versions of Apache Airflow Providers

pip freeze | grep apache-airflow-providers
apache-airflow-providers-amazon==9.0.0
apache-airflow-providers-celery==3.8.3
apache-airflow-providers-cncf-kubernetes==9.0.1
apache-airflow-providers-common-compat==1.2.1
apache-airflow-providers-common-io==1.4.2
apache-airflow-providers-common-sql==1.19.0
apache-airflow-providers-docker==3.14.0
apache-airflow-providers-elasticsearch==5.5.2
apache-airflow-providers-fab==1.5.0
apache-airflow-providers-ftp==3.11.1
apache-airflow-providers-google==10.25.0
apache-airflow-providers-grpc==3.6.0
apache-airflow-providers-hashicorp==3.8.0
apache-airflow-providers-http==4.13.2
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-microsoft-azure==11.0.0
apache-airflow-providers-mysql==5.7.3
apache-airflow-providers-odbc==4.8.0
apache-airflow-providers-openlineage==1.13.0
apache-airflow-providers-postgres==5.13.1
apache-airflow-providers-redis==3.8.0
apache-airflow-providers-sendgrid==3.6.0
apache-airflow-providers-sftp==4.11.1
apache-airflow-providers-slack==8.9.1
apache-airflow-providers-smtp==1.8.0
apache-airflow-providers-snowflake==5.8.0
apache-airflow-providers-sqlite==3.9.0
apache-airflow-providers-ssh==3.14.0

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",shamikghosh,2024-11-22 13:39:56+00:00,[],2024-12-30 00:16:15+00:00,2024-12-30 00:16:14+00:00,https://github.com/apache/airflow/issues/44286,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('provider:microsoft-azure', 'Azure-related issues'), ('pending-response', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2493794151, 'issue_id': 2683253708, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 22, 13, 39, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499881891, 'issue_id': 2683253708, 'author': 'kunaljubce', 'body': ""@shamikghosh Have you created a DAG that you're trying to load in your Airflow UI? If yes, can you share the code snippets where you're trying to setup the connection?"", 'created_at': datetime.datetime(2024, 11, 26, 7, 37, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558276963, 'issue_id': 2683253708, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 12, 22, 0, 16, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564879949, 'issue_id': 2683253708, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 12, 30, 0, 16, 14, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-22 13:39:59 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

kunaljubce on (2024-11-26 07:37:58 UTC): @shamikghosh Have you created a DAG that you're trying to load in your Airflow UI? If yes, can you share the code snippets where you're trying to setup the connection?

github-actions[bot] on (2024-12-22 00:16:50 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-12-30 00:16:14 UTC): This issue has been closed because it has not received response from the issue author.

"
2682683579,issue,closed,completed,allow extending HttpHook with requests adapters,"### Description

One of the factors making Python's `requests` library quite flexible is its use of [transport adapters](https://docs.python-requests.org/en/latest/user/advanced/#transport-adapters), allowing the user to further configure behavior.
Airflow's built-in `HttpHook` tho unfortunately does not currently expose such functionality to the user.
So far, I've found myself working around this by inheriting the hook with one adding such functionality, see below.
It would be nice tho to see the actual HttpHook extended with an extra parameter to expose this functionality to the user, foregoing the need for such sub-classes.

```
# mountable_http_hook.py
from typing import Any
from urllib.parse import urlparse

from airflow.providers.http.hooks.http import HttpHook
from requests import Session
from requests.adapters import BaseAdapter
from requests_toolbelt.adapters.socket_options import (  # type: ignore[import-untyped]
    TCPKeepAliveAdapter,
)

class MountableHttpHook(HttpHook):
    """"""Version of AirFlow's HttpHook that allows mounting custom `requests` adapters.""""""

    _adapter: BaseAdapter | None

    def __init__(
        self,
        *args,
        adapter: BaseAdapter | None = None,
        tcp_keep_alive: bool = True,
        tcp_keep_alive_idle: int = 120,
        tcp_keep_alive_count: int = 20,
        tcp_keep_alive_interval: int = 30,
        **kwargs,
    ) -> None:
        super().__init__(*args, **kwargs)

        # ensure HttpHook won't override our mounts.
        # set manually instead of by constructor in case overrides
        # pass all params on to a class that doesn't know this parameter,
        # such as is the case with Oauth2HttpHook for requests.Session.
        self.tcp_keep_alive = False

        if adapter is None and tcp_keep_alive:
            # default to the HttpHook's adapter
            adapter = TCPKeepAliveAdapter(
                idle=tcp_keep_alive_idle,
                count=tcp_keep_alive_count,
                interval=tcp_keep_alive_interval,
            )
        self._adapter = adapter

    def get_conn(self, headers: dict[Any, Any] | None = None) -> Session:
        """"""Add our adapter to the `requests.Session`.""""""
        session = super().get_conn(headers=headers)
        if self._adapter:
            scheme = urlparse(self.base_url).scheme if self.base_url else ""https""
            session.adapters = {scheme: self._adapter}  # type: ignore
        return session
```


### Use case/motivation

Adapters may help handle business logic surrounding HTTP requests, including:

- retries
- timeouts
- HTTP status codes, e.g. to specify which error codes to retry (potentially using say exponential back-off) before marking the task as failed
- SSL version
- ...


### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",kiaradlf,2024-11-22 10:02:30+00:00,['jieyao-MilestoneHub'],2024-12-03 10:44:18+00:00,2024-12-03 10:44:18+00:00,https://github.com/apache/airflow/issues/44285,"[('kind:feature', 'Feature Requests'), ('good first issue', ''), ('area:core-operators', 'Operators, Sensors and hooks within Core Airflow')]","[{'comment_id': 2494982457, 'issue_id': 2682683579, 'author': 'potiuk', 'body': 'Feel free to work and contribute it @kiaradlf , otherwise I am marking it as good first issue and hopefully someone will.', 'created_at': datetime.datetime(2024, 11, 22, 22, 33, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2495449689, 'issue_id': 2682683579, 'author': 'jieyao-MilestoneHub', 'body': 'Hi @kiaradlf and @potiuk,\r\n\r\nI’ve submitted a PR (#44302) to address issue #44285 by adding an adapter parameter to the HttpHook. This enhancement allows users to mount custom requests adapters, making it easier to manage use cases like retries and timeouts. I’ve also included thorough tests to ensure smooth functionality and maintain backward compatibility.\r\n\r\nI appreciate your time and consideration in reviewing this—thank you!', 'created_at': datetime.datetime(2024, 11, 23, 11, 37, 21, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-11-22 22:33:27 UTC): Feel free to work and contribute it @kiaradlf , otherwise I am marking it as good first issue and hopefully someone will.

jieyao-MilestoneHub (Assginee) on (2024-11-23 11:37:21 UTC): Hi @kiaradlf and @potiuk,

I’ve submitted a PR (#44302) to address issue #44285 by adding an adapter parameter to the HttpHook. This enhancement allows users to mount custom requests adapters, making it easier to manage use cases like retries and timeouts. I’ve also included thorough tests to ensure smooth functionality and maintain backward compatibility.

I appreciate your time and consideration in reviewing this—thank you!

"
2681673090,issue,closed,completed,Max tasks across dags stuck at 487,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.8.1

### What happened?

I have the airflow running on AKS cluster with just the kubernetes executor.

The following configs are set for attaining the max concurrency.

Parallelism: 128 (per scheduler)
No of Schedulers: 10
Default pool size: 1200
DB: Postgres
Airflow Version: 2.8.1
Platform: Ubuntu
We are running big data workloads and the number of Max task runs can reach 1000.

However, with the above configurations, the Max task runs across all the dags is stuck at the magic number 487 and the remaining ones are queued.

I have checked the docs about any other parameters that need to be set, however only these are mentioned.

Appreciate the suggestions from the community to identify the missing configuration.

Thanks

### What you think should happen instead?

_No response_

### How to reproduce

this setup is running on Azure K8s.

The number of concurrent tasks with kubernetes executor is stuck at 487 mac.

### Operating System

Ubuntu

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",leninkumar-sv-office,2024-11-22 03:47:11+00:00,[],2024-11-22 22:12:21+00:00,2024-11-22 22:12:21+00:00,https://github.com/apache/airflow/issues/44276,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2492819926, 'issue_id': 2681673090, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 22, 3, 47, 14, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-22 03:47:14 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2681443205,issue,open,,"Failed to clear task instances in ""List Task Instance Page""","### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.8.4

### What happened?

I am trying to bulk clear a list of task instances returned by `taskinstance/list` page. 

However, the clearing was not successful due to the following error
```
Failed to clear task instances: ""'NoneType' object has no attribute 'partial_subset'""
```

I think the issue was some of the task instances returned in that page (along with their associated dags) no longer exist. Therefore, when we try to call `partial_subset` on these dags ([code](https://github.com/apache/airflow/blob/main/airflow/www/views.py#L5312)), the code fails and the exception is caught here [this](https://github.com/apache/airflow/blob/main/airflow/www/views.py#L5353-L5360).

The result of this is that the task instances I want to clear and do still exist ended up not getting cleared.


### What you think should happen instead?

I think we should add a try catch block in `_clear_task_instances` [here](https://github.com/apache/airflow/blob/main/airflow/www/views.py#L5307-L5319)

### How to reproduce

You can try to clear a task instance whose dag no longer exist in the List Task Instance UI page.

### Operating System

NAME=""Ubuntu"" VERSION=""20.04.6 LTS (Focal Fossa)""

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",helenyi-stripe,2024-11-22 00:55:44+00:00,['helenyi-stripe'],2024-11-25 13:57:50+00:00,,https://github.com/apache/airflow/issues/44274,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2492653658, 'issue_id': 2681443205, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 22, 0, 55, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2494939923, 'issue_id': 2681443205, 'author': 'potiuk', 'body': 'Assigned you @helenyi-stripe - since you want to work on it. also marked it as good first issue for anyone to tackle it in case.', 'created_at': datetime.datetime(2024, 11, 22, 22, 1, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2498093752, 'issue_id': 2681443205, 'author': 'jieyao-MilestoneHub', 'body': 'Hi @helenyi-stripe and @potiuk,\r\n\r\nI’ve submitted a PR (https://github.com/apache/airflow/pull/44348) to address issue https://github.com/apache/airflow/issues/44274 and add test code.\r\n\r\nI appreciate your time and consideration in reviewing this—thank you!', 'created_at': datetime.datetime(2024, 11, 25, 13, 57, 48, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-22 00:55:47 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-11-22 22:01:33 UTC): Assigned you @helenyi-stripe - since you want to work on it. also marked it as good first issue for anyone to tackle it in case.

jieyao-MilestoneHub on (2024-11-25 13:57:48 UTC): Hi @helenyi-stripe and @potiuk,

I’ve submitted a PR (https://github.com/apache/airflow/pull/44348) to address issue https://github.com/apache/airflow/issues/44274 and add test code.

I appreciate your time and consideration in reviewing this—thank you!

"
2680166390,issue,open,,AIP-84 | Scope out Overview and Dashboard UI endpoints,We'll need more UI endpoints to power the summaries we want to display in the dashboard and  any Dag/Asset/Task/overview pages,bbovenzi,2024-11-21 16:47:51+00:00,['bbovenzi'],2025-01-13 13:07:46+00:00,,https://github.com/apache/airflow/issues/44259,"[('area:API', ""Airflow's REST/HTTP API""), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2587060182, 'issue_id': 2680166390, 'author': 'tirkarthi', 'body': 'Hi, I started working on task overview page which needs UI specific APIs with only duration fields like (start_date, end_date, state, queued_dttm). I have the following questions.\n\n1. How do we define the success rate interval for a given range? i.e. If it\'s 24 hours, should the success/failed ratio be calculated per hour or 2 hours?\n2. For all runs in the third bar chart what would be the grouping? Is it similar to the success/failed ratio\n3. For all runs what would the bar for each interval look like since there is already success rate of red/green in second chart\n\nIMO computing the grouping information on the server side probably in sql itself will be more efficient than sending all the durations to frontend for computation. Something like below that sends last 14 task instances and then the summary info in other fields of the response\n\n GET localhost:8000/ui/task_instances/task_instance_durations dag_id==tutorial_taskflow_api task_id==extract\n\n```json\n{\n    ""task_instances"": [\n        {\n            ""dag_id"": ""tutorial_taskflow_api"",\n            ""end_date"": ""2025-01-13T04:34:02.260203Z"",\n            ""queued_dttm"": ""2025-01-13T04:33:58.576995Z"",\n            ""start_date"": ""2025-01-13T04:34:01.982578Z"",\n            ""state"": ""success"",\n            ""task_id"": ""extract"",\n            ""try_number"": 1\n        },\n     ],\n     ""success_rate"": [\n        ""2025-01-13T00:00:00"": {""failed"": 10, ""success"": 90},\n        ""2025-01-13T02:00:00"": {""failed"": 40, ""success"": 60},\n    ], \n    ""total_entries"": 14\n}\n```\n\nThanks', 'created_at': datetime.datetime(2025, 1, 13, 13, 7, 45, tzinfo=datetime.timezone.utc)}]","tirkarthi on (2025-01-13 13:07:45 UTC): Hi, I started working on task overview page which needs UI specific APIs with only duration fields like (start_date, end_date, state, queued_dttm). I have the following questions.

1. How do we define the success rate interval for a given range? i.e. If it's 24 hours, should the success/failed ratio be calculated per hour or 2 hours?
2. For all runs in the third bar chart what would be the grouping? Is it similar to the success/failed ratio
3. For all runs what would the bar for each interval look like since there is already success rate of red/green in second chart

IMO computing the grouping information on the server side probably in sql itself will be more efficient than sending all the durations to frontend for computation. Something like below that sends last 14 task instances and then the summary info in other fields of the response

 GET localhost:8000/ui/task_instances/task_instance_durations dag_id==tutorial_taskflow_api task_id==extract

```json
{
    ""task_instances"": [
        {
            ""dag_id"": ""tutorial_taskflow_api"",
            ""end_date"": ""2025-01-13T04:34:02.260203Z"",
            ""queued_dttm"": ""2025-01-13T04:33:58.576995Z"",
            ""start_date"": ""2025-01-13T04:34:01.982578Z"",
            ""state"": ""success"",
            ""task_id"": ""extract"",
            ""try_number"": 1
        },
     ],
     ""success_rate"": [
        ""2025-01-13T00:00:00"": {""failed"": 10, ""success"": 90},
        ""2025-01-13T02:00:00"": {""failed"": 40, ""success"": 60},
    ], 
    ""total_entries"": 14
}
```

Thanks

"
2679917034,issue,closed,completed,AIP-38 | Add show_trigger_form_if_no_params config to Dag Trigger,"We need to add a check for the config value of `show_trigger_form_if_no_params`. One problem is that right now the Dags list doesn't say if a dag has params or not so we don't know if we need to show the modal or not.

Additionally, I wonder if this should be overridden if `require_confirmation_dag_change` is True.",bbovenzi,2024-11-21 15:34:52+00:00,[],2024-11-21 21:21:34+00:00,2024-11-21 21:21:34+00:00,https://github.com/apache/airflow/issues/44256,"[('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2492357015, 'issue_id': 2679917034, 'author': 'jscheffl', 'body': 'Answering the question here:\r\n\r\nI\'d propose to DROP this legacy UI parameter. It is from ancient times - and the method of triggering is anyway different in the new UI.\r\n\r\nIn the past we had the UI showing the JSON.\r\nThen AIP-50 was implemented which rendered the UI.,. or skipped.\r\n\r\nBut in the new UI anyway the modal will be displayed. Why? Because I assume the user should confirm the trigger. Should not be a single-click to start a DAG ""by accident"". And the current modal also has the advanced options in the accordeon - no need to have a config option for this.\r\n\r\nThe only question is if the information about the form parmaeters should be pre-fetched. I propose the follwing approach:\r\nWhen the user opens the modal a request is made to get the DAG params. While the resuest is running a spinner can be displayed above the accordeon. If params are defined then the spinner can be replaced with the form and the modal can grow in size. If no params the spinner could just be hidden.', 'created_at': datetime.datetime(2024, 11, 21, 21, 18, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2492362364, 'issue_id': 2679917034, 'author': 'bbovenzi', 'body': 'Works for me! Let me add this to the list of config to remove before 3.0 is released https://github.com/apache/airflow/issues/43519', 'created_at': datetime.datetime(2024, 11, 21, 21, 21, 30, tzinfo=datetime.timezone.utc)}]","jscheffl on (2024-11-21 21:18:07 UTC): Answering the question here:

I'd propose to DROP this legacy UI parameter. It is from ancient times - and the method of triggering is anyway different in the new UI.

In the past we had the UI showing the JSON.
Then AIP-50 was implemented which rendered the UI.,. or skipped.

But in the new UI anyway the modal will be displayed. Why? Because I assume the user should confirm the trigger. Should not be a single-click to start a DAG ""by accident"". And the current modal also has the advanced options in the accordeon - no need to have a config option for this.

The only question is if the information about the form parmaeters should be pre-fetched. I propose the follwing approach:
When the user opens the modal a request is made to get the DAG params. While the resuest is running a spinner can be displayed above the accordeon. If params are defined then the spinner can be replaced with the form and the modal can grow in size. If no params the spinner could just be hidden.

bbovenzi (Issue Creator) on (2024-11-21 21:21:30 UTC): Works for me! Let me add this to the list of config to remove before 3.0 is released https://github.com/apache/airflow/issues/43519

"
2679914142,issue,open,,Remove auto template format detection,"### Body

Currently we do some “smart” (in quotes because it’s not actually very smart) detection whether a log template uses Python format string (e.g. `{foo}-{bar}`) or Jinja2 template syntax. We should find a way to remove the former as a feature and just always use Jinja2.

(Below is copied from another discussion)

The problem is we do record the log file template on DagRun so we could find the correct logs for an old run. Either we somehow find a way to distinguish those old runs (add a flag in db?) or we need to convert those old template strings into Jinja format in a db migration.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",uranusjr,2024-11-21 15:34:27+00:00,['uranusjr'],2024-11-22 08:50:46+00:00,,https://github.com/apache/airflow/issues/44255,"[('area:logging', ''), ('area:db-migrations', 'PRs with DB migration'), ('airflow3.0:candidate', 'Potential candidates for Airflow 3.0'), ('airflow3.0:breaking', 'Candidates for Airflow 3.0 that contain breaking changes')]",[],
2679861184,issue,open,,AIP-38 | Update navbar color config to support light/dark mode,We have a bunch of config settings `navbar_*` to customize the display of the navigation bar. Let's figure out how to adjust those values if in light or dark mode. Or that's on the airflow user to choose a single color scheme that works well for both.,bbovenzi,2024-11-21 15:13:47+00:00,[],2024-11-21 15:34:32+00:00,,https://github.com/apache/airflow/issues/44254,"[('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]",[],
2679853930,issue,closed,completed,Check standalone_dag_processor config in get_airflow_health(),"Right now the UI has to check the scheduler config for `standalone_dag_processor` in order to determin if the dag_processor health  check should be rendered.

It would be better if we checked for `standalone_dag_processor` in the backend and simply not return a `dag_processor` object as part of the `public/health` endpoint if False",bbovenzi,2024-11-21 15:10:51+00:00,['vatsrahul1001'],2024-11-29 15:20:09+00:00,2024-11-29 15:20:09+00:00,https://github.com/apache/airflow/issues/44253,"[('area:API', ""Airflow's REST/HTTP API"")]","[{'comment_id': 2491854462, 'issue_id': 2679853930, 'author': 'vatsrahul1001', 'body': '@bbovenzi would like to work on this. Can I assign it to myself?', 'created_at': datetime.datetime(2024, 11, 21, 17, 30, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2491946187, 'issue_id': 2679853930, 'author': 'bbovenzi', 'body': '@vatsrahul1001 all yours!', 'created_at': datetime.datetime(2024, 11, 21, 18, 14, 32, tzinfo=datetime.timezone.utc)}]","vatsrahul1001 (Assginee) on (2024-11-21 17:30:07 UTC): @bbovenzi would like to work on this. Can I assign it to myself?

bbovenzi (Issue Creator) on (2024-11-21 18:14:32 UTC): @vatsrahul1001 all yours!

"
2679678754,issue,closed,completed,Databricks Provider _get_databricks_task_id only cleanses task id,"### Apache Airflow Provider(s)

databricks

### Versions of Apache Airflow Providers

apache-airflow-providers-databricks==6.13.*

### Apache Airflow version

2.10.2

### Operating System

Debian GNU/Linux 12 (bookworm)

### Deployment

Astronomer

### Deployment details

_No response_

### What happened

_get_databricks_task_id only cleanses the task id, ref:
https://github.com/apache/airflow/blob/a9242844706ca117f86d22092109939dd56435ee/providers/src/airflow/providers/databricks/plugins/databricks_workflow.py#L67
https://github.com/apache/airflow/blob/a9242844706ca117f86d22092109939dd56435ee/providers/src/airflow/providers/databricks/operators/databricks.py#L1077

However, the dag_id may also contain `.` - so the replacement of `.` with `__` should be applied to the whole string, not just the task id portion, else periods placed in the dag name results in errors such as:
```
[2024-11-21, 13:12:42 GMT] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/utils/operator_helpers.py"", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/baseoperator.py"", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/providers/databricks/operators/databricks.py"", line 1252, in execute
    self.monitor_databricks_job()
  File ""/usr/local/lib/python3.11/site-packages/airflow/providers/databricks/operators/databricks.py"", line 1203, in monitor_databricks_job
    current_task_run_id = self._get_current_databricks_task()[""run_id""]
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/providers/databricks/operators/databricks.py"", line 1165, in _get_current_databricks_task
    return {task[""task_key""]: task for task in sorted_task_runs}[
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyError: 'my.airflow.dag.with.periods__my_airflow_task'
```
(as the invalid chars are getting silently stripped by databricks, so the task key on the databricks side is `myairflowdagwithperiods__my_airflow_task` rather than `my.airflow.dag.with.periods__my_airflow_task`)

### What you think should happen instead

The replacement of `.` with `__` should be applied to the whole task key / run name string, not just the task id portion

### How to reproduce

Use the affected operator(s) e.g. DatabricksNotebookOperator on a DAG which contains `.` in the dag_id

### Anything else

Every time

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",mwoods-familiaris,2024-11-21 14:31:05+00:00,[],2024-12-26 07:47:00+00:00,2024-12-26 07:46:59+00:00,https://github.com/apache/airflow/issues/44250,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('provider:databricks', '')]","[{'comment_id': 2491375847, 'issue_id': 2679678754, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 21, 14, 31, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2493131270, 'issue_id': 2679678754, 'author': 'rawwar', 'body': 'PR: https://github.com/apache/airflow/pull/43106 will also fix this issue', 'created_at': datetime.datetime(2024, 11, 22, 8, 12, 9, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-21 14:31:09 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

rawwar on (2024-11-22 08:12:09 UTC): PR: https://github.com/apache/airflow/pull/43106 will also fix this issue

"
2679253764,issue,closed,completed,Stats of a dynamic mapped tasks disappear after an automatic retry of a failed task,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When a task gets expanded [dynamically](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/dynamic-task-mapping.html), any retried failed task makes its other stats disappear.

### What you think should happen instead?

_No response_

### How to reproduce

```python
import random
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator # add "".providers.standard"" in 3

def random_fail_task(task_id):
    if random.random() < 0.5: 
        raise Exception(f""Task {task_id} failed"")
    print(f""Task {task_id} succeeded"")


def all_success(task_id):
    print(f""Task {task_id} succeeded"")

with DAG(
    dag_id='dynamic_task_expansion',
    start_date=datetime(2023, 1, 1),
    schedule=None,
    catchup=False,
) as dag:

    tasks = PythonOperator.partial(
        task_id='task',
        retries=3, ## Change to 0 for the second coherence check
        retry_delay=timedelta(seconds=10),
        python_callable=random_fail_task, # <- Change to all_success for first coherence checks
    ).expand(op_args=[[i] for i in range(100)])
```


**Coherence checks**

1. When replacing `random_fail_task` with `random_all_success_task `, we get the exact number of tasks, all succesful (100):
![image](https://github.com/user-attachments/assets/304f9019-1339-452d-8b16-f750cccb92e2)

2. When we apply `retries=0` with `random_fail_task` the total number of tasks (succesful+failed) is once again 100:
![image](https://github.com/user-attachments/assets/2d7f47e5-7431-45bb-9b28-f0f914bd366c)


**Reproduction**
1. When running the `random_fail_task` task with `retries>=0` - some of the mapped tasks fail and are up for retry:
![image](https://github.com/user-attachments/assets/59c9d316-9004-42e2-9d78-5d7b44fa7ffe)

2. Immediately when failed tasks are triggered for re-run, some of the other tasks disappear from the UI:
![image](https://github.com/user-attachments/assets/13228a2b-a9e9-4ade-86c2-d868d24bac46)

3. And finally we're left with less tasks than what we started:
![image](https://github.com/user-attachments/assets/e136c3f2-9788-46c0-b223-58a9bda8e168)


When looking the the ""Mapped Tasks"" tab, all of the tasks still are still there.
---

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Google Cloud Composer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",shahar1,2024-11-21 12:11:16+00:00,['shahar1'],2024-11-24 01:29:22+00:00,2024-11-24 01:29:21+00:00,https://github.com/apache/airflow/issues/44245,"[('kind:bug', 'This is a clearly a bug'), ('area:webserver', 'Webserver related Issues'), ('area:core', ''), ('area:dynamic-task-mapping', 'AIP-42')]","[{'comment_id': 2491575328, 'issue_id': 2679253764, 'author': 'ashb', 'body': 'First idea: look at the network inspector and see if the issue is in the API response or just in the presentation of the data', 'created_at': datetime.datetime(2024, 11, 21, 15, 40, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2494458751, 'issue_id': 2679253764, 'author': 'shahar1', 'body': '> First idea: look at the network inspector and see if the issue is in the API response or just in the presentation of the data\r\n\r\nThanks for the tip! It seems like the issue is in the API response.', 'created_at': datetime.datetime(2024, 11, 22, 18, 8, 25, tzinfo=datetime.timezone.utc)}]","ashb on (2024-11-21 15:40:52 UTC): First idea: look at the network inspector and see if the issue is in the API response or just in the presentation of the data

shahar1 (Issue Creator) on (2024-11-22 18:08:25 UTC): Thanks for the tip! It seems like the issue is in the API response.

"
2678173170,issue,closed,completed,AIP-84 Invalid states should not cause Internal server error on get dag_runs and task_instances endpoints,"### Description

As of now, GET dag_runs and GET task_instances raise Internal server error when state is invalid.

![image](https://github.com/user-attachments/assets/2174e34b-6f94-4465-ad14-782c576c3d68)


### Use case/motivation

These should return 422 Unprocessable entity

### Related issues

https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rawwar,2024-11-21 06:18:39+00:00,['rawwar'],2024-11-21 09:54:48+00:00,2024-11-21 09:54:48+00:00,https://github.com/apache/airflow/issues/44235,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]",[],
2678167140,issue,open,,Clearing a task group re-triggers task instances that were not selected for clearing,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.8.4

### What happened?

We received a report from a user of our Airflow system stating that when they cleared a task group (without selecting the ""Past"" or ""Future"" options), task instances from other DagRuns, which were newly added to the task group but not selected for clearing, were also re-triggered.

Upon reviewing the audit logs, we confirmed that the parameters passed during the clear operation indicated that both the ""Past"" and ""Future"" options were set to False.

Although we were unable to reproduce the issue in our testing environment, we have confirmed a bug where tasks newly added to a task group (which were not included in the ""Affected Tasks"" list of the clear UI) are still executed unexpectedly.

<img width=""1172"" alt=""image"" src=""https://github.com/user-attachments/assets/8d792ba9-5a9e-4524-b778-e165df469e40"">

<img width=""769"" alt=""image"" src=""https://github.com/user-attachments/assets/8e214382-6b4a-43e4-8bc8-6a04ee9af8af"">

We suspect that this issue might be caused by the filtering of tasks to be cleared, particularly due to the fact that only the current state of the task group is stored in the metadata database, rather than the historical changes, which may have led to this bug.

```
tasks_to_set_state = [task for task in task_group.iter_tasks() if isinstance(task, BaseOperator)]
```
- https://github.com/apache/airflow/blob/2.8.4/airflow/models/dag.py#L2151
- https://github.com/apache/airflow/blob/2.8.4/airflow/api/common/mark_tasks.py#L140

Additionally, we reviewed the latest code in the main branch but could not find any fixes related to this bug.

### What you think should happen instead?

Only the task instances within the target task group and the specified DagRun should be cleared.

### How to reproduce

Step 1) Create a DAG with task groups and generate multiple DagRuns (with catchup=True).
Step 2) Add a new task to a specific task group (and also add new tasks to other task groups).
Step 3) Clear a specific task group via the web UI.

### Operating System

Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",okayhooni,2024-11-21 06:14:32+00:00,[],2024-11-21 06:16:47+00:00,,https://github.com/apache/airflow/issues/44234,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:TaskGroup', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2677442663,issue,open,,Azure Data Lake connection will not work for blob.core.windows.net domain,"### Apache Airflow Provider(s)

microsoft-azure

### Versions of Apache Airflow Providers

apache-airflow-providers-microsoft-azure 11.1.0

### Apache Airflow version

2.9.2

### Operating System

Ubuntu 22.04.4

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### What happened

Scenario: need to leverage Azure storage for Airflow remote logging. 

Step 1 is verifying the connection works, so I'm using the operator ADLSListOperator as a test case. 
On the connector I have set the following properties:
Azure Client ID: <valid client id>
Azure Client Secret: <valid secret>
Azure Tenant ID: <valid tenant>
Azure DataLake Store Name: <e.g. mystorageaccount>

The store name's fully qualified url is https://mystorageaccount.blob.core.windows.net/

I know the client id, secret, and tenant id are all valid. They match the credentials that successfully work against the storage account using the python operator and the azure.storage.blob library. If I try to leverage the ADLS Connection with ADLSListOperator from apache-airflow-providers-microsoft-azure (11.1.0), it fails. The error log seems to indicate it is trying to connect to the wrong domain - e.g. ConnectionError(MaxRetryError(""HTTPSConnectionPool(host='**none.azuredatalakestore.net**'

The domain **azuredatalakestore.net** is for legacy azure storage accounts. New storage accounts cannot use this domain. All future storage accounts use **blob.core.windows.net**.

If anyone has successfully used the operator ADLSListOperator against a storage account hosted at blob.core.windows.net, I'd be curious to know the configuration used. The documentation and examples I've found are very sparse or inconsistent.

I've tried using connector types azure_data_lake (as described above) as well as types adls and wasb.

### What you think should happen instead

I would exect ADLSListOperator to list files, but it times out. I assume because it is trying to connect to the wrong domain. 

### How to reproduce

1. Create a valid azure storage account that uses the blob.core.windows.net domain - which should be all new storage accounts on Azure. 
2. Setup a azure_data_lake connection using valid client id, client secret, tenant id, and account name. 
3. Write a DAG that leverages the ADLSListOperator.


### Anything else

Always. Hasn't worked successfully yet.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vince-vanh,2024-11-20 22:49:03+00:00,[],2024-11-20 22:51:16+00:00,,https://github.com/apache/airflow/issues/44228,"[('kind:bug', 'This is a clearly a bug'), ('provider:microsoft-azure', 'Azure-related issues'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2489691741, 'issue_id': 2677442663, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 20, 22, 49, 6, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-20 22:49:06 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2675487189,issue,closed,completed,Airflow s3 xcom backend - Show the xcom values in airflow UI than the s3 path for the xcom,"### Description

Currently while using s3 xcom backend , the result is stored in s3 path as expected but while viewing the xcom from airflow UI , the value shown is the s3 path but it would be great if we have the actual xcom value in the place.

### Use case/motivation

Usecase : It will be helpful for developers to view the xcom

### Related issues

-

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bparamjeet,2024-11-20 11:07:25+00:00,[],2024-11-21 02:31:00+00:00,2024-11-21 02:31:00+00:00,https://github.com/apache/airflow/issues/44212,"[('provider:amazon', 'AWS/Amazon - related issues'), ('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2488287142, 'issue_id': 2675487189, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 20, 11, 7, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489939128, 'issue_id': 2675487189, 'author': 'potiuk', 'body': 'Not sure which implementation you use - but xcom backend has `orm_deserialize_value` method https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/xcom/index.html#airflow.models.xcom.BaseXCom.orm_deserialize_value that is used to produce the representation of the xcom shown in webserver. You should likely override it in the way that it shows values for small varlues (if you can make quick decision based on your knowledge about Xcom) - or just indication that the value is too big if it is - (this makes very little sense and it will crash your webserver if you display very long xcom value in the webserver).', 'created_at': datetime.datetime(2024, 11, 21, 2, 30, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489939223, 'issue_id': 2675487189, 'author': 'potiuk', 'body': 'Converting to discussion.', 'created_at': datetime.datetime(2024, 11, 21, 2, 30, 55, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-20 11:07:28 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-11-21 02:30:49 UTC): Not sure which implementation you use - but xcom backend has `orm_deserialize_value` method https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/xcom/index.html#airflow.models.xcom.BaseXCom.orm_deserialize_value that is used to produce the representation of the xcom shown in webserver. You should likely override it in the way that it shows values for small varlues (if you can make quick decision based on your knowledge about Xcom) - or just indication that the value is too big if it is - (this makes very little sense and it will crash your webserver if you display very long xcom value in the webserver).

potiuk on (2024-11-21 02:30:55 UTC): Converting to discussion.

"
2674289474,issue,open,,Add `run_if`/`skip_if` functionality to non-TaskFlow operators,"### Description

Currently, the useful `run_if` and `skip_if` decorators can only be added to tasks that are decorated by the `@task` decorator, aka TaskFlow compatible operators. However, many common operators are not compatible with TaskFlow API, e.g. the `SqlExecuteQueryOperator`. If you try, you will be met with:

```
skip_if can only be used with task. decorate with @task before @skip_if.
```

This request is for `run_if` and `skip_if` to be added to all operators, not just TaskFlow compatible ones.

### Use case/motivation

I want to use `run_if` and `skip_if` for an operator that isn't TaskFlow compatible.

### Related issues

There is a similar issue that I didn't bookmark to make `@task` decorator apply to classes (aka Operators), not just functions. Depending on how that's implemented, it could solve this issue.

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",matthewblock,2024-11-20 04:17:47+00:00,[],2024-11-21 01:03:49+00:00,,https://github.com/apache/airflow/issues/44205,"[('kind:feature', 'Feature Requests'), ('area:core-operators', 'Operators, Sensors and hooks within Core Airflow'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2487401760, 'issue_id': 2674289474, 'author': 'matthewblock', 'body': ""I have never submitted a PR, but my team wrote some working code to add this to our custom operator so I want to try to contribute this functionality.\r\n\r\nMy initial thought was this could be added as arguments to `BaseOperator`:\r\n\r\n```\r\nrun_if_condition: None | AnyConditionFunc = None,\r\nrun_if_message: None | str,\r\nskip_if_condition: None | AnyConditionFunc = None,\r\nskip_if_message: None | str,\r\n```\r\n\r\nThen a lot of the code in `airflow.decorators.condition` could move to `airflow.models.baseoperator`, and the existing `run_if` and `skip_if` decorators could be modified to utilize these new arguments.\r\n\r\nI am also seeing that in the [discussion](https://github.com/apache/airflow/discussions/36479) that led to [the `run_if` and `skip_if` PR](https://github.com/apache/airflow/pull/41116), it's recommended to just use `pre_execute` for this purpose. A somewhat compelling reason not to is that you can pass a message to `run_if` and `skip_if` but not `pre_execute`. 🤷\u200d♂️ \r\n\r\nAny thoughts?"", 'created_at': datetime.datetime(2024, 11, 20, 4, 30, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489751782, 'issue_id': 2674289474, 'author': 'potiuk', 'body': '> Any thoughts?\r\n\r\nYes. The only way we can do this approved is if you figure how how to run that code ONLY in task - and NOT in scheduler. Running any DAG Author provided code in scheduler is violating basic Airlfow security model https://airflow.apache.org/docs/apache-airflow/stable/security/security_model.html - and Airflow Scheduler should never run DAG Author modifiable code. \r\n\r\nThis is for example why custom scheduler code is done via Plugins - you are not able to create custom scheduler code via DAG - you have to have plugin installed.\r\n\r\nI think it\'s possible to do it in a safe way - plugging it in in the ""pre_execute"" framework and you are welcome to try it. But it will be quite a bit more complex (likely) than what your team came up with. And currently we are only working on Airlfow 3 where the task execution is anyhow heavily changed.  But after we release Airflow 3, I think that might be a good addition.', 'created_at': datetime.datetime(2024, 11, 20, 23, 32, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489839249, 'issue_id': 2674289474, 'author': 'matthewblock', 'body': '> I think it\'s possible to do it in a safe way - plugging it in in the ""pre_execute"" framework and you are welcome to try it.\r\n\r\nYes - We did exactly this, but in a custom operator instead of `BaseOperator` - We took the inner workings of `run_if` and `skip_if` related functions, minus the logic related to Python decorator itself, to add the specified callable to `pre_execute`. I\'m seeing already how it is more difficult to modify `BaseOperator`!\r\n\r\n> A somewhat compelling reason not to is that you can pass a message to run_if and skip_if but not pre_execute. 🤷\u200d♂️\r\n\r\nAnother compelling reason is if you have a `run_if` or `skip_if` function you want to re-use in two separate tasks, where one supports TaskFlow and the other doesn\'t. You can use `pre_execute` in both cases, but you need to raise `AirflowSkipException` in your function rather than having it return `True` or `False` and relying on `run_if`/`skip_if` to raise the exception.', 'created_at': datetime.datetime(2024, 11, 21, 0, 49, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489853642, 'issue_id': 2674289474, 'author': 'potiuk', 'body': '> I\'m seeing already how it is more difficult to modify BaseOperator!\r\n\r\nYes. it is. Because it is an underpinning for a LOT of things. Say mapped operators. \r\n\r\n> Another compelling reason is if you have a run_if or skip_if function you want to re-use in two separate tasks, where one supports TaskFlow and the other doesn\'t. You can use pre_execute in both cases, but you need to raise AirflowSkipException in your function rather than having it return True or False and relying on run_if/skip_if to raise the exception.\r\n\r\nThis could be also done by extracting common code and havin `taskflow_run_if` and ""classic_run_if` wrappers. But yes it would have been easier if it could be only one.\r\n\r\nSo yes - that\'s quite possible to add it - maybe even you can attempt to do it now - but again, be aware it\'s only going to be available in Airflow3 as we do not add any more features to Airlfow 2 (but that\'s fine as well - one more good reason to migrate to Airflow 3).', 'created_at': datetime.datetime(2024, 11, 21, 1, 3, 47, tzinfo=datetime.timezone.utc)}]","matthewblock (Issue Creator) on (2024-11-20 04:30:54 UTC): I have never submitted a PR, but my team wrote some working code to add this to our custom operator so I want to try to contribute this functionality.

My initial thought was this could be added as arguments to `BaseOperator`:

```
run_if_condition: None | AnyConditionFunc = None,
run_if_message: None | str,
skip_if_condition: None | AnyConditionFunc = None,
skip_if_message: None | str,
```

Then a lot of the code in `airflow.decorators.condition` could move to `airflow.models.baseoperator`, and the existing `run_if` and `skip_if` decorators could be modified to utilize these new arguments.

I am also seeing that in the [discussion](https://github.com/apache/airflow/discussions/36479) that led to [the `run_if` and `skip_if` PR](https://github.com/apache/airflow/pull/41116), it's recommended to just use `pre_execute` for this purpose. A somewhat compelling reason not to is that you can pass a message to `run_if` and `skip_if` but not `pre_execute`. 🤷‍♂️ 

Any thoughts?

potiuk on (2024-11-20 23:32:05 UTC): Yes. The only way we can do this approved is if you figure how how to run that code ONLY in task - and NOT in scheduler. Running any DAG Author provided code in scheduler is violating basic Airlfow security model https://airflow.apache.org/docs/apache-airflow/stable/security/security_model.html - and Airflow Scheduler should never run DAG Author modifiable code. 

This is for example why custom scheduler code is done via Plugins - you are not able to create custom scheduler code via DAG - you have to have plugin installed.

I think it's possible to do it in a safe way - plugging it in in the ""pre_execute"" framework and you are welcome to try it. But it will be quite a bit more complex (likely) than what your team came up with. And currently we are only working on Airlfow 3 where the task execution is anyhow heavily changed.  But after we release Airflow 3, I think that might be a good addition.

matthewblock (Issue Creator) on (2024-11-21 00:49:58 UTC): Yes - We did exactly this, but in a custom operator instead of `BaseOperator` - We took the inner workings of `run_if` and `skip_if` related functions, minus the logic related to Python decorator itself, to add the specified callable to `pre_execute`. I'm seeing already how it is more difficult to modify `BaseOperator`!


Another compelling reason is if you have a `run_if` or `skip_if` function you want to re-use in two separate tasks, where one supports TaskFlow and the other doesn't. You can use `pre_execute` in both cases, but you need to raise `AirflowSkipException` in your function rather than having it return `True` or `False` and relying on `run_if`/`skip_if` to raise the exception.

potiuk on (2024-11-21 01:03:47 UTC): Yes. it is. Because it is an underpinning for a LOT of things. Say mapped operators. 


This could be also done by extracting common code and havin `taskflow_run_if` and ""classic_run_if` wrappers. But yes it would have been easier if it could be only one.

So yes - that's quite possible to add it - maybe even you can attempt to do it now - but again, be aware it's only going to be available in Airflow3 as we do not add any more features to Airlfow 2 (but that's fine as well - one more good reason to migrate to Airflow 3).

"
2673978406,issue,closed,completed,"AIP-81 Split CLI Commands into Local, Remote and Hybrid Commands","### Description

AIP-81 Split CLI Commands into Local, Remote and Hybrid Commands

### Use case/motivation

[AIP-81](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-81+Enhanced+Security+in+CLI+via+Integration+of+API)

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-11-20 00:33:33+00:00,['bugraoz93'],2024-12-03 22:16:02+00:00,2024-12-03 22:16:02+00:00,https://github.com/apache/airflow/issues/44204,"[('area:CLI', ''), ('kind:feature', 'Feature Requests'), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]","[{'comment_id': 2489856979, 'issue_id': 2673978406, 'author': 'potiuk', 'body': 'Oh wow :)', 'created_at': datetime.datetime(2024, 11, 21, 1, 7, 21, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-11-21 01:07:21 UTC): Oh wow :)

"
2673552429,issue,closed,completed,AIP-38  | Graph View,,bbovenzi,2024-11-19 21:10:23+00:00,['bbovenzi'],2025-01-06 11:30:04+00:00,2025-01-06 11:30:04+00:00,https://github.com/apache/airflow/issues/44200,"[('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2572920913, 'issue_id': 2673552429, 'author': 'pierrejeambrun', 'body': 'I think we have the first version. Closing and we can improve in follow up PR if needed.', 'created_at': datetime.datetime(2025, 1, 6, 11, 30, 4, tzinfo=datetime.timezone.utc)}]","pierrejeambrun on (2025-01-06 11:30:04 UTC): I think we have the first version. Closing and we can improve in follow up PR if needed.

"
2673387057,issue,closed,not_planned,max_active_tis_per_dag not working in Deferrable Operator,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.7.2

### What happened?

When using DatabricksRunNowOperator with deferrable set to True, option max_active_tis_per_dag is not limiting number of concurrent tasks run.



### What you think should happen instead?

When you set max_active_tis_per_dag, Airflow should ensure that: 
`(the count of running and queued tasks) <= max_active_tis_per_dag`
 no matter if used with deferrable, or non-defferable setting.
However this is takes effect only for non-defferable, for deferrable operator/setting it is not not limiting number of concurrent tasks run.

### How to reproduce

I believe this is the case for all of the defferable-Operators. However I only tried with DatabricksRunNowOperator. 
In order to reproduce just use defferable-operator with dynamically created tasks with max_active_tis_per_dag set to 1.
When used with DatabricksRunNowOperator it will create many run of tasks (in Airflow and in Databricks).
You can use below code for dag

    from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator
    from airflow.decorators import task

    with DAG(
        dag_id=""something"",
        description=""something"",
        default_args=DEFAULT_ARGS,
        start_date=datetime(2024, 11, 19, 17, 0, 0, tz=""UTC"")) as dag:

    @task
    def prepare_dbx_params() -> list[dict[str, str]]:
        some_list = [""aa"", ""bb"", ""cc"", ""dd"", ""ee"", ""ff""]
        dbx_params = [
            {
                ""python_named_params"": {
                    ""DBX_ARG1"": something,
                    ""DBX_ARG2"": something[0]
                }
            }
            for something in some_list
        ]
        return dbx_params


    dbx_input_params = prepare_dbx_params()

    trigger_dbx_job = DatabricksRunNowOperator.partial(
        task_id=""trigger-dbx-job-dag"",
        job_name=<NAME OF JOB>,
        databricks_conn_id=<NAME OF DBX CONNECTION>
        deferrable=True,
        max_active_tis_per_dag=1,
    ).expand_kwargs(dbx_input_params)

    trigger_dbx_job

### Operating System

macOS

### Versions of Apache Airflow Providers

apache_airflow-2.7.3
apache_airflow_providers_databricks-4.6.0


### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",ewelinastr,2024-11-19 20:03:31+00:00,[],2024-12-19 00:16:10+00:00,2024-12-19 00:16:10+00:00,https://github.com/apache/airflow/issues/44196,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('duplicate', 'Issue that is duplicated'), ('pending-response', ''), ('area:core', ''), ('area:async-operators', 'AIP-40: Deferrable (""Async"") Operators'), ('provider:databricks', '')]","[{'comment_id': 2486639799, 'issue_id': 2673387057, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 19, 20, 3, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2497841608, 'issue_id': 2673387057, 'author': 'gopidesupavan', 'body': 'Yes we have one similar thing created here. https://github.com/apache/airflow/issues/40528.\r\n\r\nIf possible use the pools, that has option to include deferred tasks https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/pools.html', 'created_at': datetime.datetime(2024, 11, 25, 12, 8, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499401013, 'issue_id': 2673387057, 'author': 'potiuk', 'body': 'Closing as duplicate (of #40528)', 'created_at': datetime.datetime(2024, 11, 26, 1, 44, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2533325983, 'issue_id': 2673387057, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 12, 11, 0, 16, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2552512111, 'issue_id': 2673387057, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 12, 19, 0, 16, 9, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-19 20:03:34 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

gopidesupavan on (2024-11-25 12:08:35 UTC): Yes we have one similar thing created here. https://github.com/apache/airflow/issues/40528.

If possible use the pools, that has option to include deferred tasks https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/pools.html

potiuk on (2024-11-26 01:44:17 UTC): Closing as duplicate (of #40528)

github-actions[bot] on (2024-12-11 00:16:41 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-12-19 00:16:09 UTC): This issue has been closed because it has not received response from the issue author.

"
2673088573,issue,closed,completed,"Do not add ""extra"" if provider is already required dependency","When we see a providers is a dependency in ""cross-provider-dependecies"" - we add it automatically - when we build the package - as optional extra. However a number of providers also have the same provider as required dependency, and in this case adding an extra makes little sense.

Example https://airflow.apache.org/docs/apache-airflow-providers-jdbc/stable/index.html#cross-provider-package-dependencies - JDBC provider has ""common.sql"" extra but it also have ""common.sql"" as required dependency.

If that is the case, we should skip generating extra when the provider is prepared. This happens in https://github.com/apache/airflow/blob/81a910db9af72db0c7d12c33bb186cb8b117322e/dev/breeze/src/airflow_breeze/utils/packages.py#L471",potiuk,2024-11-19 18:16:48+00:00,['amoghrajesh'],2024-11-28 11:37:15+00:00,2024-11-28 11:37:15+00:00,https://github.com/apache/airflow/issues/44195,"[('area:providers', ''), ('area:dependencies', 'Issues related to dependencies problems')]","[{'comment_id': 2487585411, 'issue_id': 2673088573, 'author': 'amoghrajesh', 'body': 'Assigning to self. Will take a stab at it soon.', 'created_at': datetime.datetime(2024, 11, 20, 6, 13, 18, tzinfo=datetime.timezone.utc)}]","amoghrajesh (Assginee) on (2024-11-20 06:13:18 UTC): Assigning to self. Will take a stab at it soon.

"
2672957152,issue,closed,completed,AIP84: FastAPI: Transform all assets/queuedEvent into assets/queudEvents ,"### Description

Transform all `assets/queuedEvent` into `assets/queudEvents`  so that they are consistent with other routes as we have for `assets/events`



### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vatsrahul1001,2024-11-19 17:33:48+00:00,['vatsrahul1001'],2024-11-20 08:33:16+00:00,2024-11-20 08:33:16+00:00,https://github.com/apache/airflow/issues/44193,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2486342124, 'issue_id': 2672957152, 'author': 'vatsrahul1001', 'body': 'cc: @pierrejeambrun', 'created_at': datetime.datetime(2024, 11, 19, 17, 34, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2487617538, 'issue_id': 2672957152, 'author': 'vatsrahul1001', 'body': 'PR raised: [AIP84: Transform assets/queuedEvent to assets/queuedEvents #44194](https://github.com/apache/airflow/pull/44194)', 'created_at': datetime.datetime(2024, 11, 20, 6, 35, 7, tzinfo=datetime.timezone.utc)}]","vatsrahul1001 (Issue Creator) on (2024-11-19 17:34:52 UTC): cc: @pierrejeambrun

vatsrahul1001 (Issue Creator) on (2024-11-20 06:35:07 UTC): PR raised: [AIP84: Transform assets/queuedEvent to assets/queuedEvents #44194](https://github.com/apache/airflow/pull/44194)

"
2671538892,issue,open,,'component' metadata claims Airflow is an `npm` or `application`,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Looking at Airflow SBOMs such as `apache-airflow-sbom-2.10.3-python3.12.json` and `apache-airflow-sbom-2.10.3-python3.12-python-only.json`, it identifies the artifact being described by those SBOMs as `pkg:npm/apache-airflow@2.10.3` and `pkg:application/apache-airflow@2.10.3`. These are [Purls](https://github.com/package-url/purl-spec/blob/master/PURL-TYPES.rst), but I'm pretty sure Airflow is not an npm package, and `application` does not exist as purl type entirely.

### What you think should happen instead?

* describe 'exactly what' is being described by this SBOM. Does it describe a particular artifact, such as https://pypi.org/project/apache-airflow/ ? Then it should probably use the `pypi` Purl type. If it described Airflow more 'in the abstract', perhaps we should use the `generic` Purl type or introduce an [`asf` purl type](https://github.com/package-url/purl-spec/issues/305)

### How to reproduce

Generate the SBOMs

### Operating System

n/a

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

Part of this may be an upstream issue in https://github.com/CycloneDX/cdxgen , but I figured it would be good to first determine what we want to achieve 'concretely' here, and only look at what changes we may or may not need to generalize in upstream tooling after that.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",raboof,2024-11-19 09:52:26+00:00,[],2024-12-29 23:24:24+00:00,,https://github.com/apache/airflow/issues/44178,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2485210162, 'issue_id': 2671538892, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 19, 9, 52, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564865150, 'issue_id': 2671538892, 'author': 'potiuk', 'body': 'It looks like it\'s the way how cyclonedx generates it:\r\n\r\n* when you use ""python-only"" sboms - it\'s `pkg:application/apache-airflow@2.10.4` - which is more correct, even if it\'s not PyPI URL\r\n* when you use ""both npm + python"" - it\'s `pkg:npm/apache-airflow@2.10.4`\r\n\r\nI will take a look shortly on the generation process and we should likely set it to `pkg:pypi/apache-airflow@2.10.4`. \r\n\r\nThe good news is that it\'s only the ""top"" level Purl. All the other dependencies - including our providers - are good: ""pkg:pypi/apache-airflow-providers-airbyte@4.0.0""', 'created_at': datetime.datetime(2024, 12, 29, 23, 24, 23, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-19 09:52:29 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-12-29 23:24:23 UTC): It looks like it's the way how cyclonedx generates it:

* when you use ""python-only"" sboms - it's `pkg:application/apache-airflow@2.10.4` - which is more correct, even if it's not PyPI URL
* when you use ""both npm + python"" - it's `pkg:npm/apache-airflow@2.10.4`

I will take a look shortly on the generation process and we should likely set it to `pkg:pypi/apache-airflow@2.10.4`. 

The good news is that it's only the ""top"" level Purl. All the other dependencies - including our providers - are good: ""pkg:pypi/apache-airflow-providers-airbyte@4.0.0""

"
2670084286,issue,open,,[Helm Chart] Redis and Fernet Secret using pre-install hooks instead of lookup functions ,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Problem: 

I was trying to use airflow as a child chart and noticed I couldn't upgrade my existing chart because the fernet secret would not get created. 

templates/configmaps/extra-configmaps.yaml:    {{- $_ := set $annotations ""helm.sh/hook"" ""pre-install,pre-upgrade"" }}
templates/secrets/extra-secrets.yaml:    {{- $_ := set $annotations ""helm.sh/hook"" ""pre-install,pre-upgrade"" }}
templates/secrets/fernetkey-secret.yaml:    ""helm.sh/hook"": ""pre-install""
templates/secrets/redis-secrets.yaml:# relying on the ""pre-install"" hack to prevent changing randomly generated passwords,
templates/secrets/redis-secrets.yaml:    ""helm.sh/hook"": ""pre-install""
templates/secrets/redis-secrets.yaml:    ""helm.sh/hook"": ""pre-install""

There are a couple references to secrets, fernet and redis secret that are using the pre-install hook to avoid them changing during helm upgrade 

I think this could be done with a lookup function instead 

```
apiVersion: v1
kind: Secret
metadata:
  name: my-secret
  labels:
type: Opaque
data:
  {{- $previousSecret := lookup ""v1"" ""Secret"" .Release.Namespace ""my-secret"" }}
  {{- if $previousSecret }}
  ""mysecret"": {{  $previousSecret.data.mysecret }}
  {{- else if .Values.mySecret }}
  ""mysecret"": {{ .Values.mySecret | b64enc | quote }}
  {{- else }}
  ""mysecret"": {{ randAlphaNum 32 | b64enc | quote }}
  {{- end }}

```
Because it seems that this pre-install hook was done more as a workaround than anything.  



### What you think should happen instead?

I think fernet secret should get created on helm upgrade if it does not exist.  

### How to reproduce

Try using airflow as a child chart that already has a deployed release. 

### Operating System

Fedora 40

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",JKrehling,2024-11-18 22:18:31+00:00,['JKrehling'],2024-11-19 09:45:59+00:00,,https://github.com/apache/airflow/issues/44164,"[('kind:bug', 'This is a clearly a bug'), ('area:helm-chart', 'Airflow Helm Chart'), ('area:core', '')]","[{'comment_id': 2484251400, 'issue_id': 2670084286, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 18, 22, 18, 34, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-18 22:18:34 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2669762227,issue,closed,completed,Helm chart airflow webserver not starting,"### Official Helm Chart version

1.15.0 (latest released)

### Apache Airflow version

2.10.3

### Kubernetes Version

K3s

### Helm Chart configuration

I have set only my external database

### Docker Image customizations

None

### What happened

I am trying to setup the Airflow via Helm chart, all the pods is going on, except the web server. I have tried running with the default helm chart values and making some changes, but both seems to give me the same thing. After some time, the pods restart, and keep in this loop. I can't find too much information in the logs. Below is the screenshots with he logs I have access to
**Webserver Pod Log**
![image](https://github.com/user-attachments/assets/6d0b3554-c9bf-4289-818b-452a9c4f6cc9)

**Webserver Pod Describe section**
![image](https://github.com/user-attachments/assets/f051d814-c77d-4442-8da0-947c973e7c45)

### What you think should happen instead

_No response_

### How to reproduce

1. Run the following code, without any changes in the values.yaml 
helm upgrade --install airflow apache-airflow/airflow --namespace airflow-test --create-namespace 

2. Make changes to the database and set an external database

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",PecTheFabricator,2024-11-18 20:10:20+00:00,[],2024-11-19 20:15:40+00:00,2024-11-19 20:15:40+00:00,https://github.com/apache/airflow/issues/44159,"[('area:webserver', 'Webserver related Issues'), ('area:helm-chart', 'Airflow Helm Chart')]","[{'comment_id': 2484025518, 'issue_id': 2669762227, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 18, 20, 10, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2484817151, 'issue_id': 2669762227, 'author': 'romsharon98', 'body': 'Signal 15 is sometimes triggered due to insufficient resources, such as low memory. \r\nCan you check this?', 'created_at': datetime.datetime(2024, 11, 19, 6, 36, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2485627925, 'issue_id': 2669762227, 'author': 'PecTheFabricator', 'body': '> Signal 15 is sometimes triggered due to insufficient resources, such as low memory. Can you check this?\r\n\r\nI have tried even without setting the resources limits. The resource limits was commented. Also, the cluster memory usage is always under 50%. I will try to scale up and get back', 'created_at': datetime.datetime(2024, 11, 19, 12, 48, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2485705680, 'issue_id': 2669762227, 'author': 'PecTheFabricator', 'body': '> Signal 15 is sometimes triggered due to insufficient resources, such as low memory. Can you check this?\r\n\r\nAs asked, I have increased the memory requests and limit as follow \r\n\r\n\r\n  resources:\r\n    limits:\r\n      cpu: 4\r\n      memory: 10Gi\r\n    requests:\r\n      cpu: 2\r\n      memory: 4Gi\r\n\r\nhowever the error continues. What I have noticed is that the container is restarting right after the connection is denied (see 2nd screenshot in the post)', 'created_at': datetime.datetime(2024, 11, 19, 13, 23, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2485839017, 'issue_id': 2669762227, 'author': 'romsharon98', 'body': 'Maybe it takes more than the startupProbe and because of this it getting restart?\r\nTry to increase `webserver.startupProbe` values', 'created_at': datetime.datetime(2024, 11, 19, 14, 15, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2486637819, 'issue_id': 2669762227, 'author': 'PecTheFabricator', 'body': '> Maybe it takes more than the startupProbe and because of this it getting restart? Try to increase `webserver.startupProbe` values\r\n\r\nHey, thanks for the help, it worked. I just needed to increase the values in the webserver.startuProbe and it started the webserver. \r\nThanks Again', 'created_at': datetime.datetime(2024, 11, 19, 20, 2, 33, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-18 20:10:23 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

romsharon98 on (2024-11-19 06:36:53 UTC): Signal 15 is sometimes triggered due to insufficient resources, such as low memory. 
Can you check this?

PecTheFabricator (Issue Creator) on (2024-11-19 12:48:59 UTC): I have tried even without setting the resources limits. The resource limits was commented. Also, the cluster memory usage is always under 50%. I will try to scale up and get back

PecTheFabricator (Issue Creator) on (2024-11-19 13:23:41 UTC): As asked, I have increased the memory requests and limit as follow 


  resources:
    limits:
      cpu: 4
      memory: 10Gi
    requests:
      cpu: 2
      memory: 4Gi

however the error continues. What I have noticed is that the container is restarting right after the connection is denied (see 2nd screenshot in the post)

romsharon98 on (2024-11-19 14:15:32 UTC): Maybe it takes more than the startupProbe and because of this it getting restart?
Try to increase `webserver.startupProbe` values

PecTheFabricator (Issue Creator) on (2024-11-19 20:02:33 UTC): Hey, thanks for the help, it worked. I just needed to increase the values in the webserver.startuProbe and it started the webserver. 
Thanks Again

"
2669236229,issue,closed,not_planned,pgbouncer_exporter is flagged as malicious,"### Official Helm Chart version

1.15.0 (latest released)

### Apache Airflow version

2.9.3

### Kubernetes Version

1.29.9-gke.1496000

### Helm Chart configuration

_No response_

### Docker Image customizations

No

### What happened

Our security system has flagged pg_bouncer exporter as malicious and produced the following report https://www.virustotal.com/gui/file/c9df86d4b5653044344342545a81b7ce3886f0a4ad22745b4249704bbec436ac/detection. Looks like it has some relation with suspicious ip: [130.61.173.116](https://www.virustotal.com/gui/ip-address/130.61.173.116) 

### What you think should happen instead

_No response_

### How to reproduce

deploy airflow using the latest version of the helm charts

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omarsmak,2024-11-18 16:59:42+00:00,[],2024-11-18 17:55:39+00:00,2024-11-18 17:55:39+00:00,https://github.com/apache/airflow/issues/44153,"[('kind:bug', 'This is a clearly a bug'), ('security', 'Security issues that must be fixed'), ('area:helm-chart', 'Airflow Helm Chart'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2483738303, 'issue_id': 2669236229, 'author': 'eladkal', 'body': 'Please follow security report process https://github.com/apache/airflow?tab=security-ov-file#is-this-really-a-security-vulnerability-\r\n\r\nAlso noting we do not accept scanners report. You need to explain in your report what the issue is and how it can be exploited.', 'created_at': datetime.datetime(2024, 11, 18, 17, 55, 18, tzinfo=datetime.timezone.utc)}]","eladkal on (2024-11-18 17:55:18 UTC): Please follow security report process https://github.com/apache/airflow?tab=security-ov-file#is-this-really-a-security-vulnerability-

Also noting we do not accept scanners report. You need to explain in your report what the issue is and how it can be exploited.

"
2669076408,issue,open,,Solve executor state mismatch / task killed externally,"### Body

Users have long been confused by log messages along the lines of ""task state changed externally"", or something about executor state mismatch, where the thing that emits the log is essentially saying, 🤷 , no clue what happened sorry.

E.g. see here https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally

This should mostly be avoidable.

If a task was killed by external user action, why do we not have a signal of that recorded somewhere (e.g. on the TI object or in event log), and why can't the executor (or task process or whatever) _see_ that and then either not have to log such a message, or log a more helpful one that says exactly what happened.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2024-11-18 16:08:46+00:00,[],2024-11-18 16:11:00+00:00,,https://github.com/apache/airflow/issues/44151,"[('area:logging', ''), ('kind:meta', 'High-level information important to the community'), ('area:core', '')]",[],
2668895639,issue,open,,Replace the foreign composite key on TI to `ti.id`,"Now that we have UUID primary key on TI table (since https://github.com/apache/airflow/pull/43243), let's adjust foreign key constraints on tables like XCom, Task Reschedule, TI note etc",kaxil,2024-11-18 15:14:58+00:00,['jedcunningham'],2025-02-05 16:47:30+00:00,,https://github.com/apache/airflow/issues/44147,"[('area:db-migrations', 'PRs with DB migration'), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]","[{'comment_id': 2631381889, 'issue_id': 2668895639, 'author': 'ashb', 'body': 'Thinking through this a little bit more @jedcunningham and I have decided that:\n\nTaskReschedule, XCom, TaskMap and TINote should _not_ be linked to a specific TI Try/TIHistory row and should instead apply to the whole ""dag run task"" (a concept which doesn\'t currently exist in Airflow 2 or 3 or even have a good name).\n\nOur thinking is:\n\n- TI Note: For UX reasons note should be shared across all attempts of a Task\n- TaskMap and TaskReschedule don\'t serve any value in storing beyond the currently active task (so they could either be\xa0FKd to a specific TI uuid, or be deleted when the TI state is terminal.)\n- XCom: Bit of a funny one, we clear the XCom value on start of the next attempt _anyway_, so we don\'t ever store history. However there _could_ be some value in some of these being tied to an atempt. For example, many BaseOperatorLink subclasses store the URL in xcom, and being able to see external job  logs (EMR, DataProc etc) for a previous attempt would be a nice feature.\n\nThe only other relationships _onto_ TaskInstance table is TIHistory and RenderedTaskInstanceFIelds.\n\nWhile it might be nice to see some Xcom values and the RTIF for a given TIHistory row, the cost of keeping TIHistory in sync with TaskInstance changes and the shenanigans we\'d need to pull to get the FKs update means we don\'t _need_ to update anything.\n\nOne option if we only want to avoid composite PKs of (dag_id,run_id,task_id,map_index) would be to create a ""Dag Task id"" (this name sucks though) column which is an integer/UUID key that is unique over (dag_id,run_id)\n\nFor example, we might store these TI history rows\n\n```\n dag_id | run_id | task_id |                NEW_ID                |                  id                  | map_index | try_number\n--------+--------+---------+--------------------------------------+--------------------------------------+-----------+------------\n dag1   | run1   | my_task | 64379755-f717-4463-9f8b-5e2cb16c74f8 | f4578f24-1ecb-4ae1-b23d-2da43863cf80 |        -1 |          1\n dag1   | run1   | my_task | 64379755-f717-4463-9f8b-5e2cb16c74f8 | 7fdd63b8-b6da-4726-8aab-423612dc98b7 |        -1 |          2\n```\n\n\n(Note: `id` is 100% unique, each mapped task/map_index value gets a unique id).\n\nand this ""active"" TI:\n\n\n```\n dag1   | run1   | my_task | 64379755-f717-4463-9f8b-5e2cb16c74f8 | 87dc53d3-d46c-4e80-a3fe-e7a986879951 |        -1 |          3\n```\n\n\nThe goal of the `id` column in the first place was driven by the Task Execution Interface needing a single value (that is easy/convient to use) that will uniquely identify the TI attempt. Also the fact that the number of ""PK"" columns in TI keeps growing, and adding a UUID pk now makes that tennable going forward to add more ""ID"" columns without ever bloating the FKs etc.', 'created_at': datetime.datetime(2025, 2, 3, 15, 48, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2637468621, 'issue_id': 2668895639, 'author': 'potiuk', 'body': 'I am glad then that we have not started the work on multi-team in 3.0 - because those are the kind of changes / dependencies  I expected to be worked out /cleaned before we complete 3.0.', 'created_at': datetime.datetime(2025, 2, 5, 16, 46, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2637469865, 'issue_id': 2668895639, 'author': 'potiuk', 'body': 'cc: @o-nikolas', 'created_at': datetime.datetime(2025, 2, 5, 16, 47, 28, tzinfo=datetime.timezone.utc)}]","ashb on (2025-02-03 15:48:03 UTC): Thinking through this a little bit more @jedcunningham and I have decided that:

TaskReschedule, XCom, TaskMap and TINote should _not_ be linked to a specific TI Try/TIHistory row and should instead apply to the whole ""dag run task"" (a concept which doesn't currently exist in Airflow 2 or 3 or even have a good name).

Our thinking is:

- TI Note: For UX reasons note should be shared across all attempts of a Task
- TaskMap and TaskReschedule don't serve any value in storing beyond the currently active task (so they could either be FKd to a specific TI uuid, or be deleted when the TI state is terminal.)
- XCom: Bit of a funny one, we clear the XCom value on start of the next attempt _anyway_, so we don't ever store history. However there _could_ be some value in some of these being tied to an atempt. For example, many BaseOperatorLink subclasses store the URL in xcom, and being able to see external job  logs (EMR, DataProc etc) for a previous attempt would be a nice feature.

The only other relationships _onto_ TaskInstance table is TIHistory and RenderedTaskInstanceFIelds.

While it might be nice to see some Xcom values and the RTIF for a given TIHistory row, the cost of keeping TIHistory in sync with TaskInstance changes and the shenanigans we'd need to pull to get the FKs update means we don't _need_ to update anything.

One option if we only want to avoid composite PKs of (dag_id,run_id,task_id,map_index) would be to create a ""Dag Task id"" (this name sucks though) column which is an integer/UUID key that is unique over (dag_id,run_id)

For example, we might store these TI history rows

```
 dag_id | run_id | task_id |                NEW_ID                |                  id                  | map_index | try_number
--------+--------+---------+--------------------------------------+--------------------------------------+-----------+------------
 dag1   | run1   | my_task | 64379755-f717-4463-9f8b-5e2cb16c74f8 | f4578f24-1ecb-4ae1-b23d-2da43863cf80 |        -1 |          1
 dag1   | run1   | my_task | 64379755-f717-4463-9f8b-5e2cb16c74f8 | 7fdd63b8-b6da-4726-8aab-423612dc98b7 |        -1 |          2
```


(Note: `id` is 100% unique, each mapped task/map_index value gets a unique id).

and this ""active"" TI:


```
 dag1   | run1   | my_task | 64379755-f717-4463-9f8b-5e2cb16c74f8 | 87dc53d3-d46c-4e80-a3fe-e7a986879951 |        -1 |          3
```


The goal of the `id` column in the first place was driven by the Task Execution Interface needing a single value (that is easy/convient to use) that will uniquely identify the TI attempt. Also the fact that the number of ""PK"" columns in TI keeps growing, and adding a UUID pk now makes that tennable going forward to add more ""ID"" columns without ever bloating the FKs etc.

potiuk on (2025-02-05 16:46:57 UTC): I am glad then that we have not started the work on multi-team in 3.0 - because those are the kind of changes / dependencies  I expected to be worked out /cleaned before we complete 3.0.

potiuk on (2025-02-05 16:47:28 UTC): cc: @o-nikolas

"
2668666313,issue,open,,AIP-76 Implement PartitionAtRuntime,"### Body

[AIP-76](https://cwiki.apache.org/confluence/x/2QyTEg)

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",uranusjr,2024-11-18 14:02:12+00:00,[],2024-11-18 14:02:12+00:00,,https://github.com/apache/airflow/issues/44146,"[('AIP-76', 'Asset Partitions')]",[],
2668665105,issue,open,,AIP-76 Implement PartitionBySequence and PartitionByProduct,"### Body

[AIP-76](https://cwiki.apache.org/confluence/x/2QyTEg)

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",uranusjr,2024-11-18 14:01:42+00:00,[],2024-11-18 14:01:43+00:00,,https://github.com/apache/airflow/issues/44145,"[('AIP-76', 'Asset Partitions')]",[],
2668663381,issue,open,,AIP-76 Implement partition on decorated asset functions,"### Body

[AIP-76](https://cwiki.apache.org/confluence/x/2QyTEg)

* Add partition argument to `@asset`
* Ensure asset tasks do not reference data interval and logical date in task context
* Design and implement partition accessor in task context


### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",uranusjr,2024-11-18 14:01:03+00:00,[],2024-11-18 14:03:16+00:00,,https://github.com/apache/airflow/issues/44144,"[('area:datasets', 'Issues related to the datasets feature'), ('AIP-76', 'Asset Partitions')]",[],
2668657007,issue,open,,AIP-76 Implement PartitionByInterval,"### Body

[AIP-76](https://cwiki.apache.org/confluence/x/2QyTEg)

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",uranusjr,2024-11-18 13:58:59+00:00,[],2024-11-18 13:59:32+00:00,,https://github.com/apache/airflow/issues/44143,"[('AIP-76', 'Asset Partitions')]",[],
2668654613,issue,open,,AIP-76 Implement partition related constructs,"### Body

[AIP-76](https://cwiki.apache.org/confluence/x/2QyTEg)

* Partition key
* Partition dependency
* Data completeness model
* Manual run considerations
* More?

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",uranusjr,2024-11-18 13:58:13+00:00,[],2024-11-18 13:58:13+00:00,,https://github.com/apache/airflow/issues/44142,"[('AIP-76', 'Asset Partitions')]",[],
2668431536,issue,closed,completed,"Handle ""deferral"" in `ti_update_state` endpoint in Execution API","Handle setting of various TI attrs needed for deferral: triggerer_job, next_method, next_kwargs, trigger_id & trigger_timeout",kaxil,2024-11-18 12:46:41+00:00,['amoghrajesh'],2024-11-27 15:07:39+00:00,2024-11-27 15:07:38+00:00,https://github.com/apache/airflow/issues/44137,"[('area:API', ""Airflow's REST/HTTP API""), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2668266250,issue,closed,completed,SQLTableCheckOperator issue with Oracle table alias,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

2

### What happened?

SQLTableCheckOperator operator generate SQL with table ""AS"" alias and table alias with ""AS"" not working in Oracle database.
Needs to fix sql_check_template depends on connection type remove ""AS"".

### What you think should happen instead?

_No response_

### How to reproduce

```python
from airflow.providers.common.sql.operators.sql import SQLTableCheckOperator

SQLTableCheckOperator(
        task_id=""your_task"",
        table=""yout_table"",
        checks={
            ""row_count_check"": {""check_statement"": ""COUNT(*) = 1000""}        },
        partition_clause=""BALANCE_DATE = to_date('2024-11-11','yyyy-mm-dd')"",
        conn_id=connection_should_be_oracle_db
    )
```

### Operating System

ubuntu

### Versions of Apache Airflow Providers

apache-airflow-providers-common-sql==1.7.2

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",igvog,2024-11-18 11:37:09+00:00,['igvog'],2025-01-26 13:15:46+00:00,2025-01-24 12:03:45+00:00,https://github.com/apache/airflow/issues/44135,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:core', ''), ('provider:common-sql', ''), ('provider:oracle', '')]","[{'comment_id': 2482790627, 'issue_id': 2668266250, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 18, 11, 37, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2482799941, 'issue_id': 2668266250, 'author': 'tolebiermekov', 'body': 'I came across a similar issue...', 'created_at': datetime.datetime(2024, 11, 18, 11, 41, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2482849039, 'issue_id': 2668266250, 'author': 'tolebiermekov', 'body': 'In Oracle SQL, the AS keyword is not allowed when assigning an alias to a subquery. While AS can be used for column aliases or table aliases in some contexts, Oracle does not permit its use when aliasing subqueries.', 'created_at': datetime.datetime(2024, 11, 18, 12, 3, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2612346713, 'issue_id': 2668266250, 'author': 'dabla', 'body': 'Interesting case, now that we have the [dialects](https://github.com/apache/airflow/pull/41327), we could add a function in the [Dialect](https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/common/sql/dialects/dialect.py) that would be used by the SQLTableCheckOperator, that way, we could override it when a specific implementation is needed for a database.  In this case that would also mean that we would have to introduce an OracleDialect, but that was the whole purpose of it.', 'created_at': datetime.datetime(2025, 1, 24, 11, 53, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2612366159, 'issue_id': 2668266250, 'author': 'eladkal', 'body': '> Interesting case, now that we have the [dialects](https://github.com/apache/airflow/pull/41327), we could add a function in the [Dialect](https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/common/sql/dialects/dialect.py) that would be used by the SQLTableCheckOperator, that way, we could override it when a specific implementation is needed for a database. In this case that would also mean that we would have to introduce an OracleDialect, but that was the whole purpose of it.\n\nIn that case closing as resolved', 'created_at': datetime.datetime(2025, 1, 24, 12, 3, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614414985, 'issue_id': 2668266250, 'author': 'dabla', 'body': 'The _generate_sql_query should move form the SQLTableCheckOperator to the Dialect class, then once we have this method available in Dialect, we could implement an OracleDialect.', 'created_at': datetime.datetime(2025, 1, 26, 13, 15, 34, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-18 11:37:12 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

tolebiermekov on (2024-11-18 11:41:25 UTC): I came across a similar issue...

tolebiermekov on (2024-11-18 12:03:36 UTC): In Oracle SQL, the AS keyword is not allowed when assigning an alias to a subquery. While AS can be used for column aliases or table aliases in some contexts, Oracle does not permit its use when aliasing subqueries.

dabla on (2025-01-24 11:53:14 UTC): Interesting case, now that we have the [dialects](https://github.com/apache/airflow/pull/41327), we could add a function in the [Dialect](https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/common/sql/dialects/dialect.py) that would be used by the SQLTableCheckOperator, that way, we could override it when a specific implementation is needed for a database.  In this case that would also mean that we would have to introduce an OracleDialect, but that was the whole purpose of it.

eladkal on (2025-01-24 12:03:45 UTC): In that case closing as resolved

dabla on (2025-01-26 13:15:34 UTC): The _generate_sql_query should move form the SQLTableCheckOperator to the Dialect class, then once we have this method available in Dialect, we could implement an OracleDialect.

"
2667617139,issue,closed,not_planned,"Scheduler stuck when ""too old resource version"" error is thrown","### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.8.2

### What happened?

I get the following error a lot on my airflow scheduler pods:

```
{kubernetes_executor_utils.py:121} ERROR - Unknown error in KubernetesJobWatcher. Failing
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py"", line 112, in run
    self.resource_version = self._run(
                            ^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py"", line 168, in _run
    for event in self._pod_events(kube_client=kube_client, query_kwargs=kwargs):
  File ""/home/airflow/.local/lib/python3.8/site-packages/kubernetes/watch/watch.py"", line 182, in stream
    raise client.rest.ApiException(
kubernetes.client.exceptions.ApiException: (410)
Reason: Expired: too old resource version: 725263658 (725300129)

Process KubernetesJobWatcher-8:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/multiprocessing/process.py"", line 314, in _bootstrap
    self.run()
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py"", line 112, in run
    self.resource_version = self._run(
                            ^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py"", line 168, in _run
    for event in self._pod_events(kube_client=kube_client, query_kwargs=kwargs):
  File ""/home/airflow/.local/lib/python3.8/site-packages/kubernetes/watch/watch.py"", line 182, in stream
    raise client.rest.ApiException(
kubernetes.client.exceptions.ApiException: (410)
Reason: Expired: too old resource version: 725263658 (725300129)
```

When this error appears relatively many times on my airflow scheduler pods, All my DAG runs become very slow- This is expressed in the fact that the amount of my ""scheduled"" slots is very high and in contrast the amount of my ""'queued"" and ""running"" slots is very low (about 15 slots together) even though I have defined 128 slots.

Also my resource utilization in my namespace is very low (20% cpu and memory usage) so the problem is not resources either.

NOTE: I use the package ""apache-airflow-providers-cncf-kubernetes"" on version 8.0.0 as required for Airflow 2.8.2 according to the constraints.

### What you think should happen instead?

I think Airflow should know how to handle this error so that even when the error is thrown, the scheduler should continue to work  properly and not ""freeze"".

### How to reproduce

I think it would happen on any deployment in this version of Airflow with running DAGs. 

### Operating System

rhel 8

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes==8.0.0

### Deployment

Other

### Deployment details

We are in a private cloud with constraints, we took the most of the chart but handled the constraints ourselves.

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",anon-airflow,2024-11-18 08:25:33+00:00,[],2025-01-31 00:14:57+00:00,2025-01-31 00:14:57+00:00,https://github.com/apache/airflow/issues/44127,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('pending-response', ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]","[{'comment_id': 2482254229, 'issue_id': 2667617139, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 18, 8, 25, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2505926743, 'issue_id': 2667617139, 'author': 'potiuk', 'body': 'I think there were some recent changes in Kubernetes Provider to address similar issues: https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/stable/changelog.html -> look it up and best - upgrade Airflow to the latest version (or only K8S provider)\r\n\r\nhttps://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html#installation-and-upgrade-scenarios', 'created_at': datetime.datetime(2024, 11, 28, 11, 45, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2505932737, 'issue_id': 2667617139, 'author': 'potiuk', 'body': 'Please come-back to us with results after upgrade .\r\n\r\nBTW.\r\n\r\n> NOTE: I use the package ""apache-airflow-providers-cncf-kubernetes"" on version 8.0.0 as required for Airflow 2.8.2 according to the constraints.\r\n\r\nNo. You understand it backwards. Constraints are NOT requirements - you are free to upgrade stuff after initial installation. Constraints are there EXCLUSIVELY to give you reproducible installation and non-conflicting requirements - \r\nsee detailed description why we need constraints and how you are supposed to use them https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html#constraints-files\r\n\r\nAlso you can see my talk from Airlfow Summit 2023 https://www.youtube.com/watch?v=zPjIQjjjyHI describing how to manage dependencies for Airlfow.', 'created_at': datetime.datetime(2024, 11, 28, 11, 48, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2540264837, 'issue_id': 2667617139, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 12, 13, 0, 16, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2543465797, 'issue_id': 2667617139, 'author': 'anon-airflow', 'body': 'I upgrade my airflow environment to use in the latest version of ""apache-airflow-provider-cncf-kubernetes"" package (10.0.1) and the problem didn\'t solved - the error is still thrown.', 'created_at': datetime.datetime(2024, 12, 15, 6, 13, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2559864317, 'issue_id': 2667617139, 'author': 'potiuk', 'body': 'Possibly then somoene who follows k8s part can help.', 'created_at': datetime.datetime(2024, 12, 23, 15, 3, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2578930352, 'issue_id': 2667617139, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2025, 1, 9, 0, 15, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2625997896, 'issue_id': 2667617139, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2025, 1, 31, 0, 14, 56, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-18 08:25:36 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-11-28 11:45:25 UTC): I think there were some recent changes in Kubernetes Provider to address similar issues: https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/stable/changelog.html -> look it up and best - upgrade Airflow to the latest version (or only K8S provider)

https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html#installation-and-upgrade-scenarios

potiuk on (2024-11-28 11:48:37 UTC): Please come-back to us with results after upgrade .

BTW.


No. You understand it backwards. Constraints are NOT requirements - you are free to upgrade stuff after initial installation. Constraints are there EXCLUSIVELY to give you reproducible installation and non-conflicting requirements - 
see detailed description why we need constraints and how you are supposed to use them https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html#constraints-files

Also you can see my talk from Airlfow Summit 2023 https://www.youtube.com/watch?v=zPjIQjjjyHI describing how to manage dependencies for Airlfow.

github-actions[bot] on (2024-12-13 00:16:43 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

anon-airflow (Issue Creator) on (2024-12-15 06:13:44 UTC): I upgrade my airflow environment to use in the latest version of ""apache-airflow-provider-cncf-kubernetes"" package (10.0.1) and the problem didn't solved - the error is still thrown.

potiuk on (2024-12-23 15:03:16 UTC): Possibly then somoene who follows k8s part can help.

github-actions[bot] on (2025-01-09 00:15:35 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2025-01-31 00:14:56 UTC): This issue has been closed because it has not received response from the issue author.

"
2665994702,issue,closed,completed,"Problem with ""repair_run"" option in DatabricksRunNowOperator","### Apache Airflow Provider(s)

databricks

### Versions of Apache Airflow Providers

apache-airflow-providers-databricks==6.9.0

### Apache Airflow version

2.10.1

### Operating System

linux

### Deployment

Astronomer

### Deployment details

_No response_

### What happened

When using the DatabricksRunNowOperator with the option ""repair_run=True"", if the databricks job fails, airflow should command a repair request to the databricks API, however this is failing with the following error:

``` python
Traceback (most recent call last):
  File ""/usr/local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3004, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File ""/usr/local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/providers/databricks/operators/databricks.py"", line 868, in execute
    _handle_databricks_operator_execution(self, hook, self.log, context)
  File ""/usr/local/lib/python3.12/site-packages/airflow/providers/databricks/operators/databricks.py"", line 114, in _handle_databricks_operator_execution
    operator.json[""latest_repair_id""] = hook.repair_run(operator, repair_json)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: DatabricksHook.repair_run() takes 2 positional arguments but 3 were given
``` 
Checking the operator code, this is the ofending part.
![image](https://github.com/user-attachments/assets/3abb3e93-854e-4385-bca4-90ec3d7302c6)

The repair method only receives 2 arguments but airflow is passing an extra one (""operator"").

![image](https://github.com/user-attachments/assets/cbc6b91e-32fe-4e34-85f3-dc20aa25cc2e)


### What you think should happen instead

When a Databricks job fails, the job should be repaired but it is failing instead.

``` python
Traceback (most recent call last):
  File ""/usr/local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3004, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File ""/usr/local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3158, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3182, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/providers/databricks/operators/databricks.py"", line 868, in execute
    _handle_databricks_operator_execution(self, hook, self.log, context)
  File ""/usr/local/lib/python3.12/site-packages/airflow/providers/databricks/operators/databricks.py"", line 114, in _handle_databricks_operator_execution
    operator.json[""latest_repair_id""] = hook.repair_run(operator, repair_json)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: DatabricksHook.repair_run() takes 2 positional arguments but 3 were given
``` 

### How to reproduce

Use the DatabricksRunNowOperator to run a databricks job with the ""repair_run"" argument as True. If the job fails, airflow is unable to repair the failed job and is going to try to retry it.

### Anything else

The problem occurs with every failed Databricks job.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",zerodarkzone,2024-11-17 15:09:19+00:00,['Lee-W'],2024-11-19 01:55:27+00:00,2024-11-19 01:55:27+00:00,https://github.com/apache/airflow/issues/44114,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('provider:databricks', '')]","[{'comment_id': 2481309980, 'issue_id': 2665994702, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 17, 15, 9, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2482936722, 'issue_id': 2665994702, 'author': 'eladkal', 'body': 'cc @gaurav7261 @Lee-W if not mistaken you handled fixes around repair functionality. Will you have time to take a look?', 'created_at': datetime.datetime(2024, 11, 18, 12, 42, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2483106555, 'issue_id': 2665994702, 'author': 'Lee-W', 'body': '> cc @gaurav7261 @Lee-W if not mistaken you handled fixes around repair functionality. Will you have time to take a look?\r\n\r\nYep, I just traced the code a bit. I misuse the code from the deferrable part', 'created_at': datetime.datetime(2024, 11, 18, 13, 50, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2483251178, 'issue_id': 2665994702, 'author': 'gaurav7261', 'body': '@Lee-W  I have already raised PR for that long back: https://github.com/apache/airflow/pull/42945/files', 'created_at': datetime.datetime(2024, 11, 18, 14, 42, 13, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-17 15:09:22 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2024-11-18 12:42:38 UTC): cc @gaurav7261 @Lee-W if not mistaken you handled fixes around repair functionality. Will you have time to take a look?

Lee-W (Assginee) on (2024-11-18 13:50:12 UTC): Yep, I just traced the code a bit. I misuse the code from the deferrable part

gaurav7261 on (2024-11-18 14:42:13 UTC): @Lee-W  I have already raised PR for that long back: https://github.com/apache/airflow/pull/42945/files

"
2665541146,issue,closed,completed,Resolve mypy errors on appflow,"### Body

Following https://github.com/apache/airflow/pull/42954#discussion_r1817993084 and https://github.com/apache/airflow/pull/43436 we need to fix the issue with `mypy-boto3-appflow` and remove the type ignores.

The task:
1. Bump to `mypy-boto3-appflo>=1.35.39`
2. Apply the needed fixes to appflow code to make mypy happy without using `# type: ignore[arg-type]`


### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2024-11-17 08:46:35+00:00,['jx2lee'],2024-11-18 00:23:43+00:00,2024-11-18 00:23:43+00:00,https://github.com/apache/airflow/issues/44111,"[('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('good first issue', ''), ('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2481095773, 'issue_id': 2665541146, 'author': 'jx2lee', 'body': '@eladkal  Can i take this issue? assign to me, please 🚀', 'created_at': datetime.datetime(2024, 11, 17, 9, 55, 17, tzinfo=datetime.timezone.utc)}]","jx2lee (Assginee) on (2024-11-17 09:55:17 UTC): @eladkal  Can i take this issue? assign to me, please 🚀

"
2665160934,issue,open,,Add tests related to permissions to the Get dag_runs endpoint once authentication is added to FastAPI.,"### Description

In PR https://github.com/apache/airflow/pull/43506 , I'm not writing any tests related to auth. This issue is to add additional tests them once auth is enabled in FastAPI app

CC: @pierrejeambrun 

### Use case/motivation

AIP-84

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rawwar,2024-11-17 02:41:40+00:00,['rawwar'],2024-11-20 20:06:42+00:00,,https://github.com/apache/airflow/issues/44107,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('area:auth', ''), ('AIP-84', 'Modern Rest API')]",[],
2665000359,issue,closed,completed,AIP-72: Add a Create XCom endpoint,,kaxil,2024-11-16 22:25:50+00:00,['kaxil'],2024-11-18 15:33:01+00:00,2024-11-18 15:33:01+00:00,https://github.com/apache/airflow/issues/44100,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API"")]",[],
2664056580,issue,closed,completed,ExternalTaskSensor's check_existence doesn't work in deferrable mode,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

The `check_existence` parameter has no effect when ExternalTaskSensor is run in deferrable mode.

### What you think should happen instead?

ExternalTaskSensor should support `check_existence` when running in deferrable mode

### How to reproduce

1. Use ExternalTaskSensor to monitor a non-existence DAG with `deferrable=True` and `check_existence=True`
1. The sensor immediately goes into deferred state, instead of failing.

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Google Cloud Composer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",kien-truong,2024-11-16 08:58:34+00:00,[],2024-11-16 08:59:18+00:00,2024-11-16 08:59:17+00:00,https://github.com/apache/airflow/issues/44092,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2480485056, 'issue_id': 2664056580, 'author': 'kien-truong', 'body': 'Duplicate #40745', 'created_at': datetime.datetime(2024, 11, 16, 8, 59, 17, tzinfo=datetime.timezone.utc)}]","kien-truong (Issue Creator) on (2024-11-16 08:59:17 UTC): Duplicate #40745

"
2663229632,issue,closed,completed,BUG `./scripts/tools/initialize_virtualenv.py`,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I have breeze installed then I ran this script 
```
python3.9 ./scripts/tools/initialise_virtualenv.py
``` 
to create a virtual env with `airflow` present in `/bin`. I activated the virtual env 
```
airflow version
``` 
in terminal and got this error message:

```
(apache-airflow) success@success-HP-Laptop-14-cf3xxx:~/Desktop/airflow$ airflow version
Unable to load the config, contains a configuration error.
Traceback (most recent call last):
  File ""/usr/lib/python3.9/logging/config.py"", line 389, in resolve
    found = getattr(found, frag)
AttributeError: module 'airflow.utils.log' has no attribute 'file_processor_handler'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3.9/logging/config.py"", line 391, in resolve
    self.importer(used)
  File ""/home/success/Desktop/airflow/airflow/utils/log/file_processor_handler.py"", line 27, in <module>
    from airflow.utils.helpers import parse_template_string
  File ""/home/success/Desktop/airflow/airflow/utils/helpers.py"", line 32, in <module>
    from airflow.utils.types import NOTSET
  File ""/home/success/Desktop/airflow/airflow/utils/types.py"", line 22, in <module>
    import airflow.sdk.types
ModuleNotFoundError: No module named 'airflow.sdk'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/usr/lib/python3.9/logging/config.py"", line 564, in configure
    handler = self.configure_handler(handlers[name])
  File ""/usr/lib/python3.9/logging/config.py"", line 722, in configure_handler
    klass = self.resolve(cname)
  File ""/usr/lib/python3.9/logging/config.py"", line 398, in resolve
    raise v
  File ""/usr/lib/python3.9/logging/config.py"", line 391, in resolve
    self.importer(used)
  File ""/home/success/Desktop/airflow/airflow/utils/log/file_processor_handler.py"", line 27, in <module>
    from airflow.utils.helpers import parse_template_string
  File ""/home/success/Desktop/airflow/airflow/utils/helpers.py"", line 32, in <module>
    from airflow.utils.types import NOTSET
  File ""/home/success/Desktop/airflow/airflow/utils/types.py"", line 22, in <module>
    import airflow.sdk.types
ValueError: Cannot resolve 'airflow.utils.log.file_processor_handler.FileProcessorHandler': No module named 'airflow.sdk'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/success/Desktop/airflow/.venv/bin/airflow"", line 5, in <module>
    from airflow.__main__ import main
  File ""/home/success/Desktop/airflow/airflow/__init__.py"", line 78, in <module>
    settings.initialize()
  File ""/home/success/Desktop/airflow/airflow/settings.py"", line 750, in initialize
    LOGGING_CLASS_PATH = configure_logging()
  File ""/home/success/Desktop/airflow/airflow/logging_config.py"", line 74, in configure_logging
    raise e
  File ""/home/success/Desktop/airflow/airflow/logging_config.py"", line 69, in configure_logging
    dictConfig(logging_config)
  File ""/usr/lib/python3.9/logging/config.py"", line 809, in dictConfig
    dictConfigClass(config).configure()
  File ""/usr/lib/python3.9/logging/config.py"", line 571, in configure
    raise ValueError('Unable to configure handler '
ValueError: Unable to configure handler 'processor'
```

### What you think should happen instead?

3.0.dev0

### How to reproduce

Clone the source code from guthub
```
git clone https://github.com/apache/airflow.git
cd airflow
```

Install breeze
```
uv tool install -e ./dev/breeze
```
Create virtual env
```
python3.9 -m venv .venv
python3.9 ./scripts/tools/initialise_virtualenv.py
```
This should print out the version
```
airflow version
```

### Operating System

ubuntu

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

No

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",SuccessMoses,2024-11-15 21:34:04+00:00,[],2024-11-17 09:39:31+00:00,2024-11-17 09:39:31+00:00,https://github.com/apache/airflow/issues/44078,"[('kind:bug', 'This is a clearly a bug'), ('area:logging', ''), ('good first issue', ''), ('area:core', ''), ('affected_version:main_branch', 'Issues Reported for main branch')]","[{'comment_id': 2480388077, 'issue_id': 2663229632, 'author': 'potiuk', 'body': 'Yes. `pip install -e .[EXTRA]` effectively run inside should be replaced with `uv venv` followed with appropriate `uv sync --extra EXTRA`. This is a left-over after `uv` becoming recommended tool and adding `provider` and `task_sdk`. \r\n\r\nShould be a nice first-time issue.', 'created_at': datetime.datetime(2024, 11, 16, 4, 14, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2480503557, 'issue_id': 2663229632, 'author': 'SuccessMoses', 'body': 'I will attempt a fix', 'created_at': datetime.datetime(2024, 11, 16, 10, 13, 35, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-11-16 04:14:13 UTC): Yes. `pip install -e .[EXTRA]` effectively run inside should be replaced with `uv venv` followed with appropriate `uv sync --extra EXTRA`. This is a left-over after `uv` becoming recommended tool and adding `provider` and `task_sdk`. 

Should be a nice first-time issue.

SuccessMoses (Issue Creator) on (2024-11-16 10:13:35 UTC): I will attempt a fix

"
2663165084,issue,closed,completed,Add TI in openAPI schema for Execution subapp for the Task SDK client to use,"We define `TaskInstance` model separately in the Task SDK because it isn't part of the OpenAPI schema as it isn't needed for any API routes. So we need to find a way to add additional schema to `openapi.json` so we can auto-generate the model for Task SDK 

https://github.com/apache/airflow/blob/98bda4c5c0651156fd65f790f85c52f7c92a5733/task_sdk/src/airflow/sdk/api/datamodels/ti.py#L25-L32",kaxil,2024-11-15 21:18:43+00:00,['kaxil'],2024-11-15 22:06:30+00:00,2024-11-15 22:06:30+00:00,https://github.com/apache/airflow/issues/44077,"[('area:API', ""Airflow's REST/HTTP API"")]",[],
2662989976,issue,open,,on_dag_run_created listener ignored in plugin,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

on_dag_run_created hook is being ignored in plugin

### What you think should happen instead?

on_dag_run_created should run the relevant logic in the function

### How to reproduce

```
from airflow.plugins_manager import AirflowPlugin
from airflow.listeners import hookimpl
import logging

# Configure logger
logger = logging.getLogger(__name__)

class PreDagRunListener:
    """"""
    Listener to perform actions before any task in a DAG run starts.
    """"""

    @hookimpl
    def on_dag_run_created(self, dag_run, session):
        """"""
        Hook triggered when a DAG run is created, before tasks are executed.
        """"""
        dag_id = dag_run.dag_id
        run_id = dag_run.run_id

        logger.info(f""Pre-DAG execution logic triggered for DAG {dag_id}, run_id {run_id}."")

        # Add your custom pre-DAG logic here
        # Example: Block a specific DAG
        if dag_id == ""blocked_dag"":
            logger.error(f""DAG {dag_id} is blocked from running."")
            raise ValueError(f""DAG {dag_id} failed pre-run checks."")
        
        logger.info(f""DAG {dag_id} passed pre-run checks."")

class PreDagRunPlugin(AirflowPlugin):
    name = ""pre_dag_run_plugin""
    listeners = [PreDagRunListener()]

```

### Operating System

MacOS

### Versions of Apache Airflow Providers

apache-airflow-providers-common-compat==1.2.1
apache-airflow-providers-common-io==1.4.2
apache-airflow-providers-common-sql==1.19.0
apache-airflow-providers-fab==1.5.0
apache-airflow-providers-ftp==3.11.1
apache-airflow-providers-http==4.13.2
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-smtp==1.8.0
apache-airflow-providers-sqlite==3.9.0

### Deployment

Virtualenv installation

### Deployment details

Local setup - only change i made was to not lazy load plugins with lazy_load_plugins = False

### Anything else?

I can never get an on_dag_run_created hook to run no matter what. I'm open ot anything that allows me to run dag level pre-flight checks.

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",WillRaphaelson,2024-11-15 19:56:00+00:00,[],2024-11-15 19:58:45+00:00,,https://github.com/apache/airflow/issues/44071,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('area:Listeners', '')]","[{'comment_id': 2479820446, 'issue_id': 2662989976, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 15, 19, 56, 3, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-15 19:56:03 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2662240542,issue,closed,completed,Airflow DAG Run Delay: running State Persists After Tasks Complete,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.8.4

### What happened?

I’m encountering an issue in Airflow. I’m running 1,000 DAG runs per minute, and while tasks are completing quickly, the DAG run itself remains in a running state for a few minutes even after all tasks have finished. It takes about 5-6 minutes before the DAG run finally switches to success, which causes other DAG runs to get stuck in the queue.

Does anyone know why the DAG run remains in running even though all tasks have completed?
![384353839-5a14568e-49ce-46f4-ae53-3ca560f1b892](https://github.com/user-attachments/assets/6f90d139-f00a-47b9-bbde-3d05ae11f19d)


I noticed in the Airflow UI that the dag_run statuses seem to update in batches, with all statuses changing at the same time. Is there a configuration we can update to reduce this wait time and have the status change immediately?

### What you think should happen instead?

Dag run should update it state to success immediately after finishing all tasks.

### How to reproduce

Create a simple dag with 10 tasks and create 1000 dagruns per minute. 

### Operating System

Debian GNU/Linux 12

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",dineshkumar20,2024-11-15 14:59:28+00:00,[],2024-11-16 02:13:31+00:00,2024-11-16 02:13:31+00:00,https://github.com/apache/airflow/issues/44063,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2662156758,issue,open,,Update installers improvement,"The `pre-commit run --all-files --show-diff-on-failure --color always --verbose --hook-stage manual update-installers` command updates our ""installers"" - `uv`, `pip` and derivatives to latest released versions. 

It works well, but there are still few places which are not updated:

* `.github/actions/install-pre-commit/action.yml` - there a bit more logic is needed as versions are stored as properties of the action (maybe we should simply inline them in the action script to make it easier)
* in a number of places `pre-commit-version` and `pre-commit-uv-version` are not updated yet after #44059 got implemented",potiuk,2024-11-15 14:37:20+00:00,[],2024-11-15 14:39:40+00:00,,https://github.com/apache/airflow/issues/44062,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code')]",[],
2662064329,issue,open,,AIP-84 Log User Actions / Server Accesses,"### Body

Similarly to the legacy API, the new FastAPI API should also Log user actions in the `Log` table.


Like the `action_logging` decorator or a Middleware to implement similar behavior.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-11-15 13:58:35+00:00,['vatsrahul1001'],2024-11-15 15:18:00+00:00,,https://github.com/apache/airflow/issues/44057,"[('area:logging', ''), ('area:API', ""Airflow's REST/HTTP API""), ('good first issue', ''), ('kind:meta', 'High-level information important to the community'), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2479121927, 'issue_id': 2662064329, 'author': 'vatsrahul1001', 'body': '@pierrejeambrun I would like to work on this. Assigned myself cc: @pierrejeambrun', 'created_at': datetime.datetime(2024, 11, 15, 15, 17, 55, tzinfo=datetime.timezone.utc)}]","vatsrahul1001 (Assginee) on (2024-11-15 15:17:55 UTC): @pierrejeambrun I would like to work on this. Assigned myself cc: @pierrejeambrun

"
2661451395,issue,closed,completed,Pass DAG args into `@asset` dag,"### Description

> The @asset decorator therefore need to accept all arguments on DAG (with obvious exceptions such as dag_id, and things that don’t make sense like default_args) and @task (again with exceptions along similar lines).

in [AIP-75](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=311627076)

### Use case/motivation

[Rationale](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=311626828#AIP73ExpandedDataAwareness-Rationale)

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-11-15 09:49:09+00:00,['Lee-W'],2024-11-27 01:26:03+00:00,2024-11-27 01:26:03+00:00,https://github.com/apache/airflow/issues/44049,"[('kind:feature', 'Feature Requests'), ('area:datasets', 'Issues related to the datasets feature'), ('AIP-75', 'Asset-Centric Syntax')]",[],
2659852607,issue,closed,completed,"Status of testing Providers that were prepared on November 14, 2024","### Body

I have a kind request for all the contributors to the latest provider packages release.
Could you please help us to test the RC versions of the providers?

The guidelines on how to test providers can be found in

[Verify providers by contributors](https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDER_PACKAGES.md#verify-the-release-candidate-by-contributors)

Let us know in the comment, whether the issue is addressed.

Those are providers that require testing as there were some substantial changes introduced:


## Provider [amazon: 9.1.0rc4](https://pypi.org/project/apache-airflow-providers-amazon/9.1.0rc4)
   - [ ] [feat: add OpenLineage support for RedshiftToS3Operator (#41632)](https://github.com/apache/airflow/pull/41632): @kacpermuda
     Linked issues:
       - [ ] [Linked Issue #41575](https://github.com/apache/airflow/pull/41575): @Artuz37
   - [ ] [Add SageMakerProcessingSensor (#43144)](https://github.com/apache/airflow/pull/43144): @vVv-AA
   - [ ] [Make `RedshiftDataOperator`  handle multiple queries (#42900)](https://github.com/apache/airflow/pull/42900): @jroachgolf84
   - [ ] [fix(providers/amazon): alias is_authorized_dataset to is_authorized_asset (#43470)](https://github.com/apache/airflow/pull/43470): @Lee-W
   - [ ] [Remove returns in final clause of athena hooks (#43426)](https://github.com/apache/airflow/pull/43426): @yangyulely
     Linked issues:
       - [ ] [Linked Issue #43274](https://github.com/apache/airflow/issues/43274): @iritkatriel
   - [ ] [fix: replace \s with space (#43849)](https://github.com/apache/airflow/pull/43849): @LyndonFan
     Linked issues:
       - [ ] [Linked Issue #38864](https://github.com/apache/airflow/pull/38864): @vincbeck
       - [ ] [Linked Issue #39252](https://github.com/apache/airflow/issues/39252): @Taragolis
       - [ ] [Linked Issue #39428](https://github.com/apache/airflow/pull/39428): @romsharon98
       - [ ] [Linked Issue #38734](https://github.com/apache/airflow/pull/38734): @dabla
   - [x] [Fix `HttpToS3Operator` throws exception if s3_bucket parameter is not passed (#43828)](https://github.com/apache/airflow/pull/43828): @kunaljubce
     Linked issues:
       - [x] [Linked Issue #43379](https://github.com/apache/airflow/issues/43379): @kostiantyn-lab
   - [ ] [Fix awslogs_stream_prefix pattern (#43138)](https://github.com/apache/airflow/pull/43138): @pyrr
     Linked issues:
       - [ ] [Linked Issue #43130](https://github.com/apache/airflow/issues/43130): @pyrr
   - [ ] [Check if awslogs_stream_prefix already ends with container_name (#43724)](https://github.com/apache/airflow/pull/43724): @pyrr
     Linked issues:
       - [ ] [Linked Issue #43138](https://github.com/apache/airflow/pull/43138): @pyrr
   - [ ] [bugfix description should be optional for openlineage integration with `AthenaOperator` (#43576)](https://github.com/apache/airflow/pull/43576): @Udbv
   - [ ] [Decouple volume_configurations from capacity_provider_strategy (#43047)](https://github.com/apache/airflow/pull/43047): @pyrr
     Linked issues:
       - [ ] [Linked Issue #43046](https://github.com/apache/airflow/issues/43046): @pyrr
   - [x] [GlueJobOperator: add option to wait for cleanup before returning job status (#43688)](https://github.com/apache/airflow/pull/43688): @eladkal
   - [x] [Resolve GlueJobTrigger serialization bug causing verbose to always be True (#43622)](https://github.com/apache/airflow/pull/43622): @jimwbaldwin
   - [ ] [Remove returns in final clause of S3ToDynamoDBOperator (#43456)](https://github.com/apache/airflow/pull/43456): @yangyulely
     Linked issues:
       - [ ] [Linked Issue #43274](https://github.com/apache/airflow/issues/43274): @iritkatriel
   - [ ] [Remove sqlalchemy-redshift dependency (#43271)](https://github.com/apache/airflow/pull/43271): @mobuchowski
   - [ ] [feat(providers/amazon): Use asset in common provider (#43110)](https://github.com/apache/airflow/pull/43110): @Lee-W
   - [x] [Restrict looker-sdk version 24.18.0 and microsoft-kiota-http 1.3.4 (#42954)](https://github.com/apache/airflow/pull/42954): @gopidesupavan
   - [x] [Limit mypy-boto3-appflow (#43436)](https://github.com/apache/airflow/pull/43436): @eladkal
   - [x] [Standard provider python operator (#42081)](https://github.com/apache/airflow/pull/42081): @gopidesupavan
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
   - [ ] [remove all deprecations cncf.kubernetes (#43689)](https://github.com/apache/airflow/pull/43689): @romsharon98
   - [x] [Fix docstring for AthenaTrigger (#43616)](https://github.com/apache/airflow/pull/43616): @eladkal
## Provider [apache.beam: 5.9.1rc1](https://pypi.org/project/apache-airflow-providers-apache-beam/5.9.1rc1)
   - [x] [Standard provider python operator (#42081)](https://github.com/apache/airflow/pull/42081): @gopidesupavan
## Provider [apache.drill: 2.8.1rc1](https://pypi.org/project/apache-airflow-providers-apache-drill/2.8.1rc1)
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
## Provider [apache.druid: 3.12.1rc1](https://pypi.org/project/apache-airflow-providers-apache-druid/3.12.1rc1)
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
## Provider [apache.hive: 8.2.1rc1](https://pypi.org/project/apache-airflow-providers-apache-hive/8.2.1rc1)
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
   - [x] [Explain how to use uv with airflow virtualenv and make it works (#43604)](https://github.com/apache/airflow/pull/43604): @potiuk
     Linked issues:
       - [x] [Linked Issue #43200](https://github.com/apache/airflow/issues/43200): @potiuk
   - [ ] [Move `uncompress_file` function from `airflow.utils` to Hive provider (#43526)](https://github.com/apache/airflow/pull/43526): @kaxil
## Provider [apache.impala: 1.5.2rc1](https://pypi.org/project/apache-airflow-providers-apache-impala/1.5.2rc1)
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
## Provider [apache.pinot: 4.5.1rc1](https://pypi.org/project/apache-airflow-providers-apache-pinot/4.5.1rc1)
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
## Provider [apache.spark: 4.11.3rc1](https://pypi.org/project/apache-airflow-providers-apache-spark/4.11.3rc1)
   - [x] [Standard provider python operator (#42081)](https://github.com/apache/airflow/pull/42081): @gopidesupavan
## Provider [apprise: 1.4.1rc1](https://pypi.org/project/apache-airflow-providers-apprise/1.4.1rc1)
   - [x] [Standard provider bash operator (#42252)](https://github.com/apache/airflow/pull/42252): @gopidesupavan
   - [ ] [Unify DAG schedule args and change default to None (#41453)](https://github.com/apache/airflow/pull/41453): @uranusjr
## Provider [atlassian.jira: 2.7.1rc1](https://pypi.org/project/apache-airflow-providers-atlassian-jira/2.7.1rc1)
   - [x] [Standard provider bash operator (#42252)](https://github.com/apache/airflow/pull/42252): @gopidesupavan
## Provider [celery: 3.8.4rc1](https://pypi.org/project/apache-airflow-providers-celery/3.8.4rc1)
   - [ ] [AIP-72: Remove DAG pickling (#43667)](https://github.com/apache/airflow/pull/43667): @kaxil
   - [x] [Standard provider python operator (#42081)](https://github.com/apache/airflow/pull/42081): @gopidesupavan
## Provider [cloudant: 4.0.3rc1](https://pypi.org/project/apache-airflow-providers-cloudant/4.0.3rc1)
   - [ ] [Bump `uv` to `0.4.28` (#43451)](https://github.com/apache/airflow/pull/43451): @kaxil
## Provider [cncf.kubernetes: 10.0.0rc1](https://pypi.org/project/apache-airflow-providers-cncf-kubernetes/10.0.0rc1)
   - [ ] [remove all deprecations cncf.kubernetes (#43689)](https://github.com/apache/airflow/pull/43689): @romsharon98
   - [ ] [Change default value of `namespace` in `task.kubernetes` to be None (#43402)](https://github.com/apache/airflow/pull/43402): @romsharon98
   - [ ] [Add random_name_suffix to SparkKubernetesOperator (#43800) (#43847)](https://github.com/apache/airflow/pull/43847): @mrk-andreev
     Linked issues:
       - [ ] [Linked Issue #43800](https://github.com/apache/airflow/issues/43800): @Flametaa
   - [ ] [terminate kubernetes watch in case of unknown error while flushing queue (#43645)](https://github.com/apache/airflow/pull/43645): @pavansharma36
     Linked issues:
       - [ ] [Linked Issue #43440](https://github.com/apache/airflow/issues/43440): @iw-pavan
   - [ ] [Update 'namespace' priority for 'find_pod' function (#43762)](https://github.com/apache/airflow/pull/43762): @MaksYermak
   - [ ] [AIP-72: Remove DAG pickling (#43667)](https://github.com/apache/airflow/pull/43667): @kaxil
## Provider [common.compat: 1.2.2rc1](https://pypi.org/project/apache-airflow-providers-common-compat/1.2.2rc1)
   - [ ] [serialize asset/dataset timetable conditions in OpenLineage info also for older supported Airflow 2 versions (#43434)](https://github.com/apache/airflow/pull/43434): @mobuchowski
   - [x] [Standard provider python operator (#42081)](https://github.com/apache/airflow/pull/42081): @gopidesupavan
## Provider [common.sql: 1.20.0rc1](https://pypi.org/project/apache-airflow-providers-common-sql/1.20.0rc1)
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
## Provider [databricks: 6.13.0rc1](https://pypi.org/project/apache-airflow-providers-databricks/6.13.0rc1)
   - [ ] [Add on_kill  equivalent to Databricks SQL Hook to cancel timed out queries (#42668)](https://github.com/apache/airflow/pull/42668): @R7L208
     Linked issues:
       - [ ] [Linked Issue #42115](https://github.com/apache/airflow/pull/42115): @R7L208
   - [ ] [Added support for job_parameters and dbt_commands in DatabricksRunNow Operator (#43895)](https://github.com/apache/airflow/pull/43895): @pranshupand-db
   - [ ] [Enable workload identity authentication for the Databricks provider (#41639)](https://github.com/apache/airflow/pull/41639): @basvandriel
     Linked issues:
       - [ ] [Linked Issue #41586](https://github.com/apache/airflow/issues/41586): @basvandriel
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
## Provider [dbt.cloud: 3.11.2rc1](https://pypi.org/project/apache-airflow-providers-dbt-cloud/3.11.2rc1)
   - [ ] [Added condition to check if it is a scheduled save or rerun (#43453)](https://github.com/apache/airflow/pull/43453): @krzysztof-kubis
     Linked issues:
       - [ ] [Linked Issue #43347](https://github.com/apache/airflow/issues/43347): @krzysztof-kubis
## Provider [docker: 3.14.1rc1](https://pypi.org/project/apache-airflow-providers-docker/3.14.1rc1)
   - [x] [Fix logs with leading spaces in the Docker operator (#33692) (#43840)](https://github.com/apache/airflow/pull/43840): @mrk-andreev
   - [x] [Standard provider python operator (#42081)](https://github.com/apache/airflow/pull/42081): @gopidesupavan
## Provider [elasticsearch: 5.5.3rc1](https://pypi.org/project/apache-airflow-providers-elasticsearch/5.5.3rc1)
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
## Provider [exasol: 4.6.1rc1](https://pypi.org/project/apache-airflow-providers-exasol/4.6.1rc1)
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
## Provider [fab: 1.5.1rc1](https://pypi.org/project/apache-airflow-providers-fab/1.5.1rc1)
   - [x] [fab_auth_manager: allow get_user method to return the user authenticated via Kerberos (#43662)](https://github.com/apache/airflow/pull/43662): @brouberol
## Provider [google: 10.26.0rc1](https://pypi.org/project/apache-airflow-providers-google/10.26.0rc1)
   - [ ] [Add support for IAM database authentication for CloudSQL connection (#43631)](https://github.com/apache/airflow/pull/43631): @MaksYermak
   - [ ] [Provide option to `force_delete` for `GCSToBigQueryOperator` (#43785)](https://github.com/apache/airflow/pull/43785): @nathadfield
   - [x] [Unify reattach_states parameter logic across BigQuery operators (#43259)](https://github.com/apache/airflow/pull/43259): @moiseenkov
     Linked issues:
       - [x] [Linked Issue #40664](https://github.com/apache/airflow/pull/40664): @VladaZakharova
   - [x] [Remove non-existing field  from the ListCustomTrainingJobOperator's template_fields (#43924)](https://github.com/apache/airflow/pull/43924): @moiseenkov
   - [x] [Fix validating `parent_model` parameter in `UploadModelOperator` (#43473)](https://github.com/apache/airflow/pull/43473): @VladaZakharova
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
   - [ ] [Remove Airflow 2.1 compat code in Google provider (#43952)](https://github.com/apache/airflow/pull/43952): @uranusjr
   - [x] [Explain how to use uv with airflow virtualenv and make it works (#43604)](https://github.com/apache/airflow/pull/43604): @potiuk
     Linked issues:
       - [x] [Linked Issue #43200](https://github.com/apache/airflow/issues/43200): @potiuk
   - [x] [Standard provider python operator (#42081)](https://github.com/apache/airflow/pull/42081): @gopidesupavan
   - [x] [Bump google-ads version to use v18 by default (#43474)](https://github.com/apache/airflow/pull/43474): @VladaZakharova
## Provider [http: 4.13.3rc1](https://pypi.org/project/apache-airflow-providers-http/4.13.3rc1)
   - [ ] [Fix(http) bug, pass request_kwargs to HttpHooK.run (#43459)](https://github.com/apache/airflow/pull/43459): @childe
   - [x] [Limit temporarily aiohttp to < 3.11.0 (#44006)](https://github.com/apache/airflow/pull/44006): @potiuk
## Provider [jdbc: 4.5.3rc1](https://pypi.org/project/apache-airflow-providers-jdbc/4.5.3rc1)
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
## Provider [microsoft.azure: 11.1.0rc1](https://pypi.org/project/apache-airflow-providers-microsoft-azure/11.1.0rc1)
   - [x] [Add copy_object functionality for wasbhook (#43037)](https://github.com/apache/airflow/pull/43037): @kunaljubce
     Linked issues:
       - [x] [Linked Issue #42497](https://github.com/apache/airflow/issues/42497): @kunaljubce
   - [x] [Fix Power BI trigger testcase (#43494)](https://github.com/apache/airflow/pull/43494): @ambika-garg
   - [ ] [Add min version to ipykernel,scrapbook, pywinrm (#43603)](https://github.com/apache/airflow/pull/43603): @rawwar
## Provider [microsoft.mssql: 3.9.2rc1](https://pypi.org/project/apache-airflow-providers-microsoft-mssql/3.9.2rc1)
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
## Provider [microsoft.winrm: 3.6.1rc1](https://pypi.org/project/apache-airflow-providers-microsoft-winrm/3.6.1rc1)
   - [x] [Move responsibility to run a command from WinRMOperator to WinRMHook (#43646)](https://github.com/apache/airflow/pull/43646): @dabla
## Provider [mysql: 5.7.4rc1](https://pypi.org/project/apache-airflow-providers-mysql/5.7.4rc1)
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
   - [x] [Explain how to use uv with airflow virtualenv and make it works (#43604)](https://github.com/apache/airflow/pull/43604): @potiuk
     Linked issues:
       - [x] [Linked Issue #43200](https://github.com/apache/airflow/issues/43200): @potiuk
## Provider [odbc: 4.8.1rc1](https://pypi.org/project/apache-airflow-providers-odbc/4.8.1rc1)
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
## Provider [openlineage: 1.14.0rc1](https://pypi.org/project/apache-airflow-providers-openlineage/1.14.0rc1)
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
   - [ ] [add ProcessingEngineRunFacet to OL DAG Start event (#43213)](https://github.com/apache/airflow/pull/43213): @mobuchowski
   - [ ] [serialize asset/dataset timetable conditions in OpenLineage info also for older supported Airflow 2 versions (#43434)](https://github.com/apache/airflow/pull/43434): @mobuchowski
   - [x] [openlineage: accept whole config when instantiating OpenLineageClient (#43740)](https://github.com/apache/airflow/pull/43740): @JDarDagran
   - [x] [Temporarily limit openlineage to <1.24.0 (#43732)](https://github.com/apache/airflow/pull/43732): @potiuk
   - [x] [Standard provider python operator (#42081)](https://github.com/apache/airflow/pull/42081): @gopidesupavan
## Provider [oracle: 3.12.1rc1](https://pypi.org/project/apache-airflow-providers-oracle/3.12.1rc1)
   - [ ] [Fix oracle bulk insert issue when leftover chunk is empty (#43467)](https://github.com/apache/airflow/pull/43467): @romsharon98
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
## Provider [pagerduty: 3.8.1rc1](https://pypi.org/project/apache-airflow-providers-pagerduty/3.8.1rc1)
   - [x] [Standard provider bash operator (#42252)](https://github.com/apache/airflow/pull/42252): @gopidesupavan
   - [ ] [Purge existing SLA implementation (#42285)](https://github.com/apache/airflow/pull/42285): @ferruzzi
## Provider [papermill: 3.8.2rc1](https://pypi.org/project/apache-airflow-providers-papermill/3.8.2rc1)
   - [ ] [Add min version to ipykernel,scrapbook, pywinrm (#43603)](https://github.com/apache/airflow/pull/43603): @rawwar
## Provider [pinecone: 2.1.1rc1](https://pypi.org/project/apache-airflow-providers-pinecone/2.1.1rc1)
   - [ ] [Add source_tag to PineconeHook (#43960)](https://github.com/apache/airflow/pull/43960): @TJaniF
## Provider [postgres: 5.14.0rc1](https://pypi.org/project/apache-airflow-providers-postgres/5.14.0rc1)
   - [ ] [Add AWS Redshift Serverless support to PostgresHook (#43669)](https://github.com/apache/airflow/pull/43669): @topherinternational
   - [ ] [Fix PostgresHook bug when getting AWS Redshift Serverless credentials (#43807)](https://github.com/apache/airflow/pull/43807): @topherinternational
     Linked issues:
       - [ ] [Linked Issue #43669](https://github.com/apache/airflow/pull/43669): @topherinternational
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
## Provider [presto: 5.7.0rc1](https://pypi.org/project/apache-airflow-providers-presto/5.7.0rc1)
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
## Provider [slack: 8.9.2rc1](https://pypi.org/project/apache-airflow-providers-slack/8.9.2rc1)
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
## Provider [smtp: 1.8.1rc1](https://pypi.org/project/apache-airflow-providers-smtp/1.8.1rc1)
   - [x] [Standard provider bash operator (#42252)](https://github.com/apache/airflow/pull/42252): @gopidesupavan
   - [ ] [Purge existing SLA implementation (#42285)](https://github.com/apache/airflow/pull/42285): @ferruzzi
   - [ ] [Unify DAG schedule args and change default to None (#41453)](https://github.com/apache/airflow/pull/41453): @uranusjr
## Provider [snowflake: 5.8.1rc1](https://pypi.org/project/apache-airflow-providers-snowflake/5.8.1rc1)
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
   - [x] [Standard provider python operator (#42081)](https://github.com/apache/airflow/pull/42081): @gopidesupavan
## Provider [sqlite: 3.9.1rc1](https://pypi.org/project/apache-airflow-providers-sqlite/3.9.1rc1)
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
## Provider [standard: 0.0.1rc1](https://pypi.org/project/apache-airflow-providers-standard/0.0.1rc1)
   - [ ] [Airflow Standard Provider (#41564)](https://github.com/apache/airflow/pull/41564): @romsharon98
## Provider [teradata: 2.6.1rc1](https://pypi.org/project/apache-airflow-providers-teradata/2.6.1rc1)
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
## Provider [trino: 5.9.0rc1](https://pypi.org/project/apache-airflow-providers-trino/5.9.0rc1)
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
## Provider [vertica: 3.9.1rc1](https://pypi.org/project/apache-airflow-providers-vertica/3.9.1rc1)
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42
## Provider [ydb: 2.0.0rc1](https://pypi.org/project/apache-airflow-providers-ydb/2.0.0rc1)
   - [ ] [Migrate YDB Operator to new DBAPI (#43784)](https://github.com/apache/airflow/pull/43784): @vgvoleg
   - [ ] [Add support for semicolon stripping to DbApiHook, PrestoHook, and TrinoHook (#41916)](https://github.com/apache/airflow/pull/41916): @Illumaria
     Linked issues:
       - [ ] [Linked Issue #34828](https://github.com/apache/airflow/issues/34828): @MikeWallis42

<!--

NOTE TO RELEASE MANAGER:

You can move here the providers that have doc-only changes or for which changes are trivial, and
you could assess that they are OK.

-->
All users involved in the PRs:
@basvandriel @TJaniF @vgvoleg @kacpermuda @pyrr @Illumaria @MaksYermak @pranshupand-db @kunaljubce @romsharon98 @nathadfield @kaxil @Udbv @childe @krzysztof-kubis @LyndonFan @mobuchowski @ferruzzi @brou

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2024-11-14 19:28:17+00:00,[],2024-11-19 07:31:15+00:00,2024-11-18 09:07:45+00:00,https://github.com/apache/airflow/issues/44041,"[('area:providers', ''), ('kind:meta', 'High-level information important to the community'), ('testing status', 'Status of testing releases')]","[{'comment_id': 2480703460, 'issue_id': 2659852607, 'author': 'potiuk', 'body': 'I checked all my changes - most of them were docs and dependencies. Dependencies look good (openlineage change of mine was overwritten later by @JDarDagran - so all good there as well. \r\n\r\nSo everything looks good from my side. 👍', 'created_at': datetime.datetime(2024, 11, 16, 18, 16, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2480802760, 'issue_id': 2659852607, 'author': 'JDarDagran', 'body': '> I checked all my changes - most of them were docs and dependencies. Dependencies look good (openlineage change of mine was overwritten later by @JDarDagran - so all good there as well.\r\n\r\nYup, confirming my changes from from #43740 work as expected.', 'created_at': datetime.datetime(2024, 11, 16, 20, 36, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2480811933, 'issue_id': 2659852607, 'author': 'jimwbaldwin', 'body': '> https://github.com/apache/airflow/pull/43622: @jimwbaldwin\r\n\r\nHeya, confirming that #43622 is working correctly after installing [9.1.0rc4](https://pypi.org/project/apache-airflow-providers-amazon/9.1.0rc4)', 'created_at': datetime.datetime(2024, 11, 16, 21, 15, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2481250767, 'issue_id': 2659852607, 'author': 'gopidesupavan', 'body': ""I verified all my changes and tested a few DAG examples. The Spark, Beam, and Docker operators are working as expected.\r\n\r\nFor Snowflake, I couldn't test in a real environment but confirmed that the imports appear correct.\r\n\r\nAs for the following providers—Apprise, Atlassian, PagerDuty, OpenLineage, and SMTP—they only have documentation updates, which also look fine."", 'created_at': datetime.datetime(2024, 11, 17, 12, 38, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2481258285, 'issue_id': 2659852607, 'author': 'gopidesupavan', 'body': ""> I verified all my changes and tested a few DAG examples. The Spark, Beam, and Docker operators are working as expected.\r\n> \r\n> For AWS (appflow), Snowflake, I couldn't test in a real environment but confirmed that the imports appear correct.\r\n> \r\n> As for the following providers—Apprise, Atlassian, PagerDuty, OpenLineage, and SMTP—they only have documentation updates, which also look fine.\r\n\r\ngoogle, celery providers also fine.\r\n\r\nOne thing i observed the in the docker provider connection documentation. the auth reference url broken. raised fix here https://github.com/apache/airflow/pull/44112"", 'created_at': datetime.datetime(2024, 11, 17, 13, 1, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2481271826, 'issue_id': 2659852607, 'author': 'kunaljubce', 'body': 'Verified that my changes on azure provider works as expected with [microsoft.azure: 11.1.0rc1](https://pypi.org/project/apache-airflow-providers-microsoft-azure/11.1.0rc1)!', 'created_at': datetime.datetime(2024, 11, 17, 13, 42, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2481393975, 'issue_id': 2659852607, 'author': 'brouberol', 'body': 'I checked that my changes related to using Kerberos authentication for the stable API work with `apache-airflow-providers-fab==1.5.1rc1`.', 'created_at': datetime.datetime(2024, 11, 17, 17, 30, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2481394995, 'issue_id': 2659852607, 'author': 'mrk-andreev', 'body': 'I checked pull-43840 (  https://github.com/apache/airflow/pull/43840 ) for docker: 3.14.1rc1. \r\n\r\n[Apache Airflow _  Fix logs with leading spaces in the Docker operator (#33692) (#43840)_ @mrk-andreev.pdf](https://github.com/user-attachments/files/17791713/Apache.Airflow._.Fix.logs.with.leading.spaces.in.the.Docker.operator.33692.43840._.%40mrk-andreev.pdf)', 'created_at': datetime.datetime(2024, 11, 17, 17, 34, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2481396242, 'issue_id': 2659852607, 'author': 'ambika-garg', 'body': ""Verified my changes in azure provided. Working as expected.\r\n\r\nOn Thu, Nov 14, 2024 at 2:28\u202fPM Elad Kalif ***@***.***> wrote:\r\n\r\n> Body\r\n>\r\n> I have a kind request for all the contributors to the latest provider\r\n> packages release.\r\n> Could you please help us to test the RC versions of the providers?\r\n>\r\n> The guidelines on how to test providers can be found in\r\n>\r\n> Verify providers by contributors\r\n> <https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDER_PACKAGES.md#verify-the-release-candidate-by-contributors>\r\n>\r\n> Let us know in the comment, whether the issue is addressed.\r\n>\r\n> Those are providers that require testing as there were some substantial\r\n> changes introduced:\r\n> Provider amazon: 9.1.0rc4\r\n> <https://pypi.org/project/apache-airflow-providers-amazon/9.1.0rc4>\r\n>\r\n>    - feat: add OpenLineage support for RedshiftToS3Operator (#41632)\r\n>    <https://github.com/apache/airflow/pull/41632>: @kacpermuda\r\n>    <https://github.com/kacpermuda>\r\n>    Linked issues:\r\n>       - Linked Issue #41575 <https://github.com/apache/airflow/pull/41575>:\r\n>       @Artuz37 <https://github.com/Artuz37>\r\n>    - Add SageMakerProcessingSensor (#43144)\r\n>    <https://github.com/apache/airflow/pull/43144>: @vVv-AA\r\n>    <https://github.com/vVv-AA>\r\n>    - Make RedshiftDataOperator handle multiple queries (#42900)\r\n>    <https://github.com/apache/airflow/pull/42900>: @jroachgolf84\r\n>    <https://github.com/jroachgolf84>\r\n>    - fix(providers/amazon): alias is_authorized_dataset to\r\n>    is_authorized_asset (#43470)\r\n>    <https://github.com/apache/airflow/pull/43470>: @Lee-W\r\n>    <https://github.com/Lee-W>\r\n>    - Remove returns in final clause of athena hooks (#43426)\r\n>    <https://github.com/apache/airflow/pull/43426>: @yangyulely\r\n>    <https://github.com/yangyulely>\r\n>    Linked issues:\r\n>       - Linked Issue #43274\r\n>       <https://github.com/apache/airflow/issues/43274>: @iritkatriel\r\n>       <https://github.com/iritkatriel>\r\n>    - fix: replace \\s with space (#43849)\r\n>    <https://github.com/apache/airflow/pull/43849>: @LyndonFan\r\n>    <https://github.com/LyndonFan>\r\n>    Linked issues:\r\n>       - Linked Issue #38864 <https://github.com/apache/airflow/pull/38864>:\r\n>       @vincbeck <https://github.com/vincbeck>\r\n>       - Linked Issue #39252\r\n>       <https://github.com/apache/airflow/issues/39252>: @Taragolis\r\n>       <https://github.com/Taragolis>\r\n>       - Linked Issue #39428 <https://github.com/apache/airflow/pull/39428>:\r\n>       @romsharon98 <https://github.com/romsharon98>\r\n>       - Linked Issue #38734 <https://github.com/apache/airflow/pull/38734>:\r\n>       @dabla <https://github.com/dabla>\r\n>    - Fix HttpToS3Operator throws exception if s3_bucket parameter is not\r\n>    passed (#43828) <https://github.com/apache/airflow/pull/43828>:\r\n>    @kunaljubce <https://github.com/kunaljubce>\r\n>    Linked issues:\r\n>       - Linked Issue #43379\r\n>       <https://github.com/apache/airflow/issues/43379>: @kostiantyn-lab\r\n>       <https://github.com/kostiantyn-lab>\r\n>    - Fix awslogs_stream_prefix pattern (#43138)\r\n>    <https://github.com/apache/airflow/pull/43138>: @pyrr\r\n>    <https://github.com/pyrr>\r\n>    Linked issues:\r\n>       - Linked Issue #43130\r\n>       <https://github.com/apache/airflow/issues/43130>: @pyrr\r\n>       <https://github.com/pyrr>\r\n>    - Check if awslogs_stream_prefix already ends with container_name\r\n>    (#43724) <https://github.com/apache/airflow/pull/43724>: @pyrr\r\n>    <https://github.com/pyrr>\r\n>    Linked issues:\r\n>       - Linked Issue #43138 <https://github.com/apache/airflow/pull/43138>:\r\n>       @pyrr <https://github.com/pyrr>\r\n>    - bugfix description should be optional for openlineage integration\r\n>    with AthenaOperator (#43576)\r\n>    <https://github.com/apache/airflow/pull/43576>: @Udbv\r\n>    <https://github.com/Udbv>\r\n>    - Decouple volume_configurations from capacity_provider_strategy\r\n>    (#43047) <https://github.com/apache/airflow/pull/43047>: @pyrr\r\n>    <https://github.com/pyrr>\r\n>    Linked issues:\r\n>       - Linked Issue #43046\r\n>       <https://github.com/apache/airflow/issues/43046>: @pyrr\r\n>       <https://github.com/pyrr>\r\n>    - GlueJobOperator: add option to wait for cleanup before returning job\r\n>    status (#43688) <https://github.com/apache/airflow/pull/43688>:\r\n>    @eladkal <https://github.com/eladkal>\r\n>    - Resolve GlueJobTrigger serialization bug causing verbose to always\r\n>    be True (#43622) <https://github.com/apache/airflow/pull/43622>:\r\n>    @jimwbaldwin <https://github.com/jimwbaldwin>\r\n>    - Remove returns in final clause of S3ToDynamoDBOperator (#43456)\r\n>    <https://github.com/apache/airflow/pull/43456>: @yangyulely\r\n>    <https://github.com/yangyulely>\r\n>    Linked issues:\r\n>       - Linked Issue #43274\r\n>       <https://github.com/apache/airflow/issues/43274>: @iritkatriel\r\n>       <https://github.com/iritkatriel>\r\n>    - Remove sqlalchemy-redshift dependency (#43271)\r\n>    <https://github.com/apache/airflow/pull/43271>: @mobuchowski\r\n>    <https://github.com/mobuchowski>\r\n>    - feat(providers/amazon): Use asset in common provider (#43110)\r\n>    <https://github.com/apache/airflow/pull/43110>: @Lee-W\r\n>    <https://github.com/Lee-W>\r\n>    - Restrict looker-sdk version 24.18.0 and microsoft-kiota-http 1.3.4\r\n>    (#42954) <https://github.com/apache/airflow/pull/42954>: @gopidesupavan\r\n>    <https://github.com/gopidesupavan>\r\n>    - Limit mypy-boto3-appflow (#43436)\r\n>    <https://github.com/apache/airflow/pull/43436>: @eladkal\r\n>    <https://github.com/eladkal>\r\n>    - Standard provider python operator (#42081)\r\n>    <https://github.com/apache/airflow/pull/42081>: @gopidesupavan\r\n>    <https://github.com/gopidesupavan>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>    - remove all deprecations cncf.kubernetes (#43689)\r\n>    <https://github.com/apache/airflow/pull/43689>: @romsharon98\r\n>    <https://github.com/romsharon98>\r\n>    - Fix docstring for AthenaTrigger (#43616)\r\n>    <https://github.com/apache/airflow/pull/43616>: @eladkal\r\n>    <https://github.com/eladkal>\r\n>\r\n> Provider apache.beam: 5.9.1rc1\r\n> <https://pypi.org/project/apache-airflow-providers-apache-beam/5.9.1rc1>\r\n>\r\n>    - Standard provider python operator (#42081)\r\n>    <https://github.com/apache/airflow/pull/42081>: @gopidesupavan\r\n>    <https://github.com/gopidesupavan>\r\n>\r\n> Provider apache.drill: 2.8.1rc1\r\n> <https://pypi.org/project/apache-airflow-providers-apache-drill/2.8.1rc1>\r\n>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>\r\n> Provider apache.druid: 3.12.1rc1\r\n> <https://pypi.org/project/apache-airflow-providers-apache-druid/3.12.1rc1>\r\n>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>\r\n> Provider apache.hive: 8.2.1rc1\r\n> <https://pypi.org/project/apache-airflow-providers-apache-hive/8.2.1rc1>\r\n>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>    - Explain how to use uv with airflow virtualenv and make it works\r\n>    (#43604) <https://github.com/apache/airflow/pull/43604>: @potiuk\r\n>    <https://github.com/potiuk>\r\n>    Linked issues:\r\n>       - Linked Issue #43200\r\n>       <https://github.com/apache/airflow/issues/43200>: @potiuk\r\n>       <https://github.com/potiuk>\r\n>    - Move uncompress_file function from airflow.utils to Hive provider\r\n>    (#43526) <https://github.com/apache/airflow/pull/43526>: @kaxil\r\n>    <https://github.com/kaxil>\r\n>\r\n> Provider apache.impala: 1.5.2rc1\r\n> <https://pypi.org/project/apache-airflow-providers-apache-impala/1.5.2rc1>\r\n>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>\r\n> Provider apache.pinot: 4.5.1rc1\r\n> <https://pypi.org/project/apache-airflow-providers-apache-pinot/4.5.1rc1>\r\n>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>\r\n> Provider apache.spark: 4.11.3rc1\r\n> <https://pypi.org/project/apache-airflow-providers-apache-spark/4.11.3rc1>\r\n>\r\n>    - Standard provider python operator (#42081)\r\n>    <https://github.com/apache/airflow/pull/42081>: @gopidesupavan\r\n>    <https://github.com/gopidesupavan>\r\n>\r\n> Provider apprise: 1.4.1rc1\r\n> <https://pypi.org/project/apache-airflow-providers-apprise/1.4.1rc1>\r\n>\r\n>    - Standard provider bash operator (#42252)\r\n>    <https://github.com/apache/airflow/pull/42252>: @gopidesupavan\r\n>    <https://github.com/gopidesupavan>\r\n>    - Unify DAG schedule args and change default to None (#41453)\r\n>    <https://github.com/apache/airflow/pull/41453>: @uranusjr\r\n>    <https://github.com/uranusjr>\r\n>\r\n> Provider atlassian.jira: 2.7.1rc1\r\n> <https://pypi.org/project/apache-airflow-providers-atlassian-jira/2.7.1rc1>\r\n>\r\n>    - Standard provider bash operator (#42252)\r\n>    <https://github.com/apache/airflow/pull/42252>: @gopidesupavan\r\n>    <https://github.com/gopidesupavan>\r\n>\r\n> Provider celery: 3.8.4rc1\r\n> <https://pypi.org/project/apache-airflow-providers-celery/3.8.4rc1>\r\n>\r\n>    - AIP-72: Remove DAG pickling (#43667)\r\n>    <https://github.com/apache/airflow/pull/43667>: @kaxil\r\n>    <https://github.com/kaxil>\r\n>    - Standard provider python operator (#42081)\r\n>    <https://github.com/apache/airflow/pull/42081>: @gopidesupavan\r\n>    <https://github.com/gopidesupavan>\r\n>\r\n> Provider cloudant: 4.0.3rc1\r\n> <https://pypi.org/project/apache-airflow-providers-cloudant/4.0.3rc1>\r\n>\r\n>    - Bump uv to 0.4.28 (#43451)\r\n>    <https://github.com/apache/airflow/pull/43451>: @kaxil\r\n>    <https://github.com/kaxil>\r\n>\r\n> Provider cncf.kubernetes: 10.0.0rc1\r\n> <https://pypi.org/project/apache-airflow-providers-cncf-kubernetes/10.0.0rc1>\r\n>\r\n>    - remove all deprecations cncf.kubernetes (#43689)\r\n>    <https://github.com/apache/airflow/pull/43689>: @romsharon98\r\n>    <https://github.com/romsharon98>\r\n>    - Change default value of namespace in task.kubernetes to be None\r\n>    (#43402) <https://github.com/apache/airflow/pull/43402>: @romsharon98\r\n>    <https://github.com/romsharon98>\r\n>    - Add random_name_suffix to SparkKubernetesOperator (#43800) (#43847)\r\n>    <https://github.com/apache/airflow/pull/43847>: @mrk-andreev\r\n>    <https://github.com/mrk-andreev>\r\n>    Linked issues:\r\n>       - Linked Issue #43800\r\n>       <https://github.com/apache/airflow/issues/43800>: @Flametaa\r\n>       <https://github.com/Flametaa>\r\n>    - terminate kubernetes watch in case of unknown error while flushing\r\n>    queue (#43645) <https://github.com/apache/airflow/pull/43645>:\r\n>    @pavansharma36 <https://github.com/pavansharma36>\r\n>    Linked issues:\r\n>       - Linked Issue #43440\r\n>       <https://github.com/apache/airflow/issues/43440>: @iw-pavan\r\n>       <https://github.com/iw-pavan>\r\n>    - Update 'namespace' priority for 'find_pod' function (#43762)\r\n>    <https://github.com/apache/airflow/pull/43762>: @MaksYermak\r\n>    <https://github.com/MaksYermak>\r\n>    - AIP-72: Remove DAG pickling (#43667)\r\n>    <https://github.com/apache/airflow/pull/43667>: @kaxil\r\n>    <https://github.com/kaxil>\r\n>\r\n> Provider common.compat: 1.2.2rc1\r\n> <https://pypi.org/project/apache-airflow-providers-common-compat/1.2.2rc1>\r\n>\r\n>    - serialize asset/dataset timetable conditions in OpenLineage info\r\n>    also for older supported Airflow 2 versions (#43434)\r\n>    <https://github.com/apache/airflow/pull/43434>: @mobuchowski\r\n>    <https://github.com/mobuchowski>\r\n>    - Standard provider python operator (#42081)\r\n>    <https://github.com/apache/airflow/pull/42081>: @gopidesupavan\r\n>    <https://github.com/gopidesupavan>\r\n>\r\n> Provider common.sql: 1.20.0rc1\r\n> <https://pypi.org/project/apache-airflow-providers-common-sql/1.20.0rc1>\r\n>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>\r\n> Provider databricks: 6.13.0rc1\r\n> <https://pypi.org/project/apache-airflow-providers-databricks/6.13.0rc1>\r\n>\r\n>    - Add on_kill equivalent to Databricks SQL Hook to cancel timed out\r\n>    queries (#42668) <https://github.com/apache/airflow/pull/42668>:\r\n>    @R7L208 <https://github.com/R7L208>\r\n>    Linked issues:\r\n>       - Linked Issue #42115 <https://github.com/apache/airflow/pull/42115>:\r\n>       @R7L208 <https://github.com/R7L208>\r\n>    - Added support for job_parameters and dbt_commands in\r\n>    DatabricksRunNow Operator (#43895)\r\n>    <https://github.com/apache/airflow/pull/43895>: @pranshupand-db\r\n>    <https://github.com/pranshupand-db>\r\n>    - Enable workload identity authentication for the Databricks provider\r\n>    (#41639) <https://github.com/apache/airflow/pull/41639>: @basvandriel\r\n>    <https://github.com/basvandriel>\r\n>    Linked issues:\r\n>       - Linked Issue #41586\r\n>       <https://github.com/apache/airflow/issues/41586>: @basvandriel\r\n>       <https://github.com/basvandriel>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>\r\n> Provider dbt.cloud: 3.11.2rc1\r\n> <https://pypi.org/project/apache-airflow-providers-dbt-cloud/3.11.2rc1>\r\n>\r\n>    - Added condition to check if it is a scheduled save or rerun (#43453)\r\n>    <https://github.com/apache/airflow/pull/43453>: @krzysztof-kubis\r\n>    <https://github.com/krzysztof-kubis>\r\n>    Linked issues:\r\n>       - Linked Issue #43347\r\n>       <https://github.com/apache/airflow/issues/43347>: @krzysztof-kubis\r\n>       <https://github.com/krzysztof-kubis>\r\n>\r\n> Provider docker: 3.14.1rc1\r\n> <https://pypi.org/project/apache-airflow-providers-docker/3.14.1rc1>\r\n>\r\n>    - Fix logs with leading spaces in the Docker operator (#33692) (#43840)\r\n>    <https://github.com/apache/airflow/pull/43840>: @mrk-andreev\r\n>    <https://github.com/mrk-andreev>\r\n>    - Standard provider python operator (#42081)\r\n>    <https://github.com/apache/airflow/pull/42081>: @gopidesupavan\r\n>    <https://github.com/gopidesupavan>\r\n>\r\n> Provider elasticsearch: 5.5.3rc1\r\n> <https://pypi.org/project/apache-airflow-providers-elasticsearch/5.5.3rc1>\r\n>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>\r\n> Provider exasol: 4.6.1rc1\r\n> <https://pypi.org/project/apache-airflow-providers-exasol/4.6.1rc1>\r\n>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>\r\n> Provider fab: 1.5.1rc1\r\n> <https://pypi.org/project/apache-airflow-providers-fab/1.5.1rc1>\r\n>\r\n>    - fab_auth_manager: allow get_user method to return the user\r\n>    authenticated via Kerberos (#43662)\r\n>    <https://github.com/apache/airflow/pull/43662>: @brouberol\r\n>    <https://github.com/brouberol>\r\n>\r\n> Provider google: 10.26.0rc1\r\n> <https://pypi.org/project/apache-airflow-providers-google/10.26.0rc1>\r\n>\r\n>    - Add support for IAM database authentication for CloudSQL connection\r\n>    (#43631) <https://github.com/apache/airflow/pull/43631>: @MaksYermak\r\n>    <https://github.com/MaksYermak>\r\n>    - Provide option to force_delete for GCSToBigQueryOperator (#43785)\r\n>    <https://github.com/apache/airflow/pull/43785>: @nathadfield\r\n>    <https://github.com/nathadfield>\r\n>    - Unify reattach_states parameter logic across BigQuery operators\r\n>    (#43259) <https://github.com/apache/airflow/pull/43259>: @moiseenkov\r\n>    <https://github.com/moiseenkov>\r\n>    Linked issues:\r\n>       - Linked Issue #40664 <https://github.com/apache/airflow/pull/40664>:\r\n>       @VladaZakharova <https://github.com/VladaZakharova>\r\n>    - Remove non-existing field from the ListCustomTrainingJobOperator's\r\n>    template_fields (#43924) <https://github.com/apache/airflow/pull/43924>:\r\n>    @moiseenkov <https://github.com/moiseenkov>\r\n>    - Fix validating parent_model parameter in UploadModelOperator (#43473)\r\n>    <https://github.com/apache/airflow/pull/43473>: @VladaZakharova\r\n>    <https://github.com/VladaZakharova>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>    - Remove Airflow 2.1 compat code in Google provider (#43952)\r\n>    <https://github.com/apache/airflow/pull/43952>: @uranusjr\r\n>    <https://github.com/uranusjr>\r\n>    - Explain how to use uv with airflow virtualenv and make it works\r\n>    (#43604) <https://github.com/apache/airflow/pull/43604>: @potiuk\r\n>    <https://github.com/potiuk>\r\n>    Linked issues:\r\n>       - Linked Issue #43200\r\n>       <https://github.com/apache/airflow/issues/43200>: @potiuk\r\n>       <https://github.com/potiuk>\r\n>    - Standard provider python operator (#42081)\r\n>    <https://github.com/apache/airflow/pull/42081>: @gopidesupavan\r\n>    <https://github.com/gopidesupavan>\r\n>    - Bump google-ads version to use v18 by default (#43474)\r\n>    <https://github.com/apache/airflow/pull/43474>: @VladaZakharova\r\n>    <https://github.com/VladaZakharova>\r\n>\r\n> Provider http: 4.13.3rc1\r\n> <https://pypi.org/project/apache-airflow-providers-http/4.13.3rc1>\r\n>\r\n>    - Fix(http) bug, pass request_kwargs to HttpHooK.run (#43459)\r\n>    <https://github.com/apache/airflow/pull/43459>: @childe\r\n>    <https://github.com/childe>\r\n>    - Limit temporarily aiohttp to < 3.11.0 (#44006)\r\n>    <https://github.com/apache/airflow/pull/44006>: @potiuk\r\n>    <https://github.com/potiuk>\r\n>\r\n> Provider jdbc: 4.5.3rc1\r\n> <https://pypi.org/project/apache-airflow-providers-jdbc/4.5.3rc1>\r\n>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>\r\n> Provider microsoft.azure: 11.1.0rc1\r\n> <https://pypi.org/project/apache-airflow-providers-microsoft-azure/11.1.0rc1>\r\n>\r\n>    - Add copy_object functionality for wasbhook (#43037)\r\n>    <https://github.com/apache/airflow/pull/43037>: @kunaljubce\r\n>    <https://github.com/kunaljubce>\r\n>    Linked issues:\r\n>       - Linked Issue #42497\r\n>       <https://github.com/apache/airflow/issues/42497>: @kunaljubce\r\n>       <https://github.com/kunaljubce>\r\n>    - Fix Power BI trigger testcase (#43494)\r\n>    <https://github.com/apache/airflow/pull/43494>: @ambika-garg\r\n>    <https://github.com/ambika-garg>\r\n>    - Add min version to ipykernel,scrapbook, pywinrm (#43603)\r\n>    <https://github.com/apache/airflow/pull/43603>: @rawwar\r\n>    <https://github.com/rawwar>\r\n>\r\n> Provider microsoft.mssql: 3.9.2rc1\r\n> <https://pypi.org/project/apache-airflow-providers-microsoft-mssql/3.9.2rc1>\r\n>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>\r\n> Provider microsoft.winrm: 3.6.1rc1\r\n> <https://pypi.org/project/apache-airflow-providers-microsoft-winrm/3.6.1rc1>\r\n>\r\n>    - Move responsibility to run a command from WinRMOperator to WinRMHook\r\n>    (#43646) <https://github.com/apache/airflow/pull/43646>: @dabla\r\n>    <https://github.com/dabla>\r\n>\r\n> Provider mysql: 5.7.4rc1\r\n> <https://pypi.org/project/apache-airflow-providers-mysql/5.7.4rc1>\r\n>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>    - Explain how to use uv with airflow virtualenv and make it works\r\n>    (#43604) <https://github.com/apache/airflow/pull/43604>: @potiuk\r\n>    <https://github.com/potiuk>\r\n>    Linked issues:\r\n>       - Linked Issue #43200\r\n>       <https://github.com/apache/airflow/issues/43200>: @potiuk\r\n>       <https://github.com/potiuk>\r\n>\r\n> Provider odbc: 4.8.1rc1\r\n> <https://pypi.org/project/apache-airflow-providers-odbc/4.8.1rc1>\r\n>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>\r\n> Provider openlineage: 1.14.0rc1\r\n> <https://pypi.org/project/apache-airflow-providers-openlineage/1.14.0rc1>\r\n>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>    - add ProcessingEngineRunFacet to OL DAG Start event (#43213)\r\n>    <https://github.com/apache/airflow/pull/43213>: @mobuchowski\r\n>    <https://github.com/mobuchowski>\r\n>    - serialize asset/dataset timetable conditions in OpenLineage info\r\n>    also for older supported Airflow 2 versions (#43434)\r\n>    <https://github.com/apache/airflow/pull/43434>: @mobuchowski\r\n>    <https://github.com/mobuchowski>\r\n>    - openlineage: accept whole config when instantiating\r\n>    OpenLineageClient (#43740)\r\n>    <https://github.com/apache/airflow/pull/43740>: @JDarDagran\r\n>    <https://github.com/JDarDagran>\r\n>    - Temporarily limit openlineage to <1.24.0 (#43732)\r\n>    <https://github.com/apache/airflow/pull/43732>: @potiuk\r\n>    <https://github.com/potiuk>\r\n>    - Standard provider python operator (#42081)\r\n>    <https://github.com/apache/airflow/pull/42081>: @gopidesupavan\r\n>    <https://github.com/gopidesupavan>\r\n>\r\n> Provider oracle: 3.12.1rc1\r\n> <https://pypi.org/project/apache-airflow-providers-oracle/3.12.1rc1>\r\n>\r\n>    - Fix oracle bulk insert issue when leftover chunk is empty (#43467)\r\n>    <https://github.com/apache/airflow/pull/43467>: @romsharon98\r\n>    <https://github.com/romsharon98>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>\r\n> Provider pagerduty: 3.8.1rc1\r\n> <https://pypi.org/project/apache-airflow-providers-pagerduty/3.8.1rc1>\r\n>\r\n>    - Standard provider bash operator (#42252)\r\n>    <https://github.com/apache/airflow/pull/42252>: @gopidesupavan\r\n>    <https://github.com/gopidesupavan>\r\n>    - Purge existing SLA implementation (#42285)\r\n>    <https://github.com/apache/airflow/pull/42285>: @ferruzzi\r\n>    <https://github.com/ferruzzi>\r\n>\r\n> Provider papermill: 3.8.2rc1\r\n> <https://pypi.org/project/apache-airflow-providers-papermill/3.8.2rc1>\r\n>\r\n>    - Add min version to ipykernel,scrapbook, pywinrm (#43603)\r\n>    <https://github.com/apache/airflow/pull/43603>: @rawwar\r\n>    <https://github.com/rawwar>\r\n>\r\n> Provider pinecone: 2.1.1rc1\r\n> <https://pypi.org/project/apache-airflow-providers-pinecone/2.1.1rc1>\r\n>\r\n>    - Add source_tag to PineconeHook (#43960)\r\n>    <https://github.com/apache/airflow/pull/43960>: @TJaniF\r\n>    <https://github.com/TJaniF>\r\n>\r\n> Provider postgres: 5.14.0rc1\r\n> <https://pypi.org/project/apache-airflow-providers-postgres/5.14.0rc1>\r\n>\r\n>    - Add AWS Redshift Serverless support to PostgresHook (#43669)\r\n>    <https://github.com/apache/airflow/pull/43669>: @topherinternational\r\n>    <https://github.com/topherinternational>\r\n>    - Fix PostgresHook bug when getting AWS Redshift Serverless\r\n>    credentials (#43807) <https://github.com/apache/airflow/pull/43807>:\r\n>    @topherinternational <https://github.com/topherinternational>\r\n>    Linked issues:\r\n>       - Linked Issue #43669 <https://github.com/apache/airflow/pull/43669>:\r\n>       @topherinternational <https://github.com/topherinternational>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>\r\n> Provider presto: 5.7.0rc1\r\n> <https://pypi.org/project/apache-airflow-providers-presto/5.7.0rc1>\r\n>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>\r\n> Provider slack: 8.9.2rc1\r\n> <https://pypi.org/project/apache-airflow-providers-slack/8.9.2rc1>\r\n>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>\r\n> Provider smtp: 1.8.1rc1\r\n> <https://pypi.org/project/apache-airflow-providers-smtp/1.8.1rc1>\r\n>\r\n>    - Standard provider bash operator (#42252)\r\n>    <https://github.com/apache/airflow/pull/42252>: @gopidesupavan\r\n>    <https://github.com/gopidesupavan>\r\n>    - Purge existing SLA implementation (#42285)\r\n>    <https://github.com/apache/airflow/pull/42285>: @ferruzzi\r\n>    <https://github.com/ferruzzi>\r\n>    - Unify DAG schedule args and change default to None (#41453)\r\n>    <https://github.com/apache/airflow/pull/41453>: @uranusjr\r\n>    <https://github.com/uranusjr>\r\n>\r\n> Provider snowflake: 5.8.1rc1\r\n> <https://pypi.org/project/apache-airflow-providers-snowflake/5.8.1rc1>\r\n>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>    - Standard provider python operator (#42081)\r\n>    <https://github.com/apache/airflow/pull/42081>: @gopidesupavan\r\n>    <https://github.com/gopidesupavan>\r\n>\r\n> Provider sqlite: 3.9.1rc1\r\n> <https://pypi.org/project/apache-airflow-providers-sqlite/3.9.1rc1>\r\n>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>\r\n> Provider standard: 0.0.1rc1\r\n> <https://pypi.org/project/apache-airflow-providers-standard/0.0.1rc1>\r\n>\r\n>    - Airflow Standard Provider (#41564)\r\n>    <https://github.com/apache/airflow/pull/41564>: @romsharon98\r\n>    <https://github.com/romsharon98>\r\n>\r\n> Provider teradata: 2.6.1rc1\r\n> <https://pypi.org/project/apache-airflow-providers-teradata/2.6.1rc1>\r\n>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>\r\n> Provider trino: 5.9.0rc1\r\n> <https://pypi.org/project/apache-airflow-providers-trino/5.9.0rc1>\r\n>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>\r\n> Provider vertica: 3.9.1rc1\r\n> <https://pypi.org/project/apache-airflow-providers-vertica/3.9.1rc1>\r\n>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>\r\n> Provider ydb: 2.0.0rc1\r\n> <https://pypi.org/project/apache-airflow-providers-ydb/2.0.0rc1>\r\n>\r\n>    - Migrate YDB Operator to new DBAPI (#43784)\r\n>    <https://github.com/apache/airflow/pull/43784>: @vgvoleg\r\n>    <https://github.com/vgvoleg>\r\n>    - Add support for semicolon stripping to DbApiHook, PrestoHook, and\r\n>    TrinoHook (#41916) <https://github.com/apache/airflow/pull/41916>:\r\n>    @Illumaria <https://github.com/Illumaria>\r\n>    Linked issues:\r\n>       - Linked Issue #34828\r\n>       <https://github.com/apache/airflow/issues/34828>: @MikeWallis42\r\n>       <https://github.com/MikeWallis42>\r\n>\r\n> All users involved in the PRs:\r\n> @basvandriel <https://github.com/basvandriel> @TJaniF\r\n> <https://github.com/TJaniF> @vgvoleg <https://github.com/vgvoleg>\r\n> @kacpermuda <https://github.com/kacpermuda> @pyrr\r\n> <https://github.com/pyrr> @Illumaria <https://github.com/Illumaria>\r\n> @MaksYermak <https://github.com/MaksYermak> @pranshupand-db\r\n> <https://github.com/pranshupand-db> @kunaljubce\r\n> <https://github.com/kunaljubce> @romsharon98\r\n> <https://github.com/romsharon98> @nathadfield\r\n> <https://github.com/nathadfield> @kaxil <https://github.com/kaxil> @Udbv\r\n> <https://github.com/Udbv> @childe <https://github.com/childe>\r\n> @krzysztof-kubis <https://github.com/krzysztof-kubis> @LyndonFan\r\n> <https://github.com/LyndonFan> @mobuchowski\r\n> <https://github.com/mobuchowski> @ferruzzi <https://github.com/ferruzzi>\r\n> @brou <https://github.com/brou>\r\n> Committer\r\n>\r\n>    - I acknowledge that I am a maintainer/committer of the Apache Airflow\r\n>    project.\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/apache/airflow/issues/44041>, or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AQ3NQE6JCGWJI6TS2QYZWRT2AT2XHAVCNFSM6AAAAABRZUNHKSVHI2DSMVQWIX3LMV43ASLTON2WKOZSGY2TSOBVGI3DANY>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>"", 'created_at': datetime.datetime(2024, 11, 17, 17, 37, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2482190444, 'issue_id': 2659852607, 'author': 'moiseenkov', 'body': 'Hi,\r\n#43259 and #43924 work as expected', 'created_at': datetime.datetime(2024, 11, 18, 7, 52, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2482212451, 'issue_id': 2659852607, 'author': 'eladkal', 'body': '> One thing i observed the in the docker provider connection documentation. the auth reference url broken. raised fix here https://github.com/apache/airflow/pull/44112\r\n\r\nThanks. This will be added in the next wave', 'created_at': datetime.datetime(2024, 11, 18, 8, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2482227449, 'issue_id': 2659852607, 'author': 'dabla', 'body': '[43646](https://github.com/apache/airflow/pull/43646) works as expected', 'created_at': datetime.datetime(2024, 11, 18, 8, 12, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2482268539, 'issue_id': 2659852607, 'author': 'VladaZakharova', 'body': 'Hi there! Changes https://github.com/apache/airflow/pull/43474 and https://github.com/apache/airflow/pull/43474 work fine', 'created_at': datetime.datetime(2024, 11, 18, 8, 32, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2482351398, 'issue_id': 2659852607, 'author': 'eladkal', 'body': 'Thank you everyone. Providers are released.\r\n\r\nI invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).', 'created_at': datetime.datetime(2024, 11, 18, 9, 7, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2484893322, 'issue_id': 2659852607, 'author': 'Udbv', 'body': 'Hi #43576  works as expected.', 'created_at': datetime.datetime(2024, 11, 19, 7, 31, 14, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-11-16 18:16:30 UTC): I checked all my changes - most of them were docs and dependencies. Dependencies look good (openlineage change of mine was overwritten later by @JDarDagran - so all good there as well. 

So everything looks good from my side. 👍

JDarDagran on (2024-11-16 20:36:44 UTC): Yup, confirming my changes from from #43740 work as expected.

jimwbaldwin on (2024-11-16 21:15:24 UTC): Heya, confirming that #43622 is working correctly after installing [9.1.0rc4](https://pypi.org/project/apache-airflow-providers-amazon/9.1.0rc4)

gopidesupavan on (2024-11-17 12:38:32 UTC): I verified all my changes and tested a few DAG examples. The Spark, Beam, and Docker operators are working as expected.

For Snowflake, I couldn't test in a real environment but confirmed that the imports appear correct.

As for the following providers—Apprise, Atlassian, PagerDuty, OpenLineage, and SMTP—they only have documentation updates, which also look fine.

gopidesupavan on (2024-11-17 13:01:24 UTC): google, celery providers also fine.

One thing i observed the in the docker provider connection documentation. the auth reference url broken. raised fix here https://github.com/apache/airflow/pull/44112

kunaljubce on (2024-11-17 13:42:44 UTC): Verified that my changes on azure provider works as expected with [microsoft.azure: 11.1.0rc1](https://pypi.org/project/apache-airflow-providers-microsoft-azure/11.1.0rc1)!

brouberol on (2024-11-17 17:30:53 UTC): I checked that my changes related to using Kerberos authentication for the stable API work with `apache-airflow-providers-fab==1.5.1rc1`.

mrk-andreev on (2024-11-17 17:34:09 UTC): I checked pull-43840 (  https://github.com/apache/airflow/pull/43840 ) for docker: 3.14.1rc1. 

[Apache Airflow _  Fix logs with leading spaces in the Docker operator (#33692) (#43840)_ @mrk-andreev.pdf](https://github.com/user-attachments/files/17791713/Apache.Airflow._.Fix.logs.with.leading.spaces.in.the.Docker.operator.33692.43840._.%40mrk-andreev.pdf)

ambika-garg on (2024-11-17 17:37:53 UTC): Verified my changes in azure provided. Working as expected.

On Thu, Nov 14, 2024 at 2:28 PM Elad Kalif ***@***.***> wrote:

moiseenkov on (2024-11-18 07:52:31 UTC): Hi,
#43259 and #43924 work as expected

eladkal (Issue Creator) on (2024-11-18 08:05:00 UTC): Thanks. This will be added in the next wave

dabla on (2024-11-18 08:12:27 UTC): [43646](https://github.com/apache/airflow/pull/43646) works as expected

VladaZakharova on (2024-11-18 08:32:25 UTC): Hi there! Changes https://github.com/apache/airflow/pull/43474 and https://github.com/apache/airflow/pull/43474 work fine

eladkal (Issue Creator) on (2024-11-18 09:07:46 UTC): Thank you everyone. Providers are released.

I invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).

Udbv on (2024-11-19 07:31:14 UTC): Hi #43576  works as expected.

"
2659620401,issue,closed,not_planned,Color Alternative - Color Blind / Deutan,"### Description

Is there an alternative color scheme for people with color blindness?  


### Use case/motivation

I have some difficulty interpreting certain information that is color-based.
It doesn't prevent me from using the application, but sometimes I find it more challenging to interpret the information.

![image](https://github.com/user-attachments/assets/1af58299-d217-4b9f-8741-6826ca884580)


### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",supernoi,2024-11-14 17:58:51+00:00,[],2024-11-14 19:54:07+00:00,2024-11-14 19:54:07+00:00,https://github.com/apache/airflow/issues/44038,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2477073944, 'issue_id': 2659620401, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 14, 17, 58, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2477155183, 'issue_id': 2659620401, 'author': 'eladkal', 'body': 'You can change the colors:\r\nhttps://airflow.apache.org/docs/apache-airflow/stable/howto/customize-ui.html#customizing-state-colours\r\nNote that dark mode is available in Airflow. 2.10 (experimental) and we have a color blind track for Airflow 3:\r\nhttps://github.com/apache/airflow/issues/43054 - I invite you to participate on the track and share you insights to make Airflow more color blind friendly.', 'created_at': datetime.datetime(2024, 11, 14, 18, 40, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2477289192, 'issue_id': 2659620401, 'author': 'eladkal', 'body': 'Closing as we can focus the relevant needs in #43054', 'created_at': datetime.datetime(2024, 11, 14, 19, 54, 7, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-14 17:58:54 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2024-11-14 18:40:04 UTC): You can change the colors:
https://airflow.apache.org/docs/apache-airflow/stable/howto/customize-ui.html#customizing-state-colours
Note that dark mode is available in Airflow. 2.10 (experimental) and we have a color blind track for Airflow 3:
https://github.com/apache/airflow/issues/43054 - I invite you to participate on the track and share you insights to make Airflow more color blind friendly.

eladkal on (2024-11-14 19:54:07 UTC): Closing as we can focus the relevant needs in #43054

"
2659167003,issue,open,,Add a date formatter for new public API test cases,"### Body

As per this comment: https://github.com/apache/airflow/pull/43874#discussion_r1842333308, we need to add a date formatter in our new API test cases. This is because right now, we do something like this: `""data_interval_end"": dr.data_interval_end.isoformat().replace(""+00:00"", ""Z"")` to account for the move from +00:00 to Z time format.

Adding this new formatter will clean up the tests and make it easier to extend.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",amoghrajesh,2024-11-14 15:28:31+00:00,['iprithv'],2024-11-16 02:24:24+00:00,,https://github.com/apache/airflow/issues/44033,"[('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2479824786, 'issue_id': 2659167003, 'author': 'iprithv', 'body': 'I would like to work on this.', 'created_at': datetime.datetime(2024, 11, 15, 19, 59, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2480330510, 'issue_id': 2659167003, 'author': 'amoghrajesh', 'body': 'Feel free. Assigning to you.', 'created_at': datetime.datetime(2024, 11, 16, 2, 24, 15, tzinfo=datetime.timezone.utc)}]","iprithv (Assginee) on (2024-11-15 19:59:11 UTC): I would like to work on this.

amoghrajesh (Issue Creator) on (2024-11-16 02:24:15 UTC): Feel free. Assigning to you.

"
2659117224,issue,closed,completed,AIP-38 | Dashboard | Quick link to failed dags,,bbovenzi,2024-11-14 15:07:17+00:00,['bbovenzi'],2024-12-11 23:10:00+00:00,2024-12-11 23:10:00+00:00,https://github.com/apache/airflow/issues/44032,"[('area:UI', 'Related to UI/UX. For Frontend Developers.')]",[],
2659116366,issue,closed,completed,AIP-38 | Dag Overview | Quick link to failed tasks,"- [x] Button to show number and a graph of failed tasks in a specified time range
- [x] Direct to task page filtered to failed tasks",bbovenzi,2024-11-14 15:06:55+00:00,['bbovenzi'],2025-01-27 14:08:03+00:00,2025-01-27 14:08:03+00:00,https://github.com/apache/airflow/issues/44031,"[('area:UI', 'Related to UI/UX. For Frontend Developers.')]",[],
2658796486,issue,closed,completed,Trigger provider openlineage tests after assets change,"When running selective checks, we should automatically trigger open-lineage provider tests (in main) in any PR that changes assets, as openlineage tests heavily rely on assets and are often broken after asset changes. Example https://github.com/apache/airflow/pull/44025 fixing broken test:

![Image](https://github.com/user-attachments/assets/76c8b38f-917b-48f8-af7e-e5ee375a2700)
",potiuk,2024-11-14 13:17:09+00:00,['vatsrahul1001'],2024-11-22 23:41:55+00:00,2024-11-22 23:41:55+00:00,https://github.com/apache/airflow/issues/44026,"[('area:CI', ""Airflow's tests and continious integration""), ('provider:openlineage', 'AIP-53')]","[{'comment_id': 2476341224, 'issue_id': 2658796486, 'author': 'potiuk', 'body': 'cc: @vatsrahul1001 @uranusjr => we might fix it to happen automatically (this test was broken by #41325) but kind request - until when you add new fields to assets etc, plese add ""full tests needed"" to the PR :)', 'created_at': datetime.datetime(2024, 11, 14, 13, 20, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2476368554, 'issue_id': 2658796486, 'author': 'gopidesupavan', 'body': 'Yeah running openlineage tests would be good :)', 'created_at': datetime.datetime(2024, 11, 14, 13, 32, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2477085234, 'issue_id': 2658796486, 'author': 'vatsrahul1001', 'body': '@potiuk I will look into this', 'created_at': datetime.datetime(2024, 11, 14, 18, 4, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2477157403, 'issue_id': 2658796486, 'author': 'potiuk', 'body': 'assigned you @vatsrahul1001 !', 'created_at': datetime.datetime(2024, 11, 14, 18, 41, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2480586146, 'issue_id': 2658796486, 'author': 'vatsrahul1001', 'body': '@potiuk, I was checking how selective checks work, and your [comment](https://github.com/apache/airflow/issues/43498#issuecomment-2446157132) helped me understand it better. Should we create a new` FileGroupForCi.ASSET_FILES` for all asset files and use it to trigger the open-lineage provider tests?', 'created_at': datetime.datetime(2024, 11, 16, 14, 7, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2480804198, 'issue_id': 2658796486, 'author': 'potiuk', 'body': ""Yes. That's likely best idea"", 'created_at': datetime.datetime(2024, 11, 16, 20, 41, 58, tzinfo=datetime.timezone.utc)}]","potiuk (Issue Creator) on (2024-11-14 13:20:09 UTC): cc: @vatsrahul1001 @uranusjr => we might fix it to happen automatically (this test was broken by #41325) but kind request - until when you add new fields to assets etc, plese add ""full tests needed"" to the PR :)

gopidesupavan on (2024-11-14 13:32:42 UTC): Yeah running openlineage tests would be good :)

vatsrahul1001 (Assginee) on (2024-11-14 18:04:16 UTC): @potiuk I will look into this

potiuk (Issue Creator) on (2024-11-14 18:41:13 UTC): assigned you @vatsrahul1001 !

vatsrahul1001 (Assginee) on (2024-11-16 14:07:28 UTC): @potiuk, I was checking how selective checks work, and your [comment](https://github.com/apache/airflow/issues/43498#issuecomment-2446157132) helped me understand it better. Should we create a new` FileGroupForCi.ASSET_FILES` for all asset files and use it to trigger the open-lineage provider tests?

potiuk (Issue Creator) on (2024-11-16 20:41:58 UTC): Yes. That's likely best idea

"
2658686132,issue,closed,completed,Add pre-commit to validate provider __init__ is matching the template,"### Body

To avoid issues like: https://github.com/apache/airflow/pull/44017

Some info about templating of `__init__`
https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDER_PACKAGES.md#optional-apply-template-updates

The pre commit should fail if someone modify the init to a pattern that doesn't match the template

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2024-11-14 12:36:04+00:00,[],2024-12-06 10:17:08+00:00,2024-12-06 10:17:08+00:00,https://github.com/apache/airflow/issues/44024,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:CI', ""Airflow's tests and continious integration""), ('kind:meta', 'High-level information important to the community'), ('kind:task', 'A task that needs to be completed as part of a larger issue')]","[{'comment_id': 2476285376, 'issue_id': 2658686132, 'author': 'potiuk', 'body': 'FYI. There is a nice way to create an issue by maintainer - create an `item` in a project and convert it to the issue - this way you can bypass the meta task template :)', 'created_at': datetime.datetime(2024, 11, 14, 12, 53, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508970498, 'issue_id': 2658686132, 'author': 'potiuk', 'body': 'This will not be needed after #44511 is implemented.', 'created_at': datetime.datetime(2024, 11, 30, 13, 56, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-11-14 12:53:54 UTC): FYI. There is a nice way to create an issue by maintainer - create an `item` in a project and convert it to the issue - this way you can bypass the meta task template :)

potiuk on (2024-11-30 13:56:00 UTC): This will not be needed after #44511 is implemented.

"
2658681660,issue,open,,Smarter selection of sdist packages to build,"Currently we have an edge case that when we release a new version of provider that other providers depend on, the sdist build might have a problem with fiding the dependent packages in some PRS - when the PRs are modifying only some packages  as they are not locally built.

When we build sdist providers, we only build them in some PRs for only affected providers in this PR - and their dependencies. The sdist provider builds are spit into chunks, and the chunks are built independently in parallel (for speed) which means that in some cases the required new version of package will be missing.

For example this happened in https://github.com/apache/airflow/pull/44018 (https://github.com/apache/airflow/actions/runs/11835585982) when modified standard provider, 

```
  × No solution found when resolving dependencies:
  ╰─▶ Because only apache-airflow-providers-common-sql<=1.19.0 is available
      and apache-airflow-providers-standard==0.0.1.dev0 depends on
      apache-airflow-providers-common-sql>=1.20.0.dev0, we can conclude that
      apache-airflow-providers-standard==0.0.1.dev0 cannot be used.
      And because only apache-airflow-providers-standard==0.0.1.dev0 is
      available and you require apache-airflow-providers-standard, we can
      conclude that your requirements are unsatisfiable.

Traceback (most recent call last):
  File ""/opt/airflow/scripts/in_container/install_airflow_and_providers.py"", line 565, in <module>
    install_airflow_and_providers()
  File ""/usr/local/lib/python3.9/site-packages/click/core.py"", line 1157, in __call__
    return self.main(*args, **kwargs)
  File ""/usr/local/lib/python3.9/site-packages/rich_click/rich_command.py"", line 152, in main
    rv = self.invoke(ctx)
  File ""/usr/local/lib/python3.9/site-packages/click/core.py"", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/usr/local/lib/python3.9/site-packages/click/core.py"", line 783, in invoke
    return __callback(*args, **kwargs)
  File ""/opt/airflow/scripts/in_container/install_airflow_and_providers.py"", line 541, in install_airflow_and_providers
    run_command(base_install_providers_cmd, github_actions=github_actions, check=True)
  File ""/opt/airflow/scripts/in_container/in_container_utils.py"", line 47, in run_command
    result = subprocess.run(cmd, **kwargs)
  File ""/usr/local/lib/python3.9/subprocess.py"", line 528, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['/usr/local/bin/uv', 'pip', 'install', '--python', '/usr/local/bin/python', '/dist/apache_airflow-3.0.0.dev0.tar.gz', '/dist/apache_airflow_providers_standard-0.0.1.dev0.tar.gz', '/dist/apache_airflow-3.0.0.dev0.tar.gz']' returned non-zero exit status 1.
```

We should be a bit smarter here and detect (in each chunk separately) which providers are **additionally** needed to be built - in this case  since standard depends on `comon.sql >= 1.20.0`, the sdist chunk installation should add common.sql to the list of installed providers in the chunk where `standard` is installed.",potiuk,2024-11-14 12:34:05+00:00,[],2024-11-14 12:36:26+00:00,,https://github.com/apache/airflow/issues/44023,"[('area:CI', ""Airflow's tests and continious integration"")]",[],
2658658633,issue,closed,completed,Exasol hook logging of rowcount,"### Apache Airflow Provider(s)

exasol

### Versions of Apache Airflow Providers

apache-airflow-providers-exasol = ""4.4.2""

### Apache Airflow version

2.9.3

### Operating System

Debian GNU/Linux 11 (bullseye)

### Deployment

Virtualenv installation

### Deployment details

_No response_

### What happened

Whenever a query is ran using the `ExasolHook`, the affected rows should be logged. However it is currently only loggingthe python callable instead of actually calling the function.

### What you think should happen instead

The actual rowcount should be logged

### How to reproduce

Execute any query using ExasolHook

```
hook = ExasolHook()
hook.run(""select 1"")
```

### Anything else

The issue happens everytime and is still prevalent in the most recent version. The line that needs change: https://github.com/apache/airflow/blob/b00a81000b1efe7c47a4c0e8a6167a0718dc909e/providers/src/airflow/providers/exasol/hooks/exasol.py#L248

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",XtremeBaumer,2024-11-14 12:23:09+00:00,[],2025-01-18 16:40:21+00:00,2025-01-18 16:40:21+00:00,https://github.com/apache/airflow/issues/44022,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:exasol', '')]","[{'comment_id': 2476221269, 'issue_id': 2658658633, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 14, 12, 23, 11, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-14 12:23:11 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2658429314,issue,closed,completed,Improve Test OpenAPI client tests,"We should move test open API client tests to be run inside breeze.

The Open API client tests are pretty brittle. They run in ""runner"" directly and they are not using `breeze` image as the base for running them - which means that they need to install airflow in the runner directly. This is a bit problematic in general case - because sometimes changes are introduced in providers that require ""main"" airflow to use new providers (for example when FAB gets incompatible changes or async IO is tested. 

Then you need to not only install airflow but also build and install providers - you need to build the providers locally from sources and install some dependencies which might or might not cleanly install on ""bare"" runner environment 

This has been somewhat mitigated by having a list of providers to install - but this is brittle, might change and some people find it ""hard"" to folllow - because they do not understand why sometimes they need to build and install those providers. Also what does not help is yaml keeping the scripts has some very unobvious problems where indentation might introduce unexpected end of lines etc.  

When you attempt to build and install all providers, it's not easy sometimes and it will change over time as well - and when you use `uv` it tries to install and resolve all dependencies (including optional) so while it is way faster than `pip` it also tries to install and build some of the dependencies (like kerberos) that migh need some system-level tools to get installed.

Attempt to do so has been done in https://github.com/apache/airflow/pull/44007 and naive ""build and install everything"" ends up with:

```
Caused by: Failed to download and build `gssapi==1.9.0`
  Caused by: Build backend failed to determine requirements with `build_wheel()` (exit status: 1)

[stderr]
/bin/sh: 1: krb5-config: not found
Traceback (most recent call last):
  File ""<string>"", line 14, in <module>
  File ""/home/runner/.cache/uv/builds-v0/.tmp6LD5ic/lib/python3.9/site-packages/setuptools/build_meta.py"", line 334, in get_requires_for_build_wheel
    return self._get_build_requires(config_settings, requirements=[])
  File ""/home/runner/.cache/uv/builds-v0/.tmp6LD5ic/lib/python3.9/site-packages/setuptools/build_meta.py"", line 304, in _get_build_requires
    self.run_setup()
  File ""/home/runner/.cache/uv/builds-v0/.tmp6LD5ic/lib/python3.9/site-packages/setuptools/build_meta.py"", line 320, in run_setup
    exec(code, locals())
  File ""<string>"", line 109, in <module>
  File ""<string>"", line 22, in get_output
  File ""/opt/hostedtoolcache/Python/3.9.20/x64/lib/python3.9/subprocess.py"", line 424, in check_output
    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,
  File ""/opt/hostedtoolcache/Python/3.9.20/x64/lib/python3.9/subprocess.py"", line 528, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command 'krb5-config --libs gssapi' returned non-zero exit status [127](https://github.com/apache/airflow/actions/runs/11829519567/job/32961957929#step:14:128).
```

That shows why we have CI image - because the CI image has all the necessary dependencies and base OS to install airflow + all dependencies and we keep it updated as airflow evolves and new dependencies are added.


Therefore it would be best is if the Test Open API client would be run in breeze (there we have pre-installed airflow with editable install for airflow, task_sdk and all providers so we only need to build and install python client).

This would require few things:

1) building python client outside of breeze
2) entering breeze
3) running the test script inside breeze 

Also the whole TestOpenAPI client shoudl likely be moved to ""special tests"" from ""basic tests"" - becasause it has to wait for the CI image to be ready and Basic tests are run without waiting for the image.
",potiuk,2024-11-14 10:51:45+00:00,['gopidesupavan'],2024-11-27 13:14:53+00:00,2024-11-27 13:14:53+00:00,https://github.com/apache/airflow/issues/44020,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:dev-tools', '')]","[{'comment_id': 2476036245, 'issue_id': 2658429314, 'author': 'potiuk', 'body': 'cc: @dstandish @kaxil @pierrejeambrun  - you seemed to have some hard time with that test recently, here is a way how we can make it easier to maintain in the future.', 'created_at': datetime.datetime(2024, 11, 14, 10, 57, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2476274829, 'issue_id': 2658429314, 'author': 'potiuk', 'body': 'The interesting thing here is that `uv` is trying to install and buid `krb5` because `krb5` has only ""sdist"" package + (wait for it) binary wheels for macos : https://pypi.org/project/krb5/#files\r\n\r\nSo in order to just **SEE** by uv which dependencies `krb5` has, it needs to be downloaded and converted to .whl package :exploding_head: ....\r\n\r\nThis one of the edge cases that make package resolution for Python packages so complex and brittle. It\'s a bit of miracle it works as well as it does regardless.', 'created_at': datetime.datetime(2024, 11, 14, 12, 48, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2476277056, 'issue_id': 2658429314, 'author': 'potiuk', 'body': 'Now ... This is why CI image of Airflow is so useful, because it has everything needed to build all 700 packages if necessary :D', 'created_at': datetime.datetime(2024, 11, 14, 12, 49, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2481354885, 'issue_id': 2658429314, 'author': 'gopidesupavan', 'body': 'Looking into this, probably this need a new breeze command?', 'created_at': datetime.datetime(2024, 11, 17, 16, 29, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2481703224, 'issue_id': 2658429314, 'author': 'potiuk', 'body': '> Looking into this, probably this need a new breeze command?\r\n\r\nYes. Likely best to do it this way. Should be `breeze testing openapi-tests` command ?', 'created_at': datetime.datetime(2024, 11, 18, 0, 25, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2483141789, 'issue_id': 2658429314, 'author': 'gopidesupavan', 'body': '> > Looking into this, probably this need a new breeze command?\r\n> \r\n> Yes. Likely best to do it this way. Should be `breeze testing openapi-tests` command ?\r\n\r\nYeah agree make sense :)', 'created_at': datetime.datetime(2024, 11, 18, 14, 4, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2492402179, 'issue_id': 2658429314, 'author': 'gopidesupavan', 'body': 'I think this fall under similar to docker-compose-tests? if i look at this one, it expects server running and dags require loading before running the tests? https://github.com/apache/airflow/blob/main/clients/python/test_python_client.py#L59. \r\n\r\nAny suggestions @potiuk', 'created_at': datetime.datetime(2024, 11, 21, 21, 45, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2494932820, 'issue_id': 2658429314, 'author': 'potiuk', 'body': 'Yes, I think the current way it is implemented is generally good - it does start a server and waits for the DAGs to be loaded so that we can run openapi python test - > the only change to implement is not to create a venv for Airflow to run all that but to run a script inside breeze environment - (we call a few ""in_container"" scripts in other tests.', 'created_at': datetime.datetime(2024, 11, 22, 21, 55, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2494940687, 'issue_id': 2658429314, 'author': 'gopidesupavan', 'body': 'Thanks , make sense, then I am on a correct path :), creating script which does starts server in container.', 'created_at': datetime.datetime(2024, 11, 22, 22, 2, 13, tzinfo=datetime.timezone.utc)}]","potiuk (Issue Creator) on (2024-11-14 10:57:07 UTC): cc: @dstandish @kaxil @pierrejeambrun  - you seemed to have some hard time with that test recently, here is a way how we can make it easier to maintain in the future.

potiuk (Issue Creator) on (2024-11-14 12:48:52 UTC): The interesting thing here is that `uv` is trying to install and buid `krb5` because `krb5` has only ""sdist"" package + (wait for it) binary wheels for macos : https://pypi.org/project/krb5/#files

So in order to just **SEE** by uv which dependencies `krb5` has, it needs to be downloaded and converted to .whl package :exploding_head: ....

This one of the edge cases that make package resolution for Python packages so complex and brittle. It's a bit of miracle it works as well as it does regardless.

potiuk (Issue Creator) on (2024-11-14 12:49:58 UTC): Now ... This is why CI image of Airflow is so useful, because it has everything needed to build all 700 packages if necessary :D

gopidesupavan (Assginee) on (2024-11-17 16:29:58 UTC): Looking into this, probably this need a new breeze command?

potiuk (Issue Creator) on (2024-11-18 00:25:57 UTC): Yes. Likely best to do it this way. Should be `breeze testing openapi-tests` command ?

gopidesupavan (Assginee) on (2024-11-18 14:04:26 UTC): Yeah agree make sense :)

gopidesupavan (Assginee) on (2024-11-21 21:45:52 UTC): I think this fall under similar to docker-compose-tests? if i look at this one, it expects server running and dags require loading before running the tests? https://github.com/apache/airflow/blob/main/clients/python/test_python_client.py#L59. 

Any suggestions @potiuk

potiuk (Issue Creator) on (2024-11-22 21:55:57 UTC): Yes, I think the current way it is implemented is generally good - it does start a server and waits for the DAGs to be loaded so that we can run openapi python test - > the only change to implement is not to create a venv for Airflow to run all that but to run a script inside breeze environment - (we call a few ""in_container"" scripts in other tests.

gopidesupavan (Assginee) on (2024-11-22 22:02:13 UTC): Thanks , make sense, then I am on a correct path :), creating script which does starts server in container.

"
2658415799,issue,closed,completed,Autocomplete Attribute Not Disabled for Password Fields in Login Forms,"### Description

Currently, the password input fields in Apache Airflow's login forms do not have the autocomplete attribute set to off. This allows browsers to store passwords entered by users, which poses a potential security risk—especially when accessing Airflow from shared or public computers. To enhance security and adhere to best practices for handling sensitive information, the autocomplete attribute should be disabled for all password fields in form-based authentication.

### Use case/motivation

As an employee responsible for the security of our corporate IT systems that utilize Apache Airflow, I want to enhance the protection of user credentials by disabling the autocomplete feature on password fields. This change will make our systems more secure for all users by preventing browsers from storing sensitive passwords, which could be exploited if a device is compromised or shared. Additionally, implementing this fix will ensure that our automated security scanners no longer flag this issue, helping us maintain compliance with our organization's security policies and reducing the overhead of managing reported vulnerabilities.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",MarBed190,2024-11-14 10:46:33+00:00,['geraj1010'],2024-12-15 09:25:35+00:00,2024-12-15 09:25:35+00:00,https://github.com/apache/airflow/issues/44019,"[('security', 'Security issues that must be fixed'), ('kind:feature', 'Feature Requests')]","[{'comment_id': 2476014173, 'issue_id': 2658415799, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 14, 10, 46, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2480926735, 'issue_id': 2658415799, 'author': 'geraj1010', 'body': ""I think it might just be an update to `airflow/www/forms.py`. I did find a SO https://stackoverflow.com/questions/20326511/how-to-avoid-password-being-auto-filled, on turning `autocomplete` off. However, I'm not sure if this solution is still valid, since apparently Chrome may ignore the setting. \r\n\r\nI might be interested in this :)"", 'created_at': datetime.datetime(2024, 11, 17, 4, 58, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2492928520, 'issue_id': 2658415799, 'author': 'amoghrajesh', 'body': 'I think this is something that can be worth fixing to improve the security posture of airflow.\r\n\r\nFeel free to work on this @geraj1010\r\n\r\ncc @bbovenzi', 'created_at': datetime.datetime(2024, 11, 22, 5, 55, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499702984, 'issue_id': 2658415799, 'author': 'geraj1010', 'body': ""@amoghrajesh Okay sounds good. \r\nI was off on what I thought needed to be updated. I think it's actually `www/static/js/login/Forms.tsx`"", 'created_at': datetime.datetime(2024, 11, 26, 5, 35, 54, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-14 10:46:35 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

geraj1010 (Assginee) on (2024-11-17 04:58:53 UTC): I think it might just be an update to `airflow/www/forms.py`. I did find a SO https://stackoverflow.com/questions/20326511/how-to-avoid-password-being-auto-filled, on turning `autocomplete` off. However, I'm not sure if this solution is still valid, since apparently Chrome may ignore the setting. 

I might be interested in this :)

amoghrajesh on (2024-11-22 05:55:30 UTC): I think this is something that can be worth fixing to improve the security posture of airflow.

Feel free to work on this @geraj1010

cc @bbovenzi

geraj1010 (Assginee) on (2024-11-26 05:35:54 UTC): @amoghrajesh Okay sounds good. 
I was off on what I thought needed to be updated. I think it's actually `www/static/js/login/Forms.tsx`

"
2658185832,issue,open,,KubernetesPodOperator in deferrable mode does not delete pod when task is marked externally,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

KubernetesPodOperator in deferrable mode does not delete pod when task is marked fail/success manually in the UI.

### What you think should happen instead?

KubernetesPodOperator in deferrable mode should delete pod when the task is marked fail/success

### How to reproduce

KubernetesPodOperator in deferrable mode does not delete pod when task is marked fail

### Operating System

Linux

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes==9.0.0

### Deployment

Google Cloud Composer

### Deployment details

Composer 2.9.9
Airflow 2.9.3

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",kien-truong,2024-11-14 09:32:53+00:00,[],2024-11-19 03:38:39+00:00,,https://github.com/apache/airflow/issues/44014,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]","[{'comment_id': 2475850616, 'issue_id': 2658185832, 'author': 'kien-truong', 'body': 'Related issue:\r\n\r\n* [Deferrable operator tasks do not call on_kill method when fail or restarted](https://github.com/apache/airflow/issues/36090)', 'created_at': datetime.datetime(2024, 11, 14, 9, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2484640392, 'issue_id': 2658185832, 'author': 'Lee-W', 'body': ""Yep, I think we'll probably need to wait and see how https://github.com/apache/airflow/issues/36090 is handled"", 'created_at': datetime.datetime(2024, 11, 19, 3, 38, 35, tzinfo=datetime.timezone.utc)}]","kien-truong (Issue Creator) on (2024-11-14 09:36:00 UTC): Related issue:

* [Deferrable operator tasks do not call on_kill method when fail or restarted](https://github.com/apache/airflow/issues/36090)

Lee-W on (2024-11-19 03:38:35 UTC): Yep, I think we'll probably need to wait and see how https://github.com/apache/airflow/issues/36090 is handled

"
2658090122,issue,closed,completed,Broken link to docker provider example dags,"### What do you see as an issue?

There is a broken link in the [index of providers-docker](https://github.com/apache/airflow/blob/main/docs/apache-airflow-providers-docker/index.rst?plain=1#L52):
```Example DAGs <https://github.com/apache/airflow/tree/providers-docker/|version|/providers/tests/system/docker>```

### Solving the problem

If the example DAGs still exist. The link should point to their location.

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",juhi24,2024-11-14 08:53:45+00:00,[],2024-11-15 18:37:27+00:00,2024-11-15 18:37:27+00:00,https://github.com/apache/airflow/issues/44012,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('kind:documentation', ''), ('provider:docker', '')]","[{'comment_id': 2475759213, 'issue_id': 2658090122, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 14, 8, 53, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2476476562, 'issue_id': 2658090122, 'author': 'amirmor1', 'body': 'https://github.com/apache/airflow/pull/44034', 'created_at': datetime.datetime(2024, 11, 14, 14, 20, 15, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-14 08:53:47 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

amirmor1 on (2024-11-14 14:20:15 UTC): https://github.com/apache/airflow/pull/44034

"
2657181985,issue,open,,Documentation and Examples,"Most of this should be done as work progresses, but examples and finishing touches will require some time.",ferruzzi,2024-11-14 00:24:31+00:00,[],2024-11-14 00:24:31+00:00,,https://github.com/apache/airflow/issues/44005,"[('AIP-86:Deadlines', '')]",[],
2657179036,issue,open,,Implement additional DeadlineCallbacks types,Implement DeadlineCallbacks.FAIL_DAGRUN,ferruzzi,2024-11-14 00:22:28+00:00,[],2024-11-14 00:25:36+00:00,,https://github.com/apache/airflow/issues/44004,"[('AIP-86:Deadlines', '')]",[],
2657175597,issue,open,,Implement additional DeadlineAlerts types,"Implement at least `dag_run - queued-at` and `job - start-date`, maybe `serialized_dag - last_updated` as well?

These should be added in the form of DeadlineAlerts.DAGRUN_QUEUED_AT, etc.",ferruzzi,2024-11-14 00:18:57+00:00,[],2024-11-14 00:25:52+00:00,,https://github.com/apache/airflow/issues/44003,"[('AIP-86:Deadlines', '')]",[],
2657172007,issue,open,,Implement Alert Handler,Use the old SLA callback handler (removed in https://github.com/apache/airflow/pull/42285/) as guidance,ferruzzi,2024-11-14 00:15:34+00:00,[],2024-11-14 00:27:15+00:00,,https://github.com/apache/airflow/issues/44002,"[('AIP-86:Deadlines', '')]",[],
2657170573,issue,open,,Implement callback queue,,ferruzzi,2024-11-14 00:14:15+00:00,[],2024-11-14 00:14:15+00:00,,https://github.com/apache/airflow/issues/44001,"[('AIP-86:Deadlines', '')]",[],
2657169524,issue,open,,Implement the callback interface class and the DeadlineCallbacks.LOG_ERROR callback,"A callback can be any Callable, and the DeadlineAlert object may optionally have a callback_kwargs to pass to it.

A callback maybe a Notifier:
```
callback=send_smtp_notification(
            from_email=""someone@mail.com"",
            to=""someone@mail.com"",
            subject=""[Error] The dag {{ dag.dag_id }} failed"",
            html_content=""debug logs"",
        )
```

A callback may be a partial():
```
callback=functools.partial(my_custom_callback, arg1=""Something went wrong!"", arg2=""Error Code: 123""),
```

A callback may be a Callable (with or without a callback_kwargs):
```
callback=my_custom_callback,
callback_kwargs = {""arg1"": ""Something went wrong!"", ""arg2"": ""Error Code: 123""}
```


Implement the interface which will allow for these potential usecases and implement the first built-in callback, which will simply log the miss.  Unit tests should verify that the different permutations of Notifiers, Callables and partials all work with and without callback_kwargs.",ferruzzi,2024-11-14 00:13:12+00:00,[],2024-11-14 00:23:44+00:00,,https://github.com/apache/airflow/issues/44000,"[('kind:feature', 'Feature Requests'), ('AIP-86:Deadlines', '')]",[],
2657161520,issue,open,,Add Pop at the end of the dagrun,"When a dagrun ends, if it had a deadline, attempt to pop its Deadline timestamp.

Do we also want to tie a clear_deadlines into starting a new executor so they are pruned if the executor restarts?",ferruzzi,2024-11-14 00:06:24+00:00,[],2024-11-15 18:56:25+00:00,,https://github.com/apache/airflow/issues/43999,"[('AIP-86:Deadlines', '')]",[],
2657150213,issue,open,,Add peek/pop code to the scheduler loop,"In the scheduler loop add “check for DeadlineAlert misses” which peeks at the earliest Deadline and, if it has passed, then call a callback handler.

The callback handler should peek at the earliest Deadline, if it has passed then queue the callback, and recurse until there are no past-due Deadlines..

When working on this one, the PR should include benchmarking to show the exact impact of the extra workload being added in the hot loop.",ferruzzi,2024-11-13 23:56:03+00:00,[],2024-11-14 00:26:28+00:00,,https://github.com/apache/airflow/issues/43997,"[('AIP-86:Deadlines', '')]",[],
2657144845,issue,open,,Implement the DeadlineAlerts Interface class and DeadlineAlerts.DAGRUN_EXECUTION_DATE,"Implement the DeadlineAlerts class using a builder model and the first ""type"" of DeadlineAlert with thorough test coverage.",ferruzzi,2024-11-13 23:52:09+00:00,[],2024-11-14 00:20:00+00:00,,https://github.com/apache/airflow/issues/43996,"[('AIP-86:Deadlines', '')]",[],
2657139469,issue,open,,Insert the Put call into dag processor,"Add ""store next deadline"" in dag_processing/processor.py
 - If this DAG has an Alert configured, then calculate the “need-by” and store the timestamp",ferruzzi,2024-11-13 23:46:53+00:00,[],2024-11-14 00:26:45+00:00,,https://github.com/apache/airflow/issues/43994,"[('AIP-86:Deadlines', '')]",[],
2657126251,issue,open,,Implement put/peek/pop helpers,"Add helpers to peek and pop the earliest deadline, and to store a calculated deadline
- storing in a sorted object makes this more efficient",ferruzzi,2024-11-13 23:36:59+00:00,[],2024-11-13 23:48:35+00:00,,https://github.com/apache/airflow/issues/43993,"[('AIP-86:Deadlines', '')]",[],
2657123024,issue,closed,completed,microsoft-psrp securestring jinja2.exceptions.TemplateAssertionError,"### Apache Airflow Provider(s)

microsoft-psrp

### Versions of Apache Airflow Providers

apache-airflow-providers-microsoft-psrp==2.8.0

### Apache Airflow version

2.10.2

### Operating System

Debian GNU/Linux 12 (bookworm)

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Using official helm chart and the following slightly extended Dockerfile:
```
FROM apache/airflow:2.10.2
RUN pip install --no-cache-dir ""apache-airflow[microsoft-psrp]==${AIRFLOW_VERSION}"" \
    --constraint ""https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-3.9.txt""
```

### What happened

`jinja2.exceptions.TemplateAssertionError: No filter named 'securestring'` 

It appears the securestring template filter is not working as intended. 

Summarized Error:
```
ERROR - Exception rendering Jinja template for task 'Write-Output', field 'arguments'. Template: [""{{ 'foo' | securestring }}""]
  ( ... lengthy stack trace redacted ... )
jinja2.exceptions.TemplateAssertionError: No filter named 'securestring'.
```
<details>
<summary>
Full execution log including the redacted stack trace:
</summary>

```
[2024-11-13, 16:11:36 MST] {taskinstance.py:2865} INFO - Starting attempt 1 of 1
[2024-11-13, 16:11:36 MST] {taskinstance.py:2888} INFO - Executing <Task(PsrpOperator): Write-Output> on 2024-11-13 23:11:35.867453+00:00
[2024-11-13, 16:11:36 MST] {warnings.py:112} WARNING - /home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=14633) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2024-11-13, 16:11:36 MST] {standard_task_runner.py:72} INFO - Started process 14644 to run task
[2024-11-13, 16:11:36 MST] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'psrpoperator_securestring_error_reproduction', 'Write-Output', 'manual__2024-11-13T23:11:35.867453+00:00', '--job-id', '802', '--raw', '--subdir', 'DAGS_FOLDER/psrp_operator_securestring_error_repoduction.py', '--cfg-path', '/tmp/tmpwrdenr76']
[2024-11-13, 16:11:36 MST] {standard_task_runner.py:105} INFO - Job 802: Subtask Write-Output
[2024-11-13, 16:11:37 MST] {task_command.py:467} INFO - Running <TaskInstance: psrpoperator_securestring_error_reproduction.Write-Output manual__2024-11-13T23:11:35.867453+00:00 [running]> on host experimental-temp-airflow-worker-0.experimental-temp-airflow-worker.experimental-temp-airflow.svc.cluster.local
[2024-11-13, 16:11:37 MST] {abstractoperator.py:778} ERROR - Exception rendering Jinja template for task 'Write-Output', field 'arguments'. Template: [""{{ 'foo' | securestring }}""]
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/abstractoperator.py"", line 770, in _do_render_template_fields
    rendered_content = self.render_template(
                       ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/template/templater.py"", line 183, in render_template
    return [self.render_template(element, context, jinja_env, oids) for element in value]
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/template/templater.py"", line 170, in render_template
    template = jinja_env.from_string(value)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/jinja2/environment.py"", line 1108, in from_string
    return cls.from_code(self, self.compile(source), gs, None)
                               ^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/jinja2/environment.py"", line 768, in compile
    self.handle_exception(source=source_hint)
  File ""/home/airflow/.local/lib/python3.12/site-packages/jinja2/environment.py"", line 939, in handle_exception
    raise rewrite_traceback_stack(source=source)
  File ""<unknown>"", line 1, in template
jinja2.exceptions.TemplateAssertionError: No filter named 'securestring'.
[2024-11-13, 16:11:37 MST] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3114, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context, jinja_env=jinja_env)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3533, in render_templates
    original_task.render_template_fields(context, jinja_env)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 1419, in render_template_fields
    self._do_render_template_fields(self, self.template_fields, context, jinja_env, set())
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/abstractoperator.py"", line 770, in _do_render_template_fields
    rendered_content = self.render_template(
                       ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/template/templater.py"", line 183, in render_template
    return [self.render_template(element, context, jinja_env, oids) for element in value]
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/template/templater.py"", line 170, in render_template
    template = jinja_env.from_string(value)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/jinja2/environment.py"", line 1108, in from_string
    return cls.from_code(self, self.compile(source), gs, None)
                               ^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/jinja2/environment.py"", line 768, in compile
    self.handle_exception(source=source_hint)
  File ""/home/airflow/.local/lib/python3.12/site-packages/jinja2/environment.py"", line 939, in handle_exception
    raise rewrite_traceback_stack(source=source)
  File ""<unknown>"", line 1, in template
jinja2.exceptions.TemplateAssertionError: No filter named 'securestring'.
[2024-11-13, 16:11:37 MST] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=psrpoperator_securestring_error_reproduction, task_id=Write-Output, run_id=manual__2024-11-13T23:11:35.867453+00:00, execution_date=20241113T231135, start_date=20241113T231136, end_date=20241113T231137
```

</summary>

### What you think should happen instead

The minimal example provided should return through XCom the ToString representation of a powershell SecureString which would be `""System.Security.SecureString""`

### How to reproduce

The following DAG can be used to reproduce the issue and does not require a remote connection as the task fails before attempting any connection:

```python
import datetime
from airflow import DAG
from airflow.providers.microsoft.psrp.operators.psrp import PsrpOperator

with DAG(
    dag_id=""psrpoperator_securestring_error_reproduction"",
    start_date=None,
    schedule_interval=None,
    render_template_as_native_obj=True,
):
    PsrpOperator(
        psrp_conn_id = ""SomeConnection"",
        cmdlet = ""Write-Output"", 
        arguments = [ ""{{ 'foo' | securestring }}"" ],
    )
```

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",noderat,2024-11-13 23:33:53+00:00,[],2024-11-16 11:00:01+00:00,2024-11-16 11:00:01+00:00,https://github.com/apache/airflow/issues/43992,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:microsoft-psrp', '')]","[{'comment_id': 2475037681, 'issue_id': 2657123024, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 13, 23, 33, 55, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-13 23:33:55 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2656978668,issue,open,,Deep Dive deadline storage method,"Determine storage method (in the DAG object or in a DB table or ??? )
- Must be able to sort/search by ""need-by"" and remove by ""dagrun_id""
- Must be able to allow alternative remove options for task_id or other options for future work",ferruzzi,2024-11-13 22:24:40+00:00,['ferruzzi'],2024-11-16 04:48:41+00:00,,https://github.com/apache/airflow/issues/43989,"[('AIP-86:Deadlines', '')]","[{'comment_id': 2475037241, 'issue_id': 2656978668, 'author': 'ferruzzi', 'body': 'Current thought:\n\nA new table in the DB which has a 1:1 mapping for identifier:deadline, where identifier can be a dagrun_id, task_id, etc.  and a query which returns the deadlines filtered to only return the ones which have passed', 'created_at': datetime.datetime(2024, 11, 13, 23, 33, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2477472827, 'issue_id': 2656978668, 'author': 'ferruzzi', 'body': ""If we are only storing dagrun_id and task_id in the identifier column then it's easy to infer what type of id it is from the format, but does every possible future identifier type have a name format where we can infer what it is from the pattern?  Presumably not?  So we likely need a `type` column as well just to be safe?"", 'created_at': datetime.datetime(2024, 11, 14, 21, 46, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2478545575, 'issue_id': 2656978668, 'author': '1fanwang', 'body': 'Sharing some of my early thoughts on the points raised here and from our offline discussion \n\n> ""Where should deadlines be stored?""\n\nOverall +1 to have it in a new/its-own table for clear separation of concerns, this way we can also optimize for querying specifically for deadline, also this gives us the flexibility to support `identifier` that\'s mentioned here.\n\nAlso curious on what we imagined the relationship would look like, quick gut check on my understanding of the relationship of dag dagrun task taskinstance\nwould it roughly look something like this?\n```\nDeadlineEntry -> DagRun: Optional reference\nDeadlineEntry -> TaskInstance: Optional reference\nDeadlineEntry -> DAG: Optional reference (via dag_id)\n```\n\n> Must be able to sort/search by ""need-by""\n\nWe can have a dedicated index for `need-by`\n\n> ""Does every possible future identifier type have a name format where we can infer what it is?""\n\nMaybe we can make this safe and explicit by having an `identifier_type` column instead of inferring from format?\n\n> Must be able to remove by ""dagrun_id""\n\nSomething like this? `DELETE FROM deadline_entry WHERE identifier_type = \'dagrun\' AND identifier = \'dagrun_123\'`\n\n> Must be able to allow alternative remove options for future work\n\nAssuming this is saying we might query based on different identifier types\n`DELETE FROM deadline_entry WHERE dag_id = \'dag1\' AND task_id = \'task1\'`\n\n> ""1:1 mapping for identifier:deadline""\n\nWe can enforce unique constraint on (identifier_type, identifier)\n\n\nWDYT', 'created_at': datetime.datetime(2024, 11, 15, 10, 50, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2479743865, 'issue_id': 2656978668, 'author': 'ferruzzi', 'body': 'Yeah, the more I think about this, the more I think I am on the right track.  I don\'t have much experience with db design so I want to bounce it off a couple people to verify before I/we commit to it, but yes, based on your replies I think this is the right approach.\r\n\r\nA new table called deadlines with three columns:   id, id_type, and deadline which has a primary key on id so we can lookup and drop by id, and an index on deadline so we can easily get MIN(deadline).  `dag_processor` will calculate the deadline and add a row when the dagrun is created.  A call in the scheduler loop will query `SELECT MIN(deadline) AS earliest_deadline FROM deadlines;` and compare that to now() to see if any deadline has passed.   If there are any values returned, then call the DeadlinesHandler.\r\n\r\nThe DeadlinesHandler will get all deadlines that have passed, queue their callbacks, and drop them from the table so they are not queued again next pass.  We will need to make sure we can fetch the callback given the id and id_type OR store the callback in the table directly and save that lookup.  I think the callback will already be stored in the dagrun table so we should fetch from there.\r\n\r\nAdd logic to the dagrun cleanup/exit code to the effect of ""if this dagrun has a Deadline then try to drop it from the table"".  We ""try"" because if the deadline passed then it would have been dropped already.\r\n\r\n---\r\n\r\nI don\'t understand this part of the question:\r\n\r\n```\r\nAlso curious on what we imagined the relationship would look like, quick gut check on my understanding of the relationship of dag dagrun task taskinstance\r\nwould it roughly look something like this?\r\n\r\nDeadlineEntry -> DagRun: Optional reference\r\nDeadlineEntry -> TaskInstance: Optional reference\r\nDeadlineEntry -> DAG: Optional reference (via dag_id)\r\n```', 'created_at': datetime.datetime(2024, 11, 15, 19, 9, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2480010961, 'issue_id': 2656978668, 'author': '1fanwang', 'body': ""> I don't understand this part of the question:\r\n\r\n ah sorry I was just thinking out loud on how the new Deadline table would relate to existing Airflow entities (DAG, DagRun, Task, and TaskInstance)\r\n \r\n I think I got the gist of it now after reading through your comment now, thanks!"", 'created_at': datetime.datetime(2024, 11, 15, 22, 3, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2480193055, 'issue_id': 2656978668, 'author': 'dstandish', 'body': ""my advice @ferruzzi is don't spend too much time thinking about the model ahead of time.  I would make your primary focus the functionalities that you need to implement, and just make prs for those one at a time, starting from the simplest.  the models will sort of write themselves in that way.  in process of code review, folks can help shape it along, and spot e.g. potential performance concerns etc.  \r\n\r\nwhen you try to just come up with the end state model in the beginning, it puts too much pressure on it, and inevitably it will have to be changed.  don't feel like you have to get it perfect in the first pr.  as mike tyson famously said, everybody has a plan until they get punched in the face :)"", 'created_at': datetime.datetime(2024, 11, 16, 0, 14, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2480391667, 'issue_id': 2656978668, 'author': 'potiuk', 'body': 'Very much agree. It\'s called ""emergent design"" https://en.wikipedia.org/wiki/Emergent_design', 'created_at': datetime.datetime(2024, 11, 16, 4, 21, 15, tzinfo=datetime.timezone.utc)}]","ferruzzi (Issue Creator) on (2024-11-13 23:33:33 UTC): Current thought:

A new table in the DB which has a 1:1 mapping for identifier:deadline, where identifier can be a dagrun_id, task_id, etc.  and a query which returns the deadlines filtered to only return the ones which have passed

ferruzzi (Issue Creator) on (2024-11-14 21:46:44 UTC): If we are only storing dagrun_id and task_id in the identifier column then it's easy to infer what type of id it is from the format, but does every possible future identifier type have a name format where we can infer what it is from the pattern?  Presumably not?  So we likely need a `type` column as well just to be safe?

1fanwang on (2024-11-15 10:50:01 UTC): Sharing some of my early thoughts on the points raised here and from our offline discussion 


Overall +1 to have it in a new/its-own table for clear separation of concerns, this way we can also optimize for querying specifically for deadline, also this gives us the flexibility to support `identifier` that's mentioned here.

Also curious on what we imagined the relationship would look like, quick gut check on my understanding of the relationship of dag dagrun task taskinstance
would it roughly look something like this?
```
DeadlineEntry -> DagRun: Optional reference
DeadlineEntry -> TaskInstance: Optional reference
DeadlineEntry -> DAG: Optional reference (via dag_id)
```


We can have a dedicated index for `need-by`


Maybe we can make this safe and explicit by having an `identifier_type` column instead of inferring from format?


Something like this? `DELETE FROM deadline_entry WHERE identifier_type = 'dagrun' AND identifier = 'dagrun_123'`


Assuming this is saying we might query based on different identifier types
`DELETE FROM deadline_entry WHERE dag_id = 'dag1' AND task_id = 'task1'`


We can enforce unique constraint on (identifier_type, identifier)


WDYT

ferruzzi (Issue Creator) on (2024-11-15 19:09:41 UTC): Yeah, the more I think about this, the more I think I am on the right track.  I don't have much experience with db design so I want to bounce it off a couple people to verify before I/we commit to it, but yes, based on your replies I think this is the right approach.

A new table called deadlines with three columns:   id, id_type, and deadline which has a primary key on id so we can lookup and drop by id, and an index on deadline so we can easily get MIN(deadline).  `dag_processor` will calculate the deadline and add a row when the dagrun is created.  A call in the scheduler loop will query `SELECT MIN(deadline) AS earliest_deadline FROM deadlines;` and compare that to now() to see if any deadline has passed.   If there are any values returned, then call the DeadlinesHandler.

The DeadlinesHandler will get all deadlines that have passed, queue their callbacks, and drop them from the table so they are not queued again next pass.  We will need to make sure we can fetch the callback given the id and id_type OR store the callback in the table directly and save that lookup.  I think the callback will already be stored in the dagrun table so we should fetch from there.

Add logic to the dagrun cleanup/exit code to the effect of ""if this dagrun has a Deadline then try to drop it from the table"".  We ""try"" because if the deadline passed then it would have been dropped already.

---

I don't understand this part of the question:

```
Also curious on what we imagined the relationship would look like, quick gut check on my understanding of the relationship of dag dagrun task taskinstance
would it roughly look something like this?

DeadlineEntry -> DagRun: Optional reference
DeadlineEntry -> TaskInstance: Optional reference
DeadlineEntry -> DAG: Optional reference (via dag_id)
```

1fanwang on (2024-11-15 22:03:34 UTC): ah sorry I was just thinking out loud on how the new Deadline table would relate to existing Airflow entities (DAG, DagRun, Task, and TaskInstance)
 
 I think I got the gist of it now after reading through your comment now, thanks!

dstandish on (2024-11-16 00:14:53 UTC): my advice @ferruzzi is don't spend too much time thinking about the model ahead of time.  I would make your primary focus the functionalities that you need to implement, and just make prs for those one at a time, starting from the simplest.  the models will sort of write themselves in that way.  in process of code review, folks can help shape it along, and spot e.g. potential performance concerns etc.  

when you try to just come up with the end state model in the beginning, it puts too much pressure on it, and inevitably it will have to be changed.  don't feel like you have to get it perfect in the first pr.  as mike tyson famously said, everybody has a plan until they get punched in the face :)

potiuk on (2024-11-16 04:21:15 UTC): Very much agree. It's called ""emergent design"" https://en.wikipedia.org/wiki/Emergent_design

"
2656398779,issue,closed,completed,`bool(LazySelectSequence)` can return `None`,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

The downstream effect is that 
  - when doing an Xcom pull for a group of task IDs
  - when the returned list is empty
  - an exception is raised because Jinja fails to render the template

```
[2024-11-13, 18:05:16 UTC] {abstractoperator.py:780} ERROR - Exception rendering Jinja template for task '...'
Traceback (most recent call last):
  File ""airflow/models/abstractoperator.py"", line 772, in _do_render_template_fields
    rendered_content = self.render_template(
                       ^^^^^^^^^^^^^^^^^^^^^
  File ""airflow/template/templater.py"", line 171, in render_template
    return self._render(template, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""airflow/models/abstractoperator.py"", line 727, in _render
    return super()._render(template, context, dag=dag)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""airflow/template/templater.py"", line 126, in _render
    return render_template_as_native(template, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""airflow/utils/helpers.py"", line 306, in render_template_as_native
    return render_template(template, cast(MutableMapping[str, Any], context), native=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""airflow/utils/helpers.py"", line 295, in render_template
    return jinja2.nativetypes.native_concat(nodes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""jinja2/nativetypes.py"", line 25, in native_concat
    head = list(islice(values, 2))
           ^^^^^^^^^^^^^^^^^^^^^^^
  File ""<template>"", line 24, in root
  File ""jinja2/async_utils.py"", line 45, in wrapper
    return normal_func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""jinja2/filters.py"", line 1352, in sync_do_list
    return list(value)
           ^^^^^^^^^^^
  File ""jinja2/filters.py"", line 1492, in sync_do_map
    if value:
       ^^^^^
TypeError: __bool__ should return bool, returned NoneType
```

### What you think should happen instead?

In this case an empty list should be returned

The root cause is that `LazySelectSequence`'s `__bool__` method is not airtight. I don't understand the internals well enough to know why this is so, but this is clearly an internal error, given a valid template:
```
""{{  ti.xcom_pull(task_ids=['a', 'b', 'c'], key='return_value')  | map(attribute=0) | list }}""
```

### How to reproduce

  - Use a template like this to pull the return values from a list of tasks: `""{{  ti.xcom_pull(task_ids=['a', 'b', 'c'], key='return_value')  | map(attribute=0) | list }}""`
  - Ensure that all of those tasks `Skip` or otherwise do not return a value

### Operating System

Ubuntu 24.04.1

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

  - Root cause is in #39426
  - Seems related to issue #41983
  - Can confirm we do not have this problem on 2.9.x

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",johncmerfeld,2024-11-13 18:31:25+00:00,['johncmerfeld'],2024-12-03 06:46:52+00:00,2024-12-03 06:46:51+00:00,https://github.com/apache/airflow/issues/43977,"[('kind:bug', 'This is a clearly a bug'), ('area:core', '')]","[{'comment_id': 2474421513, 'issue_id': 2656398779, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 13, 18, 31, 28, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-13 18:31:28 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2656305598,issue,closed,completed,Unable to login because of timezone error,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I am new to Airflow. I have a custom docker that i built. The ui fails to load. The classic ""Something bad has happened. For security reasons detailed information about the error is not"" message shows up. in logs for webserver I see the following error
```
72.17.0.7 - - [13/Nov/2024:11:26:31 +0000] ""POST /login/?next=http%3A%2F%2F192.168.50.197%2Fhome HTTP/1.0"" 302 239 ""http://192.168.50.197/login/?next=http%3A%2F%2F192.168.50.197%2Fhome"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:132.0) Gecko/20100101 Firefox/132.0""

         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/flask/app.py"", line 1823, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/flask/app.py"", line 1799, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/airflow/www/auth.py"", line 139, in decorated
    return _has_access(
           ^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/airflow/www/auth.py"", line 163, in _has_access
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/airflow/www/views.py"", line 1134, in index
    return self.render_template(
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/airflow/www/views.py"", line 768, in render_template
    return super().render_template(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/flask_appbuilder/baseviews.py"", line 342, in render_template
    return render_template(
           ^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/flask/templating.py"", line 147, in render_template
    return _render(app, template, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/flask/templating.py"", line 130, in _render
    rv = template.render(context)
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/jinja2/environment.py"", line 1304, in render
    self.environment.handle_exception()
  File ""/usr/local/lib/python3.12/dist-packages/jinja2/environment.py"", line 939, in handle_exception
    raise rewrite_traceback_stack(source=source)
  File ""/usr/local/lib/python3.12/dist-packages/airflow/www/templates/airflow/dags.html"", line 44, in top-level template code
    {% elif curr_ordering_direction == 'asc' and request.args.get('sorting_key') == attribute_name %}
    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/airflow/www/templates/airflow/main.html"", line 21, in top-level template code
    {% from 'airflow/_messages.html' import show_message %}
    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/flask_appbuilder/templates/appbuilder/baselayout.html"", line 2, in top-level template code
    {% import 'appbuilder/baselib.html' as baselib %}
    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/flask_appbuilder/templates/appbuilder/init.html"", line 42, in top-level template code
    {% block body %}
  File ""/usr/local/lib/python3.12/dist-packages/flask_appbuilder/templates/appbuilder/baselayout.html"", line 16, in block 'body'
    {% block messages %}
  File ""/usr/local/lib/python3.12/dist-packages/airflow/www/templates/airflow/dags.html"", line 108, in block 'messages'
    {{ super() }}
  File ""/usr/local/lib/python3.12/dist-packages/airflow/www/templates/airflow/main.html"", line 69, in block 'messages'
    {% call show_message(category='warning', dismissible=false) %}
    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/jinja2/runtime.py"", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/airflow/www/templates/airflow/_messages.html"", line 25, in template
    {{ caller() }}
  File ""/usr/local/lib/python3.12/dist-packages/jinja2/runtime.py"", line 782, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/airflow/www/templates/airflow/main.html"", line 77, in template
    >{{ macros.datetime_diff_for_humans(scheduler_job.latest_heartbeat) }}</time>.
    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/airflow/macros/__init__.py"", line 124, in datetime_diff_for_humans
    return pendulum.instance(dt).diff_for_humans(since)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/pendulum/datetime.py"", line 744, in diff_for_humans
    other = self.now()
            ^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/pendulum/datetime.py"", line 165, in now
    dt = datetime.datetime.now(local_timezone())
                               ^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/pendulum/tz/__init__.py"", line 51, in local_timezone
    return get_local_timezone()
           ^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/pendulum/tz/local_timezone.py"", line 33, in get_local_timezone
    tz = _get_system_timezone()
         ^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/pendulum/tz/local_timezone.py"", line 61, in _get_system_timezone
    return _get_unix_timezone()
           ^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/pendulum/tz/local_timezone.py"", line 179, in _get_unix_timezone
    return Timezone(etctz.replace("" "", ""_""))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/dist-packages/pendulum/tz/timezone.py"", line 65, in __new__
    return super().__new__(cls, key)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib/python3.12/zoneinfo/_tzpath.py"", line 73, in find_tzfile
    _validate_tzfile_path(key)
  File ""/usr/lib/python3.12/zoneinfo/_tzpath.py"", line 87, in _validate_tzfile_path
    raise ValueError(
ValueError: ZoneInfo keys may not be absolute paths, got: /UTC
172.17.0.7 - - [13/Nov/2024:11:26:31 +0000] ""GET /home HTTP/1.0"" 500 1588 ""http://192.168.50.197/login/?next=http%3A%2F%2F192.168.50.197%2Fhome"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:132.0) Gecko/20100101 Firefox/132.0""
172.17.0.7 - - [13/Nov/2024:11:26:32 +0000] ""GET /favicon.ico HTTP/1.0"" 404 456 ""http://192.168.50.197/home"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:132.0) Gecko/20100101 Firefox/132.0""
```

### What you think should happen instead?

I have changed `default_timezone = America/New_York` but this does not help.

### How to reproduce

Python version: 3.12.7
Airflow version: 2.10.3


### Operating System

sudo docker exec -it orch-0 bash [sudo] password for kadmin: Sorry, try again. [sudo] password for kadmin: root@4aa1bcaa1a07:/gu/data# cat /etc/os-release PRETTY_NAME=""Ubuntu 24.10"" NAME=""Ubuntu"" VERSION_ID=""24.10"" VERSION=""24.10 (Oracular Oriole)"" VERSION_CODENAME=oracular ID=ubuntu ID_LIKE=debian HOME_URL=""https://www.ubuntu.com/"" SUPPORT_URL=""https://help.ubuntu.com/"" BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/"" PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"" UBUNTU_CODENAME=oracular LOGO=ubuntu-logo

### Versions of Apache Airflow Providers

# pip freeze |grep apache-airflow-providers
apache-airflow-providers-apache-kafka==1.6.1
apache-airflow-providers-common-compat==1.2.1
apache-airflow-providers-common-io==1.4.2
apache-airflow-providers-common-sql==1.19.0
apache-airflow-providers-fab==1.5.0
apache-airflow-providers-ftp==3.11.1
apache-airflow-providers-http==4.13.1
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-postgres==5.13.0
apache-airflow-providers-smtp==1.8.0
apache-airflow-providers-sqlite==3.9.0
apache-airflow-providers-ssh==3.13.1

### Deployment

Other

### Deployment details

$ docker version
Client:
 Version:           27.3.1
 API version:       1.47
 Go version:        go1.23.1
 Git commit:        2.fc41
 Built:             Tue Sep 24 00:00:00 2024
 OS/Arch:           linux/amd64
 Context:           default
HOST OS Version (Fedora):
$ cat /etc/os-release
NAME=""Fedora Linux""
VERSION=""41 (Forty One)""
RELEASE_TYPE=stable
ID=fedora
VERSION_ID=41
VERSION_CODENAME=""""
PLATFORM_ID=""platform:f41""
PRETTY_NAME=""Fedora Linux 41 (Forty One)""
ANSI_COLOR=""0;38;2;60;110;180""
LOGO=fedora-logo-icon
CPE_NAME=""cpe:/o:fedoraproject:fedora:41""
DEFAULT_HOSTNAME=""fedora""
HOME_URL=""https://fedoraproject.org/""
DOCUMENTATION_URL=""https://docs.fedoraproject.org/en-US/fedora/f41/system-administrators-guide/""
SUPPORT_URL=""https://ask.fedoraproject.org/""
BUG_REPORT_URL=""https://bugzilla.redhat.com/""
REDHAT_BUGZILLA_PRODUCT=""Fedora""
REDHAT_BUGZILLA_PRODUCT_VERSION=41
REDHAT_SUPPORT_PRODUCT=""Fedora""
REDHAT_SUPPORT_PRODUCT_VERSION=41
SUPPORT_END=2025-05-13

### Anything else?

The issue is reproducible every time. I tried changing to older release of airflow, but it persists


### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",sandeepkalra,2024-11-13 17:46:19+00:00,[],2024-11-27 02:09:11+00:00,2024-11-27 02:09:11+00:00,https://github.com/apache/airflow/issues/43973,"[('kind:bug', 'This is a clearly a bug'), ('area:webserver', 'Webserver related Issues'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2474324609, 'issue_id': 2656305598, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 13, 17, 46, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499810521, 'issue_id': 2656305598, 'author': 'martintalero', 'body': 'Same issue here. \r\n\r\n____________       _____________\r\n ____    |__( )_________  __/__  /________      __\r\n____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /\r\n___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /\r\n _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/\r\nHTTP Request: GET https://apacheairflow.gateway.scarf.sh/scheduler?version=2.10.2&python_version=3.11&platform=Linux&arch=x86_64&database=postgresql&db_version=14.4&executor=CeleryExecutor ""HTTP/1.1 200 OK""\r\n/opt/airflow/venv/lib/python3.11/site-packages/airflow/utils/module_loading.py:42 DeprecationWarning: The `airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG` class is deprecated. Please use `\'airflow.providers.celery.executors.default_celery.DEFAULT_CELERY_CONFIG\'`. The `celery` provider must be >= 3.3.0 for that..\r\n/opt/airflow/venv/lib/python3.11/site-packages/airflow/utils/providers_configuration_loader.py:55 AirflowProviderDeprecationWarning: The celery.CELERY_APP_NAME configuration uses deprecated package name: \'airflow.executors.celery_executor\'. Change it to `airflow.providers.celery.executors.celery_executor`, and update the `-app` flag in your Celery Health Checks to use `airflow.providers.celery.executors.celery_executor.app`.\r\nLoaded executor: CeleryExecutor\r\nStarting the scheduler\r\nProcessing each file at most -1 times\r\nLaunched DagFileProcessorManager with pid: 31\r\nAdopting or resetting orphaned tasks for active dag runs\r\nConfigured default timezone UTC\r\n--- Logging error ---\r\nTraceback (most recent call last):\r\n  File ""/usr/lib/python3.11/logging/handlers.py"", line 73, in emit\r\n    if self.shouldRollover(record):\r\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/lib/python3.11/logging/handlers.py"", line 196, in shouldRollover\r\n    msg = ""%s\\n"" % self.format(record)\r\n                   ^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/lib/python3.11/logging/__init__.py"", line 953, in format\r\n    return fmt.format(record)\r\n           ^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/lib/python3.11/logging/__init__.py"", line 689, in format\r\n    record.asctime = self.formatTime(record, self.datefmt)\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/utils/log/timezone_aware.py"", line 44, in formatTime\r\n    dt = timezone.from_timestamp(record.created, tz=""local"")\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/utils/timezone.py"", line 319, in from_timestamp\r\n    tz = local_timezone()\r\n         ^^^^^^^^^^^^^^^^\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/utils/timezone.py"", line 301, in local_timezone\r\n    return pendulum.tz.local_timezone()\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/pendulum/tz/__init__.py"", line 51, in local_timezone\r\n    return get_local_timezone()\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py"", line 33, in get_local_timezone\r\n    tz = _get_system_timezone()\r\n         ^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py"", line 61, in _get_system_timezone\r\n    return _get_unix_timezone()\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py"", line 179, in _get_unix_timezone\r\n    return Timezone(etctz.replace("" "", ""_""))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/pendulum/tz/timezone.py"", line 65, in __new__\r\n    return super().__new__(cls, key)  # type: ignore[call-arg]\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/lib/python3.11/zoneinfo/_tzpath.py"", line 67, in find_tzfile\r\n    _validate_tzfile_path(key)\r\n  File ""/usr/lib/python3.11/zoneinfo/_tzpath.py"", line 81, in _validate_tzfile_path\r\n    raise ValueError(\r\nValueError: ZoneInfo keys may not be absolute paths, got: /UTC\r\nCall stack:\r\n  File ""/opt/airflow/venv/bin/airflow"", line 8, in <module>\r\n    sys.exit(main())\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/__main__.py"", line 62, in main\r\n    args.func(args)\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/cli/cli_config.py"", line 49, in command\r\n    return func(*args, **kwargs)\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/utils/cli.py"", line 115, in wrapper\r\n    return f(*args, **kwargs)\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/utils/providers_configuration_loader.py"", line 55, in wrapped_function\r\n    return func(*args, **kwargs)\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/cli/commands/scheduler_command.py"", line 59, in scheduler\r\n    run_command_with_daemon_option(\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/cli/commands/daemon_utils.py"", line 86, in run_command_with_daemon_option\r\n    callback()\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/cli/commands/scheduler_command.py"", line 62, in <lambda>\r\n    callback=lambda: _run_scheduler_job(args),\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/cli/commands/scheduler_command.py"", line 48, in _run_scheduler_job\r\n    run_job(job=job_runner.job, execute_callable=job_runner._execute)\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/utils/session.py"", line 97, in wrapper\r\n    return func(*args, session=session, **kwargs)\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/jobs/job.py"", line 421, in run_job\r\n    return execute_job(job, execute_callable=execute_callable)\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/jobs/job.py"", line 450, in execute_job\r\n    ret = execute_callable()\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py"", line 980, in _execute\r\n    self.processor_agent.start()\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/dag_processing/manager.py"", line 172, in start\r\n    process.start()\r\n  File ""/usr/lib/python3.11/multiprocessing/process.py"", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File ""/usr/lib/python3.11/multiprocessing/context.py"", line 281, in _Popen\r\n    return Popen(process_obj)\r\n  File ""/usr/lib/python3.11/multiprocessing/popen_fork.py"", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File ""/usr/lib/python3.11/multiprocessing/popen_fork.py"", line 71, in _launch\r\n    code = process_obj._bootstrap(parent_sentinel=child_r)\r\n  File ""/usr/lib/python3.11/multiprocessing/process.py"", line 314, in _bootstrap\r\n    self.run()\r\n  File ""/usr/lib/python3.11/multiprocessing/process.py"", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/dag_processing/manager.py"", line 247, in _run_processor_manager\r\n    processor_manager.start()\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/dag_processing/manager.py"", line 483, in start\r\n    self.log.info(""Processing files using up to %s processes at a time "", self._parallelism)\r\nMessage: \'Processing files using up to %s processes at a time \'\r\nArguments: (2,)', 'created_at': datetime.datetime(2024, 11, 26, 6, 59, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2501949990, 'issue_id': 2656305598, 'author': 'martintalero', 'body': 'The issue mentioned above happens with airflow version 2.10.3 and 2.10.2. The error message comes from the scheduler service. The image is a custom one as well.', 'created_at': datetime.datetime(2024, 11, 26, 21, 16, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2502066387, 'issue_id': 2656305598, 'author': 'martintalero', 'body': 'Problem is in file: airflow/utils/timezone.py\r\n\r\n$ python\r\nPython 3.11.10 (main, Sep  7 2024, 18:35:41) [GCC 13.2.0] on linux\r\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\r\n>>> from airflow.utils import timezone\r\n>>> import pendulum\r\n>>> s=timezone\r\n>>> s.local_timezone()\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/utils/timezone.py"", line 301, in local_timezone\r\n    return pendulum.tz.local_timezone()\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/pendulum/tz/__init__.py"", line 51, in local_timezone\r\n    return get_local_timezone()\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py"", line 33, in get_local_timezone\r\n    tz = _get_system_timezone()\r\n         ^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py"", line 61, in _get_system_timezone\r\n    return _get_unix_timezone()\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py"", line 179, in _get_unix_timezone\r\n    return Timezone(etctz.replace("" "", ""_""))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/airflow/venv/lib/python3.11/site-packages/pendulum/tz/timezone.py"", line 65, in __new__\r\n    return super().__new__(cls, key)  # type: ignore[call-arg]\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/lib/python3.11/zoneinfo/_tzpath.py"", line 67, in find_tzfile\r\n    _validate_tzfile_path(key)\r\n  File ""/usr/lib/python3.11/zoneinfo/_tzpath.py"", line 81, in _validate_tzfile_path\r\n    raise ValueError(\r\nValueError: ZoneInfo keys may not be absolute paths, got: /UTC', 'created_at': datetime.datetime(2024, 11, 26, 22, 31, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2502506026, 'issue_id': 2656305598, 'author': 'potiuk', 'body': 'It looks like in some cases your docker container might be build with double `/` in `/etc/localtime` symbolic link. Likely because some timezone information is missing or wrongly configured and Pendulum does not work well with it.\r\n\r\n https://github.com/python-babel/babel/issues/990#issuecomment-1762142879 contains a workaround in babel for similar issue. Applying it to your dockerfiles should fix the issue for you.l', 'created_at': datetime.datetime(2024, 11, 27, 2, 9, 1, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-13 17:46:23 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

martintalero on (2024-11-26 06:59:08 UTC): Same issue here. 

____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
HTTP Request: GET https://apacheairflow.gateway.scarf.sh/scheduler?version=2.10.2&python_version=3.11&platform=Linux&arch=x86_64&database=postgresql&db_version=14.4&executor=CeleryExecutor ""HTTP/1.1 200 OK""
/opt/airflow/venv/lib/python3.11/site-packages/airflow/utils/module_loading.py:42 DeprecationWarning: The `airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG` class is deprecated. Please use `'airflow.providers.celery.executors.default_celery.DEFAULT_CELERY_CONFIG'`. The `celery` provider must be >= 3.3.0 for that..
/opt/airflow/venv/lib/python3.11/site-packages/airflow/utils/providers_configuration_loader.py:55 AirflowProviderDeprecationWarning: The celery.CELERY_APP_NAME configuration uses deprecated package name: 'airflow.executors.celery_executor'. Change it to `airflow.providers.celery.executors.celery_executor`, and update the `-app` flag in your Celery Health Checks to use `airflow.providers.celery.executors.celery_executor.app`.
Loaded executor: CeleryExecutor
Starting the scheduler
Processing each file at most -1 times
Launched DagFileProcessorManager with pid: 31
Adopting or resetting orphaned tasks for active dag runs
Configured default timezone UTC
--- Logging error ---
Traceback (most recent call last):
  File ""/usr/lib/python3.11/logging/handlers.py"", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib/python3.11/logging/handlers.py"", line 196, in shouldRollover
    msg = ""%s\n"" % self.format(record)
                   ^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib/python3.11/logging/__init__.py"", line 953, in format
    return fmt.format(record)
           ^^^^^^^^^^^^^^^^^^
  File ""/usr/lib/python3.11/logging/__init__.py"", line 689, in format
    record.asctime = self.formatTime(record, self.datefmt)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/utils/log/timezone_aware.py"", line 44, in formatTime
    dt = timezone.from_timestamp(record.created, tz=""local"")
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/utils/timezone.py"", line 319, in from_timestamp
    tz = local_timezone()
         ^^^^^^^^^^^^^^^^
  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/utils/timezone.py"", line 301, in local_timezone
    return pendulum.tz.local_timezone()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/venv/lib/python3.11/site-packages/pendulum/tz/__init__.py"", line 51, in local_timezone
    return get_local_timezone()
           ^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py"", line 33, in get_local_timezone
    tz = _get_system_timezone()
         ^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py"", line 61, in _get_system_timezone
    return _get_unix_timezone()
           ^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py"", line 179, in _get_unix_timezone
    return Timezone(etctz.replace("" "", ""_""))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/venv/lib/python3.11/site-packages/pendulum/tz/timezone.py"", line 65, in __new__
    return super().__new__(cls, key)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib/python3.11/zoneinfo/_tzpath.py"", line 67, in find_tzfile
    _validate_tzfile_path(key)
  File ""/usr/lib/python3.11/zoneinfo/_tzpath.py"", line 81, in _validate_tzfile_path
    raise ValueError(
ValueError: ZoneInfo keys may not be absolute paths, got: /UTC
Call stack:
  File ""/opt/airflow/venv/bin/airflow"", line 8, in <module>
    sys.exit(main())
  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/__main__.py"", line 62, in main
    args.func(args)
  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/cli/cli_config.py"", line 49, in command
    return func(*args, **kwargs)
  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/utils/cli.py"", line 115, in wrapper
    return f(*args, **kwargs)
  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/utils/providers_configuration_loader.py"", line 55, in wrapped_function
    return func(*args, **kwargs)
  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/cli/commands/scheduler_command.py"", line 59, in scheduler
    run_command_with_daemon_option(
  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/cli/commands/daemon_utils.py"", line 86, in run_command_with_daemon_option
    callback()
  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/cli/commands/scheduler_command.py"", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/cli/commands/scheduler_command.py"", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/jobs/job.py"", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/jobs/job.py"", line 450, in execute_job
    ret = execute_callable()
  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py"", line 980, in _execute
    self.processor_agent.start()
  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/dag_processing/manager.py"", line 172, in start
    process.start()
  File ""/usr/lib/python3.11/multiprocessing/process.py"", line 121, in start
    self._popen = self._Popen(self)
  File ""/usr/lib/python3.11/multiprocessing/context.py"", line 281, in _Popen
    return Popen(process_obj)
  File ""/usr/lib/python3.11/multiprocessing/popen_fork.py"", line 19, in __init__
    self._launch(process_obj)
  File ""/usr/lib/python3.11/multiprocessing/popen_fork.py"", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File ""/usr/lib/python3.11/multiprocessing/process.py"", line 314, in _bootstrap
    self.run()
  File ""/usr/lib/python3.11/multiprocessing/process.py"", line 108, in run
    self._target(*self._args, **self._kwargs)
  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/dag_processing/manager.py"", line 247, in _run_processor_manager
    processor_manager.start()
  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/dag_processing/manager.py"", line 483, in start
    self.log.info(""Processing files using up to %s processes at a time "", self._parallelism)
Message: 'Processing files using up to %s processes at a time '
Arguments: (2,)

martintalero on (2024-11-26 21:16:13 UTC): The issue mentioned above happens with airflow version 2.10.3 and 2.10.2. The error message comes from the scheduler service. The image is a custom one as well.

martintalero on (2024-11-26 22:31:15 UTC): Problem is in file: airflow/utils/timezone.py

$ python
Python 3.11.10 (main, Sep  7 2024, 18:35:41) [GCC 13.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/airflow/venv/lib/python3.11/site-packages/airflow/utils/timezone.py"", line 301, in local_timezone
    return pendulum.tz.local_timezone()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/venv/lib/python3.11/site-packages/pendulum/tz/__init__.py"", line 51, in local_timezone
    return get_local_timezone()
           ^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py"", line 33, in get_local_timezone
    tz = _get_system_timezone()
         ^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py"", line 61, in _get_system_timezone
    return _get_unix_timezone()
           ^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py"", line 179, in _get_unix_timezone
    return Timezone(etctz.replace("" "", ""_""))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/venv/lib/python3.11/site-packages/pendulum/tz/timezone.py"", line 65, in __new__
    return super().__new__(cls, key)  # type: ignore[call-arg]
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib/python3.11/zoneinfo/_tzpath.py"", line 67, in find_tzfile
    _validate_tzfile_path(key)
  File ""/usr/lib/python3.11/zoneinfo/_tzpath.py"", line 81, in _validate_tzfile_path
    raise ValueError(
ValueError: ZoneInfo keys may not be absolute paths, got: /UTC

potiuk on (2024-11-27 02:09:01 UTC): It looks like in some cases your docker container might be build with double `/` in `/etc/localtime` symbolic link. Likely because some timezone information is missing or wrongly configured and Pendulum does not work well with it.

 https://github.com/python-babel/babel/issues/990#issuecomment-1762142879 contains a workaround in babel for similar issue. Applying it to your dockerfiles should fix the issue for you.l

"
2656141153,issue,closed,completed,AIP-84 Improve backfill endpoints,"After going through the [designs for backfills](https://github.com/apache/airflow/issues/42371#issuecomment-2474061771). It looks like we're missing a few things in the REST API.

1. Ability to fetch the active backfill. Right now there is only a max of one backfill per dag, but this is probably best as a filter on the list backfills endpoint
2. Add `dry_run` option to create a dag run which will return details on dag runs which will be created or updated with the backfill",bbovenzi,2024-11-13 16:50:19+00:00,['dstandish'],2025-01-22 04:08:12+00:00,2025-01-22 04:08:11+00:00,https://github.com/apache/airflow/issues/43970,"[('area:backfill', 'Specifically for backfill related')]","[{'comment_id': 2606245436, 'issue_id': 2656141153, 'author': 'phanikumv', 'body': 'Since both PRs are merged, closing this issue. Please reopen if required', 'created_at': datetime.datetime(2025, 1, 22, 4, 8, 11, tzinfo=datetime.timezone.utc)}]","phanikumv on (2025-01-22 04:08:11 UTC): Since both PRs are merged, closing this issue. Please reopen if required

"
2656054691,issue,open,,AIP-38 | Create backfill,"Add a '...' options button to the Dag details, dag card, and dag table row. 
One option should be ""Run backfill""
Clicking on it, should open a modal to configure the date range and options for creating a new backfill.
As the user changes options, we should call a dry run create backfill endpoint to get a list of dag runs to be updated or created (this endpoint doesn't exist yet)

![Image](https://github.com/user-attachments/assets/1d239b75-a748-4604-90ae-195f8d2570c3)
![Image](https://github.com/user-attachments/assets/8c99c5d5-a0be-489f-8745-bdba6eb9cc1b)
",bbovenzi,2024-11-13 16:28:38+00:00,['aritra24'],2025-02-04 16:30:54+00:00,,https://github.com/apache/airflow/issues/43969,"[('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application'), ('area:backfill', 'Specifically for backfill related')]","[{'comment_id': 2626236841, 'issue_id': 2656054691, 'author': 'aritra24', 'body': '@bbovenzi wanted to try my hand at this. Would the scope of this be to create the api end point as well? Or just the ui part?', 'created_at': datetime.datetime(2025, 1, 31, 3, 46, 33, tzinfo=datetime.timezone.utc)}]","aritra24 (Assginee) on (2025-01-31 03:46:33 UTC): @bbovenzi wanted to try my hand at this. Would the scope of this be to create the api end point as well? Or just the ui part?

"
2656034135,issue,open,,AIP-38 | Active backfills banner,"- Use public rest api to fetch all backfills and see if any are active. (we should update the API to only return the active backfill)
- Add a banner component to summarize the active backfill

![Image](https://github.com/user-attachments/assets/b3e09b65-6eec-4ec6-b5fb-6953525b71f9)
![Image](https://github.com/user-attachments/assets/954d8ff8-b8c9-4669-b8df-7818d9693245)
",bbovenzi,2024-11-13 16:19:48+00:00,[],2024-11-13 16:28:01+00:00,,https://github.com/apache/airflow/issues/43968,"[('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application'), ('area:backfill', 'Specifically for backfill related')]",[],
2656027509,issue,open,,AIP-38 | List Backfills for a Dag,"Add a ""Backfills"" tab to Dag Details page
Use the get backfills public REST API endpoint to fetch and render a table of backfills pagination and sort included.

![Image](https://github.com/user-attachments/assets/926b00c6-9a16-4399-a67a-79a8c036398b)
",bbovenzi,2024-11-13 16:17:04+00:00,[],2024-11-13 16:28:22+00:00,,https://github.com/apache/airflow/issues/43967,"[('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application'), ('area:backfill', 'Specifically for backfill related')]",[],
2655289994,issue,closed,not_planned,Add ability to disable SSL in Snowflake provider,"### Description

Currently snowflake provider uses `requests` library in non-deferrable mode and `aiohttp` library in deferrable mode to make requests to snowflake URL.  It does not have the config to disable SSL

### Use case/motivation

Ability to disable ssl verification can help when connecting to Snowflake hosted via AWS private link. Often, in deferrable mode it causes following failure `certificate verify failed: Hostname mismatch, certificate is not valid`. While this works in non-deferrable mode. 

### Related issues

NA

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rawwar,2024-11-13 12:14:30+00:00,['rawwar'],2024-11-14 01:32:57+00:00,2024-11-14 01:32:57+00:00,https://github.com/apache/airflow/issues/43963,"[('area:providers', ''), ('kind:feature', 'Feature Requests'), ('provider:snowflake', 'Issues related to Snowflake provider'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2473484283, 'issue_id': 2655289994, 'author': 'potiuk', 'body': 'It\'s a bit problematic, I would very much prefer to have an easy way to configure private certificates for it. I think accessing Snowflake without SSL is REALLY bad idea - especially that almost by definition it is ""somewhere else"" (Yeah I understand it can be in a private network, but it is not really super secure either in this case). I\'d only limit no-SSL connections to internal communication on the same node or proxy -> webapp where their network is truly internal and not accessible by any ""human"" in the company (which I hardly imagine possible when you communicate with Snowflake). \r\n\r\nIt might sound excessive but it had already costed Snowflake a lot of reputation damage when their MFA was not enforced earlier this year: https://www.informationweek.com/cyber-resilience/snowflake-s-lack-of-mfa-control-leaves-companies-vulnerable-experts-say - and we can safely assume that ""disabling security"" should not be easy.\r\n\r\nMaybe - if you attempt to do it, you should make sure there is big FAT warning generated when any such non-secured connection is attempted - like ""THIS IS NOT SECURE, MAKE SURE THAT YOU ARE NOT CONNECTING TO PRODUCTION INSTANCE"" or smth.', 'created_at': datetime.datetime(2024, 11, 13, 12, 32, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2473565425, 'issue_id': 2655289994, 'author': 'rawwar', 'body': ""@potiuk , that totally makes sense. I'll probably leave this open until someone else requests this. If more people request this, I'll go ahead and implement this."", 'created_at': datetime.datetime(2024, 11, 13, 13, 4, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2474176200, 'issue_id': 2655289994, 'author': 'eladkal', 'body': 'This is not unique to Snowflake... other providers connection can have similar request.\r\n\r\nMy prespective is: No insecure features unless the provider owner is asking for it and involved in the provider maintenance. Specificly but not limited to security concerns/questions/feedback.', 'created_at': datetime.datetime(2024, 11, 13, 16, 48, 37, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-11-13 12:32:45 UTC): It's a bit problematic, I would very much prefer to have an easy way to configure private certificates for it. I think accessing Snowflake without SSL is REALLY bad idea - especially that almost by definition it is ""somewhere else"" (Yeah I understand it can be in a private network, but it is not really super secure either in this case). I'd only limit no-SSL connections to internal communication on the same node or proxy -> webapp where their network is truly internal and not accessible by any ""human"" in the company (which I hardly imagine possible when you communicate with Snowflake). 

It might sound excessive but it had already costed Snowflake a lot of reputation damage when their MFA was not enforced earlier this year: https://www.informationweek.com/cyber-resilience/snowflake-s-lack-of-mfa-control-leaves-companies-vulnerable-experts-say - and we can safely assume that ""disabling security"" should not be easy.

Maybe - if you attempt to do it, you should make sure there is big FAT warning generated when any such non-secured connection is attempted - like ""THIS IS NOT SECURE, MAKE SURE THAT YOU ARE NOT CONNECTING TO PRODUCTION INSTANCE"" or smth.

rawwar (Issue Creator) on (2024-11-13 13:04:45 UTC): @potiuk , that totally makes sense. I'll probably leave this open until someone else requests this. If more people request this, I'll go ahead and implement this.

eladkal on (2024-11-13 16:48:37 UTC): This is not unique to Snowflake... other providers connection can have similar request.

My prespective is: No insecure features unless the provider owner is asking for it and involved in the provider maintenance. Specificly but not limited to security concerns/questions/feedback.

"
2655110819,issue,closed,completed,"Newly added attributes ""group"" and ""name"" are not correctly serialized and not returned in Asset API","### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

as title

### What you think should happen instead?

Every Asset related API should consist all the attribute supported by Asset

### How to reproduce

Get an asset from an asset RestAPI

### Operating System

macOS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

breeze

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-11-13 11:07:50+00:00,['Lee-W'],2024-12-02 10:34:43+00:00,2024-12-02 10:34:43+00:00,https://github.com/apache/airflow/issues/43958,"[('kind:bug', 'This is a clearly a bug'), ('area:API', ""Airflow's REST/HTTP API""), ('area:core', ''), ('area:datasets', 'Issues related to the datasets feature'), ('AIP-74', 'Dataset -> Asset')]",[],
2655100604,issue,open,,Support grouping Asset by group attribute,"### Description

<img width=""688"" alt=""image"" src=""https://github.com/user-attachments/assets/bd1c052b-101c-4782-afeb-6f4a4b909411"">


### Use case/motivation

https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-74+Introducing+Data+Assets

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-11-13 11:04:15+00:00,['Lee-W'],2024-12-20 12:23:56+00:00,,https://github.com/apache/airflow/issues/43957,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('area:datasets', 'Issues related to the datasets feature'), ('AIP-74', 'Dataset -> Asset')]","[{'comment_id': 2473220113, 'issue_id': 2655100604, 'author': 'Lee-W', 'body': 'This will be picked up once most of the backend work of AIP-75 is done', 'created_at': datetime.datetime(2024, 11, 13, 11, 4, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556906135, 'issue_id': 2655100604, 'author': 'Lee-W', 'body': ""Hi @bbovenzi  @pierrejeambrun , we've wrapped up most of the APIs needed. What would be the best way for us to push the UI part forward? Thanks!"", 'created_at': datetime.datetime(2024, 12, 20, 12, 23, 55, tzinfo=datetime.timezone.utc)}]","Lee-W (Issue Creator) on (2024-11-13 11:04:44 UTC): This will be picked up once most of the backend work of AIP-75 is done

Lee-W (Issue Creator) on (2024-12-20 12:23:55 UTC): Hi @bbovenzi  @pierrejeambrun , we've wrapped up most of the APIs needed. What would be the best way for us to push the UI part forward? Thanks!

"
2655082459,issue,closed,completed,Deprecate accessing outlet and inlet events through string in Airflow 3.0,"### Description

as title

### Use case/motivation

Since the introduction of DatasetAlias/AssetAlias and the name attribute to Asset, accessing outlet_events and inlet_evnets has become confusing.

e.g., What should something like the following mean?

```python
   def produce_asset_events_through_asset_alias_with_no_taskflow(*, outlet_events=None):
        bucket_name = ""bucket""
        object_path = ""my-task""
        outlet_events[""example-alias-no-taskflow""]
```

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-11-13 10:58:33+00:00,['Lee-W'],2024-12-10 13:39:37+00:00,2024-12-10 13:39:37+00:00,https://github.com/apache/airflow/issues/43956,"[('kind:feature', 'Feature Requests'), ('area:datasets', 'Issues related to the datasets feature'), ('AIP-74', 'Dataset -> Asset')]","[{'comment_id': 2473200584, 'issue_id': 2655082459, 'author': 'Lee-W', 'body': '* PR for adding deprecation warning to v2-10-test https://github.com/apache/airflow/pull/43922 (in review)\n* PR for removing it from Airflow 3 https://github.com/apache/airflow/pull/43959 (WIP)', 'created_at': datetime.datetime(2024, 11, 13, 10, 59, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2511443068, 'issue_id': 2655082459, 'author': 'uranusjr', 'body': 'I’ve decided to hold this for a now. This needs to be done in the 2.x branch, and according to discussions in https://github.com/apache/airflow/pull/44283#discussion_r1854909127, we should hold merging this kind of PRs until 2.10 is finalised. There’s not much point for us to do this right now since the PR won’t be able to be merged immediately. Let’s keep this in the back log and come back when we can actually merge the PR.', 'created_at': datetime.datetime(2024, 12, 2, 12, 43, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2511522887, 'issue_id': 2655082459, 'author': 'Lee-W', 'body': ""Got it. I'll push my uncommitted local work and then stop working on this one"", 'created_at': datetime.datetime(2024, 12, 2, 13, 18, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2531666613, 'issue_id': 2655082459, 'author': 'Lee-W', 'body': 'I think all the deprecation warnings have been merged. Close this issue', 'created_at': datetime.datetime(2024, 12, 10, 13, 39, 37, tzinfo=datetime.timezone.utc)}]","Lee-W (Issue Creator) on (2024-11-13 10:59:18 UTC): * PR for adding deprecation warning to v2-10-test https://github.com/apache/airflow/pull/43922 (in review)
* PR for removing it from Airflow 3 https://github.com/apache/airflow/pull/43959 (WIP)

uranusjr on (2024-12-02 12:43:37 UTC): I’ve decided to hold this for a now. This needs to be done in the 2.x branch, and according to discussions in https://github.com/apache/airflow/pull/44283#discussion_r1854909127, we should hold merging this kind of PRs until 2.10 is finalised. There’s not much point for us to do this right now since the PR won’t be able to be merged immediately. Let’s keep this in the back log and come back when we can actually merge the PR.

Lee-W (Issue Creator) on (2024-12-02 13:18:15 UTC): Got it. I'll push my uncommitted local work and then stop working on this one

Lee-W (Issue Creator) on (2024-12-10 13:39:37 UTC): I think all the deprecation warnings have been merged. Close this issue

"
2654013668,issue,closed,not_planned,Wrong timeout value for ExternalTaskSensor running in deferrable mode,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

The WorkflowTrigger used by ExternalTaskSensor should have a time limit set from `timeout` attribute instead of `execution_timeout`

https://github.com/apache/airflow/blob/bb234dc316f8421760255d85a37a10fd508de14f/airflow/sensors/external_task.py#L349


### What you think should happen instead?

_No response_

### How to reproduce

1. Start an ExternalTaskSensor in deferrable mode to monitor a non-running DAG, using default arguments.
2. The Sensor never timeout because the monitored DAGs do not update, and the timeout value is wrongly set from `execution_timeout` instead of `timeout`

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Google Cloud Composer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",kien-truong,2024-11-13 03:28:37+00:00,['venkateshwaracholan'],2025-02-09 00:16:34+00:00,2025-02-09 00:16:33+00:00,https://github.com/apache/airflow/issues/43948,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('pending-response', ''), ('area:core', ''), ('area:async-operators', 'AIP-40: Deferrable (""Async"") Operators'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2472296052, 'issue_id': 2654013668, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 13, 3, 28, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2472308025, 'issue_id': 2654013668, 'author': 'kien-truong', 'body': 'I can only contribute at weekend, so feel free to pick this up if anyone feels like it.', 'created_at': datetime.datetime(2024, 11, 13, 3, 41, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2479824699, 'issue_id': 2654013668, 'author': 'venkateshwaracholan', 'body': '@kien-truong I would like to work on this.', 'created_at': datetime.datetime(2024, 11, 15, 19, 59, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2480387047, 'issue_id': 2654013668, 'author': 'potiuk', 'body': 'Assigned you', 'created_at': datetime.datetime(2024, 11, 16, 4, 11, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507302897, 'issue_id': 2654013668, 'author': 'karakanb', 'body': 'I looked into this a bit but it seems like there\'s a fundamental issue here, I\'ll try to explain below.\r\n\r\n\r\nThe expected behavior would be to have a sensor that can run with retries, in case something fails during the sensor check, e.g. infra issues. The retries are not about the sensor not finding what it was supposed to, e.g. ""the task is not there"", but to recover from infra failures, e.g. the database being temporarily unavailable. This behavior works as expected with sensors in general.\r\n\r\nHowever, when combining retries on sensors with timeouts, that\'s where things start getting interesting:\r\n- When the user sets a timeout, the intention is ""wait this long _from the beginning of the first try_"", which is a very important factor that is also highlighted in the [Timeouts section of the docs](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/tasks.html#timeouts). This behavior seems to work correctly with `reschedule` mode thanks to the `task_reschedule` table that records the start timestamp for the first try.\r\n- However, when deferrable mode is used, the timeouts do not work with retries since there\'s no way to retrieve the start time of the first attempt of a task instance.\r\n\r\nIt seems like the user would want the same behavior between deferred and non-deferred versions of the sensor for the timeouts with retries, but I couldn\'t find a way to solve it without adding a new table to airflow. is the original first start time information saved somewhere?', 'created_at': datetime.datetime(2024, 11, 29, 8, 21, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507333955, 'issue_id': 2654013668, 'author': 'kien-truong', 'body': '> When the user sets a timeout, the intention is ""wait this long from the beginning of the first try\r\n\r\nYeah, even in `poke` mode sensor does not respect this behavior but use the first poke of the current try as the starting point, which I think is a bug in itself.\r\n\r\nThe document said,  \r\n> In case that the mode is poke (see below), both of them _(timeout and execution_timeout | emphasis mine)_ are equivalent (as the sensor is never rescheduled)\r\n\r\nHowever, this is only correct if the sensor doesn\'t fail and retry.', 'created_at': datetime.datetime(2024, 11, 29, 8, 42, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507405228, 'issue_id': 2654013668, 'author': 'karakanb', 'body': 'I think the document is correct with regards to what it says: it says ""the `execution_timeout` and the `timeout` behave the same for `poke`"", which means that they don\'t do what we\'d expect them to do, and instead just set the timeout for the specific retry attempt instead.\r\n\r\nI agree with you that they should behave the same way. I think it\'d be a relatively simple fix, but there needs to be a state that stores the attempts.', 'created_at': datetime.datetime(2024, 11, 29, 9, 25, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2511906398, 'issue_id': 2654013668, 'author': 'karakanb', 'body': 'Actually, testing this out, it does not work indeed across attempts, instead the timeout is only enforced within the same attempt across different reschedules. I think @kien-truong is right.', 'created_at': datetime.datetime(2024, 12, 2, 15, 51, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2545500341, 'issue_id': 2654013668, 'author': 'nathadfield', 'body': 'There is currently a behavioural discrepancy between regular and deferred sensors that has been outstanding for many months which, it seems, will finally be [addressed](https://github.com/apache/airflow/pull/33718) in 2.11.x.', 'created_at': datetime.datetime(2024, 12, 16, 12, 28, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2545507371, 'issue_id': 2654013668, 'author': 'nathadfield', 'body': 'I think it is correct that sensors that have exhausted their timeout should not retry; instead the timeout should be longer to cover a wide enough window of waiting.', 'created_at': datetime.datetime(2024, 12, 16, 12, 31, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2566027075, 'issue_id': 2654013668, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 12, 31, 0, 15, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567860555, 'issue_id': 2654013668, 'author': 'seifrajhi', 'body': 'Stills relevant, to remove stale label', 'created_at': datetime.datetime(2025, 1, 2, 14, 29, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567874193, 'issue_id': 2654013668, 'author': 'nathadfield', 'body': ""Actually, unless I'm missing some other issue, I believe this is a duplication of a previously logged issue and looks to be addressed in the forthcoming [Airflow 2.11](https://github.com/apache/airflow/pull/33718)."", 'created_at': datetime.datetime(2025, 1, 2, 14, 38, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568637999, 'issue_id': 2654013668, 'author': 'kien-truong', 'body': ""It's partially overlap with [#33718](https://github.com/apache/airflow/pull/33718), which is making Deferrable sensor to not retry when `timeout` is reach. \r\n\r\nHowever, even with that fix in place, the sensor still needs to call defer with the correct `timeout` parameter, which is not the case with `ExternalTaskSensor`: it's passing `execution_timeout` instead of `timeout`"", 'created_at': datetime.datetime(2025, 1, 3, 3, 4, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568848416, 'issue_id': 2654013668, 'author': 'nathadfield', 'body': '@dstandish Are you able to comment on this given you worked on #33718?', 'created_at': datetime.datetime(2025, 1, 3, 8, 29, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2628605394, 'issue_id': 2654013668, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2025, 2, 1, 0, 16, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2645994282, 'issue_id': 2654013668, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2025, 2, 9, 0, 16, 32, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-13 03:28:39 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

kien-truong (Issue Creator) on (2024-11-13 03:41:05 UTC): I can only contribute at weekend, so feel free to pick this up if anyone feels like it.

venkateshwaracholan (Assginee) on (2024-11-15 19:59:07 UTC): @kien-truong I would like to work on this.

potiuk on (2024-11-16 04:11:33 UTC): Assigned you

karakanb on (2024-11-29 08:21:37 UTC): I looked into this a bit but it seems like there's a fundamental issue here, I'll try to explain below.


The expected behavior would be to have a sensor that can run with retries, in case something fails during the sensor check, e.g. infra issues. The retries are not about the sensor not finding what it was supposed to, e.g. ""the task is not there"", but to recover from infra failures, e.g. the database being temporarily unavailable. This behavior works as expected with sensors in general.

However, when combining retries on sensors with timeouts, that's where things start getting interesting:
- When the user sets a timeout, the intention is ""wait this long _from the beginning of the first try_"", which is a very important factor that is also highlighted in the [Timeouts section of the docs](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/tasks.html#timeouts). This behavior seems to work correctly with `reschedule` mode thanks to the `task_reschedule` table that records the start timestamp for the first try.
- However, when deferrable mode is used, the timeouts do not work with retries since there's no way to retrieve the start time of the first attempt of a task instance.

It seems like the user would want the same behavior between deferred and non-deferred versions of the sensor for the timeouts with retries, but I couldn't find a way to solve it without adding a new table to airflow. is the original first start time information saved somewhere?

kien-truong (Issue Creator) on (2024-11-29 08:42:41 UTC): Yeah, even in `poke` mode sensor does not respect this behavior but use the first poke of the current try as the starting point, which I think is a bug in itself.

The document said,  

However, this is only correct if the sensor doesn't fail and retry.

karakanb on (2024-11-29 09:25:15 UTC): I think the document is correct with regards to what it says: it says ""the `execution_timeout` and the `timeout` behave the same for `poke`"", which means that they don't do what we'd expect them to do, and instead just set the timeout for the specific retry attempt instead.

I agree with you that they should behave the same way. I think it'd be a relatively simple fix, but there needs to be a state that stores the attempts.

karakanb on (2024-12-02 15:51:27 UTC): Actually, testing this out, it does not work indeed across attempts, instead the timeout is only enforced within the same attempt across different reschedules. I think @kien-truong is right.

nathadfield on (2024-12-16 12:28:39 UTC): There is currently a behavioural discrepancy between regular and deferred sensors that has been outstanding for many months which, it seems, will finally be [addressed](https://github.com/apache/airflow/pull/33718) in 2.11.x.

nathadfield on (2024-12-16 12:31:59 UTC): I think it is correct that sensors that have exhausted their timeout should not retry; instead the timeout should be longer to cover a wide enough window of waiting.

github-actions[bot] on (2024-12-31 00:15:15 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

seifrajhi on (2025-01-02 14:29:25 UTC): Stills relevant, to remove stale label

nathadfield on (2025-01-02 14:38:56 UTC): Actually, unless I'm missing some other issue, I believe this is a duplication of a previously logged issue and looks to be addressed in the forthcoming [Airflow 2.11](https://github.com/apache/airflow/pull/33718).

kien-truong (Issue Creator) on (2025-01-03 03:04:30 UTC): It's partially overlap with [#33718](https://github.com/apache/airflow/pull/33718), which is making Deferrable sensor to not retry when `timeout` is reach. 

However, even with that fix in place, the sensor still needs to call defer with the correct `timeout` parameter, which is not the case with `ExternalTaskSensor`: it's passing `execution_timeout` instead of `timeout`

nathadfield on (2025-01-03 08:29:10 UTC): @dstandish Are you able to comment on this given you worked on #33718?

github-actions[bot] on (2025-02-01 00:16:20 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2025-02-09 00:16:32 UTC): This issue has been closed because it has not received response from the issue author.

"
2652302510,issue,closed,completed,dag_run table - replace column conf type - byte ( that store a python pickle ) by a JSON ,"### Description

the column `conf` of the table `dag_run` is using the type `byte` ( and storing a python pickle ) on the database , since airflow only support postgres 12+ and mysql 8+ , we could use a `json` type .

And also the user could search ""efficiently"" in the web-ui a dag_run based on the parameters thanks to that change


### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",raphaelauv,2024-11-12 14:08:55+00:00,['vatsrahul1001'],2025-01-13 14:01:09+00:00,2025-01-13 14:01:09+00:00,https://github.com/apache/airflow/issues/43933,"[('kind:feature', 'Feature Requests'), ('area:db-migrations', 'PRs with DB migration')]","[{'comment_id': 2470655444, 'issue_id': 2652302510, 'author': 'raphaelauv', 'body': 'similar to https://github.com/apache/airflow/pull/43905', 'created_at': datetime.datetime(2024, 11, 12, 14, 16, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2470848617, 'issue_id': 2652302510, 'author': 'kaxil', 'body': 'Nice, yeah we should absolutely get this in for Airflow 3.0', 'created_at': datetime.datetime(2024, 11, 12, 15, 31, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2481319179, 'issue_id': 2652302510, 'author': 'rawwar', 'body': ""~~@raphaelauv , are you working on this?  if not, I would like to pick this one.~~  (Just noticed you did not check the checkbox that you will work on it). I'm picking this one."", 'created_at': datetime.datetime(2024, 11, 17, 15, 36, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507850446, 'issue_id': 2652302510, 'author': 'rawwar', 'body': '@vatsrahul1001  will be working on this one.', 'created_at': datetime.datetime(2024, 11, 29, 13, 44, 15, tzinfo=datetime.timezone.utc)}]","raphaelauv (Issue Creator) on (2024-11-12 14:16:45 UTC): similar to https://github.com/apache/airflow/pull/43905

kaxil on (2024-11-12 15:31:36 UTC): Nice, yeah we should absolutely get this in for Airflow 3.0

rawwar on (2024-11-17 15:36:14 UTC): ~~@raphaelauv , are you working on this?  if not, I would like to pick this one.~~  (Just noticed you did not check the checkbox that you will work on it). I'm picking this one.

rawwar on (2024-11-29 13:44:15 UTC): @vatsrahul1001  will be working on this one.

"
2651623355,issue,closed,completed,Dependencies conflict when trying to build airflow from source using constraint file,"### What do you see as an issue?

I tried to install airflow and provider packages from source following this [documentation](https://github.com/apache/airflow/blob/main/INSTALL)

```
pip install -e "".[devel,google]"" \
--constraint ""https://raw.githubusercontent.com/apache/airflowconstraint-main/cinstraint-no-providers-3.9.txt""

```
this happened:

![issue](https://github.com/user-attachments/assets/e44b3399-29a6-4579-bc4a-d3606a6ad8fb)


### Solving the problem

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",SuccessMoses,2024-11-12 09:41:18+00:00,[],2024-11-12 10:36:28+00:00,2024-11-12 10:36:28+00:00,https://github.com/apache/airflow/issues/43919,"[('kind:bug', 'This is a clearly a bug'), ('kind:documentation', ''), ('area:dependencies', 'Issues related to dependencies problems'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2470051880, 'issue_id': 2651623355, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 12, 9, 41, 21, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-12 09:41:21 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2651337444,issue,open,,[BUG/QUESTION]: Podmanager does not retry consuming logs,"### Apache Airflow Provider(s)

cncf-kubernetes

### Versions of Apache Airflow Providers

_No response_

### Apache Airflow version

v2.10.2

### Operating System

Debian GNU/Linux 12 (bookworm)

### Deployment

Other 3rd-party Helm chart

### Deployment details

Deployment of Airflow via ""Airflow Helm Chart (User Community)"" to Kubernetes Cluster (EKS).
Triggering Pods via KubernetesPodOperator.


### What happened

Sometimes tasks/pods are failing directly after starting in
`pod_manager: read_pod_logs`

with:
```
 kubernetes.client.exceptions.ApiException: (500)                                                                                                                                                                                                               
 Reason: Internal Server Error                                                                                                                                                                                                                                  
 HTTP response headers: HTTPHeaderDict({'Audit-Id': '70fd7044-c526-4061-9e80-ced705d0ccdc', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'Date': 'Mon, 11 Nov 2024 04:33:45 GMT', 'Content-Length': '249'})                        
 HTTP response body: b'{""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""Get \\""https://172.17.134.89:10250/containerLogs/monitoring/kowalski-auto-ua72q9pi/base?follow=true\\u0026timestamps=true\\"": remote error: tls: internal  error"",""code"":500}\n'     
```
Snippet of Log:
```
[2024-11-11T04:33:45.253+0000] {taskinstance.py:3311} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py"", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py"", line 733, in _execute_callable
    return ExecutionCallableRunner(
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/operator_helpers.py"", line 252, in run
    return self.func(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py"", line 417, in wrapper
    return func(self, *args, **kwargs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py"", line 594, in execute
    return self.execute_sync(context)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py"", line 627, in execute_sync
    self.await_pod_completion(pod=self.pod)
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 336, in wrapped_f
    return copy(f, *args, **kw)
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 475, in __call__
    do = self.iter(retry_state=retry_state)
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 376, in iter
    result = action(retry_state)
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
  File ""/usr/local/lib/python3.8/concurrent/futures/_base.py"", line 437, in result
    return self.__get_result()
  File ""/usr/local/lib/python3.8/concurrent/futures/_base.py"", line 389, in __get_result
    raise self._exception
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 478, in __call__
    result = fn(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py"", line 678, in await_pod_completion
    raise exc
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py"", line 663, in await_pod_completion
    self.pod_manager.fetch_requested_container_logs(
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/utils/pod_manager.py"", line 587, in fetch_requested_container_logs
    status = self.fetch_container_logs(pod=pod, container_name=c, follow=follow_logs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/utils/pod_manager.py"", line 510, in fetch_container_logs
    last_log_time, exc = consume_logs(since_time=last_log_time)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/utils/pod_manager.py"", line 440, in consume_logs
    logs = self.read_pod_logs(
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 336, in wrapped_f
    return copy(f, *args, **kw)
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 475, in __call__
    do = self.iter(retry_state=retry_state)
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 376, in iter
    result = action(retry_state)
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 418, in exc_check
    raise retry_exc.reraise()
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 185, in reraise
    raise self.last_attempt.result()
  File ""/usr/local/lib/python3.8/concurrent/futures/_base.py"", line 437, in result
    return self.__get_result()
  File ""/usr/local/lib/python3.8/concurrent/futures/_base.py"", line 389, in __get_result
    raise self._exception
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 478, in __call__
    result = fn(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/utils/pod_manager.py"", line 675, in read_pod_logs
    logs = self._client.read_namespaced_pod_log(
  File ""/home/airflow/.local/lib/python3.8/site-packages/kubernetes/client/api/core_v1_api.py"", line 23957, in read_namespaced_pod_log
    return self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs)  # noqa: E501
  File ""/home/airflow/.local/lib/python3.8/site-packages/kubernetes/client/api/core_v1_api.py"", line 24076, in read_namespaced_pod_log_with_http_info
    return self.api_client.call_api(
  File ""/home/airflow/.local/lib/python3.8/site-packages/kubernetes/client/api_client.py"", line 348, in call_api
    return self.__call_api(resource_path, method,
  File ""/home/airflow/.local/lib/python3.8/site-packages/kubernetes/client/api_client.py"", line 180, in __call_api
    response_data = self.request(
  File ""/home/airflow/.local/lib/python3.8/site-packages/kubernetes/client/api_client.py"", line 373, in request
    return self.rest_client.GET(url,
  File ""/home/airflow/.local/lib/python3.8/site-packages/kubernetes/client/rest.py"", line 244, in GET
    return self.request(""GET"", url,
  File ""/home/airflow/.local/lib/python3.8/site-packages/kubernetes/client/rest.py"", line 238, in request
    raise ApiException(http_resp=r)
kubernetes.client.exceptions.ApiException: (500)
Reason: Internal Server Error
HTTP response headers: HTTPHeaderDict({'Audit-Id': '70fd7044-c526-4061-9e80-ced705d0ccdc', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'Date': 'Mon, 11 Nov 2024 04:33:45 GMT', 'Content-Length': '249'})
HTTP response body: b'{""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""Get \\""https://172.17.134.89:10250/containerLogs/monitoring/kowalski-auto-ua72q9pi/base?follow=true\\u0026timestamps=true\\"": remote error: tls: internal error"",""code"":500}\n'
[2024-11-11T04:33:45.275+0000] {taskinstance.py:906} DEBUG - Task Duration set to 78.504676
[2024-11-11T04:33:45.276+0000] {taskinstance.py:928} DEBUG - Clearing next_method and next_kwargs.
[2024-11-11T04:33:45.277+0000] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=kowalski-auto, task_id=kowalski-auto, run_id=kowalski-a2a-multilappen-dkha-d81ea11e-473c-418b-adf1-ac0d3295737e, execution_date=20241111T043036, start_date=20241111T043226, end_date=20241111T043345
[2024-11-11T04:33:45.387+0000] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2024-11-11T04:33:45.388+0000] {cli_action_loggers.py:98} DEBUG - Calling callbacks: []
[2024-11-11T04:33:45.389+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 125683 for task kowalski-auto ((500)
Reason: Internal Server Error
HTTP response headers: HTTPHeaderDict({'Audit-Id': '70fd7044-c526-4061-9e80-ced705d0ccdc', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'Date': 'Mon, 11 Nov 2024 04:33:45 GMT', 'Content-Length': '249'})
HTTP response body: b'{""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""Get \\""https://172.17.134.89:10250/containerLogs/monitoring/kowalski-auto-ua72q9pi/base?follow=true\\u0026timestamps=true\\"": remote error: tls: internal error"",""code"":500}\n'
; 132)
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/cli/cli_config.py"", line 49, in command
    return func(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/cli.py"", line 116, in wrapper
    return f(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task
    return ti._run_raw_task(
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py"", line 3005, in _run_raw_task
    return _run_raw_task(
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py"", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py"", line 3159, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py"", line 3183, in _execute_task
    return _execute_task(self, context, task_orig)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py"", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py"", line 733, in _execute_callable
    return ExecutionCallableRunner(
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/operator_helpers.py"", line 252, in run
    return self.func(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py"", line 417, in wrapper
    return func(self, *args, **kwargs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py"", line 594, in execute
    return self.execute_sync(context)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py"", line 627, in execute_sync
    self.await_pod_completion(pod=self.pod)
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 336, in wrapped_f
    return copy(f, *args, **kw)
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 475, in __call__
    do = self.iter(retry_state=retry_state)
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 376, in iter
    result = action(retry_state)
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
  File ""/usr/local/lib/python3.8/concurrent/futures/_base.py"", line 437, in result
    return self.__get_result()
  File ""/usr/local/lib/python3.8/concurrent/futures/_base.py"", line 389, in __get_result
    raise self._exception
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 478, in __call__
    result = fn(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py"", line 678, in await_pod_completion
    raise exc
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py"", line 663, in await_pod_completion
    self.pod_manager.fetch_requested_container_logs(
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/utils/pod_manager.py"", line 587, in fetch_requested_container_logs
    status = self.fetch_container_logs(pod=pod, container_name=c, follow=follow_logs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/utils/pod_manager.py"", line 510, in fetch_container_logs
    last_log_time, exc = consume_logs(since_time=last_log_time)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/utils/pod_manager.py"", line 440, in consume_logs
    logs = self.read_pod_logs(
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 336, in wrapped_f
    return copy(f, *args, **kw)
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 475, in __call__
    do = self.iter(retry_state=retry_state)
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 376, in iter
    result = action(retry_state)
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 418, in exc_check
    raise retry_exc.reraise()
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 185, in reraise
    raise self.last_attempt.result()
  File ""/usr/local/lib/python3.8/concurrent/futures/_base.py"", line 437, in result
    return self.__get_result()
  File ""/usr/local/lib/python3.8/concurrent/futures/_base.py"", line 389, in __get_result
    raise self._exception
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 478, in __call__
    result = fn(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/cncf/kubernetes/utils/pod_manager.py"", line 675, in read_pod_logs
    logs = self._client.read_namespaced_pod_log(
  File ""/home/airflow/.local/lib/python3.8/site-packages/kubernetes/client/api/core_v1_api.py"", line 23957, in read_namespaced_pod_log
    return self.read_namespaced_pod_log_with_http_info(name, namespace, **kwargs)  # noqa: E501
  File ""/home/airflow/.local/lib/python3.8/site-packages/kubernetes/client/api/core_v1_api.py"", line 24076, in read_namespaced_pod_log_with_http_info
    return self.api_client.call_api(
  File ""/home/airflow/.local/lib/python3.8/site-packages/kubernetes/client/api_client.py"", line 348, in call_api
    return self.__call_api(resource_path, method,
  File ""/home/airflow/.local/lib/python3.8/site-packages/kubernetes/client/api_client.py"", line 180, in __call_api
    response_data = self.request(
  File ""/home/airflow/.local/lib/python3.8/site-packages/kubernetes/client/api_client.py"", line 373, in request
    return self.rest_client.GET(url,
  File ""/home/airflow/.local/lib/python3.8/site-packages/kubernetes/client/rest.py"", line 244, in GET
    return self.request(""GET"", url,
  File ""/home/airflow/.local/lib/python3.8/site-packages/kubernetes/client/rest.py"", line 238, in request
    raise ApiException(http_resp=r)
kubernetes.client.exceptions.ApiException: (500)
Reason: Internal Server Error
HTTP response headers: HTTPHeaderDict({'Audit-Id': '70fd7044-c526-4061-9e80-ced705d0ccdc', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'Date': 'Mon, 11 Nov 2024 04:33:45 GMT', 'Content-Length': '249'})
HTTP response body: b'{""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""Get \\""https://172.17.134.89:10250/containerLogs/monitoring/kowalski-auto-ua72q9pi/base?follow=true\\u0026timestamps=true\\"": remote error: tls: internal error"",""code"":500}\n'
[2024-11-11T04:33:45.427+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2024-11-11T04:33:45.447+0000] {taskinstance.py:3859} DEBUG - Skip locked rows, rollback
[2024-11-11T04:33:45.454+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
```

### What you think should happen instead

The exception mentioned in ""What happened"" states: ""tls: internal  error""
The assumption is that this is a case of the CSR not being approved (yet)

The comment in `podmanager:` fetch_container_logs states:
""
        Between when the pod starts and logs being available, there might be a delay due to CSR not approved
        and signed yet. In such situation, ApiException is thrown. This is why we are **retrying** on this
        specific exception.
""
This leads me to believe that in cases of failures in trying to consume logs from a pod because of CSR not being approved, will lead to retries. 
Unfortunately I do not see any retries being made in this case (see provided log)

Is my assumption correct or am I missing something?
Thanks :)

### How to reproduce

Start a pod via KubernetesPodOperator and start consuming logs before CSR is approved.

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",danielrolfes2307,2024-11-12 07:44:12+00:00,[],2024-11-21 21:19:13+00:00,,https://github.com/apache/airflow/issues/43912,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]","[{'comment_id': 2469808935, 'issue_id': 2651337444, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 12, 7, 44, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469987256, 'issue_id': 2651337444, 'author': 'potiuk', 'body': 'Yeah. Looks about right. Likely we should make the log retrieval more resilient in this case.', 'created_at': datetime.datetime(2024, 11, 12, 9, 15, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469990097, 'issue_id': 2651337444, 'author': 'potiuk', 'body': ""cc: @mrk-andreev -> might be next one related to the stuff you've been helping at the hackathon ;)"", 'created_at': datetime.datetime(2024, 11, 12, 9, 16, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2478315192, 'issue_id': 2651337444, 'author': 'SuccessMoses', 'body': 'Do you need to start a pod with `KubernetesPodOperator` to reproduce this issue or is it possible to reproduce with `dag.test()` or `dag.run()`. So far I know that the issue is with podmanager requesting logs without CSR approved', 'created_at': datetime.datetime(2024, 11, 15, 9, 18, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2484959768, 'issue_id': 2651337444, 'author': 'danielrolfes2307', 'body': ""@SuccessMoses \r\nUnfortunately I cant answer this. I'm not aware of dag.test() and dag.run()\r\n@potiuk  @mrk-andreev Any idea?"", 'created_at': datetime.datetime(2024, 11, 19, 7, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2486386676, 'issue_id': 2651337444, 'author': 'potiuk', 'body': 'No. It has nothing to do with dag.test() or dag.run() -> it should be reproduced on a real running k8s (we have `breeze k8s` command suite that allows to od it easily.', 'created_at': datetime.datetime(2024, 11, 19, 17, 55, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489634625, 'issue_id': 2651337444, 'author': 'mrk-andreev', 'body': 'I would start by investigating the error message: ""remote error: tls: internal error"", as it seems to be the root cause. A quick search suggests this might be a cloud-specific deployment issue, as seen in cases with [AWS](https://repost.aws/questions/QUK8WLbLYlSs2lKw__7h-uOQ/eks-remote-error-tls-internal-error-when-running-kubectl-logs-command) and [Google Cloud](https://stackoverflow.com/questions/67618615/kubectl-exec-logs-on-gke-returns-remote-error-tls-internal-error).\r\n\r\nOne potential workaround in Airflow could be to check the [CSR](https://docs.aws.amazon.com/eks/latest/userguide/cert-signing.html) (Certificate Signing Request) status before making the initial call to retrieve logs. At the very least, this could be a good starting point for debugging.\r\n\r\nIt would be great if we could reach out to an [AWS Hero](https://aws.amazon.com/developer/community/heroes/) with expertise in EKS to provide a more detailed, step-by-step solution. Additionally, resolving this issue might require some [AWS Credits](https://aws.amazon.com/awscredits/) since debugging could involve costs for better AWS Kubernetes support in Airflow.', 'created_at': datetime.datetime(2024, 11, 20, 22, 7, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2492358709, 'issue_id': 2651337444, 'author': 'mrk-andreev', 'body': 'It might be a feature for [EksPodOperator](https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/amazon/aws/operators/eks.py#L984C7-L984C21).', 'created_at': datetime.datetime(2024, 11, 21, 21, 19, 11, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-12 07:44:15 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-11-12 09:15:38 UTC): Yeah. Looks about right. Likely we should make the log retrieval more resilient in this case.

potiuk on (2024-11-12 09:16:55 UTC): cc: @mrk-andreev -> might be next one related to the stuff you've been helping at the hackathon ;)

SuccessMoses on (2024-11-15 09:18:37 UTC): Do you need to start a pod with `KubernetesPodOperator` to reproduce this issue or is it possible to reproduce with `dag.test()` or `dag.run()`. So far I know that the issue is with podmanager requesting logs without CSR approved

danielrolfes2307 (Issue Creator) on (2024-11-19 07:56:00 UTC): @SuccessMoses 
Unfortunately I cant answer this. I'm not aware of dag.test() and dag.run()
@potiuk  @mrk-andreev Any idea?

potiuk on (2024-11-19 17:55:58 UTC): No. It has nothing to do with dag.test() or dag.run() -> it should be reproduced on a real running k8s (we have `breeze k8s` command suite that allows to od it easily.

mrk-andreev on (2024-11-20 22:07:03 UTC): I would start by investigating the error message: ""remote error: tls: internal error"", as it seems to be the root cause. A quick search suggests this might be a cloud-specific deployment issue, as seen in cases with [AWS](https://repost.aws/questions/QUK8WLbLYlSs2lKw__7h-uOQ/eks-remote-error-tls-internal-error-when-running-kubectl-logs-command) and [Google Cloud](https://stackoverflow.com/questions/67618615/kubectl-exec-logs-on-gke-returns-remote-error-tls-internal-error).

One potential workaround in Airflow could be to check the [CSR](https://docs.aws.amazon.com/eks/latest/userguide/cert-signing.html) (Certificate Signing Request) status before making the initial call to retrieve logs. At the very least, this could be a good starting point for debugging.

It would be great if we could reach out to an [AWS Hero](https://aws.amazon.com/developer/community/heroes/) with expertise in EKS to provide a more detailed, step-by-step solution. Additionally, resolving this issue might require some [AWS Credits](https://aws.amazon.com/awscredits/) since debugging could involve costs for better AWS Kubernetes support in Airflow.

mrk-andreev on (2024-11-21 21:19:11 UTC): It might be a feature for [EksPodOperator](https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/amazon/aws/operators/eks.py#L984C7-L984C21).

"
2651196717,issue,closed,completed,[Errno 30] Read-only file system: '/opt/airflow/webserver_config.py/default_webserver_config.py' when webserverConfigConfigMapName is set,"### Official Helm Chart version

1.15.0 (latest released)

### Apache Airflow version

v2.9.3

### Kubernetes Version

v1.30.0+k3s1

### Helm Chart configuration

values.yaml
```
# Airflow webserver settings
webserver:
  enabled: true
  replicas: 1
  webserverConfigConfigMapName: airflow-webserver-config
```
airflow-webserver-config:
```
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-webserver-config
stringData:
  webserver_config.py: |-
    import os
    from __future__ import annotations
    from flask_appbuilder.const import AUTH_DB

    basedir = os.path.abspath(os.path.dirname(__file__))
    log = logging.getLogger(__name__)

    WTF_CSRF_ENABLED = True
    WTF_CSRF_TIME_LIMIT = None
    AUTH_TYPE = AUTH_DB
```

### Docker Image customizations

I use the default image docker.io/apache/airflow:2.9.3

### What happened

When I specify a custom webserver config in my values.yml with
```
webserver:
  webserverConfigConfigMapName: airflow-webserver-config
```
the webserver pod gets status CrashLoopBackOff and the pod logging is:
```
[2024-11-12T06:25:56.529+0000] {configuration.py:2090} INFO - Creating new FAB webserver config file in: /opt/airflow/webserver_config.py
Traceback (most recent call last):
  File ""/home/airflow/.local/bin/airflow"", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/__main__.py"", line 57, in main
    write_webserver_configuration_if_needed(conf)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/providers_configuration_loader.py"", line 55, in wrapped_function
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/configuration.py"", line 2091, in write_webserver_configuration_if_needed
    shutil.copy(_default_config_file_path(""default_webserver_config.py""), webserver_config)
  File ""/usr/local/lib/python3.12/shutil.py"", line 435, in copy
    copyfile(src, dst, follow_symlinks=follow_symlinks)
  File ""/usr/local/lib/python3.12/shutil.py"", line 262, in copyfile
    with open(dst, 'wb') as fdst:
         ^^^^^^^^^^^^^^^
OSError: [Errno 30] Read-only file system: '/opt/airflow/webserver_config.py/default_webserver_config.py'
```

### What you think should happen instead

The content of webserver_config.py should be replaced with the content specified in the ConfigMap.

### How to reproduce

- Deploy airflow helm chart 1.15.0 on kubernetes v1.30.0+k3s1. All pods will spin up and airflow is operational.
- Add a ConfigMap containing a custom webserver_config.py
- Update the helm release

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",mureco,2024-11-12 06:42:18+00:00,[],2024-11-12 09:06:05+00:00,2024-11-12 09:06:05+00:00,https://github.com/apache/airflow/issues/43910,"[('kind:bug', 'This is a clearly a bug'), ('area:webserver', 'Webserver related Issues'), ('area:helm-chart', 'Airflow Helm Chart'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2469723576, 'issue_id': 2651196717, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 12, 6, 42, 21, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-12 06:42:21 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2650303947,issue,closed,completed,AIP-81 Implement POST/Insert Variables in FastAPI,"### Description

Develop an endpoint in FastAPI that enables POST one or multiple `Variables` from a list. This feature should allow users to import their connections from a set of connections in a single request.

### Use case/motivation

[AIP-81](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-81+Enhanced+Security+in+CLI+via+Integration+of+API)

### Related issues

https://github.com/apache/airflow/issues/42560

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-11-11 20:17:13+00:00,['jason810496'],2025-01-20 09:45:22+00:00,2025-01-20 09:45:21+00:00,https://github.com/apache/airflow/issues/43897,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]","[{'comment_id': 2469415891, 'issue_id': 2650303947, 'author': 'jason810496', 'body': '@bugraoz93 The issue I will take, Thanks !', 'created_at': datetime.datetime(2024, 11, 12, 1, 37, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2490939445, 'issue_id': 2650303947, 'author': 'jason810496', 'body': 'While working on this issue, I came up with two approaches and would appreciate some feedback to decide which one to pursue 🙏\r\n\r\n\r\n## Context for `Variable.set`\r\nFor the `post_variable` implementation, the `Variable.set` method is used to add new variables.  \r\nRef: [variables.py#L155](https://github.com/apache/airflow/blob/main/airflow/api_fastapi/core_api/routes/public/variables.py#L155)\r\n\r\nThe `Variable.set` method involves four steps:\r\n1. **Check if the secret exists in the custom secrets backend**  \r\n   For example, if using Vault, AWS, GCP Secret Manager, etc., it will call `SecretsBackend.get_variable` to verify whether the variable is set in the external secret manager.\r\n2. **Delete the old variable in the metastore**  \r\n3. **Add the variable with the new value**  \r\n4. **Set the secret in the cache**  \r\n\r\n\r\n## Approaches\r\n\r\n### **1. Add a `get_variables` method to the `Variables` model and `SecretsBackend`**\r\nThis approach would require implementing a `get_variables` method for both core and external secret managers:  \r\n- **Core secret managers**:  \r\n  - Cache  \r\n  - Environment variables  \r\n  - Local filesystem  \r\n  - Metastore  \r\n- **External secret managers**:  \r\n  - AWS  \r\n  - GCP  \r\n  - Vault  \r\n  - Azure  \r\n  - Yandex  \r\n\r\n### **2. Call `Variable.set` concurrently using `asyncio`**\r\nInstead of creating a `get_variables` method, this approach would involve calling `Variable.set` concurrently, and have to make sure there are no thread-safety issues.\r\n\r\n## Pros and Cons\r\n\r\n### **First Approach**\r\n- **Pros**:  \r\n  - Fewer API requests to external services.\r\n- **Cons**:  \r\n  - More work is required to implement the `get_variables` method across all secret backends.\r\n\r\n### **Second Approach**\r\n- **Pros**:  \r\n  - Easier and faster to implement.  \r\n- **Cons**:  \r\n  - May result in more API requests to external services, which could impact performance.\r\n  - For example, setting up a bunch of variables in first time might call hundreds of API to external providers simultaneously.\r\n\r\n## My opinion\r\n\r\nWe can start with the **second approach** to quickly implement the main requirement (inserting multiple variables in FastAPI).  \r\nAfterward, we can gradually refactor to use the **first approach** for better efficiency.\r\n\r\ncc @bugraoz93 @pierrejeambrun', 'created_at': datetime.datetime(2024, 11, 21, 11, 59, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2491293544, 'issue_id': 2650303947, 'author': 'pierrejeambrun', 'body': 'I would say second approach is more straightforward to implement with an `asyncio.gather` on all the variables. The issue is for that to work we need the underlying methods, (called methods) to also be async and release the loop on blocking I/O.\r\n\r\nWhich is not the case at the moment. We would need to async await the calls to the secret backend and also to the DB. So I would say that this is too much work and a deeper change.\r\n\r\nThat leaves us with choice 1 only I believe.\r\n\r\nOr backup choice: no bulk endpoint, asyncio on the client side (CLI) to generate concurrent requests on the API to create all the different Variables at the same time using only 1 thread on the client. But I think that was already considered and discarded.', 'created_at': datetime.datetime(2024, 11, 21, 14, 4, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2492708098, 'issue_id': 2650303947, 'author': 'bugraoz93', 'body': ""Great findings! Thanks! My main concern was the Variable :)\r\n\r\n> Which is not the case at the moment. We would need to async await the calls to the secret backend and also to the DB. So I would say that this is too much work and a deeper change.\r\n\r\nI agree this would be too much effort. I would rather not touch those code pieces at all.\r\n\r\n> Or backup choice: no bulk endpoint, asyncio on the client side (CLI) to generate concurrent requests on the API to create all the different Variables at the same time using only 1 thread on the client. But I think that was already considered and discarded.\r\n\r\nMy idea was to have a single/common approach for all the `file inserts`. Making these changes requires more effort than just handling this case on the CLI. Let's simplify this and aim to provide the features for `v3` rather than putting more effort which can have multiple side effects too. We may need to spend operational (bugfix/maintenance) time on top of it. We can include this as an improvement point to have a common approach for all `file inserts` as a next step maybe in `v3.1` and we would have more time to implement at ease. Let's take a step back and reject this ticket. :)\r\n\r\nWhat do you think?"", 'created_at': datetime.datetime(2024, 11, 22, 1, 51, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2493509417, 'issue_id': 2650303947, 'author': 'pierrejeambrun', 'body': ""I agree that consistency would be great. If we feel like this is too much work for now and that our bandwidth needs to be focused on other priorities we can mark it for 3.1 👍.\r\n\r\nHow will you do the bulk insert for variables while we don't have this implemented ? async loop on the client / CLI as suggested above or you have something else in mind ?"", 'created_at': datetime.datetime(2024, 11, 22, 11, 12, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2493644046, 'issue_id': 2650303947, 'author': 'jason810496', 'body': 'If we implement this on the FastAPI side, I would add a wrapper `async def set_async` around the `_set` method to allow the use of `asyncio.gather`. Additionally, I have verified that the SQLAlchemy session is thread-safe for this case, as the same session object will be used across all coroutines. Ref: https://github.com/sqlalchemy/sqlalchemy/issues/5828\n\nIf implemented on the CLI side, I agree with @pierrejeambrun  approach.  \nBoth works, depends on finial decision.', 'created_at': datetime.datetime(2024, 11, 22, 12, 26, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2493873910, 'issue_id': 2650303947, 'author': 'pierrejeambrun', 'body': ""> If we implement this on the FastAPI side, I would add a wrapper async def set_async around the _set method to allow the use of asyncio.gather. Additionally, I have verified that the SQLAlchemy session is thread-safe for this case, as the same session object will be used across all coroutines. Ref: https://github.com/sqlalchemy/sqlalchemy/issues/5828\r\n\r\nThe `Variable.set` method is synced, so basically it will never release and block the main loop. The way you can run sync code from async route is with `run_in_executor` from asyncio or `run_in_threadpool` from `starlette`. https://sentry.io/answers/fastapi-difference-between-run-in-executor-and-run-in-threadpool/\r\n\r\n>Additionally, I have verified that the SQLAlchemy session is thread-safe for this case, as the same session object will be used across all coroutines. Ref: https://github.com/sqlalchemy/sqlalchemy/issues/5828\r\n\r\nI don' think so. As mentioned above you will have to use multiple threads otherwise that would just basically be the same a sequential requests (because the main loop is blocked) => might run into problem with one session shared. And there is a note about `AsyncSession` which are not threadsafe` in the issue you linked.\r\n\r\nWorth trying both, I have no preference at this point. (Maybe inside FastAPI is cleaner because from the CLI point of view, everything is handled the same -> through 1 call to the API, but not sure if this will be super easy to do because of the things highlighted above)"", 'created_at': datetime.datetime(2024, 11, 22, 14, 20, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2495521553, 'issue_id': 2650303947, 'author': 'bugraoz93', 'body': ""> I agree that consistency would be great. If we feel like this is too much work for now and that our bandwidth needs to be focused on other priorities we can mark it for 3.1 👍.\r\n\r\nYeah, this is even better than delete/create. I think handling this case separately doesn't seem too much if we compare it with the implementation and test needed for these changes. Thanks for marking this for 3.1! :) \r\n\r\n> How will you do the bulk insert for variables while we don't have this implemented ? async loop on the client / CLI as suggested above or you have something else in mind ?\r\n\r\nI was thinking the same. Async loop on the client / CLI is the right approach until 3.1. The CLI can handle requests to a single endpoint. Since Variables that are posted will always be recreated, allowing users to publish the same file again with Variables won’t cause issues, aside from increasing the number of API calls. This approach would provide a consistent experience for users across other file imports."", 'created_at': datetime.datetime(2024, 11, 23, 15, 55, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2495524132, 'issue_id': 2650303947, 'author': 'jason810496', 'body': 'Hi @bugraoz93, should I continue working on this issue?  Seem like it should be  implemented on CLI side at this stage 👀', 'created_at': datetime.datetime(2024, 11, 23, 16, 4, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2495535786, 'issue_id': 2650303947, 'author': 'bugraoz93', 'body': 'Hello @jason810496, great question! \r\n\r\nOf course, you can still work on this, but my suggestion would be to prioritize other tasks for now to focus on more urgent implementations in this AIP or other AIPs. :)\r\n\r\nThis is no longer critical for 3.0 (our target for AIP-81). While I hope we’ll eventually have this API endpoint, we now have plenty of time to discuss and implement it before 3.1. Please check the link below. It outlines the timeline for 3.0. Even after the 3.0 release, we’ll still have time to make this happen. \r\nhttps://cwiki.apache.org/confluence/pages/viewpage.action?pageId=308153069#Airflow3.0-Timelines', 'created_at': datetime.datetime(2024, 11, 23, 16, 44, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585883270, 'issue_id': 2650303947, 'author': 'bugraoz93', 'body': 'A PR (#45577) is created to make this implementation. This was brought up after a discussion in Slack to unify `Bulk` operations under the patch endpoint and allow to send create, update, and delete operations under a single endpoint. I will close this after that is merged. \nThanks @jason810496! I will create a follow-up to adopt a new agreed approach on bulk operations, please jump into new tickets if you like.', 'created_at': datetime.datetime(2025, 1, 12, 19, 10, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2601911735, 'issue_id': 2650303947, 'author': 'pierrejeambrun', 'body': 'Done in https://github.com/apache/airflow/pull/45577 I believe.', 'created_at': datetime.datetime(2025, 1, 20, 9, 45, 21, tzinfo=datetime.timezone.utc)}]","jason810496 (Assginee) on (2024-11-12 01:37:28 UTC): @bugraoz93 The issue I will take, Thanks !

jason810496 (Assginee) on (2024-11-21 11:59:05 UTC): While working on this issue, I came up with two approaches and would appreciate some feedback to decide which one to pursue 🙏


## Context for `Variable.set`
For the `post_variable` implementation, the `Variable.set` method is used to add new variables.  
Ref: [variables.py#L155](https://github.com/apache/airflow/blob/main/airflow/api_fastapi/core_api/routes/public/variables.py#L155)

The `Variable.set` method involves four steps:
1. **Check if the secret exists in the custom secrets backend**  
   For example, if using Vault, AWS, GCP Secret Manager, etc., it will call `SecretsBackend.get_variable` to verify whether the variable is set in the external secret manager.
2. **Delete the old variable in the metastore**  
3. **Add the variable with the new value**  
4. **Set the secret in the cache**  


## Approaches

### **1. Add a `get_variables` method to the `Variables` model and `SecretsBackend`**
This approach would require implementing a `get_variables` method for both core and external secret managers:  
- **Core secret managers**:  
  - Cache  
  - Environment variables  
  - Local filesystem  
  - Metastore  
- **External secret managers**:  
  - AWS  
  - GCP  
  - Vault  
  - Azure  
  - Yandex  

### **2. Call `Variable.set` concurrently using `asyncio`**
Instead of creating a `get_variables` method, this approach would involve calling `Variable.set` concurrently, and have to make sure there are no thread-safety issues.

## Pros and Cons

### **First Approach**
- **Pros**:  
  - Fewer API requests to external services.
- **Cons**:  
  - More work is required to implement the `get_variables` method across all secret backends.

### **Second Approach**
- **Pros**:  
  - Easier and faster to implement.  
- **Cons**:  
  - May result in more API requests to external services, which could impact performance.
  - For example, setting up a bunch of variables in first time might call hundreds of API to external providers simultaneously.

## My opinion

We can start with the **second approach** to quickly implement the main requirement (inserting multiple variables in FastAPI).  
Afterward, we can gradually refactor to use the **first approach** for better efficiency.

cc @bugraoz93 @pierrejeambrun

pierrejeambrun on (2024-11-21 14:04:21 UTC): I would say second approach is more straightforward to implement with an `asyncio.gather` on all the variables. The issue is for that to work we need the underlying methods, (called methods) to also be async and release the loop on blocking I/O.

Which is not the case at the moment. We would need to async await the calls to the secret backend and also to the DB. So I would say that this is too much work and a deeper change.

That leaves us with choice 1 only I believe.

Or backup choice: no bulk endpoint, asyncio on the client side (CLI) to generate concurrent requests on the API to create all the different Variables at the same time using only 1 thread on the client. But I think that was already considered and discarded.

bugraoz93 (Issue Creator) on (2024-11-22 01:51:18 UTC): Great findings! Thanks! My main concern was the Variable :)


I agree this would be too much effort. I would rather not touch those code pieces at all.


My idea was to have a single/common approach for all the `file inserts`. Making these changes requires more effort than just handling this case on the CLI. Let's simplify this and aim to provide the features for `v3` rather than putting more effort which can have multiple side effects too. We may need to spend operational (bugfix/maintenance) time on top of it. We can include this as an improvement point to have a common approach for all `file inserts` as a next step maybe in `v3.1` and we would have more time to implement at ease. Let's take a step back and reject this ticket. :)

What do you think?

pierrejeambrun on (2024-11-22 11:12:44 UTC): I agree that consistency would be great. If we feel like this is too much work for now and that our bandwidth needs to be focused on other priorities we can mark it for 3.1 👍.

How will you do the bulk insert for variables while we don't have this implemented ? async loop on the client / CLI as suggested above or you have something else in mind ?

jason810496 (Assginee) on (2024-11-22 12:26:34 UTC): If we implement this on the FastAPI side, I would add a wrapper `async def set_async` around the `_set` method to allow the use of `asyncio.gather`. Additionally, I have verified that the SQLAlchemy session is thread-safe for this case, as the same session object will be used across all coroutines. Ref: https://github.com/sqlalchemy/sqlalchemy/issues/5828

If implemented on the CLI side, I agree with @pierrejeambrun  approach.  
Both works, depends on finial decision.

pierrejeambrun on (2024-11-22 14:20:19 UTC): The `Variable.set` method is synced, so basically it will never release and block the main loop. The way you can run sync code from async route is with `run_in_executor` from asyncio or `run_in_threadpool` from `starlette`. https://sentry.io/answers/fastapi-difference-between-run-in-executor-and-run-in-threadpool/


I don' think so. As mentioned above you will have to use multiple threads otherwise that would just basically be the same a sequential requests (because the main loop is blocked) => might run into problem with one session shared. And there is a note about `AsyncSession` which are not threadsafe` in the issue you linked.

Worth trying both, I have no preference at this point. (Maybe inside FastAPI is cleaner because from the CLI point of view, everything is handled the same -> through 1 call to the API, but not sure if this will be super easy to do because of the things highlighted above)

bugraoz93 (Issue Creator) on (2024-11-23 15:55:51 UTC): Yeah, this is even better than delete/create. I think handling this case separately doesn't seem too much if we compare it with the implementation and test needed for these changes. Thanks for marking this for 3.1! :) 


I was thinking the same. Async loop on the client / CLI is the right approach until 3.1. The CLI can handle requests to a single endpoint. Since Variables that are posted will always be recreated, allowing users to publish the same file again with Variables won’t cause issues, aside from increasing the number of API calls. This approach would provide a consistent experience for users across other file imports.

jason810496 (Assginee) on (2024-11-23 16:04:49 UTC): Hi @bugraoz93, should I continue working on this issue?  Seem like it should be  implemented on CLI side at this stage 👀

bugraoz93 (Issue Creator) on (2024-11-23 16:44:43 UTC): Hello @jason810496, great question! 

Of course, you can still work on this, but my suggestion would be to prioritize other tasks for now to focus on more urgent implementations in this AIP or other AIPs. :)

This is no longer critical for 3.0 (our target for AIP-81). While I hope we’ll eventually have this API endpoint, we now have plenty of time to discuss and implement it before 3.1. Please check the link below. It outlines the timeline for 3.0. Even after the 3.0 release, we’ll still have time to make this happen. 
https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=308153069#Airflow3.0-Timelines

bugraoz93 (Issue Creator) on (2025-01-12 19:10:56 UTC): A PR (#45577) is created to make this implementation. This was brought up after a discussion in Slack to unify `Bulk` operations under the patch endpoint and allow to send create, update, and delete operations under a single endpoint. I will close this after that is merged. 
Thanks @jason810496! I will create a follow-up to adopt a new agreed approach on bulk operations, please jump into new tickets if you like.

pierrejeambrun on (2025-01-20 09:45:21 UTC): Done in https://github.com/apache/airflow/pull/45577 I believe.

"
2650300688,issue,closed,completed,AIP-81 Implement POST/Insert Multiple Pools in FastAPI,"### Description

Develop an endpoint in FastAPI that enables POST one or multiple `Pools` from a list. This feature should allow users to import their connections from a set of connections in a single request.

### Use case/motivation

[AIP-81](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-81+Enhanced+Security+in+CLI+via+Integration+of+API)

### Related issues

https://github.com/apache/airflow/issues/42560

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-11-11 20:14:53+00:00,['jason810496'],2024-11-22 08:40:36+00:00,2024-11-22 08:40:36+00:00,https://github.com/apache/airflow/issues/43896,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]","[{'comment_id': 2469416244, 'issue_id': 2650300688, 'author': 'jason810496', 'body': '@bugraoz93 The issue I will take also, Thanks !', 'created_at': datetime.datetime(2024, 11, 12, 1, 37, 49, tzinfo=datetime.timezone.utc)}]","jason810496 (Assginee) on (2024-11-12 01:37:49 UTC): @bugraoz93 The issue I will take also, Thanks !

"
2650104634,issue,open,,Create a basic guiding doc - How to Debug Your Airflow Deployment,"### Description

Create a basic guiding doc ""How to Debug Your Airflow Deployment"" that can guide users towards debugging their Airflow deployment. This doc can be based on general Airflow best practices + Agans' 9 rules of debugging:

1. Understand the system
2. Make it fail
3. Quit thinking and look
4. Divide and conquer
5. Change one thing at a time
6. Keep an audit trail
7. Check the plug
8. Get a fresh view
9. If you didn't fix it, it ain't fixed

The guide shall express above rules using Airflow examples, such that the users can read the guide and understand how to approach similar Airflow debugging scenarios.

### Use case/motivation

David A. Wheeler's Article (2004): https://dwheeler.com/essays/debugging-agans.html

### Related issues

Parent Issue: https://github.com/apache/airflow/issues/40975

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-11-11 18:43:08+00:00,['omkar-foss'],2024-11-25 06:40:07+00:00,,https://github.com/apache/airflow/issues/43892,"[('kind:feature', 'Feature Requests'), ('kind:documentation', '')]","[{'comment_id': 2472960569, 'issue_id': 2650104634, 'author': 'Dev-iL', 'body': '[Relevant discussion on Slack](https://apache-airflow.slack.com/archives/C07J87PK1BK/p1731346161804209) with some ideas on how the outputs of this should be structured and how to set the readers\' expectations.\r\n\r\nKey ideas raised therein (since the Slack thread will be unreachable eventually):\r\n- ""How to think like airflow debugger"": some kind of best practices, where we see an example of a problem and lead to an issue or PR where we describe how you could approach looking at the issue.\r\n- We should not aim to provide a “comprehensive” guide  - or anything that will point our users to “I have not found how to debug this particular problem - you must add it as this is the “official” guide. It’s nearly impossible to provide even remotely comprehensive debugging guidelines for all the kinds of deployments our users might have - and they should know that they should likely build their own “debugging recipes” specific for their deployments. It should be clear we are providing “guidelines” and “hints” - but not “recipes and end-2-end solutions how to debug”.\r\n- We should clearly set the boundaries and expectations (of the task), as this is easy to aim a bit too high for that.\r\n- The target of this effort will be the [Troubleshooting](https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html) page. We can structure it with a TOC on top, filled with examples, possibly split into categories, where each example has 3 sections:\r\n    - symptoms: what the user sees in the logs (such as error messages), ui notifications, etc.\r\n    - diagnosis: an explanation of what likely caused this (misconfiguration, unaddressed removed deprecations, ...)\r\n    - suggestions: this might have concrete steps for fixing the issue, links to other issues, further diagnosis suggestions, etc.\r\n- We may take inspiration from the [Installation of Airflow](https://airflow.apache.org/docs/apache-airflow/stable/installation/index.html) page that has sections like ""**What you are expected to handle**"" (what you should expect from yourself) and ""**What Apache Airflow Community provides for that method**"" (what you should expect from the community).', 'created_at': datetime.datetime(2024, 11, 13, 9, 24, 16, tzinfo=datetime.timezone.utc)}]","Dev-iL on (2024-11-13 09:24:16 UTC): [Relevant discussion on Slack](https://apache-airflow.slack.com/archives/C07J87PK1BK/p1731346161804209) with some ideas on how the outputs of this should be structured and how to set the readers' expectations.

Key ideas raised therein (since the Slack thread will be unreachable eventually):
- ""How to think like airflow debugger"": some kind of best practices, where we see an example of a problem and lead to an issue or PR where we describe how you could approach looking at the issue.
- We should not aim to provide a “comprehensive” guide  - or anything that will point our users to “I have not found how to debug this particular problem - you must add it as this is the “official” guide. It’s nearly impossible to provide even remotely comprehensive debugging guidelines for all the kinds of deployments our users might have - and they should know that they should likely build their own “debugging recipes” specific for their deployments. It should be clear we are providing “guidelines” and “hints” - but not “recipes and end-2-end solutions how to debug”.
- We should clearly set the boundaries and expectations (of the task), as this is easy to aim a bit too high for that.
- The target of this effort will be the [Troubleshooting](https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html) page. We can structure it with a TOC on top, filled with examples, possibly split into categories, where each example has 3 sections:
    - symptoms: what the user sees in the logs (such as error messages), ui notifications, etc.
    - diagnosis: an explanation of what likely caused this (misconfiguration, unaddressed removed deprecations, ...)
    - suggestions: this might have concrete steps for fixing the issue, links to other issues, further diagnosis suggestions, etc.
- We may take inspiration from the [Installation of Airflow](https://airflow.apache.org/docs/apache-airflow/stable/installation/index.html) page that has sections like ""**What you are expected to handle**"" (what you should expect from yourself) and ""**What Apache Airflow Community provides for that method**"" (what you should expect from the community).

"
2649459877,issue,open,,MappedTasks don't short-circuit,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

It seems like tasks created via `.expand()` do not inherit the downstream tasks of its parent/original. This becomes an issue when we expand a `task.short_circuit` because short-circuiting no longer works.

### What you think should happen instead?

My understanding from the documentation is, that `step_two.expand(do=step_one.expand(i=start()))` is supposed to be equivalent to:
```mermaid
flowchart LR
start --> A1[step_one_0] --> B1[step_two_0]
start --> A2[step_one_1] --> B2[step_two_1]
start --> A3[step_one_2] --> B3[step_two_2]
start --> A4[…] --> B4[…]
start --> A9[step_one_9] --> B9[step_two_9]
```

### How to reproduce

```python
import random
from typing import List

import pendulum
from airflow.decorators import dag, task


@dag(
    ""test_foo"",
    schedule=None,
    start_date=pendulum.now(),
    render_template_as_native_obj=True,
    dag_display_name=""Test random things"",
)
def test_foo():
    @task.python
    def start() -> List[int]:
        return [i for i in range(10)]

    @task.short_circuit
    def step_one(i: int) -> bool:
        print(f""Hello from step {i}"")
        return random.random() >= 0.5

    @task.python
    def step_two(do: bool):
        if not do:
            print(""Should've been skipped."")
        print(""Doing stuff"")

    step_two.expand(do=step_one.expand(i=start()))


test_foo()

if __name__ == ""__main__"":
    test_foo().test()
```
Obviously approximately 5 of the 10 tasks are bound to return `False` and short-circuit. If they do the log will read something like this:
```
Hello from step 5
[2024-11-11 13:31:09,695] {python.py:240} INFO - Done. Returned value was: False
[2024-11-11 13:31:09,697] {python.py:309} INFO - Condition result is False
[2024-11-11 13:31:09,698] {python.py:316} INFO - No downstream tasks; nothing to do.
```
But checking the `task_dict` variable in the module, we see that `step_one` does have `step_two` set as downstream task and vice-versa, `step_two` has `step_one` set as upstream task.
```
__pydevd_ret_val_dict['factory'].task_dict['step_one'].downstream_task_ids
{'step_two'}
__pydevd_ret_val_dict['factory'].task_dict['step_two'].upstream_task_ids
{'step_one'}
```


### Operating System

24.04.1 LTS (Noble Numbat)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

I'm testing this on Standalone

### Anything else?

I can get closer to the desired behaviour by using a list comprehension in the dependency, but that can't be done dynamically, I can't iterate `PlainXComArg`s and it also doesn't produce exactly the desired behaviour.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Chais,2024-11-11 14:14:12+00:00,['shahar1'],2024-12-18 06:40:25+00:00,,https://github.com/apache/airflow/issues/43883,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:dynamic-task-mapping', 'AIP-42'), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2470400840, 'issue_id': 2649459877, 'author': 'Chais', 'body': 'I was able to get the desired behaviour by creating a task group, pulling the `return_value` XCom from `start` and manually creating all the tasks in a loop, which I feel shouldn\'t be necessary:\r\n```python\r\nfrom random import random\r\nfrom typing import List\r\n\r\nfrom airflow.decorators import dag, task, task_group\r\nfrom airflow.models import XCom\r\nfrom pendulum import now\r\n\r\n\r\n@dag(\r\n    ""test_foo"",\r\n    schedule=None,\r\n    start_date=now(),\r\n    dag_display_name=""Test random things"",\r\n)\r\ndef test_foo():\r\n    @task.python\r\n    def start() -> List[int]:\r\n        return [i for i in range(1, 11)]\r\n\r\n    @task_group()\r\n    def tg():\r\n        @task.short_circuit\r\n        def step_one(i: int) -> bool:\r\n            print(f""Hello from step {i}"")\r\n            return random() >= 0.5\r\n\r\n        @task.python(trigger_rule=""one_done"")\r\n        def step_two(do: bool):\r\n            if not do:\r\n                print(""Should\'ve been skipped."")\r\n            print(""Doing stuff"")\r\n\r\n        for i in XCom.get_one(key=""return_value"", task_id=""start"", run_id=XCom.run_id):\r\n            step_two(step_one(i))\r\n\r\n    start() >> tg()\r\n\r\n\r\ntest_foo()\r\n\r\nif __name__ == ""__main__"":\r\n    test_foo().test()\r\n```', 'created_at': datetime.datetime(2024, 11, 12, 12, 26, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2488378866, 'issue_id': 2649459877, 'author': 'Chais', 'body': ""I'm seeing this on 2.9.3, too."", 'created_at': datetime.datetime(2024, 11, 20, 11, 50, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2516665932, 'issue_id': 2649459877, 'author': 'Paniraj2010', 'body': '@eladkal we have upgraded from airflow 2.9.3 to 2.10.3, we have similar issue. where we dont see logs on any task. We getting error. ""No task logs found. Try the Event Log tab for more context."" \r\n\r\nMay I know is this resolved?', 'created_at': datetime.datetime(2024, 12, 4, 9, 23, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2525311211, 'issue_id': 2649459877, 'author': 'shahar1', 'body': ""Hey @Chais, I've managed to reproduce the issue using the code snippets that you provided. \r\n~The root cause of this issue is that short circuit isn't currently suitable for working with task-generated mapping - when the short circuit operator runs in that case, downstream tasks have not been expanded yet - so it just returns `No downstream tasks; nothing to do.` and moves on as if nothing happend:~\r\nhttps://github.com/apache/airflow/blob/b5f033a933d6bba2433a50a62b389b6546123ac4/airflow/operators/python.py#L315\r\n~In the second example that you provided, downstream are known before hand - so it works normally.~\r\nI'll try to work on a fix. \r\n\r\n**Edit**: See my response below."", 'created_at': datetime.datetime(2024, 12, 7, 21, 8, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2527022673, 'issue_id': 2649459877, 'author': 'uranusjr', 'body': 'Shouldn’t `downstream_task_ids` still contain the unexpanded base task in this case? Something still seems off to me. And we should probably fix the operator in any case.', 'created_at': datetime.datetime(2024, 12, 9, 6, 8, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2540124333, 'issue_id': 2649459877, 'author': 'shahar1', 'body': ""> Shouldn’t `downstream_task_ids` still contain the unexpanded base task in this case? Something still seems off to me. And we should probably fix the operator in any case.\r\n\r\nAfter spending some more time, I've figured that you should be correct, and the problem seems to be actually here (see TODO comment just above it):\r\nhttps://github.com/apache/airflow/blob/caa90a18d0790fb51c919485b749047d96c315c0/airflow/models/taskinstance.py#L1331\r\n\r\nUntil this point, `task` contains the `downstream_task_ids` - but due to the early return of mapped operators, it's not recorded in the downstream tasks.\r\n\r\nThe hacky solution seems to be pushing the `downstream_task_ids` for into a hidden xcom, so the mapped short-circuit will be able it to utilize it - but I'm also open for suggestions :)"", 'created_at': datetime.datetime(2024, 12, 12, 22, 18, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2540612899, 'issue_id': 2649459877, 'author': 'uranusjr', 'body': 'Why does not recording TaskMap affect this? The function only does one thing—pushing a row to the TaskMap table. This table is only used for task mapping.', 'created_at': datetime.datetime(2024, 12, 13, 5, 58, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2542529922, 'issue_id': 2649459877, 'author': 'shahar1', 'body': ""> Why does not recording TaskMap affect this? The function only does one thing—pushing a row to the TaskMap table. This table is only used for task mapping.\r\n\r\nYou're correct, I think that now I understand better what's going on :)\r\nI've managed to come up with a solution - feel free to review the PRs when you have the time."", 'created_at': datetime.datetime(2024, 12, 13, 23, 11, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2550480010, 'issue_id': 2649459877, 'author': 'shahar1', 'body': ""I've resolved the issue for version 2.X, so hopefully it will be available starting v2.10.5/v2.11.0 (whatever comes first).\r\nI leave this issue open as there are some arrangements to do before merging the corresponding PR to `main` (v3+)."", 'created_at': datetime.datetime(2024, 12, 18, 6, 40, 23, tzinfo=datetime.timezone.utc)}]","Chais (Issue Creator) on (2024-11-12 12:26:48 UTC): I was able to get the desired behaviour by creating a task group, pulling the `return_value` XCom from `start` and manually creating all the tasks in a loop, which I feel shouldn't be necessary:
```python
from random import random
from typing import List

from airflow.decorators import dag, task, task_group
from airflow.models import XCom
from pendulum import now


@dag(
    ""test_foo"",
    schedule=None,
    start_date=now(),
    dag_display_name=""Test random things"",
)
def test_foo():
    @task.python
    def start() -> List[int]:
        return [i for i in range(1, 11)]

    @task_group()
    def tg():
        @task.short_circuit
        def step_one(i: int) -> bool:
            print(f""Hello from step {i}"")
            return random() >= 0.5

        @task.python(trigger_rule=""one_done"")
        def step_two(do: bool):
            if not do:
                print(""Should've been skipped."")
            print(""Doing stuff"")

        for i in XCom.get_one(key=""return_value"", task_id=""start"", run_id=XCom.run_id):
            step_two(step_one(i))

    start() >> tg()


test_foo()

if __name__ == ""__main__"":
    test_foo().test()
```

Chais (Issue Creator) on (2024-11-20 11:50:15 UTC): I'm seeing this on 2.9.3, too.

Paniraj2010 on (2024-12-04 09:23:28 UTC): @eladkal we have upgraded from airflow 2.9.3 to 2.10.3, we have similar issue. where we dont see logs on any task. We getting error. ""No task logs found. Try the Event Log tab for more context."" 

May I know is this resolved?

shahar1 (Assginee) on (2024-12-07 21:08:30 UTC): Hey @Chais, I've managed to reproduce the issue using the code snippets that you provided. 
~The root cause of this issue is that short circuit isn't currently suitable for working with task-generated mapping - when the short circuit operator runs in that case, downstream tasks have not been expanded yet - so it just returns `No downstream tasks; nothing to do.` and moves on as if nothing happend:~
https://github.com/apache/airflow/blob/b5f033a933d6bba2433a50a62b389b6546123ac4/airflow/operators/python.py#L315
~In the second example that you provided, downstream are known before hand - so it works normally.~
I'll try to work on a fix. 

**Edit**: See my response below.

uranusjr on (2024-12-09 06:08:26 UTC): Shouldn’t `downstream_task_ids` still contain the unexpanded base task in this case? Something still seems off to me. And we should probably fix the operator in any case.

shahar1 (Assginee) on (2024-12-12 22:18:34 UTC): After spending some more time, I've figured that you should be correct, and the problem seems to be actually here (see TODO comment just above it):
https://github.com/apache/airflow/blob/caa90a18d0790fb51c919485b749047d96c315c0/airflow/models/taskinstance.py#L1331

Until this point, `task` contains the `downstream_task_ids` - but due to the early return of mapped operators, it's not recorded in the downstream tasks.

The hacky solution seems to be pushing the `downstream_task_ids` for into a hidden xcom, so the mapped short-circuit will be able it to utilize it - but I'm also open for suggestions :)

uranusjr on (2024-12-13 05:58:59 UTC): Why does not recording TaskMap affect this? The function only does one thing—pushing a row to the TaskMap table. This table is only used for task mapping.

shahar1 (Assginee) on (2024-12-13 23:11:10 UTC): You're correct, I think that now I understand better what's going on :)
I've managed to come up with a solution - feel free to review the PRs when you have the time.

shahar1 (Assginee) on (2024-12-18 06:40:23 UTC): I've resolved the issue for version 2.X, so hopefully it will be available starting v2.10.5/v2.11.0 (whatever comes first).
I leave this issue open as there are some arrangements to do before merging the corresponding PR to `main` (v3+).

"
2649370865,issue,open,,Passing last_dag_run_state to recent dag runs endpoint returns dags where the dag has any dagrun equal to the last_dag_run_state,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Passing last_dag_run_state from the UI causes the API to return dags where any of the dagrun might have the state passed and not filtered on the last dag run state. E.g. A dag with one failure among 10 dagruns with last dagrun state as success will be still returned on passing last_dag_run_state=failed in the API.

This can be due to the below query where rank is calculated but on passing last_dag_run_state the row with the rank of 1 which is the latest dag run should be the one considered for filter. Instead the `_LastDagRunStateFilter` only considers state. I am not sure how rank from the subquery can be access here.

https://github.com/apache/airflow/blob/606ef453a1cb3f2c54f90fe756d9044b8edf0121/airflow/api_fastapi/core_api/routes/ui/dags.py#L67-L80

https://github.com/apache/airflow/blob/606ef453a1cb3f2c54f90fe756d9044b8edf0121/airflow/api_fastapi/common/parameters.py#L342-L351

One idea was to filter by rank=1 and state when last_dag_run_state is passed but that would mean instead of n recent dag runs only 1 run which is the last run will be returned.

https://github.com/apache/airflow/blob/606ef453a1cb3f2c54f90fe756d9044b8edf0121/airflow/api_fastapi/core_api/routes/ui/dags.py#L87

Get dags endpoint works though using CTE to return only correct dags : 

https://github.com/apache/airflow/blob/main/airflow/api_fastapi/common/db/dags.py

This can be reproduced by the below test where in the fixture dag_1, dag_2 will have last dagrun state as success and dag_3 will have last dagrun state as failed but on passing ""success""/""failed"" the result remains the same.

https://github.com/apache/airflow/blob/606ef453a1cb3f2c54f90fe756d9044b8edf0121/tests/api_fastapi/core_api/routes/ui/test_dags.py#L76-L77

### What you think should happen instead?

_No response_

### How to reproduce

1. Dag1 with 5 dagruns and last run as success but 3rd run as failed.
2. Dag 2 with 5 dagruns and last run as failhttps://github.com/apache/airflow/blob/main/airflow/api_fastapi/common/db/dags.pyed.
3. Go to http://localhost:8000/webapp/dags?last_dag_run_state=failed and in the recent dag runs API Dag1 is still returned but filtered out in the frontend.

### Operating System

Ubuntu

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",tirkarthi,2024-11-11 13:36:24+00:00,[],2024-11-11 13:38:46+00:00,,https://github.com/apache/airflow/issues/43882,"[('kind:bug', 'This is a clearly a bug'), ('area:API', ""Airflow's REST/HTTP API""), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2648745319,issue,open,,Add support for format_parameters to the new fastAPI endpoints,"### Description

Legacy API has support for format_parameters which is used to place restrictions on the response page. For example: `@format_parameters({""limit"": check_limit})` we can set limit on the limit of entries returned by an API.

### Use case/motivation

Cleaner migration from legacy to fastapi. Will be useful for all endpoints.

### Related issues

Child of https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",amoghrajesh,2024-11-11 09:38:10+00:00,['kandharvishnu'],2025-01-09 14:44:45+00:00,,https://github.com/apache/airflow/issues/43872,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('good first issue', ''), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2567358028, 'issue_id': 2648745319, 'author': 'rawwar', 'body': 'Wouldn\'t this confuse users when they specify ""limit"" as part of the request body/param, unaware of the maximum_page_limit? As in, if limit > maximum_page_limit, the API response returns the `maximum_page_limit` number of records. The user needs to refer to the warning logs to understand that the reason for the truncation is that they have set `maximum_page_limit`.   I think, along with the warning log, we should also update REST API documentation.\r\n\r\nI noticed that the format_parameters decorator is not added to all endpoints. Therefore, I think this needs to be mentioned in the docs for all the endpoints that use it.', 'created_at': datetime.datetime(2025, 1, 2, 7, 9, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567361355, 'issue_id': 2648745319, 'author': 'kandharvishnu', 'body': 'I am interested in working on this issue', 'created_at': datetime.datetime(2025, 1, 2, 7, 14, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567733935, 'issue_id': 2648745319, 'author': 'amoghrajesh', 'body': '> Wouldn\'t this confuse users when they specify ""limit"" as part of the request body/param, unaware of the maximum_page_limit? As in, if limit > maximum_page_limit, the API response returns the maximum_page_limit number of records. The user needs to refer to the warning logs to understand that the reason for the truncation is that they have set maximum_page_limit. I think, along with the warning log, we should also update REST API documentation.\r\n\r\nYeah, that\'s a fair point. However, we do mention this in the API doc https://github.com/apache/airflow/blob/f7da5e48a2740a2bfb1c3dd57bb463ae29599a26/docs/apache-airflow/security/api.rst#page-size-limit about default size. I am all for adding it for clarification purpose, but probably that should be a different issue, unrelated to this @rawwar?', 'created_at': datetime.datetime(2025, 1, 2, 12, 55, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567875287, 'issue_id': 2648745319, 'author': 'potiuk', 'body': '> Agree. The limit was implemented in a bit to ""silent"" way in the old  API and it created confusion. \r\n\r\nEven if we have the ""max limit"" internally - I think best thing to respond with error when:\r\n\r\na) you do not specify limit and number of responses exceeds the ""max limit"" \r\nb) you specify limit that is bigger than ""max-limit""\r\n\r\nBoth cases should result in ERROR - currently both will silently return you something different than you expected.', 'created_at': datetime.datetime(2025, 1, 2, 14, 39, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568589922, 'issue_id': 2648745319, 'author': 'rawwar', 'body': '> Both cases should result in ERROR - currently, both will silently return you something different than you expected.\r\n\r\n+1 to returning an Error. FYI, @pierrejeambrun', 'created_at': datetime.datetime(2025, 1, 3, 1, 31, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2573386999, 'issue_id': 2648745319, 'author': 'pierrejeambrun', 'body': ""I'm not sure I understand correctly.\r\n\r\nIs this issue for adding a generic `@format_parameters` decorator, or something to specifically handle the `maximum_page_limit` case ? \r\n\r\nIf it's the former, I don't see what advantages it brings in regards to Pydantic + FastAPI query param validation schema ? All of the parameter validation is already decoupled from the endpoint logic in it's current form. If it's the later (how to handle the maximum page limit), then +1 for explicit error. That validation error just need to be added to the `LimitFilter` logic."", 'created_at': datetime.datetime(2025, 1, 6, 15, 48, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2580231912, 'issue_id': 2648745319, 'author': 'rawwar', 'body': '@pierrejeambrun , I think @amoghrajesh created it for a generic `@format_parameters`. I started discussing about `maximum_page_limit` as a specific case.', 'created_at': datetime.datetime(2025, 1, 9, 13, 50, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2580442287, 'issue_id': 2648745319, 'author': 'pierrejeambrun', 'body': 'Maybe I am missing something. Can you elaborate the idea and highlight what advantages it would brings to develop this custom decorator for FastAPI compared to the validation offered natively by pydantic `Annotated` + `BeforeValidator/PostValidator` + `field/model` validators that is directly integrated with FastAPI ?', 'created_at': datetime.datetime(2025, 1, 9, 14, 44, 43, tzinfo=datetime.timezone.utc)}]","rawwar on (2025-01-02 07:09:59 UTC): Wouldn't this confuse users when they specify ""limit"" as part of the request body/param, unaware of the maximum_page_limit? As in, if limit > maximum_page_limit, the API response returns the `maximum_page_limit` number of records. The user needs to refer to the warning logs to understand that the reason for the truncation is that they have set `maximum_page_limit`.   I think, along with the warning log, we should also update REST API documentation.

I noticed that the format_parameters decorator is not added to all endpoints. Therefore, I think this needs to be mentioned in the docs for all the endpoints that use it.

kandharvishnu (Assginee) on (2025-01-02 07:14:44 UTC): I am interested in working on this issue

amoghrajesh (Issue Creator) on (2025-01-02 12:55:07 UTC): Yeah, that's a fair point. However, we do mention this in the API doc https://github.com/apache/airflow/blob/f7da5e48a2740a2bfb1c3dd57bb463ae29599a26/docs/apache-airflow/security/api.rst#page-size-limit about default size. I am all for adding it for clarification purpose, but probably that should be a different issue, unrelated to this @rawwar?

potiuk on (2025-01-02 14:39:46 UTC): Even if we have the ""max limit"" internally - I think best thing to respond with error when:

a) you do not specify limit and number of responses exceeds the ""max limit"" 
b) you specify limit that is bigger than ""max-limit""

Both cases should result in ERROR - currently both will silently return you something different than you expected.

rawwar on (2025-01-03 01:31:37 UTC): +1 to returning an Error. FYI, @pierrejeambrun

pierrejeambrun on (2025-01-06 15:48:35 UTC): I'm not sure I understand correctly.

Is this issue for adding a generic `@format_parameters` decorator, or something to specifically handle the `maximum_page_limit` case ? 

If it's the former, I don't see what advantages it brings in regards to Pydantic + FastAPI query param validation schema ? All of the parameter validation is already decoupled from the endpoint logic in it's current form. If it's the later (how to handle the maximum page limit), then +1 for explicit error. That validation error just need to be added to the `LimitFilter` logic.

rawwar on (2025-01-09 13:50:12 UTC): @pierrejeambrun , I think @amoghrajesh created it for a generic `@format_parameters`. I started discussing about `maximum_page_limit` as a specific case.

pierrejeambrun on (2025-01-09 14:44:43 UTC): Maybe I am missing something. Can you elaborate the idea and highlight what advantages it would brings to develop this custom decorator for FastAPI compared to the validation offered natively by pydantic `Annotated` + `BeforeValidator/PostValidator` + `field/model` validators that is directly integrated with FastAPI ?

"
2648453682,issue,open,,"apache-airflow-providers-amazon - Not able to connect to minio endpoint_url - Error Could not connect to the endpoint URL: ""https://sts.default.amazonaws.com/"" ","### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon 8.25.0

### Apache Airflow version

2.9.3

### Operating System

airflow@airflow-webserver-7d758d8f79-lcpnr:/opt/airflow$ cat /etc/os-release PRETTY_NAME=""Debian GNU/Linux 12 (bookworm)"" NAME=""Debian GNU/Linux"" VERSION_ID=""12"" VERSION=""12 (bookworm)"" VERSION_CODENAME=bookworm ID=debian HOME_URL=""https://www.debian.org/"" SUPPORT_URL=""https://www.debian.org/support"" BUG_REPORT_URL=""https://bugs.debian.org/""

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Deployed though official helm chart on k8s 1.29

### What happened

We are trying to connect to minio s3 object store bucket using apache-airflow-providers-amazon provider Connector.

Login to Airflow Webserver
Admin -> connections -> Connection ID (Any Name you can provide )-> Connection Type as Amazon Web services -> Extra
{
""aws_access_key_id"": ""XXXXXXXXXXXXXX"",
""aws_secret_access_key"": ""XXXXXXXXXXXXXXXXXXXXX"",
""endpoint_url"": ""[https://minio-url.xxx.com:443](https://minio-url.xxx.com/)"",
""verify"": false
}
When we test connection

Error: 'EndpointConnectionError' error occurred while testing connection: Could not connect to the endpoint URL: ""https://sts.default.amazonaws.com/""

It is trying to connect https://sts.default.amazonaws.com/ for authentication and authorization

Why it is trying to connect https://sts.default.amazonaws.com/ ? instead of ""endpoint_url"": ""[https://minio-url.xxx.com:443](https://minio-url.xxx.com/)"",?

### What you think should happen instead

It suppose to connect to minio endpoint_url ""endpoint_url"": ""[https://minio-url.xxx.com:443](https://minio-url.xxx.com/)"", instead of connecting to https://sts.default.amazonaws.com/

### How to reproduce

We are trying to connect to minio s3 object store bucket using apache-airflow-providers-amazon provider Connector.

Login to Airflow Webserver
Admin -> connections -> Connection ID (Any Name you can provide )-> Connection Type as Amazon Web services -> Extra
{
""aws_access_key_id"": ""XXXXXXXXXXXXXX"",
""aws_secret_access_key"": ""XXXXXXXXXXXXXXXXXXXXX"",
""endpoint_url"": ""[https://minio-url.xxx.com:443](https://minio-url.xxx.com/)"",
""verify"": false
}
When we test connection

Error: 'EndpointConnectionError' error occurred while testing connection: Could not connect to the endpoint URL: ""https://sts.default.amazonaws.com/""

It is trying to connect https://sts.default.amazonaws.com/ for authentication and authorization

Why it is trying to connect https://sts.default.amazonaws.com/ ? instead of ""endpoint_url"": ""[https://minio-url.xxx.com:443](https://minio-url.xxx.com/)"",?

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",mohantnr,2024-11-11 07:35:57+00:00,[],2024-11-11 11:34:45+00:00,,https://github.com/apache/airflow/issues/43871,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2467432044, 'issue_id': 2648453682, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 11, 7, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2467957237, 'issue_id': 2648453682, 'author': 'eladkal', 'body': ""minio isn't officially supported. There was attempt to add it in https://github.com/apache/airflow/pull/32955 but it got stale.\r\nYou can probably reuse the code there with custom hook/connection."", 'created_at': datetime.datetime(2024, 11, 11, 11, 34, 43, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-11 07:36:00 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2024-11-11 11:34:43 UTC): minio isn't officially supported. There was attempt to add it in https://github.com/apache/airflow/pull/32955 but it got stale.
You can probably reuse the code there with custom hook/connection.

"
2648422365,issue,closed,not_planned,"apache-airflow-providers-amazon - Not able to connect to minio endpoint_url - Error   Could not connect to the endpoint URL: ""https://sts.default.amazonaws.com/""","### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon          8.25.0

### Apache Airflow version

2.9.3

### Operating System

airflow@airflow-webserver-7d758d8f79-lcpnr:/opt/airflow$ cat /etc/os-release PRETTY_NAME=""Debian GNU/Linux 12 (bookworm)"" NAME=""Debian GNU/Linux"" VERSION_ID=""12"" VERSION=""12 (bookworm)"" VERSION_CODENAME=bookworm ID=debian HOME_URL=""https://www.debian.org/"" SUPPORT_URL=""https://www.debian.org/support"" BUG_REPORT_URL=""https://bugs.debian.org/""

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Deployed though official helm chart on k8s 1.29 

### What happened

We are trying to connect to minio s3 object store bucket using apache-airflow-providers-amazon provider Connector.

1. Login to Airflow Webserver 
2. Admin -> connections -> Connection ID (Any Name you can provide )-> Connection Type  as Amazon Web services -> Extra
{
  ""aws_access_key_id"": ""XXXXXXXXXXXXXX"",
  ""aws_secret_access_key"": ""XXXXXXXXXXXXXXXXXXXXX"",
  ""endpoint_url"": ""https://minio-url.xxx.com:443"",
  ""verify"": false
}

When we test connection 

Error: 'EndpointConnectionError' error occurred while testing connection: Could not connect to the endpoint URL: ""https://sts.default.amazonaws.com/""

It is trying to connect https://sts.default.amazonaws.com/ for authentication and authorization

Why it is trying to connect https://sts.default.amazonaws.com/ ? instead of  ""endpoint_url"": ""https://minio-url.xxx.com:443"",?

### What you think should happen instead

It suppose to connect to minio endpoint_url   ""endpoint_url"": ""https://minio-url.xxx.com:443"", instead of connecting to https://sts.default.amazonaws.com

### How to reproduce

We are trying to connect to minio s3 object store bucket using apache-airflow-providers-amazon provider Connector.

1. Login to Airflow Webserver 
2. Admin -> connections -> Connection ID (Any Name you can provide )-> Connection Type  as Amazon Web services -> Extra
{
  ""aws_access_key_id"": ""XXXXXXXXXXXXXX"",
  ""aws_secret_access_key"": ""XXXXXXXXXXXXXXXXXXXXX"",
  ""endpoint_url"": ""https://minio-url.xxx.com:443"",
  ""verify"": false
}

When we test connection 

Error: 'EndpointConnectionError' error occurred while testing connection: Could not connect to the endpoint URL: ""https://sts.default.amazonaws.com/""

It is trying to connect https://sts.default.amazonaws.com/ for authentication and authorization

Why it is trying to connect https://sts.default.amazonaws.com/ ? instead of  ""endpoint_url"": ""https://minio-url.xxx.com:443"",?

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",mohankumar-h,2024-11-11 07:28:10+00:00,[],2024-11-11 11:33:14+00:00,2024-11-11 11:33:14+00:00,https://github.com/apache/airflow/issues/43870,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2467420673, 'issue_id': 2648422365, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 11, 7, 28, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2467954377, 'issue_id': 2648422365, 'author': 'eladkal', 'body': 'duplicate of https://github.com/apache/airflow/issues/43871', 'created_at': datetime.datetime(2024, 11, 11, 11, 33, 14, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-11 07:28:13 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2024-11-11 11:33:14 UTC): duplicate of https://github.com/apache/airflow/issues/43871

"
2648084682,issue,closed,completed,Add support of max log length for OTEL task log event,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When the OTEL_TASK_LOG_EVENT is enabled, scheduler job performs extraction of the task log as span event which would get added to the task span (https://github.com/apache/airflow/blob/cf3f9c7640a5f00af0a5117b146f32994daa6fe7/airflow/jobs/scheduler_job_runner.py#L805). However, there is a risk of log entry being extremely large, that it would easily produce message that may not fit into the span event. Also, there is a potential risk of halting scheduler from running as scheduler would be busy processing it.

### What you think should happen instead?

OTEL_TASK_LOG_EVENT should support MAX length of the log entry, such that it should have a safe default value of the max length (e.g. 64k characters), which will effectively trim the task log once that max limit is reached, preventing any operational risk of scheduler if the log message is too large to be practical.

### How to reproduce

Generate a huge log (hundreds of megabytes?) and observe scheduler to hang (or causing delay) processing the log as span event.

### Operating System

Linux (amd64)

### Versions of Apache Airflow Providers

N/A

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",howardyoo,2024-11-11 04:12:18+00:00,['howardyoo'],2024-11-12 21:58:21+00:00,2024-11-12 21:58:21+00:00,https://github.com/apache/airflow/issues/43868,"[('kind:bug', 'This is a clearly a bug'), ('area:logging', ''), ('kind:feature', 'Feature Requests'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2469088994, 'issue_id': 2648084682, 'author': 'potiuk', 'body': '+1', 'created_at': datetime.datetime(2024, 11, 11, 21, 43, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2471656110, 'issue_id': 2648084682, 'author': 'dstandish', 'body': ""-1\r\n\r\nIf you are talking about uploading task execution logs from the scheduler loop, this seems like still very much not a good idea.  Even if just a snippet of logs, you don't want to be retrieving connections, reading from s3, etc.  It's not the place to do that."", 'created_at': datetime.datetime(2024, 11, 12, 21, 50, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2471661911, 'issue_id': 2648084682, 'author': 'potiuk', 'body': ""> If you are talking about uploading task execution logs from the scheduler loop, this seems like still very much not a good idea. Even if just a snippet of logs, you don't want to be retrieving connections, reading from s3, etc. It's not the place to do that.\r\n\r\nWho is talking about doing it from scheduler loop?"", 'created_at': datetime.datetime(2024, 11, 12, 21, 54, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2471662772, 'issue_id': 2648084682, 'author': 'dstandish', 'body': ""that's where the code in question is right now and that's the context for this is it not?"", 'created_at': datetime.datetime(2024, 11, 12, 21, 55, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2471663303, 'issue_id': 2648084682, 'author': 'dstandish', 'body': 'look in the top of this issue @potiuk \r\n<img width=""698"" alt=""image"" src=""https://github.com/user-attachments/assets/eef7fc4b-0da4-429a-87ee-133981879c31"">', 'created_at': datetime.datetime(2024, 11, 12, 21, 55, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2471664114, 'issue_id': 2648084682, 'author': 'dstandish', 'body': ""so it sounds like this issue is just about reducing the size of the log message in the area where it's currently added.\r\n\r\nbut it cannot be added ther."", 'created_at': datetime.datetime(2024, 11, 12, 21, 56, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2471665221, 'issue_id': 2648084682, 'author': 'potiuk', 'body': ""> that's where the code in question is right now and that's the context for this is it not?\r\n\r\nOh, I have not seen that - this should be definitely done from task/worker.."", 'created_at': datetime.datetime(2024, 11, 12, 21, 56, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2471665832, 'issue_id': 2648084682, 'author': 'dstandish', 'body': '😌 (relieved face)', 'created_at': datetime.datetime(2024, 11, 12, 21, 57, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2471667613, 'issue_id': 2648084682, 'author': 'ashb', 'body': ""No, remove this feature entirely. Logs do not belong in Otel traces/spans. Use OTel logging and associate it with a span if that's what you want to do, but this code cannot exist in the scheduler."", 'created_at': datetime.datetime(2024, 11, 12, 21, 58, 21, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-11-11 21:43:23 UTC): +1

dstandish on (2024-11-12 21:50:43 UTC): -1

If you are talking about uploading task execution logs from the scheduler loop, this seems like still very much not a good idea.  Even if just a snippet of logs, you don't want to be retrieving connections, reading from s3, etc.  It's not the place to do that.

potiuk on (2024-11-12 21:54:31 UTC): Who is talking about doing it from scheduler loop?

dstandish on (2024-11-12 21:55:08 UTC): that's where the code in question is right now and that's the context for this is it not?

dstandish on (2024-11-12 21:55:27 UTC): look in the top of this issue @potiuk 
<img width=""698"" alt=""image"" src=""https://github.com/user-attachments/assets/eef7fc4b-0da4-429a-87ee-133981879c31"">

dstandish on (2024-11-12 21:56:01 UTC): so it sounds like this issue is just about reducing the size of the log message in the area where it's currently added.

but it cannot be added ther.

potiuk on (2024-11-12 21:56:45 UTC): Oh, I have not seen that - this should be definitely done from task/worker..

dstandish on (2024-11-12 21:57:09 UTC): 😌 (relieved face)

ashb on (2024-11-12 21:58:21 UTC): No, remove this feature entirely. Logs do not belong in Otel traces/spans. Use OTel logging and associate it with a span if that's what you want to do, but this code cannot exist in the scheduler.

"
2646113394,issue,closed,completed,AIP-84 Migrating ASSETS to Legacy API to fastAPI,"### Description

Migrate the following endpoint:

- GET Assets Event  
- Post Assets Event  
- GET queued events for assets
- DELETE queued events for assets


### Use case/motivation

[AIP-84](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API)

### Related issues

Relates to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vatsrahul1001,2024-11-09 14:09:22+00:00,['vatsrahul1001'],2024-11-19 18:03:27+00:00,2024-11-19 18:03:27+00:00,https://github.com/apache/airflow/issues/43845,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2475987828, 'issue_id': 2646113394, 'author': 'vatsrahul1001', 'body': 'Done [AIP 84: Migrate GET ASSET EVENTS legacy API to fast API #43881](https://github.com/apache/airflow/pull/43881)', 'created_at': datetime.datetime(2024, 11, 14, 10, 33, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2478578887, 'issue_id': 2646113394, 'author': 'vatsrahul1001', 'body': 'Raised following PR\r\n[AIP-84: Migrating DELETE queued asset events for assets to fastAPI](https://github.com/apache/airflow/pull/44052)\r\n[AIP-84: Migrating GET queued asset events for assets to fastAPI](https://github.com/apache/airflow/pull/44048)\r\n[AIP 84: Migrate POST ASSET EVENTS legacy API to fast API](https://github.com/apache/airflow/pull/43984)\r\n[AIP 84: Migrate GET ASSET EVENTS legacy API to fast API](https://github.com/apache/airflow/pull/43881)', 'created_at': datetime.datetime(2024, 11, 15, 11, 9, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2481975618, 'issue_id': 2646113394, 'author': 'vatsrahul1001', 'body': '**Merged**\r\nhttps://github.com/apache/airflow/pull/43984\r\nhttps://github.com/apache/airflow/pull/43881\r\n\r\n**Up for Review**\r\nhttps://github.com/apache/airflow/pull/44139\r\nhttps://github.com/apache/airflow/pull/44138', 'created_at': datetime.datetime(2024, 11, 18, 5, 16, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2486401774, 'issue_id': 2646113394, 'author': 'vatsrahul1001', 'body': 'All of above PR are merged closing this issue', 'created_at': datetime.datetime(2024, 11, 19, 18, 3, 27, tzinfo=datetime.timezone.utc)}]","vatsrahul1001 (Issue Creator) on (2024-11-14 10:33:57 UTC): Done [AIP 84: Migrate GET ASSET EVENTS legacy API to fast API #43881](https://github.com/apache/airflow/pull/43881)

vatsrahul1001 (Issue Creator) on (2024-11-15 11:09:02 UTC): Raised following PR
[AIP-84: Migrating DELETE queued asset events for assets to fastAPI](https://github.com/apache/airflow/pull/44052)
[AIP-84: Migrating GET queued asset events for assets to fastAPI](https://github.com/apache/airflow/pull/44048)
[AIP 84: Migrate POST ASSET EVENTS legacy API to fast API](https://github.com/apache/airflow/pull/43984)
[AIP 84: Migrate GET ASSET EVENTS legacy API to fast API](https://github.com/apache/airflow/pull/43881)

vatsrahul1001 (Issue Creator) on (2024-11-18 05:16:04 UTC): **Merged**
https://github.com/apache/airflow/pull/43984
https://github.com/apache/airflow/pull/43881

**Up for Review**
https://github.com/apache/airflow/pull/44139
https://github.com/apache/airflow/pull/44138

vatsrahul1001 (Issue Creator) on (2024-11-19 18:03:27 UTC): All of above PR are merged closing this issue

"
2645403065,issue,closed,completed,AIP-72: Add endpoints for XCom & Variables for Execution subapp,Part of https://github.com/orgs/apache/projects/405/views/1,kaxil,2024-11-09 00:00:40+00:00,['kaxil'],2024-11-12 12:35:52+00:00,2024-11-12 12:35:51+00:00,https://github.com/apache/airflow/issues/43839,"[('area:API', ""Airflow's REST/HTTP API""), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]","[{'comment_id': 2465918568, 'issue_id': 2645403065, 'author': 'kaxil', 'body': 'Variables merged here: https://github.com/apache/airflow/pull/43832', 'created_at': datetime.datetime(2024, 11, 9, 0, 1, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465918657, 'issue_id': 2645403065, 'author': 'kaxil', 'body': 'XCom is in progress', 'created_at': datetime.datetime(2024, 11, 9, 0, 1, 57, tzinfo=datetime.timezone.utc)}]","kaxil (Issue Creator) on (2024-11-09 00:01:49 UTC): Variables merged here: https://github.com/apache/airflow/pull/43832

kaxil (Issue Creator) on (2024-11-09 00:01:57 UTC): XCom is in progress

"
2644574128,issue,closed,not_planned,Metrics dagrun.duration.success/failed.<dag_id> are missing in statsd exporter for airflow deployed on Kubernetes cluster,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

v2.10.2

### What happened?

We deployed Airflow in a Kubernetes cluster through Helm Chart (https://github.com/apache/airflow/tree/main/chart) with statds enabled, the configuration for statsd is the default one.

The issue involves being able to see most metrics except for dagrun.duration.success.<dag_id> and dagrun.duration.failed.<dag_id>

### What you think should happen instead?

The metrics dagrun.duration.success.<dag_id> and dagrun.duration.failed.<dag_id> should be visible in the exporter

### How to reproduce

1.     Deploy Airflow on Kubernetes with the official helm chart
2.     Set up Statsd exporter for metric collection
2.     Schedule sample dag runs to generate metrics
3.     dagrun.duration.success.<dag_id> and dagrun.duration.failed.<dag_id> metrics will be not be available on the statds /metrics path

### Operating System

Oracle Linux 9

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

In this issue https://github.com/apache/airflow/issues/18630 you suggest to deploy a custom Airflow image in order to enable TCPStatsClient but I am unable to do that, is there any way to do such thing through helm chart values?

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",LSparkzwz,2024-11-08 16:47:46+00:00,[],2025-01-14 11:03:54+00:00,2024-11-27 15:40:35+00:00,https://github.com/apache/airflow/issues/43833,"[('kind:bug', 'This is a clearly a bug'), ('area:metrics', ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2465254942, 'issue_id': 2644574128, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 8, 16, 47, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2476817996, 'issue_id': 2644574128, 'author': 'rwhitley45', 'body': 'I\'m having the same issue with my airflow instance in K8s. Metric counts are way low:\r\n```\r\n# HELP statsd_exporter_metrics_total The total number of metrics.\r\n# TYPE statsd_exporter_metrics_total gauge\r\nstatsd_exporter_metrics_total{type=""counter""} 4\r\nstatsd_exporter_metrics_total{type=""gauge""} 17\r\nstatsd_exporter_metrics_total{type=""summary""} 11\r\n```', 'created_at': datetime.datetime(2024, 11, 14, 16, 9, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2504193331, 'issue_id': 2644574128, 'author': 'LSparkzwz', 'body': 'up', 'created_at': datetime.datetime(2024, 11, 27, 15, 40, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2589623240, 'issue_id': 2644574128, 'author': 'RyanWang0811', 'body': 'I find the helm chart statsd have mapping config setting\nhttps://github.com/apache/airflow/blob/main/chart/values.yaml#L2088\n\nthe `dagrun.duration.success.<dag_id>` and `dagrun.duration.failed.<dag_id>` have been transfer.\nhttps://github.com/apache/airflow/blob/main/chart/files/statsd-mappings.yml#L61-L69', 'created_at': datetime.datetime(2025, 1, 14, 11, 3, 52, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-08 16:47:49 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

rwhitley45 on (2024-11-14 16:09:21 UTC): I'm having the same issue with my airflow instance in K8s. Metric counts are way low:
```
# HELP statsd_exporter_metrics_total The total number of metrics.
# TYPE statsd_exporter_metrics_total gauge
statsd_exporter_metrics_total{type=""counter""} 4
statsd_exporter_metrics_total{type=""gauge""} 17
statsd_exporter_metrics_total{type=""summary""} 11
```

LSparkzwz (Issue Creator) on (2024-11-27 15:40:51 UTC): up

RyanWang0811 on (2025-01-14 11:03:52 UTC): I find the helm chart statsd have mapping config setting
https://github.com/apache/airflow/blob/main/chart/values.yaml#L2088

the `dagrun.duration.success.<dag_id>` and `dagrun.duration.failed.<dag_id>` have been transfer.
https://github.com/apache/airflow/blob/main/chart/files/statsd-mappings.yml#L61-L69

"
2643982577,issue,open,,Include deferral in Task Instance Duration monitor,"### Description

The Task Instance Duration monitor displays task duration, distinguishing queued duration and total duration. 
However, we have a task that spends a lot of time in deferred state, and would like  the monitor to display that too (in a similar fashion as the queing time). 
Actually, after being deferred the task can get queued again, and I think the monitor currently shows the _last time_ the task was queued, rather than the total time the task instance spent in queued state. In fact, I guess you could consider that a bug. 


### Use case/motivation

We have an external service that a task depends on. If the Airflow monitor can display deferred state duration, we would be able monitor the service's response time through the Airflow UI. 

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Veldhoen,2024-11-08 13:02:26+00:00,[],2024-11-11 14:00:38+00:00,,https://github.com/apache/airflow/issues/43822,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2464702960, 'issue_id': 2643982577, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 8, 13, 2, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2468247623, 'issue_id': 2643982577, 'author': 'potiuk', 'body': ""It might be diffifcult one to get and it's a feature - so it might only be implemented (if someone picks it up) in Airflow 3.1+"", 'created_at': datetime.datetime(2024, 11, 11, 13, 59, 49, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-08 13:02:30 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-11-11 13:59:49 UTC): It might be diffifcult one to get and it's a feature - so it might only be implemented (if someone picks it up) in Airflow 3.1+

"
2642256273,issue,closed,completed,Updating to latest release fails with db migration,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Updating to latest `ghcr.io/apache/airflow/main/prod/python3.11:latest` version, a db migration is required which fails:

```
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedObject) constraint ""dsdar_dag_id_fkey"" of relation ""dag_schedule_asset_alias_reference"" does not exist

[SQL: ALTER TABLE dag_schedule_asset_alias_reference DROP CONSTRAINT dsdar_dag_id_fkey]
```

### What you think should happen instead?

Expected behaviour is a successful database migration.

### How to reproduce

Update to latest docker image.

### Operating System

Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

ghcr.io/apache/airflow/main/prod/python3.11:latest

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",prutheus,2024-11-07 21:34:49+00:00,['rawwar'],2024-11-17 17:24:56+00:00,2024-11-17 17:24:56+00:00,https://github.com/apache/airflow/issues/43806,"[(""won't fix"", ''), ('pending-response', ''), ('area:core', ''), ('area:db-migrations', 'PRs with DB migration')]","[{'comment_id': 2464629232, 'issue_id': 2642256273, 'author': 'rawwar', 'body': 'Can you please give info on your earlier airflow version ?', 'created_at': datetime.datetime(2024, 11, 8, 12, 23, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2464822520, 'issue_id': 2642256273, 'author': 'prutheus', 'body': '> Can you please give info on your earlier airflow version ?\r\n\r\n@rawwar the earlier version I was using was based on commit https://github.com/apache/airflow/commit/4323b1dd7d3a6603b7cc15056f06f5bf2de0569d (ghcr.io/apache/airflow/main/prod/python3.11:4323b1dd7d3a6603b7cc15056f06f5bf2de0569d)\r\n\r\nI have updated because I had experienced increase issues with:\r\n![image](https://github.com/user-attachments/assets/450e2c1a-f1c1-446b-a9c4-eeac469e3a54)\r\n\r\nWhere it turned out, that the user identifier was not added to a dag run causing this issue:\r\n```\r\n2024-11-07 10:42:42.265\t    ""triggered_by"": dag_run.triggered_by.value,\r\n2024-11-07 10:42:42.265\t                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-11-07 10:42:42.265\tAttributeError: \'NoneType\' object has no attribute \'value\'\r\n\r\n```\r\n\r\nI was hoping to get this resolved by updating to current latest, for which DB migration failed.', 'created_at': datetime.datetime(2024, 11, 8, 13, 53, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2468043443, 'issue_id': 2642256273, 'author': 'rawwar', 'body': 'I tried to replicate this issue. When I ran `docker compose up airflow-init`, it gave the following error:\r\n\r\n```\r\nairflow-init-1  | DB: postgresql+psycopg2://airflow:***@postgres/airflow\r\nairflow-init-1  | Performing upgrade to the metadata database postgresql+psycopg2://airflow:***@postgres/airflow\r\nairflow-init-1  | [2024-11-11T11:44:17.511+0000] {migration.py:207} INFO - Context impl PostgresqlImpl.\r\nairflow-init-1  | [2024-11-11T11:44:17.513+0000] {migration.py:210} INFO - Will assume transactional DDL.\r\nairflow-init-1  | [2024-11-11T11:44:17.516+0000] {migration.py:207} INFO - Context impl PostgresqlImpl.\r\nairflow-init-1  | [2024-11-11T11:44:17.517+0000] {migration.py:210} INFO - Will assume transactional DDL.\r\nairflow-init-1  | [2024-11-11T11:44:17.517+0000] {db.py:1174} INFO - Migrating the Airflow database\r\nairflow-init-1  | [2024-11-11T11:44:17.525+0000] {migration.py:207} INFO - Context impl PostgresqlImpl.\r\nairflow-init-1  | [2024-11-11T11:44:17.525+0000] {migration.py:210} INFO - Will assume transactional DDL.\r\nairflow-init-1  | [2024-11-11T11:44:17.544+0000] {migration.py:618} INFO - Running upgrade 1cdc775ca98f -> 522625f6d606, Add tables for backfill.\r\nairflow-init-1  | [2024-11-11T11:44:17.558+0000] {migration.py:618} INFO - Running upgrade 522625f6d606 -> 16cbcb1c8c36, Remove redundant index.\r\nairflow-init-1  | [2024-11-11T11:44:17.560+0000] {migration.py:618} INFO - Running upgrade 16cbcb1c8c36 -> 44eabb1904b4, Update dag_run_note.user_id and task_instance_note.user_id columns to String.\r\nairflow-init-1  | [2024-11-11T11:44:17.585+0000] {migration.py:618} INFO - Running upgrade 44eabb1904b4 -> 0d9e73a75ee4, Add name and group fields to DatasetModel.\r\nairflow-init-1  | [2024-11-11T11:44:17.611+0000] {migration.py:618} INFO - Running upgrade 0d9e73a75ee4 -> c3389cd7793f, Add backfill to dag run model.\r\nairflow-init-1  | [2024-11-11T11:44:17.615+0000] {migration.py:618} INFO - Running upgrade c3389cd7793f -> 5a5d66100783, Add AssetActive to track orphaning instead of a flag.\r\nairflow-init-1  | [2024-11-11T11:44:17.625+0000] {migration.py:618} INFO - Running upgrade 5a5d66100783 -> fb2d4922cd79, Tweak AssetAliasModel to match AssetModel after AIP-76.\r\nairflow-init-1  | Traceback (most recent call last):\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context\r\nairflow-init-1  |     self.dialect.do_execute(\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/default.py"", line 736, in do_execute\r\nairflow-init-1  |     cursor.execute(statement, parameters)\r\nairflow-init-1  | psycopg2.errors.NotNullViolation: column ""group"" of relation ""dataset_alias"" contains null values\r\nairflow-init-1  | \r\nairflow-init-1  | \r\nairflow-init-1  | The above exception was the direct cause of the following exception:\r\nairflow-init-1  | \r\nairflow-init-1  | Traceback (most recent call last):\r\nairflow-init-1  |   File ""/home/airflow/.local/bin/airflow"", line 8, in <module>\r\nairflow-init-1  |     sys.exit(main())\r\nairflow-init-1  |              ^^^^^^\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/__main__.py"", line 62, in main\r\nairflow-init-1  |     args.func(args)\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/cli_config.py"", line 49, in command\r\nairflow-init-1  |     return func(*args, **kwargs)\r\nairflow-init-1  |            ^^^^^^^^^^^^^^^^^^^^^\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/cli.py"", line 112, in wrapper\r\nairflow-init-1  |     return f(*args, **kwargs)\r\nairflow-init-1  |            ^^^^^^^^^^^^^^^^^^\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/providers_configuration_loader.py"", line 55, in wrapped_function\r\nairflow-init-1  |     return func(*args, **kwargs)\r\nairflow-init-1  |            ^^^^^^^^^^^^^^^^^^^^^\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/commands/db_command.py"", line 227, in migratedb\r\nairflow-init-1  |     run_db_migrate_command(args, db.upgradedb, _REVISION_HEADS_MAP, reserialize_dags=True)\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/commands/db_command.py"", line 148, in run_db_migrate_command\r\nairflow-init-1  |     command(\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py"", line 97, in wrapper\r\nairflow-init-1  |     return func(*args, session=session, **kwargs)\r\nairflow-init-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/db.py"", line 1181, in upgradedb\r\nairflow-init-1  |     command.upgrade(config, revision=to_revision or ""heads"")\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/alembic/command.py"", line 406, in upgrade\r\nairflow-init-1  |     script.run_env()\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/alembic/script/base.py"", line 586, in run_env\r\nairflow-init-1  |     util.load_python_file(self.dir, ""env.py"")\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/alembic/util/pyfiles.py"", line 95, in load_python_file\r\nairflow-init-1  |     module = load_module_py(module_id, path)\r\nairflow-init-1  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/alembic/util/pyfiles.py"", line 113, in load_module_py\r\nairflow-init-1  |     spec.loader.exec_module(module)  # type: ignore\r\nairflow-init-1  |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nairflow-init-1  |   File ""<frozen importlib._bootstrap_external>"", line 940, in exec_module\r\nairflow-init-1  |   File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/migrations/env.py"", line 139, in <module>\r\nairflow-init-1  |     run_migrations_online()\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/migrations/env.py"", line 133, in run_migrations_online\r\nairflow-init-1  |     context.run_migrations()\r\nairflow-init-1  |   File ""<string>"", line 8, in run_migrations\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/alembic/runtime/environment.py"", line 946, in run_migrations\r\nairflow-init-1  |     self.get_context().run_migrations(**kw)\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/alembic/runtime/migration.py"", line 623, in run_migrations\r\nairflow-init-1  |     step.migration_fn(**kw)\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/migrations/versions/0039_3_0_0_tweak_assetaliasmodel_to_match_asset.py"", line 60, in upgrade\r\nairflow-init-1  |     with op.batch_alter_table(""dataset_alias"", schema=None) as batch_op:\r\nairflow-init-1  |   File ""/usr/local/lib/python3.11/contextlib.py"", line 144, in __exit__\r\nairflow-init-1  |     next(self.gen)\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/alembic/operations/base.py"", line 398, in batch_alter_table\r\nairflow-init-1  |     impl.flush()\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/alembic/operations/batch.py"", line 116, in flush\r\nairflow-init-1  |     fn(*arg, **kw)\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/alembic/ddl/impl.py"", line 374, in add_column\r\nairflow-init-1  |     self._exec(base.AddColumn(table_name, column, schema=schema))\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/alembic/ddl/impl.py"", line 247, in _exec\r\nairflow-init-1  |     return conn.execute(construct, params)\r\nairflow-init-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/future/engine.py"", line 286, in execute\r\nairflow-init-1  |     return self._execute_20(\r\nairflow-init-1  |            ^^^^^^^^^^^^^^^^^\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1710, in _execute_20\r\nairflow-init-1  |     return meth(self, args_10style, kwargs_10style, execution_options)\r\nairflow-init-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/sql/ddl.py"", line 80, in _execute_on_connection\r\nairflow-init-1  |     return connection._execute_ddl(\r\nairflow-init-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1477, in _execute_ddl\r\nairflow-init-1  |     ret = self._execute_context(\r\nairflow-init-1  |           ^^^^^^^^^^^^^^^^^^^^^^\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1953, in _execute_context\r\nairflow-init-1  |     self._handle_dbapi_exception(\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 2134, in _handle_dbapi_exception\r\nairflow-init-1  |     util.raise_(\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/util/compat.py"", line 211, in raise_\r\nairflow-init-1  |     raise exception\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context\r\nairflow-init-1  |     self.dialect.do_execute(\r\nairflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/default.py"", line 736, in do_execute\r\nairflow-init-1  |     cursor.execute(statement, parameters)\r\nairflow-init-1  | sqlalchemy.exc.IntegrityError: (psycopg2.errors.NotNullViolation) column ""group"" of relation ""dataset_alias"" contains null values\r\nairflow-init-1  | \r\nairflow-init-1  | [SQL: ALTER TABLE dataset_alias ADD COLUMN ""group"" VARCHAR(1500) NOT NULL]\r\nairflow-init-1  | (Background on this error at: https://sqlalche.me/e/14/gkpj)\r\nairflow-init-1  | ERROR: You need to migrate the database. Please run `airflow db migrate`. Make sure the command is run using Airflow version 3.0.0.dev0.\r\nairflow-init-1  | 3.0.0.dev0\r\nairflow-init-1 exited with code 0\r\n```\r\n\r\n\r\nEDIT: \r\n\r\nI think PR https://github.com/apache/airflow/pull/43313 fixed this issue. But, In the latest image, I don\'t see this migration', 'created_at': datetime.datetime(2024, 11, 11, 12, 18, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469048801, 'issue_id': 2642256273, 'author': 'prutheus', 'body': 'The issue\r\n```\r\nsqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedObject) constraint ""dsdar_dag_id_fkey"" of relation ""dag_schedule_asset_alias_reference"" does not exist\r\n\r\n[SQL: ALTER TABLE dag_schedule_asset_alias_reference DROP CONSTRAINT dsdar_dag_id_fkey]\r\n```\r\n\r\nseems to have an invalid fkey definition for the table `dag_schedule_asset_alias_reference`.\r\n\r\nScanning migrations for this (https://github.com/search?q=repo%3Aapache%2Fairflow+dsdar_dag_id_fkey&type=code), I found https://github.com/apache/airflow/blob/9ee9e52dc6e6355cd655636caadbab739cae2546/airflow/migrations/versions/0041_3_0_0_rename_dataset_as_asset.py#L198\r\n\r\nIt seems that the rename migration is missing.\r\n\r\n```\r\nairflow=# \\d dag_schedule_dataset_alias_reference\r\n          Table ""public.dag_schedule_dataset_alias_reference""\r\n   Column   |           Type           | Collation | Nullable | Default \r\n------------+--------------------------+-----------+----------+---------\r\n alias_id   | integer                  |           | not null | \r\n dag_id     | character varying(250)   |           | not null | \r\n created_at | timestamp with time zone |           | not null | \r\n updated_at | timestamp with time zone |           | not null | \r\nIndexes:\r\n    ""dsdar_pkey"" PRIMARY KEY, btree (alias_id, dag_id)\r\n    ""idx_dag_schedule_dataset_alias_reference_dag_id"" btree (dag_id)\r\nForeign-key constraints:\r\n    ""dsdar_dag_fkey"" FOREIGN KEY (dag_id) REFERENCES dag(dag_id) ON DELETE CASCADE\r\n    ""dsdar_dataset_alias_fkey"" FOREIGN KEY (alias_id) REFERENCES dataset_alias(id) ON DELETE CASCADE\r\n```\r\n\r\nI have `dsdar_dag_fkey` in my table `dag_schedule_dataset_alias_reference` and not `dsdar_dag_id_fkey`.\r\n\r\n**Resolved** by re-creating the fkey with the new name:\r\n\r\n```\r\n$ ALTER TABLE dag_schedule_dataset_alias_reference DROP CONSTRAINT  dsdar_dag_fkey;\r\n$ ALTER TABLE dag_schedule_dataset_alias_reference ADD CONSTRAINT dsdar_dag_id_fkey FOREIGN KEY (dag_id) REFERENCES dag(dag_id) ON DELETE CASCADE;\r\n```', 'created_at': datetime.datetime(2024, 11, 11, 21, 27, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2481312062, 'issue_id': 2642256273, 'author': 'rawwar', 'body': ""@prutheus , can we go ahead and close this issue? As migrations between main can be broken and these aren't fixed as stability is maintained between released versions."", 'created_at': datetime.datetime(2024, 11, 17, 15, 15, 39, tzinfo=datetime.timezone.utc)}]","rawwar (Assginee) on (2024-11-08 12:23:19 UTC): Can you please give info on your earlier airflow version ?

prutheus (Issue Creator) on (2024-11-08 13:53:08 UTC): @rawwar the earlier version I was using was based on commit https://github.com/apache/airflow/commit/4323b1dd7d3a6603b7cc15056f06f5bf2de0569d (ghcr.io/apache/airflow/main/prod/python3.11:4323b1dd7d3a6603b7cc15056f06f5bf2de0569d)

I have updated because I had experienced increase issues with:
![image](https://github.com/user-attachments/assets/450e2c1a-f1c1-446b-a9c4-eeac469e3a54)

Where it turned out, that the user identifier was not added to a dag run causing this issue:
```
2024-11-07 10:42:42.265	    ""triggered_by"": dag_run.triggered_by.value,
2024-11-07 10:42:42.265	                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-11-07 10:42:42.265	AttributeError: 'NoneType' object has no attribute 'value'

```

I was hoping to get this resolved by updating to current latest, for which DB migration failed.

rawwar (Assginee) on (2024-11-11 12:18:52 UTC): I tried to replicate this issue. When I ran `docker compose up airflow-init`, it gave the following error:

```
airflow-init-1  | DB: postgresql+psycopg2://airflow:***@postgres/airflow
airflow-init-1  | Performing upgrade to the metadata database postgresql+psycopg2://airflow:***@postgres/airflow
airflow-init-1  | [2024-11-11T11:44:17.511+0000] {migration.py:207} INFO - Context impl PostgresqlImpl.
airflow-init-1  | [2024-11-11T11:44:17.513+0000] {migration.py:210} INFO - Will assume transactional DDL.
airflow-init-1  | [2024-11-11T11:44:17.516+0000] {migration.py:207} INFO - Context impl PostgresqlImpl.
airflow-init-1  | [2024-11-11T11:44:17.517+0000] {migration.py:210} INFO - Will assume transactional DDL.
airflow-init-1  | [2024-11-11T11:44:17.517+0000] {db.py:1174} INFO - Migrating the Airflow database
airflow-init-1  | [2024-11-11T11:44:17.525+0000] {migration.py:207} INFO - Context impl PostgresqlImpl.
airflow-init-1  | [2024-11-11T11:44:17.525+0000] {migration.py:210} INFO - Will assume transactional DDL.
airflow-init-1  | [2024-11-11T11:44:17.544+0000] {migration.py:618} INFO - Running upgrade 1cdc775ca98f -> 522625f6d606, Add tables for backfill.
airflow-init-1  | [2024-11-11T11:44:17.558+0000] {migration.py:618} INFO - Running upgrade 522625f6d606 -> 16cbcb1c8c36, Remove redundant index.
airflow-init-1  | [2024-11-11T11:44:17.560+0000] {migration.py:618} INFO - Running upgrade 16cbcb1c8c36 -> 44eabb1904b4, Update dag_run_note.user_id and task_instance_note.user_id columns to String.
airflow-init-1  | [2024-11-11T11:44:17.585+0000] {migration.py:618} INFO - Running upgrade 44eabb1904b4 -> 0d9e73a75ee4, Add name and group fields to DatasetModel.
airflow-init-1  | [2024-11-11T11:44:17.611+0000] {migration.py:618} INFO - Running upgrade 0d9e73a75ee4 -> c3389cd7793f, Add backfill to dag run model.
airflow-init-1  | [2024-11-11T11:44:17.615+0000] {migration.py:618} INFO - Running upgrade c3389cd7793f -> 5a5d66100783, Add AssetActive to track orphaning instead of a flag.
airflow-init-1  | [2024-11-11T11:44:17.625+0000] {migration.py:618} INFO - Running upgrade 5a5d66100783 -> fb2d4922cd79, Tweak AssetAliasModel to match AssetModel after AIP-76.
airflow-init-1  | Traceback (most recent call last):
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context
airflow-init-1  |     self.dialect.do_execute(
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/default.py"", line 736, in do_execute
airflow-init-1  |     cursor.execute(statement, parameters)
airflow-init-1  | psycopg2.errors.NotNullViolation: column ""group"" of relation ""dataset_alias"" contains null values
airflow-init-1  | 
airflow-init-1  | 
airflow-init-1  | The above exception was the direct cause of the following exception:
airflow-init-1  | 
airflow-init-1  | Traceback (most recent call last):
airflow-init-1  |   File ""/home/airflow/.local/bin/airflow"", line 8, in <module>
airflow-init-1  |     sys.exit(main())
airflow-init-1  |              ^^^^^^
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/__main__.py"", line 62, in main
airflow-init-1  |     args.func(args)
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/cli_config.py"", line 49, in command
airflow-init-1  |     return func(*args, **kwargs)
airflow-init-1  |            ^^^^^^^^^^^^^^^^^^^^^
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/cli.py"", line 112, in wrapper
airflow-init-1  |     return f(*args, **kwargs)
airflow-init-1  |            ^^^^^^^^^^^^^^^^^^
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/providers_configuration_loader.py"", line 55, in wrapped_function
airflow-init-1  |     return func(*args, **kwargs)
airflow-init-1  |            ^^^^^^^^^^^^^^^^^^^^^
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/commands/db_command.py"", line 227, in migratedb
airflow-init-1  |     run_db_migrate_command(args, db.upgradedb, _REVISION_HEADS_MAP, reserialize_dags=True)
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/commands/db_command.py"", line 148, in run_db_migrate_command
airflow-init-1  |     command(
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py"", line 97, in wrapper
airflow-init-1  |     return func(*args, session=session, **kwargs)
airflow-init-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/db.py"", line 1181, in upgradedb
airflow-init-1  |     command.upgrade(config, revision=to_revision or ""heads"")
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/alembic/command.py"", line 406, in upgrade
airflow-init-1  |     script.run_env()
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/alembic/script/base.py"", line 586, in run_env
airflow-init-1  |     util.load_python_file(self.dir, ""env.py"")
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/alembic/util/pyfiles.py"", line 95, in load_python_file
airflow-init-1  |     module = load_module_py(module_id, path)
airflow-init-1  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/alembic/util/pyfiles.py"", line 113, in load_module_py
airflow-init-1  |     spec.loader.exec_module(module)  # type: ignore
airflow-init-1  |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
airflow-init-1  |   File ""<frozen importlib._bootstrap_external>"", line 940, in exec_module
airflow-init-1  |   File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/migrations/env.py"", line 139, in <module>
airflow-init-1  |     run_migrations_online()
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/migrations/env.py"", line 133, in run_migrations_online
airflow-init-1  |     context.run_migrations()
airflow-init-1  |   File ""<string>"", line 8, in run_migrations
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/alembic/runtime/environment.py"", line 946, in run_migrations
airflow-init-1  |     self.get_context().run_migrations(**kw)
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/alembic/runtime/migration.py"", line 623, in run_migrations
airflow-init-1  |     step.migration_fn(**kw)
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/migrations/versions/0039_3_0_0_tweak_assetaliasmodel_to_match_asset.py"", line 60, in upgrade
airflow-init-1  |     with op.batch_alter_table(""dataset_alias"", schema=None) as batch_op:
airflow-init-1  |   File ""/usr/local/lib/python3.11/contextlib.py"", line 144, in __exit__
airflow-init-1  |     next(self.gen)
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/alembic/operations/base.py"", line 398, in batch_alter_table
airflow-init-1  |     impl.flush()
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/alembic/operations/batch.py"", line 116, in flush
airflow-init-1  |     fn(*arg, **kw)
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/alembic/ddl/impl.py"", line 374, in add_column
airflow-init-1  |     self._exec(base.AddColumn(table_name, column, schema=schema))
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/alembic/ddl/impl.py"", line 247, in _exec
airflow-init-1  |     return conn.execute(construct, params)
airflow-init-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/future/engine.py"", line 286, in execute
airflow-init-1  |     return self._execute_20(
airflow-init-1  |            ^^^^^^^^^^^^^^^^^
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1710, in _execute_20
airflow-init-1  |     return meth(self, args_10style, kwargs_10style, execution_options)
airflow-init-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/sql/ddl.py"", line 80, in _execute_on_connection
airflow-init-1  |     return connection._execute_ddl(
airflow-init-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1477, in _execute_ddl
airflow-init-1  |     ret = self._execute_context(
airflow-init-1  |           ^^^^^^^^^^^^^^^^^^^^^^
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1953, in _execute_context
airflow-init-1  |     self._handle_dbapi_exception(
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 2134, in _handle_dbapi_exception
airflow-init-1  |     util.raise_(
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/util/compat.py"", line 211, in raise_
airflow-init-1  |     raise exception
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context
airflow-init-1  |     self.dialect.do_execute(
airflow-init-1  |   File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/engine/default.py"", line 736, in do_execute
airflow-init-1  |     cursor.execute(statement, parameters)
airflow-init-1  | sqlalchemy.exc.IntegrityError: (psycopg2.errors.NotNullViolation) column ""group"" of relation ""dataset_alias"" contains null values
airflow-init-1  | 
airflow-init-1  | [SQL: ALTER TABLE dataset_alias ADD COLUMN ""group"" VARCHAR(1500) NOT NULL]
airflow-init-1  | (Background on this error at: https://sqlalche.me/e/14/gkpj)
airflow-init-1  | ERROR: You need to migrate the database. Please run `airflow db migrate`. Make sure the command is run using Airflow version 3.0.0.dev0.
airflow-init-1  | 3.0.0.dev0
airflow-init-1 exited with code 0
```


EDIT: 

I think PR https://github.com/apache/airflow/pull/43313 fixed this issue. But, In the latest image, I don't see this migration

prutheus (Issue Creator) on (2024-11-11 21:27:39 UTC): The issue
```
sqlalchemy.exc.ProgrammingError: (psycopg2.errors.UndefinedObject) constraint ""dsdar_dag_id_fkey"" of relation ""dag_schedule_asset_alias_reference"" does not exist

[SQL: ALTER TABLE dag_schedule_asset_alias_reference DROP CONSTRAINT dsdar_dag_id_fkey]
```

seems to have an invalid fkey definition for the table `dag_schedule_asset_alias_reference`.

Scanning migrations for this (https://github.com/search?q=repo%3Aapache%2Fairflow+dsdar_dag_id_fkey&type=code), I found https://github.com/apache/airflow/blob/9ee9e52dc6e6355cd655636caadbab739cae2546/airflow/migrations/versions/0041_3_0_0_rename_dataset_as_asset.py#L198

It seems that the rename migration is missing.

```
airflow=# \d dag_schedule_dataset_alias_reference
          Table ""public.dag_schedule_dataset_alias_reference""
   Column   |           Type           | Collation | Nullable | Default 
------------+--------------------------+-----------+----------+---------
 alias_id   | integer                  |           | not null | 
 dag_id     | character varying(250)   |           | not null | 
 created_at | timestamp with time zone |           | not null | 
 updated_at | timestamp with time zone |           | not null | 
Indexes:
    ""dsdar_pkey"" PRIMARY KEY, btree (alias_id, dag_id)
    ""idx_dag_schedule_dataset_alias_reference_dag_id"" btree (dag_id)
Foreign-key constraints:
    ""dsdar_dag_fkey"" FOREIGN KEY (dag_id) REFERENCES dag(dag_id) ON DELETE CASCADE
    ""dsdar_dataset_alias_fkey"" FOREIGN KEY (alias_id) REFERENCES dataset_alias(id) ON DELETE CASCADE
```

I have `dsdar_dag_fkey` in my table `dag_schedule_dataset_alias_reference` and not `dsdar_dag_id_fkey`.

**Resolved** by re-creating the fkey with the new name:

```
$ ALTER TABLE dag_schedule_dataset_alias_reference DROP CONSTRAINT  dsdar_dag_fkey;
$ ALTER TABLE dag_schedule_dataset_alias_reference ADD CONSTRAINT dsdar_dag_id_fkey FOREIGN KEY (dag_id) REFERENCES dag(dag_id) ON DELETE CASCADE;
```

rawwar (Assginee) on (2024-11-17 15:15:39 UTC): @prutheus , can we go ahead and close this issue? As migrations between main can be broken and these aren't fixed as stability is maintained between released versions.

"
2641977034,issue,open,,Add SPARK_APPLICATION_NAME env variable to driver and executor pods when using SparkKubernetesOperator,"### Description

I need to add an env variable in my spark pods that exposes the `SPARK_APPLICATION_NAME`, the issue here is that the operator adds a random suffix at the end, so I can't really add it at the level of the scheduler since it doesn't have access to the new name.

### Use case/motivation

I want the SparkKubernetesOperator to add an env variable containing the `SPARK_APPLICATION_NAME`, or make the random suffix fixed across the same dag run (exposing it as a macro?)


### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Flametaa,2024-11-07 19:27:40+00:00,['Flametaa'],2024-11-11 12:17:15+00:00,,https://github.com/apache/airflow/issues/43801,"[('area:providers', ''), ('kind:feature', 'Feature Requests'), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('contributors-workshop', ""Issues that are good for first-time contributor's workshop""), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:apache-spark', '')]","[{'comment_id': 2468040254, 'issue_id': 2641977034, 'author': 'potiuk', 'body': 'assigned you @Flametaa', 'created_at': datetime.datetime(2024, 11, 11, 12, 17, 13, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-11-11 12:17:13 UTC): assigned you @Flametaa

"
2641916082,issue,closed,completed,random_name_suffix is not working for sparkKubernetesOperator,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

In sparkKuberneteOperator, it doesn't matter if you set `random_name_suffix` or not as a random suffix is always added without checking this parameter (even though the documentation says that it should be controllable). https://github.com/apache/airflow/blob/providers-cncf-kubernetes/9.0.1/providers/src/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py#L166

### What you think should happen instead?

i think setting the parameter to False should result in the sparkApplication name being the same as the `name` 

### How to reproduce

```SparkKubernetesOperator(random_name_suffix=False,...)```

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Flametaa,2024-11-07 19:00:36+00:00,[],2024-11-09 17:22:39+00:00,2024-11-09 17:22:39+00:00,https://github.com/apache/airflow/issues/43800,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:apache-spark', '')]","[{'comment_id': 2463001890, 'issue_id': 2641916082, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 7, 19, 0, 39, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-07 19:00:39 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2641786296,issue,closed,completed,MsSql hook does not use extra parameters when creating PymssqlConnection,"### Apache Airflow Provider(s)

microsoft-mssql

### Versions of Apache Airflow Providers

apache-airflow-providers-microsoft-mssql==3.9.1

### Apache Airflow version

2.9.2

### Operating System

Ubuntu 22.04

### Deployment

Docker-Compose

### Deployment details

_No response_

### What happened

I found that [MsSqlHook](https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/microsoft/mssql/hooks/mssql.py#L31) in [get_conn](https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/microsoft/mssql/hooks/mssql.py#L137) method does not use extra parameters from airflow connection when creating `PymssqlConnection`. Thus, it is impossible to pass any special argument from the extra parameters of the airflow connection to `pymssql.connect` function.
I discovered this when I needed to pass the `tds_version` argument, because without it the connection to my database did not occur successfully.
Also, the documentation says that extra parameters [can be used in MSSQL connection](https://airflow.apache.org/docs/apache-airflow-providers-microsoft-mssql/stable/connections/mssql.html#:~:text=can%20be%20used%20in%20MSSQL%20connection), but in fact this is not the case.

### What you think should happen instead

I think the [get_conn](https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/microsoft/mssql/hooks/mssql.py#L137) method should look something like this:
```python
def get_conn(self) -> PymssqlConnection:
    """"""Return ``pymssql`` connection object.""""""
    conn = self.connection
    extra_conn_args = {arg_name: arg_val for arg_name, arg_val in conn.extra_dejson.items() if arg_name != ""sqlalchemy_scheme""}  # new code line
    return pymssql.connect(
        server=conn.host,
        user=conn.login,
        password=conn.password,
        database=self.schema or conn.schema,
        port=str(conn.port),
        **extra_conn_args,  # new code line
    )
```
I tested it locally, and with this code the extra parameters are successfully passed to `PymssqlConnection`. And in my case the connection to the DB was successful.

I would also like to note that I added an excluding for the `sqlalchemy_scheme` parameter, because in the hook it is used in the [sqlalchemy_scheme](https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/microsoft/mssql/hooks/mssql.py#L59) property, and in the `pymssql.connect` function it is not needed.

### How to reproduce

To reproduce the bug, you should try to add any extra parameter to the airflow connection. And this will not affect the connection to the database, because the hook does not use extra parameters to create a connection to the database.

### Anything else

I could probably create a PR with a fix, but since I'm new here, I assume that there might be a catch, and maybe it was intended that extra parameters not be passed to the pymssql connection (if so I wonder why). So, for now, I decided to just create an issue.

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",ilarionkuleshov,2024-11-07 17:59:37+00:00,"['jx2lee', 'ilarionkuleshov']",2024-11-26 07:47:18+00:00,2024-11-26 07:47:18+00:00,https://github.com/apache/airflow/issues/43798,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('provider:microsoft-mssql', '')]","[{'comment_id': 2462894449, 'issue_id': 2641786296, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 7, 17, 59, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2482747476, 'issue_id': 2641786296, 'author': 'jx2lee', 'body': '@potiuk Hi, Could i take this? assign to me please ✈️', 'created_at': datetime.datetime(2024, 11, 18, 11, 17, 48, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-07 17:59:40 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jx2lee (Assginee) on (2024-11-18 11:17:48 UTC): @potiuk Hi, Could i take this? assign to me please ✈️

"
2641638864,issue,closed,completed,Incorrect FutureWarning possibly caused by a missing PR in 2.10.3,"### Apache Airflow version

2.10.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

In Airflow 2.10.3 `configuration.py` file, we have

https://github.com/apache/airflow/blob/c99887ec11ce3e1a43f2794fcf36d27555140f00/airflow/configuration.py#L857-L859

The `self.sensitive_config_values` returns both `('database', 'sql_alchemy_conn')` and `('core', 'sql_alchemy_conn')` because in 2.10.3 the [deprecated_options](https://github.com/apache/airflow/blob/c99887ec11ce3e1a43f2794fcf36d27555140f00/airflow/configuration.py#L328) variable is not empty. I think it's supposed to be removed. See https://github.com/apache/airflow/pull/42126.

Hence the `self.get()` raises a warning:

```
airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name
```

Which confuses our users.

### What you think should happen instead?

The warning should not be raised.

### How to reproduce

1. `pip` Install Airflow 2.10.3
2. Run cli `$ airflow version`

### Operating System

Debian GNU/Linux 10 (buster)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",zachliu,2024-11-07 16:57:15+00:00,[],2024-11-19 03:33:06+00:00,2024-11-19 03:33:05+00:00,https://github.com/apache/airflow/issues/43794,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:core', ''), ('mans-hackathon', '')]","[{'comment_id': 2462769044, 'issue_id': 2641638864, 'author': 'amoghrajesh', 'body': '@dirrao was this targetted for 2.10.3?', 'created_at': datetime.datetime(2024, 11, 7, 16, 59, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466019454, 'issue_id': 2641638864, 'author': 'dirrao', 'body': '> @dirrao was this targetted for 2.10.3?\r\n\r\n@amoghrajesh This was intended for Airflow 3 not 2.10.\r\nhttps://github.com/apache/airflow/pull/42126', 'created_at': datetime.datetime(2024, 11, 9, 3, 32, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2467994189, 'issue_id': 2641638864, 'author': 'potiuk', 'body': ""Yeah - we noticed it as well during the Man's Hackathon :) .  It's something we should definitely target in 2.10.4"", 'created_at': datetime.datetime(2024, 11, 11, 11, 54, tzinfo=datetime.timezone.utc)}]","amoghrajesh on (2024-11-07 16:59:30 UTC): @dirrao was this targetted for 2.10.3?

dirrao on (2024-11-09 03:32:51 UTC): @amoghrajesh This was intended for Airflow 3 not 2.10.
https://github.com/apache/airflow/pull/42126

potiuk on (2024-11-11 11:54:00 UTC): Yeah - we noticed it as well during the Man's Hackathon :) .  It's something we should definitely target in 2.10.4

"
2641562611,issue,closed,completed,Remove strict_asset_uri_validation,"### Body

We introduced this config for people using `uri` on Dataset for a human-readable name in 2.9. Since we’ve introduced `name` to Asset, this configuration should be phased out since it is no longer reasonable to use `uri` for a value that’s not a URI.

OpenLineage is looking forward to this so it can have guarantee that URIs in assets are always validated, and can be safely converted to the corresponding OpenLineage Dataset URI format.

* [x] Remove the config in main (always hard error). https://github.com/apache/airflow/pull/43915
* [x] Deprecate the config in 2.x. https://github.com/apache/airflow/pull/43918
* [ ] Backport the `name` attribute to 2.x Dataset, and think of a way for users to easily transition. No concrete plan here yet.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",uranusjr,2024-11-07 16:37:31+00:00,['uranusjr'],2024-11-18 05:30:42+00:00,2024-11-18 05:30:41+00:00,https://github.com/apache/airflow/issues/43792,"[('kind:feature', 'Feature Requests'), ('provider:openlineage', 'AIP-53'), ('area:datasets', 'Issues related to the datasets feature')]","[{'comment_id': 2481990706, 'issue_id': 2641562611, 'author': 'uranusjr', 'body': 'It seems like we don’t need `name` for this one. OL only cares about the URI. We might still need to do it for other reasons, but we’ll see.', 'created_at': datetime.datetime(2024, 11, 18, 5, 30, 41, tzinfo=datetime.timezone.utc)}]","uranusjr (Issue Creator) on (2024-11-18 05:30:41 UTC): It seems like we don’t need `name` for this one. OL only cares about the URI. We might still need to do it for other reasons, but we’ll see.

"
2641421036,issue,closed,completed,Custom secret backend issue,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.1

### What happened?

Can't use a custom secret backend. I want to make a backend that reads a yaml file to override connections From the GCP secret manager.

Getting error:
```
Module ""plugins.custom_secrets_backend"" does not define a ""LocalSecretBackend"" attribute/class
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
File ~/.local/lib/python3.11/site-packages/airflow/utils/module_loading.py:42, in import_string(dotted_path)
     41 try:
---> 42     return getattr(module, class_name)
     43 except AttributeError:

AttributeError: partially initialized module 'plugins.custom_secrets_backend' has no attribute 'LocalSecretBackend' (most likely due to a circular import)

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
File ~/.local/lib/python3.11/site-packages/airflow/configuration.py:1211, in AirflowConfigParser.getimport(self, section, key, **kwargs)
   1210 try:
-> 1211     return import_string(full_qualified_path)
   1212 except ImportError as e:

File ~/.local/lib/python3.11/site-packages/airflow/utils/module_loading.py:44, in import_string(dotted_path)
     43 except AttributeError:
---> 44     raise ImportError(f'Module ""{module_path}"" does not define a ""{class_name}"" attribute/class')

ImportError: Module ""plugins.custom_secrets_backend"" does not define a ""LocalSecretBackend"" attribute/class

During handling of the above exception, another exception occurred:

AirflowConfigException                    Traceback (most recent call last)
Cell In[1], line 1
----> 1 from plugins.custom_secrets_backend import LocalSecretBackend

File /opt/airflow/plugins/custom_secrets_backend.py:1
----> 1 import airflow.providers.google.cloud.secrets.secret_manager
      2 import yaml
      3 from airflow.models.connection import Connection

File ~/.local/lib/python3.11/site-packages/airflow/__init__.py:52
     35     warnings.warn(
     36         ""Airflow currently can be run on POSIX-compliant Operating Systems. For development, ""
     37         ""it is regularly tested on fairly modern Linux Distros and recent versions of macOS. ""
   (...)
     42         stacklevel=1,
     43     )
     45 # The configuration module initializes and validates the conf object as a side effect the first
     46 # time it is imported. If it is not imported before importing the settings module, the conf
     47 # object will then be initted/validated as a side effect of it being imported in settings,
   (...)
     50 # those functions likely import settings).
     51 # configuration is therefore initted early here, simply by importing it.
---> 52 from airflow import configuration, settings
     54 __all__ = [
     55     ""__version__"",
     56     ""DAG"",
     57     ""Dataset"",
     58     ""XComArg"",
     59 ]
     61 # Make `airflow` a namespace package, supporting installing
     62 # airflow.providers.* in different locations (i.e. one in site, and one in user
     63 # lib.)

File ~/.local/lib/python3.11/site-packages/airflow/configuration.py:2347
   2344 WEBSERVER_CONFIG = """"  # Set by initialize_config
   2346 conf: AirflowConfigParser = initialize_config()
-> 2347 secrets_backend_list = initialize_secrets_backends()
   2348 conf.validate()

File ~/.local/lib/python3.11/site-packages/airflow/configuration.py:2255, in initialize_secrets_backends()
   2247 """"""
   2248 Initialize secrets backend.
   2249
   2250 * import secrets backend classes
   2251 * instantiate them and return them in a list
   2252 """"""
   2253 backend_list = []
-> 2255 custom_secret_backend = get_custom_secret_backend()
   2257 if custom_secret_backend is not None:
   2258     backend_list.append(custom_secret_backend)

File ~/.local/lib/python3.11/site-packages/airflow/configuration.py:2225, in get_custom_secret_backend()
   2223 def get_custom_secret_backend() -> BaseSecretsBackend | None:
   2224     """"""Get Secret Backend if defined in airflow.cfg.""""""
-> 2225     secrets_backend_cls = conf.getimport(section=""secrets"", key=""backend"")
   2227     if not secrets_backend_cls:
   2228         return None

File ~/.local/lib/python3.11/site-packages/airflow/configuration.py:1214, in AirflowConfigParser.getimport(self, section, key, **kwargs)
   1212 except ImportError as e:
   1213     log.warning(e)
-> 1214     raise AirflowConfigException(
   1215         f'The object could not be loaded. Please check ""{key}"" key in ""{section}"" section. '
   1216         f'Current value: ""{full_qualified_path}"".'
   1217     )

AirflowConfigException: The object could not be loaded. Please check ""backend"" key in ""secrets"" section. Current value: ""plugins.custom_secrets_backend.LocalSecretBackend"".
2024-11-07 17:41:21 airflow.exceptions.AirflowConfigException: The object could not be loaded. Please check ""backend"" key in ""secrets"" section. Current value: ""plugins.custom_secrets_backend.CloudSecretManagerBackend"".
```

### What you think should happen instead?

_No response_

### How to reproduce


Code for custom backend:
File name `custom_secrets_backend` under `plugins` folder
```py
import yaml
from airflow.models.connection import Connection
from airflow.providers.google.cloud.secrets.secret_manager import (
    CloudSecretManagerBackend,
)


class LocalSecretBackend(CloudSecretManagerBackend):

    def __init__(self, conn_path: str = None, env_path: str = None, **kwargs):
        super().__init__(**kwargs)
        self.env_path = env_path
        self.env_values = self._read_env_file()
        self.conn_path = conn_path
        self.conn_values = self._read_conn_file()

    def get_connection(self, conn_id: str):
        if conn_id in self.conn_values:
            return self.conn_values[conn_id]
        else:
            return super().get_connection(conn_id=conn_id)

    def get_variable(self, key: str) -> str | None:
        if key in self.env_values:
            return self.env_values[key]
        else:
            return super().get_variable(key=key)

    def _read_env_file(self):
        env_dict = {}
        with open(self.env_path, ""r"") as file:
            for line in file:
                # Remove comments and skip empty lines
                line = line.strip()
                if not line or line.startswith(""#""):
                    continue
                # Split key and value
                key, value = line.split(""="", 1)
                env_dict[key.strip()] = value.strip()
        return env_dict

    def _read_conn_file(self):
        conn_dict = {}

        # Load YAML file
        with open(self.conn_path, ""r"") as file:
            config = yaml.safe_load(file)

        # Extract connections list
        connections = config.get(""connections"", [])

        # Create each connection
        for conn in connections:
            conn_id = conn.get(""conn_id"")
            conn_type = conn.get(""conn_type"")
            host = conn.get(""host"")
            schema = conn.get(""schema"")
            login = conn.get(""login"")
            password = conn.get(""password"")
            port = conn.get(""port"")
            extra = conn.get(""extra"")

            conn_dict[conn_id] = Connection(
                conn_id=conn_id,
                conn_type=conn_type,
                host=host,
                schema=schema,
                login=login,
                password=password,
                port=port,
                extra=extra,
            )

        return conn_dict
```

in `airflow.cfg` file:
```
[secrets]
backend = plugins.custom_secrets_backend.LocalSecretBackend
backend_kwargs = {""project_id"": """", ""connections_prefix"":""airflow-connection"", ""conn_path"": ""local/local_connections.yaml""}
```

example `yaml`:
```yml

connections:
  - conn_id: mysql_conn
    conn_type: mysql
    host: 
    schema:
    login:
    password: 
    port:
```

### Operating System

Mac Sonoma 14.4.1

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",dor-schreiber,2024-11-07 15:41:48+00:00,[],2024-11-11 11:48:12+00:00,2024-11-11 11:48:12+00:00,https://github.com/apache/airflow/issues/43790,"[('kind:bug', 'This is a clearly a bug'), ('area:secrets', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2641308285,issue,open,,Lead the OTEL cleanup project,"OTEL code has increased obfuscation in the airflow codebase.

It's made it harder to follow non-OTEL code (maybe call it ""operative"" code).  And the volume of it also made it harder to evaluate the OTEL code.

There are two high level things we need to do.

1. we need to reduce the obfuscation from the OTEL code.  That is, reduce the OTEL-related noise in the scheduler and all other areas of airflow code where it is used.  
    - Here's one small example of how the situation can be improved: https://github.com/apache/airflow/pull/43787
    - we need to increase the separation between what should be operative code and non-operative code.  it should be easy to identify which parts of a function are material to the operation of the function, and what is ""just OTEL"" or ""just logging and OTEL"".  example below.
3. We need to do a thorough review of all the OTEL code to identify and root out bad behavior.
 
The first part of this should help with the second.  And progress on 2 can be made while working on 1.

Another example:

Observe here, in the middle of a critical scheduler function, we have 60 lines that do nothing but log and do OTEL stuff:
https://github.com/apache/airflow/blob/b757bd8df824d4eba952f6e140bcb373bc3f1003/airflow/models/dagrun.py#L992-L1049

It's highly obfuscatory.  And, it turns out, while `if` block looks like it must be a meaningful and consequential part of the function, it actually does nothing operative. (though it's very hard to see that)


### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2024-11-07 15:06:33+00:00,['xBis7'],2024-12-12 17:09:22+00:00,,https://github.com/apache/airflow/issues/43789,"[('kind:meta', 'High-level information important to the community'), ('telemetry', 'Telemetry-related issues')]","[{'comment_id': 2475010604, 'issue_id': 2641308285, 'author': 'kaxil', 'body': 'small PR: https://github.com/apache/airflow/pull/43982', 'created_at': datetime.datetime(2024, 11, 13, 23, 11, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2539337002, 'issue_id': 2641308285, 'author': 'xBis7', 'body': ""Hello, I just came across this issue. I have an open PR for changing the Otel implementation.\r\n\r\nhttps://github.com/apache/airflow/pull/43941\r\n\r\nI was also planning to create a follow up patch for cleanup after this one was merged. To avoid contributing to the existing obfuscation, I'll try to extract the otel-related code to new methods."", 'created_at': datetime.datetime(2024, 12, 12, 15, 49, 3, tzinfo=datetime.timezone.utc)}]","kaxil on (2024-11-13 23:11:29 UTC): small PR: https://github.com/apache/airflow/pull/43982

xBis7 (Assginee) on (2024-12-12 15:49:03 UTC): Hello, I just came across this issue. I have an open PR for changing the Otel implementation.

https://github.com/apache/airflow/pull/43941

I was also planning to create a follow up patch for cleanup after this one was merged. To avoid contributing to the existing obfuscation, I'll try to extract the otel-related code to new methods.

"
2640526900,issue,open,,Using GCP secret manager in local env and override secrets,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.1

### What happened?

When using airflow with GCP secret manager it works great.
When I wan't to use the secret manager to fetch the secrets + override some of them the secrets remain the same.wantthe 

### What you think should happen instead?

The env/yaml secrets will be given priority over the secret manager. The UI secrets can override as well, but when using secret manager we are fetching the secret before using so I'm not sure how it will work.

### How to reproduce

Create an airflow repo with secret manager backend [link](https://airflow.apache.org/docs/apache-airflow-providers-google/stable/secrets-backends/google-cloud-secret-manager-backend.html).

Set a secret and try to override it sith local secret

### Operating System

Mac Sonoma 14.4.1

### Versions of Apache Airflow Providers

apache-airflow-providers-google==10.20.0


### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",dor-schreiber,2024-11-07 10:12:48+00:00,[],2024-11-07 10:14:59+00:00,,https://github.com/apache/airflow/issues/43779,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('area:secrets', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2461832419, 'issue_id': 2640526900, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 7, 10, 12, 51, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-07 10:12:51 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2640526418,issue,closed,completed,AIP-84 Add ordering to task instances and mapped task instances list,"### Body

Check this dicussion for more context https://github.com/apache/airflow/pull/43760/files#r1831356684, additional ordering options should be available.

Extend the SortParam common logic to handle aliasing, useful for logical_date/execution.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-11-07 10:12:36+00:00,['prabhusneha'],2025-01-23 09:08:17+00:00,2025-01-23 09:08:16+00:00,https://github.com/apache/airflow/issues/43778,"[('kind:feature', 'Feature Requests')]","[{'comment_id': 2572914839, 'issue_id': 2640526418, 'author': 'prabhusneha', 'body': 'I can pick this up', 'created_at': datetime.datetime(2025, 1, 6, 11, 26, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2573094537, 'issue_id': 2640526418, 'author': 'pierrejeambrun', 'body': '> I can pick this up\r\n@prabhusneha nice 🎉 \r\n\r\ncc: @utkarsharma2', 'created_at': datetime.datetime(2025, 1, 6, 13, 18, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2573142795, 'issue_id': 2640526418, 'author': 'utkarsharma2', 'body': '@prabhusneha Sure, please go ahead.', 'created_at': datetime.datetime(2025, 1, 6, 13, 45, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2602631425, 'issue_id': 2640526418, 'author': 'pierrejeambrun', 'body': '`execution_date` was renamed in the database. Aliasing might not be needed anymore to complete this task.', 'created_at': datetime.datetime(2025, 1, 20, 14, 54, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2602739383, 'issue_id': 2640526418, 'author': 'prabhusneha', 'body': 'PR: [#45802 ](https://github.com/apache/airflow/pull/45802)', 'created_at': datetime.datetime(2025, 1, 20, 15, 41, 35, tzinfo=datetime.timezone.utc)}]","prabhusneha (Assginee) on (2025-01-06 11:26:17 UTC): I can pick this up

pierrejeambrun (Issue Creator) on (2025-01-06 13:18:07 UTC): @prabhusneha nice 🎉 

cc: @utkarsharma2

utkarsharma2 on (2025-01-06 13:45:02 UTC): @prabhusneha Sure, please go ahead.

pierrejeambrun (Issue Creator) on (2025-01-20 14:54:59 UTC): `execution_date` was renamed in the database. Aliasing might not be needed anymore to complete this task.

prabhusneha (Assginee) on (2025-01-20 15:41:35 UTC): PR: [#45802 ](https://github.com/apache/airflow/pull/45802)

"
2640522466,issue,closed,completed,Lineage between tasks are missing (Airflow -> OpenLineage -> Marquez),"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.3

### What happened?

I use Marquez to visualize dependencies. In Airflow I have installed apache-airflow-providers-openlineage and configured the OpenLineage transport:

```
AIRFLOW__OPENLINEAGE__TRANSPORT: '{“type”: “http”, “url”: “http://YOUR_IP:5000”, “endpoint”: “api/v1/lineage”}'
AIRFLOW__OPENLINEAGE__NAMESPACE: 'airflow'
AIRFLOW__OPENLINEAGE__INCLUDE_FULL_TASK_INFO: 'true'
```

Events and jobs are transferred as expected, but without the corresponding lineage. This means that I only see the tasks in Marquez, but not the DAG with the tasks.


### What you think should happen instead?

In Marquez the DAG shoud be shown incl. all tasks.

### How to reproduce

1. pip-install the required package: apache-airflow-providers-openlineage
2. configure openlinage transport like so: AIRFLOW__OPENLINEAGE__TRANSPORT: '{“type”: “http”, “url”: “http://YOUR_IP:5000”, “endpoint”: “api/v1/lineage”}'
3. execute any Airflow DAG

### Operating System

Linux

### Versions of Apache Airflow Providers

apache-airflow-providers-openlineage==1.13.0

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",thinkORo,2024-11-07 10:10:54+00:00,[],2025-01-30 08:57:18+00:00,2024-11-07 10:45:08+00:00,https://github.com/apache/airflow/issues/43776,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:openlineage', 'AIP-53')]","[{'comment_id': 2461828177, 'issue_id': 2640522466, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 7, 10, 10, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461903974, 'issue_id': 2640522466, 'author': 'thinkORo', 'body': 'After updating to Airflow 2.10.3 everything is running as expected.', 'created_at': datetime.datetime(2024, 11, 7, 10, 45, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2623888940, 'issue_id': 2640522466, 'author': 'mats1701', 'body': 'Using Airflow 2.10.4 and apache-airflow-providers-openlineage==2.0.0 I still get this exact behaviour. Tried downgrading the provider package to 1.13.0 but no luck.\nI have set the exact same environment variables.', 'created_at': datetime.datetime(2025, 1, 30, 8, 57, 16, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-07 10:10:58 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

thinkORo (Issue Creator) on (2024-11-07 10:45:09 UTC): After updating to Airflow 2.10.3 everything is running as expected.

mats1701 on (2025-01-30 08:57:16 UTC): Using Airflow 2.10.4 and apache-airflow-providers-openlineage==2.0.0 I still get this exact behaviour. Tried downgrading the provider package to 1.13.0 but no luck.
I have set the exact same environment variables.

"
2638468212,issue,closed,completed,Support for authenticating to airflow metadata database like postgres sql with cloud specific authentication mechanism,"### Description

We are deploying latest version of airflow on Azure Kubernetes cluster with components of airflow like web/worker/scheduler etc. running on different AKS pods, this is achieved using helm charts
We are using Azure Postgres SQL service as Airflow Metadata Database

Now for the airflow pods to authenticate to Postgres SQL a connection string is used which comprises of host, username, password, port etc. [Code reference](https://github.com/apache/airflow/blob/main/airflow/settings.py#L462) where the connection string is read from the config to initialize a DB session object for handling DB operations

This static connection string is not the safest of options and we want to use [Azure AAD based authentication](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/how-to-connect-with-managed-identity#create-an-azure-database-for-postgresql-flexible-server-user-for-your-managed-identity) which eliminates the need to store static passwords.
If we leverage this managed identity authentication mechanism instead of static password we can generate on demand short lived tokens and use those as passwords while connecting to Postgres which essentially means connection string will be a dynamic value

One possible way to solve this issue is that since connection string is formed using environment variables we can update the environment variables and restart airflow pods but restarting pods on every token refresh is not efficient
Hence we are looking into an optimal way to handle this in airflow code by perhaps a provider specific implementation of this Session class but not sure that is the right way, we are completely new to Airflow hence some guidance will really help :)
Hoping the above details should be sufficient

Note - This is different use case than airflow hooks as hooks will be beneficial if the running DAGs are trying to connect to some external data store, here we want the airflow components themselves to connect using custom cloud specific authentication

### Use case/motivation

Provides an ability to the clients of airflow to use Custom Authentication schemes while connecting to appropriate backend deta stores

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",harshit283,2024-11-06 15:39:35+00:00,[],2024-11-08 23:09:42+00:00,2024-11-08 23:09:42+00:00,https://github.com/apache/airflow/issues/43758,"[('provider:microsoft-azure', 'Azure-related issues'), ('area:MetaDB', 'Meta Database related issues.'), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2638294320,issue,closed,completed,Extra links not appearing on mapped tasks,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Extra external links are not appearing on mapped tasks.

### What you think should happen instead?

While I am able to add extra external links to an existing operator (the `PythonOperator` in this case) and able to view them in the UI, I am not able see the extra links when using mapped tasks of the same operator.

In this screenshot, I have a non-mapped task version of a simple DAG using the `PythonOperator` and the extra external link button is visible.
![Screenshot 2024-11-05 at 1 44 32 PM](https://github.com/user-attachments/assets/919e53f6-3813-448e-a9a6-2bc689cb267f)

In this screenshot, I have a mapped task version of the same DAG using the `PythonOperator` and the extra external link is not visible. 
![Screenshot 2024-11-05 at 1 47 27 PM](https://github.com/user-attachments/assets/e1acf9de-6c0a-4c42-b3fd-b9964942cb37)

### How to reproduce

Create a new directory and run `astro dev init`.

Create a file named `my_extra_link_plugin.py` with the following code and add it to the `plugins` folder:

```
from airflow.models.baseoperator import BaseOperator
from airflow.models.baseoperator import BaseOperatorLink
from airflow.models.taskinstancekey import TaskInstanceKey
from airflow.operators.python import PythonOperator
from airflow.plugins_manager import AirflowPlugin


# define the extra link
class PythonDocsLink(BaseOperatorLink):
    # name the link button in the UI
    name = ""Python Docs""

    # add the button to one or more operators
    operators = [PythonOperator]

    # provide the link
    def get_link(self, operator: BaseOperator, *, ti_key: TaskInstanceKey):
        return ""https://docs.python.org/3/""

# define the plugin class
class AirflowExtraLinkPlugin(AirflowPlugin):
    name = ""extra_link_plugin""
    operator_extra_links = [
        PythonDocsLink(),
    ]
```

In the `dags` folder, create a file named `add.py` that contains the following:
```
from pendulum import datetime

from airflow.models.dag import DAG
from airflow.operators.python import PythonOperator

with DAG(
    dag_id=""add"",
    schedule=None,
    start_date=datetime(2024, 11, 3),
    catchup=False,
) as dag:
    def add_function(x: int, y: int):
        return x + y

    added_values = PythonOperator(
        task_id=""add"",
        python_callable=add_function,
        op_kwargs={""x"": 7, ""y"": 10},
    )
```

In the `dags` folder, create a file named `add_mapped_task.py` that contains the following:
```
from pendulum import datetime

from airflow.models.dag import DAG
from airflow.operators.python import PythonOperator

with DAG(
    dag_id=""add_with_mapped_tasks"",
    schedule=None,
    start_date=datetime(2024, 11, 3),
    catchup=False,
) as dag:
    def add_function(x: int, y: int):
        return x + y

    added_values = PythonOperator.partial(
        task_id=""add"",
        python_callable=add_function,
        op_kwargs={""y"": 10},
        # optionally, you can set a custom index to display in the UI (Airflow 2.9+)
        map_index_template=""Input x={{ task.op_args[0] }}"",
    ).expand(op_args=[[1], [2], [3]])
```

Trigger both DAGs and click on the Details tab.

### Operating System

Sonoma 14.7

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

Using Astronomer Runtime 12.2.0

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",brentsifi,2024-11-06 14:42:58+00:00,[],2025-02-03 06:22:01+00:00,2025-02-03 06:22:00+00:00,https://github.com/apache/airflow/issues/43757,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('area:dynamic-task-mapping', 'AIP-42'), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2459938897, 'issue_id': 2638294320, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 6, 14, 43, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2462972887, 'issue_id': 2638294320, 'author': 'eladkal', 'body': 'cc @bbovenzi This seems to be UI issue(?)', 'created_at': datetime.datetime(2024, 11, 7, 18, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2517266984, 'issue_id': 2638294320, 'author': 'utkarsharma2', 'body': '@potiuk / @eladkal Do we need this for 2.10.4?', 'created_at': datetime.datetime(2024, 12, 4, 12, 52, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2517996333, 'issue_id': 2638294320, 'author': 'eladkal', 'body': ""We don't have a fix for it yet so it shouldn't block 2.10.4"", 'created_at': datetime.datetime(2024, 12, 4, 16, 45, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2630072115, 'issue_id': 2638294320, 'author': 'eladkal', 'body': 'Fixed in https://github.com/apache/airflow/pull/46337', 'created_at': datetime.datetime(2025, 2, 3, 6, 22, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-06 14:43:02 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2024-11-07 18:44:00 UTC): cc @bbovenzi This seems to be UI issue(?)

utkarsharma2 on (2024-12-04 12:52:49 UTC): @potiuk / @eladkal Do we need this for 2.10.4?

eladkal on (2024-12-04 16:45:07 UTC): We don't have a fix for it yet so it shouldn't block 2.10.4

eladkal on (2025-02-03 06:22:00 UTC): Fixed in https://github.com/apache/airflow/pull/46337

"
2638234396,issue,closed,completed,AIP-84 Get Task Instances Batch,"### Description

Currently the Get Task Instances Batch public endpoint is at `api_connexion/endpoints/task_instance_endpoint.py` under `get_task_instances_batch`. We need to migrate it to the `api_fastapi/views/public/task_instances.py` under a `get_task_instances_batch` or similar.

Features and functionality of the endpoint will remain unchanged.

### Use case/motivation

AIP-84 cWiki Link: https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API

### Related issues

Related to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-11-06 14:22:43+00:00,['pierrejeambrun'],2024-11-15 18:33:07+00:00,2024-11-15 18:33:07+00:00,https://github.com/apache/airflow/issues/43756,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]",[],
2638230205,issue,closed,completed,AIP-84 Set Task Instance Note,"### Description

Currently the Set Task Instance Note public endpoint is at `api_connexion/endpoints/task_instance_endpoint.py` under `set_task_instance_note`. We need to migrate it to the `api_fastapi/views/public/task_instances.py` under a `set_task_instance_note` or similar.

Features and functionality of the endpoint will remain unchanged.

### Use case/motivation

AIP-84 cWiki Link: https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API

### Related issues

Related to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-11-06 14:20:58+00:00,[],2024-11-06 14:48:03+00:00,2024-11-06 14:47:48+00:00,https://github.com/apache/airflow/issues/43755,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2459950793, 'issue_id': 2638230205, 'author': 'pierrejeambrun', 'body': 'This endpoint does not need to be migrated, in the same way as we are doing for the `DagRun`, the update of the task instance note should be part of the `PATCH` task instance. (improved/broader PATCH instance).\r\n\r\nClosing.', 'created_at': datetime.datetime(2024, 11, 6, 14, 47, 48, tzinfo=datetime.timezone.utc)}]","pierrejeambrun on (2024-11-06 14:47:48 UTC): This endpoint does not need to be migrated, in the same way as we are doing for the `DagRun`, the update of the task instance note should be part of the `PATCH` task instance. (improved/broader PATCH instance).

Closing.

"
2638219175,issue,closed,completed,AIP-84 Patch Mapped Task Instance,"### Description

Currently the Patch Mapped Task Instance public endpoint is at `api_connexion/endpoints/task_instance_endpoint.py` under `patch_mapped_task_instance`. We need to migrate it to the `api_fastapi/views/public/task_instances.py` under a `patch_mapped_task_instance` or similar.

### Use case/motivation

AIP-84 cWiki Link: https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API

### Related issues

Related to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-11-06 14:16:18+00:00,['omkar-foss'],2024-11-26 14:22:14+00:00,2024-11-26 14:22:05+00:00,https://github.com/apache/airflow/issues/43754,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2489409761, 'issue_id': 2638219175, 'author': 'omkar-foss', 'body': 'I suppose this is covered [here](https://github.com/apache/airflow/pull/44223/files#diff-975b8fd26f7ca1fec635278e2a788ed2e1ac10aba6e8e37a94931b6136cc2b68R493-R496) in PR  https://github.com/apache/airflow/pull/44223.', 'created_at': datetime.datetime(2024, 11, 20, 19, 43, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2500933324, 'issue_id': 2638219175, 'author': 'pierrejeambrun', 'body': 'Completed in https://github.com/apache/airflow/pull/44223', 'created_at': datetime.datetime(2024, 11, 26, 14, 22, 13, tzinfo=datetime.timezone.utc)}]","omkar-foss (Issue Creator) on (2024-11-20 19:43:29 UTC): I suppose this is covered [here](https://github.com/apache/airflow/pull/44223/files#diff-975b8fd26f7ca1fec635278e2a788ed2e1ac10aba6e8e37a94931b6136cc2b68R493-R496) in PR  https://github.com/apache/airflow/pull/44223.

pierrejeambrun on (2024-11-26 14:22:13 UTC): Completed in https://github.com/apache/airflow/pull/44223

"
2638213802,issue,closed,completed,AIP-84 Patch Task Instance,"### Description

Currently the Patch Task Instance public endpoint is at `api_connexion/endpoints/task_instance_endpoint.py` under `patch_task_instance`. We need to migrate it to the `api_fastapi/views/public/task_instances.py` under a `patch_task_instance` or similar.

Features and functionality of the endpoint will remain unchanged.

### Use case/motivation

AIP-84 cWiki Link: https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API

### Related issues

Related to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-11-06 14:14:18+00:00,['omkar-foss'],2024-11-26 10:04:11+00:00,2024-11-26 10:04:11+00:00,https://github.com/apache/airflow/issues/43753,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2489379077, 'issue_id': 2638213802, 'author': 'omkar-foss', 'body': 'This too almost done, will raise a PR in a while now.', 'created_at': datetime.datetime(2024, 11, 20, 19, 25, 41, tzinfo=datetime.timezone.utc)}]","omkar-foss (Issue Creator) on (2024-11-20 19:25:41 UTC): This too almost done, will raise a PR in a while now.

"
2638210179,issue,closed,completed,AIP-84 Set Task Instances State,"### Description

Currently the Set Task Instances State public endpoint is at `api_connexion/endpoints/task_instance_endpoint.py` under `post_set_task_instances_state`. We need to migrate it to the `api_fastapi/views/public/task_instances.py` under a `set_task_instances_state` or similar.

Features and functionality of the endpoint will remain unchanged. Please note that this is a `PUT` endpoint, contrary to the name of the old API function starting with the word `post`.

### Use case/motivation

AIP-84 cWiki Link: https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API

### Related issues

Related to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-11-06 14:12:41+00:00,['omkar-foss'],2024-11-26 14:22:20+00:00,2024-11-26 14:22:20+00:00,https://github.com/apache/airflow/issues/43752,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2489415088, 'issue_id': 2638210179, 'author': 'omkar-foss', 'body': ""I'm working on this one, will raise a PR within today."", 'created_at': datetime.datetime(2024, 11, 20, 19, 46, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2500933592, 'issue_id': 2638210179, 'author': 'pierrejeambrun', 'body': 'Completed in https://github.com/apache/airflow/pull/44223', 'created_at': datetime.datetime(2024, 11, 26, 14, 22, 20, tzinfo=datetime.timezone.utc)}]","omkar-foss (Issue Creator) on (2024-11-20 19:46:32 UTC): I'm working on this one, will raise a PR within today.

pierrejeambrun on (2024-11-26 14:22:20 UTC): Completed in https://github.com/apache/airflow/pull/44223

"
2638205921,issue,closed,completed,AIP-84 Post Clear Task Instances,"### Description

Currently the Post Clear Task Instances public endpoint is at `api_connexion/endpoints/task_instance_endpoint.py` under `post_clear_task_instances`. We need to migrate it to the `api_fastapi/views/public/task_instances.py` under a `post_clear_task_instances` or similar.

Features and functionality of the endpoint will remain unchanged.

### Use case/motivation

AIP-84 cWiki Link: https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API

### Related issues

Related to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-11-06 14:10:47+00:00,['omkar-foss'],2024-11-22 14:00:45+00:00,2024-11-22 14:00:44+00:00,https://github.com/apache/airflow/issues/43751,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2468753888, 'issue_id': 2638205921, 'author': 'omkar-foss', 'body': ""I'm working on this, will raise a PR in the next couple of days."", 'created_at': datetime.datetime(2024, 11, 11, 18, 5, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2485435932, 'issue_id': 2638205921, 'author': 'omkar-foss', 'body': 'Running late on this one, my apologies. Still working on it, will raise a PR soon.', 'created_at': datetime.datetime(2024, 11, 19, 11, 19, 3, tzinfo=datetime.timezone.utc)}]","omkar-foss (Issue Creator) on (2024-11-11 18:05:02 UTC): I'm working on this, will raise a PR in the next couple of days.

omkar-foss (Issue Creator) on (2024-11-19 11:19:03 UTC): Running late on this one, my apologies. Still working on it, will raise a PR soon.

"
2638201496,issue,closed,completed,AIP-84 Get Mapped Task Instance Dependencies,"### Description

Currently the Get Mapped Task Instance Dependencies public endpoint is at `api_connexion/endpoints/task_instance_endpoint.py` under `get_mapped_task_instance_dependencies`. We need to migrate it to the `api_fastapi/views/public/task_instances.py` under a `get_mapped_task_instance_dependencies` or similar.

Features and functionality of the endpoint will remain unchanged.

### Use case/motivation

AIP-84 cWiki Link: https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API

### Related issues

Related to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-11-06 14:08:54+00:00,['pierrejeambrun'],2024-11-07 16:55:14+00:00,2024-11-07 16:55:14+00:00,https://github.com/apache/airflow/issues/43750,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]",[],
2638196719,issue,closed,completed,AIP-84 Get Task Instance Dependencies,"### Description

Currently the Get Task Instance Dependencies public endpoint is at `api_connexion/endpoints/task_instance_endpoint.py` under `get_task_instance_dependencies`. We need to migrate it to the `api_fastapi/views/public/task_instances.py` under a `get_task_instance_dependencies` or similar.

### Use case/motivation

AIP-84 cWiki Link: https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API

### Related issues

Related to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-11-06 14:06:47+00:00,['pierrejeambrun'],2024-11-07 16:55:13+00:00,2024-11-07 16:55:13+00:00,https://github.com/apache/airflow/issues/43749,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]",[],
2638186239,issue,closed,completed,AIP-84 Get Task Instances,"### Description

Currently the Get Task Instances public endpoint is at `api_connexion/endpoints/task_instance_endpoint.py` under `get_task_instances`. We need to migrate it to the `api_fastapi/views/public/task_instances.py` under a `get_task_instances` or similar.

Features and functionality of the endpoint will remain unchanged.



### Use case/motivation

AIP-84 cWiki Link: https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API

### Related issues

Related to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-11-06 14:02:22+00:00,['pierrejeambrun'],2024-11-07 15:28:59+00:00,2024-11-07 15:28:59+00:00,https://github.com/apache/airflow/issues/43748,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]",[],
2638119214,issue,closed,completed,AIP-84 Migrate Trigger Dag Run to public FastAPI,,bbovenzi,2024-11-06 13:36:40+00:00,[],2024-11-06 13:37:37+00:00,2024-11-06 13:37:37+00:00,https://github.com/apache/airflow/issues/43745,[],[],
2638108863,issue,closed,completed,CloudBatchSubmitJobOperator incorrectly reports success when defer mode is off,"### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

apache-airflow-providers-apache-beam==5.6.2
apache-airflow-providers-celery==3.6.0
apache-airflow-providers-cncf-kubernetes==8.0.1
apache-airflow-providers-common-sql==1.11.1
apache-airflow-providers-dbt-cloud==3.7.0
apache-airflow-providers-ftp==3.7.0
apache-airflow-providers-google==10.16.0
apache-airflow-providers-hashicorp==3.6.4
apache-airflow-providers-http==4.10.0
apache-airflow-providers-imap==3.5.0
apache-airflow-providers-mysql==5.5.4
apache-airflow-providers-postgres==5.10.2
apache-airflow-providers-sendgrid==3.4.0
apache-airflow-providers-sqlite==3.7.1
apache-airflow-providers-ssh==3.10.1

### Apache Airflow version

airflow 2.7.3

### Operating System

composer-2.6.5-airflow-2.7.3

### Deployment

Google Cloud Composer

### Deployment details

_No response_

### What happened

When defer mode is off, the `CloudBatchSubmitJobOperator` operator always reports success regardless of the job's actual execution status. 
When defer mode is on, the `CloudBatchSubmitJobOperator` operator correctly reports success or failure based on the job's exit code.

### What you think should happen instead

The operator should always report the correct success or failure status based on the job's execution, regardless of whether defer mode is on or off.

### How to reproduce

Create a `CloudBatchSubmitJobOperator` that always fails (like exit 1) and run it in sync mode. 
```
([
            {
                ""taskSpec"": {
                    ""runnables"": [
                        {
                            ""script"": {
                                ""text"": ""echo This is task ${BATCH_TASK_INDEX}. This job has a total of ${BATCH_TASK_COUNT} tasks.; exit 1""
                            }
                        }
                    ],
                    ""computeResource"": {""cpuMilli"": 2000, ""memoryMib"": 16},
                    ""maxRetryCount"": 2,
                    ""maxRunDuration"": ""3600s"",
                },
                ""taskCount"": 1,
                ""parallelism"": 3,
            }
        ],
        deferrable=False,
)
```

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",adamszustak,2024-11-06 13:32:03+00:00,[],2024-11-30 15:51:56+00:00,2024-11-30 15:51:56+00:00,https://github.com/apache/airflow/issues/43744,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('area:providers', ''), ('good first issue', '')]","[{'comment_id': 2459763998, 'issue_id': 2638108863, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 6, 13, 32, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2459827471, 'issue_id': 2638108863, 'author': 'potiuk', 'body': 'cc: @VladaZakharova', 'created_at': datetime.datetime(2024, 11, 6, 13, 58, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466651262, 'issue_id': 2638108863, 'author': 'SuccessMoses', 'body': 'i would like to work on this', 'created_at': datetime.datetime(2024, 11, 10, 9, 10, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2468876369, 'issue_id': 2638108863, 'author': 'potiuk', 'body': 'assigned you', 'created_at': datetime.datetime(2024, 11, 11, 19, 23, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2496132209, 'issue_id': 2638108863, 'author': 'SuccessMoses', 'body': '@adamszustak I created a simple DAG and `CloudBatchSubmitJobOperator` always succeed regardless of whether `deferable` is True or False. It only fails if timeout occurs while `wait_for_job`. \r\n\r\nAlthough I used `unittest.mock.patch` to mock the client not actual google cloud connection.\r\n\r\n```\r\nfrom unittest.mock import patch\r\nfrom airflow.providers.google.cloud.operators.cloud_batch import CloudBatchSubmitJobOperator\r\nfrom airflow import DAG\r\nfrom google.cloud.batch_v1 import JobStatus, Job\r\n\r\n# Define your DAG\r\ndefault_args = {\r\n    \'owner\': \'airflow\',\r\n}\r\n\r\ndag = DAG(\r\n    \'cloud_batch_submit_example\',\r\n    default_args=default_args,\r\n)\r\n\r\n# Job definition\r\nCLOUD_BATCH_HOOK_PATH = ""airflow.providers.google.cloud.operators.cloud_batch.CloudBatchHook""\r\nTASK_ID = ""test""\r\nPROJECT_ID = ""testproject""\r\nREGION = ""us-central1""\r\nJOB_NAME = ""test""\r\nJOB = Job()\r\nJOB.name = JOB_NAME\r\n\r\n\r\nsubmit_job_task = CloudBatchSubmitJobOperator(\r\n    task_id=TASK_ID, project_id=PROJECT_ID, region=REGION, job_name=JOB_NAME, job=JOB,\r\n    dag=dag, deferrable=False, timeout_seconds=0\r\n)\r\n\r\nsubmit_job_task\r\n\r\nif __name__ == ""__main__"":\r\n    with (\r\n        patch(\'airflow.providers.google.cloud.hooks.cloud_batch.CloudBatchHook.get_conn\') as mock,\r\n        patch(\'google.cloud.batch_v1.Job.to_dict\') as mock_to_dict\r\n    ):\r\n        mock.return_value.get_job.return_value.status.state = JobStatus.State.FAILED #make job fail\r\n        dag.test()\r\n```\r\nlog:\r\n\r\n```\r\n[2024-11-24T17:40:38.338+0000] {dag.py:2474} INFO - dagrun id: cloud_batch_submit_example\r\n[2024-11-24T17:40:38.357+0000] {dag.py:2493} INFO - created dagrun <DagRun cloud_batch_submit_example @ 2024-11-24 17:40:37.987067+00:00: manual__2024-11-24T17:40:37.987067+00:00, state:running, queued_at: None. externally triggered: False>\r\n[2024-11-24T17:40:38.385+0000] {dag.py:2433} INFO - [DAG TEST] starting task_id=test map_index=-1\r\n[2024-11-24T17:40:38.385+0000] {dag.py:2436} INFO - [DAG TEST] running task <TaskInstance: cloud_batch_submit_example.test manual__2024-11-24T17:40:37.987067+00:00 [scheduled]>\r\n[2024-11-24 17:40:39,926] {taskinstance.py:2923} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER=\'airflow\' AIRFLOW_CTX_DAG_ID=\'cloud_batch_submit_example\' AIRFLOW_CTX_TASK_ID=\'test\' AIRFLOW_CTX_LOGICAL_DATE=\'2024-11-24T17:40:37.987067+00:00\' AIRFLOW_CTX_TRY_NUMBER=\'1\' AIRFLOW_CTX_DAG_RUN_ID=\'manual__2024-11-24T17:40:37.987067+00:00\'\r\n[2024-11-24T17:40:39.926+0000] {taskinstance.py:2923} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER=\'airflow\' AIRFLOW_CTX_DAG_ID=\'cloud_batch_submit_example\' AIRFLOW_CTX_TASK_ID=\'test\' AIRFLOW_CTX_LOGICAL_DATE=\'2024-11-24T17:40:37.987067+00:00\' AIRFLOW_CTX_TRY_NUMBER=\'1\' AIRFLOW_CTX_DAG_RUN_ID=\'manual__2024-11-24T17:40:37.987067+00:00\'\r\nTask instance is in running state\r\n Previous state of the Task instance: queued\r\nCurrent task name:test state:scheduled start_date:None\r\nDag name:cloud_batch_submit_example and current dag run status:running\r\n[2024-11-24T17:40:39.928+0000] {taskinstance.py:723} INFO - ::endgroup::\r\n[2024-11-24T17:40:39.949+0000] {connection.py:250} WARNING - Connection schemes (type: google_cloud_platform) shall not contain \'_\' according to RFC3986.\r\n[2024-11-24T17:40:39.949+0000] {base.py:66} INFO - Retrieving connection \'google_cloud_default\'\r\n[2024-11-24T17:40:39.966+0000] {taskinstance.py:346} INFO - ::group::Post task execution logs\r\n[2024-11-24T17:40:39.966+0000] {taskinstance.py:358} INFO - Marking task as SUCCESS. dag_id=cloud_batch_submit_example, task_id=test, run_id=manual__2024-11-24T17:40:37.987067+00:00, logical_date=20241124T174037, start_date=, end_date=20241124T174039\r\nTask instance in success state\r\n Previous state of the Task instance: running\r\nDag name:cloud_batch_submit_example queued_at:None\r\nTask hostname:51c25e8d352b operator:CloudBatchSubmitJobOperator\r\n[2024-11-24T17:40:39.976+0000] {dag.py:2447} INFO - [DAG TEST] end task task_id=test map_index=-1\r\n[2024-11-24T17:40:39.981+0000] {dagrun.py:935} INFO - Marking run <DagRun cloud_batch_submit_example @ 2024-11-24 17:40:37.987067+00:00: manual__2024-11-24T17:40:37.987067+00:00, state:running, queued_at: None. externally triggered: False> successful\r\nDag run in success state\r\nDag run start:2024-11-24 17:40:37.987067+00:00 end:2024-11-24 17:40:39.981712+00:00\r\n[2024-11-24T17:40:39.983+0000] {dagrun.py:987} INFO - DagRun Finished: dag_id=cloud_batch_submit_example, logical_date=2024-11-24 17:40:37.987067+00:00, run_id=manual__2024-11-24T17:40:37.987067+00:00, run_start_date=2024-11-24 17:40:37.987067+00:00, run_end_date=2024-11-24 17:40:39.981712+00:00, run_duration=1.994645, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-11-24 17:40:37.987067+00:00, data_interval_end=2024-11-24 17:40:37.987067+00:00, dag_version_name=None\r\n```', 'created_at': datetime.datetime(2024, 11, 24, 17, 42, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2496132937, 'issue_id': 2638108863, 'author': 'SuccessMoses', 'body': '@eladkal', 'created_at': datetime.datetime(2024, 11, 24, 17, 44, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2496947566, 'issue_id': 2638108863, 'author': 'eladkal', 'body': 'cc @VladaZakharova', 'created_at': datetime.datetime(2024, 11, 25, 6, 11, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2500038619, 'issue_id': 2638108863, 'author': 'adamszustak', 'body': ""@SuccessMoses What's the point of this test? Have you tried to create failing job?"", 'created_at': datetime.datetime(2024, 11, 26, 8, 59, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2500078979, 'issue_id': 2638108863, 'author': 'VladaZakharova', 'body': ""I will take a look, if @SuccessMoses hasn't started the work yet :)"", 'created_at': datetime.datetime(2024, 11, 26, 9, 18, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2500880604, 'issue_id': 2638108863, 'author': 'SuccessMoses', 'body': '@adamszustak i made modifications to `CloudBatchHook` and made the task fail when the cloud batch job fails. I will submit a PR', 'created_at': datetime.datetime(2024, 11, 26, 14, 0, 46, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-06 13:32:07 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-11-06 13:58:25 UTC): cc: @VladaZakharova

SuccessMoses on (2024-11-10 09:10:50 UTC): i would like to work on this

potiuk on (2024-11-11 19:23:37 UTC): assigned you

SuccessMoses on (2024-11-24 17:42:35 UTC): @adamszustak I created a simple DAG and `CloudBatchSubmitJobOperator` always succeed regardless of whether `deferable` is True or False. It only fails if timeout occurs while `wait_for_job`. 

Although I used `unittest.mock.patch` to mock the client not actual google cloud connection.

```
from unittest.mock import patch
from airflow.providers.google.cloud.operators.cloud_batch import CloudBatchSubmitJobOperator
from airflow import DAG
from google.cloud.batch_v1 import JobStatus, Job

# Define your DAG
default_args = {
    'owner': 'airflow',
}

dag = DAG(
    'cloud_batch_submit_example',
    default_args=default_args,
)

# Job definition
CLOUD_BATCH_HOOK_PATH = ""airflow.providers.google.cloud.operators.cloud_batch.CloudBatchHook""
TASK_ID = ""test""
PROJECT_ID = ""testproject""
REGION = ""us-central1""
JOB_NAME = ""test""
JOB = Job()
JOB.name = JOB_NAME


submit_job_task = CloudBatchSubmitJobOperator(
    task_id=TASK_ID, project_id=PROJECT_ID, region=REGION, job_name=JOB_NAME, job=JOB,
    dag=dag, deferrable=False, timeout_seconds=0
)

submit_job_task

if __name__ == ""__main__"":
    with (
        patch('airflow.providers.google.cloud.hooks.cloud_batch.CloudBatchHook.get_conn') as mock,
        patch('google.cloud.batch_v1.Job.to_dict') as mock_to_dict
    ):
        mock.return_value.get_job.return_value.status.state = JobStatus.State.FAILED #make job fail
        dag.test()
```
log:

```
[2024-11-24T17:40:38.338+0000] {dag.py:2474} INFO - dagrun id: cloud_batch_submit_example
[2024-11-24T17:40:38.357+0000] {dag.py:2493} INFO - created dagrun <DagRun cloud_batch_submit_example @ 2024-11-24 17:40:37.987067+00:00: manual__2024-11-24T17:40:37.987067+00:00, state:running, queued_at: None. externally triggered: False>
[2024-11-24T17:40:38.385+0000] {dag.py:2433} INFO - [DAG TEST] starting task_id=test map_index=-1
[2024-11-24T17:40:38.385+0000] {dag.py:2436} INFO - [DAG TEST] running task <TaskInstance: cloud_batch_submit_example.test manual__2024-11-24T17:40:37.987067+00:00 [scheduled]>
[2024-11-24 17:40:39,926] {taskinstance.py:2923} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='cloud_batch_submit_example' AIRFLOW_CTX_TASK_ID='test' AIRFLOW_CTX_LOGICAL_DATE='2024-11-24T17:40:37.987067+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-11-24T17:40:37.987067+00:00'
[2024-11-24T17:40:39.926+0000] {taskinstance.py:2923} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='cloud_batch_submit_example' AIRFLOW_CTX_TASK_ID='test' AIRFLOW_CTX_LOGICAL_DATE='2024-11-24T17:40:37.987067+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-11-24T17:40:37.987067+00:00'
Task instance is in running state
 Previous state of the Task instance: queued
Current task name:test state:scheduled start_date:None
Dag name:cloud_batch_submit_example and current dag run status:running
[2024-11-24T17:40:39.928+0000] {taskinstance.py:723} INFO - ::endgroup::
[2024-11-24T17:40:39.949+0000] {connection.py:250} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2024-11-24T17:40:39.949+0000] {base.py:66} INFO - Retrieving connection 'google_cloud_default'
[2024-11-24T17:40:39.966+0000] {taskinstance.py:346} INFO - ::group::Post task execution logs
[2024-11-24T17:40:39.966+0000] {taskinstance.py:358} INFO - Marking task as SUCCESS. dag_id=cloud_batch_submit_example, task_id=test, run_id=manual__2024-11-24T17:40:37.987067+00:00, logical_date=20241124T174037, start_date=, end_date=20241124T174039
Task instance in success state
 Previous state of the Task instance: running
Dag name:cloud_batch_submit_example queued_at:None
Task hostname:51c25e8d352b operator:CloudBatchSubmitJobOperator
[2024-11-24T17:40:39.976+0000] {dag.py:2447} INFO - [DAG TEST] end task task_id=test map_index=-1
[2024-11-24T17:40:39.981+0000] {dagrun.py:935} INFO - Marking run <DagRun cloud_batch_submit_example @ 2024-11-24 17:40:37.987067+00:00: manual__2024-11-24T17:40:37.987067+00:00, state:running, queued_at: None. externally triggered: False> successful
Dag run in success state
Dag run start:2024-11-24 17:40:37.987067+00:00 end:2024-11-24 17:40:39.981712+00:00
[2024-11-24T17:40:39.983+0000] {dagrun.py:987} INFO - DagRun Finished: dag_id=cloud_batch_submit_example, logical_date=2024-11-24 17:40:37.987067+00:00, run_id=manual__2024-11-24T17:40:37.987067+00:00, run_start_date=2024-11-24 17:40:37.987067+00:00, run_end_date=2024-11-24 17:40:39.981712+00:00, run_duration=1.994645, state=success, external_trigger=False, run_type=manual, data_interval_start=2024-11-24 17:40:37.987067+00:00, data_interval_end=2024-11-24 17:40:37.987067+00:00, dag_version_name=None
```

SuccessMoses on (2024-11-24 17:44:46 UTC): @eladkal

eladkal on (2024-11-25 06:11:17 UTC): cc @VladaZakharova

adamszustak (Issue Creator) on (2024-11-26 08:59:14 UTC): @SuccessMoses What's the point of this test? Have you tried to create failing job?

VladaZakharova on (2024-11-26 09:18:43 UTC): I will take a look, if @SuccessMoses hasn't started the work yet :)

SuccessMoses on (2024-11-26 14:00:46 UTC): @adamszustak i made modifications to `CloudBatchHook` and made the task fail when the cloud batch job fails. I will submit a PR

"
2638083339,issue,closed,completed,Dynamic Task Mapping Failure with External Python Decorator,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I am using [dynamic task mapping](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/dynamic-task-mapping.html#simple-mapping) to create two instances of a task `make_paths` that returns a single string.  
The return values are then passed to a task that expects a list of string for further processing.
However, before executing the second task, `print_paths`, an exception is thrown: `ModuleNotFoundError: No module named '***'`. The log file is attached.

When the tasks are decorated with `@task` instead of `task.external_python` , the DAG runs successfully and produces the expected output.

[error.log](https://github.com/user-attachments/files/17647662/error.log)


### What you think should happen instead?

The DAG should run without error also using the `task.external_python` decorator.

### How to reproduce

- Download and unpack the attached zip archive.
- Build and start the Docker containers using `docker compose build --no-cache && docker compose up -d`
- Open the web GUI at `http://localhost:8090/dags/test_dag` and run the test DAG. Use `airflow:airflow` as login.

[airflow_external_fail.zip](https://github.com/user-attachments/files/17647696/airflow_external_fail.zip)


### Operating System

Ubuntu 24.04 as host OS.

### Versions of Apache Airflow Providers

none used.

### Deployment

Docker-Compose

### Deployment details

Docker container based on the official `apache/airflow:2.10.2-python3.12` image.


Docker Compose version 2.24.6+ds1-0ubuntu2

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",sc-ndrf,2024-11-06 13:20:40+00:00,[],2024-11-06 13:49:29+00:00,2024-11-06 13:49:29+00:00,https://github.com/apache/airflow/issues/43743,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:dynamic-task-mapping', 'AIP-42'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2459737495, 'issue_id': 2638083339, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 6, 13, 20, 42, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-06 13:20:42 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2637951970,issue,open,,Missing TI previous tries in Logs UI after upgrading to Airflow 2.10.1+,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

After https://github.com/apache/airflow/pull/41483 we miss displaying buttons to see logs for previous tries that don't have TIHistory (because, for example, they ran in an older Airflow version).

![image](https://github.com/user-attachments/assets/e26c0d42-4684-4a64-bf14-ceb8b1f8b709)

Clicking see more on the right takes you to the other logs UI in which all tries are displayed correctly.

![image](https://github.com/user-attachments/assets/fc3d8b44-d8a6-422e-b218-1b98e706cb48)


### What you think should happen instead?

In case there are no TIHistory objects and there were more than 1 try, we should create buttons to see previous tries logs.

### How to reproduce

Create multiple tries for a TI using an older version of Airflow (<2.10.0), and upgrade to Airflow 2.10.1 or higher.

### Operating System

linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",ahidalgob,2024-11-06 12:25:44+00:00,[],2024-11-06 12:28:00+00:00,,https://github.com/apache/airflow/issues/43739,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2637140515,issue,closed,completed,Not able to connect using Consumer Key and Consumer Secret,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Hi,

I'm trying to connect to salesforce using Consumer Key and Consumer Secret, but couldn't succeed.
I'm getting the following error.

`  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/salesforce/hooks/salesforce.py"", line 137, in conn
    conn = Salesforce(
           ^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/simple_salesforce/api.py"", line 227, in __init__
    raise TypeError(
TypeError: You must provide login information or an instance and token`

Looking at the source code, I dont see any logic to consider both.. only Consumer Key is considered.

Here is the code block from simple-salesforce for reference.
<img width=""657"" alt=""image"" src=""https://github.com/user-attachments/assets/4c1fccaf-320f-4a0d-a462-ab3ebe1b93fb"">

Please correct me if I'm not doing it in the rightway.

Thanks,
Bala

### What you think should happen instead?

_No response_

### How to reproduce

I'm using http type connection with the following details,

<img width=""773"" alt=""image"" src=""https://github.com/user-attachments/assets/d7b7407d-5a45-4a72-998d-b0a3d34b5d4a"">


### Operating System

Amazon Linux 2023.5.20240916

### Versions of Apache Airflow Providers

_No response_

### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",pybala,2024-11-06 06:12:55+00:00,[],2024-11-06 06:23:00+00:00,2024-11-06 06:22:42+00:00,https://github.com/apache/airflow/issues/43725,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:salesforce', '')]","[{'comment_id': 2458799554, 'issue_id': 2637140515, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 6, 6, 12, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2458810382, 'issue_id': 2637140515, 'author': 'pybala', 'body': 'Sorry wrong repo.', 'created_at': datetime.datetime(2024, 11, 6, 6, 22, 59, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-06 06:12:57 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

pybala (Issue Creator) on (2024-11-06 06:22:59 UTC): Sorry wrong repo.

"
2636697454,issue,closed,completed,DagNotFound - Dag already exists and and it failed one time with dag_id not found,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.6.1

### What happened?

DagNotFound - Dag already exists and and it failed one time with dag_id not found. When we clear the task or resubmit it, the execution is success. Is this something related to dag parsing or dag import timeout.
The number of dags in the dags folder is approximately 1100.
The dag has successful runs for long time, but it suddenly failed with below error. 

ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/trigger_dagrun.py"", line 155, in execute
    replace_microseconds=False,
  File ""/home/airflow/.local/lib/python3.7/site-packages/airflow/api/common/trigger_dag.py"", line 128, in trigger_dag
    replace_microseconds=replace_microseconds,
  File ""/home/airflow/.local/lib/python3.7/site-packages/airflow/api/common/trigger_dag.py"", line 52, in _trigger_dag
    raise DagNotFound(f""Dag id {dag_id} not found"")
airflow.exceptions.DagNotFound: Dag id dag_test1 not found

### What you think should happen instead?

It should execute as BAU

### How to reproduce

This issue is intermittent and it occurred all of a sudden. 
The number of dags in the dags folder is approximately 1100.

### Operating System

Azure Aks 1.29.7 ubuntu

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",sktemkar,2024-11-05 23:22:57+00:00,[],2024-11-06 08:51:15+00:00,2024-11-06 08:51:15+00:00,https://github.com/apache/airflow/issues/43723,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('area:DAG-processing', '')]",[],
2636465750,issue,closed,completed,Update helm chart docs for private registry params,"### What do you see as an issue?

Update the private registry params with more meaningful descriptions, in the Parameters reference doc for Airflow Helm chart. 

Refer Slack thread for context: https://apache-airflow.slack.com/archives/CCQ7EGB1P/p1730830360500959?thread_ts=1730751844.244849&cid=CCQ7EGB1P

### Solving the problem

Use more meaningful description for the parameters.

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-11-05 21:01:53+00:00,['omkar-foss'],2024-11-06 17:46:48+00:00,2024-11-06 17:46:48+00:00,https://github.com/apache/airflow/issues/43720,"[('area:helm-chart', 'Airflow Helm Chart'), ('kind:documentation', '')]",[],
2636263674,issue,open,,Log fetches for EcsRunTaskOperator return incomplete logs,"### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

apache-airflow==2.10.1
apache-airflow-providers-airbyte==3.9.0
apache-airflow-providers-amazon==8.28.0
apache-airflow-providers-databricks==6.9.0
apache-airflow-providers-docker==3.13.0
apache-airflow-providers-slack==8.9.0
apache-airflow-providers-snowflake==5.7.0

### Apache Airflow version

2.10.1

### Operating System

Amazon Linux

### Deployment

Amazon (AWS) MWAA

### Deployment details

Our MWAA environments are managed through Terraform. We install 1 provider outside of those available in the public providers, as well as 1 internal provider.

### What happened

Related to https://github.com/apache/airflow/issues/40875

We previously reported the same issue, and after upgrading to a newer version of Airflow and the AWS provider that included the change, we were still able to detect an issue with logs appearing across log groups for ECS tasks.

In all versions of Airflow between 2.7 and 2.10, we have observed logs missing from the `*-Task` log group of our MWAA deployment. As in https://github.com/apache/airflow/issues/40875, we continue to use the `awslogs_group` argument for the EcsRunTaskOperator to specify the logs group that the UI should pull from. 

### **On successful task execution**

The log group associated with our ECS cluster ( `/ecs/airflow2/airflow2` ) retains all logs, but these are not fully reflected in the `*-Task` log group ( `airflow-airflow2-Task` ). The following files demonstrate a pair of log streams that should contain all the same events, but all lines after `[0m13:11:27  7 of 30 START test not_null_disease_location_proximity_context_results_parsed_updt_dttm  [RUN]` are missing in the log stream for the `*-Task` log group.

ECS cluster logstream (log group `/ecs/airflow2/airflow2` ): [202411015-ecs:airflow2:airflow-logstream.csv](https://github.com/user-attachments/files/17637213/202411015-ecs.airflow2.airflow-logstream.csv)

MWAA logstream (log group  `airflow-airflow2-Task` ): [20241105-airflow-airflow2-Task-logstream.csv](https://github.com/user-attachments/files/17637214/20241105-airflow-airflow2-Task-logstream.csv)

### **On unsuccessful task execution**

Additionally, we found that `::group::Post task execution logs` were able to retrieve remaining logs during failed tasks, but successful tasks left the remaining logs behind. The following is an example of a failed task that includes all log events as part of post task execution logs pulled into the log stream.

ECS cluster logstream (log group `/ecs/airflow2/airflow2` ): [20241008-ecs:airflow2:airflow2-logstream.csv](https://github.com/user-attachments/files/17637328/20241008-ecs.airflow2.airflow2-logstream.csv)

MWAA logstream (log group  `airflow-airflow2-Task` ): [20241008-airflow-airflow-Task-logstream.csv](https://github.com/user-attachments/files/17637329/20241008-airflow-airflow-Task-logstream.csv)

### **EcsRunTaskOperator call**

This is the most current version of our call to the EcsRunTaskOperator class:

```
        super().__init__(
            task_definition=task_definition,
            cluster=airflow_env_name,
            overrides={
                ""containerOverrides"": [
                    {
                        ""name"": ""dbt-om1-task"",
                        ""command"": command_list,
                    },
                ],
            },
            launch_type=""FARGATE"",
            network_configuration={
                ""awsvpcConfiguration"": {
                    ""subnets"": [
                        os.environ.get(""AIRFLOW__VAR__PRIMARY_SUBNET_ID""),
                        os.environ.get(""AIRFLOW__VAR__SECONDARY_SUBNET_ID""),
                        os.environ.get( ""AIRFLOW__VAR__TERTIARY_SUBNET_ID""),
                    ],
                    ""securityGroups"": [
                        os.environ.get(""AIRFLOW__VAR__SECURITY_GROUP_ID"")
                    ],
                    ""assignPublicIp"": ""DISABLED"",
                },
            },
            awslogs_group=f""/ecs/airflow2/airflow2"",
            awslogs_region=""us-east-1"",
            awslogs_stream_prefix=f""airflow2/dbt-om1-task"",
            awslogs_fetch_interval=timedelta(seconds=30),
            propagate_tags=""TASK_DEFINITION"",
            **kwargs,
        )
```

### What you think should happen instead

All log events in the ECS log group should be pulled forward by the MWAA `-*Task` log group using the `task_log_fetcher`.

### How to reproduce

As in https://github.com/apache/airflow/issues/40875, we are using a large (~2GB) custom DBT image inside our taskdef. This has happened with every execution of the EcsRunTaskOperator against our custom DBT image.

After triggering a task using the EcsRunTaskOperator, all logs should be copied forward from the log stream in the ECS cluster's log group to the log stream within the MWAA task log group.

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",yaningz,2024-11-05 19:15:09+00:00,[],2025-01-20 14:44:53+00:00,,https://github.com/apache/airflow/issues/43717,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('area:logging', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2599247441, 'issue_id': 2636263674, 'author': 'mjordyn', 'body': 'I encountered a very similar issue in Airflow 2.10.1 with apache-airflow-providers-amazon==8.28.0. However, in my case, I was running Airflow in a Docker container (Docker Compose), not using MWAA, and the logs missing from the Airflow UI were the first few from each of my ECS tasks. These logs were present in the ECS task CloudWatch log streams. Adding a 10 second `sleep` statement before the `logger.info` calls as a workaround caused those logs to appear as expected in the Airflow UI. All of this held true for a few separate `EcsRunTaskOperator` steps calling different ECS tasks.', 'created_at': datetime.datetime(2025, 1, 17, 21, 29, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2599292528, 'issue_id': 2636263674, 'author': 'o-nikolas', 'body': '@vincbeck Looks like the issue is still happening even after #41515 :/', 'created_at': datetime.datetime(2025, 1, 17, 22, 7, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2602607499, 'issue_id': 2636263674, 'author': 'vincbeck', 'body': '> [@vincbeck](https://github.com/vincbeck) Looks like the issue is still happening even after [#41515](https://github.com/apache/airflow/pull/41515) :/\n\n😢', 'created_at': datetime.datetime(2025, 1, 20, 14, 44, 51, tzinfo=datetime.timezone.utc)}]","mjordyn on (2025-01-17 21:29:24 UTC): I encountered a very similar issue in Airflow 2.10.1 with apache-airflow-providers-amazon==8.28.0. However, in my case, I was running Airflow in a Docker container (Docker Compose), not using MWAA, and the logs missing from the Airflow UI were the first few from each of my ECS tasks. These logs were present in the ECS task CloudWatch log streams. Adding a 10 second `sleep` statement before the `logger.info` calls as a workaround caused those logs to appear as expected in the Airflow UI. All of this held true for a few separate `EcsRunTaskOperator` steps calling different ECS tasks.

o-nikolas on (2025-01-17 22:07:26 UTC): @vincbeck Looks like the issue is still happening even after #41515 :/

vincbeck on (2025-01-20 14:44:51 UTC): 😢

"
2636129005,issue,closed,completed,AIP-38 | Add Dag source code to dag details,"Use autogenerated queries to show the dag source code. Show numbers for each log lines, support syntax highlighting depending on language, and wrap/unwrapping lines.",bbovenzi,2024-11-05 17:59:21+00:00,['bbovenzi'],2024-11-06 12:57:41+00:00,2024-11-06 12:57:41+00:00,https://github.com/apache/airflow/issues/43713,"[('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]",[],
2636123934,issue,open,,AIP-38 | Migrate to new UI,"Meta epic to track all legacy UI pages we need to migrate over to the new UI

See here for designs: https://github.com/apache/airflow/issues/42371

I'll probably update the main issue description. But here's the priority for the month:

- General
    - [x] Improve color blind accessibility by not relying just on colors for state #43054
- Dags List
    - [x] Dag Card to show last and next runs and a chart of recent runs
    - [x] Can pause/unpause a Dag from the list
    - [x] Can trigger a dag from the list #44857
    - [x] Filter dags by last dag run state, paused, and tags
    - [x] Sort dag list by display name, latest run, next run,
    - [x] Search for a dag by dag_id or dag_display_name
    - [x] Show dag import errors on dag list level
- [ ] Dag Details
    - [x] Click on a row in the Dags list to open up Dag Details
    - [x] Show header that has the same information as the Dag Card and includes actions to:
        - [x] Trigger basic Dag #44857
        - [x] Use dag params to generate forms for triggering a dag #44858
    - [x] List all dag runs and link to a specific Dag Run
    - [x] List all tasks
        - [ ] Include task group information
    - [x] Render Dag Code
- [ ] Dag Visualizations
    - [x] Graph
        - [x] Show the nodes and edges of a graph
        - [ ] Toggle showing the external dependencies of a graph (sensors, triggers, data assets with conditions)
        - [x] Show a specific dag run in the graph with each node filled with task instance or asset event info
        - [x] Click on a task to select it as a task or task instance
    - [x] Grid
        - [ ] Explore use of icons for each state to be more accessible. And how to balance that with run_type, try_number and has a note functionality
    - [ ] Gantt
    - [x] Dropdown selector on each viz to change between dag runs
    - [ ] Side drawer on each viz to see details about a task instance or dag run and the ability to quickly clear or mark as a failure
    - [x] Auto refresh #46129
- [x] Task Details
    - [x] List all task instances
- [x] Dag Run Details
    - [x] List all task instances
    - [x] Manually mark dag run as failed or success
    - [x] Clear a dag run
- [ ] Task Instance Details
    - [x] Manually mark a task failed or success
    - [x] Clear a task instance #44860
    - [x] See task instance logs across tries #44663
    - [x] See task instance details across tries
    - [ ] Mapped Task Summary page with list of all mapped instances #45990
    - [x] Allow selecting a single mapped instance with all task instance functionality
    - [x] See task instance xcoms #44667
- [ ] Backfills (theoretically optional - ui support is net new from AF2)
    - [ ] Trigger a backfill for a dag and see how many runs will be created before finally submitting
    - [ ] See if a dag has active backfills and see the state of all the backfilled runs
- [ ] Versioning
    - [ ] Show version on dag/dagrun/task/task instance
    - [ ] When viewing an older dag run, fetch the dag code for that version 
    - [ ] Fetch the dag structure for an older version or give an alert that the structure has changed


### Dashboard
Import Errors https://github.com/apache/airflow/issues/43711 
Pool Summary #43328
Historic Dag Run metrics #42700
Quick links to DAGs List #44673 
Add an event timeline to help with date range selection
Summary of components ( scheduler(s) , celery-worker(s) , triggerer(s) , dag-processor(s) )

### Dags List
Search by display_name #42714
Filter by tags #43712
Filter by last run state #42715
Update sort options #43043

### Dag Details
Trigger
Versioning (to scope)
Overview
List Runs
List Tasks
Grid View #44671 
Graph View #44200 
Gantt Chart #44672 
Code #43713 
Events per Dag #43705 
#### Backfills
List backfills https://github.com/apache/airflow/issues/43967
Active backfills banner https://github.com/apache/airflow/issues/43968
Backfill details
Create a new backfill https://github.com/apache/airflow/issues/43969
Pause/cancel backfill


### Dag Run Details
Header Card
Clear Run
Mark run as...
Change Dag Run Note

### Task Details https://github.com/apache/airflow/issues/44669
Header Card
Overview
List Task Instances

### Task Instance Details
Header Card
Logs https://github.com/apache/airflow/issues/44663
XCom https://github.com/apache/airflow/issues/44667
Rendered templates
Clear TI
Mark TI as...
Change TI Note

### Assets
To be scoped

### Admin
#### Connections
List Connections  #43703 
Edit Connection
Delete Connection
Test Connection
#### Events
List Events #43704 
Filter events
#### Pools 
List Pools #43706
Edit Pool
Delete Pool
Create Pool 
#### Providers
List Providers #43708
#### Variables
List Variables #43709
Add Variable
Edit Variable
Delete Variable

### User
Light/Dark toggle #42823 #11334
Timezone selection #42817
Color customization #43054 

### Plugins
New React UI Plugins #42702 
Iframe Plugins #42708 

### Config
Remove unnneeded config #43519
Render UI with config values",bbovenzi,2024-11-05 17:56:43+00:00,[],2025-01-31 18:08:35+00:00,,https://github.com/apache/airflow/issues/43712,"[('kind:meta', 'High-level information important to the community'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]","[{'comment_id': 2497106705, 'issue_id': 2636123934, 'author': 'hterik', 'body': '> Filter by tags https://github.com/apache/airflow/pull/43715\r\n\r\nThis appears to link to something else.', 'created_at': datetime.datetime(2024, 11, 25, 7, 39, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2498332706, 'issue_id': 2636123934, 'author': 'bbovenzi', 'body': '> > Filter by tags #43715\r\n> \r\n> This appears to link to something else.\r\n\r\nGood catch. Fixed.', 'created_at': datetime.datetime(2024, 11, 25, 15, 28, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2538365900, 'issue_id': 2636123934, 'author': 'pierrejeambrun', 'body': ""I feel like we have everything from the backend perspective. (or planned such as asset dependencies).\r\n\r\nThe only points I'm not so sure are:\r\n- Include task group information (I'm not sure we have this)\r\n- Show dag import errors on dag list level (I am not sure how we want to display them, as in the legacy we can put them at the top of the dag list, but I don't think we can group them by `dag_id` there's not really a relation there in the db model)"", 'created_at': datetime.datetime(2024, 12, 12, 9, 36, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2539468784, 'issue_id': 2636123934, 'author': 'bbovenzi', 'body': ""> I feel like we have everything from the backend perspective. (or planned such as asset dependencies).\r\n> \r\n> The only points I'm not so sure are:\r\n> \r\n> * Include task group information (I'm not sure we have this)\r\n> * Show dag import errors on dag list level (I am not sure how we want to display them, as in the legacy we can put them at the top of the dag list, but I don't think we can group them by `dag_id` there's not really a relation there in the db model)\r\n\r\n- I think almost all Task Group information is handled by grid_data and structure endpoints \r\n- We literally just need to copy the component from the home page and also show it at the top of the page on the dags list"", 'created_at': datetime.datetime(2024, 12, 12, 16, 43, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2605027347, 'issue_id': 2636123934, 'author': 'pierrejeambrun', 'body': ""I'll work on the `mark as....` (failed/success) etc. For dagruns and tasks."", 'created_at': datetime.datetime(2025, 1, 21, 15, 21, 40, tzinfo=datetime.timezone.utc)}]","hterik on (2024-11-25 07:39:30 UTC): This appears to link to something else.

bbovenzi (Issue Creator) on (2024-11-25 15:28:17 UTC): Good catch. Fixed.

pierrejeambrun on (2024-12-12 09:36:21 UTC): I feel like we have everything from the backend perspective. (or planned such as asset dependencies).

The only points I'm not so sure are:
- Include task group information (I'm not sure we have this)
- Show dag import errors on dag list level (I am not sure how we want to display them, as in the legacy we can put them at the top of the dag list, but I don't think we can group them by `dag_id` there's not really a relation there in the db model)

bbovenzi (Issue Creator) on (2024-12-12 16:43:21 UTC): - I think almost all Task Group information is handled by grid_data and structure endpoints 
- We literally just need to copy the component from the home page and also show it at the top of the page on the dags list

pierrejeambrun on (2025-01-21 15:21:40 UTC): I'll work on the `mark as....` (failed/success) etc. For dagruns and tasks.

"
2636111120,issue,closed,completed,AIP-38 | List Import Errors,"![Image](https://github.com/user-attachments/assets/49f4cc5c-1d24-421b-8055-1bb03ac3b7c2)

Use the auto-generated list import errors query in the new UI.

Add a button to the Dashboard page saying how many import errors there are (don't show the button if there are 0).
Clicking the button should open a large modal listing all import errors. For a long list, the user should be able to scroll inside the modal.",bbovenzi,2024-11-05 17:50:26+00:00,['shubhamraj-git'],2024-11-27 18:36:18+00:00,2024-11-18 13:57:42+00:00,https://github.com/apache/airflow/issues/43711,"[('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]","[{'comment_id': 2474355212, 'issue_id': 2636111120, 'author': 'shubhamraj-git', 'body': 'Hey could you please assign this to me.\r\n\r\n1. Dashboard contains only health for now, will add the stats.\r\n2. Inside the stats, we will have different buttons, one will be ""Import errors""\r\n3. Existing API : http://localhost:29091/docs#/Import%20Error/get_import_errors\r\n4. We will show list as DAG display name and accordion to view the import errors.\r\n5. I was thinking about modal size to 5, scrollable to 10 and pagination effect.\r\n6. Future work:  Will support any search / sort filter on the modal.', 'created_at': datetime.datetime(2024, 11, 13, 17, 57, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2474356768, 'issue_id': 2636111120, 'author': 'bbovenzi', 'body': 'Assigned and those 6 steps make sense to me!', 'created_at': datetime.datetime(2024, 11, 13, 17, 58, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2503358073, 'issue_id': 2636111120, 'author': 'atul-astronomer', 'body': '@shubhamraj-git Do we plan to have sort enabled also on DAG import error modal? Currently I am seeing only search enabled\r\n@bbovenzi', 'created_at': datetime.datetime(2024, 11, 27, 9, 26, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2504562884, 'issue_id': 2636111120, 'author': 'shubhamraj-git', 'body': 'Yes, As per initial talks with @bbovenzi , We can add the sort options and search options using API calls, currently (Only search) it is done on client side.', 'created_at': datetime.datetime(2024, 11, 27, 18, 36, 17, tzinfo=datetime.timezone.utc)}]","shubhamraj-git (Assginee) on (2024-11-13 17:57:46 UTC): Hey could you please assign this to me.

1. Dashboard contains only health for now, will add the stats.
2. Inside the stats, we will have different buttons, one will be ""Import errors""
3. Existing API : http://localhost:29091/docs#/Import%20Error/get_import_errors
4. We will show list as DAG display name and accordion to view the import errors.
5. I was thinking about modal size to 5, scrollable to 10 and pagination effect.
6. Future work:  Will support any search / sort filter on the modal.

bbovenzi (Issue Creator) on (2024-11-13 17:58:40 UTC): Assigned and those 6 steps make sense to me!

atul-astronomer on (2024-11-27 09:26:29 UTC): @shubhamraj-git Do we plan to have sort enabled also on DAG import error modal? Currently I am seeing only search enabled
@bbovenzi

shubhamraj-git (Assginee) on (2024-11-27 18:36:17 UTC): Yes, As per initial talks with @bbovenzi , We can add the sort options and search options using API calls, currently (Only search) it is done on client side.

"
2636091160,issue,closed,completed,AIP-38 | Show version,"Use the auto-generated get version query. In order to display the Airflow version and Git Version.

Although, we have a few options of where to render this:

- In the footer like the legacy UI
- In the User menu of the NavBar. Airflow version is a direct link to the release docs. While git version can exist in a TBD full Settings page ",bbovenzi,2024-11-05 17:40:41+00:00,['dauinh'],2024-12-06 22:00:18+00:00,2024-12-06 22:00:18+00:00,https://github.com/apache/airflow/issues/43710,"[('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]","[{'comment_id': 2510293529, 'issue_id': 2636091160, 'author': 'dauinh', 'body': 'Hi @bbovenzi! Can I take up this issue?', 'created_at': datetime.datetime(2024, 12, 1, 23, 6, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2515651752, 'issue_id': 2636091160, 'author': 'bbovenzi', 'body': 'All yours! Look for the `version` endpoint  in the the publci fastapi', 'created_at': datetime.datetime(2024, 12, 3, 22, 9, 4, tzinfo=datetime.timezone.utc)}]","dauinh (Assginee) on (2024-12-01 23:06:15 UTC): Hi @bbovenzi! Can I take up this issue?

bbovenzi (Issue Creator) on (2024-12-03 22:09:04 UTC): All yours! Look for the `version` endpoint  in the the publci fastapi

"
2636085903,issue,closed,completed,AIP-38 | List Variables,Use the auto-generated list variables query to display a table of variables. The page should be accessed in the nav via Admin -> Variables. It should use `DataTable` to handle pagination and sorting.,bbovenzi,2024-11-05 17:37:54+00:00,['shubhamraj-git'],2025-01-21 16:46:00+00:00,2025-01-21 16:45:38+00:00,https://github.com/apache/airflow/issues/43709,"[('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]","[{'comment_id': 2486495363, 'issue_id': 2636085903, 'author': 'shubhamraj-git', 'body': 'Hey @bbovenzi , Could you assign this to me?\nAlso do we have any pre-screenshots? or any pre-discussed idea?', 'created_at': datetime.datetime(2024, 11, 19, 18, 52, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2486642248, 'issue_id': 2636085903, 'author': 'bbovenzi', 'body': '@shubhamraj-git All yours. I think you can check out how the Events page at ""Browse -> Events"" works. The designs should basically be covered by our DataTable component.', 'created_at': datetime.datetime(2024, 11, 19, 20, 4, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489282217, 'issue_id': 2636085903, 'author': 'shubhamraj-git', 'body': '@bbovenzi \r\nSince the variables page have:\r\n1. list variables\r\n2. Create cariables\r\n3. Filter with different options\r\nDo we cover only 1st in this issue? and rest later?\r\nOr, This covers entire page? Although then PR would be big.', 'created_at': datetime.datetime(2024, 11, 20, 18, 28, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489638805, 'issue_id': 2636085903, 'author': 'bbovenzi', 'body': ""Yes, let's get the list done first and then do others in subsequent PRs"", 'created_at': datetime.datetime(2024, 11, 20, 22, 10, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2603968990, 'issue_id': 2636085903, 'author': 'shubhamraj-git', 'body': 'I think we can move this to Done, All the major items are completed.\nIf something comes up, can be treated as bug.\ncc: @jscheffl @pierrejeambrun @bbovenzi', 'created_at': datetime.datetime(2025, 1, 21, 8, 33, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2605241893, 'issue_id': 2636085903, 'author': 'pierrejeambrun', 'body': 'Sounds good to me, closed.\n\nThanks for all your work @shubhamraj-git 🎉', 'created_at': datetime.datetime(2025, 1, 21, 16, 45, 38, tzinfo=datetime.timezone.utc)}]","shubhamraj-git (Assginee) on (2024-11-19 18:52:30 UTC): Hey @bbovenzi , Could you assign this to me?
Also do we have any pre-screenshots? or any pre-discussed idea?

bbovenzi (Issue Creator) on (2024-11-19 20:04:54 UTC): @shubhamraj-git All yours. I think you can check out how the Events page at ""Browse -> Events"" works. The designs should basically be covered by our DataTable component.

shubhamraj-git (Assginee) on (2024-11-20 18:28:13 UTC): @bbovenzi 
Since the variables page have:
1. list variables
2. Create cariables
3. Filter with different options
Do we cover only 1st in this issue? and rest later?
Or, This covers entire page? Although then PR would be big.

bbovenzi (Issue Creator) on (2024-11-20 22:10:02 UTC): Yes, let's get the list done first and then do others in subsequent PRs

shubhamraj-git (Assginee) on (2025-01-21 08:33:45 UTC): I think we can move this to Done, All the major items are completed.
If something comes up, can be treated as bug.
cc: @jscheffl @pierrejeambrun @bbovenzi

pierrejeambrun on (2025-01-21 16:45:38 UTC): Sounds good to me, closed.

Thanks for all your work @shubhamraj-git 🎉

"
2636079444,issue,open,,AIP-38 | List Providers,"Use the auto-generated list providers query to display a list of all providers in its own page. This should work the same as the old UI. In the navbar, the admin button opens a menu and in there ""Providers"" is an option. We can just use chakra's regular Table component to render this since we don't have any advanced controls like sorting or pagination",bbovenzi,2024-11-05 17:34:35+00:00,['dauinh'],2025-01-07 18:22:22+00:00,,https://github.com/apache/airflow/issues/43708,"[('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]","[{'comment_id': 2569827455, 'issue_id': 2636079444, 'author': 'dauinh', 'body': 'Hi @bbovenzi, can i take up this issue?', 'created_at': datetime.datetime(2025, 1, 3, 21, 16, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2575957116, 'issue_id': 2636079444, 'author': 'bbovenzi', 'body': '@dauinh all yours!', 'created_at': datetime.datetime(2025, 1, 7, 18, 22, 20, tzinfo=datetime.timezone.utc)}]","dauinh (Assginee) on (2025-01-03 21:16:01 UTC): Hi @bbovenzi, can i take up this issue?

bbovenzi (Issue Creator) on (2025-01-07 18:22:20 UTC): @dauinh all yours!

"
2636061407,issue,open,,AIP-38 | List all pools,"Add a page under Admin -> Pools to list all pools. Use the auto-generated get pools query from FastAPI.

Users often don't have tons of pools. Instead of just a table, we can make this more visual and use the component we planned for the Dashboard page (see below) for a card view instead. Also in the card, instead of the text ""pool slots"", we should have the pool name and description.

![Image](https://github.com/user-attachments/assets/ca5d1f86-b7be-44d6-9f17-8f920a442133)
",bbovenzi,2024-11-05 17:28:10+00:00,['shubhamraj-git'],2025-01-11 17:25:23+00:00,,https://github.com/apache/airflow/issues/43706,"[('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]","[{'comment_id': 2580984940, 'issue_id': 2636061407, 'author': 'shubhamraj-git', 'body': 'Can you assign this to me?', 'created_at': datetime.datetime(2025, 1, 9, 18, 24, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585349283, 'issue_id': 2636061407, 'author': 'pierrejeambrun', 'body': 'Assigned!', 'created_at': datetime.datetime(2025, 1, 11, 17, 25, 21, tzinfo=datetime.timezone.utc)}]","shubhamraj-git (Assginee) on (2025-01-09 18:24:47 UTC): Can you assign this to me?

pierrejeambrun on (2025-01-11 17:25:21 UTC): Assigned!

"
2636052040,issue,closed,completed,AIP-38 | Show event logs per DAG,"After #43704, we should also list Events on the Dag Details Page. See the logic for how the Code tab is structured.

The only difference from the general Events table is that the list should already be filtered by `dag_id` and wouldn't need to display the dag_id column.
",bbovenzi,2024-11-05 17:23:37+00:00,['tirkarthi'],2024-11-13 16:40:47+00:00,2024-11-13 16:40:47+00:00,https://github.com/apache/airflow/issues/43705,"[('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]",[],
2636045037,issue,closed,completed,AIP-38 | List Event Logs,"Now that the FastAPI can list all [Event Logs](https://github.com/apache/airflow/issues/43326). We should show these Events in the UI.

- [ ] Turn the navbar's ""Browse"" button into a Menu like Docs and User are. Add ""Events"" as an option
- [ ] Add a Events page
- [ ] Use the auto-generated query to get event logs and render them as a table using `DataTable` with pagination and sorting for any supported columns",bbovenzi,2024-11-05 17:19:41+00:00,['tirkarthi'],2024-11-13 16:40:43+00:00,2024-11-13 16:40:43+00:00,https://github.com/apache/airflow/issues/43704,"[('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]","[{'comment_id': 2460596101, 'issue_id': 2636045037, 'author': 'tirkarthi', 'body': ""@bbovenzi I can take this up if you are okay. Following is my WIP now. I don't see design for the event page so below is what I came up with as per description as a table with sort and pagination. The browse menu needs to be styled as per the design. I guess the same part can be used on the event page for the dag that shouldn't have dag_id column.\r\n\r\n![image](https://github.com/user-attachments/assets/45d623c6-0c8f-4cc9-9022-e67be087e7d0)"", 'created_at': datetime.datetime(2024, 11, 6, 19, 25, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2460687943, 'issue_id': 2636045037, 'author': 'bbovenzi', 'body': ""Looking great!\r\n\r\nYeah I don't think we have a design but it should be pretty similar to the existing table in our legacy react UI.\r\nAlso, I am happy to get the functionality and UX in and we can refine the exact layout after"", 'created_at': datetime.datetime(2024, 11, 6, 20, 13, 17, tzinfo=datetime.timezone.utc)}]","tirkarthi (Assginee) on (2024-11-06 19:25:32 UTC): @bbovenzi I can take this up if you are okay. Following is my WIP now. I don't see design for the event page so below is what I came up with as per description as a table with sort and pagination. The browse menu needs to be styled as per the design. I guess the same part can be used on the event page for the dag that shouldn't have dag_id column.

![image](https://github.com/user-attachments/assets/45d623c6-0c8f-4cc9-9022-e67be087e7d0)

bbovenzi (Issue Creator) on (2024-11-06 20:13:17 UTC): Looking great!

Yeah I don't think we have a design but it should be pretty similar to the existing table in our legacy react UI.
Also, I am happy to get the functionality and UX in and we can refine the exact layout after

"
2636036277,issue,open,,AIP-38 | List Connections,"Now that we have [list connections in FastAPI](https://github.com/apache/airflow/issues/42591). We should display them in the UI.

Steps:

- [ ] Turn the ""Admin"" button in the Nav into a menu, like we do with Docs or User with a Connections option
- [ ] Add a Connections page
- [ ] Within the Connections page, load the list of connections and render them in a table using our `DataTable` component. This should include limit/offset pagination and allow order_by columns that the API supports.",bbovenzi,2024-11-05 17:14:50+00:00,['luyangliuable'],2024-11-25 23:19:56+00:00,,https://github.com/apache/airflow/issues/43703,"[('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]",[],
2635321489,issue,open,,Error migrating database from any lower Airflow version 2.x to 2.9.2,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.2

### What happened?

When upgrading the database from 2.8.2 or 2.9.2 via airflow db migrate, I get the following error message

```
INFO  [alembic.runtime.migration] Context impl MySQLImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Running upgrade 1949afb29106 -> bff083ad727d, Remove idx_last_scheduling_decision index on last_scheduling_decision in dag_run table
INFO  [alembic.runtime.migration] Running upgrade bff083ad727d -> 686269002441, Fix inconsistency between ORM and migration files.
Traceback (most recent call last):
  File ""/opt/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context
    self.dialect.do_execute(
  File ""/opt/venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py"", line 736, in do_execute
    cursor.execute(statement, parameters)
  File ""/opt/venv/lib/python3.12/site-packages/pymysql/cursors.py"", line 153, in execute
    result = self._query(query)
             ^^^^^^^^^^^^^^^^^^
  File ""/opt/venv/lib/python3.12/site-packages/pymysql/cursors.py"", line 322, in _query
    conn.query(q)
  File ""/opt/venv/lib/python3.12/site-packages/pymysql/connections.py"", line 563, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/venv/lib/python3.12/site-packages/pymysql/connections.py"", line 825, in _read_query_result
    result.read()
  File ""/opt/venv/lib/python3.12/site-packages/pymysql/connections.py"", line 1199, in read
    first_packet = self.connection._read_packet()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/venv/lib/python3.12/site-packages/pymysql/connections.py"", line 775, in _read_packet
    packet.raise_for_error()
  File ""/opt/venv/lib/python3.12/site-packages/pymysql/protocol.py"", line 219, in raise_for_error
    err.raise_mysql_exception(self._data)
  File ""/opt/venv/lib/python3.12/site-packages/pymysql/err.py"", line 150, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.ProgrammingError: (1064, ""You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'prepare stmt from @var;\n        execute stmt;\n        deallocate prepare stmt' at line 8"")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/opt/venv/bin/airflow"", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File ""/opt/venv/lib/python3.12/site-packages/airflow/__main__.py"", line 58, in main
    args.func(args)
  File ""/opt/venv/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/venv/lib/python3.12/site-packages/airflow/utils/cli.py"", line 114, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/opt/venv/lib/python3.12/site-packages/airflow/utils/providers_configuration_loader.py"", line 55, in wrapped_function
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/venv/lib/python3.12/site-packages/airflow/cli/commands/db_command.py"", line 130, in migratedb
    db.upgradedb(
  File ""/opt/venv/lib/python3.12/site-packages/airflow/utils/session.py"", line 79, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/venv/lib/python3.12/site-packages/airflow/utils/db.py"", line 1632, in upgradedb
    command.upgrade(config, revision=to_revision or ""heads"")
  File ""/opt/venv/lib/python3.12/site-packages/alembic/command.py"", line 406, in upgrade
    script.run_env()
  File ""/opt/venv/lib/python3.12/site-packages/alembic/script/base.py"", line 582, in run_env
    util.load_python_file(self.dir, ""env.py"")
  File ""/opt/venv/lib/python3.12/site-packages/alembic/util/pyfiles.py"", line 95, in load_python_file
    module = load_module_py(module_id, path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/venv/lib/python3.12/site-packages/alembic/util/pyfiles.py"", line 113, in load_module_py
    spec.loader.exec_module(module)  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""<frozen importlib._bootstrap_external>"", line 995, in exec_module
  File ""<frozen importlib._bootstrap>"", line 488, in _call_with_frames_removed
  File ""/opt/venv/lib/python3.12/site-packages/airflow/migrations/env.py"", line 120, in <module>
    run_migrations_online()
  File ""/opt/venv/lib/python3.12/site-packages/airflow/migrations/env.py"", line 114, in run_migrations_online
    context.run_migrations()
  File ""<string>"", line 8, in run_migrations
  File ""/opt/venv/lib/python3.12/site-packages/alembic/runtime/environment.py"", line 946, in run_migrations
    self.get_context().run_migrations(**kw)
  File ""/opt/venv/lib/python3.12/site-packages/alembic/runtime/migration.py"", line 628, in run_migrations
    step.migration_fn(**kw)
  File ""/opt/venv/lib/python3.12/site-packages/airflow/migrations/versions/0142_2_9_2_fix_inconsistency_between_ORM_and_migration_files.py"", line 46, in upgrade
    conn.execute(
  File ""/opt/venv/lib/python3.12/site-packages/sqlalchemy/future/engine.py"", line 286, in execute
    return self._execute_20(
           ^^^^^^^^^^^^^^^^^
  File ""/opt/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/venv/lib/python3.12/site-packages/sqlalchemy/sql/elements.py"", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File ""/opt/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 2134, in _handle_dbapi_exception
    util.raise_(
  File ""/opt/venv/lib/python3.12/site-packages/sqlalchemy/util/compat.py"", line 211, in raise_
    raise exception
  File ""/opt/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context
    self.dialect.do_execute(
  File ""/opt/venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py"", line 736, in do_execute
    cursor.execute(statement, parameters)
  File ""/opt/venv/lib/python3.12/site-packages/pymysql/cursors.py"", line 153, in execute
    result = self._query(query)
             ^^^^^^^^^^^^^^^^^^
  File ""/opt/venv/lib/python3.12/site-packages/pymysql/cursors.py"", line 322, in _query
    conn.query(q)
  File ""/opt/venv/lib/python3.12/site-packages/pymysql/connections.py"", line 563, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/venv/lib/python3.12/site-packages/pymysql/connections.py"", line 825, in _read_query_result
    result.read()
  File ""/opt/venv/lib/python3.12/site-packages/pymysql/connections.py"", line 1199, in read
    first_packet = self.connection._read_packet()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/venv/lib/python3.12/site-packages/pymysql/connections.py"", line 775, in _read_packet
    packet.raise_for_error()
  File ""/opt/venv/lib/python3.12/site-packages/pymysql/protocol.py"", line 219, in raise_for_error
    err.raise_mysql_exception(self._data)
  File ""/opt/venv/lib/python3.12/site-packages/pymysql/err.py"", line 150, in raise_mysql_exception
    raise errorclass(errno, errval)
sqlalchemy.exc.ProgrammingError: (pymysql.err.ProgrammingError) (1064, ""You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'prepare stmt from @var;\n        execute stmt;\n        deallocate prepare stmt' at line 8"")
[SQL: 
        set @var=if((SELECT true FROM information_schema.TABLE_CONSTRAINTS WHERE
            CONSTRAINT_SCHEMA = DATABASE() AND
            TABLE_NAME        = 'connection' AND
            CONSTRAINT_NAME   = 'unique_conn_id' AND
            CONSTRAINT_TYPE   = 'UNIQUE') = true,'ALTER TABLE connection
            DROP INDEX unique_conn_id','select 1');

        prepare stmt from @var;
        execute stmt;
        deallocate prepare stmt;
        ]
(Background on this error at: https://sqlalche.me/e/14/f405)
```





### What you think should happen instead?

The db migration should successfully finish.

### How to reproduce

Start with airflow 2.9.2, an empty mysql database. The initial new creation of new database objects works well.
Now run, for example

`airflow db downgrade --to-version=2.8.2 --yes`

Run
`airflow db migrate`

### Operating System

Debian GNU/Linux 11 (bullseye)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other Docker-based deployment

### Deployment details

Mysql version (docker) mysql:8.0.28. Issue occurs in other versions of mysql as well i.e 8.0.32, 8.0.36



### Anything else?

Though this issue is marked as closed https://github.com/apache/airflow/issues/40928, it is still occuring as reported by multiple users.

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",JayalakshmiH,2024-11-05 12:28:06+00:00,[],2024-12-12 13:13:51+00:00,,https://github.com/apache/airflow/issues/43690,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('area:db-migrations', 'PRs with DB migration')]","[{'comment_id': 2457041656, 'issue_id': 2635321489, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 5, 12, 28, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2457217876, 'issue_id': 2635321489, 'author': 'xia-stan', 'body': 'Can confirm that this error happens on Airflow 2.10.3 with MySQL 8.0.28. Confirmed too that a fresh DB poses no issues.', 'created_at': datetime.datetime(2024, 11, 5, 13, 42, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2459619617, 'issue_id': 2635321489, 'author': 'OlafenwaMoses', 'body': '---\r\ncross posting from the closed issue https://github.com/apache/airflow/issues/40928\r\n---\r\n\r\nI am getting same issue to. I am using MySQL `8.0.32` and any attempt to upgrade beyond `2.9.1` on an existing database leads to the error in the trace below. I have tried\r\n\r\n- Airflow `2.9.1` to `2.9.2` with MySQL `8.0.32` \r\n- Airflow `2.9.1` to `2.9.3` MySQL `8.0.32` \r\n- Airflow `2.9.1` to `2.9.3` with MySQL `8.0.36`\r\n\r\nand apparently all failed with same error despite the fix in [#40314 ](https://github.com/apache/airflow/pull/40314). \r\n\r\nAs @JayalakshmiH  mentioned, Airflow `2.9.2` and higher only works when you setup with an empty db.\r\n\r\n```\r\n2024-11-04 22:14:41 sqlalchemy.exc.ProgrammingError: (pymysql.err.ProgrammingError) (1064, ""You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near \'prepare stmt from @var;\\n        execute stmt;\\n        deallocate prepare stmt\' at line 7"")\r\n2024-11-04 22:14:41 [SQL: \r\n2024-11-04 22:14:41         set @var=if((SELECT true FROM information_schema.TABLE_CONSTRAINTS WHERE\r\n2024-11-04 22:14:41             CONSTRAINT_SCHEMA = DATABASE() AND\r\n2024-11-04 22:14:41             TABLE_NAME        = \'connection\' AND\r\n2024-11-04 22:14:41             CONSTRAINT_NAME   = \'unique_conn_id\' AND\r\n2024-11-04 22:14:41             CONSTRAINT_TYPE   = \'UNIQUE\') = true,\'ALTER TABLE connection\r\n2024-11-04 22:14:41             DROP INDEX unique_conn_id\',\'select 1\');\r\n2024-11-04 22:14:41 \r\n2024-11-04 22:14:41         prepare stmt from @var;\r\n2024-11-04 22:14:41         execute stmt;\r\n2024-11-04 22:14:41         deallocate prepare stmt;\r\n2024-11-04 22:14:41         ]\r\n2024-11-04 22:14:41 (Background on this error at: https://sqlalche.me/e/14/f405)\r\n2024-11-04 22:14:41 WARNING: Environment variable \'_AIRFLOW_DB_UPGRADE\' is deprecated please use \'_AIRFLOW_DB_MIGRATE\' instead\r\n```', 'created_at': datetime.datetime(2024, 11, 6, 12, 26, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2459947448, 'issue_id': 2635321489, 'author': 'kaxil', 'body': ""I am not able to reproduce this:\r\n\r\nMySQL: `8.0.40`\r\n\r\n```sql\r\nirflow> SELECT VERSION()\r\n[2024-11-06 14:45:01] 1 row retrieved starting from 1 in 34 ms (execution: 13 ms, fetching: 21 ms)\r\nairflow> set @var=if((SELECT true FROM information_schema.TABLE_CONSTRAINTS WHERE\r\n             CONSTRAINT_SCHEMA = DATABASE() AND\r\n             TABLE_NAME        = 'connection' AND\r\n             CONSTRAINT_NAME   = 'unique_conn_id' AND\r\n             CONSTRAINT_TYPE   = 'UNIQUE') = true,'ALTER TABLE connection\r\n             DROP INDEX unique_conn_id','select 1')\r\n[2024-11-06 14:45:06] completed in 3 ms\r\nairflow> prepare stmt from @var\r\n[2024-11-06 14:45:06] completed in 2 ms\r\nairflow> execute stmt\r\n[2024-11-06 14:45:06] 1 row retrieved starting from 1 in 18 ms (execution: 4 ms, fetching: 14 ms)\r\n```"", 'created_at': datetime.datetime(2024, 11, 6, 14, 46, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2459951977, 'issue_id': 2635321489, 'author': 'kaxil', 'body': '@JayalakshmiH By any chance are you using a different flavour of MySQL like MariaDB?', 'created_at': datetime.datetime(2024, 11, 6, 14, 48, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2460030480, 'issue_id': 2635321489, 'author': 'xia-stan', 'body': ""@kaxil : I'm using a cloud host. I double checked my MySQL version with the command you posted. It returns `8.0.28-0ubuntu0.20.04.3`. [From the package repo](https://launchpad.net/ubuntu/+source/mysql-8.0/8.0.28-0ubuntu0.20.04.3), it seems like this is definitely MySQL and not MariaDB. I didn't bother posting my error message as it was identical to @OlafenwaMoses's."", 'created_at': datetime.datetime(2024, 11, 6, 15, 14, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2460042294, 'issue_id': 2635321489, 'author': 'kaxil', 'body': 'hmm curious, maybe it was a bug in MySQL in that version', 'created_at': datetime.datetime(2024, 11, 6, 15, 17, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2460048938, 'issue_id': 2635321489, 'author': 'kaxil', 'body': 'aah yes I see a related item in their changelog for MySQL 8.0.40 ([link here](https://dev.mysql.com/doc/relnotes/mysql/8.0/en/news-8-0-40.html#mysqld-8-0-40-bug:~:text=handled%20correctly.%20(Bug%20%23-,35221658,-)))\r\n\r\nIt may not be exactly be the same though', 'created_at': datetime.datetime(2024, 11, 6, 15, 20, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2460057069, 'issue_id': 2635321489, 'author': 'kaxil', 'body': 'Could someone on this thread who were experiencing issue help and try to reproduce it with 8.0.40, please?\r\n\r\nThat way we can narrow it down', 'created_at': datetime.datetime(2024, 11, 6, 15, 23, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2462274476, 'issue_id': 2635321489, 'author': 'prabhusneha', 'body': 'I couldn\'t reproduce the issue either. Attaching some screenshots and descriptions on how I tried to reproduce it. \r\n\r\n![image](https://github.com/user-attachments/assets/03c60717-7a1c-4d43-9705-0cfb949a3630)\r\n\r\n1. As seen in the above image, started with airflow version 2.9.2. MySQL backend version 8.0.40. \r\n2. Ran a sample DAG multiple times to populate some metadata.\r\n3. Executed `airflow db downgrade --to-version=2.8.2 --yes`\r\nRevision id applied is 88344c1d9134 and downgrade completed successfully.\r\n4. Upgraded successfully using `airflow db migrate` \r\n\r\nAfter the upgrade and downgrade, I checked a few things in the database (see image below):\r\nThe unique key on the connection table is named connection_conn_id_uq. When I ran the query that caused the error mentioned in this issue, it executed without any errors.The query output is also shown in the image.\r\n\r\n<img width=""699"" alt=""image"" src=""https://github.com/user-attachments/assets/cb3adcce-e396-489b-931c-d86e09c6d60a"">', 'created_at': datetime.datetime(2024, 11, 7, 13, 43, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2538776099, 'issue_id': 2635321489, 'author': 'hatched-DavidMichon', 'body': 'I was able to reproduce this issue and narrow it down. I am using aurora mysql 8.0.39. While testing directly on the database with a unix mysql client I can run the prepare statement query without any issue.\r\n\r\nBut when I am doing the same via python, using PyMySQL, it errors out. Per this open issue https://github.com/PyMySQL/PyMySQL/issues/202, prepared statement are not supported on PyMySQL. Does anyone else has this migration issue using PyMySQL?', 'created_at': datetime.datetime(2024, 12, 12, 12, 30, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2538807872, 'issue_id': 2635321489, 'author': 'xia-stan', 'body': ""The error is reported on MySQL versions 8.0.28, 8.0.32, 8.0.36. Others have been unable to reproduce the bug on MySQL 8.0.38 and 8.0.40. The fix may have happened in 8.0.37. I don't see anything obvious in the [release notes](https://dev.mysql.com/doc/relnotes/mysql/8.0/en/news-8-0-37.html)."", 'created_at': datetime.datetime(2024, 12, 12, 12, 45, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2538818549, 'issue_id': 2635321489, 'author': 'hatched-DavidMichon', 'body': ""@xia-stan per the test I did, my understanding is that the mysql version isn't the culprit. This issue is with the python mysql connector library used. I'm using PyMySQL. I'm willing to understand if others that see no issues are using a different library."", 'created_at': datetime.datetime(2024, 12, 12, 12, 50, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2538849020, 'issue_id': 2635321489, 'author': 'xia-stan', 'body': ""@hatched-DavidMichon : Missed that point, sorry! Pulled up my config. I'm using PyMySQL 1.1.1. I went through all the posted error logs and looks like all the errors are contained to PyMySQL. I think you're right about the root cause."", 'created_at': datetime.datetime(2024, 12, 12, 13, 2, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2538863462, 'issue_id': 2635321489, 'author': 'kaxil', 'body': 'Ah that makes sense -- mystery solved! We recommend `mysqlclient` because of those sort of issues\r\n\r\n>We recommend using the mysqlclient driver and specifying it in your SqlAlchemy connection string.\r\n\r\nhttps://airflow.apache.org/docs/apache-airflow/stable/howto/set-up-database.html#setting-up-a-mysql-database', 'created_at': datetime.datetime(2024, 12, 12, 13, 8, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2538868708, 'issue_id': 2635321489, 'author': 'kaxil', 'body': 'That said, we should solve this, does anyone fancy a PR to not use prepare statement when using `PyMySQL`?', 'created_at': datetime.datetime(2024, 12, 12, 13, 11, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2538877621, 'issue_id': 2635321489, 'author': 'hatched-DavidMichon', 'body': '> Ah that makes sense -- mystery solved! We recommend `mysqlclient` because of those sort of issues\r\n> \r\n> > We recommend using the mysqlclient driver and specifying it in your SqlAlchemy connection string.\r\n> \r\n> https://airflow.apache.org/docs/apache-airflow/stable/howto/set-up-database.html#setting-up-a-mysql-database\r\n\r\n@kaxil right, this is precisely what I am at right now to solve it.', 'created_at': datetime.datetime(2024, 12, 12, 13, 13, 49, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-05 12:28:09 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

xia-stan on (2024-11-05 13:42:54 UTC): Can confirm that this error happens on Airflow 2.10.3 with MySQL 8.0.28. Confirmed too that a fresh DB poses no issues.

OlafenwaMoses on (2024-11-06 12:26:59 UTC): ---
cross posting from the closed issue https://github.com/apache/airflow/issues/40928
---

I am getting same issue to. I am using MySQL `8.0.32` and any attempt to upgrade beyond `2.9.1` on an existing database leads to the error in the trace below. I have tried

- Airflow `2.9.1` to `2.9.2` with MySQL `8.0.32` 
- Airflow `2.9.1` to `2.9.3` MySQL `8.0.32` 
- Airflow `2.9.1` to `2.9.3` with MySQL `8.0.36`

and apparently all failed with same error despite the fix in [#40314 ](https://github.com/apache/airflow/pull/40314). 

As @JayalakshmiH  mentioned, Airflow `2.9.2` and higher only works when you setup with an empty db.

```
2024-11-04 22:14:41 sqlalchemy.exc.ProgrammingError: (pymysql.err.ProgrammingError) (1064, ""You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'prepare stmt from @var;\n        execute stmt;\n        deallocate prepare stmt' at line 7"")
2024-11-04 22:14:41 [SQL: 
2024-11-04 22:14:41         set @var=if((SELECT true FROM information_schema.TABLE_CONSTRAINTS WHERE
2024-11-04 22:14:41             CONSTRAINT_SCHEMA = DATABASE() AND
2024-11-04 22:14:41             TABLE_NAME        = 'connection' AND
2024-11-04 22:14:41             CONSTRAINT_NAME   = 'unique_conn_id' AND
2024-11-04 22:14:41             CONSTRAINT_TYPE   = 'UNIQUE') = true,'ALTER TABLE connection
2024-11-04 22:14:41             DROP INDEX unique_conn_id','select 1');
2024-11-04 22:14:41 
2024-11-04 22:14:41         prepare stmt from @var;
2024-11-04 22:14:41         execute stmt;
2024-11-04 22:14:41         deallocate prepare stmt;
2024-11-04 22:14:41         ]
2024-11-04 22:14:41 (Background on this error at: https://sqlalche.me/e/14/f405)
2024-11-04 22:14:41 WARNING: Environment variable '_AIRFLOW_DB_UPGRADE' is deprecated please use '_AIRFLOW_DB_MIGRATE' instead
```

kaxil on (2024-11-06 14:46:27 UTC): I am not able to reproduce this:

MySQL: `8.0.40`

```sql
irflow> SELECT VERSION()
[2024-11-06 14:45:01] 1 row retrieved starting from 1 in 34 ms (execution: 13 ms, fetching: 21 ms)
airflow> set @var=if((SELECT true FROM information_schema.TABLE_CONSTRAINTS WHERE
             CONSTRAINT_SCHEMA = DATABASE() AND
             TABLE_NAME        = 'connection' AND
             CONSTRAINT_NAME   = 'unique_conn_id' AND
             CONSTRAINT_TYPE   = 'UNIQUE') = true,'ALTER TABLE connection
             DROP INDEX unique_conn_id','select 1')
[2024-11-06 14:45:06] completed in 3 ms
airflow> prepare stmt from @var
[2024-11-06 14:45:06] completed in 2 ms
airflow> execute stmt
[2024-11-06 14:45:06] 1 row retrieved starting from 1 in 18 ms (execution: 4 ms, fetching: 14 ms)
```

kaxil on (2024-11-06 14:48:16 UTC): @JayalakshmiH By any chance are you using a different flavour of MySQL like MariaDB?

xia-stan on (2024-11-06 15:14:21 UTC): @kaxil : I'm using a cloud host. I double checked my MySQL version with the command you posted. It returns `8.0.28-0ubuntu0.20.04.3`. [From the package repo](https://launchpad.net/ubuntu/+source/mysql-8.0/8.0.28-0ubuntu0.20.04.3), it seems like this is definitely MySQL and not MariaDB. I didn't bother posting my error message as it was identical to @OlafenwaMoses's.

kaxil on (2024-11-06 15:17:51 UTC): hmm curious, maybe it was a bug in MySQL in that version

kaxil on (2024-11-06 15:20:20 UTC): aah yes I see a related item in their changelog for MySQL 8.0.40 ([link here](https://dev.mysql.com/doc/relnotes/mysql/8.0/en/news-8-0-40.html#mysqld-8-0-40-bug:~:text=handled%20correctly.%20(Bug%20%23-,35221658,-)))

It may not be exactly be the same though

kaxil on (2024-11-06 15:23:31 UTC): Could someone on this thread who were experiencing issue help and try to reproduce it with 8.0.40, please?

That way we can narrow it down

prabhusneha on (2024-11-07 13:43:03 UTC): I couldn't reproduce the issue either. Attaching some screenshots and descriptions on how I tried to reproduce it. 

![image](https://github.com/user-attachments/assets/03c60717-7a1c-4d43-9705-0cfb949a3630)

1. As seen in the above image, started with airflow version 2.9.2. MySQL backend version 8.0.40. 
2. Ran a sample DAG multiple times to populate some metadata.
3. Executed `airflow db downgrade --to-version=2.8.2 --yes`
Revision id applied is 88344c1d9134 and downgrade completed successfully.
4. Upgraded successfully using `airflow db migrate` 

After the upgrade and downgrade, I checked a few things in the database (see image below):
The unique key on the connection table is named connection_conn_id_uq. When I ran the query that caused the error mentioned in this issue, it executed without any errors.The query output is also shown in the image.

<img width=""699"" alt=""image"" src=""https://github.com/user-attachments/assets/cb3adcce-e396-489b-931c-d86e09c6d60a"">

hatched-DavidMichon on (2024-12-12 12:30:53 UTC): I was able to reproduce this issue and narrow it down. I am using aurora mysql 8.0.39. While testing directly on the database with a unix mysql client I can run the prepare statement query without any issue.

But when I am doing the same via python, using PyMySQL, it errors out. Per this open issue https://github.com/PyMySQL/PyMySQL/issues/202, prepared statement are not supported on PyMySQL. Does anyone else has this migration issue using PyMySQL?

xia-stan on (2024-12-12 12:45:25 UTC): The error is reported on MySQL versions 8.0.28, 8.0.32, 8.0.36. Others have been unable to reproduce the bug on MySQL 8.0.38 and 8.0.40. The fix may have happened in 8.0.37. I don't see anything obvious in the [release notes](https://dev.mysql.com/doc/relnotes/mysql/8.0/en/news-8-0-37.html).

hatched-DavidMichon on (2024-12-12 12:50:17 UTC): @xia-stan per the test I did, my understanding is that the mysql version isn't the culprit. This issue is with the python mysql connector library used. I'm using PyMySQL. I'm willing to understand if others that see no issues are using a different library.

xia-stan on (2024-12-12 13:02:58 UTC): @hatched-DavidMichon : Missed that point, sorry! Pulled up my config. I'm using PyMySQL 1.1.1. I went through all the posted error logs and looks like all the errors are contained to PyMySQL. I think you're right about the root cause.

kaxil on (2024-12-12 13:08:56 UTC): Ah that makes sense -- mystery solved! We recommend `mysqlclient` because of those sort of issues


https://airflow.apache.org/docs/apache-airflow/stable/howto/set-up-database.html#setting-up-a-mysql-database

kaxil on (2024-12-12 13:11:03 UTC): That said, we should solve this, does anyone fancy a PR to not use prepare statement when using `PyMySQL`?

hatched-DavidMichon on (2024-12-12 13:13:49 UTC): @kaxil right, this is precisely what I am at right now to solve it.

"
2635019925,issue,closed,completed,Datasets tab/information not visible in UI,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I am using an admin role that has full permissions over datasets. When I press on button to view more detail about a detaset, it flickers on for a second and then says I do not have permission. I also do not see any ""Dataset"" tab in my UI. 


https://github.com/user-attachments/assets/4f8e5b12-cec8-4e05-b0c9-b11d3d6e0ade



### What you think should happen instead?

From reading the docs, I understand that there should be a Dataset tab visible for me to view. I also have all of the permissions I need. I am using the demo DAGS from airflow. 

### How to reproduce

Run a webserver, run scheduler, use one of the dataset-based example DAGS. 

### Operating System

Mac OS Sonoma 14.7.1

### Versions of Apache Airflow Providers

Providers info
apache-airflow-providers-common-compat | 1.2.1 
apache-airflow-providers-common-io     | 1.4.2 
apache-airflow-providers-common-sql    | 1.18.0
apache-airflow-providers-fab           | 1.4.1 
apache-airflow-providers-ftp           | 3.11.1
apache-airflow-providers-http          | 4.13.1
apache-airflow-providers-imap          | 3.7.0 
apache-airflow-providers-smtp          | 1.8.0 
apache-airflow-providers-sqlite        | 3.9.0 

### Deployment

Other

### Deployment details

Running locally in a conda environment

### Anything else?

server logs: 


```
127.0.0.1 - - [05/Nov/2024:04:14:29 -0600] ""GET /datasets?uri=s3%3A%2F%2Fdag1%2Foutput_1.txt HTTP/1.1"" 302 197 ""http://localhost:8080/home"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36""
127.0.0.1 - - [05/Nov/2024:04:14:29 -0600] ""GET /home HTTP/1.1"" 200 42995 ""http://localhost:8080/home"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36""
127.0.0.1 - - [05/Nov/2024:04:14:30 -0600] ""POST /last_dagruns HTTP/1.1"" 200 2 ""http://localhost:8080/home"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36""

```

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",nickhealy,2024-11-05 10:15:18+00:00,[],2024-11-06 04:16:55+00:00,2024-11-06 04:16:54+00:00,https://github.com/apache/airflow/issues/43685,"[('kind:bug', 'This is a clearly a bug'), ('pending-response', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('area:datasets', 'Issues related to the datasets feature')]","[{'comment_id': 2456770210, 'issue_id': 2635019925, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 5, 10, 15, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2457664242, 'issue_id': 2635019925, 'author': 'andysouthcombe', 'body': 'We have a similar issue.  Since upgrading to 2.10.2 the datasets menu tab is no longer visible.  When I try to browse directly to the datasets url (<airflow base url>/datasets) I get an ""access is denied"" message.  I\'ve verified my user\'s role still has the relevant dataset permissions using the Airflow CLI.  I\'ve tested locally and found:\r\n\r\n- Airflow versions 2.10.0 and 2.10.1 display the datasets menu tab normally\r\n- Airflow versions 2.10.2 and 2.10.3 no longer display the datasets tab\r\n\r\nSo it looks like something broke in 2.10.2 or there was some behaviour change that isn\'t obvious from the release notes.', 'created_at': datetime.datetime(2024, 11, 5, 16, 39, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2457756061, 'issue_id': 2635019925, 'author': 'vatsrahul1001', 'body': '@nickhealy @andysouthcombe what version of `apache-airflow-providers-fab` you are using?', 'created_at': datetime.datetime(2024, 11, 5, 17, 19, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2457962131, 'issue_id': 2635019925, 'author': 'andysouthcombe', 'body': 'apache-airflow-providers-fab==1.4.1 for me', 'created_at': datetime.datetime(2024, 11, 5, 19, 12, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2457969540, 'issue_id': 2635019925, 'author': 'andysouthcombe', 'body': 'Looks like upgrading that to 1.5 might fix the issue for me.   We installed airflow 2.10.2 with poetry and we ended up with 1.4.1, but upgrading to apache-airflow-providers-fab==1.5 has made the datasets tab reappear', 'created_at': datetime.datetime(2024, 11, 5, 19, 16, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2458693302, 'issue_id': 2635019925, 'author': 'vatsrahul1001', 'body': 'Yes `1.5 ` has the fix', 'created_at': datetime.datetime(2024, 11, 6, 4, 16, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2458693934, 'issue_id': 2635019925, 'author': 'vatsrahul1001', 'body': 'Closing this as confirmed upgrading to `apache-airflow-providers-fab==1.5` fixed the issue', 'created_at': datetime.datetime(2024, 11, 6, 4, 16, 54, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-05 10:15:21 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

andysouthcombe on (2024-11-05 16:39:19 UTC): We have a similar issue.  Since upgrading to 2.10.2 the datasets menu tab is no longer visible.  When I try to browse directly to the datasets url (<airflow base url>/datasets) I get an ""access is denied"" message.  I've verified my user's role still has the relevant dataset permissions using the Airflow CLI.  I've tested locally and found:

- Airflow versions 2.10.0 and 2.10.1 display the datasets menu tab normally
- Airflow versions 2.10.2 and 2.10.3 no longer display the datasets tab

So it looks like something broke in 2.10.2 or there was some behaviour change that isn't obvious from the release notes.

vatsrahul1001 on (2024-11-05 17:19:37 UTC): @nickhealy @andysouthcombe what version of `apache-airflow-providers-fab` you are using?

andysouthcombe on (2024-11-05 19:12:09 UTC): apache-airflow-providers-fab==1.4.1 for me

andysouthcombe on (2024-11-05 19:16:45 UTC): Looks like upgrading that to 1.5 might fix the issue for me.   We installed airflow 2.10.2 with poetry and we ended up with 1.4.1, but upgrading to apache-airflow-providers-fab==1.5 has made the datasets tab reappear

vatsrahul1001 on (2024-11-06 04:16:11 UTC): Yes `1.5 ` has the fix

vatsrahul1001 on (2024-11-06 04:16:54 UTC): Closing this as confirmed upgrading to `apache-airflow-providers-fab==1.5` fixed the issue

"
2635015401,issue,open,,Make sure `uv.lock` in breeze corresponds to `pyproject.toml`,"When you manually modify breeze's `pyproject.toml`, the `uv.lock` might not correspond to the changes you've just added. Breeze manages installation automatically when you run it, so when you use `uv tool` it should use the .lock, so whenever we modify `pyproject.toml` we should also make sure `uv sync` is run to reflect that.

This can be done with a `pre-commit`. The `uv sync` should only modify lock if `pyproject.toml` changed (unles `--frozen` option is used for `uv sync` - so we should be able to safely run `uv lock` in pre-commit for breeze, just to make sure no changes in `pyproject.toml` should be reflected in the lock file.",potiuk,2024-11-05 10:13:27+00:00,['enisnazif'],2024-11-29 12:54:23+00:00,,https://github.com/apache/airflow/issues/43684,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:dev-tools', '')]","[{'comment_id': 2466415180, 'issue_id': 2635015401, 'author': 'enisnazif', 'body': 'I can take a look at this', 'created_at': datetime.datetime(2024, 11, 9, 19, 11, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507564670, 'issue_id': 2635015401, 'author': 'kunaljubce', 'body': '@enisnazif Are you still working on this?', 'created_at': datetime.datetime(2024, 11, 29, 10, 54, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507685803, 'issue_id': 2635015401, 'author': 'enisnazif', 'body': 'Yes - sorry, hoping to finish it off this weekend!\r\n\r\nOn Fri, 29 Nov 2024, 10:54 Kunal Bhattacharya, ***@***.***>\r\nwrote:\r\n\r\n> @enisnazif <https://github.com/enisnazif> Are you still working on this?\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/apache/airflow/issues/43684#issuecomment-2507564670>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AE3G2WWOW4PJ5BD4AD5DSOT2DBBXFAVCNFSM6AAAAABRGFVXH6VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKMBXGU3DINRXGA>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>', 'created_at': datetime.datetime(2024, 11, 29, 12, 8, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507765580, 'issue_id': 2635015401, 'author': 'kunaljubce', 'body': 'Awesome, bring it on 👍', 'created_at': datetime.datetime(2024, 11, 29, 12, 54, 21, tzinfo=datetime.timezone.utc)}]","enisnazif (Assginee) on (2024-11-09 19:11:13 UTC): I can take a look at this

kunaljubce on (2024-11-29 10:54:20 UTC): @enisnazif Are you still working on this?

enisnazif (Assginee) on (2024-11-29 12:08:25 UTC): Yes - sorry, hoping to finish it off this weekend!

On Fri, 29 Nov 2024, 10:54 Kunal Bhattacharya, ***@***.***>
wrote:

kunaljubce on (2024-11-29 12:54:21 UTC): Awesome, bring it on 👍

"
2633807806,issue,closed,not_planned,AIP-81 Implement List Jobs Endpoint in FastAPI,"### Description

Develop an endpoint in FastAPI that provides a list of all jobs, allowing users to view jobs.

### Use case/motivation

See related issue.

### Related issues

#43657

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-11-04 20:45:54+00:00,['josix'],2024-11-17 15:24:50+00:00,2024-11-17 15:24:49+00:00,https://github.com/apache/airflow/issues/43661,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('good first issue', ''), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]","[{'comment_id': 2456538891, 'issue_id': 2633807806, 'author': 'josix', 'body': 'Hi @bugraoz93, I can help resolve this issue, thanks!', 'created_at': datetime.datetime(2024, 11, 5, 8, 32, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2456888912, 'issue_id': 2633807806, 'author': 'bugraoz93', 'body': 'Thanks for assigning @rawwar! For sure, @josix  go ahead, any help would be great!', 'created_at': datetime.datetime(2024, 11, 5, 11, 10, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2481315284, 'issue_id': 2633807806, 'author': 'bugraoz93', 'body': 'Closing this in favour of discussion here #43859', 'created_at': datetime.datetime(2024, 11, 17, 15, 24, 49, tzinfo=datetime.timezone.utc)}]","josix (Assginee) on (2024-11-05 08:32:11 UTC): Hi @bugraoz93, I can help resolve this issue, thanks!

bugraoz93 (Issue Creator) on (2024-11-05 11:10:53 UTC): Thanks for assigning @rawwar! For sure, @josix  go ahead, any help would be great!

bugraoz93 (Issue Creator) on (2024-11-17 15:24:49 UTC): Closing this in favour of discussion here #43859

"
2633800470,issue,closed,completed,AIP-81 Implement List Jobs with Filter Endpoint in FastAPI,"### Description

Create an endpoint in FastAPI that lists `Jobs` with the ability to filter by specific criteria like `state`, `job_type`, `hostname`, and `is_alive` status. This will allow users to efficiently query and monitor jobs based on their current status and other attributes.

### Use case/motivation

See related issue.

### Related issues

#43657

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-11-04 20:42:56+00:00,['jason810496'],2024-11-25 13:44:44+00:00,2024-11-25 13:44:44+00:00,https://github.com/apache/airflow/issues/43660,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('good first issue', ''), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]","[{'comment_id': 2458924778, 'issue_id': 2633800470, 'author': 'bugraoz93', 'body': '@jason810496 , this is the one I mentioned in the umbrella issue', 'created_at': datetime.datetime(2024, 11, 6, 7, 51, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2458953067, 'issue_id': 2633800470, 'author': 'jason810496', 'body': 'I see, I left the comment on the wrong issue while navigating between this one and #43657. 😅\r\nI meant to take on this one. Thanks!', 'created_at': datetime.datetime(2024, 11, 6, 8, 9, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2458968516, 'issue_id': 2633800470, 'author': 'bugraoz93', 'body': 'Assigned, thanks! :)', 'created_at': datetime.datetime(2024, 11, 6, 8, 17, 53, tzinfo=datetime.timezone.utc)}]","bugraoz93 (Issue Creator) on (2024-11-06 07:51:54 UTC): @jason810496 , this is the one I mentioned in the umbrella issue

jason810496 (Assginee) on (2024-11-06 08:09:18 UTC): I see, I left the comment on the wrong issue while navigating between this one and #43657. 😅
I meant to take on this one. Thanks!

bugraoz93 (Issue Creator) on (2024-11-06 08:17:53 UTC): Assigned, thanks! :)

"
2633773372,issue,closed,completed,AIP-81 Implement Missing API Endpoints for Jobs will be used from CLI Commands,"### Description

To enhance the API functionality, this umbrella ticket organizes the development of missing endpoints for `Jobs`. Implementing these endpoints will enable users to manage resources more effectively through the API, improving usability and reducing the need for multiple workarounds.
The main aim of the workload will help to make CLI secure via using these endpoints.

- List Jobs with Filter (like State, job_type, hostname, is_alive) -> https://github.com/apache/airflow/issues/43660 @jason810496 
- List Jobs -> https://github.com/apache/airflow/issues/43661 @josix 

### Use case/motivation

[AIP-81](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-81+Enhanced+Security+in+CLI+via+Integration+of+API)

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-11-04 20:29:05+00:00,"['bugraoz93', 'jason810496']",2024-12-01 13:23:55+00:00,2024-12-01 13:23:55+00:00,https://github.com/apache/airflow/issues/43657,"[('area:CLI', ''), ('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]","[{'comment_id': 2458889374, 'issue_id': 2633773372, 'author': 'jason810496', 'body': 'Hi @bugraoz93, I can take on this API. Could you please assign the issue to me? Thanks!', 'created_at': datetime.datetime(2024, 11, 6, 7, 27, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2458922385, 'issue_id': 2633773372, 'author': 'bugraoz93', 'body': 'Hi @jason810496 , this issue is consisted on two parts/endpoints. One of them is already assigned. I can assign you the other one, which is listing jobs with filters if you want to work on it', 'created_at': datetime.datetime(2024, 11, 6, 7, 50, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2459067554, 'issue_id': 2633773372, 'author': 'potiuk', 'body': 'I assigned both of you :)', 'created_at': datetime.datetime(2024, 11, 6, 9, 6, 43, tzinfo=datetime.timezone.utc)}]","jason810496 (Assginee) on (2024-11-06 07:27:02 UTC): Hi @bugraoz93, I can take on this API. Could you please assign the issue to me? Thanks!

bugraoz93 (Issue Creator) on (2024-11-06 07:50:17 UTC): Hi @jason810496 , this issue is consisted on two parts/endpoints. One of them is already assigned. I can assign you the other one, which is listing jobs with filters if you want to work on it

potiuk on (2024-11-06 09:06:43 UTC): I assigned both of you :)

"
2633765188,issue,open,,AIP-81 Unified Authentication Mechanism for CLI,"### Description

  * Automated Flow
  * User Flow

### Use case/motivation

[AIP-81](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-81+Enhanced+Security+in+CLI+via+Integration+of+API)

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-11-04 20:24:01+00:00,[],2024-11-04 20:26:12+00:00,,https://github.com/apache/airflow/issues/43656,"[('area:CLI', ''), ('kind:feature', 'Feature Requests'), ('area:auth', ''), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]",[],
2633762669,issue,closed,completed,AIP-81 Transition of all direct database calls to API calls (other than Local Commands),"### Description

_No response_

### Use case/motivation


[AIP-81](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-81+Enhanced+Security+in+CLI+via+Integration+of+API)

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-11-04 20:22:33+00:00,[],2025-01-15 01:20:17+00:00,2025-01-15 01:20:16+00:00,https://github.com/apache/airflow/issues/43655,"[('area:CLI', ''), ('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]","[{'comment_id': 2591436489, 'issue_id': 2633762669, 'author': 'bugraoz93', 'body': 'Closing in favor of #45661 using sub-issues. Duplicate.', 'created_at': datetime.datetime(2025, 1, 15, 1, 20, 16, tzinfo=datetime.timezone.utc)}]","bugraoz93 (Issue Creator) on (2025-01-15 01:20:16 UTC): Closing in favor of #45661 using sub-issues. Duplicate.

"
2633693933,issue,closed,completed,AIP-81 Implement POST/Insert Multiple Connections in FastAPI,"### Description

Develop an endpoint in FastAPI that enables `POST` one or multiple connections from a list. This feature should allow users to import their connections from a set of connections in a single request.

### Related issues

https://github.com/apache/airflow/issues/42560

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-11-04 19:45:29+00:00,['bugraoz93'],2024-12-01 14:19:46+00:00,2024-12-01 14:19:45+00:00,https://github.com/apache/airflow/issues/43652,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]",[],
2633669070,issue,closed,not_planned,AIP-81 Implement Export Connections to a File Endpoint in FastAPI,"### Description

Develop an endpoint in FastAPI that enables exporting one or multiple connections to a file. This feature should allow users to export their connections as a downloadable file in a single request.

### Related issues

#42560

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-11-04 19:33:19+00:00,['bugraoz93'],2024-11-11 20:41:51+00:00,2024-11-11 20:11:42+00:00,https://github.com/apache/airflow/issues/43651,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]","[{'comment_id': 2468987613, 'issue_id': 2633669070, 'author': 'bugraoz93', 'body': ""For the sake of workload and security perspective, there won't be any file manipulation in API in the context of [AIP-81](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-81+Enhanced+Security+in+CLI+via+Integration+of+API) and will be handled in CLI. So closed the issue."", 'created_at': datetime.datetime(2024, 11, 11, 20, 41, 50, tzinfo=datetime.timezone.utc)}]","bugraoz93 (Issue Creator) on (2024-11-11 20:41:50 UTC): For the sake of workload and security perspective, there won't be any file manipulation in API in the context of [AIP-81](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-81+Enhanced+Security+in+CLI+via+Integration+of+API) and will be handled in CLI. So closed the issue.

"
2633599460,issue,closed,completed,AIP-84 Standardize PATCH Behavior Across Endpoints in FastAPI,"### Description

#### None Values
Currently, the handling of `None` values in `PATCH` requests is inconsistent across different parts of the codebase. In some instances, when no mask is provided, None values are filtered out, while in others, such as the `Pool` model, all values (including None) are validated and persisted if the payload is valid. The objective of this work item is to standardize `PATCH` request behaviour regarding None values:
1. When a mask is provided, use the values as specified by the mask.
2. When no mask is provided, ensure the payload is validated independently as a fully-formed database entity, allowing for explicit setting of fields to None.

#### Database Calls
Avoid making database calls solely to check for existence; instead, delegate this responsibility to the database itself during the primary operation.

### Use case/motivation

A standardized approach for PATCH endpoints is needed.

### Related issues

Relates to: #42370
Follow Up of: #43102

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-11-04 18:56:21+00:00,['ajitg25'],2024-12-12 09:09:22+00:00,2024-12-12 09:09:22+00:00,https://github.com/apache/airflow/issues/43650,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('good first issue', ''), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2455473903, 'issue_id': 2633599460, 'author': 'bugraoz93', 'body': '@pierrejeambrun created this as a follow-up for `PATCH` endpoints. FYI', 'created_at': datetime.datetime(2024, 11, 4, 18, 57, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2457367582, 'issue_id': 2633599460, 'author': 'pierrejeambrun', 'body': 'Thanks a lot, great description.', 'created_at': datetime.datetime(2024, 11, 5, 14, 45, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2506405695, 'issue_id': 2633599460, 'author': 'pierrejeambrun', 'body': 'Up for grab, #good first issue!', 'created_at': datetime.datetime(2024, 11, 28, 15, 48, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2510581533, 'issue_id': 2633599460, 'author': 'ajitg25', 'body': 'Hello @pierrejeambrun,\r\n\r\nI went through the related links provided in the description, and here’s my understanding:\r\n\r\nTo standardize the implementation, I need to add a `mask` parameter to the function. If the parameter is passed, I will use the specified values; otherwise, I will validate the entire payload.\r\n\r\nI have updated the `create_or_update_pool` function accordingly:\r\n```@staticmethod\r\n@provide_session\r\ndef create_or_update_pool(\r\n    name: str,\r\n    slots: int | None,\r\n    description: str | None,\r\n    include_deferred: bool | None,\r\n    session: Session = NEW_SESSION,\r\n    mask: list[str] | None = None,\r\n) -> Pool:\r\n    """"""Create a pool with given parameters or update it if it already exists.""""""\r\n    if not name:\r\n        raise ValueError(""Pool name must not be empty"")\r\n    pool = session.scalar(select(Pool).filter_by(pool=name))\r\n    if pool is None:\r\n        pool = Pool(pool=name, slots=slots, description=description, include_deferred=include_deferred)\r\n        session.add(pool)\r\n    else:\r\n        if mask:\r\n            if \'slots\' in mask:\r\n                pool.slots = slots\r\n            if \'description\' in mask:\r\n                pool.description = description\r\n            if \'include_deferred\' in mask:\r\n                pool.include_deferred = include_deferred\r\n        else:\r\n            pool.slots = slots\r\n            pool.description = description\r\n            pool.include_deferred = include_deferred\r\n    session.commit()\r\n    return pool\r\n```\r\n    \r\nDoes this align with the requirements?\r\n\r\nAdditionally, regarding the description mentioning changes in different parts of the codebase, I found several functions with create_or_update. Are there any others that I should look into? Let me know your thoughts on the scope of changes and the specific files that need to be updated.', 'created_at': datetime.datetime(2024, 12, 2, 4, 58, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2512212486, 'issue_id': 2633599460, 'author': 'pierrejeambrun', 'body': 'Hello [ajitg25](https://github.com/ajitg25),\r\n\r\nI have assigned you. The function you are mentioning is located in the `models`. We do not want to update that part.\r\n\r\nThis issue is referring to HTTP endpoints that are located in the FastAPI Rest API. Files of interests are in `api_fastapi`. More precisely in `api_fastapi/core_api/routes/public` for the public endpoint definition. In here you will find a multitude of endpoints, and several are for `updating` a resource. (Pool, Connection, Variable, etc...). We want the behavior of those `Update` endpoints to be harmonized, especially regarding the `mask` query parameter and also for `None` value handling.\r\n\r\nThe idea is:\r\n- If a mask is provided, only update values specified in the mask (partial update should be supported, for instance if the user only specify 1 field in the payload and in the mask)\r\n- If no mask is provided, instanciate a DB entity and ensure that the payload is valid in that regard. update payload is a fully formed resource\r\n\r\nIn both cases I think we should only use `set` values. Things that the user explicitely specified in the payload. (independently if those are None or not None). You can take a look at `patch_task_instance` that leverages `model_fields_set` to only retrive explicitely set values.', 'created_at': datetime.datetime(2024, 12, 2, 17, 19, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2518216885, 'issue_id': 2633599460, 'author': 'ajitg25', 'body': '@pierrejeambrun I have raised a PR for review. PTAL!!\r\nThank you', 'created_at': datetime.datetime(2024, 12, 4, 18, 26, 44, tzinfo=datetime.timezone.utc)}]","bugraoz93 (Issue Creator) on (2024-11-04 18:57:39 UTC): @pierrejeambrun created this as a follow-up for `PATCH` endpoints. FYI

pierrejeambrun on (2024-11-05 14:45:24 UTC): Thanks a lot, great description.

pierrejeambrun on (2024-11-28 15:48:43 UTC): Up for grab, #good first issue!

ajitg25 (Assginee) on (2024-12-02 04:58:08 UTC): Hello @pierrejeambrun,

I went through the related links provided in the description, and here’s my understanding:

To standardize the implementation, I need to add a `mask` parameter to the function. If the parameter is passed, I will use the specified values; otherwise, I will validate the entire payload.

I have updated the `create_or_update_pool` function accordingly:
```@staticmethod
@provide_session
def create_or_update_pool(
    name: str,
    slots: int | None,
    description: str | None,
    include_deferred: bool | None,
    session: Session = NEW_SESSION,
    mask: list[str] | None = None,
) -> Pool:
    """"""Create a pool with given parameters or update it if it already exists.""""""
    if not name:
        raise ValueError(""Pool name must not be empty"")
    pool = session.scalar(select(Pool).filter_by(pool=name))
    if pool is None:
        pool = Pool(pool=name, slots=slots, description=description, include_deferred=include_deferred)
        session.add(pool)
    else:
        if mask:
            if 'slots' in mask:
                pool.slots = slots
            if 'description' in mask:
                pool.description = description
            if 'include_deferred' in mask:
                pool.include_deferred = include_deferred
        else:
            pool.slots = slots
            pool.description = description
            pool.include_deferred = include_deferred
    session.commit()
    return pool
```
    
Does this align with the requirements?

Additionally, regarding the description mentioning changes in different parts of the codebase, I found several functions with create_or_update. Are there any others that I should look into? Let me know your thoughts on the scope of changes and the specific files that need to be updated.

pierrejeambrun on (2024-12-02 17:19:52 UTC): Hello [ajitg25](https://github.com/ajitg25),

I have assigned you. The function you are mentioning is located in the `models`. We do not want to update that part.

This issue is referring to HTTP endpoints that are located in the FastAPI Rest API. Files of interests are in `api_fastapi`. More precisely in `api_fastapi/core_api/routes/public` for the public endpoint definition. In here you will find a multitude of endpoints, and several are for `updating` a resource. (Pool, Connection, Variable, etc...). We want the behavior of those `Update` endpoints to be harmonized, especially regarding the `mask` query parameter and also for `None` value handling.

The idea is:
- If a mask is provided, only update values specified in the mask (partial update should be supported, for instance if the user only specify 1 field in the payload and in the mask)
- If no mask is provided, instanciate a DB entity and ensure that the payload is valid in that regard. update payload is a fully formed resource

In both cases I think we should only use `set` values. Things that the user explicitely specified in the payload. (independently if those are None or not None). You can take a look at `patch_task_instance` that leverages `model_fields_set` to only retrive explicitely set values.

ajitg25 (Assginee) on (2024-12-04 18:26:44 UTC): @pierrejeambrun I have raised a PR for review. PTAL!!
Thank you

"
2633565219,issue,open,,Handle older serialized DAGs in the db,"### Body

With AIP-65, we will now have old version of serialized DAGs in the db for the UI to use. This, however, means that we cannot solely rely on reserializing all DAGs during an upgrade to avoid backwards compatibility for deserializing.

We should spend some time exploring how we should handle this long term. Some ideas:

- Add testing to make sure deserializing dags works with older serdag versions (makes future serde changes harder)
- Require migrations for serialized dags in the db, so the latest deser code works

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2024-11-04 18:40:01+00:00,['ashb'],2024-12-02 13:45:05+00:00,,https://github.com/apache/airflow/issues/43648,"[('area:serialization', ''), ('kind:meta', 'High-level information important to the community'), ('AIP-65: DAG history in UI', '')]","[{'comment_id': 2455878244, 'issue_id': 2633565219, 'author': 'potiuk', 'body': 'Interesting one indeed.', 'created_at': datetime.datetime(2024, 11, 4, 23, 3, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2511584977, 'issue_id': 2633565219, 'author': 'kaxil', 'body': 'Added this to AIP-72 board too. We will pick this up last and I might be OOO at that point, so re-assigning this to Ash for now', 'created_at': datetime.datetime(2024, 12, 2, 13, 45, 4, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-11-04 23:03:03 UTC): Interesting one indeed.

kaxil on (2024-12-02 13:45:04 UTC): Added this to AIP-72 board too. We will pick this up last and I might be OOO at that point, so re-assigning this to Ash for now

"
2633403706,issue,open,,Failed Airflow tasks aren't retried when using KubernetesExecutor,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.8.3

### What happened?

I'm creating a new issue for this since we've reproduced this on a newer version than the one mentioned in #21842 @potiuk.

We have the same setup as in the issue above:
* we use KubernetesExecutor with AWS spot instances
* all tasks have their retries set to at least 1

What happens is that when a spot instance is terminated the task is killed (by Kubernetes) – but Airflow scheduler never tries to reschedule it.

In this particular case, the pod didn't manage to start before the node initiated shutdown.

Here's an excerpt from the logs:

```
[2024-11-04 01:17:32,249][PID:7][INFO][airflow.jobs.scheduler_job_runner.SchedulerJobRunner] Received executor event with state failed for task instance TaskInstanceKey(dag_id='<dag_id>', task_id='<task_id>', run_id='scheduled__2024-11-03T00:00:00+00:00', try_number=1, map_index=-1)
[2024-11-04 01:17:28,807][PID:7][ERROR][airflow.jobs.scheduler_job_runner.SchedulerJobRunner] Executor reports task instance <TaskInstance: <dag_id>.<task_id> scheduled__2024-11-03T00:00:00+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?
[2024-11-04T01:17:28.783+0000] {kubernetes_executor_utils.py:269} ERROR - Event: <pod_name> Failed, annotations: <omitted>
```

And here are related logs over the lifespan of the task:

```
[2024-11-04 01:17:32,269][PID:7][ERROR][airflow.jobs.scheduler_job_runner.SchedulerJobRunner] Executor reports task instance <TaskInstance: <dag_id>.<task_id> scheduled__2024-11-03T00:00:00+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?
[2024-11-04T01:17:32.269+0000] {task_context_logger.py:91} ERROR - Executor reports task instance <TaskInstance: <dag_id>.<task_id> scheduled__2024-11-03T00:00:00+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?
[2024-11-04 01:17:32,268][PID:7][INFO][airflow.jobs.scheduler_job_runner.SchedulerJobRunner] TaskInstance Finished: dag_id=<dag_id>, task_id=<task_id>, run_id=scheduled__2024-11-03T00:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=small_tasks, queue=default, priority_weight=12, operator=CustomPythonOperator, queued_dttm=2024-11-04 01:17:13.321215+00:00, queued_by_job_id=2782000, pid=None
[2024-11-04T01:17:32.268+0000] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=<dag_id>, task_id=<task_id>, run_id=scheduled__2024-11-03T00:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=small_tasks, queue=default, priority_weight=12, operator=CustomPythonOperator, queued_dttm=2024-11-04 01:17:13.321215+00:00, queued_by_job_id=2782000, pid=None
[2024-11-04 01:17:32,249][PID:7][INFO][airflow.jobs.scheduler_job_runner.SchedulerJobRunner] Received executor event with state failed for task instance TaskInstanceKey(dag_id='<dag_id>', task_id='<task_id>', run_id='scheduled__2024-11-03T00:00:00+00:00', try_number=1, map_index=-1)
[2024-11-04T01:17:32.249+0000] {scheduler_job_runner.py:696} INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='<dag_id>', task_id='<task_id>', run_id='scheduled__2024-11-03T00:00:00+00:00', try_number=1, map_index=-1)
[2024-11-04 01:17:31,405][PID:7][INFO][airflow.providers.cncf.kubernetes.executors.kubernetes_executor.KubernetesExecutor] Changing state of (TaskInstanceKey(dag_id='<dag_id>', task_id='<task_id>', run_id='scheduled__2024-11-03T00:00:00+00:00', try_number=1, map_index=-1), <TaskInstanceState.FAILED: 'failed'>, '<pod_name>', 'airflow', '649610440') to failed
[2024-11-04T01:17:31.405+0000] {kubernetes_executor.py:401} INFO - Changing state of (TaskInstanceKey(dag_id='<dag_id>', task_id='<task_id>', run_id='scheduled__2024-11-03T00:00:00+00:00', try_number=1, map_index=-1), <TaskInstanceState.FAILED: 'failed'>, '<pod_name>', 'airflow', '649610440') to failed
[2024-11-04 01:17:31,404][PID:7][INFO][airflow.providers.cncf.kubernetes.executors.kubernetes_executor.KubernetesExecutor] Changing state of (TaskInstanceKey(dag_id='<dag_id>', task_id='<task_id>', run_id='scheduled__2024-11-03T00:00:00+00:00', try_number=1, map_index=-1), <TaskInstanceState.FAILED: 'failed'>, '<pod_name>', 'airflow', '649610439') to failed
[2024-11-04T01:17:31.404+0000] {kubernetes_executor.py:401} INFO - Changing state of (TaskInstanceKey(dag_id='<dag_id>', task_id='<task_id>', run_id='scheduled__2024-11-03T00:00:00+00:00', try_number=1, map_index=-1), <TaskInstanceState.FAILED: 'failed'>, '<pod_name>', 'airflow', '649610439') to failed
[2024-11-04 01:17:31,403][PID:7][INFO][airflow.providers.cncf.kubernetes.executors.kubernetes_executor.KubernetesExecutor] Changing state of (TaskInstanceKey(dag_id='<dag_id>', task_id='<task_id>', run_id='scheduled__2024-11-03T00:00:00+00:00', try_number=1, map_index=-1), <TaskInstanceState.FAILED: 'failed'>, '<pod_name>', 'airflow', '649610438') to failed
[2024-11-04T01:17:31.403+0000] {kubernetes_executor.py:401} INFO - Changing state of (TaskInstanceKey(dag_id='<dag_id>', task_id='<task_id>', run_id='scheduled__2024-11-03T00:00:00+00:00', try_number=1, map_index=-1), <TaskInstanceState.FAILED: 'failed'>, '<pod_name>', 'airflow', '649610438') to failed
[2024-11-04 01:17:28,807][PID:7][ERROR][airflow.jobs.scheduler_job_runner.SchedulerJobRunner] Executor reports task instance <TaskInstance: <dag_id>.<task_id> scheduled__2024-11-03T00:00:00+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?
[2024-11-04T01:17:28.807+0000] {task_context_logger.py:91} ERROR - Executor reports task instance <TaskInstance: <dag_id>.<task_id> scheduled__2024-11-03T00:00:00+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?
[2024-11-04 01:17:28,806][PID:7][INFO][airflow.jobs.scheduler_job_runner.SchedulerJobRunner] TaskInstance Finished: dag_id=<dag_id>, task_id=<task_id>, run_id=scheduled__2024-11-03T00:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=small_tasks, queue=default, priority_weight=12, operator=CustomPythonOperator, queued_dttm=2024-11-04 01:17:13.321215+00:00, queued_by_job_id=2782000, pid=None
[2024-11-04T01:17:28.806+0000] {scheduler_job_runner.py:733} INFO - TaskInstance Finished: dag_id=<dag_id>, task_id=<task_id>, run_id=scheduled__2024-11-03T00:00:00+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor_state=failed, try_number=1, max_tries=1, job_id=None, pool=small_tasks, queue=default, priority_weight=12, operator=CustomPythonOperator, queued_dttm=2024-11-04 01:17:13.321215+00:00, queued_by_job_id=2782000, pid=None
[2024-11-04 01:17:28,790][PID:7][INFO][airflow.jobs.scheduler_job_runner.SchedulerJobRunner] Received executor event with state failed for task instance TaskInstanceKey(dag_id='<dag_id>', task_id='<task_id>', run_id='scheduled__2024-11-03T00:00:00+00:00', try_number=1, map_index=-1)
[2024-11-04T01:17:28.790+0000] {scheduler_job_runner.py:696} INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='<dag_id>', task_id='<task_id>', run_id='scheduled__2024-11-03T00:00:00+00:00', try_number=1, map_index=-1)
[2024-11-04T01:17:28.783+0000] {kubernetes_executor_utils.py:269} ERROR - Event: <pod_name> Failed, annotations: <omitted>
[2024-11-04 01:17:27,748][PID:7][INFO][airflow.providers.cncf.kubernetes.executors.kubernetes_executor.KubernetesExecutor] Changing state of (TaskInstanceKey(dag_id='<dag_id>', task_id='<task_id>', run_id='scheduled__2024-11-03T00:00:00+00:00', try_number=1, map_index=-1), <TaskInstanceState.FAILED: 'failed'>, '<pod_name>', 'airflow', '649610402') to failed
[2024-11-04T01:17:27.748+0000] {kubernetes_executor.py:401} INFO - Changing state of (TaskInstanceKey(dag_id='<dag_id>', task_id='<task_id>', run_id='scheduled__2024-11-03T00:00:00+00:00', try_number=1, map_index=-1), <TaskInstanceState.FAILED: 'failed'>, '<pod_name>', 'airflow', '649610402') to failed
[2024-11-04 01:17:22,381][PID:7][INFO][airflow.providers.cncf.kubernetes.executors.kubernetes_executor_utils.AirflowKubernetesScheduler] Creating kubernetes pod for job is TaskInstanceKey(dag_id='<dag_id>', task_id='<task_id>', run_id='scheduled__2024-11-03T00:00:00+00:00', try_number=1, map_index=-1), with pod name <pod_name>, annotations: <omitted>
[2024-11-04T01:17:22.381+0000] {kubernetes_executor_utils.py:431} INFO - Creating kubernetes pod for job is TaskInstanceKey(dag_id='<dag_id>', task_id='<task_id>', run_id='scheduled__2024-11-03T00:00:00+00:00', try_number=1, map_index=-1), with pod name <pod_name>, annotations: <omitted>
[2024-11-04 01:17:14,893][PID:7][INFO][airflow.jobs.scheduler_job_runner.SchedulerJobRunner] Setting external_id for <TaskInstance: <dag_id>.<task_id> scheduled__2024-11-03T00:00:00+00:00 [queued]> to 2782000
[2024-11-04T01:17:14.893+0000] {scheduler_job_runner.py:723} INFO - Setting external_id for <TaskInstance: <dag_id>.<task_id> scheduled__2024-11-03T00:00:00+00:00 [queued]> to 2782000
[2024-11-04 01:17:14,877][PID:7][INFO][airflow.jobs.scheduler_job_runner.SchedulerJobRunner] Received executor event with state queued for task instance TaskInstanceKey(dag_id='<dag_id>', task_id='<task_id>', run_id='scheduled__2024-11-03T00:00:00+00:00', try_number=1, map_index=-1)
[2024-11-04T01:17:14.877+0000] {scheduler_job_runner.py:696} INFO - Received executor event with state queued for task instance TaskInstanceKey(dag_id='<dag_id>', task_id='<task_id>', run_id='scheduled__2024-11-03T00:00:00+00:00', try_number=1, map_index=-1)
[2024-11-04 01:17:13,334][PID:7][INFO][airflow.providers.cncf.kubernetes.executors.kubernetes_executor.KubernetesExecutor] Add task TaskInstanceKey(dag_id='<dag_id>', task_id='<task_id>', run_id='scheduled__2024-11-03T00:00:00+00:00', try_number=1, map_index=-1) with command ['airflow', 'tasks', 'run', '<dag_id>', '<task_id>', 'scheduled__2024-11-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/pai_non_renewable_energy-v1_dag.zip/pai_non_renewable_energy_dag.py']
[2024-11-04T01:17:13.334+0000] {kubernetes_executor.py:357} INFO - Add task TaskInstanceKey(dag_id='<dag_id>', task_id='<task_id>', run_id='scheduled__2024-11-03T00:00:00+00:00', try_number=1, map_index=-1) with command ['airflow', 'tasks', 'run', '<dag_id>', '<task_id>', 'scheduled__2024-11-03T00:00:00+00:00', '--local', '--subdir', '<dags_dir>']
[2024-11-04 01:17:13,327][PID:7][INFO][airflow.providers.cncf.kubernetes.executors.kubernetes_executor.KubernetesExecutor] Adding to queue: ['airflow', 'tasks', 'run', '<dag_id>', '<task_id>', 'scheduled__2024-11-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/pai_non_renewable_energy-v1_dag.zip/pai_non_renewable_energy_dag.py']
[2024-11-04T01:17:13.327+0000] {base_executor.py:146} INFO - Adding to queue: ['airflow', 'tasks', 'run', '<dag_id>', '<task_id>', 'scheduled__2024-11-03T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/pai_non_renewable_energy-v1_dag.zip/pai_non_renewable_energy_dag.py']
[2024-11-04 01:17:13,326][PID:7][INFO][airflow.jobs.scheduler_job_runner.SchedulerJobRunner] Sending TaskInstanceKey(dag_id='<dag_id>', task_id='<task_id>', run_id='scheduled__2024-11-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 12 and queue default
[2024-11-04T01:17:13.326+0000] {scheduler_job_runner.py:646} INFO - Sending TaskInstanceKey(dag_id='<dag_id>', task_id='<task_id>', run_id='scheduled__2024-11-03T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 12 and queue default
[2024-11-04T01:17:13.353Z] <TaskInstance: <dag_id>.<task_id> scheduled__2024-11-03T00:00:00+00:00 [scheduled]>
[2024-11-04T01:17:13.353Z], <TaskInstance: <dag_id>.<task_id> scheduled__2024-11-03T00:00:00+00:00 [scheduled]>
[2024-11-04T01:17:13.352Z], <TaskInstance: <dag_id>.<task_id> scheduled__2024-11-03T00:00:00+00:00 [scheduled]>
[2024-11-04T01:17:13.352Z], <TaskInstance: <dag_id>.<task_id> scheduled__2024-11-03T00:00:00+00:00 [scheduled]>
```

### What you think should happen instead?

Airflow scheduler should retry the task per specified retry policy rather than mark it as failed.

### How to reproduce

Terminate the node containing the pod.

### Operating System

`apache/airflow:2.8.3-python3.10` docker image

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",mewa,2024-11-04 17:27:33+00:00,[],2025-02-08 00:14:56+00:00,,https://github.com/apache/airflow/issues/43644,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('pending-response', ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2455298947, 'issue_id': 2633403706, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 4, 17, 27, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2581239135, 'issue_id': 2633403706, 'author': 'RNHTTR', 'body': 'What version of `apache-airflow-providers-cncf-kubernetes` is being used?', 'created_at': datetime.datetime(2025, 1, 9, 21, 1, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2644360074, 'issue_id': 2633403706, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2025, 2, 8, 0, 14, 55, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-04 17:27:36 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

RNHTTR on (2025-01-09 21:01:25 UTC): What version of `apache-airflow-providers-cncf-kubernetes` is being used?

github-actions[bot] on (2025-02-08 00:14:55 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

"
2633181201,issue,open,,Move core operators/sensors/triggers to standard provider,"### Body

Move all the core area operators to standard provider.

- [x] Move Data time operators/sensors to standard provider #41564
- [x] Move Hooks to standard provider #42794 
- [x] Move bash operator to standard provider #42252 
- [x] Move python operator to standard provider #42081
- [x] Move triggers to standard provider  https://github.com/apache/airflow/pull/43608
- [ ] Move decorators to standard provider #44027
- [X] Move external external task sensor to standard provider https://github.com/apache/airflow/pull/44288
- [X] Move filesystem sensor to standard provider https://github.com/apache/airflow/pull/43890
- [ ] Move core operator docs to standard provider documentation
- [x] Move latest only operator to standard provider https://github.com/apache/airflow/pull/44309
- [X] Move trigger dag run operator to standard provider https://github.com/apache/airflow/pull/44053

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",gopidesupavan,2024-11-04 15:49:49+00:00,[],2024-11-30 17:20:01+00:00,,https://github.com/apache/airflow/issues/43641,"[('area:providers', ''), ('kind:meta', 'High-level information important to the community'), ('area:core-operators', 'Operators, Sensors and hooks within Core Airflow')]","[{'comment_id': 2455069340, 'issue_id': 2633181201, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 4, 15, 49, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455840739, 'issue_id': 2633181201, 'author': 'potiuk', 'body': 'Nice!', 'created_at': datetime.datetime(2024, 11, 4, 22, 34, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2458075441, 'issue_id': 2633181201, 'author': 'gopidesupavan', 'body': 'cc: @romsharon98', 'created_at': datetime.datetime(2024, 11, 5, 20, 17, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2482048222, 'issue_id': 2633181201, 'author': 'kunaljubce', 'body': ""@gopidesupavan Can we link #43890 to `Move filesystem sensor to standard provider` in the description above? I'll take up one of the other open items from the remaining."", 'created_at': datetime.datetime(2024, 11, 18, 6, 18, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489262100, 'issue_id': 2633181201, 'author': 'hardeybisey', 'body': '@gopidesupavan can we also link #44053 for`Move TriggerDagRunOperator to standard provider`. Thanks.', 'created_at': datetime.datetime(2024, 11, 20, 18, 16, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2496492099, 'issue_id': 2633181201, 'author': 'jason810496', 'body': 'Hi @gopidesupavan, I can take `Move external external task sensor to standard provider`, thanks !\r\n\r\n( I will provide PR link later )', 'created_at': datetime.datetime(2024, 11, 25, 1, 24, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2496541814, 'issue_id': 2633181201, 'author': 'kunaljubce', 'body': '@jason810496 Apologies, I am already working on that and the PR is up, pending some fixes. Please pick up some other open issues, thanks :)', 'created_at': datetime.datetime(2024, 11, 25, 2, 21, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2503621738, 'issue_id': 2633181201, 'author': 'kunaljubce', 'body': '@gopidesupavan Now that #44288 is merged, maybe you can link it to `Move external external task sensor to standard provider`?\r\n\r\nThis would leave only `Move core operator docs to standard provider documentation`. Do we start working on this now since I see that #44027 is still a WIP?', 'created_at': datetime.datetime(2024, 11, 27, 11, 23, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507643748, 'issue_id': 2633181201, 'author': 'gopidesupavan', 'body': '> @gopidesupavan Now that #44288 is merged, maybe you can link it to `Move external external task sensor to standard provider`?\r\n> \r\n> This would leave only `Move core operator docs to standard provider documentation`. Do we start working on this now since I see that #44027 is still a WIP?\r\n\r\nYes, it needs some verification from core area to standard provider, moving if there are any missing documentation.', 'created_at': datetime.datetime(2024, 11, 29, 11, 41, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2509069159, 'issue_id': 2633181201, 'author': 'hardeybisey', 'body': '@gopidesupavan We can now link #44309 for `Move LatestOnlyOperator` to standard provider. Thanks.', 'created_at': datetime.datetime(2024, 11, 30, 17, 1, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2509079659, 'issue_id': 2633181201, 'author': 'potiuk', 'body': 'Linked', 'created_at': datetime.datetime(2024, 11, 30, 17, 20, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-04 15:49:54 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-11-04 22:34:50 UTC): Nice!

gopidesupavan (Issue Creator) on (2024-11-05 20:17:33 UTC): cc: @romsharon98

kunaljubce on (2024-11-18 06:18:03 UTC): @gopidesupavan Can we link #43890 to `Move filesystem sensor to standard provider` in the description above? I'll take up one of the other open items from the remaining.

hardeybisey on (2024-11-20 18:16:17 UTC): @gopidesupavan can we also link #44053 for`Move TriggerDagRunOperator to standard provider`. Thanks.

jason810496 on (2024-11-25 01:24:49 UTC): Hi @gopidesupavan, I can take `Move external external task sensor to standard provider`, thanks !

( I will provide PR link later )

kunaljubce on (2024-11-25 02:21:40 UTC): @jason810496 Apologies, I am already working on that and the PR is up, pending some fixes. Please pick up some other open issues, thanks :)

kunaljubce on (2024-11-27 11:23:47 UTC): @gopidesupavan Now that #44288 is merged, maybe you can link it to `Move external external task sensor to standard provider`?

This would leave only `Move core operator docs to standard provider documentation`. Do we start working on this now since I see that #44027 is still a WIP?

gopidesupavan (Issue Creator) on (2024-11-29 11:41:39 UTC): Yes, it needs some verification from core area to standard provider, moving if there are any missing documentation.

hardeybisey on (2024-11-30 17:01:47 UTC): @gopidesupavan We can now link #44309 for `Move LatestOnlyOperator` to standard provider. Thanks.

potiuk on (2024-11-30 17:20:00 UTC): Linked

"
2633133352,issue,closed,completed,AIP-84 Add gzip middleware to fastapi server,"### Description

Fastapi provides gzip middleware to compress responses. This could be done at the load balancer level (nginx) but it will be good to have this enabled since the fastapi server mostly serves JSON responses. This can make the frontend more responsive with shorter download times. E.g. 75 dags list page results in 50kB which on adding gzip middleware becomes 5kB. There was a proposal for the same in the flask webserver which required flask compress but fastapi has this without third party packages. We use this in production for legacy webserver views and it greatly speeds up the page loading times and also makes heavy calls like log download during auto refresh more efficient.

Ref PR for legacy webserver : https://github.com/apache/airflow/pull/13517
https://fastapi.tiangolo.com/advanced/middleware/#gzipmiddleware

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",tirkarthi,2024-11-04 15:31:47+00:00,['tirkarthi'],2024-11-06 13:10:36+00:00,2024-11-06 13:10:36+00:00,https://github.com/apache/airflow/issues/43640,"[('area:performance', ''), ('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2455128062, 'issue_id': 2633133352, 'author': 'tirkarthi', 'body': 'cc: @pierrejeambrun', 'created_at': datetime.datetime(2024, 11, 4, 16, 13, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2457277521, 'issue_id': 2633133352, 'author': 'pierrejeambrun', 'body': 'Good idea, thanks.', 'created_at': datetime.datetime(2024, 11, 5, 14, 9, 2, tzinfo=datetime.timezone.utc)}]","tirkarthi (Issue Creator) on (2024-11-04 16:13:33 UTC): cc: @pierrejeambrun

pierrejeambrun on (2024-11-05 14:09:02 UTC): Good idea, thanks.

"
2633050924,issue,open,,OpenLineage AirflowRunFacet.json wrong type for tags,"### Apache Airflow Provider(s)

openlineage

### Versions of Apache Airflow Providers

apache-airflow-providers-openlineage 1.12.2

### Apache Airflow version

2.9.3

### Operating System

Linux

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### What happened

The AirflowRunFacet.json currently isn't usable to validate OpenLineage logs that contain the AirflowRunFacet:
https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/openlineage/facets/AirflowRunFacet.json#L175
This is due to the type of the `tags` element (on lines 175-177):
```
""tags"": {
  ""type"": ""string""
}
```

The logs that get produced by the openlineage provider contain lists of strings as tags. When one tag is given to a DAG this is still a list that contains 1 string, when multiple tags are given to a DAG the list contains all these strings.



### What you think should happen instead

The `tags` field in AirflowRunFacet.json should be changed as follows:
```
""tags"": {
  ""type"": ""array"",
  ""items"": {
    ""type"": ""string""
  }
}
```

Which validates that it is indeed an array of strings.

### How to reproduce

Validate any given openlineage log that contains the airflow run facet with the current AirflowRunFacet.json and the validation will fail.

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",MaartenHubrechts,2024-11-04 14:57:30+00:00,[],2024-11-06 08:02:21+00:00,,https://github.com/apache/airflow/issues/43638,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('provider:openlineage', 'AIP-53')]","[{'comment_id': 2454936375, 'issue_id': 2633050924, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 4, 14, 57, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455839841, 'issue_id': 2633050924, 'author': 'potiuk', 'body': 'cc: @mobuchowski @kacpermuda', 'created_at': datetime.datetime(2024, 11, 4, 22, 34, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2457006902, 'issue_id': 2633050924, 'author': 'mobuchowski', 'body': ""@MaartenHubrechts We've moved to that before and unfortunately had to revert because it broke users workflow: https://github.com/apache/airflow/pull/41786\r\n\r\nHowever, we can add this as a separate list with that format. The information will be duplicated, but we'd be able to drop it... in Airflow 3 probably."", 'created_at': datetime.datetime(2024, 11, 5, 12, 11, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2458940925, 'issue_id': 2633050924, 'author': 'MaartenHubrechts', 'body': '@mobuchowski Thanks for the reply! It would be nice to have it as a separate list, if possible.', 'created_at': datetime.datetime(2024, 11, 6, 8, 2, 19, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-04 14:57:34 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-11-04 22:34:11 UTC): cc: @mobuchowski @kacpermuda

mobuchowski on (2024-11-05 12:11:10 UTC): @MaartenHubrechts We've moved to that before and unfortunately had to revert because it broke users workflow: https://github.com/apache/airflow/pull/41786

However, we can add this as a separate list with that format. The information will be duplicated, but we'd be able to drop it... in Airflow 3 probably.

MaartenHubrechts (Issue Creator) on (2024-11-06 08:02:19 UTC): @mobuchowski Thanks for the reply! It would be nice to have it as a separate list, if possible.

"
2632931571,issue,closed,completed,SSH/SFTP over socks proxy - host_proxy_cmd as extra param,"### Description

Now i evaluate and PoC AirFlow. 

Please consider make ""host_proxy_cmd"" as one of extra parameter and use current behavior as fallback, if not set.

I'm new in python/AirFlow world, so there is possibility i miss something :) - be gentle

### Use case/motivation

It is mandatory for me to use SFTP over socks proxy. In current tool, (which does not support proxy either) i use bash workaround `-o ProxyCommand='ncat --proxy-auth proxy_user:**** --proxy proxy_host:port %h %p'`.

`sftp -i 'private.key' -o BatchMode=yes -P 22 -o ProxyCommand='ncat --proxy-auth proxy_user:**** --proxy proxy_host:port %h %p' 'sftp_user@sftp_host:'`

As far as i dive into documentation and provider source (SFTPHook, SSHHook), there is no externalized parameter for proxy command config, but it sets internally proxy command from ssh config file ""~/.ssh/config"" (as ""proxycommand"") and then forward into encapsulated Paramiko object using paramiko.ProxyCommand(). This approach is not convenient for me, i rather have all the configuration on the same location for simple CI, avoid underlying OS artifacts as much as possible

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",mstangler,2024-11-04 14:12:11+00:00,[],2024-12-03 10:22:28+00:00,2024-12-03 10:22:28+00:00,https://github.com/apache/airflow/issues/43636,"[('area:providers', ''), ('kind:feature', 'Feature Requests'), ('good first issue', ''), ('provider:ssh', ''), ('provider:sftp', '')]","[{'comment_id': 2454823006, 'issue_id': 2632931571, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 4, 14, 12, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455837390, 'issue_id': 2632931571, 'author': 'potiuk', 'body': 'I marked it as good first issue. Hopefully someone will pick it up and implement,  but the fastest way to get is to contribute it - other than that, it will have to wait for a volunteer to implement it.', 'created_at': datetime.datetime(2024, 11, 4, 22, 32, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2478004335, 'issue_id': 2632931571, 'author': 'geraj1010', 'body': 'This is an interesting issue. I had a use case, where I had to utilize a jump server to connect to SFTP behind a firewall and ended up extending the `SFTPHook` to do that. I was thinking of providing an update to the hook to do this out-of-box if it makes sense to.', 'created_at': datetime.datetime(2024, 11, 15, 5, 41, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2511277529, 'issue_id': 2632931571, 'author': 'ajitg25', 'body': 'Hello @potiuk \r\n\r\nAs per the requirement, I have added a `host_proxy_cmd` parameter to `SSHHook` and `SFTPHook`.\r\nThe description was detailed, so I went ahead and worked on it before requesting an assignment—my apologies for that. I have raised a draft PR [here](https://github.com/apache/airflow/pull/44565). \r\n\r\nCould you please review it and let me know if my understanding is correct? Also, kindly assign this issue to me.\r\n\r\nThank you for your time and understanding!', 'created_at': datetime.datetime(2024, 12, 2, 11, 27, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2513224906, 'issue_id': 2632931571, 'author': 'potiuk', 'body': 'Looks good @ajitg25 -> And there is no need to ask for assignment. The only thing you are risking is that someone is working on it in parallel, but this is not a problem either, worst thing you can compare your changes with someone else\'s and maybe learn something. \r\n\r\nApproved. merging when it completes. Also it\'s better to just make the PR as ""ready for review"" - we genereally do not review PRs that are ""draft"" and making it ""ready for review"" make it, well, ready for review. I clicked the ""Ready for review"" for you.', 'created_at': datetime.datetime(2024, 12, 2, 23, 59, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2513617673, 'issue_id': 2632931571, 'author': 'ajitg25', 'body': 'Thank you @potiuk !!', 'created_at': datetime.datetime(2024, 12, 3, 5, 49, 43, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-04 14:12:14 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-11-04 22:32:19 UTC): I marked it as good first issue. Hopefully someone will pick it up and implement,  but the fastest way to get is to contribute it - other than that, it will have to wait for a volunteer to implement it.

geraj1010 on (2024-11-15 05:41:30 UTC): This is an interesting issue. I had a use case, where I had to utilize a jump server to connect to SFTP behind a firewall and ended up extending the `SFTPHook` to do that. I was thinking of providing an update to the hook to do this out-of-box if it makes sense to.

ajitg25 on (2024-12-02 11:27:34 UTC): Hello @potiuk 

As per the requirement, I have added a `host_proxy_cmd` parameter to `SSHHook` and `SFTPHook`.
The description was detailed, so I went ahead and worked on it before requesting an assignment—my apologies for that. I have raised a draft PR [here](https://github.com/apache/airflow/pull/44565). 

Could you please review it and let me know if my understanding is correct? Also, kindly assign this issue to me.

Thank you for your time and understanding!

potiuk on (2024-12-02 23:59:27 UTC): Looks good @ajitg25 -> And there is no need to ask for assignment. The only thing you are risking is that someone is working on it in parallel, but this is not a problem either, worst thing you can compare your changes with someone else's and maybe learn something. 

Approved. merging when it completes. Also it's better to just make the PR as ""ready for review"" - we genereally do not review PRs that are ""draft"" and making it ""ready for review"" make it, well, ready for review. I clicked the ""Ready for review"" for you.

ajitg25 on (2024-12-03 05:49:43 UTC): Thank you @potiuk !!

"
2632838198,issue,open,,Add support for map_index in /clearTaskInstances,"### Description

Enhance the `/clearTaskInstances` REST API endpoint by adding support for clearing specific task instances using map_indexes. 

This feature is especially valuable for dynamically mapped operators, which can generate multiple task instances for a single operator. Currently, while the Airflow UI allows clearing individual task instances by map_index, this capability is not available via the /clearTaskInstances endpoint

### Use case/motivation

We want to provide users with the ability to retry specific instances of a failed mapped task. To enable this, the REST API should support clearing specific `map_index`(es) for a given task ID.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",sharpnife,2024-11-04 13:35:07+00:00,['sharpnife'],2024-11-19 03:42:58+00:00,,https://github.com/apache/airflow/issues/43635,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('area:dynamic-task-mapping', 'AIP-42')]","[{'comment_id': 2454733746, 'issue_id': 2632838198, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 4, 13, 35, 10, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-04 13:35:10 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2632741352,issue,closed,completed,Airflow task stuck on DEFERRED status,"### Apache Airflow version

2.10.1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

We are trying to migrate all our DAGs to use the deferreable operators. I set it as the default in out MWAA configuration:


operators.default_deferrable | true
-- | --

This seemed to work properly for a week, but today all jobs started being stuck in the DEFERRED status. For example:

Typical / expected run:

```
ip-10-5-146-175.eu-west-1.compute.internal
*** Reading remote log from Cloudwatch log_group: airflow-DT-LAB-Airflow-Environment-Task log_stream: dag_id=DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1/run_id=scheduled__2024-11-04T08_00_00+00_00/task_id=batch_job/attempt=1.log.
[2024-11-04, 09:00:15 UTC] {local_task_job_runner.py:123} ▶ Pre task execution logs
[2024-11-04, 09:00:15 UTC] {batch.py:303} INFO - Running AWS Batch job - job definition: arn:aws:batch:eu-west-1:532132528437:job-definition/DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1:358 - on queue DT-LAB-BatchExecution-Standard
[2024-11-04, 09:00:15 UTC] {batch.py:310} INFO - AWS Batch job - container overrides: {'environment': [{'name': '_DATATOOLS_DAG_DEPLOYMENT_IDENTIFIER', 'value': 'DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1'}, {'name': '_DATATOOLS_DAG_DEPLOYMENT_TARGET', 'value': 'airflow'}, {'name': 'DATATOOLS_ENVIRONMENT', 'value': 'LAB'}, {'name': '_DATATOOLS_AIRFLOW_DEPLOYMENT_DAG_ID', 'value': 'DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1'}, {'name': '_DATATOOLS_AIRFLOW_DEPLOYMENT_ENVIRONMENT_NAME', 'value': 'shopfully-data-execution'}, {'name': '_DATATOOLS_AIRFLOW_DEPLOYMENT_DATA_INTERVAL_START_DATETIME', 'value': '2024-11-04 08:00:00+00:00'}, {'name': '_DATATOOLS_AIRFLOW_DEPLOYMENT_DATA_INTERVAL_END_DATETIME', 'value': '2024-11-04 09:00:00+00:00'}, {'name': '_DATATOOLS_PROCESS_EXTRA_ENVIRONMENT_BLOB', 'value': ''}]}
[2024-11-04, 09:00:15 UTC] {base.py:84} INFO - Retrieving connection 'aws_shopfully_data_execution'
[2024-11-04, 09:00:17 UTC] {batch.py:347} INFO - AWS Batch job (16ffa872-59c1-4eb9-a7b4-50622364db0b) started: {'ResponseMetadata': {'RequestId': '18c85cda-9a0e-4347-8801-ac6e5a1da429', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 04 Nov 2024 09:00:17 GMT', 'content-type': 'application/json', 'content-length': '198', 'connection': 'keep-alive', 'x-amzn-requestid': '18c85cda-9a0e-4347-8801-ac6e5a1da429', 'access-control-allow-origin': '*', 'x-amz-apigw-id': 'Atr9NEPzDoEEWPg=', 'access-control-expose-headers': 'X-amzn-errortype,X-amzn-requestid,X-amzn-errormessage,X-amzn-trace-id,X-amz-apigw-id,date', 'x-amzn-trace-id': 'Root=1-67288d20-10faae0a2da5180b3c952dc2'}, 'RetryAttempts': 0}, 'jobArn': 'arn:aws:batch:eu-west-1:532132528437:job/16ffa872-59c1-4eb9-a7b4-50622364db0b', 'jobName': 'DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1', 'jobId': '16ffa872-59c1-4eb9-a7b4-50622364db0b'}
[2024-11-04, 09:00:17 UTC] {taskinstance.py:288} INFO - Pausing task as DEFERRED. dag_id=DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1, task_id=batch_job, run_id=scheduled__2024-11-04T08:00:00+00:00, execution_date=20241104T080000, start_date=20241104T090015
[2024-11-04, 09:00:17 UTC] {taskinstance.py:340} ▶ Post task execution logs
[2024-11-04, 09:00:18 UTC] {base.py:84} INFO - Retrieving connection 'aws_shopfully_data_execution'
[2024-11-04, 09:00:18 UTC] {waiter_with_logging.py:129} INFO - Batch job 16ffa872-59c1-4eb9-a7b4-50622364db0b not ready yet: ['SUBMITTED']
[2024-11-04, 09:00:48 UTC] {waiter_with_logging.py:129} INFO - Batch job 16ffa872-59c1-4eb9-a7b4-50622364db0b not ready yet: ['STARTING']
[2024-11-04, 09:01:19 UTC] {waiter_with_logging.py:129} INFO - Batch job 16ffa872-59c1-4eb9-a7b4-50622364db0b not ready yet: ['STARTING']
[2024-11-04, 09:01:49 UTC] {waiter_with_logging.py:129} INFO - Batch job 16ffa872-59c1-4eb9-a7b4-50622364db0b not ready yet: ['RUNNING']
[2024-11-04, 09:02:19 UTC] {waiter_with_logging.py:129} INFO - Batch job 16ffa872-59c1-4eb9-a7b4-50622364db0b not ready yet: ['RUNNING']
[2024-11-04, 09:02:49 UTC] {waiter_with_logging.py:129} INFO - Batch job 16ffa872-59c1-4eb9-a7b4-50622364db0b not ready yet: ['RUNNING']
[2024-11-04, 09:03:19 UTC] {triggerer_job_runner.py:631} INFO - Trigger DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1/scheduled__2024-11-04T08:00:00+00:00/batch_job/-1/1 (ID 4448) fired: TriggerEvent<{'status': 'success', 'job_id': '16ffa872-59c1-4eb9-a7b4-50622364db0b'}>
[2024-11-04, 09:03:23 UTC] {local_task_job_runner.py:123} ▶ Pre task execution logs
[2024-11-04, 09:03:23 UTC] {batch.py:290} INFO - Job completed.
[2024-11-04, 09:03:24 UTC] {taskinstance.py:340} ▶ Post task execution logs
```

All runs since today at around 10:00 UTC:

```
ip-10-5-146-175.eu-west-1.compute.internal
*** Reading remote log from Cloudwatch log_group: airflow-DT-LAB-Airflow-Environment-Task log_stream: dag_id=DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1/run_id=scheduled__2024-11-04T09_00_00+00_00/task_id=batch_job/attempt=1.log.
[2024-11-04, 10:00:13 UTC] {local_task_job_runner.py:123} ▶ Pre task execution logs
[2024-11-04, 10:00:14 UTC] {batch.py:303} INFO - Running AWS Batch job - job definition: arn:aws:batch:eu-west-1:532132528437:job-definition/DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1:358 - on queue DT-LAB-BatchExecution-Standard
[2024-11-04, 10:00:14 UTC] {batch.py:310} INFO - AWS Batch job - container overrides: {'environment': [{'name': '_DATATOOLS_DAG_DEPLOYMENT_IDENTIFIER', 'value': 'DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1'}, {'name': '_DATATOOLS_DAG_DEPLOYMENT_TARGET', 'value': 'airflow'}, {'name': 'DATATOOLS_ENVIRONMENT', 'value': 'LAB'}, {'name': '_DATATOOLS_AIRFLOW_DEPLOYMENT_DAG_ID', 'value': 'DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1'}, {'name': '_DATATOOLS_AIRFLOW_DEPLOYMENT_ENVIRONMENT_NAME', 'value': 'shopfully-data-execution'}, {'name': '_DATATOOLS_AIRFLOW_DEPLOYMENT_DATA_INTERVAL_START_DATETIME', 'value': '2024-11-04 09:00:00+00:00'}, {'name': '_DATATOOLS_AIRFLOW_DEPLOYMENT_DATA_INTERVAL_END_DATETIME', 'value': '2024-11-04 10:00:00+00:00'}, {'name': '_DATATOOLS_PROCESS_EXTRA_ENVIRONMENT_BLOB', 'value': ''}]}
[2024-11-04, 10:00:14 UTC] {base.py:84} INFO - Retrieving connection 'aws_shopfully_data_execution'
[2024-11-04, 10:00:15 UTC] {batch.py:347} INFO - AWS Batch job (c6157b39-4197-4a98-9a0d-2310866a2495) started: {'ResponseMetadata': {'RequestId': 'f7570782-3748-4d95-97b6-2fcc29820dd4', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 04 Nov 2024 10:00:15 GMT', 'content-type': 'application/json', 'content-length': '198', 'connection': 'keep-alive', 'x-amzn-requestid': 'f7570782-3748-4d95-97b6-2fcc29820dd4', 'access-control-allow-origin': '*', 'x-amz-apigw-id': 'At0vfHotjoEEbJQ=', 'access-control-expose-headers': 'X-amzn-errortype,X-amzn-requestid,X-amzn-errormessage,X-amzn-trace-id,X-amz-apigw-id,date', 'x-amzn-trace-id': 'Root=1-67289b2f-4f45249a51c82f7f02fdaa9d'}, 'RetryAttempts': 0}, 'jobArn': 'arn:aws:batch:eu-west-1:532132528437:job/c6157b39-4197-4a98-9a0d-2310866a2495', 'jobName': 'DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1', 'jobId': 'c6157b39-4197-4a98-9a0d-2310866a2495'}
[2024-11-04, 10:00:15 UTC] {taskinstance.py:288} INFO - Pausing task as DEFERRED. dag_id=DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1, task_id=batch_job, run_id=scheduled__2024-11-04T09:00:00+00:00, execution_date=20241104T090000, start_date=20241104T100013
[2024-11-04, 10:00:15 UTC] {taskinstance.py:340} ▶ Post task execution logs
```

Without the expected logs from `waiter_with_logging` etc.

The tasks seem to be indefinetly stuck at DEFERRED.

### What you think should happen instead?

_No response_

### How to reproduce

Use Airflow 2.10 with deferrable operators.

### Operating System

MWAA

### Versions of Apache Airflow Providers

_No response_

### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",pvieito,2024-11-04 12:53:24+00:00,[],2024-11-08 18:41:49+00:00,2024-11-08 18:41:49+00:00,https://github.com/apache/airflow/issues/43634,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:core', ''), ('area:async-operators', 'AIP-40: Deferrable (""Async"") Operators'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2454641827, 'issue_id': 2632741352, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 4, 12, 53, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455253102, 'issue_id': 2632741352, 'author': 'kunaljubce', 'body': ""@pvieito - \r\n\r\n* Given it was working all this while and suddenly stopped working from today, assuming that your airflow platform did not get upgraded meanwhile, seems to mean something changed upstream? If you're querying an API, maybe you can add a retry logic if it's not already there?\r\n* Do you see any helpful details in the `▶ Post task execution logs`?"", 'created_at': datetime.datetime(2024, 11, 4, 17, 6, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2456756439, 'issue_id': 2632741352, 'author': 'pvieito', 'body': 'Hi @kunaljubce!\r\n\r\n- No updates in that time window.\r\n- All our tasks are vanilla Airflow AWS operators and super-simple DAGs based on some templates (basically `BatchOperator` & `GlueOperator`):\r\n\r\n```python\r\nwith DAG(\r\n        dag_id=dag_id,\r\n        schedule=dag_schedule,\r\n        start_date=pendulum.parse(dag_schedule_start_date),\r\n        tags=dag_deployment_tags,\r\n        catchup=False,\r\n) as dag:\r\n    batch_job = BatchOperator(\r\n        task_id=""batch_job"",\r\n        job_name=job_name,\r\n        job_definition=job_definition_arn,\r\n        job_queue=job_queue_name,\r\n        tags=job_tags,\r\n        aws_conn_id=job_aws_connection_id,\r\n        container_overrides=dict(\r\n            environment=aws_batch_job_environment_variables,\r\n        ))\r\n\r\n    log_batch_job() >> batch_job\r\n```\r\n\r\n- All deferred task started to mis-behave indefinetly and all new deferred task also mis-behave.\r\n- Some extra full logs of tasks being stuck in DEFERRED without any waiter logs:\r\n\r\n- With `GlueOperator`:\r\n```\r\nip-10-5-146-175.eu-west-1.compute.internal\r\n*** Reading remote log from Cloudwatch log_group: airflow-DT-LAB-Airflow-Environment-Task log_stream: dag_id=DT-LAB-Autostop-Schedule-PushProximity-1/run_id=manual__2024-11-04T09_55_51+00_00/task_id=glue_job/attempt=1.log.\r\n[2024-11-04, 09:55:59 UTC] {local_task_job_runner.py:123} ▼ Pre task execution logs\r\n[2024-11-04, 09:55:59 UTC] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: DT-LAB-Autostop-Schedule-PushProximity-1.glue_job manual__2024-11-04T09:55:51+00:00 [queued]>\r\n[2024-11-04, 09:55:59 UTC] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: DT-LAB-Autostop-Schedule-PushProximity-1.glue_job manual__2024-11-04T09:55:51+00:00 [queued]>\r\n[2024-11-04, 09:55:59 UTC] {taskinstance.py:2865} INFO - Starting attempt 1 of 1\r\n[2024-11-04, 09:55:59 UTC] {taskinstance.py:2888} INFO - Executing <Task(GlueJobOperator): glue_job> on 2024-11-04 09:55:51+00:00\r\n[2024-11-04, 09:55:59 UTC] {standard_task_runner.py:72} INFO - Started process 2450 to run task\r\n[2024-11-04, 09:55:59 UTC] {standard_task_runner.py:104} INFO - Running: [\'airflow\', \'tasks\', \'run\', \'DT-LAB-Autostop-Schedule-PushProximity-1\', \'glue_job\', \'manual__2024-11-04T09:55:51+00:00\', \'--job-id\', \'39075\', \'--raw\', \'--subdir\', \'DAGS_FOLDER/DT-LAB-Autostop-Schedule-PushProximity-1.py\', \'--cfg-path\', \'/tmp/tmpceiye8fs\']\r\n[2024-11-04, 09:55:59 UTC] {standard_task_runner.py:105} INFO - Job 39075: Subtask glue_job\r\n[2024-11-04, 09:55:59 UTC] {task_command.py:467} INFO - Running <TaskInstance: DT-LAB-Autostop-Schedule-PushProximity-1.glue_job manual__2024-11-04T09:55:51+00:00 [running]> on host ip-10-5-146-175.eu-west-1.compute.internal\r\n[2024-11-04, 09:56:00 UTC] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER=\'airflow\' AIRFLOW_CTX_DAG_ID=\'DT-LAB-Autostop-Schedule-PushProximity-1\' AIRFLOW_CTX_TASK_ID=\'glue_job\' AIRFLOW_CTX_EXECUTION_DATE=\'2024-11-04T09:55:51+00:00\' AIRFLOW_CTX_TRY_NUMBER=\'1\' AIRFLOW_CTX_DAG_RUN_ID=\'manual__2024-11-04T09:55:51+00:00\'\r\n[2024-11-04, 09:56:00 UTC] {taskinstance.py:731} ▲▲▲ Log group end\r\n[2024-11-04, 09:56:00 UTC] {glue.py:188} INFO - Initializing AWS Glue Job: DT-LAB-Autostop-Schedule-PushProximity-1. Wait for completion: True\r\n[2024-11-04, 09:56:00 UTC] {glue.py:365} INFO - Checking if job already exists: DT-LAB-Autostop-Schedule-PushProximity-1\r\n[2024-11-04, 09:56:00 UTC] {base.py:84} INFO - Retrieving connection \'aws_shopfully_data\'\r\n[2024-11-04, 09:56:02 UTC] {glue.py:209} INFO - You can monitor this Glue Job run at: https://console.aws.amazon.com/gluestudio/home?region=eu-west-1#/job/DT-LAB-Autostop-Schedule-PushProximity-1/run/jr_d8efd3e7c7ca097b048db1e2e4342cd3e8eb3f03a225335bdef726c3ff63abb3\r\n[2024-11-04, 09:56:02 UTC] {taskinstance.py:288} INFO - Pausing task as DEFERRED. dag_id=DT-LAB-Autostop-Schedule-PushProximity-1, task_id=glue_job, run_id=manual__2024-11-04T09:55:51+00:00, execution_date=20241104T095551, start_date=20241104T095559\r\n[2024-11-04, 09:56:02 UTC] {taskinstance.py:340} ▼ Post task execution logs\r\n[2024-11-04, 09:56:02 UTC] {local_task_job_runner.py:260} INFO - Task exited with return code 100 (task deferral)\r\n[2024-11-04, 09:56:02 UTC] {local_task_job_runner.py:245} ▲▲▲ Log group end\r\n[2024-11-04, 09:56:04 UTC] {base.py:84} INFO - Retrieving connection \'aws_shopfully_data\'\r\n[2024-11-04, 09:56:05 UTC] {base.py:84} INFO - Retrieving connection \'aws_shopfully_data\'\r\n```\r\n\r\n- With `BatchOperator`:\r\n\r\n```\r\nip-10-5-146-175.eu-west-1.compute.internal\r\n*** Reading remote log from Cloudwatch log_group: airflow-DT-LAB-Airflow-Environment-Task log_stream: dag_id=DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1/run_id=scheduled__2024-11-04T09_00_00+00_00/task_id=batch_job/attempt=1.log.\r\n[2024-11-04, 10:00:13 UTC] {local_task_job_runner.py:123} ▼ Pre task execution logs\r\n[2024-11-04, 10:00:13 UTC] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1.batch_job scheduled__2024-11-04T09:00:00+00:00 [queued]>\r\n[2024-11-04, 10:00:13 UTC] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1.batch_job scheduled__2024-11-04T09:00:00+00:00 [queued]>\r\n[2024-11-04, 10:00:13 UTC] {taskinstance.py:2865} INFO - Starting attempt 1 of 1\r\n[2024-11-04, 10:00:13 UTC] {taskinstance.py:2888} INFO - Executing <Task(BatchOperator): batch_job> on 2024-11-04 09:00:00+00:00\r\n[2024-11-04, 10:00:13 UTC] {standard_task_runner.py:72} INFO - Started process 2486 to run task\r\n[2024-11-04, 10:00:13 UTC] {standard_task_runner.py:104} INFO - Running: [\'airflow\', \'tasks\', \'run\', \'DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1\', \'batch_job\', \'scheduled__2024-11-04T09:00:00+00:00\', \'--job-id\', \'39077\', \'--raw\', \'--subdir\', \'DAGS_FOLDER/DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1.py\', \'--cfg-path\', \'/tmp/tmp7b1ya_1l\']\r\n[2024-11-04, 10:00:13 UTC] {standard_task_runner.py:105} INFO - Job 39077: Subtask batch_job\r\n[2024-11-04, 10:00:13 UTC] {task_command.py:467} INFO - Running <TaskInstance: DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1.batch_job scheduled__2024-11-04T09:00:00+00:00 [running]> on host ip-10-5-146-175.eu-west-1.compute.internal\r\n[2024-11-04, 10:00:14 UTC] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER=\'airflow\' AIRFLOW_CTX_DAG_ID=\'DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1\' AIRFLOW_CTX_TASK_ID=\'batch_job\' AIRFLOW_CTX_EXECUTION_DATE=\'2024-11-04T09:00:00+00:00\' AIRFLOW_CTX_TRY_NUMBER=\'1\' AIRFLOW_CTX_DAG_RUN_ID=\'scheduled__2024-11-04T09:00:00+00:00\'\r\n[2024-11-04, 10:00:14 UTC] {taskinstance.py:731} ▲▲▲ Log group end\r\n[2024-11-04, 10:00:14 UTC] {batch.py:303} INFO - Running AWS Batch job - job definition: arn:aws:batch:eu-west-1:532132528437:job-definition/DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1:358 - on queue DT-LAB-BatchExecution-Standard\r\n[2024-11-04, 10:00:14 UTC] {batch.py:310} INFO - AWS Batch job - container overrides: {\'environment\': [{\'name\': \'_DATATOOLS_DAG_DEPLOYMENT_IDENTIFIER\', \'value\': \'DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1\'}, {\'name\': \'_DATATOOLS_DAG_DEPLOYMENT_TARGET\', \'value\': \'airflow\'}, {\'name\': \'DATATOOLS_ENVIRONMENT\', \'value\': \'LAB\'}, {\'name\': \'_DATATOOLS_AIRFLOW_DEPLOYMENT_DAG_ID\', \'value\': \'DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1\'}, {\'name\': \'_DATATOOLS_AIRFLOW_DEPLOYMENT_ENVIRONMENT_NAME\', \'value\': \'shopfully-data-execution\'}, {\'name\': \'_DATATOOLS_AIRFLOW_DEPLOYMENT_DATA_INTERVAL_START_DATETIME\', \'value\': \'2024-11-04 09:00:00+00:00\'}, {\'name\': \'_DATATOOLS_AIRFLOW_DEPLOYMENT_DATA_INTERVAL_END_DATETIME\', \'value\': \'2024-11-04 10:00:00+00:00\'}, {\'name\': \'_DATATOOLS_PROCESS_EXTRA_ENVIRONMENT_BLOB\', \'value\': \'\'}]}\r\n[2024-11-04, 10:00:14 UTC] {base.py:84} INFO - Retrieving connection \'aws_shopfully_data_execution\'\r\n[2024-11-04, 10:00:15 UTC] {batch.py:347} INFO - AWS Batch job (c6157b39-4197-4a98-9a0d-2310866a2495) started: {\'ResponseMetadata\': {\'RequestId\': \'f7570782-3748-4d95-97b6-2fcc29820dd4\', \'HTTPStatusCode\': 200, \'HTTPHeaders\': {\'date\': \'Mon, 04 Nov 2024 10:00:15 GMT\', \'content-type\': \'application/json\', \'content-length\': \'198\', \'connection\': \'keep-alive\', \'x-amzn-requestid\': \'f7570782-3748-4d95-97b6-2fcc29820dd4\', \'access-control-allow-origin\': \'*\', \'x-amz-apigw-id\': \'At0vfHotjoEEbJQ=\', \'access-control-expose-headers\': \'X-amzn-errortype,X-amzn-requestid,X-amzn-errormessage,X-amzn-trace-id,X-amz-apigw-id,date\', \'x-amzn-trace-id\': \'Root=1-67289b2f-4f45249a51c82f7f02fdaa9d\'}, \'RetryAttempts\': 0}, \'jobArn\': \'arn:aws:batch:eu-west-1:532132528437:job/c6157b39-4197-4a98-9a0d-2310866a2495\', \'jobName\': \'DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1\', \'jobId\': \'c6157b39-4197-4a98-9a0d-2310866a2495\'}\r\n[2024-11-04, 10:00:15 UTC] {taskinstance.py:288} INFO - Pausing task as DEFERRED. dag_id=DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1, task_id=batch_job, run_id=scheduled__2024-11-04T09:00:00+00:00, execution_date=20241104T090000, start_date=20241104T100013\r\n[2024-11-04, 10:00:15 UTC] {taskinstance.py:340} ▼ Post task execution logs\r\n[2024-11-04, 10:00:16 UTC] {local_task_job_runner.py:260} INFO - Task exited with return code 100 (task deferral)\r\n[2024-11-04, 10:00:16 UTC] {local_task_job_runner.py:245} ▲▲▲ Log group end\r\n```', 'created_at': datetime.datetime(2024, 11, 5, 10, 9, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2457503916, 'issue_id': 2632741352, 'author': 'kunaljubce', 'body': ""@pvieito - Thanks for these, nothing apparent seems to jump out from these logs.\r\n\r\nWould it be possible for you to look at all logs for triggerer, scheduler etc.? Maybe try clearing the tasks and then follow the system logs to see if it prints something referring to your deferred tasks.  If you can add those logs below, we can try looking for a possible issue because given your information that nothing changed on the airflow platform side, there doesn't seem much for us to go on. \r\n\r\ncc: @andrewgodwin I see a similar issue reported long back (#25630), hence tagging you here."", 'created_at': datetime.datetime(2024, 11, 5, 15, 38, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2460358188, 'issue_id': 2632741352, 'author': 'pvieito', 'body': 'Hi @kunaljubce!\r\n\r\nI attach the full logs from the tasks, workers & scheduler from `2024-11-04T09:00:00` to `2024-11-04T11:30:00`.\r\n\r\n- At around 9:00 UTC as you can see for example the task with AWS Batch identifier `16ffa872-59c1-4eb9-a7b4-50622364db0b` is correctly deferred and `waiter_with_logging.py` logs the expected checks.\r\n- All seems to be working fine until around 10:00 UTC. Then all tasks (for example the one with AWS Batch identifier `c6157b39-4197-4a98-9a0d-2310866a2495`) starts being deferred but no checks are done: no more `waiter_with_logging.py` logs after the one at `2024-11-04 09:55:39.871`).\r\n\r\n[Airflow-Logs.csv](https://github.com/user-attachments/files/17650761/Airflow-Logs.csv)', 'created_at': datetime.datetime(2024, 11, 6, 17, 19, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2464412214, 'issue_id': 2632741352, 'author': 'kunaljubce', 'body': '@pvieito I looked through the logs and it does not seem to be giving us an awful lot in terms of an actual issue. I would probably have to point you back to your platform/devops team who maintains your Airflow environment to check if this is memory related? Because from the logs, I can see some of the tasks being submitted, executed, and completed. Only after a period of time does your triggerer start behaving weirdly and starts indefinitely deferring the tasks.', 'created_at': datetime.datetime(2024, 11, 8, 11, 0, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465514983, 'issue_id': 2632741352, 'author': 'gopidesupavan', 'body': 'This is difficult to reproduce and debug. There are optimizations related to triggers in Airflow 2.10.3, so try upgrading and check if any issues persist. Moving this to discussions.', 'created_at': datetime.datetime(2024, 11, 8, 18, 41, 35, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-04 12:53:28 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

kunaljubce on (2024-11-04 17:06:59 UTC): @pvieito - 

* Given it was working all this while and suddenly stopped working from today, assuming that your airflow platform did not get upgraded meanwhile, seems to mean something changed upstream? If you're querying an API, maybe you can add a retry logic if it's not already there?
* Do you see any helpful details in the `▶ Post task execution logs`?

pvieito (Issue Creator) on (2024-11-05 10:09:06 UTC): Hi @kunaljubce!

- No updates in that time window.
- All our tasks are vanilla Airflow AWS operators and super-simple DAGs based on some templates (basically `BatchOperator` & `GlueOperator`):

```python
with DAG(
        dag_id=dag_id,
        schedule=dag_schedule,
        start_date=pendulum.parse(dag_schedule_start_date),
        tags=dag_deployment_tags,
        catchup=False,
) as dag:
    batch_job = BatchOperator(
        task_id=""batch_job"",
        job_name=job_name,
        job_definition=job_definition_arn,
        job_queue=job_queue_name,
        tags=job_tags,
        aws_conn_id=job_aws_connection_id,
        container_overrides=dict(
            environment=aws_batch_job_environment_variables,
        ))

    log_batch_job() >> batch_job
```

- All deferred task started to mis-behave indefinetly and all new deferred task also mis-behave.
- Some extra full logs of tasks being stuck in DEFERRED without any waiter logs:

- With `GlueOperator`:
```
ip-10-5-146-175.eu-west-1.compute.internal
*** Reading remote log from Cloudwatch log_group: airflow-DT-LAB-Airflow-Environment-Task log_stream: dag_id=DT-LAB-Autostop-Schedule-PushProximity-1/run_id=manual__2024-11-04T09_55_51+00_00/task_id=glue_job/attempt=1.log.
[2024-11-04, 09:55:59 UTC] {local_task_job_runner.py:123} ▼ Pre task execution logs
[2024-11-04, 09:55:59 UTC] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: DT-LAB-Autostop-Schedule-PushProximity-1.glue_job manual__2024-11-04T09:55:51+00:00 [queued]>
[2024-11-04, 09:55:59 UTC] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: DT-LAB-Autostop-Schedule-PushProximity-1.glue_job manual__2024-11-04T09:55:51+00:00 [queued]>
[2024-11-04, 09:55:59 UTC] {taskinstance.py:2865} INFO - Starting attempt 1 of 1
[2024-11-04, 09:55:59 UTC] {taskinstance.py:2888} INFO - Executing <Task(GlueJobOperator): glue_job> on 2024-11-04 09:55:51+00:00
[2024-11-04, 09:55:59 UTC] {standard_task_runner.py:72} INFO - Started process 2450 to run task
[2024-11-04, 09:55:59 UTC] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'DT-LAB-Autostop-Schedule-PushProximity-1', 'glue_job', 'manual__2024-11-04T09:55:51+00:00', '--job-id', '39075', '--raw', '--subdir', 'DAGS_FOLDER/DT-LAB-Autostop-Schedule-PushProximity-1.py', '--cfg-path', '/tmp/tmpceiye8fs']
[2024-11-04, 09:55:59 UTC] {standard_task_runner.py:105} INFO - Job 39075: Subtask glue_job
[2024-11-04, 09:55:59 UTC] {task_command.py:467} INFO - Running <TaskInstance: DT-LAB-Autostop-Schedule-PushProximity-1.glue_job manual__2024-11-04T09:55:51+00:00 [running]> on host ip-10-5-146-175.eu-west-1.compute.internal
[2024-11-04, 09:56:00 UTC] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='DT-LAB-Autostop-Schedule-PushProximity-1' AIRFLOW_CTX_TASK_ID='glue_job' AIRFLOW_CTX_EXECUTION_DATE='2024-11-04T09:55:51+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-11-04T09:55:51+00:00'
[2024-11-04, 09:56:00 UTC] {taskinstance.py:731} ▲▲▲ Log group end
[2024-11-04, 09:56:00 UTC] {glue.py:188} INFO - Initializing AWS Glue Job: DT-LAB-Autostop-Schedule-PushProximity-1. Wait for completion: True
[2024-11-04, 09:56:00 UTC] {glue.py:365} INFO - Checking if job already exists: DT-LAB-Autostop-Schedule-PushProximity-1
[2024-11-04, 09:56:00 UTC] {base.py:84} INFO - Retrieving connection 'aws_shopfully_data'
[2024-11-04, 09:56:02 UTC] {glue.py:209} INFO - You can monitor this Glue Job run at: https://console.aws.amazon.com/gluestudio/home?region=eu-west-1#/job/DT-LAB-Autostop-Schedule-PushProximity-1/run/jr_d8efd3e7c7ca097b048db1e2e4342cd3e8eb3f03a225335bdef726c3ff63abb3
[2024-11-04, 09:56:02 UTC] {taskinstance.py:288} INFO - Pausing task as DEFERRED. dag_id=DT-LAB-Autostop-Schedule-PushProximity-1, task_id=glue_job, run_id=manual__2024-11-04T09:55:51+00:00, execution_date=20241104T095551, start_date=20241104T095559
[2024-11-04, 09:56:02 UTC] {taskinstance.py:340} ▼ Post task execution logs
[2024-11-04, 09:56:02 UTC] {local_task_job_runner.py:260} INFO - Task exited with return code 100 (task deferral)
[2024-11-04, 09:56:02 UTC] {local_task_job_runner.py:245} ▲▲▲ Log group end
[2024-11-04, 09:56:04 UTC] {base.py:84} INFO - Retrieving connection 'aws_shopfully_data'
[2024-11-04, 09:56:05 UTC] {base.py:84} INFO - Retrieving connection 'aws_shopfully_data'
```

- With `BatchOperator`:

```
ip-10-5-146-175.eu-west-1.compute.internal
*** Reading remote log from Cloudwatch log_group: airflow-DT-LAB-Airflow-Environment-Task log_stream: dag_id=DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1/run_id=scheduled__2024-11-04T09_00_00+00_00/task_id=batch_job/attempt=1.log.
[2024-11-04, 10:00:13 UTC] {local_task_job_runner.py:123} ▼ Pre task execution logs
[2024-11-04, 10:00:13 UTC] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1.batch_job scheduled__2024-11-04T09:00:00+00:00 [queued]>
[2024-11-04, 10:00:13 UTC] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1.batch_job scheduled__2024-11-04T09:00:00+00:00 [queued]>
[2024-11-04, 10:00:13 UTC] {taskinstance.py:2865} INFO - Starting attempt 1 of 1
[2024-11-04, 10:00:13 UTC] {taskinstance.py:2888} INFO - Executing <Task(BatchOperator): batch_job> on 2024-11-04 09:00:00+00:00
[2024-11-04, 10:00:13 UTC] {standard_task_runner.py:72} INFO - Started process 2486 to run task
[2024-11-04, 10:00:13 UTC] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1', 'batch_job', 'scheduled__2024-11-04T09:00:00+00:00', '--job-id', '39077', '--raw', '--subdir', 'DAGS_FOLDER/DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1.py', '--cfg-path', '/tmp/tmp7b1ya_1l']
[2024-11-04, 10:00:13 UTC] {standard_task_runner.py:105} INFO - Job 39077: Subtask batch_job
[2024-11-04, 10:00:13 UTC] {task_command.py:467} INFO - Running <TaskInstance: DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1.batch_job scheduled__2024-11-04T09:00:00+00:00 [running]> on host ip-10-5-146-175.eu-west-1.compute.internal
[2024-11-04, 10:00:14 UTC] {taskinstance.py:3131} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1' AIRFLOW_CTX_TASK_ID='batch_job' AIRFLOW_CTX_EXECUTION_DATE='2024-11-04T09:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2024-11-04T09:00:00+00:00'
[2024-11-04, 10:00:14 UTC] {taskinstance.py:731} ▲▲▲ Log group end
[2024-11-04, 10:00:14 UTC] {batch.py:303} INFO - Running AWS Batch job - job definition: arn:aws:batch:eu-west-1:532132528437:job-definition/DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1:358 - on queue DT-LAB-BatchExecution-Standard
[2024-11-04, 10:00:14 UTC] {batch.py:310} INFO - AWS Batch job - container overrides: {'environment': [{'name': '_DATATOOLS_DAG_DEPLOYMENT_IDENTIFIER', 'value': 'DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1'}, {'name': '_DATATOOLS_DAG_DEPLOYMENT_TARGET', 'value': 'airflow'}, {'name': 'DATATOOLS_ENVIRONMENT', 'value': 'LAB'}, {'name': '_DATATOOLS_AIRFLOW_DEPLOYMENT_DAG_ID', 'value': 'DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1'}, {'name': '_DATATOOLS_AIRFLOW_DEPLOYMENT_ENVIRONMENT_NAME', 'value': 'shopfully-data-execution'}, {'name': '_DATATOOLS_AIRFLOW_DEPLOYMENT_DATA_INTERVAL_START_DATETIME', 'value': '2024-11-04 09:00:00+00:00'}, {'name': '_DATATOOLS_AIRFLOW_DEPLOYMENT_DATA_INTERVAL_END_DATETIME', 'value': '2024-11-04 10:00:00+00:00'}, {'name': '_DATATOOLS_PROCESS_EXTRA_ENVIRONMENT_BLOB', 'value': ''}]}
[2024-11-04, 10:00:14 UTC] {base.py:84} INFO - Retrieving connection 'aws_shopfully_data_execution'
[2024-11-04, 10:00:15 UTC] {batch.py:347} INFO - AWS Batch job (c6157b39-4197-4a98-9a0d-2310866a2495) started: {'ResponseMetadata': {'RequestId': 'f7570782-3748-4d95-97b6-2fcc29820dd4', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Mon, 04 Nov 2024 10:00:15 GMT', 'content-type': 'application/json', 'content-length': '198', 'connection': 'keep-alive', 'x-amzn-requestid': 'f7570782-3748-4d95-97b6-2fcc29820dd4', 'access-control-allow-origin': '*', 'x-amz-apigw-id': 'At0vfHotjoEEbJQ=', 'access-control-expose-headers': 'X-amzn-errortype,X-amzn-requestid,X-amzn-errormessage,X-amzn-trace-id,X-amz-apigw-id,date', 'x-amzn-trace-id': 'Root=1-67289b2f-4f45249a51c82f7f02fdaa9d'}, 'RetryAttempts': 0}, 'jobArn': 'arn:aws:batch:eu-west-1:532132528437:job/c6157b39-4197-4a98-9a0d-2310866a2495', 'jobName': 'DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1', 'jobId': 'c6157b39-4197-4a98-9a0d-2310866a2495'}
[2024-11-04, 10:00:15 UTC] {taskinstance.py:288} INFO - Pausing task as DEFERRED. dag_id=DT-LAB-DataTools-Schedule-RemoteExecutionCheck-1, task_id=batch_job, run_id=scheduled__2024-11-04T09:00:00+00:00, execution_date=20241104T090000, start_date=20241104T100013
[2024-11-04, 10:00:15 UTC] {taskinstance.py:340} ▼ Post task execution logs
[2024-11-04, 10:00:16 UTC] {local_task_job_runner.py:260} INFO - Task exited with return code 100 (task deferral)
[2024-11-04, 10:00:16 UTC] {local_task_job_runner.py:245} ▲▲▲ Log group end
```

kunaljubce on (2024-11-05 15:38:52 UTC): @pvieito - Thanks for these, nothing apparent seems to jump out from these logs.

Would it be possible for you to look at all logs for triggerer, scheduler etc.? Maybe try clearing the tasks and then follow the system logs to see if it prints something referring to your deferred tasks.  If you can add those logs below, we can try looking for a possible issue because given your information that nothing changed on the airflow platform side, there doesn't seem much for us to go on. 

cc: @andrewgodwin I see a similar issue reported long back (#25630), hence tagging you here.

pvieito (Issue Creator) on (2024-11-06 17:19:34 UTC): Hi @kunaljubce!

I attach the full logs from the tasks, workers & scheduler from `2024-11-04T09:00:00` to `2024-11-04T11:30:00`.

- At around 9:00 UTC as you can see for example the task with AWS Batch identifier `16ffa872-59c1-4eb9-a7b4-50622364db0b` is correctly deferred and `waiter_with_logging.py` logs the expected checks.
- All seems to be working fine until around 10:00 UTC. Then all tasks (for example the one with AWS Batch identifier `c6157b39-4197-4a98-9a0d-2310866a2495`) starts being deferred but no checks are done: no more `waiter_with_logging.py` logs after the one at `2024-11-04 09:55:39.871`).

[Airflow-Logs.csv](https://github.com/user-attachments/files/17650761/Airflow-Logs.csv)

kunaljubce on (2024-11-08 11:00:21 UTC): @pvieito I looked through the logs and it does not seem to be giving us an awful lot in terms of an actual issue. I would probably have to point you back to your platform/devops team who maintains your Airflow environment to check if this is memory related? Because from the logs, I can see some of the tasks being submitted, executed, and completed. Only after a period of time does your triggerer start behaving weirdly and starts indefinitely deferring the tasks.

gopidesupavan on (2024-11-08 18:41:35 UTC): This is difficult to reproduce and debug. There are optimizations related to triggers in Airflow 2.10.3, so try upgrading and check if any issues persist. Moving this to discussions.

"
2632280844,issue,closed,completed,Airflow API creating roles ,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I'm trying to create a role via the airflow api but keep getting an error message. 
- I'm running airflow in a container . Local test env in podman and in AWS ECS contiainer
- Reading out roles via the API goes fine.
- tested via curl ( see picture below)
- tested via python ( see script at ""how to reproduce"")
- error on version 2.5.1 but also on 2.10
![airflow-issue-api](https://github.com/user-attachments/assets/e0c6e977-d14e-4f2c-8b1f-528b304205dc)

- same error on  version 2.10.2
```
AIRFLOW_API_URL: http://localhost:8080/api/v1
Failed to create role: 500


<!DOCTYPE html>
<html lang=""en"">
  <head>
    <link rel=""stylesheet"" href=""https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css"">
  </head>
  <body>
    <div class=""container"">
      <h1> Ooops! </h1>
      <div>
          <pre>
Something bad has happened. For security reasons detailed information about the error is not logged.

  * You should check your webserver logs and retrieve details of this error.

  * When you get the logs, it might explain the reasons, you should also Look for similar issues using:

     * <b><a href=""https://github.com/apache/airflow/discussions"">GitHub Discussions</a></b>
     * <b><a href=""https://github.com/apache/airflow/issues"">GitHub Issues</a></b>
     * <b><a href=""https://stackoverflow.com/questions/tagged/airflow"">Stack Overflow</a></b>
     * the usual search engine you use on a daily basis

    All those resources might help you to find a solution to your problem.

  * if you run Airflow on a Managed Service, consider opening an issue using the service support channels

  * only after you tried it all, and have difficulty with diagnosing and fixing the problem yourself,
    get the logs with errors, describe results of your investigation so far, and consider creating a
    <b><a href=""https://github.com/apache/airflow/issues/new/choose"">bug report</a></b> including this information.

Python version: 3.9.20
Airflow version: 2.10.2
Node: redacted
-------------------------------------------------------------------------------
Error! Please contact server admin.</pre>
      </div>
    </div>
  </body>
</html>
```

### What you think should happen instead?

API bug

### How to reproduce

Python script to create roles
``` 
import requests
import json

# Configuratie
AIRFLOW_API_URL = ""http://localhost:8080/api/v1""
USERNAME = 'admin'
PASSWORD = 'admin'

auth = (USERNAME, PASSWORD)

print(f""AIRFLOW_API_URL: {AIRFLOW_API_URL}"")

# Functie voor het ophalen van rollen in Airflow via API
def get_airflow_roles(uri, auth):
    url = f""{uri}/roles""
    response = requests.get(url, auth=auth)

    if response.status_code == 200:
        print(""Get roles:"")
        print(response.json())
    else:
        print(f""Failed to get roles: {response.status_code}"")
        print(response.text)

# Functie voor het aanmaken van een rol in Airflow via API
def create_airflow_role(uri, auth, role_name):
    url = f""{uri}/roles""
    headers = {'Content-Type': 'application/json'}
    data = json.dumps({
        ""name"": role_name
    })

    response = requests.post(url, headers=headers, auth=auth, data=data)

    if response.status_code == 201:
        print(f""Role '{role_name}' created."")
    else:
        print(f""Failed to create role: {response.status_code}"")
        print(response.text)

# Hoofdfunctie
if __name__ == ""__main__"":
    # Voorbeeld: Ophalen van bestaande rollen
    #get_airflow_roles(AIRFLOW_API_URL, auth)

    # Voorbeeld: Aanmaken van een nieuwe rol zonder permissies
    nieuwe_rol_naam = ""admin_config""
    create_airflow_role(AIRFLOW_API_URL, auth, nieuwe_rol_naam)
```


### Operating System

MacOS 15.1

### Versions of Apache Airflow Providers

No extra providers. Just default airflow installation

### Deployment

Docker-Compose

### Deployment details

pod man-compose.yml local test environment;
```
version: '3.7'
services:
  postgres:
    image: postgres:14-alpine
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
      POSTGRES_HOST_AUTH_METHOD: trust
    ports:
      - ""5432:5432""
    volumes:
      - postgres_db:/var/lib/postgresql/data

  airflow-init:
    # image: apache/airflow:2.5.1-python3.9
    image: apache/airflow:2.10.2-python3.9
    environment:
      AIRFLOW__API__AUTH_BACKENDS: ""airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session""
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: ""EKrgNDRARUhv3jOhbixG6EwZGKpUxwVa6KLj0nuTA4A=""
      AIRFLOW__CORE__LOAD_EXAMPLES: 'True'
      SQLALCHEMY_WARN_20: '1'
      SQLALCHEMY_SILENCE_UBER_WARNING: '1'
    volumes:
      - ./dags:/opt/airflow/dags
    entrypoint: [""airflow"", ""db"", ""init""]
    depends_on:
      - postgres

  airflow-create-user:
    image: apache/airflow:2.10.2-python3.9
    environment:
      AIRFLOW__API__AUTH_BACKENDS: ""airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session""  # Geïdentificeerde typfout gecorrigeerd
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: ""EKrgNDRARUhv3jOhbixG6EwZGKpUxwVa6KLj0nuTA4A=""
      AIRFLOW__CORE__LOAD_EXAMPLES: 'True'
    volumes:
      - ./dags:/opt/airflow/dags
    entrypoint: [""airflow"", ""users"", ""create"", ""--username"", ""admin"", ""--password"", ""admin"", ""--firstname"", ""Admin"", ""--lastname"", ""User"", ""--role"", ""Admin"", ""--email"", ""admin@example.com""]
    depends_on:
      - postgres
      - airflow-init

  airflow-webserver:
    image: apache/airflow:2.10.2-python3.9
    environment:
      AIRFLOW__API__AUTH_BACKENDS: ""airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session""
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: ""EKrgNDRARUhv3jOhbixG6EwZGKpUxwVa6KLj0nuTA4A=""
      AIRFLOW__CORE__LOAD_EXAMPLES: 'True'
      AIRFLOW__WEBSERVER__RBAC: 'True'
      SQLALCHEMY_WARN_20: '1'
      SQLALCHEMY_SILENCE_UBER_WARNING: '1'
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'True'
    volumes:
      - ./dags:/opt/airflow/dags
      # - ./config/webserver_config.py:/opt/airflow/webserver_config.py
    command: webserver
    ports:
      - ""8080:8080""
    depends_on:
      - postgres
      - airflow-init

  airflow-scheduler:
    image: apache/airflow:2.10.2-python3.9
    environment:
      AIRFLOW__API__AUTH_BACKENDS: ""airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session""
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__FERNET_KEY: ""EKrgNDRARUhv3jOhbixG6EwZGKpUxwVa6KLj0nuTA4A=""
      AIRFLOW__CORE__LOAD_EXAMPLES: 'True'
      SQLALCHEMY_WARN_20: '1'
      SQLALCHEMY_SILENCE_UBER_WARNING: '1'
    volumes:
      - ./dags:/opt/airflow/dags
    depends_on:
      - postgres
      - airflow-init

volumes:
  postgres_db:

```

### Anything else?

Same error in 
- different versions (2.5.1 and 2.10.2) 
- tested in podman, docker and AWS ECS container


### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",D2CIT,2024-11-04 09:27:49+00:00,[],2024-11-04 20:37:25+00:00,2024-11-04 20:37:25+00:00,https://github.com/apache/airflow/issues/43627,"[('kind:bug', 'This is a clearly a bug'), ('area:API', ""Airflow's REST/HTTP API""), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2454199433, 'issue_id': 2632280844, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 4, 9, 27, 53, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-04 09:27:53 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2631973688,issue,closed,not_planned,Is there operator to create or update AWS DMS Serverless replication?,"### Description

Is there operator to create or update AWS DMS Serverless replication?

### Use case/motivation

AWS DMS provide the Serverless replication which provide better effect for cost saving. so many customer want to use Airflow to create AWS DMS Serverless replication.

### Related issues

No.

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",xudalei1977,2024-11-04 06:42:40+00:00,[],2024-11-04 07:46:57+00:00,2024-11-04 07:46:57+00:00,https://github.com/apache/airflow/issues/43621,"[('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2453924924, 'issue_id': 2631973688, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 4, 6, 42, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2454016932, 'issue_id': 2631973688, 'author': 'eladkal', 'body': 'Duplicate of https://github.com/apache/airflow/issues/39954.\r\n@ellisms is working on it.\r\n\r\nClosing as duplicate', 'created_at': datetime.datetime(2024, 11, 4, 7, 46, 57, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-04 06:42:43 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2024-11-04 07:46:57 UTC): Duplicate of https://github.com/apache/airflow/issues/39954.
@ellisms is working on it.

Closing as duplicate

"
2631964327,issue,closed,completed,Deferred Glue Operators seem to always be verbose,"### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.28.0

### Apache Airflow version

2.10.1

### Operating System

Amazon Linux 2023

### Deployment

Amazon (AWS) MWAA

### Deployment details

MWAA in production and mwaa-local-runner for local testing. Bug exists on both.

### What happened

Deferred GlueJobOperator tasks are getting job failures due to rate limiting on fetching logs from Cloudwatch, we have verbose set to False. The traceback shows a function `print_job_logs` that should only be called when verbose is true. Additionally when I watch the task logs in real time they are definitely pulling the Glue logs from Cloudwatch into Airflow.

```python
Traceback (most recent call last):

  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/jobs/triggerer_job_runner.py"", line 558, in cleanup_finished_triggers
    result = details[""task""].result()
             ^^^^^^^^^^^^^^^^^^^^^^^^

  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/jobs/triggerer_job_runner.py"", line 630, in run_trigger
    async for event in trigger.run():

  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/amazon/aws/triggers/glue.py"", line 73, in run
    await hook.async_job_completion(self.job_name, self.run_id, self.verbose)

  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/amazon/aws/hooks/glue.py"", line 315, in async_job_completion
    ret = self._handle_state(job_run_state, job_name, run_id, verbose, next_log_tokens)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/amazon/aws/hooks/glue.py"", line 334, in _handle_state
    self.print_job_logs(

  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/amazon/aws/hooks/glue.py"", line 278, in print_job_logs
    continuation_tokens.output_stream_continuation = display_logs_from(
                                                     ^^^^^^^^^^^^^^^^^^

  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/amazon/aws/hooks/glue.py"", line 245, in display_logs_from
    for response in paginator.paginate(

  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/botocore/paginate.py"", line 269, in __iter__
    response = self._make_request(current_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/botocore/paginate.py"", line 357, in _make_request
    return self._method(**current_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/botocore/client.py"", line 569, in _api_call
    return self._make_api_call(operation_name, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/botocore/client.py"", line 1023, in _make_api_call
    raise error_class(parsed_response, operation_name)

botocore.errorfactory.ThrottlingException: An error occurred (ThrottlingException) when calling the FilterLogEvents operation (reached max retries: 4): Rate exceeded
```

A colleague and I read through the operator code and it all looks correct, so we suspect that the issues is occurring when the task is serialized as that specifically converts the bool to a string. The string ""False"" would then be truthy and cause this code to run. I'm guessing that the tasks are serialized when they are deferred.

Serialization:
https://github.com/apache/airflow/blob/ff6038b8d154ccdde65a4ebd652ad41d88e8c33e/providers/src/airflow/providers/amazon/aws/triggers/glue.py#L58-L69

Code which is getting called when verbose is false:
https://github.com/apache/airflow/blob/ff6038b8d154ccdde65a4ebd652ad41d88e8c33e/providers/src/airflow/providers/amazon/aws/hooks/glue.py#L333-L338

Example python showing bool serialization issues:
```python
Python 3.11.7 (main, Oct 26 2024, 04:00:37) [GCC 11.4.1 20230605 (Red Hat 11.4.1-2)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> a = False
>>> str(a)
'False'
>>> bool(str(a))
True
>>>
```


### What you think should happen instead

Cloudwatch logs should not be read into Airflow when verbose is False.

If the serialization process is the issue, then it should either change to serialize differently or the deserialization should be changed to understand `""True""` and `""False""`.



### How to reproduce

1. Create a GlueJobOperator task.
2. Set deferrable=True and verbose=False
3. Run the task and watch the Airflow logs while it is in a deferred state.

### Anything else

Seems to be happening everytime if I watch the tasks. Completed tasks (success or failed) do not show the logs but deferred tasks do, which to me backs up some kind of serialization bug in the Operator.

The traceback shows that code is being called when running even though in the log it is not shown.

Would be willing to submit a PR if someone could assist.

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jimwbaldwin,2024-11-04 06:35:09+00:00,[],2024-11-05 19:51:08+00:00,2024-11-05 19:51:08+00:00,https://github.com/apache/airflow/issues/43620,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('area:logging', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2453917528, 'issue_id': 2631964327, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 4, 6, 35, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453923232, 'issue_id': 2631964327, 'author': 'jimwbaldwin', 'body': ""I had a quick look at other serialize functions and they just seem to make `dict[str, Any]` and don't change the types. Could it be as simple as removing that str?"", 'created_at': datetime.datetime(2024, 11, 4, 6, 40, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453968074, 'issue_id': 2631964327, 'author': 'jimwbaldwin', 'body': 'There is a test for verbose=True but not verbose=False, as it is always True then this will pass fine.\r\n\r\nhttps://github.com/apache/airflow/blob/ff6038b8d154ccdde65a4ebd652ad41d88e8c33e/providers/tests/amazon/aws/operators/test_glue.py#L158-L183', 'created_at': datetime.datetime(2024, 11, 4, 7, 18, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2454018553, 'issue_id': 2631964327, 'author': 'eladkal', 'body': 'Feel free to raise PR with your code suggestions (and improving the tests). It will be easier to review and look into it with a PR', 'created_at': datetime.datetime(2024, 11, 4, 7, 48, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2454079424, 'issue_id': 2631964327, 'author': 'gopidesupavan', 'body': 'I didn’t quite understand your question. In your configuration, the deferrable parameter is set to False in the re-produce step (step 2). This means the operator won’t use the deferrable mode; instead, it will execute normally on the worker.', 'created_at': datetime.datetime(2024, 11, 4, 8, 26, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2454083510, 'issue_id': 2631964327, 'author': 'jimwbaldwin', 'body': ""> I didn’t quite understand your question. In your configuration, the deferrable parameter is set to False in the re-produce step (step 2). This means the operator won’t use the deferrable mode; instead, it will execute normally on the worker.\r\n\r\nSorry, I made a mistake typing it up. I'll fix it."", 'created_at': datetime.datetime(2024, 11, 4, 8, 28, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2454087046, 'issue_id': 2631964327, 'author': 'jimwbaldwin', 'body': 'Fixed in description.\r\n\r\n1. Create a GlueJobOperator task.\r\n2. Set deferrable=True and verbose=False\r\n3. Run the task and watch the Airflow logs while it is in a deferred state.', 'created_at': datetime.datetime(2024, 11, 4, 8, 30, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455856293, 'issue_id': 2631964327, 'author': 'jimwbaldwin', 'body': ""Heya, I've added a test to this PR which shows the bug.\r\nhttps://github.com/apache/airflow/pull/43622"", 'created_at': datetime.datetime(2024, 11, 4, 22, 46, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455974649, 'issue_id': 2631964327, 'author': 'jimwbaldwin', 'body': 'Here is the test showing the problem and it is currently failing.\r\n<img width=""1147"" alt=""image"" src=""https://github.com/user-attachments/assets/8129fbf7-a039-4351-a6e4-0f1829dc4b58"">', 'created_at': datetime.datetime(2024, 11, 5, 0, 26, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2458030955, 'issue_id': 2631964327, 'author': 'jimwbaldwin', 'body': 'Should be fixed now by my reference PR.', 'created_at': datetime.datetime(2024, 11, 5, 19, 50, 55, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-04 06:35:12 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jimwbaldwin (Issue Creator) on (2024-11-04 06:40:57 UTC): I had a quick look at other serialize functions and they just seem to make `dict[str, Any]` and don't change the types. Could it be as simple as removing that str?

jimwbaldwin (Issue Creator) on (2024-11-04 07:18:44 UTC): There is a test for verbose=True but not verbose=False, as it is always True then this will pass fine.

https://github.com/apache/airflow/blob/ff6038b8d154ccdde65a4ebd652ad41d88e8c33e/providers/tests/amazon/aws/operators/test_glue.py#L158-L183

eladkal on (2024-11-04 07:48:07 UTC): Feel free to raise PR with your code suggestions (and improving the tests). It will be easier to review and look into it with a PR

gopidesupavan on (2024-11-04 08:26:31 UTC): I didn’t quite understand your question. In your configuration, the deferrable parameter is set to False in the re-produce step (step 2). This means the operator won’t use the deferrable mode; instead, it will execute normally on the worker.

jimwbaldwin (Issue Creator) on (2024-11-04 08:28:36 UTC): Sorry, I made a mistake typing it up. I'll fix it.

jimwbaldwin (Issue Creator) on (2024-11-04 08:30:14 UTC): Fixed in description.

1. Create a GlueJobOperator task.
2. Set deferrable=True and verbose=False
3. Run the task and watch the Airflow logs while it is in a deferred state.

jimwbaldwin (Issue Creator) on (2024-11-04 22:46:55 UTC): Heya, I've added a test to this PR which shows the bug.
https://github.com/apache/airflow/pull/43622

jimwbaldwin (Issue Creator) on (2024-11-05 00:26:06 UTC): Here is the test showing the problem and it is currently failing.
<img width=""1147"" alt=""image"" src=""https://github.com/user-attachments/assets/8129fbf7-a039-4351-a6e4-0f1829dc4b58"">

jimwbaldwin (Issue Creator) on (2024-11-05 19:50:55 UTC): Should be fixed now by my reference PR.

"
2631914700,issue,closed,completed,Move Asset user-facing components to task_sdk,"### Description

## Why
[AIP-72](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-72+Task+Execution+Interface+aka+Task+SDK)

## What

Move asset user-facing components (e.g., `Asset`, `AssetAlias`, `AssetAll`, `AssetAny`, `Dataset`, `Model`, `@asset` ) to `task_sdk`

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-11-04 05:57:01+00:00,['Lee-W'],2024-11-20 09:09:42+00:00,2024-11-20 09:09:42+00:00,https://github.com/apache/airflow/issues/43619,"[('kind:feature', 'Feature Requests'), ('area:datasets', 'Issues related to the datasets feature'), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]","[{'comment_id': 2467102237, 'issue_id': 2631914700, 'author': 'Lee-W', 'body': 'Updated: Nov 11 th\n\nPR https://github.com/apache/airflow/pull/43773 in reivew', 'created_at': datetime.datetime(2024, 11, 11, 2, 18, 17, tzinfo=datetime.timezone.utc)}]","Lee-W (Issue Creator) on (2024-11-11 02:18:17 UTC): Updated: Nov 11 th

PR https://github.com/apache/airflow/pull/43773 in reivew

"
2631057680,issue,closed,completed,"Status of testing Providers that were prepared on November 03, 2024","### Body


I have a kind request for all the contributors to the latest provider packages release.
Could you please help us to test the RC versions of the providers?

The guidelines on how to test providers can be found in

[Verify providers by contributors](https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDER_PACKAGES.md#verify-the-release-candidate-by-contributors)

Let us know in the comment, whether the issue is addressed.

Those are providers that require testing as there were some substantial changes introduced:


## Provider [amazon: 9.1.0rc3](https://pypi.org/project/apache-airflow-providers-amazon/9.1.0rc3)
   - [ ] [feat: add OpenLineage support for RedshiftToS3Operator (#41632)](https://github.com/apache/airflow/pull/41632): @kacpermuda
     Linked issues:
       - [ ] [Linked Issue #41575](https://github.com/apache/airflow/pull/41575): @Artuz37
   - [ ] [Add SageMakerProcessingSensor (#43144)](https://github.com/apache/airflow/pull/43144): @vVv-AA
   - [ ] [Make `RedshiftDataOperator`  handle multiple queries (#42900)](https://github.com/apache/airflow/pull/42900): @jroachgolf84
   - [x] [fix(providers/amazon): alias is_authorized_dataset to is_authorized_asset (#43470)](https://github.com/apache/airflow/pull/43470): @Lee-W
   - [ ] [Remove returns in final clause of athena hooks (#43426)](https://github.com/apache/airflow/pull/43426): @yangyulely
     Linked issues:
       - [ ] [Linked Issue #43274](https://github.com/apache/airflow/issues/43274): @iritkatriel
   - [x] [Remove sqlalchemy-redshift dependency (#43271)](https://github.com/apache/airflow/pull/43271): @mobuchowski
   - [x] [feat(providers/amazon): Use asset in common provider (#43110)](https://github.com/apache/airflow/pull/43110): @Lee-W
   - [ ] [Restrict looker-sdk version 24.18.0 and microsoft-kiota-http 1.3.4 (#42954)](https://github.com/apache/airflow/pull/42954): @gopidesupavan
   - [x] [Limit mypy-boto3-appflow (#43436)](https://github.com/apache/airflow/pull/43436): @eladkal
   - [ ] [Standard provider python operator (#42081)](https://github.com/apache/airflow/pull/42081): @gopidesupavan
## Provider [fab: 1.5.0rc3](https://pypi.org/project/apache-airflow-providers-fab/1.5.0rc3)
   - [x] [feat(providers/fab): Use asset in common provider (#43112)](https://github.com/apache/airflow/pull/43112): @Lee-W
   - [x] [Fix revoke Dag stale permission on airflow < 2.10 (#42844)](https://github.com/apache/airflow/pull/42844): @joaopamaral
     Linked issues:
       - [x] [Linked Issue #42743](https://github.com/apache/airflow/issues/42743): @RostD
   - [x] [fix(providers/fab): alias is_authorized_dataset to is_authorized_asset (#43469)](https://github.com/apache/airflow/pull/43469): @Lee-W
   - [ ] [Docs: Change CustomSecurityManager method name (#43034)](https://github.com/apache/airflow/pull/43034): @kgw7401
   - [ ] [Bump Flask-AppBuilder to ``4.5.2`` (#43309)](https://github.com/apache/airflow/pull/43309): @kaxil
   - [ ] [Upgrade FAB to 4.5.1 (#43251)](https://github.com/apache/airflow/pull/43251): @potiuk
   - [ ] [Move user and roles schemas to fab provider (#42869)](https://github.com/apache/airflow/pull/42869): @vincbeck
   - [ ] [Move the session auth backend to FAB auth manager (#42878)](https://github.com/apache/airflow/pull/42878): @vincbeck
     Linked issues:
       - [ ] [Linked Issue #42634](https://github.com/apache/airflow/pull/42634): @vincbeck
   - [ ] [Add logging to the migration commands (#43516)](https://github.com/apache/airflow/pull/43516): @ephraimbuddy
   - [x] [Rename dataset as asset in UI (#43073)](https://github.com/apache/airflow/pull/43073): @Lee-W

<!--

NOTE TO RELEASE MANAGER:

You can move here the providers that have doc-only changes or for which changes are trivial, and
you could assess that they are OK.

-->
All users involved in the PRs:
@mobuchowski @kgw7401 @kacpermuda @kaxil @Lee-W @eladkal @yangyulely @vincbeck @potiuk @vVv-AA @joaopamaral @gopidesupavan @jroachgolf84 @ephraimbuddy


### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2024-11-03 09:28:27+00:00,[],2024-11-05 08:08:37+00:00,2024-11-05 08:08:36+00:00,https://github.com/apache/airflow/issues/43615,"[('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('kind:meta', 'High-level information important to the community'), ('testing status', 'Status of testing releases'), ('provider:fab', '')]","[{'comment_id': 2453499945, 'issue_id': 2631057680, 'author': 'gopidesupavan', 'body': 'Oh @eladkal common compat is required for aws provider, it has changes to appflow which references to standard provider. So as standard provider not available it should fallback to old imports using compat provider. \r\n\r\nhttps://github.com/apache/airflow/pull/42081/files#diff-8721ac5046348753733a590d30f9d7fead6ae61bf4f2e05a3167bdcde2cafc18R28', 'created_at': datetime.datetime(2024, 11, 3, 17, 12, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453500628, 'issue_id': 2631057680, 'author': 'gopidesupavan', 'body': 'Last release was oct 14th for compat provider, so along with aws provider now common compat also required i believe WDYT?', 'created_at': datetime.datetime(2024, 11, 3, 17, 14, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453506474, 'issue_id': 2631057680, 'author': 'potiuk', 'body': '> Last release was oct 14th for compat provider, so along with aws provider now common compat also required i believe WDYT?\r\n\r\nAgreed. Good call. And we might (in the next version of AWS provider) already start using ""standard"" provider as dependnecy as long as we release it and it works for 2.8 an 2.9 (https://github.com/apache/airflow/pull/43556)', 'created_at': datetime.datetime(2024, 11, 3, 17, 36, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2454404869, 'issue_id': 2631057680, 'author': 'Lee-W', 'body': 'Verify https://github.com/apache/airflow/pull/43469, https://github.com/apache/airflow/pull/43112, https://github.com/apache/airflow/pull/43470 and https://github.com/apache/airflow/pull/43110', 'created_at': datetime.datetime(2024, 11, 4, 10, 57, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455889203, 'issue_id': 2631057680, 'author': 'joaopamaral', 'body': 'Tested https://github.com/apache/airflow/pull/42844 with success.', 'created_at': datetime.datetime(2024, 11, 4, 23, 11, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2456493972, 'issue_id': 2631057680, 'author': 'eladkal', 'body': 'Thank you everyone. Providers are released.\r\nAmazon provider is excluded and will have rc4\r\n\r\nI invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).', 'created_at': datetime.datetime(2024, 11, 5, 8, 8, 37, tzinfo=datetime.timezone.utc)}]","gopidesupavan on (2024-11-03 17:12:31 UTC): Oh @eladkal common compat is required for aws provider, it has changes to appflow which references to standard provider. So as standard provider not available it should fallback to old imports using compat provider. 

https://github.com/apache/airflow/pull/42081/files#diff-8721ac5046348753733a590d30f9d7fead6ae61bf4f2e05a3167bdcde2cafc18R28

gopidesupavan on (2024-11-03 17:14:55 UTC): Last release was oct 14th for compat provider, so along with aws provider now common compat also required i believe WDYT?

potiuk on (2024-11-03 17:36:10 UTC): Agreed. Good call. And we might (in the next version of AWS provider) already start using ""standard"" provider as dependnecy as long as we release it and it works for 2.8 an 2.9 (https://github.com/apache/airflow/pull/43556)

Lee-W on (2024-11-04 10:57:37 UTC): Verify https://github.com/apache/airflow/pull/43469, https://github.com/apache/airflow/pull/43112, https://github.com/apache/airflow/pull/43470 and https://github.com/apache/airflow/pull/43110

joaopamaral on (2024-11-04 23:11:50 UTC): Tested https://github.com/apache/airflow/pull/42844 with success.

eladkal (Issue Creator) on (2024-11-05 08:08:37 UTC): Thank you everyone. Providers are released.
Amazon provider is excluded and will have rc4

I invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).

"
2630432941,issue,closed,completed,Add Custom Title to the Header in UI,"### Description

Add custom title in the UI header, next to Airflow logo

### Use case/motivation

Usually Airflow is deployed as several independent instances: one for test purpose, one for stage, one for production etc. As a result when using UI it's hard to tell which version is actually used. How about adding some label to the header next to airflow logo?

It could be configurable with UI ENV parameters like:
`VITE_HEADER_TAG=dev/prod/local/anything`
`VITE_HEADER_TAG_COLOR=red`

And here is example how it might look:
![airflow-env](https://github.com/user-attachments/assets/d9c49c32-630a-4915-9e1a-1edfca4aef7f)

If this will be greenlighted i'm willing to write the code

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vanmik,2024-11-02 12:34:34+00:00,[],2024-11-04 13:44:33+00:00,2024-11-03 09:02:53+00:00,https://github.com/apache/airflow/issues/43605,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2452976741, 'issue_id': 2630432941, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 2, 12, 34, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452983508, 'issue_id': 2630432941, 'author': 'potiuk', 'body': '@bbovenzi @pierrejeambrun  > something maybe to consider for Airflow 3 new UI if not already there?', 'created_at': datetime.datetime(2024, 11, 2, 13, 1, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453090575, 'issue_id': 2630432941, 'author': 'jscheffl', 'body': 'Is it the same like INSTANCE_NAME we have in Airflow 2 to be carried-over to Airflow 3?\r\nIn-deed this should be planned. Adding this to the UI project.\r\n\r\nScreenshot is from Airflow 2... please take a look to configuration reference... also the title bar color can be changed for direct visual highlight (e.g. we color the dev env with red)\r\n\r\nhttps://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#instance-name', 'created_at': datetime.datetime(2024, 11, 2, 18, 43, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453324913, 'issue_id': 2630432941, 'author': 'eladkal', 'body': 'Following what @jscheffl mentioned. Please see the docs:\r\nhttps://airflow.apache.org/docs/apache-airflow/stable/howto/customize-ui.html#id2\r\n\r\n![Screenshot 2024-11-03 at 9 17 59](https://github.com/user-attachments/assets/a3d8165a-eb3c-4562-a972-1a01ae4c698f)\r\n\r\n\r\nI think we can close this issue as solved', 'created_at': datetime.datetime(2024, 11, 3, 7, 18, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453335016, 'issue_id': 2630432941, 'author': 'potiuk', 'body': 'Indeed.', 'created_at': datetime.datetime(2024, 11, 3, 9, 2, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2454755178, 'issue_id': 2630432941, 'author': 'bbovenzi', 'body': 'Yes, this is already a task to pass the config to the new UI: https://github.com/apache/airflow/issues/43166', 'created_at': datetime.datetime(2024, 11, 4, 13, 44, 32, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-02 12:34:36 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-11-02 13:01:51 UTC): @bbovenzi @pierrejeambrun  > something maybe to consider for Airflow 3 new UI if not already there?

jscheffl on (2024-11-02 18:43:24 UTC): Is it the same like INSTANCE_NAME we have in Airflow 2 to be carried-over to Airflow 3?
In-deed this should be planned. Adding this to the UI project.

Screenshot is from Airflow 2... please take a look to configuration reference... also the title bar color can be changed for direct visual highlight (e.g. we color the dev env with red)

https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#instance-name

eladkal on (2024-11-03 07:18:22 UTC): Following what @jscheffl mentioned. Please see the docs:
https://airflow.apache.org/docs/apache-airflow/stable/howto/customize-ui.html#id2

![Screenshot 2024-11-03 at 9 17 59](https://github.com/user-attachments/assets/a3d8165a-eb3c-4562-a972-1a01ae4c698f)


I think we can close this issue as solved

potiuk on (2024-11-03 09:02:59 UTC): Indeed.

bbovenzi on (2024-11-04 13:44:32 UTC): Yes, this is already a task to pass the config to the new UI: https://github.com/apache/airflow/issues/43166

"
2629490026,issue,closed,completed,AIP-84 Migrate public endpoint to list import errors to FastAPI,"Migrate api_connexion's list import errors, GET. `/importErrors` to the public FastAPI api",bbovenzi,2024-11-01 17:10:22+00:00,[],2024-11-05 14:30:41+00:00,2024-11-05 14:30:41+00:00,https://github.com/apache/airflow/issues/43595,"[('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2452256513, 'issue_id': 2629490026, 'author': 'bbovenzi', 'body': 'cc: [jason810496](https://github.com/jason810496)', 'created_at': datetime.datetime(2024, 11, 1, 17, 11, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455283788, 'issue_id': 2629490026, 'author': 'jason810496', 'body': 'Thanks @bbovenzi for creating issues !', 'created_at': datetime.datetime(2024, 11, 4, 17, 21, 3, tzinfo=datetime.timezone.utc)}]","bbovenzi (Issue Creator) on (2024-11-01 17:11:17 UTC): cc: [jason810496](https://github.com/jason810496)

jason810496 on (2024-11-04 17:21:03 UTC): Thanks @bbovenzi for creating issues !

"
2629484443,issue,closed,completed,AIP-84 Migrate public endpoint Get Import Error to FastAPI,"Migrate api_connexion's Get ImportError endpoint, `/importErrors/{import_error_id}` to the public FastAPI",bbovenzi,2024-11-01 17:08:14+00:00,[],2024-11-05 14:30:40+00:00,2024-11-05 14:30:40+00:00,https://github.com/apache/airflow/issues/43594,"[('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2452251984, 'issue_id': 2629484443, 'author': 'bbovenzi', 'body': 'cc: [jason810496](https://github.com/jason810496)', 'created_at': datetime.datetime(2024, 11, 1, 17, 8, 20, tzinfo=datetime.timezone.utc)}]","bbovenzi (Issue Creator) on (2024-11-01 17:08:20 UTC): cc: [jason810496](https://github.com/jason810496)

"
2629342310,issue,closed,completed,Airflow-Keycloak: force of POST method at logout?,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

After upgrading to version 2.10.2 (from 2.8.2) when logging out from the web UI, I get the following error:
`""POST /logout/ HTTP/1.1"" 405 463 ""https://tst-airflow........./home"" `
It might be that this error originates from our side. So, I wanted to avoid the POST-method. I am using Keycloak for authentication, and part of the code is this:
```
from airflow.www.fab_security.manager import AUTH_OAUTH
from airflow.providers.fab.auth_manager.security_manager.override import FabAirflowSecurityManagerOverride
...
      class CustomAuthRemoteUserView(AuthOAuthView):
        @expose(""/logout/"")
        def logout(self):
            """"""Delete access token before logging out.""""""
            return super().logout()
      class CustomSecurityManager(FabAirflowSecurityManagerOverride):
        authoauthview = CustomAuthRemoteUserView
```
I hoped to solve my 405-463 error by overriding this new code in https://github.com/shahar1/airflow/blob/main/providers/src/airflow/providers/fab/auth_manager/security_manager/override.py :

```
# The following logic patches the logout method within AuthView, so it supports POST method
# to make CSRF protection effective. It is backward-compatible with Airflow versions <= 2.9.2 as it still
# allows utilizing the GET method for them.
# You could remove the patch and configure it when it is supported
# natively by Flask-AppBuilder (https://github.com/dpgaspar/Flask-AppBuilder/issues/2248)
if packaging.version.parse(packaging.version.parse(airflow_version).base_version) < packaging.version.parse(
    ""2.10.0""
):
    _methods = [""GET"", ""POST""]
else:
    _methods = [""POST""]


class _ModifiedAuthView(AuthView):
    @expose(""/logout/"", methods=_methods)
    def logout(self):
        return super().logout()


for auth_view in [AuthDBView, AuthLDAPView, AuthOAuthView, AuthOIDView, AuthRemoteUserView]:
    auth_view.__bases__ = (_ModifiedAuthView,)
```
and to use my class CustomAuthRemoteUserView again. But I don't succeed. Is it even possible to override that new class _ModifiedAuthView?

### What you think should happen instead?

I would have expected that my old override for logout still would work. I am afraid that the new class _ModifiedAuthView that has been added to __bases__ is dominating now.

### How to reproduce

I think that viewing the code and overthinking how this newly added code for logout can be overridden will be sufficient.

### Operating System

Kubernetes, via Helm-installation. (User Community, version 8.9.0)

### Versions of Apache Airflow Providers

apache-airflow-providers-fab             1.3.0

### Deployment

Other 3rd-party Helm chart

### Deployment details

Helm chart from User Community, version 8.9.0

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",NanineO,2024-11-01 16:02:54+00:00,[],2024-11-04 10:03:52+00:00,2024-11-04 10:03:52+00:00,https://github.com/apache/airflow/issues/43592,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:fab', '')]","[{'comment_id': 2452128811, 'issue_id': 2629342310, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 11, 1, 16, 2, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2454275069, 'issue_id': 2629342310, 'author': 'NanineO', 'body': 'I could solve it myself. Simpy replacing in my code:\r\n```\r\n      class CustomAuthRemoteUserView(AuthOAuthView):\r\n        @expose(""/logout/"")\r\n```\r\nby:\r\n```\r\n      class CustomAuthRemoteUserView(AuthOAuthView):\r\n        @expose(""/logout/"", methods=[""GET"", ""POST""])\r\n```\r\ndid the trick.', 'created_at': datetime.datetime(2024, 11, 4, 10, 3, 52, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-11-01 16:02:59 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

NanineO (Issue Creator) on (2024-11-04 10:03:52 UTC): I could solve it myself. Simpy replacing in my code:
```
      class CustomAuthRemoteUserView(AuthOAuthView):
        @expose(""/logout/"")
```
by:
```
      class CustomAuthRemoteUserView(AuthOAuthView):
        @expose(""/logout/"", methods=[""GET"", ""POST""])
```
did the trick.

"
2629069108,issue,closed,completed,Add API endpoint(s) for Task Status,"Part of [AIP-72](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-72+Task+Execution+Interface+aka+Task+SDK?src=contextnavpagetreemode).

We need two endpoints:

1. TI state update: that can update the state of a given Task instance from the worker via Supervisor process to the API
2. Heartbeat: Allows sending the heartbeat of a TI to show it is alive.",kaxil,2024-11-01 13:44:35+00:00,['kaxil'],2024-11-06 17:04:19+00:00,2024-11-06 17:04:18+00:00,https://github.com/apache/airflow/issues/43586,"[('area:API', ""Airflow's REST/HTTP API""), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]","[{'comment_id': 2455737908, 'issue_id': 2629069108, 'author': 'kaxil', 'body': 'Part 1 is covered by https://github.com/apache/airflow/pull/43602', 'created_at': datetime.datetime(2024, 11, 4, 21, 28, 12, tzinfo=datetime.timezone.utc)}]","kaxil (Issue Creator) on (2024-11-04 21:28:12 UTC): Part 1 is covered by https://github.com/apache/airflow/pull/43602

"
2629032769,issue,closed,completed,Status of testing of Apache Airflow 2.10.3rc2,"### Body

We are kindly requesting that contributors to [Apache Airflow RC 2.10.3rc2](https://pypi.org/project/apache-airflow/2.10.3rc2/) help test the RC.

Please let us know by commenting if the issue is addressed in the latest RC.

- [ ] [Bump dompurify from 2.2.9 to 2.5.6 in /airflow/www (#42263) (#42270)](https://github.com/apache/airflow/pull/42270): @pierrejeambrun
- [ ] [fix: Correct docstring format in _get_template_context (#42244) (#42272)](https://github.com/apache/airflow/pull/42272): @romsharon98
- [ ] [ Update StatsD Image Tag from failed dependencies check (#42264) (#42281)](https://github.com/apache/airflow/pull/42281): @jscheffl
     Linked issues:
     - [Update StatsD Image Tag from failed dependencies check (#42264)](https://github.com/apache/airflow/pull/42264)
- [ ] [Use `selectinload` in trigger (#42351)](https://github.com/apache/airflow/pull/42351): @vincbeck @josephangbc
     Linked issues:
     - [Use `selectinload` in trigger (#40487)](https://github.com/apache/airflow/pull/40487)
- [ ] [apply otel_service on metrics (#42242) (#42441)](https://github.com/apache/airflow/pull/42441): @romsharon98
- [ ] [ Bugfix task execution from runner in Windows (#42426) (#42478)](https://github.com/apache/airflow/pull/42478): @jscheffl
- [ ] [[BACKPORT] Fix the span link of task instance to point to the correct span in the scheduler_job_loop (#42430) (#42480)](https://github.com/apache/airflow/pull/42480): @howardyoo @shahar1
     Linked issues:
     - [Fix the span link of task instance to point to the correct span in the scheduler_job_loop (issue #42429) (#42430)](https://github.com/apache/airflow/pull/42430)
- [ ] [ Do not attempt to provide not stringified objects to UI via xcom if pickling is active (#42388) (#42486)](https://github.com/apache/airflow/pull/42486): @jscheffl
- [ ] [fix: ensure DAG trigger form submits with updated parameters upon key… (#42499)](https://github.com/apache/airflow/pull/42499): @pierrejeambrun
- [ ] [Handle ENTER key correctly in trigger form and allow manual JSON (#42… (#42535)](https://github.com/apache/airflow/pull/42535): @pierrejeambrun
- [ ] [Reduce eyestrain in dark mode with reduced contrast and saturation (#… (#42583)](https://github.com/apache/airflow/pull/42583): @pierrejeambrun
- [ ] [Support of host.name in OTEL metrics and usage of OTEL_RESOURCE_ATTRI… (#42604)](https://github.com/apache/airflow/pull/42604): @potiuk
- [ ] [Prevent redirect loop on /home with tags/lastrun filters (#42607) (#42609) (#42628)](https://github.com/apache/airflow/pull/42628): @jmaicher @jscheffl
     Linked issues:
     - [Prevent redirect loop on /home with tags/lastrun filters (#42607) (#42609)](https://github.com/apache/airflow/pull/42609)
- [ ] [Limit branches for pull request target workflow (#42635)](https://github.com/apache/airflow/pull/42635): @potiuk
- [ ] [[BACKPORT] Add retry logic in the scheduler for updating trigger timeouts in case of deadlocks. (#41429) (#42651)](https://github.com/apache/airflow/pull/42651): @shahar1 @TakawaAkirayo
     Linked issues:
     - [Add retry logic in the scheduler for updating trigger timeouts in case of deadlocks. (#41429)](https://github.com/apache/airflow/pull/41429)
- [ ] [Correctly select task in DAG Graph View when clicking on its name (#42697)](https://github.com/apache/airflow/pull/42697): @bbovenzi @jonhspyro
     Linked issues:
     - [Correctly select task in DAG Graph View when clicking on its name (#38782)](https://github.com/apache/airflow/pull/38782)
- [ ] [ fix(datasets/managers): fix error handling file loc when dataset alias resolved into new datasets (#42733)](https://github.com/apache/airflow/pull/42733): @uranusjr @Lee-W
     Linked issues:
     - [DAG priority parsing request is not inserted correctly on Postgres (#42704)](https://github.com/apache/airflow/issues/42704)
- [ ] [Fix dag warning documentation (#42858) (#42888)](https://github.com/apache/airflow/pull/42888): @pierrejeambrun
- [ ] [Deprecate session auth backend (backport) (#42911)](https://github.com/apache/airflow/pull/42911): @vincbeck
     Linked issues:
     - [Deprecate session auth backend (#42909)](https://github.com/apache/airflow/pull/42909)
- [ ] [Improving validation of task retries to handle None values (#42532) (#42915)](https://github.com/apache/airflow/pull/42915): @brightview4578 @jscheffl
     Linked issues:
     - [Improving validation of task retries to handle None values (#42532)](https://github.com/apache/airflow/pull/42532)
- [ ] [Remove the referrer from Webserver to Scarf (#42901) (#42942)](https://github.com/apache/airflow/pull/42942): @kaxil
- [ ] [Removed unicodecsv dependency for providers with Airflow version 2.8.… (#42970)](https://github.com/apache/airflow/pull/42970): @potiuk
- [ ] [Restrict looker-sdk version 24.18.0 and microsoft-kiota-http 1.3.4 (#… (#42977)](https://github.com/apache/airflow/pull/42977): @potiuk
- [ ] [Fix canary build test test_cli_internal_api_background process termination #42781 (#42983)](https://github.com/apache/airflow/pull/42983): @gopidesupavan
- [ ] [Fix PythonOperator when DAG has hyphen in name (#42993)](https://github.com/apache/airflow/pull/42993): @k-slash @jason810496
     Linked issues:
     - [PythonVirtualenvOperator - SyntaxError: cannot assign to operator (#42796)](https://github.com/apache/airflow/issues/42796)
- [ ] [Update json schema pre-commit to have draft7 schema in file (#43005) (#43007)](https://github.com/apache/airflow/pull/43007): @pierrejeambrun
- [ ] [Flush the session between writing and deletion of RTIF (#42928) (#43012)](https://github.com/apache/airflow/pull/43012): @ephraimbuddy
- [ ] [Disable flaky mssql based integration tests (#42811) (#43016)](https://github.com/apache/airflow/pull/43016): @gopidesupavan
- [ ] [Improve startup of K8S tests (#42721) (#43025)](https://github.com/apache/airflow/pull/43025): @gopidesupavan
- [ ] [update k8s tests urllib3 retry config status_forcelist and allowed_me… (#43026)](https://github.com/apache/airflow/pull/43026): @gopidesupavan
- [ ] [increase backoff_factor and add try/catch in k8s tests (#42940) (#43030)](https://github.com/apache/airflow/pull/43030): @gopidesupavan
- [ ] [Add retry on error 502 and 504 (#42994) (#43044)](https://github.com/apache/airflow/pull/43044): @majorosdonat @jscheffl
     Linked issues:
     - [Add retry on error 502 and 504 (#42994)](https://github.com/apache/airflow/pull/42994)
- [ ] [[Backport] Remove zombie from executor (#43065)](https://github.com/apache/airflow/pull/43065): @uranusjr
- [ ] [Added task_instance_mutation_hook for mapped operator index 0 (#42661) (#43089)](https://github.com/apache/airflow/pull/43089): @jscheffl @AutomationDev85
     Linked issues:
     - [Added task_instance_mutation_hook for mapped operator index 0 (#42661)](https://github.com/apache/airflow/pull/42661)
- [ ] [ AIP-69: Breeze adjustments for introduction of Edge Executor (#41731) (#43139)](https://github.com/apache/airflow/pull/43139): @jscheffl
     Linked issues:
     - [AIP-69: Breeze adjustments for introduction of Edge Executor (#41731)](https://github.com/apache/airflow/pull/41731)
- [ ] [[Backport] BashOperator: Execute templated bash script as file (#43191)](https://github.com/apache/airflow/pull/43191): @Joffreybvn
     Linked issues:
     - [BashOperator: Execute templated bash script as file (#42783)](https://github.com/apache/airflow/pull/42783)
- [ ] [Do not fail the build if only trove-classifiers change (#43236) (#43237)](https://github.com/apache/airflow/pull/43237): @potiuk
- [ ] [Mark sometimes failing heartbeat test and view test as flaky (#43250) (#43257)](https://github.com/apache/airflow/pull/43257): @potiuk
- [ ] [Skip example importability tests for providers in non-main branches (… (#43263)](https://github.com/apache/airflow/pull/43263): @potiuk
- [ ] [Better handling masking of values of set variable  (#43123) (#43278)](https://github.com/apache/airflow/pull/43278): @potiuk
- [ ] [Fix edge-case when conflicting constraints prevent k8s env creation (… (#43298)](https://github.com/apache/airflow/pull/43298): @potiuk
     Linked issues:
     - [Make Helm artifacts reproducible (#36930)](https://github.com/apache/airflow/pull/36930)
- [ ] [fix schedule_downstream_tasks bug (#42582) (#43299)](https://github.com/apache/airflow/pull/43299): @potiuk
- [ ] [Fixes behaviour of example dag tests for main/other branches (#43273) (#43307)](https://github.com/apache/airflow/pull/43307): @potiuk @ashb
     Linked issues:
     - [Split providers out of the main ""airflow/"" tree into a UV workspace project (#42505)](https://github.com/apache/airflow/pull/42505)
     - [Skip example importability tests for providers in non-main branches (#43260)](https://github.com/apache/airflow/pull/43260)
- [ ] [Check python version that was used to install pre-commit venvs (#43282) (#43310)](https://github.com/apache/airflow/pull/43310): @potiuk
- [ ] [Add isolation mode exclusion for mapped operator test (#43297) (#43311)](https://github.com/apache/airflow/pull/43311): @potiuk
- [ ] [Backport: Bump Flask-AppBuilder to ``4.5.2`` (#43309) (#43318)](https://github.com/apache/airflow/pull/43318): @kaxil
- [ ] [Masking configuration values irrelevant to DAG author (#43040) (#43336)](https://github.com/apache/airflow/pull/43336): @potiuk
- [ ] [Suppress warnings when masking sensitive confs (#43335) (#43337)](https://github.com/apache/airflow/pull/43337): @potiuk
- [ ] [Backport: Remove Scarf analytics from Airflow Webserver (#43346) (#43348)](https://github.com/apache/airflow/pull/43348): @kaxil
- [ ] [ Resolve warning in Dataset Alias migration (#43425)](https://github.com/apache/airflow/pull/43425): @Lee-W @kaxil
     Linked issues:
     - [Resolve warning in Dataset Alias migration (#43421)](https://github.com/apache/airflow/pull/43421)
- [ ] [Ensure total_entries in /api/v1/dags (#43377) (#43429)](https://github.com/apache/airflow/pull/43429): @pierrejeambrun
- [ ] [include limit and offset in request body schema for List task instances (batch) endpoint (#43479)](https://github.com/apache/airflow/pull/43479): @rawwar
- [ ] [Fix broken stat scheduler_loop_duration (#42886) (#43544)](https://github.com/apache/airflow/pull/43544): @potiuk
- [ ] [Fix TrySelector for Mapped Tasks in Logs and Details Grid Panel (#43565) Backport (#43566)](https://github.com/apache/airflow/pull/43566): @jscheffl
- [ ] [Conditionally add OTEL events when processing executor events (#43558) (#43567)](https://github.com/apache/airflow/pull/43567): @jedcunningham
     Linked issues:
     - [Conditionally add OTEL events when processing executor events (#43558)](https://github.com/apache/airflow/pull/43558)
- [ ] [Mark all tasks as skipped when failing a dag_run manually including t… (#43572)](https://github.com/apache/airflow/pull/43572): @utkarsharma2
- [ ] [ FIX: Don't raise a warning in ExecutorSafeguard when execute is called from an extended operator (#42849) Backport (#43577)](https://github.com/apache/airflow/pull/43577): @dabla @jscheffl
     Linked issues:
     - [FIX: Don't raise a warning in ExecutorSafeguard when execute is called from an extended operator (#42849)](https://github.com/apache/airflow/pull/42849)


Thanks to all who contributed to the release (probably not a complete list!):
@uranusjr @howardyoo @majorosdonat @ashb @rawwar @jonhspyro @kaxil @potiuk @pierrejeambrun @Lee-W @bbovenzi @ephraimbuddy @AutomationDev85 @vincbeck @jscheffl @k-slash @jason810496 @shahar1 @jedcunningham @romsharon98 @dabla @jmaicher @gopidesupavan @utkarsharma2 @TakawaAkirayo @josephangbc @brightview4578 @Joffreybvn

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",utkarsharma2,2024-11-01 13:25:53+00:00,[],2024-11-01 13:58:01+00:00,2024-11-01 13:58:01+00:00,https://github.com/apache/airflow/issues/43585,"[('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2451917682, 'issue_id': 2629032769, 'author': 'utkarsharma2', 'body': 'Updated the [old issue](https://github.com/apache/airflow/issues/43441) to retain the already-tested PR and comments.', 'created_at': datetime.datetime(2024, 11, 1, 13, 58, 1, tzinfo=datetime.timezone.utc)}]","utkarsharma2 (Issue Creator) on (2024-11-01 13:58:01 UTC): Updated the [old issue](https://github.com/apache/airflow/issues/43441) to retain the already-tested PR and comments.

"
2629007234,issue,closed,completed,Move execution time code (much of LocalTaskJob) over to Task SDK,Part of AIP-72,kaxil,2024-11-01 13:11:04+00:00,['ashb'],2024-11-29 18:33:38+00:00,2024-11-29 18:33:38+00:00,https://github.com/apache/airflow/issues/43584,"[('area:core', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]","[{'comment_id': 2477614507, 'issue_id': 2629007234, 'author': 'kaxil', 'body': 'A big PR was merged today: https://github.com/apache/airflow/pull/43893', 'created_at': datetime.datetime(2024, 11, 14, 23, 26, 49, tzinfo=datetime.timezone.utc)}]","kaxil (Issue Creator) on (2024-11-14 23:26:49 UTC): A big PR was merged today: https://github.com/apache/airflow/pull/43893

"
2628945997,issue,open,,CJ job to bump provider to RC versions in constraints of RC Airflow,"Sometimes we have a neeed to use RC provider version in RC candidate of airflow when the provider has not been released yet but we want to use it regardless. This happened for example with 2.10.3rc2 which would be best if installed wth 1.5.0rc2 of apache-airflow-providers-fab.

This could be automated as a separate CI, manually triggerable job that could upate the constraints - similar to the ""release image"" joib we have for release manager.

This job should look as follows:

Three parameters:

airflow version: x.y.zrcn
provider name; PROVIDER
provider version to use: X.Y.ZrcN

* find `constraints-x.y.zrcn` tag
* branch off from that tag and create a branch `constraints-x.y.zrcn-fix` (or use it if already there)
* modify, commit and push `apache-airlfow-providers-PROVIDER` lines to use `==X.Y.ZrcN` in all constraint files in this tag
* move tag the new commit wtih `constraints-x.y.zrcn`

",potiuk,2024-11-01 12:29:42+00:00,[],2024-11-03 09:44:46+00:00,,https://github.com/apache/airflow/issues/43583,"[('area:CI', ""Airflow's tests and continious integration"")]","[{'comment_id': 2453339200, 'issue_id': 2628945997, 'author': 'eladkal', 'body': ""Don't we need to run the dependency to make sure it actually works?\r\nManually changing the line in the constraint file will work only if the dependencies of previous version agrees with the new version."", 'created_at': datetime.datetime(2024, 11, 3, 9, 16, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453346178, 'issue_id': 2628945997, 'author': 'potiuk', 'body': ""That's for exceptional cases when they differ enough (or are == for example). But we could writte this job to do full dependency resolution as well."", 'created_at': datetime.datetime(2024, 11, 3, 9, 44, 45, tzinfo=datetime.timezone.utc)}]","eladkal on (2024-11-03 09:16:49 UTC): Don't we need to run the dependency to make sure it actually works?
Manually changing the line in the constraint file will work only if the dependencies of previous version agrees with the new version.

potiuk (Issue Creator) on (2024-11-03 09:44:45 UTC): That's for exceptional cases when they differ enough (or are == for example). But we could writte this job to do full dependency resolution as well.

"
2628888596,issue,open,,Automatically import move standard operators / hooks etc with deprecation from old location,"### Body

For compatibility reason we wanted to ""redirect"" all the moved ""airflow.operators"" , ""airflow.hooks"" etc. to ""standard"" provider in Airflow 3 - with deprectation warning.

In the past we were doing it with our deprecation include but @ashb mentioned that he found another way. @gopidesupavan - that would be a follow up after #42081 and #42252 .

@ashb -> can you please explain the mechanism you found?

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",potiuk,2024-11-01 11:49:53+00:00,[],2024-11-01 20:37:52+00:00,,https://github.com/apache/airflow/issues/43582,"[('kind:meta', 'High-level information important to the community'), ('area:core', '')]","[{'comment_id': 2451781314, 'issue_id': 2628888596, 'author': 'gopidesupavan', 'body': 'Oh yeah that would helpful :).\r\n\r\nWhen this discussion began a while ago, i heard from the discussion using meta path, then I experimented with using this to redirect old imports to new ones.\r\n\r\nash please suggest your approach. \r\n\r\n```\r\nimport sys\r\nimport importlib\r\n\r\nclass ModuleRedirectSpec:\r\n    def __init__(self, redirects):\r\n        self.redirects = redirects\r\n\r\n    def find_spec(self, old_module_ref, path, target=None):\r\n        if old_module_ref in self.redirects:\r\n            new_module_name = self.redirects[old_module_ref]\r\n            return importlib.util.find_spec(new_module_name)\r\n        return None\r\n\r\nredirects = {\r\n    ""airflow.operators.bash"": ""airflow.providers.standard.operators.bash""\r\n}\r\nsys.meta_path.insert(0, ModuleRedirectSpec(redirects))\r\n```', 'created_at': datetime.datetime(2024, 11, 1, 12, 16, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452555722, 'issue_id': 2628888596, 'author': 'potiuk', 'body': ""And let's not forget deprecations :)"", 'created_at': datetime.datetime(2024, 11, 1, 20, 37, 51, tzinfo=datetime.timezone.utc)}]","gopidesupavan on (2024-11-01 12:16:54 UTC): Oh yeah that would helpful :).

When this discussion began a while ago, i heard from the discussion using meta path, then I experimented with using this to redirect old imports to new ones.

ash please suggest your approach. 

```
import sys
import importlib

class ModuleRedirectSpec:
    def __init__(self, redirects):
        self.redirects = redirects

    def find_spec(self, old_module_ref, path, target=None):
        if old_module_ref in self.redirects:
            new_module_name = self.redirects[old_module_ref]
            return importlib.util.find_spec(new_module_name)
        return None

redirects = {
    ""airflow.operators.bash"": ""airflow.providers.standard.operators.bash""
}
sys.meta_path.insert(0, ModuleRedirectSpec(redirects))
```

potiuk (Issue Creator) on (2024-11-01 20:37:51 UTC): And let's not forget deprecations :)

"
2628877853,issue,open,,AIP-72: Ensure `from airflow import DAG` works,"We need to think about how we get the `from airflow import DAG` to work with the task SDK etc.
And what the canonical import path we want users to be (I think it's either `from airflow` or `from airflow.sdk`) ",kaxil,2024-11-01 11:42:40+00:00,[],2025-01-17 15:24:16+00:00,,https://github.com/apache/airflow/issues/43581,"[('area:core', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2628156504,issue,closed,completed,ConsumeFromTopicOperator does not fail even if wrong credentials are given,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I created a kafka connection with the dummy values and yet the Operator succeeded with the following task log:

```
[2024-11-01, 07:35:06 IST] {local_task_job_runner.py:123} ▼ Pre task execution logs
[2024-11-01, 07:35:06 IST] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: kafka_issue.consume_task manual__2024-11-01T02:05:05.395642+00:00 [queued]>
[2024-11-01, 07:35:06 IST] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: kafka_issue.consume_task manual__2024-11-01T02:05:05.395642+00:00 [queued]>
[2024-11-01, 07:35:06 IST] {taskinstance.py:2866} INFO - Starting attempt 1 of 2
[2024-11-01, 07:35:06 IST] {taskinstance.py:2889} INFO - Executing <Task(ConsumeFromTopicOperator): consume_task> on 2024-11-01 02:05:05.395642+00:00
[2024-11-01, 07:35:06 IST] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'kafka_issue', 'consume_task', 'manual__2024-11-01T02:05:05.395642+00:00', '--job-id', '35', '--raw', '--subdir', 'DAGS_FOLDER/kafka_min.py', '--cfg-path', '/tmp/tmpflvhwk6b']
[2024-11-01, 07:35:06 IST] {standard_task_runner.py:105} INFO - Job 35: Subtask consume_task
[2024-11-01, 07:35:06 IST] {logging_mixin.py:190} WARNING - /usr/local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py:70 DeprecationWarning: This process (pid=217) is multi-threaded, use of fork() may lead to deadlocks in the child.
[2024-11-01, 07:35:06 IST] {standard_task_runner.py:72} INFO - Started process 218 to run task
[2024-11-01, 07:35:06 IST] {task_command.py:467} INFO - Running <TaskInstance: kafka_issue.consume_task manual__2024-11-01T02:05:05.395642+00:00 [running]> on host 65fc791ee8f4
[2024-11-01, 07:35:06 IST] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='kafka_issue' AIRFLOW_CTX_TASK_ID='consume_task' AIRFLOW_CTX_EXECUTION_DATE='2024-11-01T02:05:05.395642+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-11-01T02:05:05.395642+00:00'
[2024-11-01, 07:35:06 IST] {taskinstance.py:731} ▲▲▲ Log group end
[2024-11-01, 07:35:06 IST] {base.py:84} INFO - Retrieving connection 'kafka_connection'
[2024-11-01, 07:36:06 IST] {consume.py:167} INFO - Reached end of log. Exiting.
[2024-11-01, 07:36:06 IST] {consume.py:182} INFO - committing offset at end_of_batch
[2024-11-01, 07:36:06 IST] {taskinstance.py:340} ▼ Post task execution logs
[2024-11-01, 07:36:06 IST] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=kafka_issue, task_id=consume_task, run_id=manual__2024-11-01T02:05:05.395642+00:00, execution_date=20241101T020505, start_date=20241101T020506, end_date=20241101T020606
[2024-11-01, 07:36:06 IST] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2024-11-01, 07:36:06 IST] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-11-01, 07:36:06 IST] {local_task_job_runner.py:245} ▲▲▲ Log group end
```


### What you think should happen instead?

When credentials are wrong, the operator should fail

### How to reproduce

Connection I used:
```
{
  ""bootstrap.servers"": ""hello.com:9092"",
  ""group.id"": ""ea"",
  ""auto.offset.reset"": ""earliest"",
  ""sasl.mechanism"": ""PLAIN"",
  ""sasl.username"": ""ewfew"",
  ""sasl.password"": ""Owefw""
}
```

I used the following DAG:

```
from airflow import DAG
from airflow.operators.dummy_operator import DummyOperator
from airflow.providers.apache.kafka.operators.consume import ConsumeFromTopicOperator
from datetime import datetime, timedelta
import json

KAFKA_TOPIC = ""random""

def print_msg(msg):
    print(msg)


default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

with DAG('kafka_issue',
         start_date=datetime(2024, 1, 20),
         max_active_runs=1,
         schedule_interval=None,
         default_args=default_args,
         ) as dag:

    consumer_topic = ConsumeFromTopicOperator(
        task_id=""consume_task"",
        kafka_config_id=""kafka_connection"",        
        topics=[KAFKA_TOPIC],
        apply_function=""kafka_min.print_msg"",
        commit_cadence=""end_of_batch"",
        max_messages=5,
        max_batch_size=2,
    )
    testconsume = DummyOperator(
        task_id='test_consume_kafka'
    )


    testconsume >> consumer_topic 
```

to reproduce the issue, please name your dag file `kafka_min` or update the apply_function accordingly

### Operating System

MacOS - 15.0.1 (24A348)

### Versions of Apache Airflow Providers

confluent-kafka
apache-airflow-providers-apache-kafka

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rawwar,2024-11-01 02:10:15+00:00,[],2024-12-08 22:17:24+00:00,2024-12-08 22:17:23+00:00,https://github.com/apache/airflow/issues/43569,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('provider:apache-kafka', ''), ('mans-hackathon', '')]","[{'comment_id': 2466415572, 'issue_id': 2628156504, 'author': 'VikSil', 'body': 'We were able to reproduce the issue with guidance from @potiuk during Man\'s hackathon. The environment was set up with Breeze integration to Kafka, and the environment was started by running:\r\n\r\n    breeze start-airflow --integration kafka --load-example-dags --load-default-connections\r\n\r\nWe also, tried to add Kafka credentials in `/airflow/scripts/ci/docker-compose/integration-kafka.yml` file.\r\nThere appears to be two ways to do it:\r\n\r\n1. as suggested [here](https://docs.mia-platform.eu/docs/fast_data/bucket_storage_support/configuration/kafka_connection_configuration):\r\n          KAFKA_USERNAME: \'admin\'\r\n          KAFKA_PASSWORD: \'secret\'\r\n          KAFKA_SASL_MECHANISM: SCRAM-SHA-256\r\n2. as suggested [here](https://docs.informatica.com/integration-cloud/data-integration-connectors/current-version/kafka-connector/kafka-connections/configuring-sasl-plain-authentication-for-a-kafka-broker.html):\r\n          KAFKA_SASL_MECHANISM: PLAIN\r\n          KAFKA_SASL_JAAS_CONFIG: ""org.apache.kafka.common.security.plain.PlainLoginModule required username=\\""kafka-user\\"" password=\\""kafka-password\\"";""\r\n\r\nWith either configuration the Kafka container starts, up and Airflow marks the `ConsumeFromTopicOperator` task as `success`.\r\n\r\nFurthermore, we noticed that the connection to bootstrap server ""hello.com:9092"" that we used does not lead anywhere. When entered into the browser the lookup lasts forever. `https://hello.com/` is a project that has been discontinued.', 'created_at': datetime.datetime(2024, 11, 9, 19, 12, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466420818, 'issue_id': 2628156504, 'author': 'SuccessMoses', 'body': '@VikSil are you working on this?', 'created_at': datetime.datetime(2024, 11, 9, 19, 30, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2467674227, 'issue_id': 2628156504, 'author': 'potiuk', 'body': 'Hey @dylanbstorey  and maybe @aritra24  -> we\'ve been working with @KS0107 @VikSil over the weekend during the Man\'s hackathon trying to reproduce that one and test it and we have some interesting findings, and maybe you will be able to take a look and confirm them or help to guide the rest of the team (And I see @SuccessMoses is interested as well) - to nail and fix that one.\r\n\r\nThe first problem - we had a bit of hard time to see what\'s going on in our integration tests when we attempted to introduce user/password and tried to configure our ""integration"" Kafka compose setup to have authentication enabled.\r\n\r\nNo matted what we did, it seems that some of the settings - for example bootstrap server, did not really matter as if they were either not used (or maybe we did not understnd how it works).  But indeed it seems that the original error report is correct and Kafka consumer does not fail when wrong credentials are used.\r\n\r\nAlso @mrk-andreev  had some comments on it- about the bootstrap server - but  am unforutnately not to well wersed with Kafka. So @dylanbstorey @aritra24 -> maybe you cn take a close look and give some thoughts about it?', 'created_at': datetime.datetime(2024, 11, 11, 9, 40, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2467722806, 'issue_id': 2628156504, 'author': 'aritra24', 'body': 'I can spend some time looking at this later today, let me get back to you in a few hours. 🤔', 'created_at': datetime.datetime(2024, 11, 11, 10, 1, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2467739469, 'issue_id': 2628156504, 'author': 'rawwar', 'body': '@aritra24 , consumer config also takes `error_cb`. I was able to make Consumer fail by using an `error_cb`. Consider looking into this direction, if there is no better way\r\n\r\n```\r\nclass KafkaAuthenticationError(Exception):\r\n    """"""Custom exception for Kafka authentication failures""""""\r\n    pass\r\n\r\ndef error_callback(err):\r\n    """"""Callback function to handle Kafka errors""""""\r\n    if err.code() == KafkaError._AUTHENTICATION:\r\n        raise KafkaAuthenticationError(f""Authentication failed: {err}"")\r\n    print(""Exception received: "", err)\r\n```', 'created_at': datetime.datetime(2024, 11, 11, 10, 7, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2468749868, 'issue_id': 2628156504, 'author': 'aritra24', 'body': 'Are you seeing this on main? Or a specific version of airflow?', 'created_at': datetime.datetime(2024, 11, 11, 18, 2, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2468977432, 'issue_id': 2628156504, 'author': 'VikSil', 'body': '@aritra24 I had the main branch cloned.', 'created_at': datetime.datetime(2024, 11, 11, 20, 34, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2476511458, 'issue_id': 2628156504, 'author': 'dylanbstorey', 'body': ""IIRC - much of the library used doesn't require an external kafka service to be available when the code is interpretted so by design I believe its silently failing and needs the implementation of callbacks to take action on those interfaces/activities. rawwar's comment is the correct line of inquiry for changing default behavior and escalation of failures through the provider."", 'created_at': datetime.datetime(2024, 11, 14, 14, 34, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2495488848, 'issue_id': 2628156504, 'author': 'SuccessMoses', 'body': '@rawwar I created a PR to try to fix this issue, but it is not complete yet. I wanted to hear your thoughts.\r\n\r\nI managed to make task fail using error_cb in `Consumer`.\r\n\r\n```\r\n ▶ Log message source details\r\n[2024-11-23, 13:24:36 UTC] {local_task_job_runner.py:121} ▼ Pre task execution logs\r\n[2024-11-23, 13:24:36 UTC] {taskinstance.py:2403} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: kafka_issue.consume_task manual__2024-11-23T13:24:25.243056+00:00 [queued]>\r\n[2024-11-23, 13:24:36 UTC] {taskinstance.py:2403} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: kafka_issue.consume_task manual__2024-11-23T13:24:25.243056+00:00 [queued]>\r\n[2024-11-23, 13:24:36 UTC] {taskinstance.py:2654} INFO - Starting attempt 1 of 2\r\n[2024-11-23, 13:24:36 UTC] {taskinstance.py:2677} INFO - Executing <Task(ConsumeFromTopicOperator): consume_task> on 2024-11-23 13:24:25.243056+00:00\r\n[2024-11-23, 13:24:36 UTC] {standard_task_runner.py:131} INFO - Started process 15454 to run task\r\n[2024-11-23, 13:24:36 UTC] {standard_task_runner.py:160} INFO - Running: [\'airflow\', \'tasks\', \'run\', \'kafka_issue\', \'consume_task\', \'manual__2024-11-23T13:24:25.243056+00:00\', \'--raw\', \'--subdir\', \'DAGS_FOLDER/kafka_min.py\', \'--cfg-path\', \'/tmp/tmpmivb_e_5\']\r\n[2024-11-23, 13:24:36 UTC] {standard_task_runner.py:161} INFO - Subtask consume_task\r\n[2024-11-23, 13:24:36 UTC] {task_command.py:446} INFO - Running <TaskInstance: kafka_issue.consume_task manual__2024-11-23T13:24:25.243056+00:00 [running]> on host 2c7e65d643ac\r\n[2024-11-23, 13:24:36 UTC] {taskinstance.py:2910} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER=\'airflow\' AIRFLOW_CTX_DAG_ID=\'kafka_issue\' AIRFLOW_CTX_TASK_ID=\'consume_task\' AIRFLOW_CTX_LOGICAL_DATE=\'2024-11-23T13:24:25.243056+00:00\' AIRFLOW_CTX_TRY_NUMBER=\'1\' AIRFLOW_CTX_DAG_RUN_ID=\'manual__2024-11-23T13:24:25.243056+00:00\'\r\n[2024-11-23, 13:24:36 UTC] {logging_mixin.py:191} INFO - Task instance is in running state\r\n[2024-11-23, 13:24:36 UTC] {logging_mixin.py:191} INFO -  Previous state of the Task instance: queued\r\n[2024-11-23, 13:24:36 UTC] {logging_mixin.py:191} INFO - Current task name:consume_task state:running start_date:2024-11-23 13:24:36.673356+00:00\r\n[2024-11-23, 13:24:36 UTC] {logging_mixin.py:191} INFO - Dag name:kafka_issue and current dag run status:running\r\n[2024-11-23, 13:24:36 UTC] {taskinstance.py:723} ▲▲▲ Log group end\r\n[2024-11-23, 13:24:36 UTC] {base.py:66} INFO - Retrieving connection \'kafka_connection\'\r\n[2024-11-23, 13:25:07 UTC] {logging_mixin.py:191} INFO - Exception received:  KafkaError{code=_TRANSPORT,val=-195,str=""hello.com:9092/bootstrap: Connection setup timed out in state CONNECT (after 30403ms in state CONNECT)""}\r\n[2024-11-23, 13:25:36 UTC] {taskinstance.py:3097} ERROR - Task failed with exception\r\nTraceback (most recent call last):\r\n  File ""/opt/airflow/airflow/models/taskinstance.py"", line 759, in _execute_task\r\n    result = _execute_callable(context=context, **execute_callable_kwargs)\r\n  File ""/opt/airflow/airflow/models/taskinstance.py"", line 725, in _execute_callable\r\n    return ExecutionCallableRunner(\r\n  File ""/opt/airflow/airflow/utils/operator_helpers.py"", line 268, in run\r\n    return func(*args, **kwargs)\r\n  File ""/opt/airflow/airflow/models/baseoperator.py"", line 375, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File ""/opt/airflow/providers/src/airflow/providers/apache/kafka/operators/consume.py"", line 162, in execute\r\n    msgs = consumer.consume(num_messages=batch_size, timeout=self.poll_timeout)\r\n  File ""/opt/airflow/providers/src/airflow/providers/apache/kafka/hooks/consume.py"", line 31, in error_callback\r\n    print(""Exception received: "", err)\r\n  File ""/opt/airflow/providers/src/airflow/providers/apache/kafka/hooks/consume.py"", line 32, in error_callback\r\n    raise KafkaAuthenticationError(f""Authentication failed: {err}"")\r\nairflow.providers.apache.kafka.hooks.consume.KafkaAuthenticationError: Authentication failed: KafkaError{code=_TRANSPORT,val=-195,str=""hello.com:9092/bootstrap: Connection setup timed out in state CONNECT (after 30403ms in state CONNECT)""}\r\n[2024-11-23, 13:25:36 UTC] {logging_mixin.py:191} INFO - Task instance in failure state\r\n[2024-11-23, 13:25:36 UTC] {logging_mixin.py:191} INFO - Task start:2024-11-23 13:24:36.673356+00:00 end:2024-11-23 13:25:36.809691+00:00 duration:60.136335\r\n[2024-11-23, 13:25:36 UTC] {logging_mixin.py:191} INFO - Task:<Task(ConsumeFromTopicOperator): consume_task> dag:<DAG: kafka_issue> dagrun:<DagRun kafka_issue @ 2024-11-23 13:24:25.243056+00:00: manual__2024-11-23T13:24:25.243056+00:00, state:running, queued_at: 2024-11-23 13:24:25.252615+00:00. externally triggered: True>\r\n[2024-11-23, 13:25:36 UTC] {logging_mixin.py:191} INFO - Failure caused by Authentication failed: KafkaError{code=_TRANSPORT,val=-195,str=""hello.com:9092/bootstrap: Connection setup timed out in state CONNECT (after 30403ms in state CONNECT)""}\r\n[2024-11-23, 13:25:36 UTC] {taskinstance.py:1139} INFO - Marking task as UP_FOR_RETRY. dag_id=kafka_issue, task_id=consume_task, run_id=manual__2024-11-23T13:24:25.243056+00:00, logical_date=20241123T132425, start_date=20241123T132436, end_date=20241123T132536\r\n[2024-11-23, 13:25:36 UTC] {taskinstance.py:346} ▼ Post task execution logs\r\n[2024-11-23, 13:25:36 UTC] {standard_task_runner.py:178} ERROR - Failed to execute task_id=consume_task pid=15454\r\nTraceback (most recent call last):\r\n  File ""/opt/airflow/airflow/task/standard_task_runner.py"", line 171, in _start_by_fork\r\n    ret = args.func(args, dag=self.dag)\r\n  File ""/opt/airflow/airflow/cli/cli_config.py"", line 49, in command\r\n    return func(*args, **kwargs)\r\n  File ""/opt/airflow/airflow/utils/cli.py"", line 112, in wrapper\r\n    return f(*args, **kwargs)\r\n  File ""/opt/airflow/airflow/cli/commands/task_command.py"", line 462, in task_run\r\n    task_return_code = _run_task_by_selected_method(args, _dag, ti)\r\n  File ""/opt/airflow/airflow/cli/commands/task_command.py"", line 257, in _run_task_by_selected_method\r\n    return _run_raw_task(args, ti)\r\n  File ""/opt/airflow/airflow/cli/commands/task_command.py"", line 325, in _run_raw_task\r\n    return ti._run_raw_task(\r\n  File ""/opt/airflow/airflow/utils/session.py"", line 97, in wrapper\r\n    return func(*args, session=session, **kwargs)\r\n  File ""/opt/airflow/airflow/models/taskinstance.py"", line 2790, in _run_raw_task\r\n    return _run_raw_task(\r\n  File ""/opt/airflow/airflow/models/taskinstance.py"", line 279, in _run_raw_task\r\n    TaskInstance._execute_task_with_callbacks(\r\n  File ""/opt/airflow/airflow/models/taskinstance.py"", line 2937, in _execute_task_with_callbacks\r\n    result = self._execute_task(context, task_orig)\r\n  File ""/opt/airflow/airflow/models/taskinstance.py"", line 2961, in _execute_task\r\n    return _execute_task(self, context, task_orig)\r\n  File ""/opt/airflow/airflow/models/taskinstance.py"", line 759, in _execute_task\r\n    result = _execute_callable(context=context, **execute_callable_kwargs)\r\n  File ""/opt/airflow/airflow/models/taskinstance.py"", line 725, in _execute_callable\r\n    return ExecutionCallableRunner(\r\n  File ""/opt/airflow/airflow/utils/operator_helpers.py"", line 268, in run\r\n    return func(*args, **kwargs)\r\n  File ""/opt/airflow/airflow/models/baseoperator.py"", line 375, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File ""/opt/airflow/providers/src/airflow/providers/apache/kafka/operators/consume.py"", line 162, in execute\r\n    msgs = consumer.consume(num_messages=batch_size, timeout=self.poll_timeout)\r\n  File ""/opt/airflow/providers/src/airflow/providers/apache/kafka/hooks/consume.py"", line 31, in error_callback\r\n    print(""Exception received: "", err)\r\n  File ""/opt/airflow/providers/src/airflow/providers/apache/kafka/hooks/consume.py"", line 32, in error_callback\r\n    raise KafkaAuthenticationError(f""Authentication failed: {err}"")\r\nairflow.providers.apache.kafka.hooks.consume.KafkaAuthenticationError: Authentication failed: KafkaError{code=_TRANSPORT,val=-195,str=""hello.com:9092/bootstrap: Connection setup timed out in state CONNECT (after 30403ms in state CONNECT)""}\r\n[2024-11-23, 13:25:36 UTC] {local_task_job_runner.py:263} INFO - Task exited with return code 1\r\n[2024-11-23, 13:25:36 UTC] {local_task_job_runner.py:242} ▲▲▲ Log group end\r\n```\r\n\r\nThis is the connection I used:\r\n\r\n```\r\n{\r\n  ""bootstrap.servers"": ""hello.com:9092"",\r\n  ""group.id"": ""ea""\r\n}\r\n```', 'created_at': datetime.datetime(2024, 11, 23, 13, 59, 38, tzinfo=datetime.timezone.utc)}]","VikSil on (2024-11-09 19:12:44 UTC): We were able to reproduce the issue with guidance from @potiuk during Man's hackathon. The environment was set up with Breeze integration to Kafka, and the environment was started by running:

    breeze start-airflow --integration kafka --load-example-dags --load-default-connections

We also, tried to add Kafka credentials in `/airflow/scripts/ci/docker-compose/integration-kafka.yml` file.
There appears to be two ways to do it:

1. as suggested [here](https://docs.mia-platform.eu/docs/fast_data/bucket_storage_support/configuration/kafka_connection_configuration):
          KAFKA_USERNAME: 'admin'
          KAFKA_PASSWORD: 'secret'
          KAFKA_SASL_MECHANISM: SCRAM-SHA-256
2. as suggested [here](https://docs.informatica.com/integration-cloud/data-integration-connectors/current-version/kafka-connector/kafka-connections/configuring-sasl-plain-authentication-for-a-kafka-broker.html):
          KAFKA_SASL_MECHANISM: PLAIN
          KAFKA_SASL_JAAS_CONFIG: ""org.apache.kafka.common.security.plain.PlainLoginModule required username=\""kafka-user\"" password=\""kafka-password\"";""

With either configuration the Kafka container starts, up and Airflow marks the `ConsumeFromTopicOperator` task as `success`.

Furthermore, we noticed that the connection to bootstrap server ""hello.com:9092"" that we used does not lead anywhere. When entered into the browser the lookup lasts forever. `https://hello.com/` is a project that has been discontinued.

SuccessMoses on (2024-11-09 19:30:44 UTC): @VikSil are you working on this?

potiuk on (2024-11-11 09:40:56 UTC): Hey @dylanbstorey  and maybe @aritra24  -> we've been working with @KS0107 @VikSil over the weekend during the Man's hackathon trying to reproduce that one and test it and we have some interesting findings, and maybe you will be able to take a look and confirm them or help to guide the rest of the team (And I see @SuccessMoses is interested as well) - to nail and fix that one.

The first problem - we had a bit of hard time to see what's going on in our integration tests when we attempted to introduce user/password and tried to configure our ""integration"" Kafka compose setup to have authentication enabled.

No matted what we did, it seems that some of the settings - for example bootstrap server, did not really matter as if they were either not used (or maybe we did not understnd how it works).  But indeed it seems that the original error report is correct and Kafka consumer does not fail when wrong credentials are used.

Also @mrk-andreev  had some comments on it- about the bootstrap server - but  am unforutnately not to well wersed with Kafka. So @dylanbstorey @aritra24 -> maybe you cn take a close look and give some thoughts about it?

aritra24 on (2024-11-11 10:01:11 UTC): I can spend some time looking at this later today, let me get back to you in a few hours. 🤔

rawwar (Issue Creator) on (2024-11-11 10:07:12 UTC): @aritra24 , consumer config also takes `error_cb`. I was able to make Consumer fail by using an `error_cb`. Consider looking into this direction, if there is no better way

```
class KafkaAuthenticationError(Exception):
    """"""Custom exception for Kafka authentication failures""""""
    pass

def error_callback(err):
    """"""Callback function to handle Kafka errors""""""
    if err.code() == KafkaError._AUTHENTICATION:
        raise KafkaAuthenticationError(f""Authentication failed: {err}"")
    print(""Exception received: "", err)
```

aritra24 on (2024-11-11 18:02:43 UTC): Are you seeing this on main? Or a specific version of airflow?

VikSil on (2024-11-11 20:34:19 UTC): @aritra24 I had the main branch cloned.

dylanbstorey on (2024-11-14 14:34:35 UTC): IIRC - much of the library used doesn't require an external kafka service to be available when the code is interpretted so by design I believe its silently failing and needs the implementation of callbacks to take action on those interfaces/activities. rawwar's comment is the correct line of inquiry for changing default behavior and escalation of failures through the provider.

SuccessMoses on (2024-11-23 13:59:38 UTC): @rawwar I created a PR to try to fix this issue, but it is not complete yet. I wanted to hear your thoughts.

I managed to make task fail using error_cb in `Consumer`.

```
 ▶ Log message source details
[2024-11-23, 13:24:36 UTC] {local_task_job_runner.py:121} ▼ Pre task execution logs
[2024-11-23, 13:24:36 UTC] {taskinstance.py:2403} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: kafka_issue.consume_task manual__2024-11-23T13:24:25.243056+00:00 [queued]>
[2024-11-23, 13:24:36 UTC] {taskinstance.py:2403} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: kafka_issue.consume_task manual__2024-11-23T13:24:25.243056+00:00 [queued]>
[2024-11-23, 13:24:36 UTC] {taskinstance.py:2654} INFO - Starting attempt 1 of 2
[2024-11-23, 13:24:36 UTC] {taskinstance.py:2677} INFO - Executing <Task(ConsumeFromTopicOperator): consume_task> on 2024-11-23 13:24:25.243056+00:00
[2024-11-23, 13:24:36 UTC] {standard_task_runner.py:131} INFO - Started process 15454 to run task
[2024-11-23, 13:24:36 UTC] {standard_task_runner.py:160} INFO - Running: ['airflow', 'tasks', 'run', 'kafka_issue', 'consume_task', 'manual__2024-11-23T13:24:25.243056+00:00', '--raw', '--subdir', 'DAGS_FOLDER/kafka_min.py', '--cfg-path', '/tmp/tmpmivb_e_5']
[2024-11-23, 13:24:36 UTC] {standard_task_runner.py:161} INFO - Subtask consume_task
[2024-11-23, 13:24:36 UTC] {task_command.py:446} INFO - Running <TaskInstance: kafka_issue.consume_task manual__2024-11-23T13:24:25.243056+00:00 [running]> on host 2c7e65d643ac
[2024-11-23, 13:24:36 UTC] {taskinstance.py:2910} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='kafka_issue' AIRFLOW_CTX_TASK_ID='consume_task' AIRFLOW_CTX_LOGICAL_DATE='2024-11-23T13:24:25.243056+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-11-23T13:24:25.243056+00:00'
[2024-11-23, 13:24:36 UTC] {logging_mixin.py:191} INFO - Task instance is in running state
[2024-11-23, 13:24:36 UTC] {logging_mixin.py:191} INFO -  Previous state of the Task instance: queued
[2024-11-23, 13:24:36 UTC] {logging_mixin.py:191} INFO - Current task name:consume_task state:running start_date:2024-11-23 13:24:36.673356+00:00
[2024-11-23, 13:24:36 UTC] {logging_mixin.py:191} INFO - Dag name:kafka_issue and current dag run status:running
[2024-11-23, 13:24:36 UTC] {taskinstance.py:723} ▲▲▲ Log group end
[2024-11-23, 13:24:36 UTC] {base.py:66} INFO - Retrieving connection 'kafka_connection'
[2024-11-23, 13:25:07 UTC] {logging_mixin.py:191} INFO - Exception received:  KafkaError{code=_TRANSPORT,val=-195,str=""hello.com:9092/bootstrap: Connection setup timed out in state CONNECT (after 30403ms in state CONNECT)""}
[2024-11-23, 13:25:36 UTC] {taskinstance.py:3097} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/opt/airflow/airflow/models/taskinstance.py"", line 759, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File ""/opt/airflow/airflow/models/taskinstance.py"", line 725, in _execute_callable
    return ExecutionCallableRunner(
  File ""/opt/airflow/airflow/utils/operator_helpers.py"", line 268, in run
    return func(*args, **kwargs)
  File ""/opt/airflow/airflow/models/baseoperator.py"", line 375, in wrapper
    return func(self, *args, **kwargs)
  File ""/opt/airflow/providers/src/airflow/providers/apache/kafka/operators/consume.py"", line 162, in execute
    msgs = consumer.consume(num_messages=batch_size, timeout=self.poll_timeout)
  File ""/opt/airflow/providers/src/airflow/providers/apache/kafka/hooks/consume.py"", line 31, in error_callback
    print(""Exception received: "", err)
  File ""/opt/airflow/providers/src/airflow/providers/apache/kafka/hooks/consume.py"", line 32, in error_callback
    raise KafkaAuthenticationError(f""Authentication failed: {err}"")
airflow.providers.apache.kafka.hooks.consume.KafkaAuthenticationError: Authentication failed: KafkaError{code=_TRANSPORT,val=-195,str=""hello.com:9092/bootstrap: Connection setup timed out in state CONNECT (after 30403ms in state CONNECT)""}
[2024-11-23, 13:25:36 UTC] {logging_mixin.py:191} INFO - Task instance in failure state
[2024-11-23, 13:25:36 UTC] {logging_mixin.py:191} INFO - Task start:2024-11-23 13:24:36.673356+00:00 end:2024-11-23 13:25:36.809691+00:00 duration:60.136335
[2024-11-23, 13:25:36 UTC] {logging_mixin.py:191} INFO - Task:<Task(ConsumeFromTopicOperator): consume_task> dag:<DAG: kafka_issue> dagrun:<DagRun kafka_issue @ 2024-11-23 13:24:25.243056+00:00: manual__2024-11-23T13:24:25.243056+00:00, state:running, queued_at: 2024-11-23 13:24:25.252615+00:00. externally triggered: True>
[2024-11-23, 13:25:36 UTC] {logging_mixin.py:191} INFO - Failure caused by Authentication failed: KafkaError{code=_TRANSPORT,val=-195,str=""hello.com:9092/bootstrap: Connection setup timed out in state CONNECT (after 30403ms in state CONNECT)""}
[2024-11-23, 13:25:36 UTC] {taskinstance.py:1139} INFO - Marking task as UP_FOR_RETRY. dag_id=kafka_issue, task_id=consume_task, run_id=manual__2024-11-23T13:24:25.243056+00:00, logical_date=20241123T132425, start_date=20241123T132436, end_date=20241123T132536
[2024-11-23, 13:25:36 UTC] {taskinstance.py:346} ▼ Post task execution logs
[2024-11-23, 13:25:36 UTC] {standard_task_runner.py:178} ERROR - Failed to execute task_id=consume_task pid=15454
Traceback (most recent call last):
  File ""/opt/airflow/airflow/task/standard_task_runner.py"", line 171, in _start_by_fork
    ret = args.func(args, dag=self.dag)
  File ""/opt/airflow/airflow/cli/cli_config.py"", line 49, in command
    return func(*args, **kwargs)
  File ""/opt/airflow/airflow/utils/cli.py"", line 112, in wrapper
    return f(*args, **kwargs)
  File ""/opt/airflow/airflow/cli/commands/task_command.py"", line 462, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File ""/opt/airflow/airflow/cli/commands/task_command.py"", line 257, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
  File ""/opt/airflow/airflow/cli/commands/task_command.py"", line 325, in _run_raw_task
    return ti._run_raw_task(
  File ""/opt/airflow/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File ""/opt/airflow/airflow/models/taskinstance.py"", line 2790, in _run_raw_task
    return _run_raw_task(
  File ""/opt/airflow/airflow/models/taskinstance.py"", line 279, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File ""/opt/airflow/airflow/models/taskinstance.py"", line 2937, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
  File ""/opt/airflow/airflow/models/taskinstance.py"", line 2961, in _execute_task
    return _execute_task(self, context, task_orig)
  File ""/opt/airflow/airflow/models/taskinstance.py"", line 759, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File ""/opt/airflow/airflow/models/taskinstance.py"", line 725, in _execute_callable
    return ExecutionCallableRunner(
  File ""/opt/airflow/airflow/utils/operator_helpers.py"", line 268, in run
    return func(*args, **kwargs)
  File ""/opt/airflow/airflow/models/baseoperator.py"", line 375, in wrapper
    return func(self, *args, **kwargs)
  File ""/opt/airflow/providers/src/airflow/providers/apache/kafka/operators/consume.py"", line 162, in execute
    msgs = consumer.consume(num_messages=batch_size, timeout=self.poll_timeout)
  File ""/opt/airflow/providers/src/airflow/providers/apache/kafka/hooks/consume.py"", line 31, in error_callback
    print(""Exception received: "", err)
  File ""/opt/airflow/providers/src/airflow/providers/apache/kafka/hooks/consume.py"", line 32, in error_callback
    raise KafkaAuthenticationError(f""Authentication failed: {err}"")
airflow.providers.apache.kafka.hooks.consume.KafkaAuthenticationError: Authentication failed: KafkaError{code=_TRANSPORT,val=-195,str=""hello.com:9092/bootstrap: Connection setup timed out in state CONNECT (after 30403ms in state CONNECT)""}
[2024-11-23, 13:25:36 UTC] {local_task_job_runner.py:263} INFO - Task exited with return code 1
[2024-11-23, 13:25:36 UTC] {local_task_job_runner.py:242} ▲▲▲ Log group end
```

This is the connection I used:

```
{
  ""bootstrap.servers"": ""hello.com:9092"",
  ""group.id"": ""ea""
}
```

"
2627241056,issue,closed,not_planned,amazon provider converts values to int when the tuning operator expect it as string,"### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

8.2.0

### Apache Airflow version

2.6.3

### Operating System

mw1.small

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### What happened

I have this task related to a tuning job on a dag:

`tuning_dict = {""task_id"": ""tuning"", ""config"": {""HyperParameterTuningJobConfig"": {""ParameterRanges"": {""CategoricalParameterRanges"": [{""Name"": ""max_features"", ""Values"": [""sqrt"", ""log2""]}, {""Name"": ""criterion"", ""Values"": [""gini"", ""entropy"", ""log_loss""]}], ""ContinuousParameterRanges"": [{""Name"": ""ccp_alpha"", ""MinValue"": ""0.0"", ""MaxValue"": ""0.02""}], ""IntegerParameterRanges"": [{""Name"": ""min_samples_leaf"", ""MinValue"": ""2"", ""MaxValue"": ""15""}, {""Name"": ""n_estimators"", ""MinValue"": ""50"", ""MaxValue"": ""500""}]}, ""HyperParameterTuningJobObjective"": {""Name"": ""validation:accuracy"", ""Type"": ""Maximize""}, ""Strategy"": ""Bayesian"", ""RandomSeed"": 123}, ...`
    
The key ContinuousParameterRanges contains some hyperparameters for mi tunning job that are casted as a string. This is a must based on the TuningOperator: https://github.com/apache/airflow/blob/providers-amazon/3.4.0/airflow/providers/amazon/aws/example_dags/example_sagemaker.py (line 202).

But I'm seeing that they are converted to float in the case of ContinuousParameterRanges or to int in the case of IntegerParameterRanges because of this bunch of code: https://github.com/apache/airflow/blob/providers-amazon/8.20.0/airflow/providers/amazon/aws/operators/sagemaker.py (line 99 or function parse_config_integers/parse_integers)

So when I execute the dag I get this kind of erros: 

```
Invalid type for parameter HyperParameterTuningJobConfig.ParameterRanges.ContinuousParameterRanges[0].MinValue, value: 0.0, type: <class 'float'>, valid types: <class 'str'>
Invalid type for parameter HyperParameterTuningJobConfig.ParameterRanges.ContinuousParameterRanges[0].MaxValue, value: 0.02, type: <class 'float'>, valid types: <class 'str'>
Invalid type for parameter HyperParameterTuningJobConfig.ParameterRanges.IntegerParameterRanges[0].MinValue, value: 2, type: <class 'int'>, valid types: <class 'str'>
Invalid type for parameter HyperParameterTuningJobConfig.ParameterRanges.IntegerParameterRanges[0].MaxValue, value: 15, type: <class 'int'>, valid types: <class 'str'>
Invalid type for parameter HyperParameterTuningJobConfig.ParameterRanges.IntegerParameterRanges[1].MinValue, value: 50, type: <class 'int'>, valid types: <class 'str'>
Invalid type for parameter HyperParameterTuningJobConfig.ParameterRanges.IntegerParameterRanges[1].MaxValue, value: 500, type: <class 'int'>, valid types: <class 'str'>
```

Any help?


### What you think should happen instead

I think that those parameters don't have to be converted as float or string

### How to reproduce

Generate a dag with this task:

'tuning_dict = {""task_id"": ""tuning"", ""config"": {""HyperParameterTuningJobConfig"": {""ParameterRanges"": {""CategoricalParameterRanges"": [{""Name"": ""max_features"", ""Values"": [""sqrt"", ""log2""]}, {""Name"": ""criterion"", ""Values"": [""gini"", ""entropy"", ""log_loss""]}], ""ContinuousParameterRanges"": [{""Name"": ""ccp_alpha"", ""MinValue"": ""0.0"", ""MaxValue"": ""0.02""}], ""IntegerParameterRanges"": [{""Name"": ""min_samples_leaf"", ""MinValue"": ""2"", ""MaxValue"": ""15""}, {""Name"": ""n_estimators"", ""MinValue"": ""50"", ""MaxValue"": ""500""}]}, ""HyperParameterTuningJobObjective"": {""Name"": ""validation:accuracy"", ""Type"": ""Maximize""}, ""Strategy"": ""Bayesian"", ""RandomSeed"": 123}, ""ResourceLimits"": {""MaxNumberOfTrainingJobs"": 10, ""MaxParallelTrainingJobs"": 4, ""MaxRuntimeInSeconds"": 7200}, ""Tags"": [{""Key"": ""USER"", ""Value"": ""santiago.sarratea@itti.digital""}, {""Key"": ""TRIBU"", ""Value"": ""Central Data""}, {""Key"": ""SQUAD"", ""Value"": ""Personalization and Relevance""}, {""Key"": ""ONLINE_OR_BATCH"", ""Value"": ""batch""}, {""Key"": ""PREDICTION_TYPE"", ""Value"": ""clasificacion binaria""}, {""Key"": ""VERSION_DESCRIPTION"", ""Value"": ""Version inicial""}, {""Key"": ""DESCRIPTION"", ""Value"": ""Desarrollo de deployment de pipeline de entrenamiento""}], ""HyperParameterTuningJobName"": ""mlpipeline-training-tuning"", ""TrainingJobDefinition"": {""AlgorithmSpecification"": {""TrainingImage"": ""<Training_image>"", ""TrainingInputMode"": ""File"", ""MetricDefinitions"": [{""Name"": ""validation:accuracy"", ""Regex"": ""validation-accuracy=(.*?);""}, {""Name"": ""validation:recall"", ""Regex"": ""validation-recall=(.*?);""}, {""Name"": ""validation:precision"", ""Regex"": ""validation-precision=(.*?);""}]}, ""InputDataConfig"": [{""ChannelName"": ""ingestion"", ""DataSource"": {""S3DataSource"": {""S3DataType"": ""S3Prefix"", ""S3Uri"": ""<BUCKET>"", ""S3DataDistributionType"": ""FullyReplicated""}}}], ""OutputDataConfig"": {""S3OutputPath"": ""s3://pr-ueno-prod-sagemaker/ml-projects/mlpipeline/training_pipeline/tuning/output""}, ""ResourceConfig"": {""InstanceType"": ""ml.m5.large"", ""InstanceCount"": 1, ""VolumeSizeInGB"": 10}, ""StoppingCondition"": {""MaxRuntimeInSeconds"": 7200}, ""RoleArn"": ""<ROLE>"", ""StaticHyperParameters"": {}}}}`


### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",francesco-camussoni-ueno,2024-10-31 16:02:02+00:00,[],2024-12-26 00:15:16+00:00,2024-12-26 00:15:15+00:00,https://github.com/apache/airflow/issues/43552,"[('kind:bug', 'This is a clearly a bug'), (""Can't Reproduce"", 'The problem cannot be reproduced'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('good first issue', ''), ('pending-response', '')]","[{'comment_id': 2450250271, 'issue_id': 2627241056, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 31, 16, 2, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2450815234, 'issue_id': 2627241056, 'author': 'eladkal', 'body': 'cc @ferruzzi @vincbeck', 'created_at': datetime.datetime(2024, 10, 31, 21, 5, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2450883896, 'issue_id': 2627241056, 'author': 'ferruzzi', 'body': 'Interesting.    Are you using the `SageMakerTuningOperator` for this?  I\'m not sure the issue is quite what you think it is.  If you look in the tuning operator where the integer fields are defined ([on L875](https://github.com/apache/airflow/blob/providers-amazon/8.20.0/airflow/providers/amazon/aws/operators/sagemaker.py#L875)), neither of the ones you point out are being flagged for converting to ints.  And if you are using the [create_tuning_job hook](https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/amazon/aws/hooks/sagemaker.py#L337C9-L337C26) directly, it doesn\'t appear to be processing the config at all.\r\n\r\n[EDIT:  In fact, none of the existing official operators or hooks seem to have ""ContinuousParameterRanges"" listed as a field which needs to be converted to an int...]', 'created_at': datetime.datetime(2024, 10, 31, 22, 2, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2451865544, 'issue_id': 2627241056, 'author': 'francesco-camussoni-ueno', 'body': 'I know is executiing this file: [operators/sagemaker.py](https://github.com/apache/airflow/blob/5fa80b6aea60f93cdada66f160e2b54f723865ca/airflow/providers/amazon/aws/operators/sagemaker.py#L115) \r\n\r\nBecause in the log I have this INFO log  _After preprocessing the config is:_\r\n\r\n```\r\n[2024-10-31, 15:57:06 UTC] {{sagemaker.py:96}} INFO - Preprocessing the config and doing required s3_operations\r\n[2024-10-31, 15:57:06 UTC] {{sagemaker.py:100}} INFO - After preprocessing the config is:\r\n {\r\n    ""HyperParameterTuningJobConfig"": {\r\n        ""HyperParameterTuningJobObjective"": {\r\n            ""Name"": ""validation:accuracy"",\r\n            ""Type"": ""Maximize""\r\n        },\r\n        ""ParameterRanges"": {\r\n            ""CategoricalParameterRanges"": [\r\n                {\r\n                    ""Name"": ""max_features"",\r\n                    ""Values"": [\r\n                        ""sqrt"",\r\n                        ""log2""\r\n                    ]\r\n                },\r\n                {\r\n                    ""Name"": ""criterion"",\r\n                    ""Values"": [\r\n                        ""gini"",\r\n                        ""entropy"",\r\n                        ""log_loss""\r\n                    ]\r\n                }\r\n            ],\r\n            ""ContinuousParameterRanges"": [\r\n                {\r\n                    ""MaxValue"": 0.02,\r\n                    ""MinValue"": 0.0,\r\n                    ""Name"": ""ccp_alpha""\r\n                }\r\n            ],\r\n            ""IntegerParameterRanges"": [\r\n                {\r\n                    ""MaxValue"": 15,\r\n                    ""MinValue"": 2,\r\n                    ""Name"": ""min_samples_leaf""\r\n                },\r\n                {\r\n                    ""MaxValue"": 500,\r\n                    ""MinValue"": 50,\r\n                    ""Name"": ""n_estimators""\r\n                }\r\n            ]\r\n        },\r\n        ""RandomSeed"": 123,\r\n        ""Strategy"": ""Bayesian""\r\n    },\r\n    ""HyperParameterTuningJobName"": ""mlpipeline-training-tuning"",\r\n    ""ResourceLimits"": {\r\n        ""MaxNumberOfTrainingJobs"": 10,\r\n        ""MaxParallelTrainingJobs"": 4,\r\n        ""MaxRuntimeInSeconds"": 7200\r\n    },\r\n```\r\n    \r\nBasically, the problem is that after the transformation I\'m seeing those values as int or float instead of str', 'created_at': datetime.datetime(2024, 11, 1, 13, 22, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2477386742, 'issue_id': 2627241056, 'author': 'vincbeck', 'body': 'Can you please try with the latest provider package? I cannot reproduce that with the latest version, plus we are running the system test `example_sagemaker` every hour and it is running successfully. When I look at the logs, here is what I have:\r\n\r\n```\r\n[2024-11-14T20:11:23.441+0000] {sagemaker.py:112} INFO - After preprocessing the config is:\r\n {\r\n    ""HyperParameterTuningJobConfig"": {\r\n        ""HyperParameterTuningJobObjective"": {\r\n            ""MetricName"": ""test:accuracy"",\r\n            ""Type"": ""Maximize""\r\n        },\r\n        ""ParameterRanges"": {\r\n            ""CategoricalParameterRanges"": [],\r\n            ""IntegerParameterRanges"": [\r\n                {\r\n                    ""MaxValue"": ""600"",\r\n                    ""MinValue"": ""1"",\r\n                    ""Name"": ""k""\r\n                },\r\n                {\r\n                    ""MaxValue"": ""600"",\r\n                    ""MinValue"": ""1"",\r\n                    ""Name"": ""sample_size""\r\n                }\r\n            ]\r\n        },...\r\n```', 'created_at': datetime.datetime(2024, 11, 14, 20, 51, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2506874455, 'issue_id': 2627241056, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 11, 29, 0, 16, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2511450456, 'issue_id': 2627241056, 'author': 'francesco-camussoni-ueno', 'body': 'Thank you vinbeck, I will try to set up a new airflow environment with the last provider and I will come here with the results', 'created_at': datetime.datetime(2024, 12, 2, 12, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2549963173, 'issue_id': 2627241056, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 12, 18, 0, 15, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562033328, 'issue_id': 2627241056, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 12, 26, 0, 15, 15, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-31 16:02:06 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2024-10-31 21:05:54 UTC): cc @ferruzzi @vincbeck

ferruzzi on (2024-10-31 22:02:02 UTC): Interesting.    Are you using the `SageMakerTuningOperator` for this?  I'm not sure the issue is quite what you think it is.  If you look in the tuning operator where the integer fields are defined ([on L875](https://github.com/apache/airflow/blob/providers-amazon/8.20.0/airflow/providers/amazon/aws/operators/sagemaker.py#L875)), neither of the ones you point out are being flagged for converting to ints.  And if you are using the [create_tuning_job hook](https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/amazon/aws/hooks/sagemaker.py#L337C9-L337C26) directly, it doesn't appear to be processing the config at all.

[EDIT:  In fact, none of the existing official operators or hooks seem to have ""ContinuousParameterRanges"" listed as a field which needs to be converted to an int...]

francesco-camussoni-ueno (Issue Creator) on (2024-11-01 13:22:35 UTC): I know is executiing this file: [operators/sagemaker.py](https://github.com/apache/airflow/blob/5fa80b6aea60f93cdada66f160e2b54f723865ca/airflow/providers/amazon/aws/operators/sagemaker.py#L115) 

Because in the log I have this INFO log  _After preprocessing the config is:_

```
[2024-10-31, 15:57:06 UTC] {{sagemaker.py:96}} INFO - Preprocessing the config and doing required s3_operations
[2024-10-31, 15:57:06 UTC] {{sagemaker.py:100}} INFO - After preprocessing the config is:
 {
    ""HyperParameterTuningJobConfig"": {
        ""HyperParameterTuningJobObjective"": {
            ""Name"": ""validation:accuracy"",
            ""Type"": ""Maximize""
        },
        ""ParameterRanges"": {
            ""CategoricalParameterRanges"": [
                {
                    ""Name"": ""max_features"",
                    ""Values"": [
                        ""sqrt"",
                        ""log2""
                    ]
                },
                {
                    ""Name"": ""criterion"",
                    ""Values"": [
                        ""gini"",
                        ""entropy"",
                        ""log_loss""
                    ]
                }
            ],
            ""ContinuousParameterRanges"": [
                {
                    ""MaxValue"": 0.02,
                    ""MinValue"": 0.0,
                    ""Name"": ""ccp_alpha""
                }
            ],
            ""IntegerParameterRanges"": [
                {
                    ""MaxValue"": 15,
                    ""MinValue"": 2,
                    ""Name"": ""min_samples_leaf""
                },
                {
                    ""MaxValue"": 500,
                    ""MinValue"": 50,
                    ""Name"": ""n_estimators""
                }
            ]
        },
        ""RandomSeed"": 123,
        ""Strategy"": ""Bayesian""
    },
    ""HyperParameterTuningJobName"": ""mlpipeline-training-tuning"",
    ""ResourceLimits"": {
        ""MaxNumberOfTrainingJobs"": 10,
        ""MaxParallelTrainingJobs"": 4,
        ""MaxRuntimeInSeconds"": 7200
    },
```
    
Basically, the problem is that after the transformation I'm seeing those values as int or float instead of str

vincbeck on (2024-11-14 20:51:53 UTC): Can you please try with the latest provider package? I cannot reproduce that with the latest version, plus we are running the system test `example_sagemaker` every hour and it is running successfully. When I look at the logs, here is what I have:

```
[2024-11-14T20:11:23.441+0000] {sagemaker.py:112} INFO - After preprocessing the config is:
 {
    ""HyperParameterTuningJobConfig"": {
        ""HyperParameterTuningJobObjective"": {
            ""MetricName"": ""test:accuracy"",
            ""Type"": ""Maximize""
        },
        ""ParameterRanges"": {
            ""CategoricalParameterRanges"": [],
            ""IntegerParameterRanges"": [
                {
                    ""MaxValue"": ""600"",
                    ""MinValue"": ""1"",
                    ""Name"": ""k""
                },
                {
                    ""MaxValue"": ""600"",
                    ""MinValue"": ""1"",
                    ""Name"": ""sample_size""
                }
            ]
        },...
```

github-actions[bot] on (2024-11-29 00:16:19 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

francesco-camussoni-ueno (Issue Creator) on (2024-12-02 12:47:00 UTC): Thank you vinbeck, I will try to set up a new airflow environment with the last provider and I will come here with the results

github-actions[bot] on (2024-12-18 00:15:43 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-12-26 00:15:15 UTC): This issue has been closed because it has not received response from the issue author.

"
2626737761,issue,open,,Manual trigger scheduling issues: logical_date<>data_interval_end mismatch,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When manually retriggering a DAG, the passed logical_date seems partly interpreted as data_interval_start and partly as data_interval_end

Context: I have a DAG run that fails and want to retrigger it on the exact same moment as the schedule, but with different parameters. During this process I encountered the what I believe is buggy behaviour.

Strange behaviour 1:
1. A scheduled monthly DAG run has finished with:
   - Data interval start | 2024-09-01, 00:00:00 CEST
   - Data interval end | 2024-10-01, 00:00:00 CEST
2. I trigger the DAG again where I fill the logical_date with the data_interval_start
3. It throws the following error: ""The logical date [...] already exists""

Strange behaviour 2:
1. A scheduled monthly DAG run has finished with:
   - Data interval start | 2024-09-01, 00:00:00 CEST
   - Data interval end | 2024-10-01, 00:00:00 CEST
2. I trigger the DAG again where I fill the logical_date with the data_interval_end
3. It triggers a run with the same data_interval_end and data_interval_start as the scheduled run
4. The sensor we have in place however pokes an unexpected date: 
   - The scheduled poke date was: 2024-08-31T22:00:00+00:00
   - The triggered poke date was: 2024-09-30T22:00:00+00:00

### What you think should happen instead?

Behaviour 1:
<img width=""1701"" alt=""image"" src=""https://github.com/user-attachments/assets/17c5a80c-e732-4bf0-b667-392db8590322"">
I expected this error to be thrown in the case of filling the data_interval_end, not in the case of the data_interval_start. Because if I fill the data_interval_end, I get the data_interval_start and data_interval_end of the already scheduled DAG run as well as the same run name of that DAG run

Behaviour 2:
<img width=""1691"" alt=""image"" src=""https://github.com/user-attachments/assets/7c1550f2-6b91-4c82-a74d-8c54acd193bb"">
I expected the error of behaviour 1 to be thrown here.
I expected the sensor to poke depending on the data_interval_start not based on the data_interval_end
<img width=""725"" alt=""image"" src=""https://github.com/user-attachments/assets/93ec1f08-085f-4dba-a20e-97a12a9c0d62"">
<img width=""713"" alt=""image"" src=""https://github.com/user-attachments/assets/79dc2325-8859-4927-8404-6dccc9b3e325"">


### How to reproduce

See ""What happened""

### Operating System

Debian GNU/Linux 11 (bullseye)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

( Probably has overlap with https://github.com/apache/airflow/issues/41014 ?)

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",diederikvdv,2024-10-31 12:31:26+00:00,[],2024-10-31 12:31:30+00:00,,https://github.com/apache/airflow/issues/43547,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2449734976, 'issue_id': 2626737761, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 31, 12, 31, 29, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-31 12:31:29 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2626490135,issue,closed,not_planned,"Status of testing Providers that were prepared on October 31, 2024","### Body

I have a kind request for all the contributors to the latest provider packages release.
Could you please help us to test the RC versions of the providers?

The guidelines on how to test providers can be found in

[Verify providers by contributors](https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDER_PACKAGES.md#verify-the-release-candidate-by-contributors)

Let us know in the comment, whether the issue is addressed.

Those are providers that require testing as there were some substantial changes introduced:


## Provider [amazon: 9.1.0rc2](https://pypi.org/project/apache-airflow-providers-amazon/9.1.0rc2)
   - [ ] [feat: add OpenLineage support for RedshiftToS3Operator (#41632)](https://github.com/apache/airflow/pull/41632): @kacpermuda
     Linked issues:
       - [ ] [Linked Issue #41575](https://github.com/apache/airflow/pull/41575): @Artuz37
   - [ ] [Add SageMakerProcessingSensor (#43144)](https://github.com/apache/airflow/pull/43144): @vVv-AA
   - [ ] [Make `RedshiftDataOperator`  handle multiple queries (#42900)](https://github.com/apache/airflow/pull/42900): @jroachgolf84
   - [ ] [fix(providers/amazon): alias is_authorized_dataset to is_authorized_asset (#43470)](https://github.com/apache/airflow/pull/43470): @Lee-W
   - [ ] [Remove returns in final clause of athena hooks (#43426)](https://github.com/apache/airflow/pull/43426): @yangyulely
     Linked issues:
       - [ ] [Linked Issue #43274](https://github.com/apache/airflow/issues/43274): @iritkatriel
   - [ ] [Remove sqlalchemy-redshift dependency (#43271)](https://github.com/apache/airflow/pull/43271): @mobuchowski
   - [ ] [feat(providers/amazon): Use asset in common provider (#43110)](https://github.com/apache/airflow/pull/43110): @Lee-W
   - [ ] [Restrict looker-sdk version 24.18.0 and microsoft-kiota-http 1.3.4 (#42954)](https://github.com/apache/airflow/pull/42954): @gopidesupavan
   - [x] [Limit mypy-boto3-appflow (#43436)](https://github.com/apache/airflow/pull/43436): @eladkal
## Provider [fab: 1.5.0rc2](https://pypi.org/project/apache-airflow-providers-fab/1.5.0rc2)
   - [ ] [feat(providers/fab): Use asset in common provider (#43112)](https://github.com/apache/airflow/pull/43112): @Lee-W
   - [ ] [Fix revoke Dag stale permission on airflow < 2.10 (#42844)](https://github.com/apache/airflow/pull/42844): @joaopamaral
     Linked issues:
       - [ ] [Linked Issue #42743](https://github.com/apache/airflow/issues/42743): @RostD
   - [ ] [fix(providers/fab): alias is_authorized_dataset to is_authorized_asset (#43469)](https://github.com/apache/airflow/pull/43469): @Lee-W
   - [ ] [Bump Flask-AppBuilder to ``4.5.2`` (#43309)](https://github.com/apache/airflow/pull/43309): @kaxil
   - [ ] [Upgrade FAB to 4.5.1 (#43251)](https://github.com/apache/airflow/pull/43251): @potiuk
   - [ ] [Move user and roles schemas to fab provider (#42869)](https://github.com/apache/airflow/pull/42869): @vincbeck
   - [ ] [Move the session auth backend to FAB auth manager (#42878)](https://github.com/apache/airflow/pull/42878): @vincbeck
     Linked issues:
       - [ ] [Linked Issue #42634](https://github.com/apache/airflow/pull/42634): @vincbeck

<!--

NOTE TO RELEASE MANAGER:

You can move here the providers that have doc-only changes or for which changes are trivial, and
you could assess that they are OK.

-->
All users involved in the PRs:
@yangyulely @kacpermuda @mobuchowski @kaxil @vVv-AA @Lee-W @jroachgolf84 @joaopamaral @vincbeck @gopidesupavan @potiuk @eladkal

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2024-10-31 10:18:49+00:00,[],2024-11-03 09:27:29+00:00,2024-11-03 09:27:28+00:00,https://github.com/apache/airflow/issues/43545,"[('provider:amazon', 'AWS/Amazon - related issues'), ('kind:meta', 'High-level information important to the community'), ('testing status', 'Status of testing releases'), ('provider:fab', '')]","[{'comment_id': 2453342011, 'issue_id': 2626490135, 'author': 'eladkal', 'body': 'Closing as vote is cancelled', 'created_at': datetime.datetime(2024, 11, 3, 9, 27, 28, tzinfo=datetime.timezone.utc)}]","eladkal (Issue Creator) on (2024-11-03 09:27:28 UTC): Closing as vote is cancelled

"
2626226146,issue,open,,Add color to log lines in UI for debug based on keywords,"### Description

Add colors to logs for debug rows. Similar to what was done for errors and warnings in Airflow 2.10.

### Use case/motivation

In Airflow 2.10.0, the errors and warnings in task logs were colored. This is very cool, and I love it!

Would is be possible to color the debug lines as well?

The goal is to further improve the readability of the logs. The suggested color is grey :wink:

This is how it looks in PyCharm using some log coloring plugin:

![image](https://github.com/user-attachments/assets/b7f6d618-592f-4d36-a058-b72def741f2e)


### Related issues

https://github.com/apache/airflow/issues/37443


### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",porn,2024-10-31 07:59:10+00:00,"['csking101', 'dauinh']",2024-11-28 06:42:22+00:00,,https://github.com/apache/airflow/issues/43541,"[('area:logging', ''), ('kind:feature', 'Feature Requests'), ('good first issue', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2449260397, 'issue_id': 2626226146, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 31, 7, 59, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2450354970, 'issue_id': 2626226146, 'author': 'potiuk', 'body': 'Maybe you would like to attempt to implement it ? I think it\'s a good idea and it falls into ""improve debugging"" project of ours. cc: @omkar-foss', 'created_at': datetime.datetime(2024, 10, 31, 16, 48, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2450356192, 'issue_id': 2626226146, 'author': 'potiuk', 'body': 'Marked it as ""good first issue"" so that maybe if not you @porn, then someone else could take it and try', 'created_at': datetime.datetime(2024, 10, 31, 16, 49, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2451318002, 'issue_id': 2626226146, 'author': 'porn', 'body': '> Marked it as ""good first issue"" so that maybe if not you @porn, then someone else could take it and try\r\n\r\nHey @potiuk, I\'m afraid I won\'t be able to get to that :disappointed:', 'created_at': datetime.datetime(2024, 11, 1, 5, 27, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2451715294, 'issue_id': 2626226146, 'author': 'potiuk', 'body': 'No worries :). Possibly someone else will now that it is marked as ""good first issue""', 'created_at': datetime.datetime(2024, 11, 1, 11, 18, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453010168, 'issue_id': 2626226146, 'author': 'csking101', 'body': 'Hi, could I take up this issue?', 'created_at': datetime.datetime(2024, 11, 2, 14, 35, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453025750, 'issue_id': 2626226146, 'author': 'potiuk', 'body': 'Sure!. Just be aware that we are working on Airflow 3 UI now that is moved to a completely different folder (ui) - and the old UI is not going to get any nrew feature and will be deleted in Airflow 3, so you should add it for Airflow 3 - I am not sure if that part is already ported @bbovenzi @pierrejeambrun', 'created_at': datetime.datetime(2024, 11, 2, 15, 37, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2454791662, 'issue_id': 2626226146, 'author': 'bbovenzi', 'body': 'On the radar for the new UI!', 'created_at': datetime.datetime(2024, 11, 4, 13, 59, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2502216630, 'issue_id': 2626226146, 'author': 'dauinh', 'body': ""Hi! Could I work on this issue as well? \r\n\r\n@csking101 I'd be happy collaborate!"", 'created_at': datetime.datetime(2024, 11, 27, 0, 1, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2503934652, 'issue_id': 2626226146, 'author': 'pierrejeambrun', 'body': ""We didn't hear back for a while, @csking101  are you still working on that ? In the meantime @dauinh I also assigned you, feel free to work on it or collaborate together if @csking101 reports back some activity :)"", 'created_at': datetime.datetime(2024, 11, 27, 13, 52, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2504044728, 'issue_id': 2626226146, 'author': 'csking101', 'body': ""Sure, I'd love to collaborate on the issue! How can we get in touch?\r\n\r\nOn Wed, Nov 27, 2024, 7:22 PM Pierre Jeambrun ***@***.***>\r\nwrote:\r\n\r\n> We didn't hear back for a while, @csking101 <https://github.com/csking101>\r\n> are you still working on that ? In the meantime @dauinh\r\n> <https://github.com/dauinh> I also assigned you, feel free to work on it\r\n> or collaborate together if @csking101 <https://github.com/csking101>\r\n> reports back some activity :)\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/apache/airflow/issues/43541#issuecomment-2503934652>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AKCK6HHRIKZDBPX44UL5KAD2CXFDHAVCNFSM6AAAAABQ5XTOUCVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKMBTHEZTINRVGI>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>"", 'created_at': datetime.datetime(2024, 11, 27, 14, 38, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2504292901, 'issue_id': 2626226146, 'author': 'pierrejeambrun', 'body': ""I think the easiest way would be the community slack, that's also a great place to ask for opinions or advice if you need:\r\nhttps://join.slack.com/t/apache-airflow/shared_invite/zt-2v3dlwdrb-KVQfM0ztx2I2QfWSAuQq5g\r\n\r\nBut feel free to user whatever mean you see fit :)"", 'created_at': datetime.datetime(2024, 11, 27, 16, 23, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2504813019, 'issue_id': 2626226146, 'author': 'dauinh', 'body': ""Thank you for sharing the link! I'm on the community slack, my handler is Dauinh :)"", 'created_at': datetime.datetime(2024, 11, 27, 21, 41, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2505368748, 'issue_id': 2626226146, 'author': 'omkar-foss', 'body': ""Hi @csking101 @dauinh! I think a good starting point in code may be to look at the code for log level color mapping [here](https://github.com/apache/airflow/blob/main/airflow/www/static/js/dag/details/taskInstance/Logs/utils.ts#L34-L40) in the old UI and how it gets used.\r\n\r\nCurrently this is in `airflow/www` (old UI), and needs to be ported to `airflow/ui` (new UI) for Airflow 3, so all your code changes would come in `airflow/ui` (Jarek also mentioned in [above comment](https://github.com/apache/airflow/issues/43541#issuecomment-2453025750)).\r\n\r\nAlso if you're looking for a channel to brainstorm on this issue, I guess `airflow-3-ui` ([channel link](https://apache-airflow.slack.com/archives/C0809U4S1Q9)) on community slack might be a nice place :)"", 'created_at': datetime.datetime(2024, 11, 28, 6, 42, 21, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-31 07:59:13 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-10-31 16:48:34 UTC): Maybe you would like to attempt to implement it ? I think it's a good idea and it falls into ""improve debugging"" project of ours. cc: @omkar-foss

potiuk on (2024-10-31 16:49:14 UTC): Marked it as ""good first issue"" so that maybe if not you @porn, then someone else could take it and try

porn (Issue Creator) on (2024-11-01 05:27:51 UTC): Hey @potiuk, I'm afraid I won't be able to get to that :disappointed:

potiuk on (2024-11-01 11:18:22 UTC): No worries :). Possibly someone else will now that it is marked as ""good first issue""

csking101 (Assginee) on (2024-11-02 14:35:48 UTC): Hi, could I take up this issue?

potiuk on (2024-11-02 15:37:30 UTC): Sure!. Just be aware that we are working on Airflow 3 UI now that is moved to a completely different folder (ui) - and the old UI is not going to get any nrew feature and will be deleted in Airflow 3, so you should add it for Airflow 3 - I am not sure if that part is already ported @bbovenzi @pierrejeambrun

bbovenzi on (2024-11-04 13:59:47 UTC): On the radar for the new UI!

dauinh (Assginee) on (2024-11-27 00:01:34 UTC): Hi! Could I work on this issue as well? 

@csking101 I'd be happy collaborate!

pierrejeambrun on (2024-11-27 13:52:27 UTC): We didn't hear back for a while, @csking101  are you still working on that ? In the meantime @dauinh I also assigned you, feel free to work on it or collaborate together if @csking101 reports back some activity :)

csking101 (Assginee) on (2024-11-27 14:38:34 UTC): Sure, I'd love to collaborate on the issue! How can we get in touch?

On Wed, Nov 27, 2024, 7:22 PM Pierre Jeambrun ***@***.***>
wrote:

pierrejeambrun on (2024-11-27 16:23:06 UTC): I think the easiest way would be the community slack, that's also a great place to ask for opinions or advice if you need:
https://join.slack.com/t/apache-airflow/shared_invite/zt-2v3dlwdrb-KVQfM0ztx2I2QfWSAuQq5g

But feel free to user whatever mean you see fit :)

dauinh (Assginee) on (2024-11-27 21:41:34 UTC): Thank you for sharing the link! I'm on the community slack, my handler is Dauinh :)

omkar-foss on (2024-11-28 06:42:21 UTC): Hi @csking101 @dauinh! I think a good starting point in code may be to look at the code for log level color mapping [here](https://github.com/apache/airflow/blob/main/airflow/www/static/js/dag/details/taskInstance/Logs/utils.ts#L34-L40) in the old UI and how it gets used.

Currently this is in `airflow/www` (old UI), and needs to be ported to `airflow/ui` (new UI) for Airflow 3, so all your code changes would come in `airflow/ui` (Jarek also mentioned in [above comment](https://github.com/apache/airflow/issues/43541#issuecomment-2453025750)).

Also if you're looking for a channel to brainstorm on this issue, I guess `airflow-3-ui` ([channel link](https://apache-airflow.slack.com/archives/C0809U4S1Q9)) on community slack might be a nice place :)

"
2625901246,issue,closed,completed,Cannot import the DAG file with _config suffix.,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

The  DAG file name goes with ``_config.py``, and the DAG won't be imported. For instance, the DAG is named xxxx_config.py, it won't be imported successfully. 

### What you think should happen instead?

_No response_

### How to reproduce

create a DAG, and name as wf_sample_config.py. 
the context in wf_sample_config.py is as follows:
```python
from datetime import datetime
from airflow.decorators import  task
from airflow import DAG

with DAG (
    dag_id=""a_testing_for_naming"",
    start_date=datetime(2024, 10, 31),
    schedule=None
) as dag:

    @task
    def start(params = None, task=None, ti=None, dag_run=None):
        pass

    start()
```

You won't see the a_testing_for_naming DAG in your DAG view in the web UI.
How do you solve it?
just change the name with _cfg. For example, wf_sample_cfg.py. You can see the DAG loaded successfully.

### Operating System

Debian GNU/Linux 11 (bullseye)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

python version : python 3.12

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",yc0,2024-10-31 03:41:20+00:00,[],2024-11-01 08:06:26+00:00,2024-11-01 08:06:26+00:00,https://github.com/apache/airflow/issues/43537,"[(""Can't Reproduce"", 'The problem cannot be reproduced'), ('pending-response', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('area:DAG-processing', '')]","[{'comment_id': 2450162542, 'issue_id': 2625901246, 'author': 'rawwar', 'body': ""Before I test this out, I have a quick question: Can you please check your .airflowignore for any entries that might be causing this?\r\n\r\nEDIT: I just tested this on 2.10.2, and I can't replicate this issue."", 'created_at': datetime.datetime(2024, 10, 31, 15, 24, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2451236469, 'issue_id': 2625901246, 'author': 'yc0', 'body': 'here comes our .airflowignore\r\n```\r\n.archive/*\r\n.config/*\r\n.lib/*\r\nextends/*\r\n```', 'created_at': datetime.datetime(2024, 11, 1, 3, 43, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2451258145, 'issue_id': 2625901246, 'author': 'rawwar', 'body': 'so, by default [dag_ignore_file_syntax](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#dag-ignore-file-syntax) is set to regex. And, it looks like `.config/*` is getting matched with any file with suffix config`. I tried setting `AIRFLOW__CORE__DAG_IGNORE_FILE_SYNTAX=glob` and it did not ignore my dag(file named `example_config.py`)\r\n\r\n\r\nJust tested. \r\n```\r\nIn [2]: re2.findall(r"".config/*"", ""example_config.py"")\r\nOut[2]: [\'_config\']\r\n```\r\n\r\nSolution for you is to use glob syntax and set AIRFLOW__CORE__DAG_IGNORE_FILE_SYNTAX=glob', 'created_at': datetime.datetime(2024, 11, 1, 4, 10, 6, tzinfo=datetime.timezone.utc)}]","rawwar on (2024-10-31 15:24:39 UTC): Before I test this out, I have a quick question: Can you please check your .airflowignore for any entries that might be causing this?

EDIT: I just tested this on 2.10.2, and I can't replicate this issue.

yc0 (Issue Creator) on (2024-11-01 03:43:49 UTC): here comes our .airflowignore
```
.archive/*
.config/*
.lib/*
extends/*
```

rawwar on (2024-11-01 04:10:06 UTC): so, by default [dag_ignore_file_syntax](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#dag-ignore-file-syntax) is set to regex. And, it looks like `.config/*` is getting matched with any file with suffix config`. I tried setting `AIRFLOW__CORE__DAG_IGNORE_FILE_SYNTAX=glob` and it did not ignore my dag(file named `example_config.py`)


Just tested. 
```
In [2]: re2.findall(r"".config/*"", ""example_config.py"")
Out[2]: ['_config']
```

Solution for you is to use glob syntax and set AIRFLOW__CORE__DAG_IGNORE_FILE_SYNTAX=glob

"
2625715830,issue,open,,AIP-84 Add user id when updating notes using PATCH dag_run and task_instance endpoint of FastAPI app,"### Description

While working on the PR #43508 , @pierrejeambrun confirmed, we don't have a way to fetch user id as of now. This issue is to add User ID, once auth features are merged.

We also need to add tests for verifying auth

### Use case/motivation

Reference comment - https://github.com/apache/airflow/pull/43508#discussion_r1822827784

### Related issues

- [ ] Update PATCH dag_run
- [ ] Update PATCH task_instance
- [ ] Trigger a Dag Run

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rawwar,2024-10-31 01:19:56+00:00,['rawwar'],2024-11-21 07:18:40+00:00,,https://github.com/apache/airflow/issues/43534,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]",[],
2625225009,issue,open,,Default args no longer available in DAG description,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I'm not sure where the change was made, but default_args was visible at least in 2.5.3 and no longer exists in the Airflow UI Dag description view in Airflow 2.9 or 2.10.

### What you think should happen instead?

`default_args` should be cleanly (i.e. jsonified) displayed.

### How to reproduce

Open the Dag description view in the Airflow UI

### Operating System

n/a

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",RNHTTR,2024-10-30 20:06:43+00:00,[],2024-11-04 21:53:03+00:00,,https://github.com/apache/airflow/issues/43525,"[('kind:bug', 'This is a clearly a bug'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2448373066, 'issue_id': 2625225009, 'author': 'bbovenzi', 'body': '@RNHTTR do you happen to have any screenshots from 2.5.3?', 'created_at': datetime.datetime(2024, 10, 30, 21, 1, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2450322594, 'issue_id': 2625225009, 'author': 'potiuk', 'body': ':scream:', 'created_at': datetime.datetime(2024, 10, 31, 16, 34, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455776290, 'issue_id': 2625225009, 'author': 'RNHTTR', 'body': '> @RNHTTR do you happen to have any screenshots from 2.5.3?\r\n\r\n<img width=""912"" alt=""image"" src=""https://github.com/user-attachments/assets/d9d61ddd-5ef9-4c1f-aa06-fd571ce25800"">', 'created_at': datetime.datetime(2024, 11, 4, 21, 53, 2, tzinfo=datetime.timezone.utc)}]","bbovenzi on (2024-10-30 21:01:16 UTC): @RNHTTR do you happen to have any screenshots from 2.5.3?

potiuk on (2024-10-31 16:34:40 UTC): :scream:

RNHTTR (Issue Creator) on (2024-11-04 21:53:02 UTC): <img width=""912"" alt=""image"" src=""https://github.com/user-attachments/assets/d9d61ddd-5ef9-4c1f-aa06-fd571ce25800"">

"
2624673695,issue,open,,Clean up webserver config,"For Airflow 3, we should take a look at which config & settings fields that we want to keep or remove. Particularly for the webserver.

Config we should remove:
- `dag_default_view`: Unnecessary with new designs
- `dag_orientation`: Better to customize in the UI (either changes dag to dag, or part of i18n)
- `grid_view_sorting_order`: Better to customize in the UI. Matters more on a dag to dag level
- `default_dag_run_display_number`: Better for the UI to decide
- `default_wrap`: same as dag_orientation
- `show_recent_stats_for_completed_runs`: Unnecessary with new designs
- `default_ui_timezone`: we should default to the system timezone
- `log_fetch_*`: we should just use the auto_refresh_interval
- `log_animation_speed`: Better for the UI to decide
- `navbar_logo_text_color`: this doesn't exist anymore
- `show_trigger_form_if_no_params` [We'll always have a confirm step](https://github.com/apache/airflow/issues/44256#issuecomment-2492357015)

Settings we should remove:
- `STATE_COLORS`: current colors don't work well in both light and dark mode. Doesn't allow individual users to customize to what they personally can see best


Dag fields to remove:
- `orientation`
-  `default_views`

Task fields to remove:
- `ui_color`
- `ui_fgcolor`: I know some people like these, but having both state and operator colors gets too loud and unusable too fast
",bbovenzi,2024-10-30 16:35:33+00:00,[],2024-11-21 21:21:22+00:00,,https://github.com/apache/airflow/issues/43519,"[('area:webserver', 'Webserver related Issues'), ('AIP-38', 'Modern Web Application')]","[{'comment_id': 2448191262, 'issue_id': 2624673695, 'author': 'kaxil', 'body': 'Nice, a lot of cleanup! but I assume this can happen once we remove the `airflow/www` dir', 'created_at': datetime.datetime(2024, 10, 30, 19, 35, 38, tzinfo=datetime.timezone.utc)}]","kaxil on (2024-10-30 19:35:38 UTC): Nice, a lot of cleanup! but I assume this can happen once we remove the `airflow/www` dir

"
2624668555,issue,closed,completed,Add scheduler by Dataset and/or interval,"### Description

In recent features it is possible to add dataset scheduling with conditional expressions (https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/datasets.html#advanced-dataset-scheduling-with-conditional-expressions)

The idea would be to add a time-based schedule to these expressions as well. For example: schedule=['0 0 1 * *' | Dataset(path)]

### Use case/motivation

There are some events involving many Datasets as triggers. If a Dataset is late or gives an error, the entire pipeline will depend on it. The idea would be to add a time condition to start the DAG anyway.

Another problem is, if all the datasets are ready and the pipeline is ready, the pipeline must run after a certain time.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",FelipeArisi,2024-10-30 16:33:10+00:00,[],2024-11-05 10:44:16+00:00,2024-11-05 10:44:16+00:00,https://github.com/apache/airflow/issues/43518,"[('kind:feature', 'Feature Requests'), ('pending-response', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('area:datasets', 'Issues related to the datasets feature')]","[{'comment_id': 2452146949, 'issue_id': 2624668555, 'author': 'tirkarthi', 'body': 'Does DatasetOrTimeSchedule help your use case?\r\n\r\nhttps://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/timetable.html#dataset-event-based-scheduling-with-time-based-scheduling', 'created_at': datetime.datetime(2024, 11, 1, 16, 13, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2456834444, 'issue_id': 2624668555, 'author': 'FelipeArisi', 'body': ""I didn't know this one! It will work, thank you very much!"", 'created_at': datetime.datetime(2024, 11, 5, 10, 44, 16, tzinfo=datetime.timezone.utc)}]","tirkarthi on (2024-11-01 16:13:53 UTC): Does DatasetOrTimeSchedule help your use case?

https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/timetable.html#dataset-event-based-scheduling-with-time-based-scheduling

FelipeArisi (Issue Creator) on (2024-11-05 10:44:16 UTC): I didn't know this one! It will work, thank you very much!

"
2624541458,issue,closed,completed,Specifying podTemplateFile spark configuration in the SparkApplication manifest overrides a path Airflow container looks for,"### Apache Airflow Provider(s)

cncf-kubernetes

### Versions of Apache Airflow Providers

8.1.1

### Apache Airflow version

2.8.4

### Operating System

Debian GNU/Linux

### Deployment

Other 3rd-party Helm chart

### Deployment details

k8s: 1.29
terraform helm provider: 2.16.1


### What happened

Attempting to submit a spark job via SparkKubernetesOperator in airflow. We have the [spark-operator](https://github.com/kubeflow/spark-operator) v2.0.2 installed. 

spark-application-manifest.yaml
```yaml
apiVersion: ""sparkoperator.k8s.io/v1beta2""
kind: SparkApplication
metadata:
  name: {{ application_name }}
  namespace: airflow
spec:
  type: Python
  mode: cluster
  image: {{ image_path }}
  imagePullPolicy: IfNotPresent
  pythonVersion: ""3""
  mainApplicationFile: ""local:////opt/spark/work-dir/runner/v2/__main__.py""
  sparkVersion: ""3.5.1""
  timeToLiveSeconds: 600
  sparkConf:
    spark.serializer: org.apache.spark.serializer.KryoSerializer
    spark.dynamicAllocation.executorIdleTimeout: 600s
    spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-1.options.claimName: OnDemand
    spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-1.options.storageClass: io2-storage-class
    spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-1.options.sizeLimit: 25Gi
    spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-1.mount.path: /opt/spark/shuffle
    spark.kubernetes.driver.volumes.persistentVolumeClaim.spark-local-dir-1.mount.readOnly: ""false""
    spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.claimName: OnDemand
    spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.storageClass: io2-storage-class
    spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.sizeLimit: 25Gi
    spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.path: /opt/spark/shuffle
    spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.readOnly: ""false""
    spark.local.dir: /opt/spark/shuffle
    spark.kubernetes.driver.podTemplateFile: ""/opt/spark/conf/pod_template.yaml""
    spark.kubernetes.executor.podTemplateFile: ""/opt/spark/conf/pod_template.yaml""
  hadoopConf:
    fs.s3a.aws.credentials.provider: ""com.amazonaws.auth.WebIdentityTokenCredentialsProvider"" 
    fs.s3a.impl: ""org.apache.hadoop.fs.s3a.S3AFileSystem""
  restartPolicy:
    type: Never
  driver:
    cores: 8
    coreRequest: ""8""
    coreLimit: ""8""
    memory: ""20G"" 
    memoryOverheadFactor: 0.10 
    serviceAccount: spark-app
  executor:
    deleteOnTermination: false
    cores: 8
    coreRequest: ""8""
    coreLimit: ""8""
    instances: 29 
    memory: ""20G"" 
    memoryOverheadFactor: 0.10 
    serviceAccount: spark-app
```

Note `spark.kubernetes.driver.podTemplateFile` paramter. The path `/opt/spark/conf/pod_template.yaml` exist in the Spark Operator pod in our cluster which we mounted on as a config map. I confirmed this exist in the spark operator pod

After the dag gets started and attempts to start the spark job, I get this error

```
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/abstractoperator.py"", line 699, in _do_render_template_fields
    rendered_content = self.render_template(
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/template/templater.py"", line 186, in render_template
    return {k: self.render_template(v, context, jinja_env, oids) for k, v in value.items()}
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/template/templater.py"", line 186, in <dictcomp>
    return {k: self.render_template(v, context, jinja_env, oids) for k, v in value.items()}
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/template/templater.py"", line 186, in render_template
    return {k: self.render_template(v, context, jinja_env, oids) for k, v in value.items()}
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/template/templater.py"", line 186, in <dictcomp>
    return {k: self.render_template(v, context, jinja_env, oids) for k, v in value.items()}
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/template/templater.py"", line 186, in render_template
    return {k: self.render_template(v, context, jinja_env, oids) for k, v in value.items()}
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/template/templater.py"", line 186, in <dictcomp>
    return {k: self.render_template(v, context, jinja_env, oids) for k, v in value.items()}
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/template/templater.py"", line 171, in render_template
    template = jinja_env.get_template(value)
  File ""/home/airflow/.local/lib/python3.10/site-packages/jinja2/environment.py"", line 1010, in get_template
    return self._load_template(name, globals)
  File ""/home/airflow/.local/lib/python3.10/site-packages/jinja2/environment.py"", line 969, in _load_template
    template = self.loader.load(self, name, self.make_globals(globals))
  File ""/home/airflow/.local/lib/python3.10/site-packages/jinja2/loaders.py"", line 125, in load
    source, filename, uptodate = self.get_source(environment, name)
  File ""/home/airflow/.local/lib/python3.10/site-packages/jinja2/loaders.py"", line 204, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: /opt/spark/conf/pod_template.yaml
[2024-10-30, 11:20:16 EDT] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 2360, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 2498, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 2910, in render_templates
    original_task.render_template_fields(context)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py"", line 1241, in render_template_fields
    self._do_render_template_fields(self, self.template_fields, context, jinja_env, set())
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py"", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/abstractoperator.py"", line 699, in _do_render_template_fields
    rendered_content = self.render_template(
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/template/templater.py"", line 186, in render_template
    return {k: self.render_template(v, context, jinja_env, oids) for k, v in value.items()}
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/template/templater.py"", line 186, in <dictcomp>
    return {k: self.render_template(v, context, jinja_env, oids) for k, v in value.items()}
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/template/templater.py"", line 186, in render_template
    return {k: self.render_template(v, context, jinja_env, oids) for k, v in value.items()}
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/template/templater.py"", line 186, in <dictcomp>
    return {k: self.render_template(v, context, jinja_env, oids) for k, v in value.items()}
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/template/templater.py"", line 186, in render_template
    return {k: self.render_template(v, context, jinja_env, oids) for k, v in value.items()}
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/template/templater.py"", line 186, in <dictcomp>
    return {k: self.render_template(v, context, jinja_env, oids) for k, v in value.items()}
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/template/templater.py"", line 171, in render_template
    template = jinja_env.get_template(value)
  File ""/home/airflow/.local/lib/python3.10/site-packages/jinja2/environment.py"", line 1010, in get_template
    return self._load_template(name, globals)
  File ""/home/airflow/.local/lib/python3.10/site-packages/jinja2/environment.py"", line 969, in _load_template
    template = self.loader.load(self, name, self.make_globals(globals))
  File ""/home/airflow/.local/lib/python3.10/site-packages/jinja2/loaders.py"", line 125, in load
    source, filename, uptodate = self.get_source(environment, name)
  File ""/home/airflow/.local/lib/python3.10/site-packages/jinja2/loaders.py"", line 204, in get_source
    raise TemplateNotFound(template)
jinja2.exceptions.TemplateNotFound: /opt/spark/conf/pod_template.yaml
```

However if I remove `spark.kubernetes.driver.podTemplateFile` then the job gets submitted without an issue

### What you think should happen instead

Airflow should pass along the SparkApplication Manifest along to the Spark Operator that exist in our Kubernetes cluster without trying to read `spark.kubernetes.driver.podTemplateFile` file path and look for it in the airflow pod which should just be submitting the SparkApplication manifest along to the Spark Operator pod

### How to reproduce

Have airflow 2.8.4 installed, have SparkOperator 2.0.2 installed, create a similar SparkApplication manifest as described above and specify `spark.kubernetes.driver.podTemplateFile` and `spark.kubernetes.executor.podTemplateFile` and try to run

### Anything else

I have also experience the same issue using the older version of the Spark Operator so this is isolated from the Spark-Operator version

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",BCantos17,2024-10-30 15:50:36+00:00,[],2024-10-31 18:08:36+00:00,2024-10-31 18:08:09+00:00,https://github.com/apache/airflow/issues/43517,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('pending-response', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2447611448, 'issue_id': 2624541458, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 30, 15, 50, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448513112, 'issue_id': 2624541458, 'author': 'gopidesupavan', 'body': 'Can you please try on the latest airflow release?', 'created_at': datetime.datetime(2024, 10, 30, 22, 0, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2450521655, 'issue_id': 2624541458, 'author': 'BCantos17', 'body': ""Hi @gopidesupavan, I appreciate the response but I do not think updating to the latest release is a proper fix. I would need to get approval from the rest of the staff unless it's a patch version which I am already on the latest for 2.8.X\r\n\r\nI did however find more details during my troubleshooting. It has more to with jinja templating than the CNCF provider. We had to use the parameter `template_spec` on the SparkKubernetesOperator to pass along the SparkApplication manifest file which required some custom logic involving calling jinja functions. Something here were awry and I switched to using `application_file` to pass long the manifest file. I have no clue why adding in this one spark configuration caused the template rendering logic to fail but my assumption is there are some hardcoded keyword lookup in the jinja library (i hope this isnt the case)\r\n\r\nClosing the issue as I have found a work around"", 'created_at': datetime.datetime(2024, 10, 31, 18, 8, 9, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-30 15:50:40 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

gopidesupavan on (2024-10-30 22:00:21 UTC): Can you please try on the latest airflow release?

BCantos17 (Issue Creator) on (2024-10-31 18:08:09 UTC): Hi @gopidesupavan, I appreciate the response but I do not think updating to the latest release is a proper fix. I would need to get approval from the rest of the staff unless it's a patch version which I am already on the latest for 2.8.X

I did however find more details during my troubleshooting. It has more to with jinja templating than the CNCF provider. We had to use the parameter `template_spec` on the SparkKubernetesOperator to pass along the SparkApplication manifest file which required some custom logic involving calling jinja functions. Something here were awry and I switched to using `application_file` to pass long the manifest file. I have no clue why adding in this one spark configuration caused the template rendering logic to fail but my assumption is there are some hardcoded keyword lookup in the jinja library (i hope this isnt the case)

Closing the issue as I have found a work around

"
2624338395,issue,open,,Replace docker-compose breeze setup with testcontainers,"Part of `breeze` is a whole set of docker-compose files that are used to setup environment where various databases and integrations are set-up. This feature uses composability (sic!) of docker compose files and allows us to start breeze with the right set of database, integrations as docker-compose containers running together.

However there is a new tool called testcontainers https://testcontainers.com/?language=python  that seem to have similar aim, but rather than using declarative and composable docker compose yaml files, they utiise native programming language integration to launch and shut-down necessary containers during running tests. 

We could employ it to replace what breeze currently does for that part of our dev environment, and the added benefit is that you would not have to use breeze if you would like to run tests with ""postgres"", ""kafka"" etc.  You would be able to do it easily using regular unit tests. 

The part of breeze where it installs and manage the common container with all dependencies would still remain, but we could - I think fairly easily simplify and replace the part of Breeze where necessary containers are started via `--backend` and `--integration` database and plug them in pytest fixtures that could start/teardown necessary containers as needed even outside of breeze.
",potiuk,2024-10-30 14:42:30+00:00,[],2024-10-30 14:44:43+00:00,,https://github.com/apache/airflow/issues/43514,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:dev-tools', '')]",[],
2624316461,issue,closed,completed,CloudDataFusionStartPipelineOperator.partial().expand_kwargs() not working in airflow 2.9.1,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.1

### What happened?

Our dag working fine in airflow 2.5.3 , but when we created new composer 2.9.5 and airflow 2.9.1 in this same dag and code is not working , only one task is not working as we call the data dynamically from previous tasks , issue with one task i.e. CloudDataFusionStartPipelineOperator.partial(task_id='start_cdfpipeline_name',
Location='us-cenrtal-1',
Timeout=900,
).expand_kwargs(previous _task.output)
Failing as ""airflow.exception.AirflowException: Starting a pipeline failed with code 400""

### What you think should happen instead?

_No response_

### How to reproduce

Plz provide your help to fix the issue

### Operating System

Na

### Versions of Apache Airflow Providers

2.9.1

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",VinodMalleshappa,2024-10-30 14:36:41+00:00,[],2024-11-03 09:02:08+00:00,2024-11-03 09:02:08+00:00,https://github.com/apache/airflow/issues/43513,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('area:providers', ''), ('pending-response', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2447366384, 'issue_id': 2624316461, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 30, 14, 36, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453137552, 'issue_id': 2624316461, 'author': 'RNHTTR', 'body': 'What version of `airflow.providers.google.cloud.operators.datafusion` is installed in each environment?', 'created_at': datetime.datetime(2024, 11, 2, 21, 13, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453312766, 'issue_id': 2624316461, 'author': 'VinodMalleshappa', 'body': 'Not sure how to check, but we have used the from airflow.providers.google.cloud.operators.datafusion import CloudDataFusionStartPipelineOperator and we use gcp cloud composer  @RNHTTR  and the data fusion version in pipeline we use as 6.9.2 with memory 8192Mb &virtual core 4, and driver resource memory 8192 Mb, virtual core 4', 'created_at': datetime.datetime(2024, 11, 3, 6, 18, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453334775, 'issue_id': 2624316461, 'author': 'potiuk', 'body': ""You can see provider's version in the UI of Airflow. Also there is a https://airflow.apache.org/docs/apache-airflow-providers-google/stable/changelog.html changelog of Google provider which is the one that interfaces with cloud fusion.\r\n\r\nGoogle team is testing and running the providers (including system testing them), they also manage CloudDataFusion sand they are managing composer, so Composer should be your support  - especially that you have paid support with them.\r\n\r\nSo I suggest you open a support ticket with them, this does look like a problem that squarely falls into the support of the service you pay for rather than volunteer-run free open-source software that they manage.\r\n\r\nConverting it into discussion if more discussion needed."", 'created_at': datetime.datetime(2024, 11, 3, 9, 2, 3, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-30 14:36:46 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

RNHTTR on (2024-11-02 21:13:19 UTC): What version of `airflow.providers.google.cloud.operators.datafusion` is installed in each environment?

VinodMalleshappa (Issue Creator) on (2024-11-03 06:18:58 UTC): Not sure how to check, but we have used the from airflow.providers.google.cloud.operators.datafusion import CloudDataFusionStartPipelineOperator and we use gcp cloud composer  @RNHTTR  and the data fusion version in pipeline we use as 6.9.2 with memory 8192Mb &virtual core 4, and driver resource memory 8192 Mb, virtual core 4

potiuk on (2024-11-03 09:02:03 UTC): You can see provider's version in the UI of Airflow. Also there is a https://airflow.apache.org/docs/apache-airflow-providers-google/stable/changelog.html changelog of Google provider which is the one that interfaces with cloud fusion.

Google team is testing and running the providers (including system testing them), they also manage CloudDataFusion sand they are managing composer, so Composer should be your support  - especially that you have paid support with them.

So I suggest you open a support ticket with them, this does look like a problem that squarely falls into the support of the service you pay for rather than volunteer-run free open-source software that they manage.

Converting it into discussion if more discussion needed.

"
2623976417,issue,open,,Point users to a TaskInstance's Event Log if tasks are missing,"### Description

Task logs can be missing for a wide variety of reasons. Two of the most common ones are:
* The task was stuck in queued and failed by `task_queued_timeout`
* The task became a zombie

In these scenarios, it'd be helpful to populate the TaskInstance's logs with a message pointing users to the TaskInstance's Event Log.

### Related issues

* #40975 
* #42136 

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",RNHTTR,2024-10-30 12:40:18+00:00,[],2024-10-30 14:06:20+00:00,,https://github.com/apache/airflow/issues/43510,"[('area:logging', ''), ('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2447285675, 'issue_id': 2623976417, 'author': 'potiuk', 'body': 'I moved it to ""Debugging Improvements"" project @RNHTTR -> the result of the debugging survey prepared by @omkar-foss and team has been turned into a number of action points discussed there and I strongly feel (if you agree with me) that this one falls into the same ""bucket"". So far we are only discussing what could be done there and there are few small things implemented, but possibly that one should go there as well.', 'created_at': datetime.datetime(2024, 10, 30, 14, 6, 18, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-10-30 14:06:18 UTC): I moved it to ""Debugging Improvements"" project @RNHTTR -> the result of the debugging survey prepared by @omkar-foss and team has been turned into a number of action points discussed there and I strongly feel (if you agree with me) that this one falls into the same ""bucket"". So far we are only discussing what could be done there and there are few small things implemented, but possibly that one should go there as well.

"
2622991526,issue,closed,completed,Skip DB and integration tests in CI for new UI only changes,"### Description

https://github.com/apache/airflow/pull/42779#issuecomment-2399326165

This was discussed in the PR as a comment and got lost, so opening this as an issue for tracking. Currently, changes to only frontend code in airflow/ui folder still run the DB tests (takes 20 mins each) and integration tests (takes 2 mins each) which are not needed. Earlier frontend code used to be part of `airflow/www`. With new UI changes in separate folder that don't change DB components and use API from `airflow/api_fastapi` these can be skipped to reduce load on CI and also enable faster loop for frontend changes.

cc: @bbovenzi @pierrejeambrun @potiuk 

### Use case/motivation

Reduces CI usage for frontend only changes.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",tirkarthi,2024-10-30 05:52:15+00:00,[],2024-10-30 12:09:15+00:00,2024-10-30 12:09:15+00:00,https://github.com/apache/airflow/issues/43498,"[('area:CI', ""Airflow's tests and continious integration""), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2446157132, 'issue_id': 2622991526, 'author': 'potiuk', 'body': 'Sure. That should be a simple change to selective checks.\r\n\r\nIf you look at the https://github.com/apache/airflow/issues/43498 output (particularly in Bulid Info/ Selective checks) : https://github.com/apache/airflow/actions/runs/11237464231/job/31239971306?pr=42779 at the output you will see this:\r\n\r\n![image](https://github.com/user-attachments/assets/67e478d6-ac36-4ccd-bc8d-b9e775fd4096)\r\n\r\nAnd then the reason why tests is that we cannot determine which tests to run in case only ""new UI files change"" :\r\n![image](https://github.com/user-attachments/assets/fd036790-ddb6-43cf-bbdf-38c2072fd7d9)\r\n\r\nWe simply have no check to see if only new UI changed . Generally the approach we have is that we handled all the ""known"" files and depending on a file type we classify it - and if there are any change to  files we have no idea about, we run ""all tests"" - just in case.\r\n\r\nIt happens here: https://github.com/apache/airflow/blob/4fc16f154d2a159dd52955b40d5ef440d3d82425/dev/breeze/src/airflow_breeze/utils/selective_checks.py#L804\r\n\r\nSo the thing is ere that we should just add additional check here and if files changed match ""UI files"" - they should be added as ""all_ui_files"" and removed when we calculate remaining files. Then remaining files will be 0 and in turn only ""Always"" tests will run rather than all files.\r\n\r\nWe could also add the test case here https://github.com/apache/airflow/blob/4fc16f154d2a159dd52955b40d5ef440d3d82425/dev/breeze/tests/test_selective_checks.py#L853 to test that case.\r\n\r\nIf anyone would like to implement it- happy to review  :)', 'created_at': datetime.datetime(2024, 10, 30, 8, 21, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2446288806, 'issue_id': 2622991526, 'author': 'tirkarthi', 'body': 'Thanks @potiuk for the details. I will raise a PR for this.', 'created_at': datetime.datetime(2024, 10, 30, 9, 20, 29, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-10-30 08:21:34 UTC): Sure. That should be a simple change to selective checks.

If you look at the https://github.com/apache/airflow/issues/43498 output (particularly in Bulid Info/ Selective checks) : https://github.com/apache/airflow/actions/runs/11237464231/job/31239971306?pr=42779 at the output you will see this:

![image](https://github.com/user-attachments/assets/67e478d6-ac36-4ccd-bc8d-b9e775fd4096)

And then the reason why tests is that we cannot determine which tests to run in case only ""new UI files change"" :
![image](https://github.com/user-attachments/assets/fd036790-ddb6-43cf-bbdf-38c2072fd7d9)

We simply have no check to see if only new UI changed . Generally the approach we have is that we handled all the ""known"" files and depending on a file type we classify it - and if there are any change to  files we have no idea about, we run ""all tests"" - just in case.

It happens here: https://github.com/apache/airflow/blob/4fc16f154d2a159dd52955b40d5ef440d3d82425/dev/breeze/src/airflow_breeze/utils/selective_checks.py#L804

So the thing is ere that we should just add additional check here and if files changed match ""UI files"" - they should be added as ""all_ui_files"" and removed when we calculate remaining files. Then remaining files will be 0 and in turn only ""Always"" tests will run rather than all files.

We could also add the test case here https://github.com/apache/airflow/blob/4fc16f154d2a159dd52955b40d5ef440d3d82425/dev/breeze/tests/test_selective_checks.py#L853 to test that case.

If anyone would like to implement it- happy to review  :)

tirkarthi (Issue Creator) on (2024-10-30 09:20:29 UTC): Thanks @potiuk for the details. I will raise a PR for this.

"
2621782955,issue,open,,Optimize conftest.py code,Some of our conftest.py code run during test collection (particularly selecting tests to run based on pytest marker) might be slowing down test collection a lot. Likely we can achieve a lot of collection speed-up if we optimize it.,potiuk,2024-10-29 16:33:21+00:00,[],2024-11-09 18:55:46+00:00,,https://github.com/apache/airflow/issues/43488,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:CI', ""Airflow's tests and continious integration"")]","[{'comment_id': 2444797276, 'issue_id': 2621782955, 'author': 'ashb', 'body': 'To whoever picks this up: Some of the code is now in test_common/pytest_plugin.py -- (runs as same time, just not in conftest.py directly anymore)', 'created_at': datetime.datetime(2024, 10, 29, 16, 36, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2444905059, 'issue_id': 2621782955, 'author': 'ashb', 'body': 'https://github.com/zupo/awesome-pytest-speedup has some useful tips', 'created_at': datetime.datetime(2024, 10, 29, 17, 21, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2446467602, 'issue_id': 2621782955, 'author': 'ashb', 'body': 'Yup, conftest def makes a huge difference:\r\n\r\n```\r\nroot@a5bfa17bbe52:/opt/airflow# time pytest --collect-only providers/tests/ >/dev/null\r\n\r\nreal    4m15.573s\r\nuser    1m42.059s\r\nsys     0m3.675s\r\nroot@a5bfa17bbe52:/opt/airflow# time pytest --collect-only --noconftest providers/ >/dev/null\r\n\r\nreal    1m15.592s\r\nuser    1m2.304s\r\nsys     0m2.346s\r\n```', 'created_at': datetime.datetime(2024, 10, 30, 10, 21, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466410676, 'issue_id': 2621782955, 'author': 'potiuk', 'body': 'We tried it with @enisnazif  at Man\'s Hackathon and we **really** could not reproduce the ""--noconftest is faster"" .. If anything, we noticed that it is very unreliable, and sometimes takes longer and sometimes shorter - and it\'s very much independent if we use or don\'t use conftest. And we look at the  3s / 1000 tests as a benchmark (from pytest documentation ) and it does not seem too much off the charts. \r\n\r\nI guess we should park that one until we separate providers. \r\n\r\n```\r\nroot@4aca4313d70f:/opt/airflow# time pytest --collect-only providers/tests/ >/dev/null\r\n\r\nreal\t2m8.699s\r\nuser\t1m48.176s\r\nsys\t0m2.447s\r\nroot@4aca4313d70f:/opt/airflow# time pytest --collect-only providers/tests/ >/dev/null\r\n\r\nreal\t2m3.896s\r\nuser\t1m48.006s\r\nsys\t0m1.969s\r\nroot@4aca4313d70f:/opt/airflow# time pytest --collect-only --noconftest  providers/tests/ >/dev/null\r\n\r\nreal\t2m49.071s\r\nuser\t2m22.759s\r\nsys\t0m2.963s\r\nroot@4aca4313d70f:/opt/airflow# time pytest --collect-only --noconftest  providers/tests/ >/dev/null\r\n\r\nreal\t3m26.905s\r\nuser\t2m23.507s\r\nsys\t0m2.877s\r\n```', 'created_at': datetime.datetime(2024, 11, 9, 18, 55, 44, tzinfo=datetime.timezone.utc)}]","ashb on (2024-10-29 16:36:35 UTC): To whoever picks this up: Some of the code is now in test_common/pytest_plugin.py -- (runs as same time, just not in conftest.py directly anymore)

ashb on (2024-10-29 17:21:37 UTC): https://github.com/zupo/awesome-pytest-speedup has some useful tips

ashb on (2024-10-30 10:21:33 UTC): Yup, conftest def makes a huge difference:

```
root@a5bfa17bbe52:/opt/airflow# time pytest --collect-only providers/tests/ >/dev/null

real    4m15.573s
user    1m42.059s
sys     0m3.675s
root@a5bfa17bbe52:/opt/airflow# time pytest --collect-only --noconftest providers/ >/dev/null

real    1m15.592s
user    1m2.304s
sys     0m2.346s
```

potiuk (Issue Creator) on (2024-11-09 18:55:44 UTC): We tried it with @enisnazif  at Man's Hackathon and we **really** could not reproduce the ""--noconftest is faster"" .. If anything, we noticed that it is very unreliable, and sometimes takes longer and sometimes shorter - and it's very much independent if we use or don't use conftest. And we look at the  3s / 1000 tests as a benchmark (from pytest documentation ) and it does not seem too much off the charts. 

I guess we should park that one until we separate providers. 

```
root@4aca4313d70f:/opt/airflow# time pytest --collect-only providers/tests/ >/dev/null

real	2m8.699s
user	1m48.176s
sys	0m2.447s
root@4aca4313d70f:/opt/airflow# time pytest --collect-only providers/tests/ >/dev/null

real	2m3.896s
user	1m48.006s
sys	0m1.969s
root@4aca4313d70f:/opt/airflow# time pytest --collect-only --noconftest  providers/tests/ >/dev/null

real	2m49.071s
user	2m22.759s
sys	0m2.963s
root@4aca4313d70f:/opt/airflow# time pytest --collect-only --noconftest  providers/tests/ >/dev/null

real	3m26.905s
user	2m23.507s
sys	0m2.877s
```

"
2621687269,issue,open,,GoogleAdsToGcsOperator sends default headers which is no longer supported by the API,"### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

10.21.1

### Apache Airflow version

Version 2.0 

### Operating System

Debian GNU/Linux 12 (bookworm)

### Deployment

Docker-Compose

### Deployment details

Pretty standard `docker-compose up`. In the docker compose mounted the volumes to the standard directories and set some variables to be loaded into xcoms (eg user, gcs_conn_id). Nothing's customised otherwise. 

### What happened

I wanted to test `GoogleAdsToGcsOperator` and I wrote a GAQL query to hit the customer_client resource and the task failed with the following error:

```
Fault: errors {
  error_code {
    request_error: PAGE_SIZE_NOT_SUPPORTED
  }
  message: ""Setting the page size is not supported. Search Responses will have fixed page size of \'10000\' rows.""
}
```

I did not specify any page size (this is initialised [here](https://github.com/apache/airflow/blob/755c10b99752b1868eb410a00563eff537300e91/providers/src/airflow/providers/google/ads/transfers/ads_to_gcs.py#L86C9-L86C32))

This was the task I ran in my DAG:


```
# I get these from xcoms. It's loaded in with docker-compose
DEFAULT_ARGS = {
    # connections
    'gcp_conn_id': 'google_cloud_default', 
    'google_ads_conn_id': 'google_ads_conn',
    'retries': 2
}

GoogleAdsToGcsOperator(
        task_id='get_google_ads_sub_accounts',
        bucket=BUCKET,
        obj=file_path,
        api_version=API_VERSION, # v17
        gzip=False,
        # default_args=DEFAULT_ARGS,
        client_ids=[MCC_ACCOUNT_ID], # MCC accountID
        attributes=[
            ""customer_client.descriptive_name""
            , ""customer_client.id""
            , ""customer_client.manager""
            , ""customer.id""
            , ""customer_client.hidden""
            , ""customer_client.status""
        ],
        query=""""""
            SELECT customer_client.descriptive_name
                 , customer_client.id
                 , customer_client.manager
                 , customer.id
                 , customer_client.hidden
                 , customer_client.status
            FROM customer_client
            WHERE customer_client.status NOT IN ('CANCELED', 'CLOSED')
            """"""
    )

```

### What you think should happen instead

a file should be created in my GCS bucket with the request response. 

### How to reproduce

If you run the following task with your connection ID information you would probably get the same error. Do not set the page_size parameter so that the __init__ func passes the default. 

### Anything else

The entire request response headers are:
```
Headers: {
  ""google.ads.googleads.v17.errors.googleadsfailure-bin"": ""\ni\n\u0002\b(\u0012cSetting the page size is not supported. Search Responses will have fixed page size of '10000' rows.\u0012\u0016kUF6eMWJI6-xDnmxB7Dhww"",
  ""grpc-status-details-bin"": ""\b\u0003\u0012%Request contains an invalid argument.\u001a\u0001\nDtype.googleapis.com/google.ads.googleads.v17.errors.GoogleAdsFailure\u0012\u0001\ni\n\u0002\b(\u0012cSetting the page size is not supported. Search Responses will have fixed page size of '10000' rows.\u0012\u0016kUF6eMWJI6-xDnmxB7Dhww"",
  ""request-id"": """"
}
Fault: errors {
  error_code {
    request_error: PAGE_SIZE_NOT_SUPPORTED
  }
  message: ""Setting the page size is not supported. Search Responses will have fixed page size of \'10000\' rows.""
}
```

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",shervinrad100,2024-10-29 15:54:19+00:00,['shervinrad100'],2024-10-30 18:03:50+00:00,,https://github.com/apache/airflow/issues/43486,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('area:providers', '')]","[{'comment_id': 2444688390, 'issue_id': 2621687269, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 29, 15, 54, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2444813061, 'issue_id': 2621687269, 'author': 'bugraoz93', 'body': '@shervinrad100 \r\nYes, indeed, this was changed in the last release of the API. I saw you want to commit to submitting a PR. I can assign it to you if you want. My only nit is to be careful with older API versions, as the code should be backwards compatible with all versions.\r\n\r\n> Note: Setting a page_size in version v17 of the Google Ads API will return an error. The earlier versions will change start throwing errors on August 12, 2024. This field will be removed in a future version of the Google Ads API. See the announcement.\r\n\r\nhttps://developers.google.com/google-ads/api/docs/reporting/paging', 'created_at': datetime.datetime(2024, 10, 29, 16, 43, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2444831625, 'issue_id': 2621687269, 'author': 'bugraoz93', 'body': 'In the documents, it seems like they will drop this for earlier versions as well. Never mind about the backward compatibility part. I think they are already deprecating older versions of their API. \r\nhttps://developers.google.com/google-ads/api/docs/sunset-dates', 'created_at': datetime.datetime(2024, 10, 29, 16, 50, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2444848974, 'issue_id': 2621687269, 'author': 'shervinrad100', 'body': '>  I saw you want to commit to submitting a PR. I can assign it to you if you want\r\n\r\nYes please would love to contribute and this seems like a quick fix anyway. \r\n\r\nI was thinking of simply removing the default assignment. If you want backward compatibility then I can add the following to the __init__:\r\n```\r\ndef __init__(...\r\n...\r\nself.api_version = api_version\r\nif self.api_version < 17:\r\n    self.page_size = page_size or 10000\r\n    # warning message on deprecation. something like:\r\n    # warnings.warn(""Setting a page_size in version v17 of the Google Ads API will return an error. The earlier versions will change start throwing errors on August 12, 2024."")\r\n\r\n   \r\n```', 'created_at': datetime.datetime(2024, 10, 29, 16, 57, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445332327, 'issue_id': 2621687269, 'author': 'potiuk', 'body': '> I was thinking of simply removing the default assignment. \r\n\r\nThis is already done and (just) merged via #43474', 'created_at': datetime.datetime(2024, 10, 29, 21, 7, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445428618, 'issue_id': 2621687269, 'author': 'shervinrad100', 'body': '@potiuk I might be wrong here but I believe [#43474](https://github.com/apache/airflow/pull/43474) would force us to declare the API version but would still assign page_size as default which can cause the error. \r\n\r\nI believe the page_size default assignment should also be removed.', 'created_at': datetime.datetime(2024, 10, 29, 22, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445490957, 'issue_id': 2621687269, 'author': 'bugraoz93', 'body': '> This is already done and (just) merged via #43474\r\n\r\nI think they deprecated this part where we are passing the `page_size` in the API call. [ads_to_gcs.py\r\n](https://github.com/apache/airflow/blob/755c10b99752b1868eb410a00563eff537300e91/providers/src/airflow/providers/google/ads/transfers/ads_to_gcs.py#L111)', 'created_at': datetime.datetime(2024, 10, 29, 23, 13, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445500753, 'issue_id': 2621687269, 'author': 'bugraoz93', 'body': ""> If you want backward compatibility then I can add the following to the init:\r\n\r\nI think we already passed the 12 of August 2024 so I don't think we should support any backward compatibility in this case. This should already deprecated for all the APIs. Maybe we don't even need to pass any `page_size` at all."", 'created_at': datetime.datetime(2024, 10, 29, 23, 21, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2446406298, 'issue_id': 2621687269, 'author': 'shervinrad100', 'body': ""@bugraoz93  If you assign the issue to me I'd be happy to push a PR today"", 'created_at': datetime.datetime(2024, 10, 30, 10, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2446762605, 'issue_id': 2621687269, 'author': 'potiuk', 'body': 'Feel free.', 'created_at': datetime.datetime(2024, 10, 30, 11, 37, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2447960111, 'issue_id': 2621687269, 'author': 'shervinrad100', 'body': 'ready for your review ✅', 'created_at': datetime.datetime(2024, 10, 30, 18, 3, 49, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-29 15:54:22 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

bugraoz93 on (2024-10-29 16:43:04 UTC): @shervinrad100 
Yes, indeed, this was changed in the last release of the API. I saw you want to commit to submitting a PR. I can assign it to you if you want. My only nit is to be careful with older API versions, as the code should be backwards compatible with all versions.


https://developers.google.com/google-ads/api/docs/reporting/paging

bugraoz93 on (2024-10-29 16:50:41 UTC): In the documents, it seems like they will drop this for earlier versions as well. Never mind about the backward compatibility part. I think they are already deprecating older versions of their API. 
https://developers.google.com/google-ads/api/docs/sunset-dates

shervinrad100 (Issue Creator) on (2024-10-29 16:57:55 UTC): Yes please would love to contribute and this seems like a quick fix anyway. 

I was thinking of simply removing the default assignment. If you want backward compatibility then I can add the following to the __init__:
```
def __init__(...
...
self.api_version = api_version
if self.api_version < 17:
    self.page_size = page_size or 10000
    # warning message on deprecation. something like:
    # warnings.warn(""Setting a page_size in version v17 of the Google Ads API will return an error. The earlier versions will change start throwing errors on August 12, 2024."")

   
```

potiuk on (2024-10-29 21:07:16 UTC): This is already done and (just) merged via #43474

shervinrad100 (Issue Creator) on (2024-10-29 22:16:00 UTC): @potiuk I might be wrong here but I believe [#43474](https://github.com/apache/airflow/pull/43474) would force us to declare the API version but would still assign page_size as default which can cause the error. 

I believe the page_size default assignment should also be removed.

bugraoz93 on (2024-10-29 23:13:14 UTC): I think they deprecated this part where we are passing the `page_size` in the API call. [ads_to_gcs.py
](https://github.com/apache/airflow/blob/755c10b99752b1868eb410a00563eff537300e91/providers/src/airflow/providers/google/ads/transfers/ads_to_gcs.py#L111)

bugraoz93 on (2024-10-29 23:21:01 UTC): I think we already passed the 12 of August 2024 so I don't think we should support any backward compatibility in this case. This should already deprecated for all the APIs. Maybe we don't even need to pass any `page_size` at all.

shervinrad100 (Issue Creator) on (2024-10-30 10:03:00 UTC): @bugraoz93  If you assign the issue to me I'd be happy to push a PR today

potiuk on (2024-10-30 11:37:11 UTC): Feel free.

shervinrad100 (Issue Creator) on (2024-10-30 18:03:49 UTC): ready for your review ✅

"
2621538186,issue,open,,Task end_date prevents downstream tasks from running,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.7.1

### What happened?

I have a task with multiple upstream dependencies, one of which is a deprecated dataset.  To retire this dataset, I set its task's `end_date` to the last day that the data was available, and I updated the code for the downstream task to stop looking for that data.  Now, the downstream task is stuck in ""No status"" and must be manually started from the command line.

The only `trigger_rule`s that work around this issue are ones that have the wrong semantics for my task (e.g. `always`).  I also want to make sure we can still backfill the deprecated dataset if needed, so I want to keep the DAG structure and dependencies backward compatible.

### What you think should happen instead?

In descending order of personal preference:

1. The downstream task should only be prevented from running if _all_ of its upstream dependencies are outside of their `start_date`/`end_date` window, not just one.
2. A task outside of its date window should behave like a ""skipped"" task, propagating down to children but allowing downstream behavior to be controlled via `trigger_rule`s.
3. If the scheduling behavior cannot be changed, perhaps a workaround would be to expose the `logical_date` at the DAG level via `AirflowParsingContext`, so that dependencies could be conditionally changed:
```
if deprecated_task.start_date <= get_parsing_context().logical_date <= deprecated_task.end_date:
    deprecated_task >> downstream_task
```

### How to reproduce

```
#!/usr/bin/env python3
import datetime
import logging

from airflow.decorators import dag
from airflow.models.baseoperator import cross_downstream
from airflow.operators.empty import EmptyOperator
from airflow.utils.task_group import TaskGroup


logger = logging.getLogger(__name__)


@dag(
    schedule='@daily',
    start_date=datetime.datetime(2024, 10, 21),
)
def test_task_end_date():
    with TaskGroup(group_id='single_dep'):
        (
            EmptyOperator(task_id='upstream')
            >> EmptyOperator(task_id='expired', end_date=datetime.datetime(2024, 10, 24))
            >> EmptyOperator(task_id='downstream')
        )

    with TaskGroup(group_id='multiple_deps'):
        [
            EmptyOperator(task_id='upstream_1'),
            EmptyOperator(task_id='upstream_2'),
            EmptyOperator(task_id='expired', end_date=datetime.datetime(2024, 10, 24)),
        ] >> EmptyOperator(task_id='downstream')

    with TaskGroup(group_id='trigger_rules'):
        cross_downstream([
            EmptyOperator(task_id='upstream_1'),
            EmptyOperator(task_id='upstream_2'),
            EmptyOperator(task_id='expired', end_date=datetime.datetime(2024, 10, 24)),
        ], [
            EmptyOperator(task_id='all_success'),
            EmptyOperator(task_id='all_done', trigger_rule='all_done'),
            EmptyOperator(task_id='one_success', trigger_rule='one_success'),
            EmptyOperator(task_id='none_failed', trigger_rule='none_failed'),
            EmptyOperator(task_id='none_failed_min_one_success', trigger_rule='none_failed_min_one_success'),
            EmptyOperator(task_id='none_skipped', trigger_rule='none_skipped'),
            EmptyOperator(task_id='always', trigger_rule='always'),
        ])


dag = test_task_end_date()


if __name__ == '__main__':
    dag.cli()
```

![Screenshot from 2024-10-29 09-59-27](https://github.com/user-attachments/assets/510bae83-50b8-41a8-8443-b943bcd85986)


### Operating System

CentOS Stream 8

### Versions of Apache Airflow Providers

N/A

### Deployment

Other

### Deployment details

Self-hosted/standalone

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Gollum999,2024-10-29 15:05:17+00:00,[],2024-11-04 18:42:50+00:00,,https://github.com/apache/airflow/issues/43484,"[('kind:feature', 'Feature Requests'), ('area:core', ''), ('airflow3.x candidate', 'Candidates for Airlfow 3.x (beyond Airflow 3.0)')]","[{'comment_id': 2453146103, 'issue_id': 2621538186, 'author': 'RNHTTR', 'body': 'Since this would be adding functionality, I think this is a feature request and not a bug. I think skipping the task instance whose `end_date` is passed makes sense.\r\n\r\nTo work around this, would it make sense to remove the deprecated task from the Dag? Also, the `logical_date` is already available via [task instance context](https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html#variables).', 'created_at': datetime.datetime(2024, 11, 2, 21, 21, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455447045, 'issue_id': 2621538186, 'author': 'Gollum999', 'body': ""In my case, I do not want to remove the deprecated task for at least a few months because we may still need to backfill the data that it produces.\r\n\r\nUsing `logical_date` inside of the task instance is a good idea, I had not thought of that.  I'd imagine I can conditionally `raise AirflowSkipException` to get the behavior I am looking for.  Thanks!"", 'created_at': datetime.datetime(2024, 11, 4, 18, 42, 48, tzinfo=datetime.timezone.utc)}]","RNHTTR on (2024-11-02 21:21:47 UTC): Since this would be adding functionality, I think this is a feature request and not a bug. I think skipping the task instance whose `end_date` is passed makes sense.

To work around this, would it make sense to remove the deprecated task from the Dag? Also, the `logical_date` is already available via [task instance context](https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html#variables).

Gollum999 (Issue Creator) on (2024-11-04 18:42:48 UTC): In my case, I do not want to remove the deprecated task for at least a few months because we may still need to backfill the data that it produces.

Using `logical_date` inside of the task instance is a good idea, I had not thought of that.  I'd imagine I can conditionally `raise AirflowSkipException` to get the behavior I am looking for.  Thanks!

"
2621531354,issue,closed,completed,AIP-38 | Migrate UI to Chakra v3,"[Chakra v3 is out ](https://www.chakra-ui.com/blog/00-announcing-v3). We should try to move the new UI to use it to avoid the pain of migrating later. By the time Airflow 3 is released, chakra will certainly have had a few patch releases.",bbovenzi,2024-10-29 15:02:57+00:00,['bbovenzi'],2024-11-01 17:43:49+00:00,2024-11-01 17:43:49+00:00,https://github.com/apache/airflow/issues/43483,"[('area:UI', 'Related to UI/UX. For Frontend Developers.')]",[],
2621469581,issue,closed,completed,Add KubernetesPodOperator's `name` in templated fields,"### Description

Unless it's been kept out templated fields on purpose, I would like to add the param `name` of KubernetesPodOperator into template_fields.

My guess is that we wished to protect the pod name and guarantee the right syntax via `KubernetesPodOperator._set_name`. But I think we could have the best of both worlds by adding `name` to template_fields and have the protected name through something like `self.pod_name = self._set_pod_name(name)`, or equivalent.

Would that be worth a shot? I'm willing to submit a short PR about this.

Thanks a lot!

### Use case/motivation

The current use-case is when we explore pods from the Kubernetes side, it's easy to have `pod_name=task_id` but that's not sufficient when we have several DAGs with identical tasks in them.
Discriminant and dynamic information can be passed to labels or annotations since they're templated fields, but the pod name isn't.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",anteverse,2024-10-29 14:41:25+00:00,['insomnes'],2025-02-01 13:30:04+00:00,2025-02-01 13:30:04+00:00,https://github.com/apache/airflow/issues/43480,"[('area:providers', ''), ('kind:feature', 'Feature Requests'), ('good first issue', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]","[{'comment_id': 2445322895, 'issue_id': 2621469581, 'author': 'potiuk', 'body': 'Sure. Go ahead.', 'created_at': datetime.datetime(2024, 10, 29, 21, 1, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2623240258, 'issue_id': 2621469581, 'author': 'insomnes', 'body': ""@eilon246810 hi! I am very interested in this feature, so I have created my PR based on your previous one (with some extra tests). \n\nLet me know if you'd like to recreate it yourself."", 'created_at': datetime.datetime(2025, 1, 30, 0, 22, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2628049561, 'issue_id': 2621469581, 'author': 'insomnes', 'body': 'Fixing the problem with tests led me to the bug in `SparkKubernetesOperator` tests, so I have fixed them in addition to this feature too.', 'created_at': datetime.datetime(2025, 1, 31, 18, 34, 44, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-10-29 21:01:12 UTC): Sure. Go ahead.

insomnes (Assginee) on (2025-01-30 00:22:45 UTC): @eilon246810 hi! I am very interested in this feature, so I have created my PR based on your previous one (with some extra tests). 

Let me know if you'd like to recreate it yourself.

insomnes (Assginee) on (2025-01-31 18:34:44 UTC): Fixing the problem with tests led me to the bug in `SparkKubernetesOperator` tests, so I have fixed them in addition to this feature too.

"
2621292078,issue,closed,completed,AIP-84 Add ability to update dag run note in PATCH dag_run endpoint,"### Description

updating dag run note when updating the dag run state can be helpful as suggested by @bbovenzi. Related comment - https://github.com/apache/airflow/pull/42973#discussion_r1820795519

### Use case/motivation

_No response_

### Related issues

#42973

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rawwar,2024-10-29 13:38:31+00:00,['rawwar'],2024-11-08 13:08:58+00:00,2024-11-08 13:08:58+00:00,https://github.com/apache/airflow/issues/43476,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]",[],
2621203149,issue,closed,completed,Trigger Rule `all_done` does not honor the dag_run marked failed manually and brings back the dag_run to running.,"### Apache Airflow version

main (development)

### What happened?

If a dag_run is marked failed before a task which has trigger rule as `all_done` then it will automatically bring the dag_run back to running state and will run the task with trigger rule `all_done`.


https://github.com/user-attachments/assets/ad934322-ff4f-4aa5-b47a-8d08aaaf9065



### What you think should happen instead?

Failing dag_run from UI should be absolute. The dag_run should not come to running state on its own until unless cleared.

### How to reproduce

With the below simple DAG, when the tasks 1,2 & 3 are running mark the **`dag_run`** as failed.
Wait for a few seconds and refresh the WebUI and you'll notice that the final_task comes running.

```
from airflow.decorators import dag, task
import pendulum
import time

@dag(
    dag_id='trigger_rule_demo',
    schedule=None,
    start_date=pendulum.datetime(2024, 1, 1),
    catchup=False,
    default_args={'retries': 0}
)
def trigger_rule_showcase():
    
    @task
    def task_1():
        time.sleep(30)
        return ""Task 1 done""
    
    @task
    def task_2():
        time.sleep(45)
        return ""Task 2 done""
    
    @task
    def task_3():
        time.sleep(60)
        return ""Task 3 done""
    
    @task(trigger_rule='all_done')
    def final_task():
        return ""Final task done""

    [task_1(), task_2(), task_3()] >> final_task()

dag = trigger_rule_showcase()
```

### Operating System

ProductName:            macOS ProductVersion:         15.0.1 BuildVersion:           24A348


### Deployment

Virtualenv installation


### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",abhishekbhakat,2024-10-29 13:07:47+00:00,[],2024-10-29 17:46:24+00:00,2024-10-29 17:46:24+00:00,https://github.com/apache/airflow/issues/43475,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2621153345,issue,open,,Adding tags to DAG runs,"### Description

Currently I see only option to tag DAGs. I don't see such an option for DAG runs.
It will be quite useful for me, to add and filter runs by tags.

### Use case/motivation

I'd like to filter runs based on tags. For example, I have different DAGs  (predict, train, etc.) that I could tag with tags like 'model_name=mymodel', then I can see all runs that used that model.
This also gives the user a way to group dagruns based on keys like experiment=name, such management exists in other dag runners (e.g kubeflow)

### Related issues

_No response_

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",asaff1,2024-10-29 12:51:08+00:00,[],2024-10-30 15:25:19+00:00,,https://github.com/apache/airflow/issues/43472,"[('kind:feature', 'Feature Requests')]","[{'comment_id': 2444125925, 'issue_id': 2621153345, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 29, 12, 51, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445296724, 'issue_id': 2621153345, 'author': 'potiuk', 'body': ""Yep. Would be a great feature to have. Maybe someone will pick it up and implement (but FYI. The fastest way to get something like tha timplemented, is to contribute it, second fastest is to find someone who will contribute it  - and for example pay them to do so - other than that someone will have to volunteer and implement it - and since this is an open-source project, it's on a discretion of whoever signs up and take that one."", 'created_at': datetime.datetime(2024, 10, 29, 20, 45, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445356673, 'issue_id': 2621153345, 'author': 'asaff1', 'body': '@potiuk I might be able to contribute it. I can submit a PR. Any guidelines for that?\r\n\r\nAs I see what needs to be done, is to add a new DB model `DagRunTag` with key, value fields (in models/dagrun.py),\r\n then adding a many to many relationship between `DagRun` and `DagRunTag`.\r\nThen I need to add CRUD operations to the API, and finally for the UI I need to add a filtering / search by tags.\r\nIs that a good direction?', 'created_at': datetime.datetime(2024, 10, 29, 21, 23, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445394498, 'issue_id': 2621153345, 'author': 'potiuk', 'body': 'Yep. And it could be done in stages as well You could even add the list of things to do in the description of this issue with \r\n\r\n- [ ] thing to do\r\n- [ ] another thing to do\r\n\r\nAnd check them one-by-one while doing it. \r\n\r\nAlso I think that one needs a say from @bbovenzi and @pierrejeambrun from the UI perspective, because if we implement something like that, it needs to someohow fit into navigation patterns in the new UI \r\n\r\n(Note that this might be Airflow 3 - only change anyway)', 'created_at': datetime.datetime(2024, 10, 29, 21, 49, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2447289071, 'issue_id': 2621153345, 'author': 'pierrejeambrun', 'body': 'Interesting. I think we can make that work in the UI in terms of display and filtering in the DagRun table.\r\n\r\nThe thing I am wondering is how and when will tags be added to a specific dagrun ? For dags it is a dag authoring time, which makes sense, for `dag_runs` it is less clear to me. Will a user need  to do that manually, or do the runs inherit the `DAG` tags by default, etc, etc', 'created_at': datetime.datetime(2024, 10, 30, 14, 7, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2447402407, 'issue_id': 2621153345, 'author': 'asaff1', 'body': '@pierrejeambrun For my use case it is beneficial enough to set tags at creation time, or manually after creation (from the UI / API).', 'created_at': datetime.datetime(2024, 10, 30, 14, 48, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2447437378, 'issue_id': 2621153345, 'author': 'bbovenzi', 'body': ""First, before we add a new table. Let's see if we can use or improve existing fields to achieve this.\r\n\r\n- Filter the dags for which you want to see dag runs. filter dags by tags then show me runs from only those dags\r\n- I wonder if we can filter by dag_run.conf values."", 'created_at': datetime.datetime(2024, 10, 30, 15, 0, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2447510452, 'issue_id': 2621153345, 'author': 'pierrejeambrun', 'body': '> For my use case it is beneficial enough to set tags at creation time, or manually after creation (from the UI / API).\r\n\r\n`Set tags at creation time` what do you mean by that ?', 'created_at': datetime.datetime(2024, 10, 30, 15, 19, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2447540086, 'issue_id': 2621153345, 'author': 'asaff1', 'body': '@pierrejeambrun I mean, setting tags when manually triggering DAGs via the UI, or by the REST API. (So I mean, at the creation time of dag runs.)', 'created_at': datetime.datetime(2024, 10, 30, 15, 25, 18, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-29 12:51:11 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-10-29 20:45:48 UTC): Yep. Would be a great feature to have. Maybe someone will pick it up and implement (but FYI. The fastest way to get something like tha timplemented, is to contribute it, second fastest is to find someone who will contribute it  - and for example pay them to do so - other than that someone will have to volunteer and implement it - and since this is an open-source project, it's on a discretion of whoever signs up and take that one.

asaff1 (Issue Creator) on (2024-10-29 21:23:35 UTC): @potiuk I might be able to contribute it. I can submit a PR. Any guidelines for that?

As I see what needs to be done, is to add a new DB model `DagRunTag` with key, value fields (in models/dagrun.py),
 then adding a many to many relationship between `DagRun` and `DagRunTag`.
Then I need to add CRUD operations to the API, and finally for the UI I need to add a filtering / search by tags.
Is that a good direction?

potiuk on (2024-10-29 21:49:46 UTC): Yep. And it could be done in stages as well You could even add the list of things to do in the description of this issue with 

- [ ] thing to do
- [ ] another thing to do

And check them one-by-one while doing it. 

Also I think that one needs a say from @bbovenzi and @pierrejeambrun from the UI perspective, because if we implement something like that, it needs to someohow fit into navigation patterns in the new UI 

(Note that this might be Airflow 3 - only change anyway)

pierrejeambrun on (2024-10-30 14:07:36 UTC): Interesting. I think we can make that work in the UI in terms of display and filtering in the DagRun table.

The thing I am wondering is how and when will tags be added to a specific dagrun ? For dags it is a dag authoring time, which makes sense, for `dag_runs` it is less clear to me. Will a user need  to do that manually, or do the runs inherit the `DAG` tags by default, etc, etc

asaff1 (Issue Creator) on (2024-10-30 14:48:17 UTC): @pierrejeambrun For my use case it is beneficial enough to set tags at creation time, or manually after creation (from the UI / API).

bbovenzi on (2024-10-30 15:00:33 UTC): First, before we add a new table. Let's see if we can use or improve existing fields to achieve this.

- Filter the dags for which you want to see dag runs. filter dags by tags then show me runs from only those dags
- I wonder if we can filter by dag_run.conf values.

pierrejeambrun on (2024-10-30 15:19:25 UTC): `Set tags at creation time` what do you mean by that ?

asaff1 (Issue Creator) on (2024-10-30 15:25:18 UTC): @pierrejeambrun I mean, setting tags when manually triggering DAGs via the UI, or by the REST API. (So I mean, at the creation time of dag runs.)

"
2620741129,issue,closed,not_planned,Incorrect Rendering of Error for DBT Output in Airflow Logs,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.1

### What happened?

A dbt run concluded successfully and the logs rendered as if there was an error:
![image](https://github.com/user-attachments/assets/6312db21-848c-4b2c-bbf7-f9213d4e0cec)

This likely happened because of the word ""ERROR"" in the logs.

### What you think should happen instead?

The log should not be rendered as if there was no error

### How to reproduce

Run this code on an airflow instance (in my case I used astro to reproduce the error seen in MWAA):

```python
from airflow.decorators import dag, task
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

# Define the basic parameters of the DAG, like schedule and start_date
@dag(
    start_date=datetime(2024, 1, 1),
    schedule=""@daily"",
    catchup=False,
)
def error_render():
    @task(task_id=""render_error"")
    def log_error():
        logger.info(""Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1"")

    log_error()

error_render()
```

I obtained the following output:

![image](https://github.com/user-attachments/assets/58b4f68e-f428-481e-ac5f-3b7ac0110366)


### Operating System

Amazon Linux 2023

### Versions of Apache Airflow Providers

_No response_

### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",GabrielEisenbergOlympus,2024-10-29 09:57:24+00:00,['mithindev'],2024-12-09 00:17:13+00:00,2024-12-09 00:17:13+00:00,https://github.com/apache/airflow/issues/43465,"[('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('area:logging', ''), ('pending-response', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2443753728, 'issue_id': 2620741129, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 29, 9, 57, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2444981664, 'issue_id': 2620741129, 'author': 'potiuk', 'body': 'seems like coloring is taken from the ERROR specified in the message, nice good first issue for someone to take a look at.', 'created_at': datetime.datetime(2024, 10, 29, 17, 58, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2449710801, 'issue_id': 2620741129, 'author': 'mithindev', 'body': 'Hi @potiuk, I’d like to work on it.', 'created_at': datetime.datetime(2024, 10, 31, 12, 17, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2449858316, 'issue_id': 2620741129, 'author': 'mithindev', 'body': '## Error Reproduction\r\n\r\nI successfully reproduced the error:\r\n\r\n![Error Screenshot](https://github.com/user-attachments/assets/30338020-c669-4a28-9650-ddeb164fd402)\r\n\r\nI am currently working on a solution.', 'created_at': datetime.datetime(2024, 10, 31, 13, 34, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452881556, 'issue_id': 2620741129, 'author': 'mithindev', 'body': 'Hello @nathadfield,\r\n\r\nI wanted to get a bit more context regarding the expected outcome for this task. Based on my current investigation, it appears that the coloring is applied by matching specific keywords like “error,” “warn,” or “exception.” However, this approach seems limited, as it doesn’t account for other relevant keywords, such as “Failed,” which might also warrant a red color.\r\n\r\nCould you clarify if the goal is to apply color based on log states rather than just matching specific words? Should I proceed with an approach that aligns coloring with log states or levels instead?', 'created_at': datetime.datetime(2024, 11, 2, 5, 54, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453500983, 'issue_id': 2620741129, 'author': 'jscheffl', 'body': 'This is a side effect of a ""feature"" being introduced in Airflow 2.10.0 All lines are colored based on configurable keywords. Keywords can be configured via https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#color-log-error-keywords and https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#color-log-warning-keywords\r\n\r\nSo this is not an issue, except if you dislike the feature and want to revert it. Otherwise I\'d propose to adjust te keywords if it botheres you. (I don\'t like it for most 80% of cases but acknowledge for some it is useful).\r\n\r\nTherefore... it is not a bug.\r\n\r\nFor reference it was listed in the release notes in https://airflow.apache.org/docs/apache-airflow/stable/release_notes.html#new-features - The PR which added the feature was https://github.com/apache/airflow/pull/39006', 'created_at': datetime.datetime(2024, 11, 3, 17, 16, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2478752894, 'issue_id': 2620741129, 'author': 'GabrielEisenbergOlympus', 'body': ""Hi @jscheffl, perhaps there could be a little more nuance as to how errors etc are highlighted. This could get rather complex, but I'll suggest it anyway.\r\n\r\nYou could generate error highlighting based on the underlying technologies being used in a task. In that way, you could have custom highlighting that gets defined at a task level. \r\n\r\nIn this case, dbt outputs the outcome in a consistent manner as shown above. So perhaps the highlighting for the task could be tuned based on custom regex. In this case, the output line could be ignored in highlighting. Is this overkill for something simple? Probably. \r\n\r\nDo with this what you will 😆"", 'created_at': datetime.datetime(2024, 11, 15, 12, 55, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2480499855, 'issue_id': 2620741129, 'author': 'jscheffl', 'body': ""> Hi @jscheffl, perhaps there could be a little more nuance as to how errors etc are highlighted. This could get rather complex, but I'll suggest it anyway.\r\n\r\nYes, this is what I feared. Also I was not a big fan of the feature first-place.\r\n\r\nSo what I wanted to say is: It will be complex. Or at least medium-complex. As logs are rendered on the client side and can also be a couple of megabytes which are re-loaded time-over time it need to be implemented in a way not crashing browser by overload. And it need to be a bit more clever than the current mechanism. Or the current mechanism needs to be reverted... which can also be that you just change your configuration to disable actually if it bothers you.\r\n\r\nNote that the UI is in complete rework, it is planned to re-implement it fully with Airflow 3. So an improvement will most likely only be relevant for Airflow 2.10/2.11 line.\r\n\r\nLooking forward for your proposal!"", 'created_at': datetime.datetime(2024, 11, 16, 9, 59, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2509489651, 'issue_id': 2620741129, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 12, 1, 0, 19, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2526534126, 'issue_id': 2620741129, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 12, 9, 0, 17, 12, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-29 09:57:27 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-10-29 17:58:56 UTC): seems like coloring is taken from the ERROR specified in the message, nice good first issue for someone to take a look at.

mithindev (Assginee) on (2024-10-31 12:17:31 UTC): Hi @potiuk, I’d like to work on it.

mithindev (Assginee) on (2024-10-31 13:34:36 UTC): ## Error Reproduction

I successfully reproduced the error:

![Error Screenshot](https://github.com/user-attachments/assets/30338020-c669-4a28-9650-ddeb164fd402)

I am currently working on a solution.

mithindev (Assginee) on (2024-11-02 05:54:46 UTC): Hello @nathadfield,

I wanted to get a bit more context regarding the expected outcome for this task. Based on my current investigation, it appears that the coloring is applied by matching specific keywords like “error,” “warn,” or “exception.” However, this approach seems limited, as it doesn’t account for other relevant keywords, such as “Failed,” which might also warrant a red color.

Could you clarify if the goal is to apply color based on log states rather than just matching specific words? Should I proceed with an approach that aligns coloring with log states or levels instead?

jscheffl on (2024-11-03 17:16:09 UTC): This is a side effect of a ""feature"" being introduced in Airflow 2.10.0 All lines are colored based on configurable keywords. Keywords can be configured via https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#color-log-error-keywords and https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#color-log-warning-keywords

So this is not an issue, except if you dislike the feature and want to revert it. Otherwise I'd propose to adjust te keywords if it botheres you. (I don't like it for most 80% of cases but acknowledge for some it is useful).

Therefore... it is not a bug.

For reference it was listed in the release notes in https://airflow.apache.org/docs/apache-airflow/stable/release_notes.html#new-features - The PR which added the feature was https://github.com/apache/airflow/pull/39006

GabrielEisenbergOlympus (Issue Creator) on (2024-11-15 12:55:32 UTC): Hi @jscheffl, perhaps there could be a little more nuance as to how errors etc are highlighted. This could get rather complex, but I'll suggest it anyway.

You could generate error highlighting based on the underlying technologies being used in a task. In that way, you could have custom highlighting that gets defined at a task level. 

In this case, dbt outputs the outcome in a consistent manner as shown above. So perhaps the highlighting for the task could be tuned based on custom regex. In this case, the output line could be ignored in highlighting. Is this overkill for something simple? Probably. 

Do with this what you will 😆

jscheffl on (2024-11-16 09:59:37 UTC): Yes, this is what I feared. Also I was not a big fan of the feature first-place.

So what I wanted to say is: It will be complex. Or at least medium-complex. As logs are rendered on the client side and can also be a couple of megabytes which are re-loaded time-over time it need to be implemented in a way not crashing browser by overload. And it need to be a bit more clever than the current mechanism. Or the current mechanism needs to be reverted... which can also be that you just change your configuration to disable actually if it bothers you.

Note that the UI is in complete rework, it is planned to re-implement it fully with Airflow 3. So an improvement will most likely only be relevant for Airflow 2.10/2.11 line.

Looking forward for your proposal!

github-actions[bot] on (2024-12-01 00:19:47 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-12-09 00:17:12 UTC): This issue has been closed because it has not received response from the issue author.

"
2620690917,issue,closed,completed,Scheduler has no Kubernetes API access when disabling API token automounting,"### Official Helm Chart version

1.15.0 (latest released)

### Apache Airflow version

2.9.3

### Kubernetes Version

1.29.7

### Helm Chart configuration

_No response_

### Docker Image customizations

_No response_

### What happened

The Airflow scheduler requires Kubernetes API access. When disabling automountServiceAccountToken, the API token is not mounted in the pod(s) resulting in a CrashLoopBackOff with the following error: 
```
+ airflow-scheduler-5944fd4567-6jtq7 › scheduler
airflow-scheduler-5944fd4567-6jtq7 scheduler
airflow-scheduler-5944fd4567-6jtq7 scheduler /home/airflow/.local/lib/python3.12/site-packages/airflow/metrics/statsd_logger.py:184 RemovedInAirflow3Warning: The basic metric validator will be deprecated in the future in favor of pattern-matching.  You can try this now by setting config option metrics_use_pattern_match to True.
airflow-scheduler-5944fd4567-6jtq7 scheduler   ____________       _____________
airflow-scheduler-5944fd4567-6jtq7 scheduler  ____    |__( )_________  __/__  /________      __
airflow-scheduler-5944fd4567-6jtq7 scheduler ____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
airflow-scheduler-5944fd4567-6jtq7 scheduler ___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
airflow-scheduler-5944fd4567-6jtq7 scheduler  _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
airflow-scheduler-5944fd4567-6jtq7 scheduler [2024-10-29T09:29:38.255+0000] {task_context_logger.py:63} INFO - Task context logging is enabled
airflow-scheduler-5944fd4567-6jtq7 scheduler /home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py:143 FutureWarning: The config section [kubernetes] has been renamed to [kubernetes_executor]. Please update your `conf.get*` call to use the new name
airflow-scheduler-5944fd4567-6jtq7 scheduler [2024-10-29T09:29:38.342+0000] {executor_loader.py:235} INFO - Loaded executor: KubernetesExecutor
airflow-scheduler-5944fd4567-6jtq7 scheduler [2024-10-29T09:29:38.675+0000] {scheduler_job_runner.py:799} INFO - Starting the scheduler
airflow-scheduler-5944fd4567-6jtq7 scheduler [2024-10-29T09:29:38.676+0000] {scheduler_job_runner.py:806} INFO - Processing each file at most -1 times
airflow-scheduler-5944fd4567-6jtq7 scheduler [2024-10-29T09:29:38.676+0000] {kubernetes_executor.py:287} INFO - Start Kubernetes executor
airflow-scheduler-5944fd4567-6jtq7 scheduler [2024-10-29T09:29:38.681+0000] {scheduler_job_runner.py:863} ERROR - Exception when executing SchedulerJob._run_scheduler_loop
airflow-scheduler-5944fd4567-6jtq7 scheduler Traceback (most recent call last):
airflow-scheduler-5944fd4567-6jtq7 scheduler   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/scheduler_job_runner.py"", line 837, in _execute
airflow-scheduler-5944fd4567-6jtq7 scheduler     self.job.executor.start()
airflow-scheduler-5944fd4567-6jtq7 scheduler   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py"", line 295, in start
airflow-scheduler-5944fd4567-6jtq7 scheduler     self.kube_client = get_kube_client()
airflow-scheduler-5944fd4567-6jtq7 scheduler                        ^^^^^^^^^^^^^^^^^
airflow-scheduler-5944fd4567-6jtq7 scheduler   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/kube_client.py"", line 129, in get_kube_client
airflow-scheduler-5944fd4567-6jtq7 scheduler     config.load_incluster_config(client_configuration=configuration)
airflow-scheduler-5944fd4567-6jtq7 scheduler   File ""/home/airflow/.local/lib/python3.12/site-packages/kubernetes/config/incluster_config.py"", line 121, in load_incluster_config
airflow-scheduler-5944fd4567-6jtq7 scheduler     try_refresh_token=try_refresh_token).load_and_set(client_configuration)
airflow-scheduler-5944fd4567-6jtq7 scheduler                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
airflow-scheduler-5944fd4567-6jtq7 scheduler   File ""/home/airflow/.local/lib/python3.12/site-packages/kubernetes/config/incluster_config.py"", line 54, in load_and_set
airflow-scheduler-5944fd4567-6jtq7 scheduler     self._load_config()
airflow-scheduler-5944fd4567-6jtq7 scheduler   File ""/home/airflow/.local/lib/python3.12/site-packages/kubernetes/config/incluster_config.py"", line 73, in _load_config
airflow-scheduler-5944fd4567-6jtq7 scheduler     raise ConfigException(""Service token file does not exist."")
airflow-scheduler-5944fd4567-6jtq7 scheduler kubernetes.config.config_exception.ConfigException: Service token file does not exist.
airflow-scheduler-5944fd4567-6jtq7 scheduler [2024-10-29T09:29:38.731+0000] {kubernetes_executor.py:745} INFO - Shutting down Kubernetes executor
airflow-scheduler-5944fd4567-6jtq7 scheduler [2024-10-29T09:29:38.740+0000] {manager.py:321} WARNING - Ending without manager process.
airflow-scheduler-5944fd4567-6jtq7 scheduler [2024-10-29T09:29:38.740+0000] {scheduler_job_runner.py:875} INFO - Exited execute loop
airflow-scheduler-5944fd4567-6jtq7 scheduler Traceback (most recent call last):
airflow-scheduler-5944fd4567-6jtq7 scheduler   File ""/home/airflow/.local/bin/airflow"", line 8, in <module>
airflow-scheduler-5944fd4567-6jtq7 scheduler     sys.exit(main())
airflow-scheduler-5944fd4567-6jtq7 scheduler              ^^^^^^
airflow-scheduler-5944fd4567-6jtq7 scheduler   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/__main__.py"", line 58, in main
airflow-scheduler-5944fd4567-6jtq7 scheduler     args.func(args)
airflow-scheduler-5944fd4567-6jtq7 scheduler   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command
airflow-scheduler-5944fd4567-6jtq7 scheduler     return func(*args, **kwargs)
airflow-scheduler-5944fd4567-6jtq7 scheduler            ^^^^^^^^^^^^^^^^^^^^^
airflow-scheduler-5944fd4567-6jtq7 scheduler   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 114, in wrapper
airflow-scheduler-5944fd4567-6jtq7 scheduler     return f(*args, **kwargs)
airflow-scheduler-5944fd4567-6jtq7 scheduler            ^^^^^^^^^^^^^^^^^^
airflow-scheduler-5944fd4567-6jtq7 scheduler   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/providers_configuration_loader.py"", line 55, in wrapped_function
airflow-scheduler-5944fd4567-6jtq7 scheduler     return func(*args, **kwargs)
airflow-scheduler-5944fd4567-6jtq7 scheduler            ^^^^^^^^^^^^^^^^^^^^^
airflow-scheduler-5944fd4567-6jtq7 scheduler   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/scheduler_command.py"", line 58, in scheduler
airflow-scheduler-5944fd4567-6jtq7 scheduler     run_command_with_daemon_option(
airflow-scheduler-5944fd4567-6jtq7 scheduler   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/daemon_utils.py"", line 85, in run_command_with_daemon_option
airflow-scheduler-5944fd4567-6jtq7 scheduler     callback()
airflow-scheduler-5944fd4567-6jtq7 scheduler   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/scheduler_command.py"", line 61, in <lambda>
airflow-scheduler-5944fd4567-6jtq7 scheduler     callback=lambda: _run_scheduler_job(args),
airflow-scheduler-5944fd4567-6jtq7 scheduler                      ^^^^^^^^^^^^^^^^^^^^^^^^
airflow-scheduler-5944fd4567-6jtq7 scheduler   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/scheduler_command.py"", line 49, in _run_scheduler_job
airflow-scheduler-5944fd4567-6jtq7 scheduler     run_job(job=job_runner.job, execute_callable=job_runner._execute)
airflow-scheduler-5944fd4567-6jtq7 scheduler   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 79, in wrapper
airflow-scheduler-5944fd4567-6jtq7 scheduler     return func(*args, session=session, **kwargs)
airflow-scheduler-5944fd4567-6jtq7 scheduler            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
airflow-scheduler-5944fd4567-6jtq7 scheduler   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/job.py"", line 402, in run_job
airflow-scheduler-5944fd4567-6jtq7 scheduler     return execute_job(job, execute_callable=execute_callable)
airflow-scheduler-5944fd4567-6jtq7 scheduler            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
airflow-scheduler-5944fd4567-6jtq7 scheduler   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/job.py"", line 431, in execute_job
airflow-scheduler-5944fd4567-6jtq7 scheduler     ret = execute_callable()
airflow-scheduler-5944fd4567-6jtq7 scheduler           ^^^^^^^^^^^^^^^^^^
airflow-scheduler-5944fd4567-6jtq7 scheduler   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/scheduler_job_runner.py"", line 837, in _execute
airflow-scheduler-5944fd4567-6jtq7 scheduler     self.job.executor.start()
airflow-scheduler-5944fd4567-6jtq7 scheduler   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py"", line 295, in start
airflow-scheduler-5944fd4567-6jtq7 scheduler     self.kube_client = get_kube_client()
airflow-scheduler-5944fd4567-6jtq7 scheduler                        ^^^^^^^^^^^^^^^^^
airflow-scheduler-5944fd4567-6jtq7 scheduler   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/kube_client.py"", line 129, in get_kube_client
airflow-scheduler-5944fd4567-6jtq7 scheduler     config.load_incluster_config(client_configuration=configuration)
airflow-scheduler-5944fd4567-6jtq7 scheduler   File ""/home/airflow/.local/lib/python3.12/site-packages/kubernetes/config/incluster_config.py"", line 121, in load_incluster_config
airflow-scheduler-5944fd4567-6jtq7 scheduler     try_refresh_token=try_refresh_token).load_and_set(client_configuration)
airflow-scheduler-5944fd4567-6jtq7 scheduler                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
airflow-scheduler-5944fd4567-6jtq7 scheduler   File ""/home/airflow/.local/lib/python3.12/site-packages/kubernetes/config/incluster_config.py"", line 54, in load_and_set
airflow-scheduler-5944fd4567-6jtq7 scheduler     self._load_config()
airflow-scheduler-5944fd4567-6jtq7 scheduler   File ""/home/airflow/.local/lib/python3.12/site-packages/kubernetes/config/incluster_config.py"", line 73, in _load_config
airflow-scheduler-5944fd4567-6jtq7 scheduler     raise ConfigException(""Service token file does not exist."")
airflow-scheduler-5944fd4567-6jtq7 scheduler kubernetes.config.config_exception.ConfigException: Service token file does not exist.
```

### What you think should happen instead

In my opinion the Airflow Helm Chart should provision the token so the scheduler sucessfully runs on the Kubernetes cluster.

### How to reproduce

Set `scheduler.serviceAccount.automountServiceAccountToken: false`

### Anything else

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",DjVinnii,2024-10-29 09:39:23+00:00,[],2025-01-06 13:12:49+00:00,2025-01-06 13:12:48+00:00,https://github.com/apache/airflow/issues/43464,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('area:helm-chart', 'Airflow Helm Chart'), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2444977767, 'issue_id': 2620690917, 'author': 'potiuk', 'body': ""I believe Airflow scheduler does not require the token - it requires it when K8S executor is used, but when you use local or celery executor it should work fine\r\n\r\nI am not sure however what was the intetion - it's been added everywhere in https://github.com/apache/airflow/pull/32808 and maybe @amoghrajesh  can comment on it, but maybe you already know how to change it - seems that you want to submit a PR for it?"", 'created_at': datetime.datetime(2024, 10, 29, 17, 56, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2446063391, 'issue_id': 2620690917, 'author': 'DjVinnii', 'body': ""> I believe Airflow scheduler does not require the token - it requires it when K8S executor is used, but when you use local or celery executor it should work fine\r\n\r\nAh yes, I forgot to mention that I'm indeed using the K8S executor. I have to disable to K8S Service Account token automount due to a cluster policy and suspect that this might be the case for more users.\r\n\r\n> I am not sure however what was the intetion - it's been added everywhere in #32808 and maybe @amoghrajesh can comment on it, but maybe you already know how to change it - seems that you want to submit a PR for it?\r\n\r\nI am indeed willing to submit a PR, however I don't know what the best way will be to solve this. Maybe @amoghrajesh has some insights on this."", 'created_at': datetime.datetime(2024, 10, 30, 7, 29, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2446418044, 'issue_id': 2620690917, 'author': 'amoghrajesh', 'body': ""@potiuk @DjVinnii I checked the issue. The fix was to complete this one https://github.com/apache/airflow/issues/30722. The idea was to not mount the service account tokens to reduce the security risk of the token being exposed if a pod is compromised.\r\n\r\nOn further reading, I see that the token is always needed for scheduler and if this is set to false, the serviceaccount token will not be automatically mounted into the pods that use this service account (scheduler for example). The scheduler will not be able to authenticate to the K8s API, which is needed for tasks like creating and managing pods. \r\n\r\n\r\n> I am indeed willing to submit a PR, however I don't know what the best way will be to solve this. Maybe @amoghrajesh has some insights on this.\r\n\r\n@DjVinnii I think the most ideal fix here would be to remove the option from the scheduler service account. It can be optional for other pods but it is always supposed to be true for scheduler."", 'created_at': datetime.datetime(2024, 10, 30, 10, 6, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2446479233, 'issue_id': 2620690917, 'author': 'amoghrajesh', 'body': '@DjVinnii feel free to submit a PR that implements this logic', 'created_at': datetime.datetime(2024, 10, 30, 10, 26, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2553996462, 'issue_id': 2620690917, 'author': 'gsingh935', 'body': ""@amoghrajesh - there is actually a workaround that can be done to set automountserviceaccount to false for scheduler pod.\r\n\r\nThis setup allows you to manually project the service account token with specific parameters, giving you more control over token expiration and access\r\nThe configuration sets an expiration time for the token, enhancing security by limiting the token's lifetime\r\n\r\n```yaml\r\nscheduler:   \r\n  serviceAccount: \r\n    automountServiceAccountToken: false\r\n  extraVolumes:                                       \r\n  - name: serviceaccount-token\r\n    projected:\r\n      defaultMode: 0444\r\n      sources:\r\n      - serviceAccountToken:\r\n          expirationSeconds: 3607\r\n          path: token\r\n      - configMap:\r\n          name: kube-root-ca.crt\r\n          items:\r\n          - key: ca.crt\r\n            path: ca.crt\r\n      - downwardAPI:\r\n          items:\r\n          - path: namespace\r\n            fieldRef:\r\n              apiVersion: v1\r\n              fieldPath: metadata.namespace\r\n  extraVolumeMounts:\r\n  - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\r\n    name: serviceaccount-token\r\n```"", 'created_at': datetime.datetime(2024, 12, 19, 13, 32, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-10-29 17:56:59 UTC): I believe Airflow scheduler does not require the token - it requires it when K8S executor is used, but when you use local or celery executor it should work fine

I am not sure however what was the intetion - it's been added everywhere in https://github.com/apache/airflow/pull/32808 and maybe @amoghrajesh  can comment on it, but maybe you already know how to change it - seems that you want to submit a PR for it?

DjVinnii (Issue Creator) on (2024-10-30 07:29:18 UTC): Ah yes, I forgot to mention that I'm indeed using the K8S executor. I have to disable to K8S Service Account token automount due to a cluster policy and suspect that this might be the case for more users.


I am indeed willing to submit a PR, however I don't know what the best way will be to solve this. Maybe @amoghrajesh has some insights on this.

amoghrajesh on (2024-10-30 10:06:45 UTC): @potiuk @DjVinnii I checked the issue. The fix was to complete this one https://github.com/apache/airflow/issues/30722. The idea was to not mount the service account tokens to reduce the security risk of the token being exposed if a pod is compromised.

On further reading, I see that the token is always needed for scheduler and if this is set to false, the serviceaccount token will not be automatically mounted into the pods that use this service account (scheduler for example). The scheduler will not be able to authenticate to the K8s API, which is needed for tasks like creating and managing pods. 



@DjVinnii I think the most ideal fix here would be to remove the option from the scheduler service account. It can be optional for other pods but it is always supposed to be true for scheduler.

amoghrajesh on (2024-10-30 10:26:54 UTC): @DjVinnii feel free to submit a PR that implements this logic

gsingh935 on (2024-12-19 13:32:00 UTC): @amoghrajesh - there is actually a workaround that can be done to set automountserviceaccount to false for scheduler pod.

This setup allows you to manually project the service account token with specific parameters, giving you more control over token expiration and access
The configuration sets an expiration time for the token, enhancing security by limiting the token's lifetime

```yaml
scheduler:   
  serviceAccount: 
    automountServiceAccountToken: false
  extraVolumes:                                       
  - name: serviceaccount-token
    projected:
      defaultMode: 0444
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          name: kube-root-ca.crt
          items:
          - key: ca.crt
            path: ca.crt
      - downwardAPI:
          items:
          - path: namespace
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
  extraVolumeMounts:
  - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
    name: serviceaccount-token
```

"
2620272984,issue,closed,completed,GitSync invalid key format in /etc/git-secret/ssh,"### Official Helm Chart version

1.10.0

### Apache Airflow version

2.6.2

### Kubernetes Version

1.30

### Helm Chart configuration

```  dags:
    persistence:
      subPath: dags/platform
      size: 10Gi
      storageClassName: default-efs
    gitSync:
      enabled: true
      repo: git@github.com:my-org/dag-repo.git
      branch: main
      rev: HEAD

      sshKeySecret: airflow-git-sync-secret
```
### Docker Image customizations

_No response_

### What happened

I have created a deploy ssh key in the repo to enable gitsync to only access this repo in particular with read only access. I generate a ssh key locally and updated the public key on the deploy key. The private key is stored in AWS secrets manager as a plain text file and I let external secret operator create a secret that by construction is base64 encoded and matches the format indicated by the documentation:

```
    # If you are using an ssh clone url, you can load
    # the ssh private key to a k8s secret like the one below
    #   ---
    #   apiVersion: v1
    #   kind: Secret
    #   metadata:
    #     name: airflow-ssh-secret
    #   data:
    #     # key needs to be gitSshKey
    #     gitSshKey: <base64_encoded_data>
```

However I receive the error: 

> ""msg""=""too many failures, aborting"" ""error""=""Run(git clone -v --no-checkout -b main --depth 1 git@github.com:my-org/dag-repository.git /git): exit status 128: { stdout: """", stderr: ""Cloning into '/git'...\nWarning: Permanently added 'github.com,20.X.X.X.X' (ECDSA) to the list of known hosts.\r\nLoad key \""/etc/git-secret/ssh\"": invalid format\r\ngit@github.com: Permission denied (publickey).\r\nfatal: Could not read from remote repository.\n\nPlease make sure you have the correct access rights\nand the repository exists."" }"" ""failCount""=1

I have create d a debug pod with the secret mounted as it is on the airflow-worker pod and the pem file looks fine for me:
cat /etc/git-secret/ssh

```
-----BEGIN OPENSSH PRIVATE KEY-----

-----END OPENSSH PRIVATE KEY-----

```



### What you think should happen instead

_No response_

### How to reproduce

Deploy Airflow on kubernetes 1.30 and deploy the 1.10 airflow helm chart trying to connect using a deploy ssh key to the dag folder.

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",laserpedro,2024-10-29 06:33:47+00:00,[],2024-10-29 10:38:02+00:00,2024-10-29 10:38:02+00:00,https://github.com/apache/airflow/issues/43458,"[('kind:bug', 'This is a clearly a bug'), ('area:helm-chart', 'Airflow Helm Chart'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2443326528, 'issue_id': 2620272984, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 29, 6, 33, 50, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-29 06:33:50 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2619970607,issue,closed,not_planned,KeyError: 'task_id',"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

[2024-10-29T02:33:38.169+0000] {app.py:1744} ERROR - Exception on /dags/user_automation_dag/grid [GET]
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.9/site-packages/flask/app.py"", line 2529, in wsgi_app
    response = self.full_dispatch_request()
  File ""/home/airflow/.local/lib/python3.9/site-packages/flask/app.py"", line 1825, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/home/airflow/.local/lib/python3.9/site-packages/flask/app.py"", line 1823, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/home/airflow/.local/lib/python3.9/site-packages/flask/app.py"", line 1799, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/www/auth.py"", line 47, in decorated
    return func(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/www/decorators.py"", line 166, in view_func
    return f(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/www/decorators.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py"", line 76, in wrapper
    return func(*args, session=session, **kwargs)
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/www/views.py"", line 2665, in grid
    dag = get_airflow_app().dag_bag.get_dag(dag_id, session=session)
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py"", line 73, in wrapper
    return func(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py"", line 190, in get_dag
    self._add_dag_from_db(dag_id=dag_id, session=session)
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py"", line 272, in _add_dag_from_db
    dag = row.dag
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/models/serialized_dag.py"", line 222, in dag
    return SerializedDAG.from_dict(data)
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/serialization/serialized_objects.py"", line 1363, in from_dict
    return cls.deserialize_dag(serialized_obj[""dag""])
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/serialization/serialized_objects.py"", line 1270, in deserialize_dag
    v = {task[""task_id""]: SerializedBaseOperator.deserialize_operator(task) for task in v}
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/serialization/serialized_objects.py"", line 1270, in <dictcomp>
    v = {task[""task_id""]: SerializedBaseOperator.deserialize_operator(task) for task in v}
KeyError: 'task_id'
172.18.0.1 - - [29/Oct/2024:02:33:38 +0000] ""GET /dags/user_automation_dag/grid HTTP/1.1"" 500 1538 ""http://localhost:8080/home"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36""

### What you think should happen instead?

_No response_

### How to reproduce

docker-compose up -d

### Operating System

ubuntu v24

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",guavacoderepo,2024-10-29 02:39:22+00:00,['guavacoderepo'],2025-01-07 11:59:32+00:00,2024-11-20 00:15:39+00:00,https://github.com/apache/airflow/issues/43455,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('pending-response', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2443048505, 'issue_id': 2619970607, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 29, 2, 39, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443826051, 'issue_id': 2619970607, 'author': 'potiuk', 'body': ""There is no way to reproduce it - without extra information on how you get there. It seems that you have some broken serialized dag in your DB and until we know how it got there, it will be impossible to diagnose and fix it.\r\n\r\nEasy fix is to reserialize all your dags (see the docs) and see if it occurs again. If it does, then you should provide all the details on which dag and which task is creating the issue and how to reproduce it - but without that it's impossible to diagnose it. I assign it to you to provide more information."", 'created_at': datetime.datetime(2024, 10, 29, 10, 26, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2471993654, 'issue_id': 2619970607, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 11, 13, 0, 15, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2487026598, 'issue_id': 2619970607, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 11, 20, 0, 15, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2575108638, 'issue_id': 2619970607, 'author': 'doanthai', 'body': '@guavacoderepo Try upgrade version 2.10.4. In my case, the error was fixed after upgrade version from 2.9.3 to 2.10.4', 'created_at': datetime.datetime(2025, 1, 7, 11, 59, 31, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-29 02:39:25 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-10-29 10:26:43 UTC): There is no way to reproduce it - without extra information on how you get there. It seems that you have some broken serialized dag in your DB and until we know how it got there, it will be impossible to diagnose and fix it.

Easy fix is to reserialize all your dags (see the docs) and see if it occurs again. If it does, then you should provide all the details on which dag and which task is creating the issue and how to reproduce it - but without that it's impossible to diagnose it. I assign it to you to provide more information.

github-actions[bot] on (2024-11-13 00:15:11 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-11-20 00:15:38 UTC): This issue has been closed because it has not received response from the issue author.

doanthai on (2025-01-07 11:59:31 UTC): @guavacoderepo Try upgrade version 2.10.4. In my case, the error was fixed after upgrade version from 2.9.3 to 2.10.4

"
2619454191,issue,closed,completed,Scheduler : ERROR - Process timed out  for all DAGS,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.3

### What happened?

Whenever I start a DAG, Iam getting a timeout in scheduler and exit code with some errors as below:

```
[34m2024-10-28T20:03:57.467+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
--
<TaskInstance: edgetag_currency.task_pull manual__2024-10-28T19:57:46.193365+00:00 [scheduled]>[0m
[[34m2024-10-28T20:03:57.468+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG edgetag_currency has 0/16 running and queued tasks[0m
[[34m2024-10-28T20:03:57.469+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
<TaskInstance: edgetag_currency.task_pull manual__2024-10-28T19:57:46.193365+00:00 [scheduled]>[0m
[[34m2024-10-28T20:03:57.479+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='edgetag_currency', task_id='task_pull', run_id='manual__2024-10-28T19:57:46.193365+00:00', try_number=2, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-10-28T20:03:57.480+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'edgetag_currency', 'task_pull', 'manual__2024-10-28T19:57:46.193365+00:00', '--local', '--subdir', 'DAGS_FOLDER/dags/currency_pull.py'][0m
[[34m2024-10-28T20:03:58.482+0000[0m] {[34mtimeout.py:[0m68} ERROR[0m - Process timed out, PID: 7[0m
[[34m2024-10-28T20:03:58.491+0000[0m] {[34mcelery_executor.py:[0m279} INFO[0m - [Try 1 of 3] Task Timeout Error for Task: (TaskInstanceKey(dag_id='edgetag_currency', task_id='task_pull', run_id='manual__2024-10-28T19:57:46.193365+00:00', try_number=2, map_index=-1)).[0m
[[34m2024-10-28T20:03:59.567+0000[0m] {[34mtimeout.py:[0m68} ERROR[0m - Process timed out, PID: 7[0m
[[34m2024-10-28T20:03:59.571+0000[0m] {[34mcelery_executor.py:[0m279} INFO[0m - [Try 2 of 3] Task Timeout Error for Task: (TaskInstanceKey(dag_id='edgetag_currency', task_id='task_pull', run_id='manual__2024-10-28T19:57:46.193365+00:00', try_number=2, map_index=-1)).[0m
[[34m2024-10-28T20:04:01.715+0000[0m] {[34mtimeout.py:[0m68} ERROR[0m - Process timed out, PID: 7[0m
[[34m2024-10-28T20:04:01.720+0000[0m] {[34mcelery_executor.py:[0m279} INFO[0m - [Try 3 of 3] Task Timeout Error for Task: (TaskInstanceKey(dag_id='edgetag_currency', task_id='task_pull', run_id='manual__2024-10-28T19:57:46.193365+00:00', try_number=2, map_index=-1)).[0m
[[34m2024-10-28T20:04:03.529+0000[0m] {[34mtimeout.py:[0m68} ERROR[0m - Process timed out, PID: 7[0m
[[34m2024-10-28T20:04:03.533+0000[0m] {[34mcelery_executor.py:[0m290} ERROR[0m - Error sending Celery task: Timeout, PID: 7
Celery Task ID: TaskInstanceKey(dag_id='edgetag_currency', task_id='task_pull', run_id='manual__2024-10-28T19:57:46.193365+00:00', try_number=2, map_index=-1)
Traceback (most recent call last):
File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/utils/functional.py"", line 32, in __call__
return self.__value__
^^^^^^^^^^^^^^
AttributeError: 'ChannelPromise' object has no attribute '__value__'. Did you mean: '__call__'?
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/transport/virtual/base.py"", line 951, in create_channel
return self._avail_channels.pop()
^^^^^^^^^^^^^^^^^^^^^^^^^^
IndexError: pop from empty list
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/celery/executors/celery_executor_utils.py"", line 220, in send_task_to_executor
result = task_to_run.apply_async(args=[command], queue=queue)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.12/site-packages/celery/app/task.py"", line 594, in apply_async
return app.send_task(
^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.12/site-packages/celery/app/base.py"", line 801, in send_task
amqp.send_task_message(P, name, message, **options)
File ""/home/airflow/.local/lib/python3.12/site-packages/celery/app/amqp.py"", line 518, in send_task_message
ret = producer.publish(
^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/messaging.py"", line 186, in publish
return _publish(
^^^^^^^^^
File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/connection.py"", line 556, in _ensured
return fun(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/messaging.py"", line 195, in _publish
channel = self.channel
^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/messaging.py"", line 218, in _get_channel
channel = self._channel = channel()
^^^^^^^^^
File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/utils/functional.py"", line 34, in __call__
value = self.__value__ = self.__contract__()
^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/messaging.py"", line 234, in <lambda>
channel = ChannelPromise(lambda: connection.default_channel)
^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/connection.py"", line 953, in default_channel
self._ensure_connection(**conn_opts)
File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/connection.py"", line 459, in _ensure_connection
return retry_over_time(
^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/utils/functional.py"", line 318, in retry_over_time
return fun(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/connection.py"", line 934, in _connection_factory
self._connection = self._establish_connection()
^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/connection.py"", line 860, in _establish_connection
conn = self.transport.establish_connection()
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/transport/virtual/base.py"", line 975, in establish_connection
self._avail_channels.append(self.create_channel(self))
^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/transport/virtual/base.py"", line 953, in create_channel
channel = self.Channel(connection)
^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/transport/redis.py"", line 744, in __init__
self.client.ping()
File ""/home/airflow/.local/lib/python3.12/site-packages/redis/commands/core.py"", line 1217, in ping
return self.execute_command(""PING"", **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.12/site-packages/redis/client.py"", line 542, in execute_command
conn = self.connection or pool.get_connection(command_name, **options)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.12/site-packages/redis/connection.py"", line 1109, in get_connection
connection.connect()
File ""/home/airflow/.local/lib/python3.12/site-packages/redis/connection.py"", line 288, in connect
self.on_connect()
File ""/home/airflow/.local/lib/python3.12/site-packages/redis/connection.py"", line 354, in on_connect
auth_response = self.read_response()
^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.12/site-packages/redis/connection.py"", line 512, in read_response
response = self._parser.read_response(disable_decoding=disable_decoding)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.12/site-packages/redis/_parsers/resp2.py"", line 15, in read_response
result = self._read_response(disable_decoding=disable_decoding)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.12/site-packages/redis/_parsers/resp2.py"", line 25, in _read_response
raw = self._buffer.readline()
^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.12/site-packages/redis/_parsers/socket.py"", line 115, in readline
self._read_from_socket()
File ""/home/airflow/.local/lib/python3.12/site-packages/redis/_parsers/socket.py"", line 65, in _read_from_socket
data = self._sock.recv(socket_read_size)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py"", line 69, in handle_timeout
raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: Timeout, PID: 7
[0m
[[34m2024-10-28T20:04:03.534+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='edgetag_currency', task_id='task_pull', run_id='manual__2024-10-28T19:57:46.193365+00:00', try_number=2, map_index=-1)[0m
[[34m2024-10-28T20:04:03.545+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=edgetag_currency, task_id=task_pull, run_id=manual__2024-10-28T19:57:46.193365+00:00, map_index=-1, run_start_date=None, run_end_date=2024-10-28 19:57:57.429347+00:00, run_duration=None, state=queued, executor_state=failed, try_number=2, max_tries=1, job_id=None, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2024-10-28 20:03:57.470287+00:00, queued_by_job_id=1, pid=None[0m
[[34m2024-10-28T20:04:03.545+0000[0m] {[34mtask_context_logger.py:[0m91} ERROR[0m - The executor reported that the task instance <TaskInstance: edgetag_currency.task_pull manual__2024-10-28T19:57:46.193365+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally[0m
[[34m2024-10-28T20:04:04.619+0000[0m] {[34mdagrun.py:[0m819} ERROR[0m - Marking run <DagRun edgetag_currency @ 2024-10-28 19:57:46.193365+00:00: manual__2024-10-28T19:57:46.193365+00:00, state:running, queued_at: 2024-10-28 19:57:46.229834+00:00. externally triggered: True> failed[0m
[[34m2024-10-28T20:04:04.620+0000[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=edgetag_currency, execution_date=2024-10-28 19:57:46.193365+00:00, run_id=manual__2024-10-28T19:57:46.193365+00:00, run_start_date=2024-10-28 19:57:47.166462+00:00, run_end_date=2024-10-28 20:04:04.619902+00:00, run_duration=377.45344, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-10-28 08:00:00+00:00, data_interval_end=2024-10-28 16:00:00+00:00, dag_hash=eff246fcd6ccb6a38bec38f44b82f466[0m

```


Current needed vars for timeout are been extended mostly by default values.
This is  AWS ECS fargate with 3 services (webserver,scheduler and worker)


Current redis is 7.0.7 , I did test also with version 6
Postgres is pg14

### What you think should happen instead?

Dag should run succesfully

### How to reproduce

Deploy  Airflow 2.9.3 to AWS ECS fargate and try to run a DAG


### Operating System

Windows/Ubuntu

### Versions of Apache Airflow Providers

n/a

### Deployment

Other Docker-based deployment

### Deployment details

AWS ECS Fargate

### Anything else?

My current vars
AIRFLOW__CELERY__BROKER_URL = **********
AIRFLOW__CORE__DAG_FILE_PROCESSOR_TIMEOUT=600
AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=180
AIRFLOW__CORE__EXECUTOR=CeleryExecutor
AIRFLOW__CORE__SQL_ALCHEMY_CONN=**********
AIRFLOW__CORE__STORE_SERIALIZED_DAGS=True
AIRFLOW__WEBSERVER__LOG_FETCH_DELAY_SEC=30
AIRFLOW__WEBSERVER__LOG_FETCH_TIMEOUT_SEC=60

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",mrexhepi,2024-10-28 20:19:28+00:00,[],2024-10-29 16:13:07+00:00,2024-10-29 16:13:07+00:00,https://github.com/apache/airflow/issues/43449,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('area:core', ''), ('provider:redis', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:celery', '')]","[{'comment_id': 2442542156, 'issue_id': 2619454191, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 28, 20, 19, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2444740003, 'issue_id': 2619454191, 'author': 'mrexhepi', 'body': 'One solution that just worked for me is: \r\nSet env var AIRFLOW__CELERY__OPERATION_TIMEOUT \r\nand use redis TLS connections by seting BROKER URL   `rediss://:token@endpoint:6379/0`  instead of  `redis://:token@endpoint:6379/0`', 'created_at': datetime.datetime(2024, 10, 29, 16, 13, 7, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-28 20:19:32 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

mrexhepi (Issue Creator) on (2024-10-29 16:13:07 UTC): One solution that just worked for me is: 
Set env var AIRFLOW__CELERY__OPERATION_TIMEOUT 
and use redis TLS connections by seting BROKER URL   `rediss://:token@endpoint:6379/0`  instead of  `redis://:token@endpoint:6379/0`

"
2619189654,issue,closed,completed,Airflow scheduler thinks slots are not available when worker dies running tasks,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I am running Airflow 2.10.2 on an AWS EKS cluster with the CeleryExecutor. I ran into an issue where the Airflow workers were dying because they were OOMing. The workers had triggered many KubernetesPodOperator tasks which were in the process of running when the workers died. The workers would restart but the Airflow scheduler would get stuck in a bad state thinking it had no slots available even when the tasks that were triggered by the now dead worker had completed. New tasks would fail to get scheduled and I would see repeated logs like
```
[2024-10-23T18:37:10.786+0000] {base_executor.py:292} INFO - Executor parallelism limit reached. 0 open slots.
```
Looking at the logs, it seems like the scheduler would realize that these are zombie tasks and would set them to failed/up for retry but it still would not free up slots for it to schedule new tasks
```
 ERROR - Detected zombie job:
```



### What you think should happen instead?

If the scheduler realizes that the tasks are zombie tasks and sets them to failed/up_for_retry, it should also free up the slots that those tasks are taking up on the scheduler

### How to reproduce

Deploy Airflow on kubernetes with the AIRFLOW__CORE__PARALLELISM set to a lower number and one worker whose concurrency is greater than or equal to AIRFLOW__CORE__PARALLELISM. Then create a DAG that runs the max number of KubernetesPodOperator tasks concurrently. Wait until the tasks have started and then delete the worker with 
```
kubectl delete pod airflow-worker-0 --grace-period=0 --force
```
The task pods will continue running and succeed. And the scheduler will register that these tasks are zombies and mark them as failed. But it will not open up slots for these tasks and so it will get stuck. Restarting the scheduler will unstick it. 

Note also that we don't have to max out the number of tasks that the worker is running relative to AIRFLOW__CORE__PARALLELISM to create this issue. Whatever number of tasks that the worker is running when it fails will indefinitely take up scheduler slots. I checked this by killing the worker while it was running 3 tasks with `AIRFLOW__CORE__PARALLELISM` set to 4. When the worker came back up it was only able to run 1 task at time. 


Here's a configuration and a DAG that I used to create the issue

```
    AIRFLOW__CORE__PARALLELISM: 4
    AIRFLOW__SCHEDULER__SCHEDULER_ZOMBIE_TASK_THRESHOLD: 120
    AIRFLOW__CELERY__WORKER_CONCURRENCY: 4
```

```python
from datetime import datetime

from airflow import DAG
from airflow.operators.empty import EmptyOperator
from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator
from kubernetes.client.models import V1ResourceRequirements

dag = DAG(
    dag_id=""high_task_dag_1"",
    schedule_interval=""* * * * *"",
    start_date=datetime(2024, 10, 1),
    max_active_runs=1,
    concurrency=4,
    catchup=False,
)

empty_operator = EmptyOperator(task_id=""start"", dag=dag)


for i in range(10):
    (
        KubernetesPodOperator(
            dag=dag,
            task_id=f""task_{i}"",
            cmds=[""sleep""],
            arguments=[""30""],
            image=""busybox:latest"",
            container_resources=V1ResourceRequirements(
                requests={""cpu"": ""0.1"", ""memory"": ""10Mi""},
                limits={""cpu"": ""0.1"", ""memory"": ""10Mi""},
            ),
            service_account_name=""salesman"",
        )
        >> empty_operator
    )
```

### Operating System

Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.28.0
apache-airflow-providers-celery==3.8.1
apache-airflow-providers-cncf-kubernetes==8.4.1
apache-airflow-providers-common-compat==1.2.0
apache-airflow-providers-common-io==1.4.0
apache-airflow-providers-common-sql==1.16.0
apache-airflow-providers-docker==3.13.0
apache-airflow-providers-elasticsearch==5.5.0
apache-airflow-providers-fab==1.3.0
apache-airflow-providers-ftp==3.11.0
apache-airflow-providers-google==10.22.0
apache-airflow-providers-grpc==3.6.0
apache-airflow-providers-hashicorp==3.8.0
apache-airflow-providers-http==4.13.0
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-microsoft-azure==10.4.0
apache-airflow-providers-mysql==5.7.0
apache-airflow-providers-odbc==4.7.0
apache-airflow-providers-openlineage==1.11.0
apache-airflow-providers-postgres==5.12.0
apache-airflow-providers-redis==3.8.0
apache-airflow-providers-sendgrid==3.6.0
apache-airflow-providers-sftp==4.11.0
apache-airflow-providers-slack==8.9.0
apache-airflow-providers-smtp==1.8.0
apache-airflow-providers-snowflake==5.7.0
apache-airflow-providers-sqlite==3.9.0
apache-airflow-providers-ssh==3.13.1

### Deployment

Other 3rd-party Helm chart

### Deployment details

I am using this helm chart https://github.com/airflow-helm/charts/tree/main/charts/airflow

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",benrifkind,2024-10-28 18:25:21+00:00,[],2024-11-05 22:50:26+00:00,2024-11-05 14:50:48+00:00,https://github.com/apache/airflow/issues/43448,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('pending-response', ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2457380525, 'issue_id': 2619189654, 'author': 'jedcunningham', 'body': '#42932, coming in 2.10.3, should fix this.', 'created_at': datetime.datetime(2024, 11, 5, 14, 50, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2457637003, 'issue_id': 2619189654, 'author': 'potiuk', 'body': 'Can you please upgrade and report back @benrifkind ?', 'created_at': datetime.datetime(2024, 11, 5, 16, 27, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2457829013, 'issue_id': 2619189654, 'author': 'benrifkind', 'body': ""@potiuk Upgrading seems to have worked. I can't recreate this issue on 2.10.3. Thanks!"", 'created_at': datetime.datetime(2024, 11, 5, 17, 56, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2458306629, 'issue_id': 2619189654, 'author': 'potiuk', 'body': ""> @potiuk Upgrading seems to have worked. I can't recreate this issue on 2.10.3. Thanks!\r\n\r\nThanks!  That's encouraging !"", 'created_at': datetime.datetime(2024, 11, 5, 22, 50, 25, tzinfo=datetime.timezone.utc)}]","jedcunningham on (2024-11-05 14:50:48 UTC): #42932, coming in 2.10.3, should fix this.

potiuk on (2024-11-05 16:27:38 UTC): Can you please upgrade and report back @benrifkind ?

benrifkind (Issue Creator) on (2024-11-05 17:56:03 UTC): @potiuk Upgrading seems to have worked. I can't recreate this issue on 2.10.3. Thanks!

potiuk on (2024-11-05 22:50:25 UTC): Thanks!  That's encouraging !

"
2618369385,issue,closed,completed,Status of testing of Apache Airflow 2.10.3rc2,"### Body

We are kindly requesting that contributors to [Apache Airflow RC 2.10.3rc1](https://pypi.org/project/apache-airflow/2.10.3rc1/) help test the RC.

Please let us know by commenting if the issue is addressed in the latest RC.

- [ ] [Bump dompurify from 2.2.9 to 2.5.6 in /airflow/www (#42263) (#42270)](https://github.com/apache/airflow/pull/42270): @pierrejeambrun
- [ ] [fix: Correct docstring format in _get_template_context (#42244) (#42272)](https://github.com/apache/airflow/pull/42272): @romsharon98
- [x] [ Update StatsD Image Tag from failed dependencies check (#42264) (#42281)](https://github.com/apache/airflow/pull/42281): @jscheffl
     Linked issues:
     - [Update StatsD Image Tag from failed dependencies check (#42264)](https://github.com/apache/airflow/pull/42264)
- [ ] [Use `selectinload` in trigger (#42351)](https://github.com/apache/airflow/pull/42351): @vincbeck @josephangbc
     Linked issues:
     - [Use `selectinload` in trigger (#40487)](https://github.com/apache/airflow/pull/40487)
- [ ] [apply otel_service on metrics (#42242) (#42441)](https://github.com/apache/airflow/pull/42441): @romsharon98
- [x] [ Bugfix task execution from runner in Windows (#42426) (#42478)](https://github.com/apache/airflow/pull/42478): @jscheffl
- [ ] [[BACKPORT] Fix the span link of task instance to point to the correct span in the scheduler_job_loop (#42430) (#42480)](https://github.com/apache/airflow/pull/42480): @shahar1 @howardyoo
     Linked issues:
     - [Fix the span link of task instance to point to the correct span in the scheduler_job_loop (issue #42429) (#42430)](https://github.com/apache/airflow/pull/42430)
- [x] [ Do not attempt to provide not stringified objects to UI via xcom if pickling is active (#42388) (#42486)](https://github.com/apache/airflow/pull/42486): @jscheffl
- [ ] [fix: ensure DAG trigger form submits with updated parameters upon key… (#42499)](https://github.com/apache/airflow/pull/42499): @pierrejeambrun
- [ ] [Handle ENTER key correctly in trigger form and allow manual JSON (#42… (#42535)](https://github.com/apache/airflow/pull/42535): @pierrejeambrun
- [ ] [Reduce eyestrain in dark mode with reduced contrast and saturation (#… (#42583)](https://github.com/apache/airflow/pull/42583): @pierrejeambrun
- [x] [Support of host.name in OTEL metrics and usage of OTEL_RESOURCE_ATTRI… (#42604)](https://github.com/apache/airflow/pull/42604): @potiuk
- [x] [Prevent redirect loop on /home with tags/lastrun filters (#42607) (#42609) (#42628)](https://github.com/apache/airflow/pull/42628): @jmaicher @jscheffl
     Linked issues:
     - [Prevent redirect loop on /home with tags/lastrun filters (#42607) (#42609)](https://github.com/apache/airflow/pull/42609)
- [x] [Limit branches for pull request target workflow (#42635)](https://github.com/apache/airflow/pull/42635): @potiuk
- [ ] [[BACKPORT] Add retry logic in the scheduler for updating trigger timeouts in case of deadlocks. (#41429) (#42651)](https://github.com/apache/airflow/pull/42651): @shahar1 @TakawaAkirayo
     Linked issues:
     - [Add retry logic in the scheduler for updating trigger timeouts in case of deadlocks. (#41429)](https://github.com/apache/airflow/pull/41429)
- [ ] [Correctly select task in DAG Graph View when clicking on its name (#42697)](https://github.com/apache/airflow/pull/42697): @jonhspyro @bbovenzi
     Linked issues:
     - [Correctly select task in DAG Graph View when clicking on its name (#38782)](https://github.com/apache/airflow/pull/38782)
- [ ] [ fix(datasets/managers): fix error handling file loc when dataset alias resolved into new datasets (#42733)](https://github.com/apache/airflow/pull/42733): @Lee-W @uranusjr
     Linked issues:
     - [DAG priority parsing request is not inserted correctly on Postgres (#42704)](https://github.com/apache/airflow/issues/42704)
- [ ] [Fix dag warning documentation (#42858) (#42888)](https://github.com/apache/airflow/pull/42888): @pierrejeambrun
- [ ] [Deprecate session auth backend (backport) (#42911)](https://github.com/apache/airflow/pull/42911): @vincbeck
     Linked issues:
     - [Deprecate session auth backend (#42909)](https://github.com/apache/airflow/pull/42909)
- [x] [Improving validation of task retries to handle None values (#42532) (#42915)](https://github.com/apache/airflow/pull/42915): @sonu4578 @jscheffl
     Linked issues:
     - [Improving validation of task retries to handle None values (#42532)](https://github.com/apache/airflow/pull/42532)
- [ ] [Remove the referrer from Webserver to Scarf (#42901) (#42942)](https://github.com/apache/airflow/pull/42942): @kaxil
- [x] [Removed unicodecsv dependency for providers with Airflow version 2.8.… (#42970)](https://github.com/apache/airflow/pull/42970): @potiuk
- [x] [Restrict looker-sdk version 24.18.0 and microsoft-kiota-http 1.3.4 (#… (#42977)](https://github.com/apache/airflow/pull/42977): @potiuk
- [ ] [Fix canary build test test_cli_internal_api_background process termination #42781 (#42983)](https://github.com/apache/airflow/pull/42983): @gopidesupavan
- [ ] [Fix PythonOperator when DAG has hyphen in name (#42993)](https://github.com/apache/airflow/pull/42993): @jason810496 @k-slash
     Linked issues:
     - [PythonVirtualenvOperator - SyntaxError: cannot assign to operator (#42796)](https://github.com/apache/airflow/issues/42796)
- [ ] [Update json schema pre-commit to have draft7 schema in file (#43005) (#43007)](https://github.com/apache/airflow/pull/43007): @pierrejeambrun
- [ ] [Flush the session between writing and deletion of RTIF (#42928) (#43012)](https://github.com/apache/airflow/pull/43012): @ephraimbuddy
- [ ] [Disable flaky mssql based integration tests (#42811) (#43016)](https://github.com/apache/airflow/pull/43016): @gopidesupavan
- [ ] [Improve startup of K8S tests (#42721) (#43025)](https://github.com/apache/airflow/pull/43025): @gopidesupavan
- [ ] [update k8s tests urllib3 retry config status_forcelist and allowed_me… (#43026)](https://github.com/apache/airflow/pull/43026): @gopidesupavan
- [ ] [increase backoff_factor and add try/catch in k8s tests (#42940) (#43030)](https://github.com/apache/airflow/pull/43030): @gopidesupavan
- [x] [Add retry on error 502 and 504 (#42994) (#43044)](https://github.com/apache/airflow/pull/43044): @majorosdonat @jscheffl
     Linked issues:
     - [Add retry on error 502 and 504 (#42994)](https://github.com/apache/airflow/pull/42994)
- [ ] [[Backport] Remove zombie from executor (#43065)](https://github.com/apache/airflow/pull/43065): @uranusjr
- [x] [Added task_instance_mutation_hook for mapped operator index 0 (#42661) (#43089)](https://github.com/apache/airflow/pull/43089): @AutomationDev85 @jscheffl
     Linked issues:
     - [Added task_instance_mutation_hook for mapped operator index 0 (#42661)](https://github.com/apache/airflow/pull/42661)
- [x] [ AIP-69: Breeze adjustments for introduction of Edge Executor (#41731) (#43139)](https://github.com/apache/airflow/pull/43139): @jscheffl
     Linked issues:
     - [AIP-69: Breeze adjustments for introduction of Edge Executor (#41731)](https://github.com/apache/airflow/pull/41731)
- [ ] [[Backport] BashOperator: Execute templated bash script as file (#43191)](https://github.com/apache/airflow/pull/43191): @Joffreybvn
     Linked issues:
     - [BashOperator: Execute templated bash script as file (#42783)](https://github.com/apache/airflow/pull/42783)
- [x] [Do not fail the build if only trove-classifiers change (#43236) (#43237)](https://github.com/apache/airflow/pull/43237): @potiuk
- [x] [Mark sometimes failing heartbeat test and view test as flaky (#43250) (#43257)](https://github.com/apache/airflow/pull/43257): @potiuk
- [x] [Skip example importability tests for providers in non-main branches (… (#43263)](https://github.com/apache/airflow/pull/43263): @potiuk
- [x] [Better handling masking of values of set variable  (#43123) (#43278)](https://github.com/apache/airflow/pull/43278): @potiuk
- [x] [Fix edge-case when conflicting constraints prevent k8s env creation (… (#43298)](https://github.com/apache/airflow/pull/43298): @potiuk
     Linked issues:
     - [Make Helm artifacts reproducible (#36930)](https://github.com/apache/airflow/pull/36930)
- [x] [fix schedule_downstream_tasks bug (#42582) (#43299)](https://github.com/apache/airflow/pull/43299): @potiuk
- [x] [Fixes behaviour of example dag tests for main/other branches (#43273) (#43307)](https://github.com/apache/airflow/pull/43307): @potiuk @ashb
     Linked issues:
     - [Split providers out of the main ""airflow/"" tree into a UV workspace project (#42505)](https://github.com/apache/airflow/pull/42505)
     - [Skip example importability tests for providers in non-main branches (#43260)](https://github.com/apache/airflow/pull/43260)
- [x] [Check python version that was used to install pre-commit venvs (#43282) (#43310)](https://github.com/apache/airflow/pull/43310): @potiuk
- [x] [Add isolation mode exclusion for mapped operator test (#43297) (#43311)](https://github.com/apache/airflow/pull/43311): @potiuk
- [ ] [Backport: Bump Flask-AppBuilder to ``4.5.2`` (#43309) (#43318)](https://github.com/apache/airflow/pull/43318): @kaxil
- [x] [Masking configuration values irrelevant to DAG author (#43040) (#43336)](https://github.com/apache/airflow/pull/43336): @potiuk
- [x] [Suppress warnings when masking sensitive confs (#43335) (#43337)](https://github.com/apache/airflow/pull/43337): @potiuk
- [ ] [Backport: Remove Scarf analytics from Airflow Webserver (#43346) (#43348)](https://github.com/apache/airflow/pull/43348): @kaxil
- [ ] [ Resolve warning in Dataset Alias migration (#43425)](https://github.com/apache/airflow/pull/43425): @Lee-W @kaxil
     Linked issues:
     - [Resolve warning in Dataset Alias migration (#43421)](https://github.com/apache/airflow/pull/43421)
- [ ] [Ensure total_entries in /api/v1/dags (#43377) (#43429)](https://github.com/apache/airflow/pull/43429): @pierrejeambrun
- [ ] [include limit and offset in request body schema for List task instances (batch) endpoint (#43479)](https://github.com/apache/airflow/pull/43479): @rawwar
- [x] [Fix broken stat scheduler_loop_duration (#42886) (#43544)](https://github.com/apache/airflow/pull/43544): @potiuk
- [x] [Fix TrySelector for Mapped Tasks in Logs and Details Grid Panel (#43565) Backport (#43566)](https://github.com/apache/airflow/pull/43566): @jscheffl
- [ ] [Conditionally add OTEL events when processing executor events (#43558) (#43567)](https://github.com/apache/airflow/pull/43567): @jedcunningham
     Linked issues:
     - [Conditionally add OTEL events when processing executor events (#43558)](https://github.com/apache/airflow/pull/43558)
- [ ] [Mark all tasks as skipped when failing a dag_run manually including t… (#43572)](https://github.com/apache/airflow/pull/43572): @utkarsharma2 @abhishekbhakat 
- [x] [ FIX: Don't raise a warning in ExecutorSafeguard when execute is called from an extended operator (#42849) Backport (#43577)](https://github.com/apache/airflow/pull/43577): @dabla @jscheffl
     Linked issues:
     - [FIX: Don't raise a warning in ExecutorSafeguard when execute is called from an extended operator (#42849)](https://github.com/apache/airflow/pull/42849)



Thanks to all who contributed to the release (probably not a complete list!):
@uranusjr @howardyoo @majorosdonat @ashb @rawwar @jonhspyro @kaxil @potiuk @pierrejeambrun @Lee-W @bbovenzi @ephraimbuddy @AutomationDev85 @vincbeck @jscheffl @k-slash @jason810496 @shahar1 @jedcunningham @romsharon98 @dabla @jmaicher @gopidesupavan @utkarsharma2 @TakawaAkirayo @josephangbc @brightview4578 @Joffreybvn @abhishekbhakat 

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",utkarsharma2,2024-10-28 13:03:51+00:00,[],2024-11-05 14:30:32+00:00,2024-11-05 14:30:31+00:00,https://github.com/apache/airflow/issues/43441,"[('kind:meta', 'High-level information important to the community'), ('testing status', 'Status of testing releases')]","[{'comment_id': 2441539643, 'issue_id': 2618369385, 'author': 'raphaelauv', 'body': 'hi , the docker image are not yet available -> https://hub.docker.com/r/apache/airflow/tags?name=2.10.3', 'created_at': datetime.datetime(2024, 10, 28, 13, 6, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2441589835, 'issue_id': 2618369385, 'author': 'utkarsharma2', 'body': ""> hi , the docker image are not yet available -> https://hub.docker.com/r/apache/airflow/tags?name=2.10.3\r\n\r\n@raphaelauv Thanks, I'm looking into it."", 'created_at': datetime.datetime(2024, 10, 28, 13, 27, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443765273, 'issue_id': 2618369385, 'author': 'raphaelauv', 'body': 'I can\'t open from UI the variables,pools and connection ( no problem with 2.10.2 )\r\n\r\n![image](https://github.com/user-attachments/assets/09fda63f-681b-4cac-be16-2157984c6e79)\r\n\r\n\r\nit fail with \r\n\r\n```log\r\n[2024-10-29T10:00:34.055+0000] {app.py:1744} ERROR - Exception on /variable/list/ [GET]\r\n Traceback (most recent call last):\r\n   File ""/home/airflow/.local/lib/python3.12/site-packages/flask/app.py"", line 2529, in wsgi_app\r\n     response = self.full_dispatch_request()\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n   File ""/home/airflow/.local/lib/python3.12/site-packages/flask/app.py"", line 1825, in full_dispatch_request\r\n     rv = self.handle_user_exception(e)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n   File ""/home/airflow/.local/lib/python3.12/site-packages/flask/app.py"", line 1823, in full_dispatch_request\r\n     rv = self.dispatch_request()\r\n          ^^^^^^^^^^^^^^^^^^^^^^^\r\n   File ""/home/airflow/.local/lib/python3.12/site-packages/flask/app.py"", line 1799, in dispatch_request\r\n     return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n   File ""/home/airflow/.local/lib/python3.12/site-packages/flask_appbuilder/security/decorators.py"", line 137, in wraps\r\n     return f(self, *args, **kwargs)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^\r\n   File ""/home/airflow/.local/lib/python3.12/site-packages/flask_appbuilder/views.py"", line 550, in list\r\n     widgets = self._list()\r\n               ^^^^^^^^^^^^\r\n   File ""/home/airflow/.local/lib/python3.12/site-packages/flask_appbuilder/baseviews.py"", line 1186, in _list\r\n     form = self.search_form.refresh()\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n   File ""/home/airflow/.local/lib/python3.12/site-packages/flask_appbuilder/forms.py"", line 327, in refresh\r\n     form = self(obj=obj)\r\n            ^^^^^^^^^^^^^\r\n   File ""/home/airflow/.local/lib/python3.12/site-packages/wtforms/form.py"", line 209, in __call__\r\n     return type.__call__(cls, *args, **kwargs)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n   File ""/home/airflow/.local/lib/python3.12/site-packages/flask_wtf/form.py"", line 73, in __init__\r\n     super().__init__(formdata=formdata, **kwargs)\r\n   File ""/home/airflow/.local/lib/python3.12/site-packages/wtforms/form.py"", line 281, in __init__\r\n     super().__init__(self._unbound_fields, meta=meta_obj, prefix=prefix)\r\n   File ""/home/airflow/.local/lib/python3.12/site-packages/wtforms/form.py"", line 49, in __init__\r\n     field = meta.bind_field(self, unbound_field, options)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n   File ""/home/airflow/.local/lib/python3.12/site-packages/wtforms/meta.py"", line 28, in bind_field\r\n     return unbound_field.bind(form=form, **options)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n   File ""/home/airflow/.local/lib/python3.12/site-packages/wtforms/fields/core.py"", line 387, in bind\r\n     return self.field_class(*self.args, **kw)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n   File ""/home/airflow/.local/lib/python3.12/site-packages/wtforms/fields/core.py"", line 133, in __init__\r\n     for k, v in flags.items():\r\n                 ^^^^^^^^^^^\r\n AttributeError: \'tuple\' object has no attribute \'items\'\r\n```', 'created_at': datetime.datetime(2024, 10, 29, 10, 2, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2444650195, 'issue_id': 2618369385, 'author': 'pierrejeambrun', 'body': 'Thanks for reporting @raphaelauv this happens because we need to release the FAB provider to update the constraints of the RC to the latest FlaskAppBuilder. (latest release of WTForm broke Flask app builder).', 'created_at': datetime.datetime(2024, 10, 29, 15, 39, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445327677, 'issue_id': 2618369385, 'author': 'potiuk', 'body': '> Thanks for reporting @raphaelauv this happens because we need to release the FAB provider to update the constraints of the RC to the latest FlaskAppBuilder. (latest release of WTForm broke Flask app builder).\r\n\r\nYeah. We should regenerate constraints and rebuild images as a final check after we release FAB provider.', 'created_at': datetime.datetime(2024, 10, 29, 21, 4, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445653949, 'issue_id': 2618369385, 'author': 'thirtyseven', 'body': 'I have tested 2.10.3rc1 and confirmed that #42351 works.', 'created_at': datetime.datetime(2024, 10, 30, 1, 54, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2446953571, 'issue_id': 2618369385, 'author': 'k-slash', 'body': 'The same here for #42796 . It works under 2.10.3rc1, thanks !', 'created_at': datetime.datetime(2024, 10, 30, 12, 20, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448202573, 'issue_id': 2618369385, 'author': 'jscheffl', 'body': 'From the VOTE of provider tests of @eladkal in https://lists.apache.org/thread/76jbfwzlh7vshl9rzmpk9okl5ot3bxhc I understood that FAB will be excluded... does it make sense to test this release then atm? Or shall I wait until provider release is cut?', 'created_at': datetime.datetime(2024, 10, 30, 19, 40, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448405386, 'issue_id': 2618369385, 'author': 'jscheffl', 'body': 'Checked source tree of 2.10.3rc1 for fixes #42281 #42478 #42486 #42628 #42915 #43044 #43089 #43139 - and can confirm it is included.', 'created_at': datetime.datetime(2024, 10, 30, 21, 20, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2450140599, 'issue_id': 2618369385, 'author': 'dabla', 'body': ""Is it normal following [fix](https://github.com/apache/airflow/pull/42849) isn't in 2.10.3?  See [issue](https://github.com/apache/airflow/issues/41470) about it."", 'created_at': datetime.datetime(2024, 10, 31, 15, 15, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2451547149, 'issue_id': 2618369385, 'author': 'jscheffl', 'body': ""> Is it normal following [fix](https://github.com/apache/airflow/pull/42849) isn't in 2.10.3? See [issue](https://github.com/apache/airflow/issues/41470) about it.\r\n\r\n@dabla The issue/fix were not marked as 2.10 relevant, therefore it seems they got out of sight. I assume it is a mistake. ANd a maintainer need to backport this to v2-10-test branch. SHall I take this... so that it goes at least in 2.10.4 if too late now?"", 'created_at': datetime.datetime(2024, 11, 1, 9, 1, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2451555445, 'issue_id': 2618369385, 'author': 'dabla', 'body': ""> > Is it normal following [fix](https://github.com/apache/airflow/pull/42849) isn't in 2.10.3? See [issue](https://github.com/apache/airflow/issues/41470) about it.\r\n> \r\n> @dabla The issue/fix were not marked as 2.10 relevant, therefore it seems they got out of sight. I assume it is a mistake. ANd a maintainer need to backport this to v2-10-test branch. SHall I take this... so that it goes at least in 2.10.4 if too late now?\r\n\r\nIt’s not a real issue, just a wrong logging issue.  I’m willing to backport it to the 2-10 test branch as well if necessary…"", 'created_at': datetime.datetime(2024, 11, 1, 9, 6, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452014844, 'issue_id': 2618369385, 'author': 'jscheffl', 'body': 'I tested both 43566 and 43577 and can confirm these are in RC2.\r\n\r\nUnfortunately I found a minor glitch with 43566 (Mapped Operator Try Selector) and raised another PR to fix also index 0... where the logic in JavaScript was bad. I am not sure whether this glitch should halt RC2, I assume it is bothering but rather low/medium.', 'created_at': datetime.datetime(2024, 11, 1, 14, 56, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455777116, 'issue_id': 2618369385, 'author': 'potiuk', 'body': 'Checked that all changes are merged are in.', 'created_at': datetime.datetime(2024, 11, 4, 21, 53, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2456439296, 'issue_id': 2618369385, 'author': 'danxian-baiheng', 'body': 'Im facing the same error as @raphaelauv but with 2.10.2.\r\n```\r\nPython version: 3.11.6\r\nAirflow version: 2.10.2\r\n-------------------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/flask/app.py"", line 2529, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/flask/app.py"", line 1825, in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/flask/app.py"", line 1823, in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n         ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/flask/app.py"", line 1799, in dispatch_request\r\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/flask_appbuilder/security/decorators.py"", line 137, in wraps\r\n    return f(self, *args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/flask_appbuilder/views.py"", line 550, in list\r\n    widgets = self._list()\r\n              ^^^^^^^^^^^^\r\n  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/flask_appbuilder/baseviews.py"", line 1186, in _list\r\n    form = self.search_form.refresh()\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/flask_appbuilder/forms.py"", line 327, in refresh\r\n    form = self(obj=obj)\r\n           ^^^^^^^^^^^^^\r\n  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/wtforms/form.py"", line 209, in __call__\r\n    return type.__call__(cls, *args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/flask_wtf/form.py"", line 73, in __init__\r\n    super().__init__(formdata=formdata, **kwargs)\r\n  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/wtforms/form.py"", line 281, in __init__\r\n    super().__init__(self._unbound_fields, meta=meta_obj, prefix=prefix)\r\n  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/wtforms/form.py"", line 49, in __init__\r\n    field = meta.bind_field(self, unbound_field, options)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/wtforms/meta.py"", line 28, in bind_field\r\n    return unbound_field.bind(form=form, **options)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/wtforms/fields/core.py"", line 387, in bind\r\n    return self.field_class(*self.args, **kw)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/wtforms/fields/core.py"", line 133, in __init__\r\n    for k, v in flags.items():\r\n                ^^^^^^^^^^^\r\nAttributeError: \'tuple\' object has no attribute \'items\'\r\n```\r\nIs this resolved in the latest version, or it\'s caused by my fault?\r\nCurrently we are using 2.6.3 in prod now, and we want to upgrade to a newer version(must >= 2.9.0).\r\nWhich version will be recommended?', 'created_at': datetime.datetime(2024, 11, 5, 7, 37, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2456513490, 'issue_id': 2618369385, 'author': 'potiuk', 'body': ""This is because we were waiting for FAB provider 1.5.0 - it's been released today and we tested it before so we are just about to fix constraints for 2.10.3 to make sure it uses the latest fab provider."", 'created_at': datetime.datetime(2024, 11, 5, 8, 19, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2456514177, 'issue_id': 2618369385, 'author': 'potiuk', 'body': '> Which version will be recommended?\r\n\r\n2.10.3 once released', 'created_at': datetime.datetime(2024, 11, 5, 8, 19, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2456515344, 'issue_id': 2618369385, 'author': 'potiuk', 'body': ""(it's always the latest version that is recommended BTW)."", 'created_at': datetime.datetime(2024, 11, 5, 8, 19, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2457330731, 'issue_id': 2618369385, 'author': 'utkarsharma2', 'body': 'We\'ve just released Apache Airflow 2.10.3 🎉\r\n\r\n📦 PyPI: https://pypi.org/project/apache-airflow/2.10.3/\r\n📚 Docs: https://airflow.apache.org/docs/apache-airflow/2.10.3/\r\n🛠 Release Notes: https://airflow.apache.org/docs/apache-airflow/2.10.3/release_notes.html\r\n🐳 Docker Image: ""docker pull apache/airflow:2.10.3""\r\n🚏 Constraints: https://github.com/apache/airflow/tree/constraints-2.10.3\r\n\r\nThanks to all the contributors who made this possible.', 'created_at': datetime.datetime(2024, 11, 5, 14, 30, 31, tzinfo=datetime.timezone.utc)}]","raphaelauv on (2024-10-28 13:06:57 UTC): hi , the docker image are not yet available -> https://hub.docker.com/r/apache/airflow/tags?name=2.10.3

utkarsharma2 (Issue Creator) on (2024-10-28 13:27:10 UTC): @raphaelauv Thanks, I'm looking into it.

raphaelauv on (2024-10-29 10:02:24 UTC): I can't open from UI the variables,pools and connection ( no problem with 2.10.2 )

![image](https://github.com/user-attachments/assets/09fda63f-681b-4cac-be16-2157984c6e79)


it fail with 

```log
[2024-10-29T10:00:34.055+0000] {app.py:1744} ERROR - Exception on /variable/list/ [GET]
 Traceback (most recent call last):
   File ""/home/airflow/.local/lib/python3.12/site-packages/flask/app.py"", line 2529, in wsgi_app
     response = self.full_dispatch_request()
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File ""/home/airflow/.local/lib/python3.12/site-packages/flask/app.py"", line 1825, in full_dispatch_request
     rv = self.handle_user_exception(e)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File ""/home/airflow/.local/lib/python3.12/site-packages/flask/app.py"", line 1823, in full_dispatch_request
     rv = self.dispatch_request()
          ^^^^^^^^^^^^^^^^^^^^^^^
   File ""/home/airflow/.local/lib/python3.12/site-packages/flask/app.py"", line 1799, in dispatch_request
     return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File ""/home/airflow/.local/lib/python3.12/site-packages/flask_appbuilder/security/decorators.py"", line 137, in wraps
     return f(self, *args, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^
   File ""/home/airflow/.local/lib/python3.12/site-packages/flask_appbuilder/views.py"", line 550, in list
     widgets = self._list()
               ^^^^^^^^^^^^
   File ""/home/airflow/.local/lib/python3.12/site-packages/flask_appbuilder/baseviews.py"", line 1186, in _list
     form = self.search_form.refresh()
            ^^^^^^^^^^^^^^^^^^^^^^^^^^
   File ""/home/airflow/.local/lib/python3.12/site-packages/flask_appbuilder/forms.py"", line 327, in refresh
     form = self(obj=obj)
            ^^^^^^^^^^^^^
   File ""/home/airflow/.local/lib/python3.12/site-packages/wtforms/form.py"", line 209, in __call__
     return type.__call__(cls, *args, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File ""/home/airflow/.local/lib/python3.12/site-packages/flask_wtf/form.py"", line 73, in __init__
     super().__init__(formdata=formdata, **kwargs)
   File ""/home/airflow/.local/lib/python3.12/site-packages/wtforms/form.py"", line 281, in __init__
     super().__init__(self._unbound_fields, meta=meta_obj, prefix=prefix)
   File ""/home/airflow/.local/lib/python3.12/site-packages/wtforms/form.py"", line 49, in __init__
     field = meta.bind_field(self, unbound_field, options)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File ""/home/airflow/.local/lib/python3.12/site-packages/wtforms/meta.py"", line 28, in bind_field
     return unbound_field.bind(form=form, **options)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File ""/home/airflow/.local/lib/python3.12/site-packages/wtforms/fields/core.py"", line 387, in bind
     return self.field_class(*self.args, **kw)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File ""/home/airflow/.local/lib/python3.12/site-packages/wtforms/fields/core.py"", line 133, in __init__
     for k, v in flags.items():
                 ^^^^^^^^^^^
 AttributeError: 'tuple' object has no attribute 'items'
```

pierrejeambrun on (2024-10-29 15:39:47 UTC): Thanks for reporting @raphaelauv this happens because we need to release the FAB provider to update the constraints of the RC to the latest FlaskAppBuilder. (latest release of WTForm broke Flask app builder).

potiuk on (2024-10-29 21:04:17 UTC): Yeah. We should regenerate constraints and rebuild images as a final check after we release FAB provider.

thirtyseven on (2024-10-30 01:54:09 UTC): I have tested 2.10.3rc1 and confirmed that #42351 works.

k-slash on (2024-10-30 12:20:59 UTC): The same here for #42796 . It works under 2.10.3rc1, thanks !

jscheffl on (2024-10-30 19:40:15 UTC): From the VOTE of provider tests of @eladkal in https://lists.apache.org/thread/76jbfwzlh7vshl9rzmpk9okl5ot3bxhc I understood that FAB will be excluded... does it make sense to test this release then atm? Or shall I wait until provider release is cut?

jscheffl on (2024-10-30 21:20:48 UTC): Checked source tree of 2.10.3rc1 for fixes #42281 #42478 #42486 #42628 #42915 #43044 #43089 #43139 - and can confirm it is included.

dabla on (2024-10-31 15:15:13 UTC): Is it normal following [fix](https://github.com/apache/airflow/pull/42849) isn't in 2.10.3?  See [issue](https://github.com/apache/airflow/issues/41470) about it.

jscheffl on (2024-11-01 09:01:07 UTC): @dabla The issue/fix were not marked as 2.10 relevant, therefore it seems they got out of sight. I assume it is a mistake. ANd a maintainer need to backport this to v2-10-test branch. SHall I take this... so that it goes at least in 2.10.4 if too late now?

dabla on (2024-11-01 09:06:54 UTC): It’s not a real issue, just a wrong logging issue.  I’m willing to backport it to the 2-10 test branch as well if necessary…

jscheffl on (2024-11-01 14:56:30 UTC): I tested both 43566 and 43577 and can confirm these are in RC2.

Unfortunately I found a minor glitch with 43566 (Mapped Operator Try Selector) and raised another PR to fix also index 0... where the logic in JavaScript was bad. I am not sure whether this glitch should halt RC2, I assume it is bothering but rather low/medium.

potiuk on (2024-11-04 21:53:32 UTC): Checked that all changes are merged are in.

danxian-baiheng on (2024-11-05 07:37:17 UTC): Im facing the same error as @raphaelauv but with 2.10.2.
```
Python version: 3.11.6
Airflow version: 2.10.2
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/flask/app.py"", line 2529, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/flask/app.py"", line 1825, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/flask/app.py"", line 1823, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/flask/app.py"", line 1799, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/flask_appbuilder/security/decorators.py"", line 137, in wraps
    return f(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/flask_appbuilder/views.py"", line 550, in list
    widgets = self._list()
              ^^^^^^^^^^^^
  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/flask_appbuilder/baseviews.py"", line 1186, in _list
    form = self.search_form.refresh()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/flask_appbuilder/forms.py"", line 327, in refresh
    form = self(obj=obj)
           ^^^^^^^^^^^^^
  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/wtforms/form.py"", line 209, in __call__
    return type.__call__(cls, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/flask_wtf/form.py"", line 73, in __init__
    super().__init__(formdata=formdata, **kwargs)
  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/wtforms/form.py"", line 281, in __init__
    super().__init__(self._unbound_fields, meta=meta_obj, prefix=prefix)
  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/wtforms/form.py"", line 49, in __init__
    field = meta.bind_field(self, unbound_field, options)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/wtforms/meta.py"", line 28, in bind_field
    return unbound_field.bind(form=form, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/wtforms/fields/core.py"", line 387, in bind
    return self.field_class(*self.args, **kw)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/wtforms/fields/core.py"", line 133, in __init__
    for k, v in flags.items():
                ^^^^^^^^^^^
AttributeError: 'tuple' object has no attribute 'items'
```
Is this resolved in the latest version, or it's caused by my fault?
Currently we are using 2.6.3 in prod now, and we want to upgrade to a newer version(must >= 2.9.0).
Which version will be recommended?

potiuk on (2024-11-05 08:19:04 UTC): This is because we were waiting for FAB provider 1.5.0 - it's been released today and we tested it before so we are just about to fix constraints for 2.10.3 to make sure it uses the latest fab provider.

potiuk on (2024-11-05 08:19:24 UTC): 2.10.3 once released

potiuk on (2024-11-05 08:19:58 UTC): (it's always the latest version that is recommended BTW).

utkarsharma2 (Issue Creator) on (2024-11-05 14:30:31 UTC): We've just released Apache Airflow 2.10.3 🎉

📦 PyPI: https://pypi.org/project/apache-airflow/2.10.3/
📚 Docs: https://airflow.apache.org/docs/apache-airflow/2.10.3/
🛠 Release Notes: https://airflow.apache.org/docs/apache-airflow/2.10.3/release_notes.html
🐳 Docker Image: ""docker pull apache/airflow:2.10.3""
🚏 Constraints: https://github.com/apache/airflow/tree/constraints-2.10.3

Thanks to all the contributors who made this possible.

"
2618352355,issue,closed,completed,Scheduler not terminating in case of repeated DB errors.,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Scheduler was running and launching tasks normally.
Suddenly there was auth error on database operations. 

```
psycopg2.OperationalError: connection to server at ""<Host>"" (<IP>), port 6432 failed: FATAL:  server login has been failing, try again later (server_login_retry)
connection to server at ""<HOST>"" (<IP>), port 6432 failed: FATAL:  server login has been failing, try again later (server_login_retry)


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/opt/app-root/lib64/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py"", line 984, in _execute
    self._run_scheduler_loop()
```

After few retries it exited scheduler loop but process was not terminated.

### What you think should happen instead?

After shutting down all executor and dag_processer process should exit.

### How to reproduce

Using hybrid executors with Celery, Kubernetes

Introduce db errors.

### Operating System

Mac/Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### Anything else?

There are below logs repeated which indicates some threads not exited.

```
[2024-10-27T06:17:10.658+0000] {kubernetes_executor_utils.py:101} INFO - Kubernetes watch timed out waiting for events. Restarting watch.
[2024-10-27T06:17:11.658+0000] {kubernetes_executor_utils.py:140} INFO - Event: and now my watch begins starting at resource_version: 0
[2024-10-27T06:17:11.702+0000] {kubernetes_executor_utils.py:309} INFO - Event: 666aac59b268675b6b2590ff-bs-8ace-s4sjuxfo is Running, annotations: <omitted>
[2024-10-27T06:17:11.712+0000] {kubernetes_executor_utils.py:309} INFO - Event: 666aac59b268675b6b2590ff-bs-44fe-iwuzjfao is Running, annotations: <omitted>
[2024-10-27T06:17:41.715+0000] {kubernetes_executor_utils.py:101} INFO - Kubernetes watch timed out waiting for events. Restarting watch.
```

I see old PR for similar issue https://github.com/apache/airflow/pull/28685
Should I change catch block to catch all exceptions?

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",iw-pavan,2024-10-28 12:56:43+00:00,[],2024-11-12 12:24:55+00:00,2024-11-12 12:24:55+00:00,https://github.com/apache/airflow/issues/43440,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('area:MetaDB', 'Meta Database related issues.'), ('good first issue', ''), ('area:core', '')]","[{'comment_id': 2442042351, 'issue_id': 2618352355, 'author': 'potiuk', 'body': 'Feel free to propose a pr. Details can be discussed when you propose it.', 'created_at': datetime.datetime(2024, 10, 28, 16, 19, 4, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-10-28 16:19:04 UTC): Feel free to propose a pr. Details can be discussed when you propose it.

"
2618331650,issue,open,,Determine if including try_number in the UniqueConstraint improves index efficiency,"This is part of the future work of https://github.com/apache/airflow/pull/43243 that migrate TI table's primary key to be a UUID7 string

Determine if including try_number in the UniqueConstraint improves index efficiency.",kaxil,2024-10-28 12:47:52+00:00,[],2025-01-17 15:24:15+00:00,,https://github.com/apache/airflow/issues/43439,"[('area:performance', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2618329793,issue,open,,Investigate batch processing for updating TI UUIDs to avoid long locks on large datasets,"This is part of the future work of https://github.com/apache/airflow/pull/43243 that migrate TI table's primary key to be a UUID7 string

Investigate batch processing for updating UUIDs to avoid long locks on large datasets. Maybe run with batches of 10000 rows??? -- TBD. We can also run some benchmarks once we do that.
",kaxil,2024-10-28 12:47:03+00:00,"['kaxil', 'vatsrahul1001']",2025-01-17 15:24:15+00:00,,https://github.com/apache/airflow/issues/43438,"[('area:performance', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2618327826,issue,open,,Evaluate if the TI History table requires similar updates (TI pk) for UUID v7 compatibility,This is part of the future work of [this PR](https://github.com/apache/airflow/pull/43243) that migrate TI History table's primary key to be a UUID7 string,kaxil,2024-10-28 12:46:11+00:00,[],2025-02-03 15:54:00+00:00,,https://github.com/apache/airflow/issues/43437,"[('area:db-migrations', 'PRs with DB migration'), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]","[{'comment_id': 2631397831, 'issue_id': 2618327826, 'author': 'ashb', 'body': 'See https://github.com/apache/airflow/issues/44147#issuecomment-2631381889 for a lot more discussion on this topic. We should at least keep the UUID `id` column of the TI in the History table.', 'created_at': datetime.datetime(2025, 2, 3, 15, 53, 59, tzinfo=datetime.timezone.utc)}]","ashb on (2025-02-03 15:53:59 UTC): See https://github.com/apache/airflow/issues/44147#issuecomment-2631381889 for a lot more discussion on this topic. We should at least keep the UUID `id` column of the TI in the History table.

"
2617834580,issue,open,,param object do not accept array ,"### Apache Airflow version

2.10.2

### What happened?

I can't trigger a dag with `[ ]` or `[{""toto"": 5}]` ( that are valid JSON )

```python

with DAG(
        dag_id=""a"",
        params={
            ""x"": Param(None, type=""object""),
        })
```

it log

```log
Invalid input for param batch: [{'toto': 5}] is not of type 'object' Failed validating 'type' in schema: {'type': 'object'} On instance: [{'toto': 5}]
```


### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",raphaelauv,2024-10-28 09:27:20+00:00,[],2024-11-05 19:05:29+00:00,,https://github.com/apache/airflow/issues/43433,"[('kind:bug', 'This is a clearly a bug'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('legacy ui', 'Whether legacy UI change should be allowed in PR')]","[{'comment_id': 2453092870, 'issue_id': 2617834580, 'author': 'jscheffl', 'body': 'I assume this is correct.\r\n\r\nThe term ""object"" in the JSON schema validation we run refers to ""dict"" object types. See: https://json-schema.org/understanding-json-schema/reference/object.\r\n\r\nSo in your case you need to define it as `array`. Note that empty arrays in the UI form are always triggered with `none` value... there was also a ticket open for this to discuss how the form shall handle empty fields...', 'created_at': datetime.datetime(2024, 11, 2, 18, 48, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453556417, 'issue_id': 2617834580, 'author': 'raphaelauv', 'body': 'hey , using ""array"" change my value(array) to a string in another array\r\n\r\n`[\'[{""toto"": 5}]\']`\r\n\r\n```python\r\nfrom airflow.models import Param\r\nfrom airflow.operators.python import PythonOperator\r\nfrom airflow.utils.dates import days_ago\r\nfrom airflow import DAG\r\n\r\nwith DAG(\r\n    dag_id=""a"",\r\n    schedule_interval=None,\r\n    start_date=days_ago(1),\r\n    params={\r\n        ""x"": Param([], type=""array""),\r\n    }\r\n):\r\n    def a(toto):\r\n        print(toto)\r\n        print(type(toto))\r\n\r\n    PythonOperator(task_id=""toto"", python_callable=a, op_kwargs={""toto"": ""{{params.x}}""})\r\n```\r\n\r\nso I must use `string` until a better support\r\n\r\n```python\r\nwith DAG(\r\n    dag_id=""a"",\r\n    schedule_interval=None,\r\n    start_date=days_ago(1),\r\n    params={\r\n        ""x"": Param(None, type=""string""),\r\n    }\r\n):\r\n    def a(toto):\r\n        toto = json.loads(toto)\r\n        print(toto)\r\n\r\n    PythonOperator(task_id=""toto"", python_callable=a, op_kwargs={""toto"": ""{{params.x}}""})\r\n```', 'created_at': datetime.datetime(2024, 11, 3, 19, 51, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453609553, 'issue_id': 2617834580, 'author': 'jscheffl', 'body': ""Yea... and no. Please check the details in https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/params.html#use-params-to-provide-a-trigger-ui-form -might not be obvious but exactly for this case a feature was added:\r\n\r\n> If you add the attribute items with a\r\ndictionary that contains a field type\r\nwith a value other than “string”, a JSON entry\r\nfield will be generated for more array types and\r\nadditional type validation as described in\r\n[JSON Schema Array Items](https://json-schema.org/understanding-json-schema/reference/array.html#items).\r\n\r\n...means in case you want to have non-string items be added define an item type and then a JSON style input box will be added just not validating to an object.\r\n\r\nOne example is contained in the example DAGs: https://github.com/apache/airflow/blob/main/airflow/example_dags/example_params_ui_tutorial.py#L136 - just using numbers and not dict's."", 'created_at': datetime.datetime(2024, 11, 3, 23, 0, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2454217893, 'issue_id': 2617834580, 'author': 'raphaelauv', 'body': 'thanks for your help , but I don\'t want to trigger the dag with an array of string or integer, but an array of dict/map\r\n\r\n`[{""toto"": 5,""tata"":""hello""}]`\r\n\r\nI tried \r\n```python\r\n""x"": Param(None, type=""array"",items={""type"": ""object""}),\r\n```\r\n\r\nbut the front fail with\r\n\r\n```\r\nAn invalid form control with name=\'element_x\' is not focusable.\r\n<textarea class=""form-control"" name=""element_x"" id=""element_x"" valuetype=""advancedarray"" rows=""6"" required="""" style=""display: none;""></textarea>\r\n```', 'created_at': datetime.datetime(2024, 11, 4, 9, 36, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455643752, 'issue_id': 2617834580, 'author': 'jscheffl', 'body': 'Okay, @raphaelauv maybe you were hitting a side effect... I did a check on my side adding to airflow/example_dags/example_params_ui_tutorial.py with:\r\n\r\n```\r\n        # An array of numbers\r\n        ""array_of_dicts"": Param(\r\n            [{}, {""key"": ""value""}, {""one"": ""two""}],\r\n            ""Only dicts are accepted in this array"",\r\n            type=""array"",\r\n            title=""Array of dicts"",\r\n            items={""type"": ""object""},\r\n        ),\r\n```\r\n..alongside to other parameters.\r\n\r\nThis renders correctly on my side with:\r\n![image](https://github.com/user-attachments/assets/a91e1b47-2204-4863-9073-7697d07bd910)\r\n\r\nCan you check adding a ""valid"" default value? Because you define `None`per default - which is not valid according to specs. So maybe this special case with a invalid/empty default was not tested. If you want to permit `None` as valid value then you need to set `type=[""none"", ""array""]`.', 'created_at': datetime.datetime(2024, 11, 4, 20, 31, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455707046, 'issue_id': 2617834580, 'author': 'raphaelauv', 'body': 'so with `type=[""none"", ""array""]` dag parsing is failing\r\n\r\n```log\r\nBroken DAG: [/opt/airflow/dags/dags/aaaa.py] Traceback (most recent call last):\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/jsonschema/validators.py"", line 1328, in validate\r\n    cls.check_schema(schema)\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/jsonschema/validators.py"", line 317, in check_schema\r\n    raise exceptions.SchemaError.create_from(error)\r\njsonschema.exceptions.SchemaError: [\'none\', \'array\'] is not valid under any of the given schemas\r\n\r\n```\r\n----\r\n\r\nbut using a not none default value is working\r\n\r\n```python\r\nparams={\r\n            ""x"": Param([{}], type=""array"", items={""type"": ""object""}),\r\n        }\r\n```\r\n\r\nit let me trigger the dag with the argument `[{""toto"": 5,""tata"":""hello""}]`', 'created_at': datetime.datetime(2024, 11, 4, 21, 9, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455742806, 'issue_id': 2617834580, 'author': 'jscheffl', 'body': '> so with `type=[""none"", ""array""]` dag parsing is failing\r\n\r\naaah, sorry RTFM.... https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/params.html --> `type=[""null"", ""array""]` is the right typing in JSON schema...', 'created_at': datetime.datetime(2024, 11, 4, 21, 31, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455814004, 'issue_id': 2617834580, 'author': 'raphaelauv', 'body': 'yeah it work\r\n\r\n```python\r\nparams={\r\n    ""x"": Param(None, type=[""null"", ""array""], items={""type"": ""object""}),\r\n}\r\n```\r\n\r\nso conclusion it\'s not obvious, if we decide to not let accept array in the type `object`\r\nthan I will add a little example in the doc', 'created_at': datetime.datetime(2024, 11, 4, 22, 17, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2457950263, 'issue_id': 2617834580, 'author': 'jscheffl', 'body': 'Yeah - a small hint/clarificatio nin the docs would be good - there might be others running into the same pitfall.\r\n\r\nAlso if the default is None as you initially had... might be a small bug. For DAGs that are not scheduled ""non valid defaults"" are possible for all other fields. So is a small bug in the form generation.', 'created_at': datetime.datetime(2024, 11, 5, 19, 5, 12, tzinfo=datetime.timezone.utc)}]","jscheffl on (2024-11-02 18:48:50 UTC): I assume this is correct.

The term ""object"" in the JSON schema validation we run refers to ""dict"" object types. See: https://json-schema.org/understanding-json-schema/reference/object.

So in your case you need to define it as `array`. Note that empty arrays in the UI form are always triggered with `none` value... there was also a ticket open for this to discuss how the form shall handle empty fields...

raphaelauv (Issue Creator) on (2024-11-03 19:51:01 UTC): hey , using ""array"" change my value(array) to a string in another array

`['[{""toto"": 5}]']`

```python
from airflow.models import Param
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago
from airflow import DAG

with DAG(
    dag_id=""a"",
    schedule_interval=None,
    start_date=days_ago(1),
    params={
        ""x"": Param([], type=""array""),
    }
):
    def a(toto):
        print(toto)
        print(type(toto))

    PythonOperator(task_id=""toto"", python_callable=a, op_kwargs={""toto"": ""{{params.x}}""})
```

so I must use `string` until a better support

```python
with DAG(
    dag_id=""a"",
    schedule_interval=None,
    start_date=days_ago(1),
    params={
        ""x"": Param(None, type=""string""),
    }
):
    def a(toto):
        toto = json.loads(toto)
        print(toto)

    PythonOperator(task_id=""toto"", python_callable=a, op_kwargs={""toto"": ""{{params.x}}""})
```

jscheffl on (2024-11-03 23:00:38 UTC): Yea... and no. Please check the details in https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/params.html#use-params-to-provide-a-trigger-ui-form -might not be obvious but exactly for this case a feature was added:

dictionary that contains a field type
with a value other than “string”, a JSON entry
field will be generated for more array types and
additional type validation as described in
[JSON Schema Array Items](https://json-schema.org/understanding-json-schema/reference/array.html#items).

...means in case you want to have non-string items be added define an item type and then a JSON style input box will be added just not validating to an object.

One example is contained in the example DAGs: https://github.com/apache/airflow/blob/main/airflow/example_dags/example_params_ui_tutorial.py#L136 - just using numbers and not dict's.

raphaelauv (Issue Creator) on (2024-11-04 09:36:59 UTC): thanks for your help , but I don't want to trigger the dag with an array of string or integer, but an array of dict/map

`[{""toto"": 5,""tata"":""hello""}]`

I tried 
```python
""x"": Param(None, type=""array"",items={""type"": ""object""}),
```

but the front fail with

```
An invalid form control with name='element_x' is not focusable.
<textarea class=""form-control"" name=""element_x"" id=""element_x"" valuetype=""advancedarray"" rows=""6"" required="""" style=""display: none;""></textarea>
```

jscheffl on (2024-11-04 20:31:43 UTC): Okay, @raphaelauv maybe you were hitting a side effect... I did a check on my side adding to airflow/example_dags/example_params_ui_tutorial.py with:

```
        # An array of numbers
        ""array_of_dicts"": Param(
            [{}, {""key"": ""value""}, {""one"": ""two""}],
            ""Only dicts are accepted in this array"",
            type=""array"",
            title=""Array of dicts"",
            items={""type"": ""object""},
        ),
```
..alongside to other parameters.

This renders correctly on my side with:
![image](https://github.com/user-attachments/assets/a91e1b47-2204-4863-9073-7697d07bd910)

Can you check adding a ""valid"" default value? Because you define `None`per default - which is not valid according to specs. So maybe this special case with a invalid/empty default was not tested. If you want to permit `None` as valid value then you need to set `type=[""none"", ""array""]`.

raphaelauv (Issue Creator) on (2024-11-04 21:09:30 UTC): so with `type=[""none"", ""array""]` dag parsing is failing

```log
Broken DAG: [/opt/airflow/dags/dags/aaaa.py] Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.12/site-packages/jsonschema/validators.py"", line 1328, in validate
    cls.check_schema(schema)
  File ""/home/airflow/.local/lib/python3.12/site-packages/jsonschema/validators.py"", line 317, in check_schema
    raise exceptions.SchemaError.create_from(error)
jsonschema.exceptions.SchemaError: ['none', 'array'] is not valid under any of the given schemas

```
----

but using a not none default value is working

```python
params={
            ""x"": Param([{}], type=""array"", items={""type"": ""object""}),
        }
```

it let me trigger the dag with the argument `[{""toto"": 5,""tata"":""hello""}]`

jscheffl on (2024-11-04 21:31:18 UTC): aaah, sorry RTFM.... https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/params.html --> `type=[""null"", ""array""]` is the right typing in JSON schema...

raphaelauv (Issue Creator) on (2024-11-04 22:17:34 UTC): yeah it work

```python
params={
    ""x"": Param(None, type=[""null"", ""array""], items={""type"": ""object""}),
}
```

so conclusion it's not obvious, if we decide to not let accept array in the type `object`
than I will add a little example in the doc

jscheffl on (2024-11-05 19:05:12 UTC): Yeah - a small hint/clarificatio nin the docs would be good - there might be others running into the same pitfall.

Also if the default is None as you initially had... might be a small bug. For DAGs that are not scheduled ""non valid defaults"" are possible for all other fields. So is a small bug in the form generation.

"
2617812421,issue,closed,completed,Invalid metric values for completed tasks task.cpu_usage_percent/task.mem_usage_percent,"### Apache Airflow version

2.10.2

### What happened?

We have Airflow `(2.10.2)` in k8s cluster deployed by an official helm-chart. This helm-release contains `statsd` component, and Airflow send its metrics to `statsd`.
We use `celery-executor`, so our tasks are running inside worker-pods.
Also we have Victoriametrics release in this cluster. We scrape metrics from `statsd` with `VMScrapeConfig`.

Out of all the metrics provided by Airflow we need [2 essential ones](https://github.com/apache/airflow/blob/v2-10-stable/docs/apache-airflow/administration-and-deployment/logging-monitoring/metrics.rst#gauges):

- `task.cpu_usage_percent.<dag_id>.<task_id>`
- `task.mem_usage_percent.<dag_id>.<task_id>`

We can see lifecycles of out tasks/dags in airflow-webinterface, so we know when each task started and ended.
But if we get metric values from statsd at the moment when our task already ended (hours after the completion), we still get the `cpu_usage` and `mem_usage` for this task.

### What you think should happen instead?

According to documentation `task.cpu_usage_percent.<dag_id>.<task_id>` and `task.mem_usage_percent.<dag_id>.<task_id>` are gauges that show:

> Percentage of CPU/memory used by a task

So we assume, that if task ended at 02:15 PM, then at 02:16 PM or even later Airflow shouldn't send either `task.cpu_usage_percent.*` or `task.mem_usage_percent.*` for this task to `statsd`, right?

The essential meaning of these 2 metrics is to show how much resources in use for each task/dag at the moment, That way we can visualize dynamic of the resource-usage of Airflow or create alerting-solutions. Correct me if I'm wrong.

### How to reproduce

**Configuration**

statsd:
```yaml
statsd:
  extraMappings:
  - match: airflow.task.cpu_usage.*.*
    name: ""airflow_task_cpu_usage""
    help: ""Percentage of CPU used by a task""
    labels:
      dag_id: ""$1""
      task_id: ""$2""
  - match: airflow.task.mem_usage.*.*
    name: ""airflow_task_mem_usage""
    help: ""Percentage of memory used by a task""
    labels:
      dag_id: ""$1""
      task_id: ""$2""
```

VMScrapeConfig:
```yaml
apiVersion: operator.victoriametrics.com/v1beta1
kind: VMScrapeConfig
metadata:
  name: airflow-service-scrape
  namespace: monitoring
spec:
  staticConfigs:
    - targets: [airflow-statsd.airflow.svc.cluster.local:9102]
  metricsPath: /metrics
  scrapeInterval: 30s
  scrapeTimeout: 15s
```


### Operating System

Kubernetes  v1.31.0-eks

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Even though I have provided `VMScrapeConfig` configuration, there is no need to use it in test-cases for reproduction of the issue. Because we check `statsd` endpoint (on port 9102) also to see the raw metrics delivered from Airflow - it still has the same issue.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",eanikindfi,2024-10-28 09:20:16+00:00,[],2024-10-29 07:19:54+00:00,2024-10-29 07:19:54+00:00,https://github.com/apache/airflow/issues/43432,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:metrics', ''), ('area:core', '')]","[{'comment_id': 2441013664, 'issue_id': 2617812421, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 28, 9, 20, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2441282413, 'issue_id': 2617812421, 'author': 'potiuk', 'body': 'I think, it\'s the nature of statsd and we cannot do much about it (but I am not 100% sure) - and we are moving away from statsd in favour of open-telemetry https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/metrics.html#setup-opentelemetry.\r\n\r\nWhile we are not removing statsd, it\'s very likely we are not going to invest any time into improving or even fixing statsd problems.\r\n\r\nMybe you could switch to using open-telemetry and see if you have similar problem there - this is anyhow ""future"" of airflow telemetry - and it has way better features than statsd (including support for open-telemetry traces in 2.10.*) - so the best course of action would be to try it out @eanikindfi - otherwise you might hope  that some will look at this one, but it\'s unlikely statsd will get a lot of love from the side of maintainers. \r\n\r\ncc: @howardyoo @ferruzzi', 'created_at': datetime.datetime(2024, 10, 28, 11, 8, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2442561210, 'issue_id': 2617812421, 'author': 'ferruzzi', 'body': 'It\'s an interesting one.  As it stands, the metric emits the current loads every so often, but there is no code in the task cleanup stage which triggers that to update down (is downdate a word?) to 0%.  It would be an interesting puzzle.  At what point should that get zeroed out?  If it is done during the cleanup stage and something happens, then it\'s misleading because it hasn\'t actually finished yet.  \r\n\r\nPerhaps some kind of check for ""get all the tasks which have completed since the last time these metrics were updated and zero out their resource metrics""? \r\n\r\nI wonder if this can be handled on the user/dashboarding end, something like ""if the percentages haven\'t been updated after a certain amount of time then display them as zero""?', 'created_at': datetime.datetime(2024, 10, 28, 20, 29, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443416580, 'issue_id': 2617812421, 'author': 'potiuk', 'body': 'Yeah. It looks like a ""recipient-only""  thing when you can compare status of the task with metrics.\r\n\r\nLet me convert it to a discussion, because it\'s unlikely to be solved differently.', 'created_at': datetime.datetime(2024, 10, 29, 7, 19, 49, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-28 09:20:18 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-10-28 11:08:59 UTC): I think, it's the nature of statsd and we cannot do much about it (but I am not 100% sure) - and we are moving away from statsd in favour of open-telemetry https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/metrics.html#setup-opentelemetry.

While we are not removing statsd, it's very likely we are not going to invest any time into improving or even fixing statsd problems.

Mybe you could switch to using open-telemetry and see if you have similar problem there - this is anyhow ""future"" of airflow telemetry - and it has way better features than statsd (including support for open-telemetry traces in 2.10.*) - so the best course of action would be to try it out @eanikindfi - otherwise you might hope  that some will look at this one, but it's unlikely statsd will get a lot of love from the side of maintainers. 

cc: @howardyoo @ferruzzi

ferruzzi on (2024-10-28 20:29:56 UTC): It's an interesting one.  As it stands, the metric emits the current loads every so often, but there is no code in the task cleanup stage which triggers that to update down (is downdate a word?) to 0%.  It would be an interesting puzzle.  At what point should that get zeroed out?  If it is done during the cleanup stage and something happens, then it's misleading because it hasn't actually finished yet.  

Perhaps some kind of check for ""get all the tasks which have completed since the last time these metrics were updated and zero out their resource metrics""? 

I wonder if this can be handled on the user/dashboarding end, something like ""if the percentages haven't been updated after a certain amount of time then display them as zero""?

potiuk on (2024-10-29 07:19:49 UTC): Yeah. It looks like a ""recipient-only""  thing when you can compare status of the task with metrics.

Let me convert it to a discussion, because it's unlikely to be solved differently.

"
2617714374,issue,open,,AIP-84 Sync documentation and permissions,"More context here https://github.com/apache/airflow/issues/43140#issuecomment-2431304177

To avoid additional mismatch between the code and the documentation, we could add a small layer to automatically detect and document the API permissions for each route. Ideally the swagger and maybe also this [documentation](https://airflow.apache.org/docs/apache-airflow-providers-fab/stable/auth-manager/access-control.html#dag-level-permissions) page  should be automatically populated from the FastAPI code.

> Note: To be able to work on this, [authentication and permissions](https://github.com/apache/airflow/issues/42360) need to be implemented first.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-10-28 08:44:51+00:00,['SuccessMoses'],2024-11-12 09:47:46+00:00,,https://github.com/apache/airflow/issues/43430,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('good first issue', ''), ('kind:documentation', '')]","[{'comment_id': 2441258201, 'issue_id': 2617714374, 'author': 'potiuk', 'body': 'Love it. Might be great first contribution after the main implementation is done :)', 'created_at': datetime.datetime(2024, 10, 28, 10, 58, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466442404, 'issue_id': 2617714374, 'author': 'SuccessMoses', 'body': '@potiuk I want to work on this', 'created_at': datetime.datetime(2024, 11, 9, 20, 16, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466451623, 'issue_id': 2617714374, 'author': 'SuccessMoses', 'body': 'is this a good first issue', 'created_at': datetime.datetime(2024, 11, 9, 20, 32, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2470066220, 'issue_id': 2617714374, 'author': 'pierrejeambrun', 'body': '@SuccessMoses\r\n\r\nThank you for your interest. Yes this is a good first issue, you are assigned :)', 'created_at': datetime.datetime(2024, 11, 12, 9, 47, 45, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-10-28 10:58:04 UTC): Love it. Might be great first contribution after the main implementation is done :)

SuccessMoses (Assginee) on (2024-11-09 20:16:52 UTC): @potiuk I want to work on this

SuccessMoses (Assginee) on (2024-11-09 20:32:36 UTC): is this a good first issue

pierrejeambrun (Issue Creator) on (2024-11-12 09:47:45 UTC): @SuccessMoses

Thank you for your interest. Yes this is a good first issue, you are assigned :)

"
2617389323,issue,closed,completed,`otel_debugging_on` emits traces ONLY to the console and not to the configured otel host,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I configured Airflow with the following OpenTelemetry Traces configuration - 

```
airflow:
  image:
    tag: 2.10.2-python3.12
  extraPipPackages:
    - ""apache-airflow[otel]""
  config:
    AIRFLOW__TRACES__OTEL_DEBUGGING_ON: ""True""
    AIRFLOW__TRACES__OTEL_ON: ""True""
    AIRFLOW__TRACES__OTEL_HOST: ""my_otel_host""
    AIRFLOW__TRACES__OTEL_PORT: <my_otel_port>
    AIRFLOW__TRACES__OTEL_SERVICE: ""airflow""
```
    
  The above configuration when applied should emit airflow traces to the configured otel host. But they are only getting emitted in the console and not to the configured otel host.
  The reason for this is the `otel_debugging_on` attribute. When this is set to `true`, we see the above behaviour where the airflow does not emit traces to the host and only emits in the console. The [airflow docs](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#config-traces-otel-debugging-on) also do mention that it will emit traces to both console and the otel host.
  If the attribute is removed from the otel traces config or is set to `False` we can see the traces emitted to the configured host.
  
  Proposed Change - 
  The `otel_debugging_on` when set to true in otel traces configuration, the traces should get emitted to both the console and the configured otel host.
  
  
 Airflow Version Used - `2.10.2`

### What you think should happen instead?

_No response_

### How to reproduce

Configure your airflow with the following traces configuration and check your otel host for traces. The traces should be visible with the config applied - 

```
    AIRFLOW__TRACES__OTEL_ON: ""True""
    AIRFLOW__TRACES__OTEL_HOST: ""my_otel_host""
    AIRFLOW__TRACES__OTEL_PORT: <my_otel_port>
    AIRFLOW__TRACES__OTEL_SERVICE: ""airflow""
```

Now add the following attribute within the above configured traces config. Once this is added, the traces are no longer visible in the configured otel host but are visible in the console/logs.

```
    AIRFLOW__TRACES__OTEL_DEBUGGING_ON: ""True""
```

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other 3rd-party Helm chart

### Deployment details

Used [airflow community helm chart](https://github.com/airflow-helm/charts/tree/main/charts/airflow) for deploying airflow.

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",ninad-opsverse,2024-10-28 06:00:13+00:00,[],2024-10-31 16:39:30+00:00,2024-10-31 16:39:30+00:00,https://github.com/apache/airflow/issues/43427,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:core', ''), ('telemetry', 'Telemetry-related issues')]","[{'comment_id': 2440620075, 'issue_id': 2617389323, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 28, 6, 0, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2441228409, 'issue_id': 2617389323, 'author': 'potiuk', 'body': 'This looks like reasonable thing to do. @howardyoo @ferruzzi  ?', 'created_at': datetime.datetime(2024, 10, 28, 10, 44, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2441636853, 'issue_id': 2617389323, 'author': 'ninad-opsverse', 'body': 'Hey, I will raise a PR for this in a bit.', 'created_at': datetime.datetime(2024, 10, 28, 13, 46, 16, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-28 06:00:16 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-10-28 10:44:28 UTC): This looks like reasonable thing to do. @howardyoo @ferruzzi  ?

ninad-opsverse (Issue Creator) on (2024-10-28 13:46:16 UTC): Hey, I will raise a PR for this in a bit.

"
2616662311,issue,closed,completed,"Status of testing Providers that were prepared on October 27, 2024","### Body


I have a kind request for all the contributors to the latest provider packages release.
Could you please help us to test the RC versions of the providers?

The guidelines on how to test providers can be found in

[Verify providers by contributors](https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDER_PACKAGES.md#verify-the-release-candidate-by-contributors)

Let us know in the comment, whether the issue is addressed.

Those are providers that require testing as there were some substantial changes introduced:


## Provider [amazon: 9.1.0rc1](https://pypi.org/project/apache-airflow-providers-amazon/9.1.0rc1)
   - [ ] [feat: add OpenLineage support for RedshiftToS3Operator (#41632)](https://github.com/apache/airflow/pull/41632): @kacpermuda
     Linked issues:
       - [ ] [Linked Issue #41575](https://github.com/apache/airflow/pull/41575): @Artuz37
   - [ ] [Add SageMakerProcessingSensor (#43144)](https://github.com/apache/airflow/pull/43144): @vVv-AA
   - [ ] [Make `RedshiftDataOperator`  handle multiple queries (#42900)](https://github.com/apache/airflow/pull/42900): @jroachgolf84
   - [ ] [Remove sqlalchemy-redshift dependency (#43271)](https://github.com/apache/airflow/pull/43271): @mobuchowski
   - [ ] [feat(providers/amazon): Use asset in common provider (#43110)](https://github.com/apache/airflow/pull/43110): @Lee-W
   - [x] [Restrict looker-sdk version 24.18.0 and microsoft-kiota-http 1.3.4 (#42954)](https://github.com/apache/airflow/pull/42954): @gopidesupavan
## Provider [apache.beam: 5.9.0rc1](https://pypi.org/project/apache-airflow-providers-apache-beam/5.9.0rc1)
   - [ ] [Add early job_id xcom_push for google provider Beam Pipeline operators (#42982)](https://github.com/apache/airflow/pull/42982): @olegkachur-e
## Provider [apache.druid: 3.12.0rc1](https://pypi.org/project/apache-airflow-providers-apache-druid/3.12.0rc1)
   - [ ] [Add possibility to override the conn type for Druid (#42793)](https://github.com/apache/airflow/pull/42793): @Rasnar
## Provider [apache.hdfs: 4.6.0rc1](https://pypi.org/project/apache-airflow-providers-apache-hdfs/4.6.0rc1)
   - [ ] [added MultipleFilesWebHdfsSensor (#43045)](https://github.com/apache/airflow/pull/43045): @eilon246810
   - [x] [Fixed failing static checks & provider tests (#43122)](https://github.com/apache/airflow/pull/43122): @kaxil
## Provider [apache.livy: 3.9.2rc1](https://pypi.org/project/apache-airflow-providers-apache-livy/3.9.2rc1)
   - [x] [Add lower bound to asgiref in providers (#43001)](https://github.com/apache/airflow/pull/43001): @rawwar
## Provider [apache.spark: 4.11.2rc1](https://pypi.org/project/apache-airflow-providers-apache-spark/4.11.2rc1)
   - [ ] [Changed conf property from str to dict in SparkSqlOperator (#42835)](https://github.com/apache/airflow/pull/42835): @duhizjame
## Provider [cloudant: 4.0.2rc1](https://pypi.org/project/apache-airflow-providers-cloudant/4.0.2rc1)
   - [ ] [More correctly work around `uv sync` issue with python 3.10+ only dep (#43227)](https://github.com/apache/airflow/pull/43227): @ashb
   - [ ] [Update cloudant versions to be py-version aware so `uv lock` can work. (#43217)](https://github.com/apache/airflow/pull/43217): @ashb
     Linked issues:
       - [ ] [Linked Issue #41555](https://github.com/apache/airflow/pull/41555): @topherinternational
       - [ ] [Linked Issue #21004](https://github.com/apache/airflow/issues/21004): @uranusjr
## Provider [cncf.kubernetes: 9.0.1rc1](https://pypi.org/project/apache-airflow-providers-cncf-kubernetes/9.0.1rc1)
   - [ ] [(fix): HybridExecutor tasks of other executor rescheduled in kubernetes executor (#43003)](https://github.com/apache/airflow/pull/43003): @pavansharma36
     Linked issues:
       - [ ] [Linked Issue #42151](https://github.com/apache/airflow/issues/42151): @iw-pavan
   - [ ] [fix: use instance base_container_name to fetch logs on trigger_reentry (#42960)](https://github.com/apache/airflow/pull/42960): @peloyeje
   - [x] [Add kubernetes_conn_id  to templated fields (#42786)](https://github.com/apache/airflow/pull/42786): @gopidesupavan
     Linked issues:
       - [ ] [Linked Issue #42546](https://github.com/apache/airflow/issues/42546): @nikhilkarve
   - [x] [✨ Allow node_selector templating in KPO (#43051)](https://github.com/apache/airflow/pull/43051): @bdsoha
   - [ ] [Adding support for Kubernetes package version 31.0.0 (#42907)](https://github.com/apache/airflow/pull/42907): @dirrao
## Provider [common.sql: 1.19.0rc1](https://pypi.org/project/apache-airflow-providers-common-sql/1.19.0rc1)
   - [ ] [Feature: Added fast_executemany parameter to insert_rows of DbApiHook (#43357)](https://github.com/apache/airflow/pull/43357): @dabla
   - [ ] [Make conn id parameters templated in GenericTransfer and also allow passing hook parameters like in BaseSQLOperator (#42891)](https://github.com/apache/airflow/pull/42891): @dabla
   - [x] [SqlSensor enhancement: #37437 (#43107)](https://github.com/apache/airflow/pull/43107): @vVv-AA
     Linked issues:
       - [x] [Linked Issue #37437](https://github.com/apache/airflow/issues/37437): @amolsr
   - [x] [fix mypy failure(provider) in SqlSensor (#43206)](https://github.com/apache/airflow/pull/43206): @rawwar
## Provider [databricks: 6.12.0rc1](https://pypi.org/project/apache-airflow-providers-databricks/6.12.0rc1)
   - [x] [Add TimeoutError to be a retryable error in databricks provider (#43137)](https://github.com/apache/airflow/pull/43137): @rawwar
     Linked issues:
       - [x] [Linked Issue #43128](https://github.com/apache/airflow/issues/43128): @rawwar
   - [x] [Add ClientConnectorError to be a retryable error in databricks provider (#43091)](https://github.com/apache/airflow/pull/43091): @rawwar
   - [ ] [DatabricksHook: fix status property to work with ClientResponse used in async mode (#43333)](https://github.com/apache/airflow/pull/43333): @lucafurrer
     Linked issues:
       - [ ] [Linked Issue #43269](https://github.com/apache/airflow/issues/43269): @lucafurrer
   - [x] [[DatabricksHook] Respect connection settings (#42618)](https://github.com/apache/airflow/pull/42618): @xitep
## Provider [dbt.cloud: 3.11.1rc1](https://pypi.org/project/apache-airflow-providers-dbt-cloud/3.11.1rc1)
   - [x] [Add lower bound to asgiref in providers (#43001)](https://github.com/apache/airflow/pull/43001): @rawwar
## Provider [fab: 1.5.0rc1](https://pypi.org/project/apache-airflow-providers-fab/1.5.0rc1)
   - [ ] [feat(providers/fab): Use asset in common provider (#43112)](https://github.com/apache/airflow/pull/43112): @Lee-W
   - [ ] [Fix revoke Dag stale permission on airflow < 2.10 (#42844)](https://github.com/apache/airflow/pull/42844): @joaopamaral
     Linked issues:
       - [ ] [Linked Issue #42743](https://github.com/apache/airflow/issues/42743): @RostD
   - [ ] [Bump Flask-AppBuilder to ``4.5.2`` (#43309)](https://github.com/apache/airflow/pull/43309): @kaxil
   - [x] [Upgrade FAB to 4.5.1 (#43251)](https://github.com/apache/airflow/pull/43251): @potiuk
   - [ ] [Move user and roles schemas to fab provider (#42869)](https://github.com/apache/airflow/pull/42869): @vincbeck
   - [ ] [Move the session auth backend to FAB auth manager (#42878)](https://github.com/apache/airflow/pull/42878): @vincbeck
     Linked issues:
       - [ ] [Linked Issue #42634](https://github.com/apache/airflow/pull/42634): @vincbeck
## Provider [google: 10.25.0rc1](https://pypi.org/project/apache-airflow-providers-google/10.25.0rc1)
   - [ ] [feat: add Hook Level Lineage support for GCSHook (#42507)](https://github.com/apache/airflow/pull/42507): @kacpermuda
     Linked issues:
       - [ ] [Linked Issue #40819](https://github.com/apache/airflow/pull/40819): @mobuchowski
   - [ ] [feat: sensor to check status of Dataform action (#43055)](https://github.com/apache/airflow/pull/43055): @steve148
   - [ ] [Create Operators for Google Cloud Vertex AI Context Caching (#43008)](https://github.com/apache/airflow/pull/43008): @CYarros10
   - [x] [Fix outdated CloudRunExecuteJobOperator docs (#43195)](https://github.com/apache/airflow/pull/43195): @p13rr0m
   - [ ] [Fix TestTranslationLegacyModelPredictLink dataset_id error for google provider translation operator (#42463)](https://github.com/apache/airflow/pull/42463): @olegkachur-e
   - [ ] [Add a debug log for creating batch workloads in dataproc (#43265)](https://github.com/apache/airflow/pull/43265): @romsharon98
   - [x] [add min version to plyvel (#43129)](https://github.com/apache/airflow/pull/43129): @rawwar
   - [ ] [vertex ai training operators: add display_name to rendered fields (#43028)](https://github.com/apache/airflow/pull/43028): @WSHoekstra
     Linked issues:
       - [ ] [Linked Issue #43027](https://github.com/apache/airflow/issues/43027): @WSHoekstra
   - [x] [Make google provider pyarrow dependency explicit (#42996)](https://github.com/apache/airflow/pull/42996): @saucoide
     Linked issues:
       - [x] [Linked Issue #42924](https://github.com/apache/airflow/issues/42924): @saucoide
   - [x] [Restrict looker-sdk version 24.18.0 and microsoft-kiota-http 1.3.4 (#42954)](https://github.com/apache/airflow/pull/42954): @gopidesupavan
## Provider [http: 4.13.2rc1](https://pypi.org/project/apache-airflow-providers-http/4.13.2rc1)
   - [x] [fix: HttpSensorTrigger to include `method` when serializing (#42925)](https://github.com/apache/airflow/pull/42925): @rawwar
   - [ ] [Use url_from_endpoint inside HttpHook. (#42785)](https://github.com/apache/airflow/pull/42785): @simi
   - [x] [add lowerbound to requests-toolbelt and replace requests_toolbelt  with requests-toolbelt (#43020)](https://github.com/apache/airflow/pull/43020): @rawwar
     Linked issues:
       - [x] [Linked Issue #42989](https://github.com/apache/airflow/issues/42989): @potiuk
   - [x] [Add lower bound to asgiref in providers (#43001)](https://github.com/apache/airflow/pull/43001): @rawwar
## Provider [jenkins: 3.7.2rc1](https://pypi.org/project/apache-airflow-providers-jenkins/3.7.2rc1)
   - [ ] [Remove unnecessary return value from `jenkins_request_with_headers` (#43207)](https://github.com/apache/airflow/pull/43207): @romsharon98
## Provider [microsoft.azure: 11.0.0rc1](https://pypi.org/project/apache-airflow-providers-microsoft-azure/11.0.0rc1)
   - [x] [Add context to Azure Service Bus Message callback (#43370)](https://github.com/apache/airflow/pull/43370): @perry2of5
   - [ ] [Feature: Added event_handler parameter in MSGraphAsyncOperator (#42539)](https://github.com/apache/airflow/pull/42539): @dabla
   - [ ] [Add documentation for the PowerBIDatasetRefresh Operator. (#42754)](https://github.com/apache/airflow/pull/42754): @ambika-garg
   - [x] [Add upperbound to microsoft-kiota-abstractions (#43021)](https://github.com/apache/airflow/pull/43021): @rawwar
   - [x] [Restrict looker-sdk version 24.18.0 and microsoft-kiota-http 1.3.4 (#42954)](https://github.com/apache/airflow/pull/42954): @gopidesupavan
## Provider [mongo: 4.2.2rc1](https://pypi.org/project/apache-airflow-providers-mongo/4.2.2rc1)
   - [x] [fix(providers/mongo): prevent applying lower method on bool field (#43024)](https://github.com/apache/airflow/pull/43024): @josix
     Linked issues:
       - [x] [Linked Issue #42930](https://github.com/apache/airflow/issues/42930): @FarouziAbir
## Provider [mysql: 5.7.3rc1](https://pypi.org/project/apache-airflow-providers-mysql/5.7.3rc1)
   - [x] [Improve the cursor type definition of mysql  (#43376)](https://github.com/apache/airflow/pull/43376): @yangyulely
## Provider [odbc: 4.8.0rc1](https://pypi.org/project/apache-airflow-providers-odbc/4.8.0rc1)
   - [ ] [Also use ODBC connection for sqlalchemy engine in OdbcHook like JdbcHook (#43145)](https://github.com/apache/airflow/pull/43145): @dabla
## Provider [openlineage: 1.13.0rc1](https://pypi.org/project/apache-airflow-providers-openlineage/1.13.0rc1)
   - [ ] [feat(providers/openlineage): Use asset in common provider (#43111)](https://github.com/apache/airflow/pull/43111): @Lee-W
   - [ ] [Ignore attr-defined for compat import (#43301)](https://github.com/apache/airflow/pull/43301): @uranusjr
     Linked issues:
       - [ ] [Linked Issue #43142](https://github.com/apache/airflow/pull/43142): @Lee-W
   - [ ] [nit: remove taskgroup's tooltip from OL's AirflowJobFacet (#43152)](https://github.com/apache/airflow/pull/43152): @kacpermuda
   - [ ] [require 1.2.1 common.compat for openlineage provider (#43039)](https://github.com/apache/airflow/pull/43039): @mobuchowski
## Provider [oracle: 3.12.0rc1](https://pypi.org/project/apache-airflow-providers-oracle/3.12.0rc1)
   - [x] [Add sequence insert support to OracleHook (#42947)](https://github.com/apache/airflow/pull/42947): @Lee2532
## Provider [slack: 8.9.1rc1](https://pypi.org/project/apache-airflow-providers-slack/8.9.1rc1)
   - [ ] [adding support for snippet type in slack api (#43305)](https://github.com/apache/airflow/pull/43305): @Bowrna
   - [ ] [passing the filetype for SlackAPIFileOperator (#43069)](https://github.com/apache/airflow/pull/43069): @Bowrna
     Linked issues:
       - [ ] [Linked Issue #42889](https://github.com/apache/airflow/issues/42889): @hzlmn
## Provider [ssh: 3.14.0rc1](https://pypi.org/project/apache-airflow-providers-ssh/3.14.0rc1)
   - [x] [SSHHook expose auth_timeout parameter (#43048)](https://github.com/apache/airflow/pull/43048): @gyandeeps

<!--

NOTE TO RELEASE MANAGER:

You can move here the providers that have doc-only changes or for which changes are trivial, and
you could assess that they are OK.

-->
All users involved in the PRs:
@steve148 @gyandeeps @mobuchowski @gopidesupavan @uranusjr @dabla @kacpermuda @eilon246810 @kaxil @ambika-garg @Bowrna @joaopamaral @pavansharma36 @rawwar @vVv-AA @bdsoha @olegkachur-e @p13rr0m @romshar


### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2024-10-27 15:07:00+00:00,[],2024-10-31 15:11:14+00:00,2024-10-31 07:16:35+00:00,https://github.com/apache/airflow/issues/43416,"[('area:providers', ''), ('kind:meta', 'High-level information important to the community'), ('testing status', 'Status of testing releases')]","[{'comment_id': 2440216558, 'issue_id': 2616662311, 'author': 'potiuk', 'body': 'Checked my changes - all look good !', 'created_at': datetime.datetime(2024, 10, 27, 23, 25, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2440439607, 'issue_id': 2616662311, 'author': 'josix', 'body': 'Checked [mongo: 4.2.2rc1](https://pypi.org/project/apache-airflow-providers-mongo/4.2.2rc1), it works as expected, thanks!', 'created_at': datetime.datetime(2024, 10, 28, 3, 0, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2440628395, 'issue_id': 2616662311, 'author': 'bdsoha', 'body': 'Checked [cncf kubernetes/9.0.1rc1](https://pypi.org/project/apache-airflow-providers-cncf-kubernetes/9.0.1rc1), looks OK!', 'created_at': datetime.datetime(2024, 10, 28, 6, 5, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2440672447, 'issue_id': 2616662311, 'author': 'Lee2532', 'body': 'checked [oracle: 3.12.0rc1](https://pypi.org/project/apache-airflow-providers-oracle/3.12.0rc1), it works, thanks!', 'created_at': datetime.datetime(2024, 10, 28, 6, 36, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2441320989, 'issue_id': 2616662311, 'author': 'rawwar', 'body': ""Checked following and don't see any issues.\r\n#43001 , #43206, #43001  ,#43129 ,#42925 ,#43020 , #43001 ,#43021"", 'created_at': datetime.datetime(2024, 10, 28, 11, 27, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2441715692, 'issue_id': 2616662311, 'author': 'gyandeeps', 'body': 'Checked [ssh: 3.14.0rc1](https://pypi.org/project/apache-airflow-providers-ssh/3.14.0rc1) and it looks good. Thanks.', 'created_at': datetime.datetime(2024, 10, 28, 14, 15, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2442072363, 'issue_id': 2616662311, 'author': 'perry2of5', 'body': 'Azure Service Bus message callbacks look good in 11.0.0rc1 (from PR #[43370](https://github.com/apache/airflow/pull/43370))', 'created_at': datetime.datetime(2024, 10, 28, 16, 31, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2442324075, 'issue_id': 2616662311, 'author': 'p13rr0m', 'body': 'Checked https://github.com/apache/airflow/pull/43195. Looks good as well.', 'created_at': datetime.datetime(2024, 10, 28, 18, 27, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443461453, 'issue_id': 2616662311, 'author': 'xitep', 'body': 'Verified #42618 locally with `apache-airflow-providers-databricks==6.12.0rc1` ... works as expected for me', 'created_at': datetime.datetime(2024, 10, 29, 7, 36, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443469827, 'issue_id': 2616662311, 'author': 'saucoide', 'body': 'checked #42996 / #42924', 'created_at': datetime.datetime(2024, 10, 29, 7, 41, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443484451, 'issue_id': 2616662311, 'author': 'yangyulely', 'body': 'Checked [mysql: 5.7.3rc1](https://pypi.org/project/apache-airflow-providers-mysql/5.7.3rc1), all looks good.', 'created_at': datetime.datetime(2024, 10, 29, 7, 50, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2444035805, 'issue_id': 2616662311, 'author': 'Lee-W', 'body': '[amazon: 9.1.0rc1](https://pypi.org/project/apache-airflow-providers-amazon/9.1.0rc1) and [fab: 1.5.0rc1](https://pypi.org/project/apache-airflow-providers-fab/1.5.0rc1) do not work with airflow 2.10.x on dataset permissions.\r\n\r\ncorresponding PRs have been sent\r\n\r\nhttps://github.com/apache/airflow/pull/43470\r\nhttps://github.com/apache/airflow/pull/43469', 'created_at': datetime.datetime(2024, 10, 29, 12, 10, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445821681, 'issue_id': 2616662311, 'author': 'gopidesupavan', 'body': 'Checked my changes all are okay!..', 'created_at': datetime.datetime(2024, 10, 30, 4, 30, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2446020229, 'issue_id': 2616662311, 'author': 'eladkal', 'body': 'I will exclude amazon and fab from this release', 'created_at': datetime.datetime(2024, 10, 30, 7, 4, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2446043586, 'issue_id': 2616662311, 'author': 'vVv-AA', 'body': 'Tested SqlSensor enhancement: #37437 (#43107)\r\nLooks ok.\r\n\r\nOn Wed, 30 Oct 2024, 12:35 Elad Kalif, ***@***.***> wrote:\r\n\r\n> I will exclude amazon and fab from this release\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/apache/airflow/issues/43416#issuecomment-2446020229>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/ADAMDO67YQGZYERCZYBRO4LZ6CAJ5AVCNFSM6AAAAABQV42RR2VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDINBWGAZDAMRSHE>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>', 'created_at': datetime.datetime(2024, 10, 30, 7, 14, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2446157037, 'issue_id': 2616662311, 'author': 'dabla', 'body': 'Will check mine tonight or tomorrow', 'created_at': datetime.datetime(2024, 10, 30, 8, 21, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2449181514, 'issue_id': 2616662311, 'author': 'eladkal', 'body': 'Thank you everyone. Providers are released.\r\nfab and amazon providers are excluded from this wave\r\n\r\nI invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).', 'created_at': datetime.datetime(2024, 10, 31, 7, 16, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2450131492, 'issue_id': 2616662311, 'author': 'dabla', 'body': '[43357](https://github.com/apache/airflow/pull/43357)\r\n[43145](https://github.com/apache/airflow/pull/43145)\r\n\r\nA bit late, but works as expected', 'created_at': datetime.datetime(2024, 10, 31, 15, 11, 13, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-10-27 23:25:10 UTC): Checked my changes - all look good !

josix on (2024-10-28 03:00:15 UTC): Checked [mongo: 4.2.2rc1](https://pypi.org/project/apache-airflow-providers-mongo/4.2.2rc1), it works as expected, thanks!

bdsoha on (2024-10-28 06:05:39 UTC): Checked [cncf kubernetes/9.0.1rc1](https://pypi.org/project/apache-airflow-providers-cncf-kubernetes/9.0.1rc1), looks OK!

Lee2532 on (2024-10-28 06:36:43 UTC): checked [oracle: 3.12.0rc1](https://pypi.org/project/apache-airflow-providers-oracle/3.12.0rc1), it works, thanks!

rawwar on (2024-10-28 11:27:21 UTC): Checked following and don't see any issues.
#43001 , #43206, #43001  ,#43129 ,#42925 ,#43020 , #43001 ,#43021

gyandeeps on (2024-10-28 14:15:29 UTC): Checked [ssh: 3.14.0rc1](https://pypi.org/project/apache-airflow-providers-ssh/3.14.0rc1) and it looks good. Thanks.

perry2of5 on (2024-10-28 16:31:04 UTC): Azure Service Bus message callbacks look good in 11.0.0rc1 (from PR #[43370](https://github.com/apache/airflow/pull/43370))

p13rr0m on (2024-10-28 18:27:20 UTC): Checked https://github.com/apache/airflow/pull/43195. Looks good as well.

xitep on (2024-10-29 07:36:20 UTC): Verified #42618 locally with `apache-airflow-providers-databricks==6.12.0rc1` ... works as expected for me

saucoide on (2024-10-29 07:41:26 UTC): checked #42996 / #42924

yangyulely on (2024-10-29 07:50:10 UTC): Checked [mysql: 5.7.3rc1](https://pypi.org/project/apache-airflow-providers-mysql/5.7.3rc1), all looks good.

Lee-W on (2024-10-29 12:10:14 UTC): [amazon: 9.1.0rc1](https://pypi.org/project/apache-airflow-providers-amazon/9.1.0rc1) and [fab: 1.5.0rc1](https://pypi.org/project/apache-airflow-providers-fab/1.5.0rc1) do not work with airflow 2.10.x on dataset permissions.

corresponding PRs have been sent

https://github.com/apache/airflow/pull/43470
https://github.com/apache/airflow/pull/43469

gopidesupavan on (2024-10-30 04:30:56 UTC): Checked my changes all are okay!..

eladkal (Issue Creator) on (2024-10-30 07:04:38 UTC): I will exclude amazon and fab from this release

vVv-AA on (2024-10-30 07:14:49 UTC): Tested SqlSensor enhancement: #37437 (#43107)
Looks ok.

On Wed, 30 Oct 2024, 12:35 Elad Kalif, ***@***.***> wrote:

dabla on (2024-10-30 08:21:31 UTC): Will check mine tonight or tomorrow

eladkal (Issue Creator) on (2024-10-31 07:16:35 UTC): Thank you everyone. Providers are released.
fab and amazon providers are excluded from this wave

I invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).

dabla on (2024-10-31 15:11:13 UTC): [43357](https://github.com/apache/airflow/pull/43357)
[43145](https://github.com/apache/airflow/pull/43145)

A bit late, but works as expected

"
2616462442,issue,closed,completed,Connection UI Error,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Airflow is running correctly but creating a connection It shows

Something bad has happened. For security reasons detailed information about the error is not logged.

  * You should check your webserver logs and retrieve details of this error.

  * When you get the logs, it might explain the reasons, you should also Look for similar issues using:

     * [GitHub Discussions](https://github.com/apache/airflow/discussions)
     * [GitHub Issues](https://github.com/apache/airflow/issues)
     * [Stack Overflow](https://stackoverflow.com/questions/tagged/airflow)
     * the usual search engine you use on a daily basis

    All those resources might help you to find a solution to your problem.

  * if you run Airflow on a Managed Service, consider opening an issue using the service support channels

  * only after you tried it all, and have difficulty with diagnosing and fixing the problem yourself,
    get the logs with errors, describe results of your investigation so far, and consider creating a
    [bug report](https://github.com/apache/airflow/issues/new/choose) including this information.

Python version: 3.12.3
Airflow version: 2.10.2
Node: redacted
-------------------------------------------------------------------------------
Error! Please contact server admin.

### What you think should happen instead?

_No response_

### How to reproduce

I don't understand the problem It is just shows the error you may see

### Operating System

Windows

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

I'm running airflow in EC2 Instance

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",umair7228,2024-10-27 09:50:06+00:00,[],2024-10-27 21:45:57+00:00,2024-10-27 21:45:57+00:00,https://github.com/apache/airflow/issues/43411,"[('kind:bug', 'This is a clearly a bug'), ('area:webserver', 'Webserver related Issues'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2439931466, 'issue_id': 2616462442, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 27, 9, 50, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2440025614, 'issue_id': 2616462442, 'author': 'tirkarthi', 'body': 'Can you please post the traceback if any from the websever logs removing sensitive information?', 'created_at': datetime.datetime(2024, 10, 27, 13, 31, 8, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-27 09:50:08 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

tirkarthi on (2024-10-27 13:31:08 UTC): Can you please post the traceback if any from the websever logs removing sensitive information?

"
2614253478,issue,open,reopened,AIP-38 Add some ids for external tools / integration tests,"### Body

Currently the new UI does not contain any identifier in the DOM to allow external tools (plugins/integration tests framework) to find and manipulates element. Targeting via sibling and child selectors is not reliable.

Some of the major components should have an identifier, we could think of:
- Navigation links
- Toobar (Filters + Searchbar)
- Main content
- Tables, etc...

This should be done for existing code, but a strategy should be in place to know when and how we should add such identifiers during development time.-

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-10-25 14:19:54+00:00,['vatsrahul1001'],2024-11-06 17:58:47+00:00,,https://github.com/apache/airflow/issues/43381,"[('good first issue', ''), ('kind:meta', 'High-level information important to the community'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]","[{'comment_id': 2437964759, 'issue_id': 2614253478, 'author': 'pierrejeambrun', 'body': 'cc: @vatsrahul1001', 'created_at': datetime.datetime(2024, 10, 25, 14, 26, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2438109912, 'issue_id': 2614253478, 'author': 'bbovenzi', 'body': '@vatsrahul1001 Can you tell me more about where and how we should tag elements for your testing? We should probably put in more `data-testid`s too that we can easily use for unit tests as well.', 'created_at': datetime.datetime(2024, 10, 25, 15, 21, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2441869212, 'issue_id': 2614253478, 'author': 'vatsrahul1001', 'body': '@bbovenzi I think we can have some unique attributes for common elements like @pierrejeambrun mentioned below. Yes adding `data-testid` attribute will help\r\n\r\nNavigation links\r\nToobar (Filters + Searchbar)\r\nMain content\r\nTables\r\nDAGs Run button\r\nTask view buttons\r\n\r\n> @vatsrahul1001 Can you tell me more about where and how we should tag elements for your testing? We should probably put in more `data-testid`s too that we can easily use for unit tests as well.', 'created_at': datetime.datetime(2024, 10, 28, 15, 12, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2459826730, 'issue_id': 2614253478, 'author': 'vatsrahul1001', 'body': 'Re-opening this as we might need to open a few more PRs to address this. This will be parallel to 3.0 UI dev work.', 'created_at': datetime.datetime(2024, 11, 6, 13, 58, 7, tzinfo=datetime.timezone.utc)}]","pierrejeambrun (Issue Creator) on (2024-10-25 14:26:01 UTC): cc: @vatsrahul1001

bbovenzi on (2024-10-25 15:21:55 UTC): @vatsrahul1001 Can you tell me more about where and how we should tag elements for your testing? We should probably put in more `data-testid`s too that we can easily use for unit tests as well.

vatsrahul1001 (Assginee) on (2024-10-28 15:12:23 UTC): @bbovenzi I think we can have some unique attributes for common elements like @pierrejeambrun mentioned below. Yes adding `data-testid` attribute will help

Navigation links
Toobar (Filters + Searchbar)
Main content
Tables
DAGs Run button
Task view buttons

vatsrahul1001 (Assginee) on (2024-11-06 13:58:07 UTC): Re-opening this as we might need to open a few more PRs to address this. This will be parallel to 3.0 UI dev work.

"
2613996818,issue,closed,completed,HttpToS3Operator throws exception if s3_bucket parameter is not passed,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.1

### What happened?

When using the **HttpToS3Operator** operator without **s3_bucket** parameter, I get this error:
```text
[2024-10-25, 15:05:43 EEST] {local_task_job_runner.py:123} ▶ Pre task execution logs
[2024-10-25, 15:05:43 EEST] {http_to_s3.py:165} INFO - Calling HTTP method
[2024-10-25, 15:05:43 EEST] {base.py:84} INFO - Retrieving connection 'http_conn'
[2024-10-25, 15:05:44 EEST] {base.py:84} INFO - Retrieving connection 'aws_conn'
[2024-10-25, 15:05:44 EEST] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/utils/operator_helpers.py"", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py"", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/amazon/aws/transfers/http_to_s3.py"", line 168, in execute
    self.s3_hook.load_bytes(
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/amazon/aws/hooks/s3.py"", line 158, in wrapper
    return func(*bound_args.args, **bound_args.kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/amazon/aws/hooks/s3.py"", line 132, in wrapper
    return func(*bound_args.args, **bound_args.kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/amazon/aws/hooks/s3.py"", line 1205, in load_bytes
    self._upload_file_obj(f, key, bucket_name, replace, encrypt, acl_policy)
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/amazon/aws/hooks/s3.py"", line 1255, in _upload_file_obj
    client.upload_fileobj(
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/boto3/s3/inject.py"", line 635, in upload_fileobj
    future = manager.upload(
             ^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/s3transfer/manager.py"", line 323, in upload
    self._validate_if_bucket_supported(bucket)
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/s3transfer/manager.py"", line 492, in _validate_if_bucket_supported
    match = pattern.match(bucket)
            ^^^^^^^^^^^^^^^^^^^^^
TypeError: expected string or bytes-like object, got 'NoneType'
[2024-10-25, 15:05:44 EEST] {taskinstance.py:1225} INFO - Marking task as UP_FOR_RETRY. dag_id=test, task_id=download, run_id=manual__2024-10-25T12:05:38.785000+00:00, execution_date=20241025T120538, start_date=20241025T120543, end_date=20241025T120544
[2024-10-25, 15:05:44 EEST] {taskinstance.py:340} ▶ Post task execution logs
```

### What you think should happen instead?

the operator worked without errors, since S3Hook gets the S3 bucket name from the **service_config** in the extra connection information

### How to reproduce

Create and run this simple DAG
```python
from datetime import datetime, timedelta

from airflow import DAG
from airflow.providers.amazon.aws.transfers.http_to_s3 import HttpToS3Operator

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(year=2019, month=1, day=1),
    'email': ['noreply@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}


with DAG(
        dag_id='http-to-s3-test',
        default_args=default_args,
        description='http-to-s3-test',
        catchup=False,
        schedule_interval=None) as dag:

    download = HttpToS3Operator(
        task_id='download',
        aws_conn_id='aws_conn',
        http_conn_id='http_conn',
        method='GET',
        extra_options={'check_response': True},
        endpoint='/test.txt',
        s3_key='test.txt',
        replace=True,
    )
```

### Operating System

Amazon Linux 2023.5.20240916

### Versions of Apache Airflow Providers

```
apache-airflow-providers-amazon==8.28.0
apache-airflow-providers-apache-spark==4.10.0
apache-airflow-providers-atlassian-jira==2.7.0
apache-airflow-providers-celery==3.8.1
apache-airflow-providers-common-compat==1.2.0
apache-airflow-providers-common-io==1.4.0
apache-airflow-providers-common-sql==1.16.0
apache-airflow-providers-fab==1.3.0
apache-airflow-providers-ftp==3.11.0
apache-airflow-providers-http==4.13.0
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-microsoft-mssql==3.9.0
apache-airflow-providers-postgres==5.12.0
apache-airflow-providers-sftp==4.11.0
apache-airflow-providers-slack==8.9.0
apache-airflow-providers-smtp==1.8.0
apache-airflow-providers-sqlite==3.9.0
apache-airflow-providers-ssh==3.13.1
```

### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",kostiantyn-lab,2024-10-25 12:38:46+00:00,['kunaljubce'],2024-11-08 16:18:49+00:00,2024-11-08 16:18:48+00:00,https://github.com/apache/airflow/issues/43379,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('good first issue', ''), ('area:core', '')]","[{'comment_id': 2438647766, 'issue_id': 2613996818, 'author': 'potiuk', 'body': 'Maybe you would like to fix it and contribute PR @kostiantyn-lab ? If not then I marked it as a good first issue and it will wait for someon who would like to fix it.', 'created_at': datetime.datetime(2024, 10, 25, 19, 30, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2438653778, 'issue_id': 2613996818, 'author': 'Rustix69', 'body': ""@potiuk  I would like work on this, if that's okay."", 'created_at': datetime.datetime(2024, 10, 25, 19, 34, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2438744137, 'issue_id': 2613996818, 'author': 'potiuk', 'body': ""> @potiuk I would like work on this, if that's okay.\r\n\r\nFeel free."", 'created_at': datetime.datetime(2024, 10, 25, 20, 32, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439547992, 'issue_id': 2613996818, 'author': 'kostiantyn-lab', 'body': '> @potiuk I would like work on this, if that\'s okay.\r\n\r\nit looks like we need to change this [condition](https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/amazon/aws/hooks/s3.py#L119) to something like this\r\n\r\nbefore\r\n```python\r\nif ""bucket_name"" not in bound_args.arguments:\r\n```\r\n\r\nafter\r\n```python\r\n# check that bucket_name is passed to parameters and does not have NULL or empty value\r\nif not bound_args.arguments.get(""bucket_name""):\r\n```', 'created_at': datetime.datetime(2024, 10, 26, 11, 46, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439551288, 'issue_id': 2613996818, 'author': 'potiuk', 'body': 'just make a PR - discussing it on the changed code is so much easier.', 'created_at': datetime.datetime(2024, 10, 26, 11, 59, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2440035901, 'issue_id': 2613996818, 'author': 'Rustix69', 'body': '> > @potiuk I would like work on this, if that\'s okay.\r\n> \r\n> it looks like we need to change this [condition](https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/amazon/aws/hooks/s3.py#L119) to something like this\r\n> \r\n> before\r\n> \r\n> ```python\r\n> if ""bucket_name"" not in bound_args.arguments:\r\n> ```\r\n> \r\n> after\r\n> \r\n> ```python\r\n> # check that bucket_name is passed to parameters and does not have NULL or empty value\r\n> if not bound_args.arguments.get(""bucket_name""):\r\n> ```\r\n\r\nCan you help me in Creating the Unit Test Case, I had already Modified the Code ...', 'created_at': datetime.datetime(2024, 10, 27, 14, 1, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2440188684, 'issue_id': 2613996818, 'author': 'potiuk', 'body': '> Can you help me in Creating the Unit Test Case, I had already Modified the Code ...\r\n\r\nGenerally - look at the other, realated unit test cases. Use them as learning examples. Adding unit tests as part of the fix is ""integral"" part of code change.', 'created_at': datetime.datetime(2024, 10, 27, 21, 56, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2441651951, 'issue_id': 2613996818, 'author': 'kostiantyn-lab', 'body': '> Can you help me in Creating the Unit Test Case, I had already Modified the Code ...\r\n\r\nYou can use this test as an example. https://github.com/apache/airflow/blob/main/providers/tests/amazon/aws/hooks/test_s3.py#L1143', 'created_at': datetime.datetime(2024, 10, 28, 13, 51, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443143081, 'issue_id': 2613996818, 'author': 'Rustix69', 'body': '> > Can you help me in Creating the Unit Test Case, I had already Modified the Code ...\r\n> \r\n> Generally - look at the other, realated unit test cases. Use them as learning examples. Adding unit tests as part of the fix is ""integral"" part of code change.\r\n\r\nPlz review and merge my code I had also added unit test cases. @potiuk', 'created_at': datetime.datetime(2024, 10, 29, 3, 49, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443840739, 'issue_id': 2613996818, 'author': 'potiuk', 'body': '> > > Can you help me in Creating the Unit Test Case, I had already Modified the Code ...\r\n> > \r\n> > \r\n> > Generally - look at the other, realated unit test cases. Use them as learning examples. Adding unit tests as part of the fix is ""integral"" part of code change.\r\n> \r\n> Plz review and merge my code I had also added unit test cases. @potiuk\r\n\r\nOnce you fix failing test cases, sure. Also if you want to get review, ping in general ""can you please review""  in the PR or ask for review in #new-contributors slack channel. There are many reviewers here who can review and approve your code. It does not have to be me.', 'created_at': datetime.datetime(2024, 10, 29, 10, 33, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443969126, 'issue_id': 2613996818, 'author': 'Rustix69', 'body': '> > > > Can you help me in Creating the Unit Test Case, I had already Modified the Code ...\r\n> > > \r\n> > > \r\n> > > Generally - look at the other, realated unit test cases. Use them as learning examples. Adding unit tests as part of the fix is ""integral"" part of code change.\r\n> > \r\n> > \r\n> > Plz review and merge my code I had also added unit test cases. @potiuk\r\n> \r\n> Once you fix failing test cases, sure. Also if you want to get review, ping in general ""can you please review"" in the PR or ask for review in #new-contributors slack channel. There are many reviewers here who can review and approve your code. It does not have to be me.\r\n\r\nYou have already Reviewed and approved my code when will the PR be merged actually its my first PR....', 'created_at': datetime.datetime(2024, 10, 29, 11, 36, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2444993422, 'issue_id': 2613996818, 'author': 'potiuk', 'body': '> You have already Reviewed and approved my code when will the PR be merged actually its my first PR....\r\n\r\nThe tests need to pass. As a first time contributor - someone needs to approve your ""CI"" workflows - that\'s why pinging it in your PR is the best way to drag attention to it. Simply ""please approve and run my workflow here"" in the PR will do. And again - any one of maintainers can do it - it does not have to be me. I might be on holidays, have vacations or simply be busy with 100 other prs :)\r\n\r\nAnd it will be merged when it gets green - then you can also write ""Hey my PR is green, can it be merged"" - either in PR or in slack - with link to your PR. Doing it in PR makes it ... easier than an issue - because in here you might not easily see which PR you are asking for - you need to scroll the conversation all the way up and see ""may be fixed by ...."" so pinging in the PR is just ...  more efficient.', 'created_at': datetime.datetime(2024, 10, 29, 18, 4, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453486792, 'issue_id': 2613996818, 'author': 'kunaljubce', 'body': '@potiuk I can take this up. Please assign this to me!', 'created_at': datetime.datetime(2024, 11, 3, 16, 29, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453491770, 'issue_id': 2613996818, 'author': 'potiuk', 'body': 'Assigned.', 'created_at': datetime.datetime(2024, 11, 3, 16, 44, 9, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-10-25 19:30:45 UTC): Maybe you would like to fix it and contribute PR @kostiantyn-lab ? If not then I marked it as a good first issue and it will wait for someon who would like to fix it.

Rustix69 on (2024-10-25 19:34:59 UTC): @potiuk  I would like work on this, if that's okay.

potiuk on (2024-10-25 20:32:37 UTC): Feel free.

kostiantyn-lab (Issue Creator) on (2024-10-26 11:46:03 UTC): it looks like we need to change this [condition](https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/amazon/aws/hooks/s3.py#L119) to something like this

before
```python
if ""bucket_name"" not in bound_args.arguments:
```

after
```python
# check that bucket_name is passed to parameters and does not have NULL or empty value
if not bound_args.arguments.get(""bucket_name""):
```

potiuk on (2024-10-26 11:59:57 UTC): just make a PR - discussing it on the changed code is so much easier.

Rustix69 on (2024-10-27 14:01:36 UTC): Can you help me in Creating the Unit Test Case, I had already Modified the Code ...

potiuk on (2024-10-27 21:56:35 UTC): Generally - look at the other, realated unit test cases. Use them as learning examples. Adding unit tests as part of the fix is ""integral"" part of code change.

kostiantyn-lab (Issue Creator) on (2024-10-28 13:51:58 UTC): You can use this test as an example. https://github.com/apache/airflow/blob/main/providers/tests/amazon/aws/hooks/test_s3.py#L1143

Rustix69 on (2024-10-29 03:49:24 UTC): Plz review and merge my code I had also added unit test cases. @potiuk

potiuk on (2024-10-29 10:33:31 UTC): Once you fix failing test cases, sure. Also if you want to get review, ping in general ""can you please review""  in the PR or ask for review in #new-contributors slack channel. There are many reviewers here who can review and approve your code. It does not have to be me.

Rustix69 on (2024-10-29 11:36:27 UTC): You have already Reviewed and approved my code when will the PR be merged actually its my first PR....

potiuk on (2024-10-29 18:04:39 UTC): The tests need to pass. As a first time contributor - someone needs to approve your ""CI"" workflows - that's why pinging it in your PR is the best way to drag attention to it. Simply ""please approve and run my workflow here"" in the PR will do. And again - any one of maintainers can do it - it does not have to be me. I might be on holidays, have vacations or simply be busy with 100 other prs :)

And it will be merged when it gets green - then you can also write ""Hey my PR is green, can it be merged"" - either in PR or in slack - with link to your PR. Doing it in PR makes it ... easier than an issue - because in here you might not easily see which PR you are asking for - you need to scroll the conversation all the way up and see ""may be fixed by ...."" so pinging in the PR is just ...  more efficient.

kunaljubce (Assginee) on (2024-11-03 16:29:17 UTC): @potiuk I can take this up. Please assign this to me!

potiuk on (2024-11-03 16:44:09 UTC): Assigned.

"
2613951437,issue,open,,Airflow 2 to 3 Rest API Changes,"### Rest API Breaking Change

The goal is to make an exhaustive list of of the breaking / noticeable changes for the Rest API moving from airflow 2 to airflow 3.

Some of those changes are inherent to FastAPI and hard to circumvent, some others are here just for code clarity or simplicity and can be 'fixed'. That being said I think that we should take the opportunity of Airflow 3 to clean/adjust the code allowing ourselves a few breaking changes when necessary.

- Th API now returns 422 instead of 400 in cases for validation/deserialization errors (body, query/path parameters).

- When listing a resource for instance on GET `/dags`, `fields` parameter is not supported anymore to obtain a partial response. Full objects will be returned by the endpoint.

- Passing `list` query parameter switched from `form, non exploded`  to `form, exploded`, i.e before `?my_list=item1,item2` now `?my_list=item1&my_list=item2`

- `execution_date` deprecated and is now removed. Any payload or query parameters mentioning this field has been removed.

- datetime format are `RFC3339-compliant` in FastAPI, more permissive than `ISO8601`. meaning that the API returns `zulu` datetime for responses, more info [here](https://github.com/fastapi/fastapi/discussions/7693#discussioncomment-5143311). Both `Z` and `00+xx` are supported for payload and params. Workaround here https://github.com/pydantic/pydantic/issues/6028#issuecomment-1597526916. This is due to pydantic v2 default behavior

- Patch on DagRun and TaskInstance are more generic and allow in addition to update the ressource state to update the note content. Therefore the two legacy dedicated endpoints to update DagRun note and TaskInstance note have been removed. Same for the set task instance state, it is now handled by the broader PATCH on task intances.


- `assets/queuedEvent` endpoints have moved to `assets/queuedEvents` for consistency.

- dag_parsing endpoint now returns a 409 when the `DagPriorityParsingRequest` already exists. (It was returning 201 before).

- `clearTaskInstances` endpoint, default value for `reset_dag_runs` has been updated from `False` to `True`.


### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-10-25 12:19:13+00:00,"['bbovenzi', 'pierrejeambrun']",2025-01-08 15:37:52+00:00,,https://github.com/apache/airflow/issues/43378,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('kind:documentation', ''), ('kind:meta', 'High-level information important to the community'), ('airflow3.0:candidate', 'Potential candidates for Airflow 3.0')]","[{'comment_id': 2498330156, 'issue_id': 2613951437, 'author': 'pierrejeambrun', 'body': 'Waiting for the newsfragment format to be settle to create associated significant newsfragments.', 'created_at': datetime.datetime(2024, 11, 25, 15, 27, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507815962, 'issue_id': 2613951437, 'author': 'rawwar', 'body': '@pierrejeambrun , regarding\r\n> When listing a resource for instance on GET /dags, fields parameter is not supported anymore to obtain a partial response. Full objects will be returned by the endpoint.\r\n\r\nWe can use [model_serializer](https://github.com/apache/airflow/pull/44383#discussion_r1860083457) to decide what fields to return. Would this be ok?', 'created_at': datetime.datetime(2024, 11, 29, 13, 24, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508123639, 'issue_id': 2613951437, 'author': 'omkar-foss', 'body': 'I suppose FastAPI throws 422 instead of 400 in cases where there are missing `required` parameters (or validation errors) in the request header or body.\r\n\r\n@pierrejeambrun please see if this needs to be listed as a breaking change, thought to highlight it, just in case! :)', 'created_at': datetime.datetime(2024, 11, 29, 16, 34, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508687024, 'issue_id': 2613951437, 'author': 'pierrejeambrun', 'body': '> I suppose FastAPI throws 422 instead of 400 in cases where there are missing required parameters (or validation errors) in the request header or body.\r\n@pierrejeambrun please see if this needs to be listed as a breaking change, thought to highlight it, just in case! :)\r\n\r\nGood idea, done', 'created_at': datetime.datetime(2024, 11, 29, 21, 45, 57, tzinfo=datetime.timezone.utc)}]","pierrejeambrun (Issue Creator) on (2024-11-25 15:27:18 UTC): Waiting for the newsfragment format to be settle to create associated significant newsfragments.

rawwar on (2024-11-29 13:24:44 UTC): @pierrejeambrun , regarding

We can use [model_serializer](https://github.com/apache/airflow/pull/44383#discussion_r1860083457) to decide what fields to return. Would this be ok?

omkar-foss on (2024-11-29 16:34:02 UTC): I suppose FastAPI throws 422 instead of 400 in cases where there are missing `required` parameters (or validation errors) in the request header or body.

@pierrejeambrun please see if this needs to be listed as a breaking change, thought to highlight it, just in case! :)

pierrejeambrun (Issue Creator) on (2024-11-29 21:45:57 UTC): @pierrejeambrun please see if this needs to be listed as a breaking change, thought to highlight it, just in case! :)

Good idea, done

"
2613322997,issue,closed,completed,AttributeError: 'tuple' object has no attribute 'items' when entering UI Admin Variables Section,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

After re-creating the containers of both Airflow 2.10.2, Postgres 15.4, and Python 3.12, without any changes from previous configurations, I encountered an error when trying to access the Variables UI Panel.

```python
airflow-1  | [2024-10-25T07:07:01.991+0000] {app.py:1744} ERROR - Exception on /variable/list/ [GET]
airflow-1  | Traceback (most recent call last):
airflow-1  |   File ""/usr/local/lib/python3.12/site-packages/flask/app.py"", line 2529, in wsgi_app
airflow-1  |     response = self.full_dispatch_request()
airflow-1  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
airflow-1  |   File ""/usr/local/lib/python3.12/site-packages/flask/app.py"", line 1825, in full_dispatch_request
airflow-1  |     rv = self.handle_user_exception(e)
airflow-1  |          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
airflow-1  |   File ""/usr/local/lib/python3.12/site-packages/flask/app.py"", line 1823, in full_dispatch_request
airflow-1  |     rv = self.dispatch_request()
airflow-1  |          ^^^^^^^^^^^^^^^^^^^^^^^
airflow-1  |   File ""/usr/local/lib/python3.12/site-packages/flask/app.py"", line 1799, in dispatch_request
airflow-1  |     return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
airflow-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
airflow-1  |   File ""/usr/local/lib/python3.12/site-packages/flask_appbuilder/security/decorators.py"", line 137, in wraps
airflow-1  |     return f(self, *args, **kwargs)
airflow-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^
airflow-1  |   File ""/usr/local/lib/python3.12/site-packages/flask_appbuilder/views.py"", line 550, in list
airflow-1  |     widgets = self._list()
airflow-1  |               ^^^^^^^^^^^^
airflow-1  |   File ""/usr/local/lib/python3.12/site-packages/flask_appbuilder/baseviews.py"", line 1186, in _list
airflow-1  |     form = self.search_form.refresh()
airflow-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^
airflow-1  |   File ""/usr/local/lib/python3.12/site-packages/flask_appbuilder/forms.py"", line 327, in refresh
airflow-1  |     form = self(obj=obj)
airflow-1  |            ^^^^^^^^^^^^^
airflow-1  |   File ""/usr/local/lib/python3.12/site-packages/wtforms/form.py"", line 209, in __call__
airflow-1  |     return type.__call__(cls, *args, **kwargs)
airflow-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
airflow-1  |   File ""/usr/local/lib/python3.12/site-packages/flask_wtf/form.py"", line 73, in __init__
airflow-1  |     super().__init__(formdata=formdata, **kwargs)
airflow-1  |   File ""/usr/local/lib/python3.12/site-packages/wtforms/form.py"", line 281, in __init__
airflow-1  |     super().__init__(self._unbound_fields, meta=meta_obj, prefix=prefix)
airflow-1  |   File ""/usr/local/lib/python3.12/site-packages/wtforms/form.py"", line 49, in __init__
airflow-1  |     field = meta.bind_field(self, unbound_field, options)
airflow-1  |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
airflow-1  |   File ""/usr/local/lib/python3.12/site-packages/wtforms/meta.py"", line 28, in bind_field
airflow-1  |     return unbound_field.bind(form=form, **options)
airflow-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
airflow-1  |   File ""/usr/local/lib/python3.12/site-packages/wtforms/fields/core.py"", line 387, in bind
airflow-1  |     return self.field_class(*self.args, **kw)
airflow-1  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
airflow-1  |   File ""/usr/local/lib/python3.12/site-packages/wtforms/fields/core.py"", line 133, in __init__
airflow-1  |     for k, v in flags.items():
airflow-1  |                 ^^^^^^^^^^^
airflow-1  | AttributeError: 'tuple' object has no attribute 'items'

```

### What you think should happen instead?

Accessing correctly to any airflow panel

### How to reproduce

Re-create containers for Airflow 2.10.2, Postgres 15.4, and Python 3.12.
Attempt to access the Variables UI Panel.
Observe the error.


### Operating System

PRETTY_NAME=""Debian GNU/Linux 11 (bullseye)"" NAME=""Debian GNU/Linux"" VERSION_ID=""11"" VERSION=""11 (bullseye)"" VERSION_CODENAME=bullseye ID=debian HOME_URL=""https://www.debian.org/"" SUPPORT_URL=""https://www.debian.org/support"" BUG_REPORT_URL=""https://bugs.debian.org/""

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==9.0.0
apache-airflow-providers-common-compat==1.2.1
apache-airflow-providers-common-io==1.4.2
apache-airflow-providers-common-sql==1.18.0
apache-airflow-providers-fab==1.4.1
apache-airflow-providers-facebook==3.6.0
apache-airflow-providers-ftp==3.11.1
apache-airflow-providers-google==10.23.0
apache-airflow-providers-http==4.13.1
apache-airflow-providers-imap==3.7.0

### Deployment

Docker-Compose

### Deployment details

Here is the docker-compose.yml
```docker-compose
services:
  postgres:
    platform: linux/arm64
    image: postgres:15.4
    restart: unless-stopped
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - ""pgsql-data:/var/lib/postgresql/data""
    ports:
      - ""5432:5432""
  airflow:
    platform: linux/arm64
    build:
      context: .
      dockerfile: ./src/airflow/Dockerfile
    restart: unless-stopped
    depends_on:
      - postgres
    environment:
      - EXECUTOR=Local
      - AIRFLOW__CORE__FERNET_KEY=***
      - AIRFLOW__WEBSERVER__WORKERS=4
      - AIRFLOW__WEBSERVER__BASE_URL=http://localhost:8080
      - GOOGLE_APPLICATION_CREDENTIALS=***
    volumes:
      - ./src/dags:/usr/local/airflow/dags
      - ./src/utils:/usr/local/airflow/utils
      - ./confs:/usr/local/airflow/confs
      - ./src/airflow:/usr/local/airflow
    env_file:
      - ./.env    
    ports:
      - ""8080:8080""
    command: webserver
    healthcheck:
      test: [""CMD-SHELL"", ""[ -f /usr/local/airflow/airflow-webserver.pid ]""]
      interval: 30s
      timeout: 30s
      retries: 3

volumes:
  pgsql-data:
    driver: local


```

### Anything else?

Here the entire pip freeze
```bash
apache-airflow-providers-amazon==9.0.0
apache-airflow-providers-common-compat==1.2.1
apache-airflow-providers-common-io==1.4.2
apache-airflow-providers-common-sql==1.18.0
apache-airflow-providers-fab==1.4.1
apache-airflow-providers-facebook==3.6.0
apache-airflow-providers-ftp==3.11.1
apache-airflow-providers-google==10.23.0
apache-airflow-providers-http==4.13.1
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-mysql==5.7.1
apache-airflow-providers-postgres==5.13.0
apache-airflow-providers-slack==8.9.0
apache-airflow-providers-smtp==1.8.0
apache-airflow-providers-sqlite==3.9.0
apache-airflow-providers-ssh==3.13.1
root@c9f1c90f96d2:/usr/local/airflow# pip freeze
aiofiles==23.2.1
aiohappyeyeballs==2.4.3
aiohttp==3.10.10
aiosignal==1.3.1
airflow-add-ons==0.2.14
alembic==1.13.3
annotated-types==0.7.0
anyio==4.6.2.post1
apache-airflow==2.10.2
apache-airflow-providers-amazon==9.0.0
apache-airflow-providers-common-compat==1.2.1
apache-airflow-providers-common-io==1.4.2
apache-airflow-providers-common-sql==1.18.0
apache-airflow-providers-fab==1.4.1
apache-airflow-providers-facebook==3.6.0
apache-airflow-providers-ftp==3.11.1
apache-airflow-providers-google==10.23.0
apache-airflow-providers-http==4.13.1
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-mysql==5.7.1
apache-airflow-providers-postgres==5.13.0
apache-airflow-providers-slack==8.9.0
apache-airflow-providers-smtp==1.8.0
apache-airflow-providers-sqlite==3.9.0
apache-airflow-providers-ssh==3.13.1
apispec==6.7.0
argcomplete==3.5.1
asana==5.0.10
asgiref==3.8.1
asn1crypto==1.5.1
attrs==24.2.0
Authlib==1.3.2
babel==2.16.0
backoff==2.2.1
bcrypt==4.2.0
beautifulsoup4==4.12.3
blinker==1.8.2
boto3==1.35.48
botocore==1.35.48
Brotli==1.1.0
cachelib==0.9.0
cachetools==5.3.3
cattrs==24.1.2
certifi==2024.8.30
cffi==1.17.1
chardet==5.2.0
charset-normalizer==3.4.0
cli_exit_tools==1.2.7
click==8.1.7
clickclick==20.10.2
colorama==0.4.6
colorlog==6.8.2
ConfigUpdater==3.2
connexion==2.14.2
cramjam==2.9.0
cron-descriptor==1.4.5
croniter==3.0.3
cryptography==43.0.3
curlify==2.2.1
customerio==2.1
Cython==3.0.11
db-dtypes==1.3.0
decorator==5.1.1
Deprecated==1.2.14
dill==0.3.9
dnspython==2.7.0
docstring_parser==0.16
docutils==0.21.2
email_validator==2.2.0
emoji==2.13.2
et-xmlfile==1.1.0
facebook-sdk==3.1.0
facebook_business==20.0.3
facebookads==2.11.4
fastparquet==2024.5.0
fasttext @ git+https://github.com/pualien/fastText@4a3165b25e8000a880818b44c54bd1b2b5504fd2
fasttext-langdetect==1.0.5
filelock==3.16.1
Flask==2.2.5
Flask-AppBuilder==4.5.0
Flask-Babel==2.0.0
Flask-Caching==2.3.0
Flask-JWT-Extended==4.6.0
Flask-Limiter==3.8.0
Flask-Login==0.6.3
Flask-Session==0.5.0
Flask-SQLAlchemy==2.5.1
Flask-WTF==1.2.2
frozenlist==1.5.0
fsspec==2024.10.0
gcloud-aio-auth==5.3.2
gcloud-aio-bigquery==7.1.0
gcloud-aio-storage==9.3.0
gcloud-connectors==0.2.2
gcsfs==2024.10.0
google-ads==25.1.0
google-analytics-admin==0.23.1
google-analytics-data==0.18.12
google-api-core==2.21.0
google-api-python-client==2.90.0
google-auth==2.35.0
google-auth-httplib2==0.2.0
google-auth-oauthlib==1.2.1
google-cloud-aiplatform==1.70.0
google-cloud-appengine-logging==1.5.0
google-cloud-audit-log==0.3.0
google-cloud-automl==2.14.0
google-cloud-batch==0.17.30
google-cloud-bigquery==3.26.0
google-cloud-bigquery-datatransfer==3.17.0
google-cloud-bigquery-storage==2.27.0
google-cloud-bigtable==2.26.0
google-cloud-build==3.26.0
google-cloud-compute==1.20.0
google-cloud-container==2.53.0
google-cloud-core==2.4.1
google-cloud-datacatalog==3.21.0
google-cloud-dataflow-client==0.8.13
google-cloud-dataform==0.5.12
google-cloud-dataplex==2.3.0
google-cloud-dataproc==5.15.0
google-cloud-dataproc-metastore==1.16.0
google-cloud-dlp==3.25.0
google-cloud-kms==3.1.0
google-cloud-language==2.15.0
google-cloud-logging==3.11.3
google-cloud-memcache==1.10.0
google-cloud-monitoring==2.23.0
google-cloud-orchestration-airflow==1.15.0
google-cloud-os-login==2.15.0
google-cloud-pubsub==2.25.2
google-cloud-redis==2.16.0
google-cloud-resource-manager==1.13.0
google-cloud-run==0.10.10
google-cloud-secret-manager==2.21.0
google-cloud-spanner==3.49.1
google-cloud-speech==2.28.0
google-cloud-storage==2.18.2
google-cloud-storage-transfer==1.13.0
google-cloud-tasks==2.17.0
google-cloud-texttospeech==2.20.0
google-cloud-translate==3.17.0
google-cloud-videointelligence==2.14.0
google-cloud-vision==3.8.0
google-cloud-workflows==1.15.0
google-crc32c==1.6.0
google-re2==1.1.20240702
google-resumable-media==2.7.2
googleads==43.0.0
googleapis-common-protos==1.65.0
greenlet==3.1.1
grpc-google-iam-v1==0.13.1
grpc-interceptor==0.15.4
grpcio==1.67.0
grpcio-gcp==0.2.2
grpcio-status==1.62.3
grpcio-tools==1.62.3
gspread==5.4.0
gspread-pandas==3.2.2
gunicorn==23.0.0
h11==0.14.0
h2==4.1.0
hpack==4.0.0
httpcore==1.0.6
httplib2==0.22.0
httpx==0.27.2
hyperframe==6.0.1
idna==3.10
IMAPClient==2.1.0
immutabledict==4.2.0
importlib_metadata==8.4.0
importlib_resources==6.4.5
inflection==0.5.1
iniconfig==2.0.0
isodate==0.7.2
itsdangerous==2.2.0
jaraco.classes==3.4.0
jaraco.context==6.0.1
jaraco.functools==4.1.0
jeepney==0.8.0
Jinja2==3.1.4
jmespath==1.0.1
json-merge-patch==0.2
jsonpath-ng==1.7.0
jsonschema==4.23.0
jsonschema-specifications==2024.10.1
jwplatform==2.2.2
keyring==25.3.0
lazy-object-proxy==1.10.0
lib-detect-testenv==2.0.8
limits==3.13.0
linkify-it-py==2.0.3
lockfile==0.12.2
looker-sdk==24.16.2
lxml==5.3.0
Mako==1.3.6
markdown-it-py==3.0.0
MarkupSafe==3.0.2
marshmallow==3.23.0
marshmallow-oneofschema==3.1.1
marshmallow-sqlalchemy==0.28.2
mdit-py-plugins==0.4.2
mdurl==0.1.2
methodtools==0.4.7
mock==5.1.0
more-itertools==10.5.0
multidict==6.1.0
multiprocess==0.70.17
mysql-connector-python==9.1.0
mysqlclient==2.2.5
neterr==1.1.1
numpy==1.26.4
oauth2client==4.1.3
oauthlib==3.2.2
openpyxl==3.1.5
opentelemetry-api==1.27.0
opentelemetry-exporter-otlp==1.27.0
opentelemetry-exporter-otlp-proto-common==1.27.0
opentelemetry-exporter-otlp-proto-grpc==1.27.0
opentelemetry-exporter-otlp-proto-http==1.27.0
opentelemetry-proto==1.27.0
opentelemetry-sdk==1.27.0
opentelemetry-semantic-conventions==0.48b0
ordered-set==4.1.0
ordereddict==1.1
orjson==3.10.7
packaging==24.1
pandas==2.1.4
pandas-gbq==0.24.0
paramiko==3.5.0
pathspec==0.12.1
pendulum==3.0.0
platformdirs==4.3.6
pluggy==1.5.0
ply==3.11
portalocker==2.10.1
prison==0.2.1
propcache==0.2.0
proto-plus==1.25.0
protobuf==4.25.5
psutil==6.1.0
psycopg2-binary==2.9.10
py==1.11.0
pyarrow==17.0.0
pyasn1==0.6.1
pyasn1_modules==0.4.0
PyAthena==3.9.0
pybind11==2.13.6
pycountry==24.6.1
pycparser==2.22
pydantic==2.9.2
pydantic_core==2.23.4
pydata-google-auth==1.8.2
Pygments==2.18.0
PyJWT==2.9.0
pymongo==4.10.1
pymongo-ssh==0.0.15
PyMySQL==1.1.0
PyNaCl==1.5.0
pyOpenSSL==24.2.1
pyparsing==3.2.0
pytest==8.3.3
python-daemon==3.1.0
python-dateutil==2.9.0.post0
python-nvd3==0.16.0
python-slugify==8.0.4
python3-saml==1.16.0
pytz==2024.2
PyYAML==6.0.1
qdrant-client==1.12.0
redshift-connector==2.1.3
referencing==0.35.1
requests==2.32.3
requests-file==2.1.0
requests-oauthlib==2.0.0
requests-toolbelt==1.0.0
retry==0.9.2
rfc3339-validator==0.1.4
rich==13.9.3
rich-argparse==1.5.2
rpds-py==0.20.0
rsa==4.9
s3transfer==0.10.3
scramp==1.4.5
SecretStorage==3.3.3
sentry-sdk==2.17.0
setproctitle==1.3.3
setuptools==75.2.0
shapely==2.0.6
simplejson==3.19.3
six==1.16.0
slack_sdk==3.33.2
sniffio==1.3.1
soupsieve==2.6
SQLAlchemy==1.4.54
sqlalchemy-bigquery==1.12.0
sqlalchemy-connector==0.1.43
SQLAlchemy-JSONField==1.0.2
sqlalchemy-redshift==0.8.14
sqlalchemy-spanner==1.7.0
SQLAlchemy-Utils==0.41.2
sqlparse==0.5.1
sshtunnel==0.4.0
swagger-ui-bundle==0.0.9
tabulate==0.9.0
tenacity==9.0.0
termcolor==2.5.0
text-unidecode==1.3
time-machine==2.16.0
timeout-decorator==0.5.0
tldextract==5.1.2
toml==0.10.2
tornado==6.4.1
typing_extensions==4.12.2
tzdata==2024.2
uc-micro-py==1.0.3
unbounce-python-api==1.3.5
unicodecsv==0.14.1
universal_pathlib==0.2.5
uritemplate==4.1.1
urllib3==2.2.3
usercustomize==1.0.0
watchtower==3.3.1
Werkzeug==2.2.3
wheel==0.44.0
wirerope==0.4.7
wrapt==1.16.0
wrapt-timeout-decorator==1.3.12.2
WTForms==3.2.1
xlrd==1.2.0
xmlsec==1.3.14
xmltodict==0.14.2
yarl==1.16.0
zeep==4.3.1
zipp==3.20.2
```

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",matteosdocsity,2024-10-25 07:31:05+00:00,[],2024-10-28 10:11:38+00:00,2024-10-25 18:07:02+00:00,https://github.com/apache/airflow/issues/43375,"[('kind:bug', 'This is a clearly a bug'), ('duplicate', 'Issue that is duplicated'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2438501136, 'issue_id': 2613322997, 'author': 'potiuk', 'body': 'Duplicate of #43356 . You have not used constraints https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html when installing Airflow (please do as we recommend), this is not an airflow issue - this is FAB issue that is already fixed and 2.10.3 will be using the fixed version.', 'created_at': datetime.datetime(2024, 10, 25, 18, 6, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439746246, 'issue_id': 2613322997, 'author': 'matteosdocsity', 'body': ""I understand this is a consequence of the version of apache-airflow-providers-fab , but indeed it's related on wrong and NOT tested [constraint](https://github.com/apache/airflow/blob/constraints-2.10.2/constraints-3.12.txt) for WTForms==3.2.0|3.2.1..btw taking attention on logs it was quite easy to fix, but maybe something in the process of releases and testing was not a complete E2E testing, I'm guessing. I don't want to be polemical, but to highlight the issue"", 'created_at': datetime.datetime(2024, 10, 26, 21, 47, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439748185, 'issue_id': 2613322997, 'author': 'potiuk', 'body': ""No. You are entirely wrong @matteosdocsity . the constraints you linked to show WTForms 3.1.2 and the broken ones are 3.2.0 and 3.2.1 . You mixed numbers.\r\n\r\nSo it's you who made a mistake by looking up the numbers wrong, not testing.\r\n\r\nNext time be a little more diligent please - being not polemical makes sense when you do double checking."", 'created_at': datetime.datetime(2024, 10, 26, 21, 57, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439749793, 'issue_id': 2613322997, 'author': 'matteosdocsity', 'body': ""Oh ok sorry @potiuk you're right I have not tested forcing the installation of WTForms 3.1.2...the mistake was the release not tested of the fab provider, got it!"", 'created_at': datetime.datetime(2024, 10, 26, 22, 4, 42, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-10-25 18:06:49 UTC): Duplicate of #43356 . You have not used constraints https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html when installing Airflow (please do as we recommend), this is not an airflow issue - this is FAB issue that is already fixed and 2.10.3 will be using the fixed version.

matteosdocsity (Issue Creator) on (2024-10-26 21:47:45 UTC): I understand this is a consequence of the version of apache-airflow-providers-fab , but indeed it's related on wrong and NOT tested [constraint](https://github.com/apache/airflow/blob/constraints-2.10.2/constraints-3.12.txt) for WTForms==3.2.0|3.2.1..btw taking attention on logs it was quite easy to fix, but maybe something in the process of releases and testing was not a complete E2E testing, I'm guessing. I don't want to be polemical, but to highlight the issue

potiuk on (2024-10-26 21:57:32 UTC): No. You are entirely wrong @matteosdocsity . the constraints you linked to show WTForms 3.1.2 and the broken ones are 3.2.0 and 3.2.1 . You mixed numbers.

So it's you who made a mistake by looking up the numbers wrong, not testing.

Next time be a little more diligent please - being not polemical makes sense when you do double checking.

matteosdocsity (Issue Creator) on (2024-10-26 22:04:42 UTC): Oh ok sorry @potiuk you're right I have not tested forcing the installation of WTForms 3.1.2...the mistake was the release not tested of the fab provider, got it!

"
2612493217,issue,open,,"Five ""otel_*"" options appear twice in the ""Configuration Reference""","### What do you see as an issue?

In https://github.com/apache/airflow/commit/ccf12026b44685f2d9917cd313676784e63c81af five configuration options have been moved from the ""metrics"" section to the ""traces"" section. Unfortunately, they have also been left in ""metrics"" untouched, and now they are effectively duplicated in the documentation. This is unfortunate, because users cannot easily tell what's the difference between them and which option should they prefer to use.

Here's a list of the duplicated options:

- otel_debugging_on
- otel_host
- otel_on
- otel_port
- otel_ssl_active

If you search for them on this page: https://airflow.apache.org/docs/apache-airflow/2.10.2/configurations-ref.html, you'll see that each of them occurs twice.

### Solving the problem

The problem can be solved by removing the `[metrics] otel_*` options from the `airflow/config_templates/config.yml` file (and keeping the `[traces] otel_*` options intact).

### Anything else

The newer options also have descriptions that do not render as nicely as the old ones in the documentation (the markdown highlights are gone). It may be a good moment to fix this.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bart113,2024-10-24 20:23:43+00:00,['InderParmar'],2025-02-06 22:09:12+00:00,,https://github.com/apache/airflow/issues/43366,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('kind:documentation', '')]","[{'comment_id': 2436451248, 'issue_id': 2612493217, 'author': 'potiuk', 'body': 'Can you please fix it maybe and make a PR for that ? Better fix should be to add them to deprecated options here: https://github.com/apache/airflow/blob/main/airflow/configuration.py#L347 - otherwise I marked it as ""good first issue"" and it will have to wait for someone to pick it up.', 'created_at': datetime.datetime(2024, 10, 24, 22, 29, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2436705209, 'issue_id': 2612493217, 'author': 'InderParmar', 'body': 'Hi, I am eager to work on this issue, can you please assign this issue to me, i will start working on it asap, and can you do me a favour by adding the ""hacktoberfest"" label to this issue.', 'created_at': datetime.datetime(2024, 10, 25, 2, 52, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2438445391, 'issue_id': 2612493217, 'author': 'potiuk', 'body': 'Absolutely - feel free !', 'created_at': datetime.datetime(2024, 10, 25, 17, 52, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439022016, 'issue_id': 2612493217, 'author': 'InderParmar', 'body': 'Hi, @bart113 , @potiuk , can you please review the PR, and if possible, can you please add the ""Hacktoberfest"" label on this issue, that will be great.\r\nAlso, please let me know if there are any concerns regarding the PR. \r\nHere is the summary of the changes done:\r\n- **Removed Duplicated Options**: The following otel_* options were removed from the [metrics] section in config.yml:\r\n  - otel_debugging_on\r\n  - otel_host\r\n  - otel_on\r\n  - otel_port\r\n  - otel_ssl_active\r\n  \r\n- **Formatting Update in Traces Section**: Improved markdown consistency in the [traces] section for better documentation clarity.\r\n\r\n- **Deprecated Options Added**: Added otel_* options to the deprecated list in configuration.py, directing users to the correct [traces] section options.', 'created_at': datetime.datetime(2024, 10, 25, 23, 5, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641169832, 'issue_id': 2612493217, 'author': 'ferruzzi', 'body': ""They weren't moved, there is a set of options for the metrics and a set for traces, they are two separate features which CAN both handled by otel but are not necessarily.  A user could use statsd for metrics and otel for traces, or use otel for metrics and not want to enable traces at all, or have metrics and traces sent to two different frontend clients, etc.    Some change may be suitable, but simply removing the metrics settings is not the right answer."", 'created_at': datetime.datetime(2025, 2, 6, 22, 9, 11, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-10-24 22:29:08 UTC): Can you please fix it maybe and make a PR for that ? Better fix should be to add them to deprecated options here: https://github.com/apache/airflow/blob/main/airflow/configuration.py#L347 - otherwise I marked it as ""good first issue"" and it will have to wait for someone to pick it up.

InderParmar (Assginee) on (2024-10-25 02:52:57 UTC): Hi, I am eager to work on this issue, can you please assign this issue to me, i will start working on it asap, and can you do me a favour by adding the ""hacktoberfest"" label to this issue.

potiuk on (2024-10-25 17:52:23 UTC): Absolutely - feel free !

InderParmar (Assginee) on (2024-10-25 23:05:09 UTC): Hi, @bart113 , @potiuk , can you please review the PR, and if possible, can you please add the ""Hacktoberfest"" label on this issue, that will be great.
Also, please let me know if there are any concerns regarding the PR. 
Here is the summary of the changes done:
- **Removed Duplicated Options**: The following otel_* options were removed from the [metrics] section in config.yml:
  - otel_debugging_on
  - otel_host
  - otel_on
  - otel_port
  - otel_ssl_active
  
- **Formatting Update in Traces Section**: Improved markdown consistency in the [traces] section for better documentation clarity.

- **Deprecated Options Added**: Added otel_* options to the deprecated list in configuration.py, directing users to the correct [traces] section options.

ferruzzi on (2025-02-06 22:09:11 UTC): They weren't moved, there is a set of options for the metrics and a set for traces, they are two separate features which CAN both handled by otel but are not necessarily.  A user could use statsd for metrics and otel for traces, or use otel for metrics and not want to enable traces at all, or have metrics and traces sent to two different frontend clients, etc.    Some change may be suitable, but simply removing the metrics settings is not the right answer.

"
2612326103,issue,closed,completed,Azure Service Bus message receiver does not pass context to callback method,"### Apache Airflow Provider(s)

microsoft-azure

### Versions of Apache Airflow Providers

10.5.1

### Apache Airflow version

2.10.2

### Operating System

linux

### Deployment

Docker-Compose

### Deployment details

Getting-started docker deployment with PostgreSQL DB

### What happened

The callback to receive an Azure Service Bus message does not include the context and so data from the message cannot be passed back through XComs.

### What you think should happen instead

The callback to receive an Azure Service Bus message should include the context and so data from the message can be passed back through XComs.

### How to reproduce

Method signature indicates only the message is passed into the callback. This would be more useful if the context was passed in so the callback could push some data into XComs such as the location of file from the message.

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",perry2of5,2024-10-24 18:45:57+00:00,[],2024-10-26 21:32:55+00:00,2024-10-26 21:32:55+00:00,https://github.com/apache/airflow/issues/43361,"[('kind:bug', 'This is a clearly a bug'), ('provider:microsoft-azure', 'Azure-related issues'), ('area:providers', ''), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2436108362, 'issue_id': 2612326103, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 24, 18, 46, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2436246932, 'issue_id': 2612326103, 'author': 'perry2of5', 'body': ""Looks like AWS S3 sensor solves this without breaking backwards compatibility by inspecting a function to determine if a context can be passed in:\r\nhttps://github.com/apache/airflow/blob/main/providers/src/airflow/providers/amazon/aws/sensors/s3.py#L169\r\n\r\nGoogle Cloud DataprocCreateClusterOperator does the opposite and removes an argument for backwards compatibility:\r\nhttps://github.com/apache/airflow/blob/main/providers/src/airflow/providers/google/cloud/operators/dataproc.py#L680\r\n\r\nThe Snowflake provider injects the session into the operator kwargs if and only if it was passed into the function:\r\nhttps://github.com/apache/airflow/blob/main/providers/src/airflow/providers/snowflake/utils/snowpark.py#L40\r\n\r\nI think I'll fix this by allowing the callback to include the context if it wants to and then the operator can inspect the arguments and only pass the context if the callback can handle it."", 'created_at': datetime.datetime(2024, 10, 24, 20, 7, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2436486985, 'issue_id': 2612326103, 'author': 'perry2of5', 'body': ""I couldn't figure out to have a type allow two different signatures so I just changed to a new signature which accepts the context. I highly doubt this will inconvenience anyone since the callback hasn't been release in the wild for more than a month and I'm, to my knowledge, the only person who tried to use it. Still, this will require a major release of the azure provider unless we want to revisit this decision."", 'created_at': datetime.datetime(2024, 10, 24, 23, 2, 20, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-24 18:46:01 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

perry2of5 (Issue Creator) on (2024-10-24 20:07:40 UTC): Looks like AWS S3 sensor solves this without breaking backwards compatibility by inspecting a function to determine if a context can be passed in:
https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/amazon/aws/sensors/s3.py#L169

Google Cloud DataprocCreateClusterOperator does the opposite and removes an argument for backwards compatibility:
https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/google/cloud/operators/dataproc.py#L680

The Snowflake provider injects the session into the operator kwargs if and only if it was passed into the function:
https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/snowflake/utils/snowpark.py#L40

I think I'll fix this by allowing the callback to include the context if it wants to and then the operator can inspect the arguments and only pass the context if the callback can handle it.

perry2of5 (Issue Creator) on (2024-10-24 23:02:20 UTC): I couldn't figure out to have a type allow two different signatures so I just changed to a new signature which accepts the context. I highly doubt this will inconvenience anyone since the callback hasn't been release in the wild for more than a month and I'm, to my knowledge, the only person who tried to use it. Still, this will require a major release of the azure provider unless we want to revisit this decision.

"
2612006405,issue,closed,completed,WTForms==3.2.1 breaks UI,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When I deploy docker image of webserver with latest WTForms version the UI for Connections and Variables management breaks with following error:
```
2024-10-24 18:55:09 [2024-10-24T15:55:09.102+0000] {app.py:1744} ERROR - Exception on /variable/list/ [GET]
2024-10-24 18:55:09 Traceback (most recent call last):
2024-10-24 18:55:09   File ""/usr/local/lib/python3.12/site-packages/flask/app.py"", line 2529, in wsgi_app
2024-10-24 18:55:09     response = self.full_dispatch_request()
2024-10-24 18:55:09                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-10-24 18:55:09   File ""/usr/local/lib/python3.12/site-packages/flask/app.py"", line 1825, in full_dispatch_request
2024-10-24 18:55:09     rv = self.handle_user_exception(e)
2024-10-24 18:55:09          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-10-24 18:55:09   File ""/usr/local/lib/python3.12/site-packages/flask/app.py"", line 1823, in full_dispatch_request
2024-10-24 18:55:09     rv = self.dispatch_request()
2024-10-24 18:55:09          ^^^^^^^^^^^^^^^^^^^^^^^
2024-10-24 18:55:09   File ""/usr/local/lib/python3.12/site-packages/flask/app.py"", line 1799, in dispatch_request
2024-10-24 18:55:09     return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
2024-10-24 18:55:09            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-10-24 18:55:09   File ""/usr/local/lib/python3.12/site-packages/flask_appbuilder/security/decorators.py"", line 137, in wraps
2024-10-24 18:55:09     return f(self, *args, **kwargs)
2024-10-24 18:55:09            ^^^^^^^^^^^^^^^^^^^^^^^^
2024-10-24 18:55:09   File ""/usr/local/lib/python3.12/site-packages/flask_appbuilder/views.py"", line 550, in list
2024-10-24 18:55:09     widgets = self._list()
2024-10-24 18:55:09               ^^^^^^^^^^^^
2024-10-24 18:55:09   File ""/usr/local/lib/python3.12/site-packages/flask_appbuilder/baseviews.py"", line 1186, in _list
2024-10-24 18:55:09     form = self.search_form.refresh()
2024-10-24 18:55:09            ^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-10-24 18:55:09   File ""/usr/local/lib/python3.12/site-packages/flask_appbuilder/forms.py"", line 327, in refresh
2024-10-24 18:55:09     form = self(obj=obj)
2024-10-24 18:55:09            ^^^^^^^^^^^^^
2024-10-24 18:55:09   File ""/usr/local/lib/python3.12/site-packages/wtforms/form.py"", line 209, in __call__
2024-10-24 18:55:09     return type.__call__(cls, *args, **kwargs)
2024-10-24 18:55:09            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-10-24 18:55:09   File ""/usr/local/lib/python3.12/site-packages/flask_wtf/form.py"", line 73, in __init__
2024-10-24 18:55:09     super().__init__(formdata=formdata, **kwargs)
2024-10-24 18:55:09   File ""/usr/local/lib/python3.12/site-packages/wtforms/form.py"", line 281, in __init__
2024-10-24 18:55:09     super().__init__(self._unbound_fields, meta=meta_obj, prefix=prefix)
2024-10-24 18:55:09   File ""/usr/local/lib/python3.12/site-packages/wtforms/form.py"", line 49, in __init__
2024-10-24 18:55:09     field = meta.bind_field(self, unbound_field, options)
2024-10-24 18:55:09             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-10-24 18:55:09   File ""/usr/local/lib/python3.12/site-packages/wtforms/meta.py"", line 28, in bind_field
2024-10-24 18:55:09     return unbound_field.bind(form=form, **options)
2024-10-24 18:55:09            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-10-24 18:55:09   File ""/usr/local/lib/python3.12/site-packages/wtforms/fields/core.py"", line 387, in bind
2024-10-24 18:55:09     return self.field_class(*self.args, **kw)
2024-10-24 18:55:09            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-10-24 18:55:09   File ""/usr/local/lib/python3.12/site-packages/wtforms/fields/core.py"", line 133, in __init__
2024-10-24 18:55:09     for k, v in flags.items():
2024-10-24 18:55:09                 ^^^^^^^^^^^
2024-10-24 18:55:09 AttributeError: 'tuple' object has no attribute 'items'
```
Downgrading WTForms to 3.1.2 helps. 

Please consider restricting upper version of this dependency until the compatibility with Flask-WTF is fixed.



### What you think should happen instead?

_No response_

### How to reproduce

Try to install 2.10.2 without restricting the WTForms lib.

### Operating System

Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",RinSer,2024-10-24 15:57:44+00:00,[],2024-10-24 18:35:37+00:00,2024-10-24 18:35:37+00:00,https://github.com/apache/airflow/issues/43356,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('area:dependencies', 'Issues related to dependencies problems'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2435676582, 'issue_id': 2612006405, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 24, 15, 57, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2436065414, 'issue_id': 2612006405, 'author': 'adrianrego', 'body': ""This has been resolved in the `main` branch and 2.10.3 milestone.  I guess we're just waiting for a release.  We manage our own airflow image/environment so we manually pinned the WTForms ourselves while we wait for a new release."", 'created_at': datetime.datetime(2024, 10, 24, 18, 23, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2436088589, 'issue_id': 2612006405, 'author': 'RinSer', 'body': 'Great news, thanks for a quick response. Hope my post will be handy if someone faces the same issue.', 'created_at': datetime.datetime(2024, 10, 24, 18, 35, 37, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-24 15:57:48 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

adrianrego on (2024-10-24 18:23:18 UTC): This has been resolved in the `main` branch and 2.10.3 milestone.  I guess we're just waiting for a release.  We manage our own airflow image/environment so we manually pinned the WTForms ourselves while we wait for a new release.

RinSer (Issue Creator) on (2024-10-24 18:35:37 UTC): Great news, thanks for a quick response. Hope my post will be handy if someone faces the same issue.

"
2611947075,issue,closed,not_planned,Bug: `AirflowSkipException` in `@task.virtualenv` sets task as failed instead of skipped,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When using the `@task.virtualenv` decorator and raising an `AirflowSkipException` inside the task, the task is incorrectly marked as failed instead of skipped. This behavior differs from regular `@task` decorated functions, where raising an `AirflowSkipException` correctly marks the task as skipped.

### What you think should happen instead?

The task should be marked as skipped when the `AirflowSkipException` is raised, consistent with the behavior of regular `@task` decorated functions.


### How to reproduce

Run the following DAG:

```python
from datetime import datetime
from airflow.decorators import dag, task


@dag(
    dag_id='taskflow_raise_skip_in_venv',
    start_date=datetime(2023, 1, 1),
    schedule=None,
    is_paused_upon_creation=False,
    catchup=False
)
def taskflow_raise_skip_in_venv():

    @task.virtualenv(
        requirements=[""stomp-py==8.1.2"", ""requests==2.31.0""],
        venv_cache_path=""/tmp"",
    )
    def potentially_skipped_task():
        from airflow.exceptions import AirflowSkipException
        raise AirflowSkipException(""This task should be skipped!"")
    
    potentially_skipped_task()

taskflow_raise_skip_in_venv()
```

### Operating System

Ubuntu 22

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

LocalExecutor

### Anything else?

Error logs:

```
1cc17766fc4d
*** Found local files:
***   * /opt/airflow/logs/dag_id=taskflow_raise_skip_in_venv/run_id=manual__2024-10-24T15:28:21.408988+00:00/task_id=potentially_skipped_task/attempt=1.log
[2024-10-24, 15:28:22 UTC] {local_task_job_runner.py:123} ▶ Pre task execution logs
[2024-10-24, 15:28:22 UTC] {python.py:809} INFO - Python virtual environment will be cached in /tmp/venv-ab7e4c2f
[2024-10-24, 15:28:22 UTC] {python.py:823} INFO - Re-using cached Python virtual environment in /tmp/venv-ab7e4c2f
[2024-10-24, 15:28:22 UTC] {process_utils.py:186} INFO - Executing cmd: /tmp/venv-ab7e4c2f/bin/python /tmp/venv-callgdxwk2nh/script.py /tmp/venv-callgdxwk2nh/script.in /tmp/venv-callgdxwk2nh/script.out /tmp/venv-callgdxwk2nh/string_args.txt /tmp/venv-callgdxwk2nh/termination.log
[2024-10-24, 15:28:22 UTC] {process_utils.py:190} INFO - Output:
[2024-10-24, 15:28:25 UTC] {process_utils.py:194} INFO - Traceback (most recent call last):
[2024-10-24, 15:28:25 UTC] {process_utils.py:194} INFO -   File ""/tmp/venv-callgdxwk2nh/script.py"", line 47, in <module>
[2024-10-24, 15:28:25 UTC] {process_utils.py:194} INFO -     res = potentially_skipped_task(*arg_dict[""args""], **arg_dict[""kwargs""])
[2024-10-24, 15:28:25 UTC] {process_utils.py:194} INFO -           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-10-24, 15:28:25 UTC] {process_utils.py:194} INFO -   File ""/tmp/venv-callgdxwk2nh/script.py"", line 20, in potentially_skipped_task
[2024-10-24, 15:28:25 UTC] {process_utils.py:194} INFO -     raise AirflowSkipException(""This task should be skipped!"")
[2024-10-24, 15:28:25 UTC] {process_utils.py:194} INFO - ***.exceptions.AirflowSkipException: This task should be skipped!
[2024-10-24, 15:28:25 UTC] {taskinstance.py:3310} ERROR - Task failed with exception
```

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",pedro-cf,2024-10-24 15:31:46+00:00,[],2024-10-27 13:51:23+00:00,2024-10-27 13:45:48+00:00,https://github.com/apache/airflow/issues/43355,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2440030369, 'issue_id': 2611947075, 'author': 'eladkal', 'body': 'When you are inside virtual env you are not ""really"" in the context of Airflow. You can not use special Airflow classes with in or better say `AirflowSkipException` has not special meaning inside the virtual env as Airflow does not look inside of it. What you need to do is return specific error code from the Python callable function and Airflow can check this code against what you specify in the `skip_on_exit_code` parameter:\r\n\r\nhttps://github.com/apache/airflow/blob/63ff22f4038f34354dc5807036d1bf10653c2ecd/airflow/operators/python.py#L674-L676\r\n\r\nClosing as this is not a bug. If you believe this is something that wasn\'t easy to understand please help us by raising PR to improve the docs.', 'created_at': datetime.datetime(2024, 10, 27, 13, 45, 48, tzinfo=datetime.timezone.utc)}]","eladkal on (2024-10-27 13:45:48 UTC): When you are inside virtual env you are not ""really"" in the context of Airflow. You can not use special Airflow classes with in or better say `AirflowSkipException` has not special meaning inside the virtual env as Airflow does not look inside of it. What you need to do is return specific error code from the Python callable function and Airflow can check this code against what you specify in the `skip_on_exit_code` parameter:

https://github.com/apache/airflow/blob/63ff22f4038f34354dc5807036d1bf10653c2ecd/airflow/operators/python.py#L674-L676

Closing as this is not a bug. If you believe this is something that wasn't easy to understand please help us by raising PR to improve the docs.

"
2611933224,issue,closed,completed,Bug: `@task.skip_if` decorator fails when used with `@task.virtualenv` in Airflow TaskFlow API,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When using the `@task.skip_if` decorator in combination with the `@task.virtualenv` decorator in an Airflow DAG using the TaskFlow API, the task fails with a `NameError: name 'task' is not defined` error.


### What you think should happen instead?

The task should execute.


### How to reproduce

Run the following DAG:

```python
from datetime import datetime
from airflow.decorators import dag, task

@dag(
    dag_id='taskflow_skip_if_virtualenv_bug',
    start_date=datetime(2023, 1, 1),
    schedule=None,
    is_paused_upon_creation=False,
    catchup=False
)
def taskflow_skip_if_virtualenv_bug():

    @task.skip_if(condition=lambda context: False)
    @task.virtualenv(
        requirements=[""stomp-py==8.1.2"", ""requests==2.31.0""],
        venv_cache_path=""/tmp"",
    )
    def potentially_skipped_task():
        import requests
        print(""This task should be skipped, but it might run anyway!"")
        response = requests.get(""https://example.com"")
        print(f""Response status code: {response.status_code}"")

    potentially_skipped_task()

taskflow_skip_if_virtualenv_bug()
```

### Operating System

Ubuntu 22

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

LocalExecutor

### Anything else?

Error logs:

```
[2024-10-24, 15:18:42 UTC] {process_utils.py:194} INFO - Traceback (most recent call last):
[2024-10-24, 15:18:42 UTC] {process_utils.py:194} INFO -   File ""/tmp/venv-call4tpdkgmu/script.py"", line 18, in <module>
[2024-10-24, 15:18:42 UTC] {process_utils.py:194} INFO -     @task.skip_if(condition=lambda context: False)
[2024-10-24, 15:18:42 UTC] {process_utils.py:194} INFO -      ^^^^
[2024-10-24, 15:18:42 UTC] {process_utils.py:194} INFO - NameError: name 'task' is not defined
[2024-10-24, 15:18:43 UTC] {taskinstance.py:3310} ERROR - Task failed with exception
```

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",pedro-cf,2024-10-24 15:25:52+00:00,[],2025-01-15 12:45:47+00:00,2024-10-26 09:46:52+00:00,https://github.com/apache/airflow/issues/43354,"[('kind:bug', 'This is a clearly a bug'), ('area:core', '')]","[{'comment_id': 2437322034, 'issue_id': 2611933224, 'author': 'josix', 'body': ""In this example, I believe the correct syntax should be `@task.skip_if(condition=lambda context: True)` to skip the task. The virtualenv operator will skip successfully if the condition returns True. However, I think the virtualenv operator shouldn't depend on the `task` module - cmiiw. I'll investigate this further."", 'created_at': datetime.datetime(2024, 10, 25, 9, 28, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2437381550, 'issue_id': 2611933224, 'author': 'pedro-cf', 'body': ""> In this example, I believe the correct syntax should be `@task.skip_if(condition=lambda context: True)` to skip the task. The virtualenv operator will skip successfully if the condition returns True. However, I think the virtualenv operator shouldn't depend on the `task` module - cmiiw. I'll investigate this further.\r\n\r\nSorry, the intention is not to skip the task, but to execute the task, I got a bit messed up with another bug report. Updated the text above."", 'created_at': datetime.datetime(2024, 10, 25, 9, 59, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2437786227, 'issue_id': 2611933224, 'author': 'josix', 'body': 'I\'ve investigated the root cause of the issue. The problem occurs in the rendered Python script (as shown below) that executes in the virtual environment [through jinja templating](https://github.com/apache/airflow/blob/9e8fb40ba71d9bd07ae40503d5e7aa93ef042bb0/airflow/utils/python_virtualenv_script.jinja2). Specifically, in the Script section (where DAG authors define their code), we\'re using [`inspect.getsource(obj)` ](https://github.com/apache/airflow/blob/9e8fb40ba71d9bd07ae40503d5e7aa93ef042bb0/airflow/operators/python.py#L510) to generate the code. However, this method is also capturing the task decorator along with the callable function, which is causing the bug.\r\n\r\n```python\r\nfrom __future__ import annotations\r\n\r\nimport pickle\r\nimport sys\r\n\r\n\r\n \r\nif sys.version_info >= (3,6):\r\n    try:\r\n        from airflow.plugins_manager import integrate_macros_plugins\r\n        integrate_macros_plugins()\r\n    except ImportError:\r\n        \r\n        pass\r\n\r\n\r\n# Script\r\n@task.skip_if(lambda x: False)\r\ndef potentially_skipped_task():\r\n    import requests\r\n    import time\r\n\r\n    print(""This task should be skipped, but it might run anyway!"")\r\n    response = requests.get(""https://example.com"")\r\n    print(f""Response status code: {response.status_code}"")\r\n\r\n\r\n# monkey patching for the cases when python_callable is part of the dag module.\r\n\r\n\r\nimport types\r\n\r\nunusual_prefix_df02535c9d01616ab041af6470774738d51e9a41_43354  = types.ModuleType(""unusual_prefix_df02535c9d01616ab041af6470774738d51e9a41_43354"")\r\n\r\nunusual_prefix_df02535c9d01616ab041af6470774738d51e9a41_43354.potentially_skipped_task = potentially_skipped_task\r\n\r\nsys.modules[""unusual_prefix_df02535c9d01616ab041af6470774738d51e9a41_43354""] = unusual_prefix_df02535c9d01616ab041af6470774738d51e9a41_43354\r\n\r\n\r\n\r\n\r\narg_dict = {""args"": [], ""kwargs"": {}}\r\n\r\n\r\n# Read string args\r\nwith open(sys.argv[3], ""r"") as file:\r\n    virtualenv_string_args = list(map(lambda x: x.strip(), list(file)))\r\n\r\n\r\n\r\ntry:\r\n    res = potentially_skipped_task(*arg_dict[""args""], **arg_dict[""kwargs""])\r\nexcept Exception as e:\r\n    with open(sys.argv[4], ""w"") as file:\r\n        file.write(str(e))\r\n    raise\r\n\r\n# Write output\r\nwith open(sys.argv[2], ""wb"") as file:\r\n    if res is not None:\r\n        pickle.dump(res, file)\r\n```', 'created_at': datetime.datetime(2024, 10, 25, 13, 29, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2438113546, 'issue_id': 2611933224, 'author': 'josix', 'body': 'Just found that #41832 has covered this issue, I guess we would have a patch at v2.10.3 cc @phi-friday', 'created_at': datetime.datetime(2024, 10, 25, 15, 23, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439452465, 'issue_id': 2611933224, 'author': 'eladkal', 'body': 'Closing as fixed in 2.10.3', 'created_at': datetime.datetime(2024, 10, 26, 9, 46, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2458867316, 'issue_id': 2611933224, 'author': 'phi-friday', 'body': '> Closing as fixed in 2.10.3\r\n\r\n```python\r\nIn [2]: import airflow\r\n/Users/***/git/python/repo/***/.venv/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name\r\n\r\nIn [3]: airflow.__version__\r\nOut[3]: \'2.10.3\'\r\n\r\nIn [4]: from airflow.utils.decorators import remove_task_decorator\r\n\r\nIn [5]: import inspect\r\n\r\nIn [6]: source = inspect.getsource(remove_task_decorator)\r\n\r\nIn [7]: print(source)\r\ndef remove_task_decorator(python_source: str, task_decorator_name: str) -> str:\r\n    """"""\r\n    Remove @task or similar decorators as well as @setup and @teardown.\r\n\r\n    :param python_source: The python source code\r\n    :param task_decorator_name: the decorator name\r\n\r\n    TODO: Python 3.9+: Rewrite this to use ast.parse and ast.unparse\r\n    """"""\r\n\r\n    def _remove_task_decorator(py_source, decorator_name):\r\n        # if no line starts with @decorator_name, we can early exit\r\n        for line in py_source.split(""\\n""):\r\n            if line.startswith(decorator_name):\r\n                break\r\n        else:\r\n            return python_source\r\n        split = python_source.split(decorator_name, 1)\r\n        before_decorator, after_decorator = split[0], split[1]\r\n        if after_decorator[0] == ""("":\r\n            after_decorator = _balance_parens(after_decorator)\r\n        if after_decorator[0] == ""\\n"":\r\n            after_decorator = after_decorator[1:]\r\n        return before_decorator + after_decorator\r\n\r\n    decorators = [""@setup"", ""@teardown"", task_decorator_name]\r\n    for decorator in decorators:\r\n        python_source = _remove_task_decorator(python_source, decorator)\r\n    return python_source\r\n```\r\n\r\nThis does not appear to have been fixed in 2.10.3.\r\n(If it was fixed, `run_if` should be added to the `decorators` variable).\r\nHas this PR #41832  been pushed back to `2.11` or `3.0`?', 'created_at': datetime.datetime(2024, 11, 6, 7, 10, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2592751156, 'issue_id': 2611933224, 'author': 'josix', 'body': ""Thanks to @phi-friday's checking, I just created one PR to add this patch to v2-10."", 'created_at': datetime.datetime(2025, 1, 15, 12, 45, 46, tzinfo=datetime.timezone.utc)}]","josix on (2024-10-25 09:28:51 UTC): In this example, I believe the correct syntax should be `@task.skip_if(condition=lambda context: True)` to skip the task. The virtualenv operator will skip successfully if the condition returns True. However, I think the virtualenv operator shouldn't depend on the `task` module - cmiiw. I'll investigate this further.

pedro-cf (Issue Creator) on (2024-10-25 09:59:07 UTC): Sorry, the intention is not to skip the task, but to execute the task, I got a bit messed up with another bug report. Updated the text above.

josix on (2024-10-25 13:29:50 UTC): I've investigated the root cause of the issue. The problem occurs in the rendered Python script (as shown below) that executes in the virtual environment [through jinja templating](https://github.com/apache/airflow/blob/9e8fb40ba71d9bd07ae40503d5e7aa93ef042bb0/airflow/utils/python_virtualenv_script.jinja2). Specifically, in the Script section (where DAG authors define their code), we're using [`inspect.getsource(obj)` ](https://github.com/apache/airflow/blob/9e8fb40ba71d9bd07ae40503d5e7aa93ef042bb0/airflow/operators/python.py#L510) to generate the code. However, this method is also capturing the task decorator along with the callable function, which is causing the bug.

```python
from __future__ import annotations

import pickle
import sys


 
if sys.version_info >= (3,6):
    try:
        from airflow.plugins_manager import integrate_macros_plugins
        integrate_macros_plugins()
    except ImportError:
        
        pass


# Script
@task.skip_if(lambda x: False)
def potentially_skipped_task():
    import requests
    import time

    print(""This task should be skipped, but it might run anyway!"")
    response = requests.get(""https://example.com"")
    print(f""Response status code: {response.status_code}"")


# monkey patching for the cases when python_callable is part of the dag module.


import types

unusual_prefix_df02535c9d01616ab041af6470774738d51e9a41_43354  = types.ModuleType(""unusual_prefix_df02535c9d01616ab041af6470774738d51e9a41_43354"")

unusual_prefix_df02535c9d01616ab041af6470774738d51e9a41_43354.potentially_skipped_task = potentially_skipped_task

sys.modules[""unusual_prefix_df02535c9d01616ab041af6470774738d51e9a41_43354""] = unusual_prefix_df02535c9d01616ab041af6470774738d51e9a41_43354




arg_dict = {""args"": [], ""kwargs"": {}}


# Read string args
with open(sys.argv[3], ""r"") as file:
    virtualenv_string_args = list(map(lambda x: x.strip(), list(file)))



try:
    res = potentially_skipped_task(*arg_dict[""args""], **arg_dict[""kwargs""])
except Exception as e:
    with open(sys.argv[4], ""w"") as file:
        file.write(str(e))
    raise

# Write output
with open(sys.argv[2], ""wb"") as file:
    if res is not None:
        pickle.dump(res, file)
```

josix on (2024-10-25 15:23:31 UTC): Just found that #41832 has covered this issue, I guess we would have a patch at v2.10.3 cc @phi-friday

eladkal on (2024-10-26 09:46:53 UTC): Closing as fixed in 2.10.3

phi-friday on (2024-11-06 07:10:27 UTC): ```python
In [2]: import airflow
/Users/***/git/python/repo/***/.venv/lib/python3.12/site-packages/airflow/configuration.py:859 FutureWarning: section/key [core/sql_alchemy_conn] has been deprecated, you should use[database/sql_alchemy_conn] instead. Please update your `conf.get*` call to use the new name

In [3]: airflow.__version__
Out[3]: '2.10.3'

In [4]: from airflow.utils.decorators import remove_task_decorator

In [5]: import inspect

In [6]: source = inspect.getsource(remove_task_decorator)

In [7]: print(source)
def remove_task_decorator(python_source: str, task_decorator_name: str) -> str:
    """"""
    Remove @task or similar decorators as well as @setup and @teardown.

    :param python_source: The python source code
    :param task_decorator_name: the decorator name

    TODO: Python 3.9+: Rewrite this to use ast.parse and ast.unparse
    """"""

    def _remove_task_decorator(py_source, decorator_name):
        # if no line starts with @decorator_name, we can early exit
        for line in py_source.split(""\n""):
            if line.startswith(decorator_name):
                break
        else:
            return python_source
        split = python_source.split(decorator_name, 1)
        before_decorator, after_decorator = split[0], split[1]
        if after_decorator[0] == ""("":
            after_decorator = _balance_parens(after_decorator)
        if after_decorator[0] == ""\n"":
            after_decorator = after_decorator[1:]
        return before_decorator + after_decorator

    decorators = [""@setup"", ""@teardown"", task_decorator_name]
    for decorator in decorators:
        python_source = _remove_task_decorator(python_source, decorator)
    return python_source
```

This does not appear to have been fixed in 2.10.3.
(If it was fixed, `run_if` should be added to the `decorators` variable).
Has this PR #41832  been pushed back to `2.11` or `3.0`?

josix on (2025-01-15 12:45:46 UTC): Thanks to @phi-friday's checking, I just created one PR to add this patch to v2-10.

"
2611596235,issue,closed,completed,Problem with `retry_from_failure` flag in `DbtCloudRunJobOperator`,"### Apache Airflow Provider(s)

dbt-cloud

### Versions of Apache Airflow Providers

Astronomer Runtime 12.1.0 based on Airflow 2.10.1+astro.1
Git Version: .release:7a1ffe6438b5ea8fcf75c4e5a356a6c23ab18404

### Apache Airflow version

Airflow 2.10.1+astro.1

### Operating System

Debian GNU/Linux 12 (bookworm)

### Deployment

Astronomer

### Deployment details

pure: `https://github.com/dbt-labs/airflow-dbt-cloud`

### What happened

With the `retry_from_failure=True` flag set, each run only executes the models that failed in the previous run, which is fine. However, if there is an error in one model that can't be resolved (e.g., due to a data source issue), the flag prevents the other models from being refreshed, even in subsequent **scheduled** runs.

### What you think should happen instead

I think `retry_from_failure` should only apply to reruns. Two improvements might be thought of.

1. add a condition that triggers `{account_id}/jobs/{job_id}/rerun/` only during task reruns
e.g. replacing line 463 in `providers/dbt/cloud/hooks/dbt.py` 
from `if retry_from_failure:` to something like `if retry_from_failure and context['task_instance'].try_number!=1:`

2. a more general solution replacing the flag with a parameter with several values ​​e.g.
`retry_from_failure = [""Never"", ""Rerun"", ""Always""]`

### How to reproduce

You don't need to do anything specific to reproduce the issue. The flag works on every run, but it should likely only affect reruns.

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",krzysztof-kubis,2024-10-24 13:19:32+00:00,['krzysztof-kubis'],2024-11-09 03:33:59+00:00,2024-11-09 03:33:59+00:00,https://github.com/apache/airflow/issues/43347,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('kind:feature', 'Feature Requests'), ('good first issue', ''), ('provider:dbt-cloud', '')]","[{'comment_id': 2436286820, 'issue_id': 2611596235, 'author': 'potiuk', 'body': 'Feel free to propose a PR and run it. Assigned you.', 'created_at': datetime.datetime(2024, 10, 24, 20, 32, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2456261147, 'issue_id': 2611596235, 'author': 'jaklan', 'body': '@potiuk we are still waiting for the review of this one-line change: https://github.com/apache/airflow/pull/43453', 'created_at': datetime.datetime(2024, 11, 5, 5, 22, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466020650, 'issue_id': 2611596235, 'author': 'josh-fell', 'body': 'Closed via #43453', 'created_at': datetime.datetime(2024, 11, 9, 3, 33, 59, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-10-24 20:32:32 UTC): Feel free to propose a PR and run it. Assigned you.

jaklan on (2024-11-05 05:22:54 UTC): @potiuk we are still waiting for the review of this one-line change: https://github.com/apache/airflow/pull/43453

josh-fell on (2024-11-09 03:33:59 UTC): Closed via #43453

"
2610913345,issue,open,,"Add SBOMS generated as artifacts in ""canary"" build","Currently, we have ""sbom"" generation done as part of the release. But it does not happen when ""canary"" build is executed in main. And @raboof - the Security Response engineer from the ASF is now building automation and exposure of the SBOMs for all ASF projects, so they are interested in being able to retrieve SBOMS from our main build automatically.

We should restore #41931 ""Publish docs"" scheme and add ""update-sbom"" command  to include SBOM generation from main - so that it can be published in our test site. It should be as easy as running single command during doc publishing and likely fixing it so that it works with main constraints rather than only with released versions.

This is basically the command run to generted SBOM (and here support for `main` should be added if it does not work:

```
breeze sbom update-sbom-information --airflow-version ${VERSION} --airflow-site-directory ${AIRFLOW_SITE_DIRECTORY} --force --all-combinations --run-in-parallel
```

The actual step in release process for that is https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md#publish-documentation
",potiuk,2024-10-24 08:56:21+00:00,[],2024-10-24 08:58:35+00:00,,https://github.com/apache/airflow/issues/43343,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:CI', ""Airflow's tests and continious integration"")]",[],
2609806447,issue,closed,completed,Out of the box gunicorn_config.py doesn't allow cipher suite configuration.,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

sslyze localhost:8080
```
Cipher suites {'TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA', 'TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384', 'TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256', 'TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA'} are supported, but should be rejected.
```

### What you think should happen instead?

The **webserver()** function in **airflow/cli/commands/webserver_command.py** should either allow the cipher suite to be tailored (or pass an sslyze audit out of the box.

### How to reproduce

1. start the web server
2. scan the web server with sslyze.

### Operating System

""Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

apache-airflow-providers-common-compat==1.2.1
apache-airflow-providers-common-io==1.4.2
apache-airflow-providers-fab==1.4.1
apache-airflow-providers-http==4.13.1


### Deployment

Virtualenv installation

### Deployment details

nstr

### Anything else?

Occurs every time.

PR available upon request.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",3BK,2024-10-23 20:46:00+00:00,[],2024-10-24 00:39:14+00:00,2024-10-24 00:39:14+00:00,https://github.com/apache/airflow/issues/43332,"[('kind:bug', 'This is a clearly a bug'), ('security', 'Security issues that must be fixed'), ('area:webserver', 'Webserver related Issues'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2433425828, 'issue_id': 2609806447, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 23, 20, 46, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2433486315, 'issue_id': 2609806447, 'author': '3BK', 'body': 'At present, the gunicorn config is hard coded as shown below.\r\n```\r\nrun_args = [\r\n            sys.executable,\r\n            ""-m"",\r\n            ""gunicorn"",\r\n            ""--workers"",\r\n            str(num_workers),\r\n            ""--worker-class"",\r\n            str(args.workerclass),\r\n            ""--timeout"",\r\n            str(worker_timeout),\r\n            ""--bind"",\r\n            args.hostname + "":"" + str(args.port),\r\n            ""--name"",\r\n            ""airflow-webserver"",\r\n            ""--pid"",\r\n            pid_file,\r\n            ""--config"",\r\n            ""python:airflow.www.gunicorn_config"",\r\n        ]\r\n```', 'created_at': datetime.datetime(2024, 10, 23, 21, 20, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2433496379, 'issue_id': 2609806447, 'author': '3BK', 'body': 'Here is a potential fix for the webserver() function in airflow/cli/commands/webserver_command.py. \r\n```\r\n if cipher_suite:\r\n            run_args += [""--ciphers"", cipher_suite]\r\n\r\n```\r\nref: https://docs.gunicorn.org/en/latest/settings.html#ciphers', 'created_at': datetime.datetime(2024, 10, 23, 21, 27, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2433503419, 'issue_id': 2609806447, 'author': '3BK', 'body': 'The alternative would be to hard code the cipher suites so they pass an OWASP scan.\r\nhttps://cheatsheetseries.owasp.org/cheatsheets/Transport_Layer_Security_Cheat_Sheet.html', 'created_at': datetime.datetime(2024, 10, 23, 21, 32, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2433671306, 'issue_id': 2609806447, 'author': 'potiuk', 'body': 'Look at the top of the document you linked to:\n\nNote\n\n\xa0\n\nSettings can be specified by using environment variable\xa0GUNICORN_CMD_ARGS. All available command line arguments can be used. For example, to specify the bind address and number of workers:\n\n$ GUNICORN_CMD_ARGS=""--bind=127.0.0.1 --worker', 'created_at': datetime.datetime(2024, 10, 23, 22, 50, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2433672272, 'issue_id': 2609806447, 'author': 'potiuk', 'body': 'This is how you can set arguments', 'created_at': datetime.datetime(2024, 10, 23, 22, 50, 24, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-23 20:46:03 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

3BK (Issue Creator) on (2024-10-23 21:20:55 UTC): At present, the gunicorn config is hard coded as shown below.
```
run_args = [
            sys.executable,
            ""-m"",
            ""gunicorn"",
            ""--workers"",
            str(num_workers),
            ""--worker-class"",
            str(args.workerclass),
            ""--timeout"",
            str(worker_timeout),
            ""--bind"",
            args.hostname + "":"" + str(args.port),
            ""--name"",
            ""airflow-webserver"",
            ""--pid"",
            pid_file,
            ""--config"",
            ""python:airflow.www.gunicorn_config"",
        ]
```

3BK (Issue Creator) on (2024-10-23 21:27:39 UTC): Here is a potential fix for the webserver() function in airflow/cli/commands/webserver_command.py. 
```
 if cipher_suite:
            run_args += [""--ciphers"", cipher_suite]

```
ref: https://docs.gunicorn.org/en/latest/settings.html#ciphers

3BK (Issue Creator) on (2024-10-23 21:32:47 UTC): The alternative would be to hard code the cipher suites so they pass an OWASP scan.
https://cheatsheetseries.owasp.org/cheatsheets/Transport_Layer_Security_Cheat_Sheet.html

potiuk on (2024-10-23 22:50:07 UTC): Look at the top of the document you linked to:

Note

 

Settings can be specified by using environment variable GUNICORN_CMD_ARGS. All available command line arguments can be used. For example, to specify the bind address and number of workers:

$ GUNICORN_CMD_ARGS=""--bind=127.0.0.1 --worker

potiuk on (2024-10-23 22:50:24 UTC): This is how you can set arguments

"
2609535751,issue,open,,AIP-38 | Add Pool Summary stats to Dashboard,"Now that we have Get Pools in FastAPI and the UI Dashboard page. Let's add the Pool Slots Summary component:

![Image](https://github.com/user-attachments/assets/f2f7e778-b7d3-499a-b332-b6441bc50aae)

We should aggregate information across all pools to have a combined total for each slot type. Each section can have a tooltip to say what slot type it is and across how many pools, if greater than one.",bbovenzi,2024-10-23 18:40:59+00:00,['LefterisXefteris'],2024-11-11 15:05:16+00:00,,https://github.com/apache/airflow/issues/43328,"[('good first issue', ''), ('AIP-38', 'Modern Web Application')]","[{'comment_id': 2433356810, 'issue_id': 2609535751, 'author': 'LefterisXefteris', 'body': ""Hello @bbovenzi !\r\nI would like to work on this issue, if that's okay."", 'created_at': datetime.datetime(2024, 10, 23, 20, 18, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2433377030, 'issue_id': 2609535751, 'author': 'bbovenzi', 'body': '@LefterisXefteris go for it', 'created_at': datetime.datetime(2024, 10, 23, 20, 28, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2433379243, 'issue_id': 2609535751, 'author': 'LefterisXefteris', 'body': 'Thank you very much!', 'created_at': datetime.datetime(2024, 10, 23, 20, 29, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455594338, 'issue_id': 2609535751, 'author': 'LefterisXefteris', 'body': ""Hello @bbovenzi! I'm working on this issue and I can now dedicate time to complete it. I notice you mentioned a modern web application, but when I run the development servers (frontend with yarn dev in airflow/www/ and backend with airflow webserver), I'm only seeing the old web interface with the navigation bar at the top. Could you please guide me on how to access the modern web application interface? I want to ensure I'm working on the correct UI version for this implementation."", 'created_at': datetime.datetime(2024, 11, 4, 20, 2, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455610970, 'issue_id': 2609535751, 'author': 'bbovenzi', 'body': 'The new UI code is in `airflow/ui` and you only have to run `breeze start-airflow --dev-mode` and the new UI will be at localhost:29091 with hot reload already working', 'created_at': datetime.datetime(2024, 11, 4, 20, 12, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455614603, 'issue_id': 2609535751, 'author': 'bbovenzi', 'body': 'Make sure to pull down the latest from main. I just did some big changes to the UI on Friday', 'created_at': datetime.datetime(2024, 11, 4, 20, 14, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2463337647, 'issue_id': 2609535751, 'author': 'LefterisXefteris', 'body': ""Hello @bbovenzi !\r\nI've set up the new development environment following the instructions in `dev/breeze/README.md`. Current status:\r\n\r\n✅ Successfully implemented:\r\n- Breeze installation and configuration using the new UV-based workflow\r\n- Created UV virtual environment and synced dependencies from `dev/breeze/uv.lock`\r\n- Legacy UI accessible at :28080\r\n- Postgres backend running\r\n\r\n❌ Issue:\r\nReceiving 404 Not Found when attempting to access the FastAPI UI at `localhost:29091`. \r\n\r\nCurrent setup:\r\n```bash\r\n# Environment\r\nbreeze start-airflow --dev-mode OR\r\nbreeze start-airflow --dev-mode --python 3.10 --use-uv --backend postgres\r\n\r\n# Running containers show proper port mapping\r\n$ docker ps\r\nCONTAINER ID   IMAGE                                              PORTS                                                                                              \r\nd16ac27f695e   ghcr.io/apache/airflow/main/ci/python3.10:latest   0.0.0.0:29091->9091/tcp"", 'created_at': datetime.datetime(2024, 11, 7, 22, 15, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2464881780, 'issue_id': 2609535751, 'author': 'bbovenzi', 'body': 'When you run `breeze start-airflow` the other services are running but not the FastAPI one. Is that right?\r\n\r\nIf so, can you try to if there are any errors in the terminal? You can also just run fastapi by running `breeze` and then inside of breeze running `airflow fastapi-api`', 'created_at': datetime.datetime(2024, 11, 8, 14, 19, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465471328, 'issue_id': 2609535751, 'author': 'LefterisXefteris', 'body': 'When running Airflow in the Breeze development environment, there\'s an inconsistency between the webserver and FastAPI services accessibility:\r\n\r\n### Working\r\n- Legacy UI (webserver) is accessible at `http://localhost:28080`\r\n- Successfully authenticating with credentials\r\n- Webserver service starts correctly with `airflow webserver` command\r\n - API documentation IS accessible at http://localhost:29091/docs`\r\n\r\n### Not Working\r\n- FastAPI endpoints return 404 responses when accessing through `http://localhost:29091`\r\n- Service starts without errors using `airflow fastapi-api` command\r\n\r\n\r\n### Environment Details\r\n- Using Breeze development environment\r\n- Python virtual environment configured\r\n- PostgreSQL backend\r\n- Default authentication configured\r\n\r\n### Questions\r\n1. Are additional development dependencies required? Specifically:\r\n   - `pip install -e "".[devel-all]""`\r\n   - Specific provider packages\r\n   - Hatch build system dependencies\r\n\r\n### Attempted Steps\r\n1. Started services via Breeze environment\r\n2. Started FastAPI service with `airflow fastapi-api`\r\n\r\n### Expected Behavior\r\nFastAPI endpoints should be accessible at `http://localhost:29091with proper authentication.\r\n\r\n### Logs\r\n404 - not found', 'created_at': datetime.datetime(2024, 11, 8, 18, 12, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465678001, 'issue_id': 2609535751, 'author': 'bbovenzi', 'body': 'Wait, have you navigated to `http://localhost:29091/webapp` or `http://localhost:29091/docs`?', 'created_at': datetime.datetime(2024, 11, 8, 20, 15, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465693089, 'issue_id': 2609535751, 'author': 'LefterisXefteris', 'body': 'When I navigate to /docs, it works perfectly. However, when I try to access http://localhost:29091/webapp, I get an Internal Server Error.', 'created_at': datetime.datetime(2024, 11, 8, 20, 24, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465700130, 'issue_id': 2609535751, 'author': 'bbovenzi', 'body': 'And then looking at your logs in the terminal running breeze? does it explain the internal error at all? Also, feel free to message me on the Airflow slack for faster back and forth debugging help', 'created_at': datetime.datetime(2024, 11, 8, 20, 30, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465706343, 'issue_id': 2609535751, 'author': 'LefterisXefteris', 'body': 'this is what i get at the log:\r\n\r\n172.18.0.1:57800 - ""GET /webapp HTTP/1.1"" 307\r\n172.18.0.1:57800 - ""GET /webapp/ HTTP/1.1"" 500\r\n\r\nKey Error:\r\njinja2.exceptions.TemplateNotFound: /index.html\r\n\r\nLocation:\r\nFile ""/opt/airflow/airflow/api_fastapi/core_api/app.py"", line 76, in webapp\r\nreturn templates.TemplateResponse(""/index.html"", {""request"": request}, media_type=""text/html"")', 'created_at': datetime.datetime(2024, 11, 8, 20, 34, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2468393258, 'issue_id': 2609535751, 'author': 'bbovenzi', 'body': ""We did resolve this on slack. Manually building the UI once with `pnpm install && pnpm build` created the `/dist` directory necessary for fastapi to host the UI's html file."", 'created_at': datetime.datetime(2024, 11, 11, 15, 5, 14, tzinfo=datetime.timezone.utc)}]","LefterisXefteris (Assginee) on (2024-10-23 20:18:19 UTC): Hello @bbovenzi !
I would like to work on this issue, if that's okay.

bbovenzi (Issue Creator) on (2024-10-23 20:28:51 UTC): @LefterisXefteris go for it

LefterisXefteris (Assginee) on (2024-10-23 20:29:40 UTC): Thank you very much!

LefterisXefteris (Assginee) on (2024-11-04 20:02:34 UTC): Hello @bbovenzi! I'm working on this issue and I can now dedicate time to complete it. I notice you mentioned a modern web application, but when I run the development servers (frontend with yarn dev in airflow/www/ and backend with airflow webserver), I'm only seeing the old web interface with the navigation bar at the top. Could you please guide me on how to access the modern web application interface? I want to ensure I'm working on the correct UI version for this implementation.

bbovenzi (Issue Creator) on (2024-11-04 20:12:02 UTC): The new UI code is in `airflow/ui` and you only have to run `breeze start-airflow --dev-mode` and the new UI will be at localhost:29091 with hot reload already working

bbovenzi (Issue Creator) on (2024-11-04 20:14:09 UTC): Make sure to pull down the latest from main. I just did some big changes to the UI on Friday

LefterisXefteris (Assginee) on (2024-11-07 22:15:16 UTC): Hello @bbovenzi !
I've set up the new development environment following the instructions in `dev/breeze/README.md`. Current status:

✅ Successfully implemented:
- Breeze installation and configuration using the new UV-based workflow
- Created UV virtual environment and synced dependencies from `dev/breeze/uv.lock`
- Legacy UI accessible at :28080
- Postgres backend running

❌ Issue:
Receiving 404 Not Found when attempting to access the FastAPI UI at `localhost:29091`. 

Current setup:
```bash
# Environment
breeze start-airflow --dev-mode OR
breeze start-airflow --dev-mode --python 3.10 --use-uv --backend postgres

# Running containers show proper port mapping
$ docker ps
CONTAINER ID   IMAGE                                              PORTS                                                                                              
d16ac27f695e   ghcr.io/apache/airflow/main/ci/python3.10:latest   0.0.0.0:29091->9091/tcp

bbovenzi (Issue Creator) on (2024-11-08 14:19:03 UTC): When you run `breeze start-airflow` the other services are running but not the FastAPI one. Is that right?

If so, can you try to if there are any errors in the terminal? You can also just run fastapi by running `breeze` and then inside of breeze running `airflow fastapi-api`

LefterisXefteris (Assginee) on (2024-11-08 18:12:59 UTC): When running Airflow in the Breeze development environment, there's an inconsistency between the webserver and FastAPI services accessibility:

### Working
- Legacy UI (webserver) is accessible at `http://localhost:28080`
- Successfully authenticating with credentials
- Webserver service starts correctly with `airflow webserver` command
 - API documentation IS accessible at http://localhost:29091/docs`

### Not Working
- FastAPI endpoints return 404 responses when accessing through `http://localhost:29091`
- Service starts without errors using `airflow fastapi-api` command


### Environment Details
- Using Breeze development environment
- Python virtual environment configured
- PostgreSQL backend
- Default authentication configured

### Questions
1. Are additional development dependencies required? Specifically:
   - `pip install -e "".[devel-all]""`
   - Specific provider packages
   - Hatch build system dependencies

### Attempted Steps
1. Started services via Breeze environment
2. Started FastAPI service with `airflow fastapi-api`

### Expected Behavior
FastAPI endpoints should be accessible at `http://localhost:29091with proper authentication.

### Logs
404 - not found

bbovenzi (Issue Creator) on (2024-11-08 20:15:43 UTC): Wait, have you navigated to `http://localhost:29091/webapp` or `http://localhost:29091/docs`?

LefterisXefteris (Assginee) on (2024-11-08 20:24:28 UTC): When I navigate to /docs, it works perfectly. However, when I try to access http://localhost:29091/webapp, I get an Internal Server Error.

bbovenzi (Issue Creator) on (2024-11-08 20:30:07 UTC): And then looking at your logs in the terminal running breeze? does it explain the internal error at all? Also, feel free to message me on the Airflow slack for faster back and forth debugging help

LefterisXefteris (Assginee) on (2024-11-08 20:34:57 UTC): this is what i get at the log:

172.18.0.1:57800 - ""GET /webapp HTTP/1.1"" 307
172.18.0.1:57800 - ""GET /webapp/ HTTP/1.1"" 500

Key Error:
jinja2.exceptions.TemplateNotFound: /index.html

Location:
File ""/opt/airflow/airflow/api_fastapi/core_api/app.py"", line 76, in webapp
return templates.TemplateResponse(""/index.html"", {""request"": request}, media_type=""text/html"")

bbovenzi (Issue Creator) on (2024-11-11 15:05:14 UTC): We did resolve this on slack. Manually building the UI once with `pnpm install && pnpm build` created the `/dist` directory necessary for fastapi to host the UI's html file.

"
2609454435,issue,closed,completed,AIP-84 Migrate the public endpoint Get Event Log to FastAPI,"Migrate the public endpoint `get_event_log` for a single event log item to FastAPI

Part of: https://github.com/apache/airflow/issues/42370",bbovenzi,2024-10-23 18:15:11+00:00,[],2024-10-29 14:34:59+00:00,2024-10-29 14:34:59+00:00,https://github.com/apache/airflow/issues/43327,"[('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2433057699, 'issue_id': 2609454435, 'author': 'bbovenzi', 'body': 'cc: @jason810496', 'created_at': datetime.datetime(2024, 10, 23, 18, 16, 17, tzinfo=datetime.timezone.utc)}]","bbovenzi (Issue Creator) on (2024-10-23 18:16:17 UTC): cc: @jason810496

"
2609448884,issue,closed,completed,AIP-84 Migrate the public endpoint Get Event Logs to FastAPI,"Migrate the public endpoint `get_event_logs` to FastAPI

Part of https://github.com/apache/airflow/issues/42370",bbovenzi,2024-10-23 18:13:21+00:00,['jason810496'],2024-11-05 10:51:23+00:00,2024-11-05 10:51:23+00:00,https://github.com/apache/airflow/issues/43326,"[('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2433059959, 'issue_id': 2609448884, 'author': 'bbovenzi', 'body': 'cc: @jason810496', 'created_at': datetime.datetime(2024, 10, 23, 18, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434547160, 'issue_id': 2609448884, 'author': 'Bowrna', 'body': '@bbovenzi could i try fixing this issue if its not taken?', 'created_at': datetime.datetime(2024, 10, 24, 7, 49, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2434605842, 'issue_id': 2609448884, 'author': 'jason810496', 'body': 'Hi @Bowrna, I’m already working on this issue.', 'created_at': datetime.datetime(2024, 10, 24, 8, 18, 36, tzinfo=datetime.timezone.utc)}]","bbovenzi (Issue Creator) on (2024-10-23 18:17:00 UTC): cc: @jason810496

Bowrna on (2024-10-24 07:49:21 UTC): @bbovenzi could i try fixing this issue if its not taken?

jason810496 (Assginee) on (2024-10-24 08:18:36 UTC): Hi @Bowrna, I’m already working on this issue.

"
2608962578,issue,open,,No celery logs when celery_stdout_stderr_separation is turned on,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Hi, I have trouble with `AIRFLOW__LOGGING__CELERY_STDOUT_STDERR_SEPARATION` config variable ([docs](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#celery-stdout-stderr-separation)).

By default celery outputs all logs to stderr. What I'm trying to do is to send INFO/WARNING severity logs to stdout  and ERROR severity to stderr . This is what the config  `AIRFLOW__LOGGING__CELERY_STDOUT_STDERR_SEPARATION`  is supposed to do. I've tried using it, but what really happens is I get no logs from celery at all.
The logs I am talking about look like `2024-10-23 09:31:46 [2024-10-23 07:31:46,805: DEBUG/ForkPoolWorker-29] Loaded DAG <DAG: tmp-dag>`. They are not the task logs.

I've also tried to set `AIRFLOW__LOGGING__CELERY_LOGGING_LEVEL`, but while the stdout_stderr separation is turned on, it doesn't have effect.

### What you think should happen instead?

I think the amount of logs displayed should not change by setting the config variable, they should just be separated into stdout/stderr.

### How to reproduce

I've used docker-compose.yml from [documentation](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#fetching-docker-compose-yaml) (tried airflow versions 2.10.2 and 2.7.3) and added AIRFLOW__LOGGING__CELERY_STDOUT_STDERR_SEPARATION: True  to environment.

1) Run with `docker compose up -d`
2) Get logs ` docker logs tmp-airflow-airflow-worker-1` (name is dependent on running containers)
3) No celery logs to see

Whole `docker-compose.yml` file looks like:
```yaml
---
x-airflow-common:
  &airflow-common
  # In order to add custom dependencies or upgrade provider packages you can use your extended image.
  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml
  # and uncomment the ""build"" line below, Then run `docker-compose build` to build the images.
  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.10.2}
  # build: .
  environment:
    &airflow-common-env
    AIRFLOW__LOGGING__CELERY_LOGGING_LEVEL: DEBUG
    AIRFLOW__LOGGING__CELERY_STDOUT_STDERR_SEPARATION: True
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    # yamllint disable rule:line-length
    # Use simple http server on scheduler for health checks
    # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server
    # yamllint enable rule:line-length
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks
    # for other purpose (development, test and especially production usage) build/extend Airflow image.
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
    # The following line can be used to set a custom config file, stored in the local config folder
    # If you want to use it, outcomment it and replace airflow.cfg with the name of your config file
    # AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'
  volumes:
    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
  user: ""${AIRFLOW_UID:-50000}:0""
  depends_on:
    &airflow-common-depends-on
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: [""CMD"", ""pg_isready"", ""-U"", ""airflow""]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always

  redis:
    # Redis is limited to 7.2-bookworm due to licencing change
    # https://redis.io/blog/redis-adopts-dual-source-available-licensing/
    image: redis:7.2-bookworm
    expose:
      - 6379
    healthcheck:
      test: [""CMD"", ""redis-cli"", ""ping""]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    restart: always

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - ""8080:8080""
    healthcheck:
      test: [""CMD"", ""curl"", ""--fail"", ""http://localhost:8080/health""]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: [""CMD"", ""curl"", ""--fail"", ""http://localhost:8974/health""]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-worker:
    <<: *airflow-common
    command: celery worker
    healthcheck:
      # yamllint disable rule:line-length
      test:
        - ""CMD-SHELL""
        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d ""celery@$${HOSTNAME}"" || celery --app airflow.executors.celery_executor.app inspect ping -d ""celery@$${HOSTNAME}""'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    environment:
      <<: *airflow-common-env
      # Required to handle warm shutdown of the celery workers properly
      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation
      DUMB_INIT_SETSID: ""0""
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-triggerer:
    <<: *airflow-common
    command: triggerer
    healthcheck:
      test: [""CMD-SHELL"", 'airflow jobs check --job-type TriggererJob --hostname ""$${HOSTNAME}""']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    # yamllint disable rule:line-length
    command:
      - -c
      - |
        if [[ -z ""${AIRFLOW_UID}"" ]]; then
          echo
          echo -e ""\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m""
          echo ""If you are on Linux, you SHOULD follow the instructions below to set ""
          echo ""AIRFLOW_UID environment variable, otherwise files will be owned by root.""
          echo ""For other operating systems you can get rid of the warning with manually created .env file:""
          echo ""    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user""
          echo
        fi
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources=""false""
        if (( mem_available < 4000 )) ; then
          echo
          echo -e ""\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m""
          echo ""At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))""
          echo
          warning_resources=""true""
        fi
        if (( cpus_available < 2 )); then
          echo
          echo -e ""\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m""
          echo ""At least 2 CPUs recommended. You have $${cpus_available}""
          echo
          warning_resources=""true""
        fi
        if (( disk_available < one_meg * 10 )); then
          echo
          echo -e ""\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m""
          echo ""At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))""
          echo
          warning_resources=""true""
        fi
        if [[ $${warning_resources} == ""true"" ]]; then
          echo
          echo -e ""\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m""
          echo ""Please follow the instructions to increase amount of resources available:""
          echo ""   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin""
          echo
        fi
        mkdir -p /sources/logs /sources/dags /sources/plugins
        chown -R ""${AIRFLOW_UID}:0"" /sources/{logs,dags,plugins}
        exec /entrypoint airflow version
    # yamllint enable rule:line-length
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
      _PIP_ADDITIONAL_REQUIREMENTS: ''
    user: ""0:0""
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}:/sources

  airflow-cli:
    <<: *airflow-common
    profiles:
      - debug
    environment:
      <<: *airflow-common-env
      CONNECTION_CHECK_MAX_COUNT: ""0""
    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
    command:
      - bash
      - -c
      - airflow

  # You can enable flower by adding ""--profile flower"" option e.g. docker-compose --profile flower up
  # or by explicitly targeted on the command line e.g. docker-compose up flower.
  # See: https://docs.docker.com/compose/profiles/
  flower:
    <<: *airflow-common
    command: celery flower
    profiles:
      - flower
    ports:
      - ""5555:5555""
    healthcheck:
      test: [""CMD"", ""curl"", ""--fail"", ""http://localhost:5555/""]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

volumes:
  postgres-db-volume:
```


### Operating System

MacOS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

Running with Docker Desktop

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",xvenge00,2024-10-23 15:08:28+00:00,[],2024-10-23 15:10:54+00:00,,https://github.com/apache/airflow/issues/43320,"[('kind:bug', 'This is a clearly a bug'), ('area:logging', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:celery', '')]","[{'comment_id': 2432556502, 'issue_id': 2608962578, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 23, 15, 8, 32, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-23 15:08:32 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2607965457,issue,closed,completed,"Split providers into ""regular"" python sub-projects - each with own pyproject.toml","Currently all providers after #42505 are all in a single ""providers"" project. This has gone through several teething problems (mainly connected to bugs in `uv` and ways how to integrate development environment with IDEs such as Pycharm and VSCode, but seems that those problems are largely solved now and we can possibly move to the next step where each provider will have it's own `pyproject.toml` with its own dependencies and the workspace setting of `uv` will allow us to resolve all those dependencies together and keep our setup with constraints, CI image that is used for CI worfklow and breeze and local development of providers with Breeze.

This has a number of changes to be implemented. Ideally each provider will have it's own complete ""directory"" where things are kept together:

* code
* docs
* tests
* system tests
* dependencies defined in pyproject.toml
* ability to build provider package straight using PEP-compliant frontends without having to copy providers code and generating some of the files (__init__.py, pyproject.toml, READMEs etc.)


The package building is currently done dynamically via `breeze` commands, where code is extracted and pacakge is prepared, also dependency information (including devel dependencies) is kept in `provider.yaml`. Ideally all the information that is needed to generate dependencies and build packages, should be moved to `pyproject.toml` and our breeze/CI automation should retrieve information from there, rather than provider.yaml.

This can be done in stages:

1) we could only move code and tests first - no docs or other files
2) we could do it provider-by-provider if we temporarily implement incremental change in our tooling to support both cases

Or it could be done via automated script that would convert all providers at once - this was earlier POC's implementing this approach (not nearly close to be complete - just testing viability of such approach) that could be used as base for the new solution.:

Script: https://github.com/apache/airflow/pull/28291
Result of runningn the script: https://github.com/apache/airflow/pull/28292



",potiuk,2024-10-23 09:46:26+00:00,[],2024-12-01 12:30:34+00:00,2024-11-30 13:59:01+00:00,https://github.com/apache/airflow/issues/43304,"[('area:providers', ''), ('duplicate', 'Issue that is duplicated'), ('area:dependencies', 'Issues related to dependencies problems')]","[{'comment_id': 2440621949, 'issue_id': 2607965457, 'author': 'Bowrna', 'body': 'Let me check this one @potiuk. i will check the work involved in migrating a single provider by verifying with your PR and then see how far it can be automated.', 'created_at': datetime.datetime(2024, 10, 28, 6, 1, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2441233857, 'issue_id': 2607965457, 'author': 'potiuk', 'body': '> Let me check this one @potiuk. i will check the work involved in migrating a single provider by verifying with your PR and then see how far it can be automated.\r\n\r\nThis is a LOT of work - just be warned. \r\n\r\nLikely many scripts, CI workflows, documentation, contributing docs, breeze will have to be changed. And it can be staged as mentioned above, but this also means that it will have to be made ""more complex"" for a while (to support both approaches and intermixing them) temporarily - until it becomes back simpler (and way simpler in places). \r\n\r\nSo that one is not for the faint of heart :D.', 'created_at': datetime.datetime(2024, 10, 28, 10, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2486513132, 'issue_id': 2607965457, 'author': 'hardeybisey', 'body': '@Bowrna is it okay if I work with you on this?', 'created_at': datetime.datetime(2024, 11, 19, 19, 1, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2487501750, 'issue_id': 2607965457, 'author': 'Bowrna', 'body': 'Sure @hardeybisey let me share my work in PR and we can do this together', 'created_at': datetime.datetime(2024, 11, 20, 5, 42, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489231404, 'issue_id': 2607965457, 'author': 'hardeybisey', 'body': 'Thanks @Bowrna , I will wait for you to share the PR.', 'created_at': datetime.datetime(2024, 11, 20, 17, 58, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508971221, 'issue_id': 2607965457, 'author': 'potiuk', 'body': 'I described it a bit better in #44511, as we have a bit more information and findings after initial playing with `uv workspace` and soem more related tasks. Closing this one as duplicate as #44511 is just more complete and ""fresh"".', 'created_at': datetime.datetime(2024, 11, 30, 13, 59, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508974561, 'issue_id': 2607965457, 'author': 'Bowrna', 'body': 'thanks @potiuk Let me check #44511', 'created_at': datetime.datetime(2024, 11, 30, 14, 11, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2509015880, 'issue_id': 2607965457, 'author': 'potiuk', 'body': '> thanks @potiuk Let me check #44511\r\n\r\nJust to let you know @Bowrna -> that one will be rather complex to implement. I think I will take a stab on it soon as there will be MANY moving pieces and likely some non-obvious choices, so I guess maybe review from your side will be best.', 'created_at': datetime.datetime(2024, 11, 30, 15, 57, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2509746261, 'issue_id': 2607965457, 'author': 'Bowrna', 'body': 'sure @potiuk that would work best.', 'created_at': datetime.datetime(2024, 12, 1, 12, 30, 33, tzinfo=datetime.timezone.utc)}]","Bowrna on (2024-10-28 06:01:31 UTC): Let me check this one @potiuk. i will check the work involved in migrating a single provider by verifying with your PR and then see how far it can be automated.

potiuk (Issue Creator) on (2024-10-28 10:47:00 UTC): This is a LOT of work - just be warned. 

Likely many scripts, CI workflows, documentation, contributing docs, breeze will have to be changed. And it can be staged as mentioned above, but this also means that it will have to be made ""more complex"" for a while (to support both approaches and intermixing them) temporarily - until it becomes back simpler (and way simpler in places). 

So that one is not for the faint of heart :D.

hardeybisey on (2024-11-19 19:01:26 UTC): @Bowrna is it okay if I work with you on this?

Bowrna on (2024-11-20 05:42:33 UTC): Sure @hardeybisey let me share my work in PR and we can do this together

hardeybisey on (2024-11-20 17:58:54 UTC): Thanks @Bowrna , I will wait for you to share the PR.

potiuk (Issue Creator) on (2024-11-30 13:59:01 UTC): I described it a bit better in #44511, as we have a bit more information and findings after initial playing with `uv workspace` and soem more related tasks. Closing this one as duplicate as #44511 is just more complete and ""fresh"".

Bowrna on (2024-11-30 14:11:54 UTC): thanks @potiuk Let me check #44511

potiuk (Issue Creator) on (2024-11-30 15:57:34 UTC): Just to let you know @Bowrna -> that one will be rather complex to implement. I think I will take a stab on it soon as there will be MANY moving pieces and likely some non-obvious choices, so I guess maybe review from your side will be best.

Bowrna on (2024-12-01 12:30:33 UTC): sure @potiuk that would work best.

"
2607880020,issue,closed,completed,An error occurred in webservice， request： /task_stats,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.0

### What happened?

10.1.4.76 - - [23/Oct/2024:16:39:41 +0800] ""POST /dag_stats HTTP/1.1"" 200 4583 ""http://10.1.9.104:10725/home"" ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36 Edg/130.0.0.0""
[2024-10-23 16:40:20 +0800] [3310] [CRITICAL] WORKER TIMEOUT (pid:3316)
[2024-10-23 16:40:20 +0800] [3316] [ERROR] Error handling request /task_stats
Traceback (most recent call last):
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/gunicorn/workers/sync.py"", line 134, in handle
    self.handle_request(listener, req, client, addr)
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/gunicorn/workers/sync.py"", line 177, in handle_request
    respiter = self.wsgi(environ, resp.start_response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/flask/app.py"", line 2552, in __call__
    return self.wsgi_app(environ, start_response)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/flask/app.py"", line 2529, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/flask/app.py"", line 1823, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/flask/app.py"", line 1799, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/airflow/www/auth.py"", line 250, in decorated
    return _has_access(
           ^^^^^^^^^^^^
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/airflow/www/auth.py"", line 163, in _has_access
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/airflow/www/views.py"", line 1338, in task_stats
    qry = session.execute(
          ^^^^^^^^^^^^^^^^
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/sqlalchemy/orm/session.py"", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/sqlalchemy/sql/elements.py"", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 2138, in _handle_dbapi_exception
    util.raise_(exc_info[1], with_traceback=exc_info[2])
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/sqlalchemy/util/compat.py"", line 211, in raise_
    raise exception
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context
    self.dialect.do_execute(
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/sqlalchemy/engine/default.py"", line 736, in do_execute
    cursor.execute(statement, parameters)
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/pymysql/cursors.py"", line 153, in execute
    result = self._query(query)
             ^^^^^^^^^^^^^^^^^^
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/pymysql/cursors.py"", line 322, in _query
    conn.query(q)
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/pymysql/connections.py"", line 563, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/pymysql/connections.py"", line 825, in _read_query_result
    result.read()
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/pymysql/connections.py"", line 1199, in read
    first_packet = self.connection._read_packet()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/pymysql/connections.py"", line 744, in _read_packet
    packet_header = self._read_bytes(4)
                    ^^^^^^^^^^^^^^^^^^^
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/pymysql/connections.py"", line 782, in _read_bytes
    data = self._rfile.read(num_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/tsysmart/local/python/lib/python3.11/socket.py"", line 706, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/ttrs/airflow_dir/venv/lib/python3.11/site-packages/gunicorn/workers/base.py"", line 204, in handle_abort
    sys.exit(1)
SystemExit: 1
10.1.4.76 - - [23/Oct/2024:16:40:20 +0800] ""POST /task_stats HTTP/1.1"" 500 0 ""-"" ""-""
[2024-10-23 16:40:20 +0800] [3316] [INFO] Worker exiting (pid: 3316)
[2024-10-23 16:40:21 +0800] [5542] [INFO] Booting worker with pid: 5542
10.1.4.76 - - [23/Oct/2024:16:40:22 +0800] ""POST /next_run_datasets_summary HTTP/1.1"" 200 2 ""http://10.1.9.104:10725/home"" ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36 Edg/130.0.0.0""
10.1.4.76 - - [23/Oct/2024:16:40:22 +0800] ""POST /last_dagruns HTTP/1.1"" 200 8626 ""http://10.1.9.104:10725/home"" ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36 Edg/130.0.0.0""
10.1.4.76 - - [23/Oct/2024:16:40:32 +0800] ""POST /dag_stats HTTP/1.1"" 200 4583 ""http://10.1.9.104:10725/home"" ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36 Edg/130.0.0.0""


### What you think should happen instead?

_No response_

### How to reproduce

Is it possible that airlow, which runs locally, does not modify the underlying code and does not add components because there are too many DAGs (40 or so?)? Causing this error?

### Operating System

ubuntu

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bangbangDong,2024-10-23 09:19:28+00:00,[],2024-10-24 19:16:59+00:00,2024-10-24 19:16:59+00:00,https://github.com/apache/airflow/issues/43302,"[('kind:bug', 'This is a clearly a bug'), ('area:webserver', 'Webserver related Issues'), ('pending-response', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2432586292, 'issue_id': 2607880020, 'author': 'shahar1', 'body': 'It is not as clear what the problem is and how to reproduce it with a minimal example.\r\nCould you please elaborate?', 'created_at': datetime.datetime(2024, 10, 23, 15, 16, 5, tzinfo=datetime.timezone.utc)}]","shahar1 on (2024-10-23 15:16:05 UTC): It is not as clear what the problem is and how to reproduce it with a minimal example.
Could you please elaborate?

"
2607774638,issue,closed,completed,Sensor returns success state when fatal executor exception,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.3

### What happened?

Execution of a sensor that pokes every 5 minutes failed within a celery worker with the following error message:

```
[2024-10-18 17:11:41,311: INFO/MainProcess] Task airflow.providers.celery.executors.celery_executor_utils.execute_command[73e87a6c-2aba-4982-ba21-80cf5dfb5bcd] received
[2024-10-18 17:11:41,327: INFO/ForkPoolWorker-2] [73e87a6c-2aba-4982-ba21-80cf5dfb5bcd] Executing command in Celery: ['airflow', 'tasks', 'run', 'IncSims_TESTTWIN', 'US.WaitForTap', 'scheduled__2024-10-18T10:15:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/test_dag.py']
[2024-10-18 17:18:41,836: ERROR/ForkPoolWorker-2] [73e87a6c-2aba-4982-ba21-80cf5dfb5bcd] Failed to execute task.
Traceback (most recent call last):  File ""/home/airflow/.local/lib/python3.10/site-packages/celery/app/trace.py"", line 453, in trace_task    R = retval = fun(*args, **kwargs)  File ""/home/airflow/.local/lib/python3.10/site-packages/celery/app/trace.py"", line 736, in __protected_call__    return self.run(*args, **kwargs)  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/celery/executors/celery_executor_utils.py"", line 136, in execute_command    _execute_in_fork(command_to_exec, celery_task_id)  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/celery/executors/celery_executor_utils.py"", line 151, in _execute_in_fork    raise AirflowException(msg)airflow.exceptions.AirflowException: Celery command failed on host: worker.host.com with celery_task_id 73e87a6c-2aba-4982-ba21-80cf5dfb5bcd (PID: 24011, Return Code: 256)
[2024-10-18 17:18:43,196: ERROR/ForkPoolWorker-2] Task airflow.providers.celery.executors.celery_executor_utils.execute_command[73e87a6c-2aba-4982-ba21-80cf5dfb5bcd] raised unexpected: AirflowException('Celery command failed on host: worker.host.com with celery_task_id 73e87a6c-2aba-4982-ba21-80cf5dfb5bcd (PID: 24011, Return Code: 256)')
```

The scheduler errors with

```
[2024-10-18 17:19:03 UTC] {scheduler_job_runner.py:846} ERROR - The executor reported that the task instance <TaskInstance: [someTaskInstance](http://sometaskinstance/).test scheduled__2024-10-18T16:00:00+00:00 [queued]> finished with state success, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally
```

As a result, the task was marked as success and downstream tasks were executed. 

### What you think should happen instead?

The sensor's conditional was not `true` so running downstream tasks incorrectly run.

If the execution of the task failed, I would expect airflow to mark the task instance as failed and not success.

### How to reproduce

Unfortunately, I haven't found an easy way to reproduce the issue

### Operating System

Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Running on kubernetes cluster using celery executors with redis.

Using a postgres database for metadata

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",awdavidson,2024-10-23 08:44:41+00:00,[],2024-10-24 18:14:45+00:00,2024-10-24 18:14:45+00:00,https://github.com/apache/airflow/issues/43300,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:celery', '')]","[{'comment_id': 2431333318, 'issue_id': 2607774638, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 23, 8, 44, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2436049814, 'issue_id': 2607774638, 'author': 'potiuk', 'body': ""I think this is a low-level failure - becuase there was not enough memory for example (because apparently just starting a new process faile. When you run out of memory you cannot do much other than failing things - and likely airflow will not be able to do anything .. because it will have no memory to do anything\r\n\r\nBut it's hard to say - since it is not reproducible, and no obvious solution, we  can at most convert it into discussion and maybe someone will have similar issue and more details/reproducible case."", 'created_at': datetime.datetime(2024, 10, 24, 18, 14, 39, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-23 08:44:45 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-10-24 18:14:39 UTC): I think this is a low-level failure - becuase there was not enough memory for example (because apparently just starting a new process faile. When you run out of memory you cannot do much other than failing things - and likely airflow will not be able to do anything .. because it will have no memory to do anything

But it's hard to say - since it is not reproducible, and no obvious solution, we  can at most convert it into discussion and maybe someone will have similar issue and more details/reproducible case.

"
2607421776,issue,closed,completed,Airflow webserver and worker constantly restart everyday at the same time,"### Official Helm Chart version

1.13.1

### Apache Airflow version

2.8.3

### Kubernetes Version

1.30.4


### Docker Image customizations

No

### What happened

I created an airflow application by helm chart on my k8s cluster, and I found that the worker and webserver will restart everyday at the same time for no reason, there's nothing special in the log, and also not liveness issue. Besides, the helm chart is deployed by argocd, and I found that the argocd sync time is exactly the same as the pod restart time. 

### What you think should happen instead

_No response_

### How to reproduce

It happens everyday at the same time

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",robinisme2,2024-10-23 06:24:01+00:00,[],2024-10-24 19:19:47+00:00,2024-10-24 19:19:47+00:00,https://github.com/apache/airflow/issues/43295,"[('kind:bug', 'This is a clearly a bug'), ('area:helm-chart', 'Airflow Helm Chart'), ('pending-response', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2431020483, 'issue_id': 2607421776, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 23, 6, 24, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2432663620, 'issue_id': 2607421776, 'author': 'nathadfield', 'body': ""@robinisme2 I don't think there's much we can do to help you here currently because this is more than likely a configuration/implementation issue on your side rather than an Airflow bug AFAIK.  If you are able to identify a bug, validate that it still exists in the latest version and then provide a way for anyone to reproduce it then you might get the assistance you are looking for."", 'created_at': datetime.datetime(2024, 10, 23, 15, 37, 31, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-23 06:24:05 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

nathadfield on (2024-10-23 15:37:31 UTC): @robinisme2 I don't think there's much we can do to help you here currently because this is more than likely a configuration/implementation issue on your side rather than an Airflow bug AFAIK.  If you are able to identify a bug, validate that it still exists in the latest version and then provide a way for anyone to reproduce it then you might get the assistance you are looking for.

"
2607021842,issue,closed,completed,Schedule cycle tasks accurate to seconds,"### Description

The crontab expression for the current airflow scheduling task is 5 bits, instead of the usual 6 bits that are accurate to the second

### Use case/motivation

airflow cycle scheduling task crontab does not support seconds, is there any method to support the scheduling task to run the scheduling task at 8:12 minutes 27 seconds every day

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bangbangDong,2024-10-23 02:55:34+00:00,[],2024-10-24 18:01:59+00:00,2024-10-24 18:01:58+00:00,https://github.com/apache/airflow/issues/43293,"[('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2606130027,issue,open,,return in finally can swallow exceptions,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?


In a few places in the code, you have a `return`  statement in a `finally` block which would swallow any in-flight exception:

https://github.com/apache/airflow/blob/be553780e56cf8c34a65aecf2c52a33b82e0e039/providers/src/airflow/providers/amazon/aws/hooks/athena.py#L165

https://github.com/apache/airflow/blob/be553780e56cf8c34a65aecf2c52a33b82e0e039/providers/src/airflow/providers/amazon/aws/hooks/athena.py#L188

https://github.com/apache/airflow/blob/be553780e56cf8c34a65aecf2c52a33b82e0e039/providers/src/airflow/providers/amazon/aws/hooks/athena.py#L292

https://github.com/apache/airflow/blob/be553780e56cf8c34a65aecf2c52a33b82e0e039/providers/src/airflow/providers/amazon/aws/transfers/s3_to_dynamodb.py#L243

This means that if an unhandled exception (like a `BaseException` such as `KeyboardInterrupt`) is raised from the `try` body, or any exception is raised from an `except`: clause, it will not propagate on as expected.

See also https://docs.python.org/3/tutorial/errors.html#defining-clean-up-actions.

### What you think should happen instead?

_No response_

### How to reproduce

This is from analysis of the source code.

### Operating System

All

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",iritkatriel,2024-10-22 18:03:21+00:00,['yangyulely'],2024-10-29 07:51:29+00:00,,https://github.com/apache/airflow/issues/43274,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('good first issue', ''), ('area:core', '')]","[{'comment_id': 2429920962, 'issue_id': 2606130027, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 22, 18, 3, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2435992168, 'issue_id': 2606130027, 'author': 'potiuk', 'body': 'Marked it as good first issue. But It would be great - since you know already what to do - could fix it. Otherwise it will wait for someone to pick it up.', 'created_at': datetime.datetime(2024, 10, 24, 17, 52, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2437558066, 'issue_id': 2606130027, 'author': 'yangyulely', 'body': ""@potiuk , I would like work on this, if that's okay."", 'created_at': datetime.datetime(2024, 10, 25, 11, 37, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2438640190, 'issue_id': 2606130027, 'author': 'potiuk', 'body': 'Feel free!', 'created_at': datetime.datetime(2024, 10, 25, 19, 25, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2438643220, 'issue_id': 2606130027, 'author': 'Rustix69', 'body': '@potiuk Can I also work on this issue ? I had found the Solution.', 'created_at': datetime.datetime(2024, 10, 25, 19, 27, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2438739493, 'issue_id': 2606130027, 'author': 'potiuk', 'body': 'Sure. Talk to @yangyulely and try to coordinate together.', 'created_at': datetime.datetime(2024, 10, 25, 20, 30, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439571931, 'issue_id': 2606130027, 'author': 'Rustix69', 'body': '@potiuk I had fixed the Bugs in the try-except blocks such that it will raise the errors and not swallow the exceptions.', 'created_at': datetime.datetime(2024, 10, 26, 12, 49, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439754875, 'issue_id': 2606130027, 'author': 'Rustix69', 'body': '@yangyulely Can we start Contribution to this bug ?', 'created_at': datetime.datetime(2024, 10, 26, 22, 28, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2440408795, 'issue_id': 2606130027, 'author': 'yangyulely', 'body': 'I will try to create a PR this week', 'created_at': datetime.datetime(2024, 10, 28, 2, 28, 34, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-22 18:03:24 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-10-24 17:52:28 UTC): Marked it as good first issue. But It would be great - since you know already what to do - could fix it. Otherwise it will wait for someone to pick it up.

yangyulely (Assginee) on (2024-10-25 11:37:19 UTC): @potiuk , I would like work on this, if that's okay.

potiuk on (2024-10-25 19:25:46 UTC): Feel free!

Rustix69 on (2024-10-25 19:27:38 UTC): @potiuk Can I also work on this issue ? I had found the Solution.

potiuk on (2024-10-25 20:30:27 UTC): Sure. Talk to @yangyulely and try to coordinate together.

Rustix69 on (2024-10-26 12:49:17 UTC): @potiuk I had fixed the Bugs in the try-except blocks such that it will raise the errors and not swallow the exceptions.

Rustix69 on (2024-10-26 22:28:09 UTC): @yangyulely Can we start Contribution to this bug ?

yangyulely (Assginee) on (2024-10-28 02:28:34 UTC): I will try to create a PR this week

"
2605793305,issue,closed,completed,500 on any admin pages,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Getting 500 on any of the admin pages: pools, connections, users, etc.

### What you think should happen instead?

No 500 thrown

### How to reproduce

Not sure if it's specific to our authentication configuration as we just switched to authentication through Google OAuth, but here's the trace from the webserver(we also upgraded from 2.9.3 to 2.10.2): 
```

[2024-10-22T15:15:37.443+0000] {app.py:1744} ERROR - Exception on /users/list/ [GET]
-- | -- | --
22 October 2024 at 11:15 (UTC-4:00) | Traceback (most recent call last):
22 October 2024 at 11:15 (UTC-4:00) | File ""/usr/local/lib/python3.10/site-packages/flask/app.py"", line 2529, in wsgi_app
22 October 2024 at 11:15 (UTC-4:00) | response = self.full_dispatch_request()
22 October 2024 at 11:15 (UTC-4:00) | File ""/usr/local/lib/python3.10/site-packages/flask/app.py"", line 1825, in full_dispatch_request
22 October 2024 at 11:15 (UTC-4:00) | rv = self.handle_user_exception(e)
22 October 2024 at 11:15 (UTC-4:00) | File ""/usr/local/lib/python3.10/site-packages/flask/app.py"", line 1823, in full_dispatch_request
22 October 2024 at 11:15 (UTC-4:00) | rv = self.dispatch_request()
22 October 2024 at 11:15 (UTC-4:00) | File ""/usr/local/lib/python3.10/site-packages/flask/app.py"", line 1799, in dispatch_request
22 October 2024 at 11:15 (UTC-4:00) | return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
22 October 2024 at 11:15 (UTC-4:00) | File ""/usr/local/lib/python3.10/site-packages/flask_appbuilder/security/decorators.py"", line 137, in wraps
22 October 2024 at 11:15 (UTC-4:00) | return f(self, *args, **kwargs)
22 October 2024 at 11:15 (UTC-4:00) | File ""/usr/local/lib/python3.10/site-packages/flask_appbuilder/views.py"", line 550, in list
22 October 2024 at 11:15 (UTC-4:00) | widgets = self._list()
22 October 2024 at 11:15 (UTC-4:00) | File ""/usr/local/lib/python3.10/site-packages/flask_appbuilder/baseviews.py"", line 1186, in _list
22 October 2024 at 11:15 (UTC-4:00) | form = self.search_form.refresh()
22 October 2024 at 11:15 (UTC-4:00) | File ""/usr/local/lib/python3.10/site-packages/flask_appbuilder/forms.py"", line 327, in refresh
22 October 2024 at 11:15 (UTC-4:00) | form = self(obj=obj)
22 October 2024 at 11:15 (UTC-4:00) | File ""/usr/local/lib/python3.10/site-packages/wtforms/form.py"", line 209, in __call__
22 October 2024 at 11:15 (UTC-4:00) | return type.__call__(cls, *args, **kwargs)
22 October 2024 at 11:15 (UTC-4:00) | File ""/usr/local/lib/python3.10/site-packages/flask_wtf/form.py"", line 73, in __init__
22 October 2024 at 11:15 (UTC-4:00) | super().__init__(formdata=formdata, **kwargs)
22 October 2024 at 11:15 (UTC-4:00) | File ""/usr/local/lib/python3.10/site-packages/wtforms/form.py"", line 281, in __init__
22 October 2024 at 11:15 (UTC-4:00) | super().__init__(self._unbound_fields, meta=meta_obj, prefix=prefix)
22 October 2024 at 11:15 (UTC-4:00) | File ""/usr/local/lib/python3.10/site-packages/wtforms/form.py"", line 49, in __init__
22 October 2024 at 11:15 (UTC-4:00) | field = meta.bind_field(self, unbound_field, options)
22 October 2024 at 11:15 (UTC-4:00) | File ""/usr/local/lib/python3.10/site-packages/wtforms/meta.py"", line 28, in bind_field
22 October 2024 at 11:15 (UTC-4:00) | return unbound_field.bind(form=form, **options)
22 October 2024 at 11:15 (UTC-4:00) | File ""/usr/local/lib/python3.10/site-packages/wtforms/fields/core.py"", line 387, in bind
22 October 2024 at 11:15 (UTC-4:00) | return self.field_class(*self.args, **kw)
22 October 2024 at 11:15 (UTC-4:00) | File ""/usr/local/lib/python3.10/site-packages/wtforms/fields/core.py"", line 133, in __init__
22 October 2024 at 11:15 (UTC-4:00) | for k, v in flags.items():
22 October 2024 at 11:15 (UTC-4:00) | AttributeError: 'tuple' object has no attribute 'items'

```

### Operating System

Amazon Linux release 2023.6.20241010 (Amazon Linux)

### Versions of Apache Airflow Providers

```
apache-airflow-providers-amazon==8.28.0
apache-airflow-providers-asana==2.6.0
apache-airflow-providers-celery==3.8.3
apache-airflow-providers-common-compat==1.2.1
apache-airflow-providers-common-io==1.4.2
apache-airflow-providers-common-sql==1.18.0
apache-airflow-providers-databricks==6.9.0
apache-airflow-providers-datadog==3.7.1
apache-airflow-providers-elasticsearch==4.4.0
apache-airflow-providers-fab==1.4.1
apache-airflow-providers-ftp==3.11.1
apache-airflow-providers-http==4.13.0
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-postgres==5.12.0
apache-airflow-providers-slack==8.9.0
apache-airflow-providers-smtp==1.8.0
apache-airflow-providers-sqlite==3.9.0
```

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",eliskovets,2024-10-22 15:29:03+00:00,[],2024-10-23 14:35:50+00:00,2024-10-22 22:27:13+00:00,https://github.com/apache/airflow/issues/43270,"[('kind:bug', 'This is a clearly a bug'), ('area:webserver', 'Webserver related Issues'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2430432006, 'issue_id': 2605793305, 'author': 'eliskovets', 'body': ""Closing, looks like it's caused by wtforms=3.2.1."", 'created_at': datetime.datetime(2024, 10, 22, 22, 27, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2431561116, 'issue_id': 2605793305, 'author': 'CorentinLimier', 'body': '@eliskovets we have the exact same issue after having deployed to airflow 2.10.2 \r\n\r\nGetting 500 on any of the admin pages: pools, connections, users, etc.\r\n\r\n```python\r\n  File ""/home/airflow/.local/lib/python3.11/site-packages/wtforms/fields/core.py"", line 133, in __init__\r\n    for k, v in flags.items():\r\n                ^^^^^^^^^^^\r\nAttributeError: \'tuple\' object has no attribute \'items\'\r\n```\r\n\r\nBefore we were with airflow==2.10.1 and already with wtforms=3.2.1 and we could use the admin pages correctly. \r\n\r\nDid you fix this issue ? By using another wtforms version maybe ?', 'created_at': datetime.datetime(2024, 10, 23, 9, 51, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2431696817, 'issue_id': 2605793305, 'author': 'eliskovets', 'body': ""Yes, I've switched back to wtforms 3.1.2 and it fixed the issue. \nWe have our own docker image and I noticed that we didn't use --constraint when installing requirements with pip. After it's been added, the issue with admin pages is fixed."", 'created_at': datetime.datetime(2024, 10, 23, 10, 40, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2432225062, 'issue_id': 2605793305, 'author': 'CorentinLimier', 'body': 'Indeed @eliskovets ! Thank you very much, it solved our issue as well.\r\n\r\nFor those having same issue, see [here](https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html#installation-tools) for more details on --constraint parameter and an example of constraint file [here](https://raw.githubusercontent.com/apache/airflow/constraints-2.10.2/constraints-3.11.txt) that sets the version of WTForms to 3.1.2', 'created_at': datetime.datetime(2024, 10, 23, 13, 37, 48, tzinfo=datetime.timezone.utc)}]","eliskovets (Issue Creator) on (2024-10-22 22:27:13 UTC): Closing, looks like it's caused by wtforms=3.2.1.

CorentinLimier on (2024-10-23 09:51:52 UTC): @eliskovets we have the exact same issue after having deployed to airflow 2.10.2 

Getting 500 on any of the admin pages: pools, connections, users, etc.

```python
  File ""/home/airflow/.local/lib/python3.11/site-packages/wtforms/fields/core.py"", line 133, in __init__
    for k, v in flags.items():
                ^^^^^^^^^^^
AttributeError: 'tuple' object has no attribute 'items'
```

Before we were with airflow==2.10.1 and already with wtforms=3.2.1 and we could use the admin pages correctly. 

Did you fix this issue ? By using another wtforms version maybe ?

eliskovets (Issue Creator) on (2024-10-23 10:40:34 UTC): Yes, I've switched back to wtforms 3.1.2 and it fixed the issue. 
We have our own docker image and I noticed that we didn't use --constraint when installing requirements with pip. After it's been added, the issue with admin pages is fixed.

CorentinLimier on (2024-10-23 13:37:48 UTC): Indeed @eliskovets ! Thank you very much, it solved our issue as well.

For those having same issue, see [here](https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html#installation-tools) for more details on --constraint parameter and an example of constraint file [here](https://raw.githubusercontent.com/apache/airflow/constraints-2.10.2/constraints-3.11.txt) that sets the version of WTForms to 3.1.2

"
2605749332,issue,closed,completed,Databricks Provider fails in deferred mode due to `AttributeError: 'ClientResponse' object has no attribute 'status_code'`,"### Apache Airflow Provider(s)

databricks

### Versions of Apache Airflow Providers

6.11.0

### Apache Airflow version

2.9.3

### Operating System

Kubernetes

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Repackaged of airflow image with Oracle driver

### What happened

When using the DatabricksRunNowOperator with `deferrable=True` the databricks job gets started but the Task fails at the first poll to check the job status with 

```
[2024-10-22, 16:40:13 CEST] {baseoperator.py:1695} ERROR - Trigger failed:
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/triggerer_job_runner.py"", line 529, in cleanup_finished_triggers
    result = details[""task""].result()
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/triggerer_job_runner.py"", line 601, in run_trigger
    async for event in trigger.run():
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/databricks/triggers/databricks.py"", line 88, in run
    run_state = await self.hook.a_get_run_state(self.run_id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/databricks/hooks/databricks.py"", line 450, in a_get_run_state
    response = await self._a_do_api_call(GET_RUN_ENDPOINT, json)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/databricks/hooks/databricks_base.py"", line 625, in _a_do_api_call
    async for attempt in self._a_get_retry_object():
  File ""/home/airflow/.local/lib/python3.12/site-packages/tenacity/asyncio/__init__.py"", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/tenacity/asyncio/__init__.py"", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/tenacity/_utils.py"", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/tenacity/__init__.py"", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/concurrent/futures/_base.py"", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/concurrent/futures/_base.py"", line 401, in __get_result
    raise self._exception
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/databricks/hooks/databricks_base.py"", line 641, in _a_do_api_call
    self.log.debug(""Response Status Code: %s"", response.status_code)
                                               ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'ClientResponse' object has no attribute 'status_code'
```

This was working with version `6.3.0`
It works fine with `deferred=False`

### What you think should happen instead

The task should continue polling until the databricks job has finished.


### How to reproduce

Use a simple DatabricksRunNowOperator with the mentionned version above. Databricks runs on Azure.

```python
    job_run = DatabricksRunNowOperator(task_id=""databricks_job"", job_name=""TestJob"",
                                       databricks_conn_id=""databricks"", deferrable=True)
```

### Anything else

Happens every time

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",lucafurrer,2024-10-22 15:10:48+00:00,[],2024-10-25 08:04:55+00:00,2024-10-25 08:04:55+00:00,https://github.com/apache/airflow/issues/43269,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('provider:databricks', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2429555907, 'issue_id': 2605749332, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 22, 15, 10, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2429567605, 'issue_id': 2605749332, 'author': 'lucafurrer', 'body': 'From what I got the problem has been introduced by https://github.com/apache/airflow/commit/9b90d2f216adf6aea1c5a53100e24c80ddb6efb7\r\n\r\nShort google showed that the returned object does not have a property `status_code` but only `status`\r\n[ClientResponse](https://docs.aiohttp.org/en/stable/client_reference.html#aiohttp.ClientResponse.status)', 'created_at': datetime.datetime(2024, 10, 22, 15, 15, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2435004210, 'issue_id': 2605749332, 'author': 'shubham-kanungo-de', 'body': '@lucafurrer i was also facing same , solved using commenting log lines.\r\n#self.log.debug(""Response Status Code: %s"", response.status_code)\r\n#self.log.debug(""Response text: %s"", response.text)', 'created_at': datetime.datetime(2024, 10, 24, 11, 21, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2435007596, 'issue_id': 2605749332, 'author': 'lucafurrer', 'body': '@shubham-kanungo-de we rolled back to the previous version but would like to have it fixed later on.', 'created_at': datetime.datetime(2024, 10, 24, 11, 23, 33, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-22 15:10:52 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

lucafurrer (Issue Creator) on (2024-10-22 15:15:36 UTC): From what I got the problem has been introduced by https://github.com/apache/airflow/commit/9b90d2f216adf6aea1c5a53100e24c80ddb6efb7

Short google showed that the returned object does not have a property `status_code` but only `status`
[ClientResponse](https://docs.aiohttp.org/en/stable/client_reference.html#aiohttp.ClientResponse.status)

shubham-kanungo-de on (2024-10-24 11:21:53 UTC): @lucafurrer i was also facing same , solved using commenting log lines.
#self.log.debug(""Response Status Code: %s"", response.status_code)
#self.log.debug(""Response text: %s"", response.text)

lucafurrer (Issue Creator) on (2024-10-24 11:23:33 UTC): @shubham-kanungo-de we rolled back to the previous version but would like to have it fixed later on.

"
2605526688,issue,closed,completed,"Remove ""pull_request_target"" workflows and replace them with storing images as artifacts","I've learned that as of artifact@v4 APIs for GitHub Actions, it should be possible to get rid of the GitHub Registry use as CI cache.

> [!NOTE]
> I think that change might be way smaller. We should be able to continue using Github Registry for both - latest image and image cache, and only change CI to pull artifacts from latest ""canary"" build and build image cache from that.

In our CI/CD pipelines we are using ""pull_request_target"" workflow in order to be able to publish images to GitHub registry from Pull Requests coming from forks. This is done in order to speed up the workflows - our images take a lot of time to build and by reusing the cache, we are saving a lot of build time.

However after following a discussion in Apache Software Foundation `#builds` channel https://the-asf.slack.com/archives/CF6PY5M4N/p1729595424091089 - with Pulsar and Netbeans CI/CD maintainers, it looks like we have a viable alternative - that will remove the need of using GH Registry for PRs - instead we could use artifacts.

Our images are quite big (2GB) but Pulsar team successfully uses artifacts to store 1.7GB ones, so this should be quite possible for us to use the artifacts instead of registry.

We dicussed also - with Pulsar and Netbeans CI maintainers about caching strategy we use and it seems entirely doable to switch to artifacts and get the same effect, without the need of having ""pull_request_target"" workflow. That would be a major win for security of our workflows to switch to it.

This is especially atractive because the new artifact API v4 introduced in February 2024 is very fast, implements automated chunking of binaries uploaded/downloaded and parallell uploads/downloads - which according to Pulsar/Netbeans team had a huge impact on upload/download times for the images - see https://github.blog/news-insights/product-news/get-started-with-v4-of-github-actions-artifacts/ - there are a lot of details in the post, but the way uploading artifacts works with v4 is that you directly interact with Azure Blob Storage, the files are chunked etc. The examples they show in the blog post is speeding up the upload of 1GB file from 2 m to 3 s.

Also v4 when `run-id` and other parameters have been added - making it suitable to implement similar caching solution we have, where artifacts from ""canary"" build could be used by all PRs as source of their image cache.

Seems that artifacts have no individual size limits and they also do not have ""total limits"" with the ASF ""Enterprise Account"" level - there are things 

The way it **should** work:

All workflows - PRs from apache and fork work in the same way:

1) Pull latest image (stored as artifact) from latest ""main"" successful build (we can use GH API to find the run-id to pull it from and the download-artifact@v4 action already supports pulling artifacts from other workflows and from other runs.

2) The PRs first unpack and push the ""main"" image to be present in local docker registry - no need to have GH Registry ""PUSH"" access.

3) The PRs then build the image locally not using `--cache-from` but to use such local registry cache. Achieving basically the same (or even bettter) caching speedups comparing to what we have now.

4) The successful ""canary"" build artifact could be uploaded at ""finalize"" time to become the new source of cache for all the PRs.

5) Breeze locak CI  image could also be modified to pull latest image when rebuilding the image forcefully - otherwise rely on locally built images. 

6) The `--image-tag` option of the local breeze (which pulls the image from a specific workflow run) could be easily adapted to be changed to `--pr PR#` - allowing to replicate a failure from specific PR locally without looking up the image.

7) The artifacts can have arbitrary set expiry time, so they could be available for a few days after the PR or ""canary"" build completed.

The benefits:

* we get rid of ""Build image"" ""pull_request_target"" workflow - that would be a huge security improvement
* we get rid of the need of creating new images every time we create a new ""release"" branch.
* we get rid of GitHub Registry
* all the runs - commiter and non-commiter, from fork or apache would be identical, making it much easier to test changes in workflows
* looking at the upload/download stats for artifacts, it is likely that switching to artifacts might get some speedup comparing to using the registry

We can use https://github.com/assignUser/stash from Jacob Wujciak (ASF member) to make things easier. This will soon be available as Apache-wide action: https://github.com/apache/infrastructure-actions/pull/32

",potiuk,2024-10-22 13:50:43+00:00,['potiuk'],2024-12-29 22:15:43+00:00,2024-12-29 21:58:28+00:00,https://github.com/apache/airflow/issues/43268,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:CI', ""Airflow's tests and continious integration"")]","[{'comment_id': 2532789459, 'issue_id': 2605526688, 'author': 'gopidesupavan', 'body': 'Going to take a stab on this , when ever i open CI/CD project `pull_request_target` reminds  me its dangerous .... :)', 'created_at': datetime.datetime(2024, 12, 10, 20, 20, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2543101546, 'issue_id': 2605526688, 'author': 'gopidesupavan', 'body': '@potiuk\r\n\r\nBefore actually dive into the code, i just tried the above flow with simple docker image. sorry if i am confusing too much :) \r\n\r\nThe first flow option 1 is same as above steps\r\n\r\n#### Option1:\r\n    Build the docker image and pushed to artifactory\r\n    Download the image from artifactory and unpack and push to local registry\r\n    Build from the local registry.\r\n  \r\n  Looks like this is not using the existing image from the local registry. instead its building again.\r\n  \r\n  RUN: https://github.com/gopidesupavan/prt-removal/actions/runs/12329425958/job/34413743618#step:9:208\r\n  Workflow: https://github.com/gopidesupavan/prt-removal/blob/main/.github/workflows/build-push-reuse.yml\r\n  \r\n  After searching further, it seems docker buildx not loading image that exported from sources(tar file). https://github.com/docker/buildx/issues/847\r\n  \r\n  And as per docker buildx it seems the way is to use named context https://docs.docker.com/build/ci/github-actions/named-contexts/\r\n\r\n#### Option2:\r\n     Build the docker image with inline cache and pushed to artifactory\r\n     Download the image from artifactory and unpack and push to local registry\r\n     Build from the local registry with inline cache.\r\n\r\n  It works well with the inline cache. It uses the existing image from the local registry and not building any layers again. as \r\n  there are no changes in the docker file.\r\n  RUN: https://github.com/gopidesupavan/prt-removal/actions/runs/12329496451/job/34413888753#step:9:193\r\n  Workflow: https://github.com/gopidesupavan/prt-removal/blob/main/.github/workflows/with-inline-cache.yml\r\n\r\n#### Option3:\r\n    Build the docker image with local cache and pushed cache dir to artifactory\r\n    Download the cache from artifactory to tmp dir\r\n    Build the image with local cache referring to the local cache dir. (in pr flow)\r\n  \r\n  It also works well with the local cache type. all the layers from the local cache dir and not building any layers again.\r\n  RUN: https://github.com/gopidesupavan/prt-removal/actions/runs/12329584735/job/34414063998#step:7:193\r\n  Workflow: https://github.com/gopidesupavan/prt-removal/blob/main/.github/workflows/with-local-cache.yml\r\n\r\n\r\nConsider that in the above every option first two steps executed in the schedule run. push the schedule run image and use it pr flows.\r\n\r\nSo basically i tried to mimic the scenario. in each workflow that i used above first job is in the context of schedule run and next job is pr flow.\r\n\r\nUnable to fully evaluate the side effects of the inline cache. IMHO, the last option seems similar to what we aim to achieve, but instead of pushing the image, we could push all the local caches for the different images (3.9, 3.10, 3.11, 3.12) built during the scheduled run. These cache artifacts (using the run-id as you suggested) could then be downloaded and utilized in the PR workflow.', 'created_at': datetime.datetime(2024, 12, 14, 13, 2, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2543111233, 'issue_id': 2605526688, 'author': 'gopidesupavan', 'body': 'My hunch is that the builder context is being lost when the job exits. When we download the artifact and attempt to use it, it likely creates a completely new builder context. As a result, it tries to rebuild everything from scratch instead of utilizing the downloaded image from the artifact.', 'created_at': datetime.datetime(2024, 12, 14, 13, 31, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564241318, 'issue_id': 2605526688, 'author': 'potiuk', 'body': '> My hunch is that the builder context is being lost when the job exits. When we download the artifact and attempt to use it, it likely creates a completely new builder context. As a result, it tries to rebuild everything from scratch instead of utilizing the downloaded image from the artifact.\r\n\r\nI have another idea to check - exploring it while working on #42999 and #44511... Stay tuned :)', 'created_at': datetime.datetime(2024, 12, 28, 7, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564850409, 'issue_id': 2605526688, 'author': 'gopidesupavan', 'body': 'Woohoooo nice :)', 'created_at': datetime.datetime(2024, 12, 29, 22, 8, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564851759, 'issue_id': 2605526688, 'author': 'potiuk', 'body': ""Indeed :).. Let's see how many new problems it brings :)"", 'created_at': datetime.datetime(2024, 12, 29, 22, 15, 42, tzinfo=datetime.timezone.utc)}]","gopidesupavan on (2024-12-10 20:20:57 UTC): Going to take a stab on this , when ever i open CI/CD project `pull_request_target` reminds  me its dangerous .... :)

gopidesupavan on (2024-12-14 13:02:43 UTC): @potiuk

Before actually dive into the code, i just tried the above flow with simple docker image. sorry if i am confusing too much :) 

The first flow option 1 is same as above steps

#### Option1:
    Build the docker image and pushed to artifactory
    Download the image from artifactory and unpack and push to local registry
    Build from the local registry.
  
  Looks like this is not using the existing image from the local registry. instead its building again.
  
  RUN: https://github.com/gopidesupavan/prt-removal/actions/runs/12329425958/job/34413743618#step:9:208
  Workflow: https://github.com/gopidesupavan/prt-removal/blob/main/.github/workflows/build-push-reuse.yml
  
  After searching further, it seems docker buildx not loading image that exported from sources(tar file). https://github.com/docker/buildx/issues/847
  
  And as per docker buildx it seems the way is to use named context https://docs.docker.com/build/ci/github-actions/named-contexts/

#### Option2:
     Build the docker image with inline cache and pushed to artifactory
     Download the image from artifactory and unpack and push to local registry
     Build from the local registry with inline cache.

  It works well with the inline cache. It uses the existing image from the local registry and not building any layers again. as 
  there are no changes in the docker file.
  RUN: https://github.com/gopidesupavan/prt-removal/actions/runs/12329496451/job/34413888753#step:9:193
  Workflow: https://github.com/gopidesupavan/prt-removal/blob/main/.github/workflows/with-inline-cache.yml

#### Option3:
    Build the docker image with local cache and pushed cache dir to artifactory
    Download the cache from artifactory to tmp dir
    Build the image with local cache referring to the local cache dir. (in pr flow)
  
  It also works well with the local cache type. all the layers from the local cache dir and not building any layers again.
  RUN: https://github.com/gopidesupavan/prt-removal/actions/runs/12329584735/job/34414063998#step:7:193
  Workflow: https://github.com/gopidesupavan/prt-removal/blob/main/.github/workflows/with-local-cache.yml


Consider that in the above every option first two steps executed in the schedule run. push the schedule run image and use it pr flows.

So basically i tried to mimic the scenario. in each workflow that i used above first job is in the context of schedule run and next job is pr flow.

Unable to fully evaluate the side effects of the inline cache. IMHO, the last option seems similar to what we aim to achieve, but instead of pushing the image, we could push all the local caches for the different images (3.9, 3.10, 3.11, 3.12) built during the scheduled run. These cache artifacts (using the run-id as you suggested) could then be downloaded and utilized in the PR workflow.

gopidesupavan on (2024-12-14 13:31:17 UTC): My hunch is that the builder context is being lost when the job exits. When we download the artifact and attempt to use it, it likely creates a completely new builder context. As a result, it tries to rebuild everything from scratch instead of utilizing the downloaded image from the artifact.

potiuk (Issue Creator) on (2024-12-28 07:15:00 UTC): I have another idea to check - exploring it while working on #42999 and #44511... Stay tuned :)

gopidesupavan on (2024-12-29 22:08:11 UTC): Woohoooo nice :)

potiuk (Issue Creator) on (2024-12-29 22:15:42 UTC): Indeed :).. Let's see how many new problems it brings :)

"
2605476452,issue,closed,completed,Airflow 2.9.3: Unable to parse the connection string,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.3

### What happened?

We are experiencing Azure Blob connection string parse error on Airflow 2.9.3 (self hosted, deployed through official helm chart). The same code is working fine on Auzre Managed Airflow 2.6.3. 
Both environments are using Python 3.8

`Broken DAG: [DAG Path]
Traceback (most recent call last):
File ""/home/airflow/.local/lib/python3.8/site-packages/azure/storage/blob/_blob_service_client.py"", line 195, in from_connection_string
account_url,secondary,credential=parse_connection_str(conn_str,credential,'blob')
File ""/home/airflow/.local/lib/python3.8/site-packages/azure/storage/blob/_shared/base_client.py"", line 387, in parse_connection_str
conn_str=conn_str.rstrip("","")
AttributeError: 'NoneType' object has no attribute 'rstrip'`

The code to get connection's details is as follow:
**We read a Variable. That variable stores the connection details (conn_id). This connection id exists in the Connections list.**

The Test connection was successfull from web ui.

**My query is**: Are there any changes from airflow 2.6.3 to airflow 2.9.3 over connection details (conn string, url format etc)

### What you think should happen instead?

The connection should work from the DAG as Test connection was successful.
 

### How to reproduce

Create a Azure Blog Storage connection type with all details, Test the connection from web UI.

Try to load connection from source code.


### Operating System

Debian GNU Linux 12

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon          8.25.0
apache-airflow-providers-celery          3.7.2
apache-airflow-providers-cncf-kubernetes 8.3.3
apache-airflow-providers-common-io       1.3.2
apache-airflow-providers-common-sql      1.14.2
apache-airflow-providers-docker          3.12.2
apache-airflow-providers-elasticsearch   5.4.1
apache-airflow-providers-fab             1.2.2
apache-airflow-providers-ftp             3.10.0
apache-airflow-providers-google          10.21.0
apache-airflow-providers-grpc            3.5.2
apache-airflow-providers-hashicorp       3.7.1
apache-airflow-providers-http            4.12.0
apache-airflow-providers-imap            3.6.1
apache-airflow-providers-microsoft-azure 10.0.0
apache-airflow-providers-microsoft-winrm 3.4.0
apache-airflow-providers-mysql           5.6.2
apache-airflow-providers-odbc            4.6.2
apache-airflow-providers-openlineage     1.9.1
apache-airflow-providers-postgres        5.11.2
apache-airflow-providers-redis           3.7.1
apache-airflow-providers-sendgrid        3.5.1
apache-airflow-providers-sftp            4.10.2
apache-airflow-providers-slack           8.7.1
apache-airflow-providers-smtp            1.7.1
apache-airflow-providers-snowflake       4.1.0
apache-airflow-providers-sqlite          3.8.1
apache-airflow-providers-ssh             3.11.2


### Deployment

Official Apache Airflow Helm Chart

### Deployment details

We are using AKS (Azure Kubernetes) 1.29 

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",nitinmahawadiwar,2024-10-22 13:33:11+00:00,[],2024-10-24 17:56:44+00:00,2024-10-24 17:56:44+00:00,https://github.com/apache/airflow/issues/43267,"[('kind:bug', 'This is a clearly a bug'), ('provider:microsoft-azure', 'Azure-related issues'), ('area:providers', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2429303229, 'issue_id': 2605476452, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 22, 13, 33, 15, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-22 13:33:15 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2605251209,issue,open,,Not Showing next run in Airflow UI for a DAG scheduled using custom TimeTable,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.6.2

### What happened?

I have created a DAG which I want to schedule on 18th every month if 18th falls on [Mon, Tue, Wed, Thu] else schedule it on next Monday. 
This is how I have created my MonthlyProdTimetable.

`from datetime import timedelta
import datetime as dt
from typing import Optional
import pendulum
from pendulum import Date, DateTime, Time, timezone
from airflow.utils.timezone import convert_to_utc, make_aware, make_naive

from airflow.plugins_manager import AirflowPlugin
from airflow.timetables.base import DagRunInfo, DataInterval, TimeRestriction, Timetable

class MonthlyProdTimetable(Timetable):
    
    def infer_manual_data_interval(self, run_after: DateTime)-> DataInterval:
        return DataInterval.exact(run_after)
    
    def _get_next(self, current: DateTime) -> DateTime:
        print(f""current == {current}"")
        eighteenth = pendulum.datetime(current.year, current.month, 18, 10,0,0, tz='America/Chicago')
        if eighteenth.day_of_week in range(0,4):
            cur_month_sched_dt = eighteenth
        else:
            cur_month_sched_dt = eighteenth.next(pendulum.MONDAY).at(10,0,0).in_timezone('America/Chicago')

        if current <= cur_month_sched_dt:
            return cur_month_sched_dt
        else:
            next_month_eighteenth = current.add(months=1).date(18).at(10,0,0).in_timezone('America/Chicago')
            if next_month_eighteenth.day_of_week in range(0,4):
                cur_month_sched_dt = next_month_eighteenth
            else:
                cur_month_sched_dt = next_month_eighteenth.next(pendulum.MONDAY).at(10,0,0).in_timezone('America/Chicago')
            return cur_month_sched_dt
        

    def next_dagrun_info(
            self,
            *,
            last_automated_data_interval:Optional[DataInterval],
            restriction: TimeRestriction,
    ) -> Optional[DagRunInfo]:
        print(""TimeTable has been called!"")
        if not restriction.catchup:
            start_time_candidates = [self._get_next(pendulum.now(""America/Chicago""))]
            if last_automated_data_interval is not None:
                start_time_candidates.append(self._get_next(last_automated_data_interval.end))
            if restriction.earliest is not None:
                start_time_candidates.append(self._get_next(restriction.earliest))
            next_start_time = max(start_time_candidates)
            print(f""This is next_start_time == {next_start_time}"")
        if restriction.latest is not None and restriction.latest < next_start_time:
            return None
        return DagRunInfo.exact(next_start_time)


class MonthlyTimeTablePlugin(AirflowPlugin):
    name = ""monthly_timetable_plugin""
    timetables = [MonthlyProdTimetable]`

In the Airflow UI I cannot see what is the Next Run for this DAG. 
![image](https://github.com/user-attachments/assets/f1876a16-d0ca-4c5b-9f7a-6197a7127596)

What am I missing here?

### What you think should happen instead?

Airflow UI should show Next Run for this DAG

### How to reproduce

Will update 

### Operating System

On Docker

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vaibhavg-DA,2024-10-22 12:00:55+00:00,[],2024-10-22 12:03:08+00:00,,https://github.com/apache/airflow/issues/43262,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2429089697, 'issue_id': 2605251209, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 22, 12, 0, 59, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-22 12:00:59 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2604914151,issue,closed,completed,"""No module named"" error when using AwaitMessageSensor in Cloud Composer","### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

2.9.3

### Apache Airflow version

Apache Airflow Providers Google 10.12.0

### Operating System

Ubuntu

### Deployment

Google Cloud Composer

### Deployment details

_No response_

### What happened

We’re trying to use AwaitMessageSensor from Airflow provider to get messages from Kafka. AwaitMessageSensor has a parameter “apply_function”. This parameter is mandatory, and describes the function is used to process Kafka messages. The value of this parameter should be a dotted string and is the path to the function.

No matter which path we try, the sensor never found the function.

PYTHONPATH has the following paths:
/opt/python3.11/bin
/opt/python3.11/lib/python311.zip
/opt/python3.11/lib/python3.11
/opt/python3.11/lib/python3.11/lib-dynload
/opt/python3.11/lib/python3.11/site-packages
/home/airflow/gcs/dags
/etc/airflow/config
/home/airflow/gcs/plugins
 
Path of the file: /home/airflow/gcs/dags
Path the DAG is running in: /home/airflow

We tried different values for the apply_function
process_messages
event_based_triggering.process_messages
dags.event_based_triggering.process_messages
gcs.dags.event_based_triggering.process_messages
airflow.gcs.dags.event_based_triggering.process_messages
home.airflow.gcs.dags.event_based_triggering.process_messages

### What you think should happen instead

Below is the error in log:

```
ERROR - Trigger failed:
Traceback (most recent call last):
  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/jobs/triggerer_job_runner.py"", line 529, in cleanup_finished_triggers
    result = details[""task""].result()
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/jobs/triggerer_job_runner.py"", line 602, in run_trigger
    async for event in trigger.run():
  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/providers/apache/kafka/triggers/await_message.py"", line 99, in run
    processing_call = import_string(self.apply_function)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/utils/module_loading.py"", line 39, in import_string
    module = import_module(module_path)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/python3.11/lib/python3.11/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""<frozen importlib._bootstrap>"", line 1204, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 1176, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 1126, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1204, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 1176, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 1140, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'name_of_a_module'
```

We did some digging and tests -
If we use the module we created as an import at the top of the DAG file, the import statement works, and we are able to call the function in the DAG.
`from util import my_process`
 
But, in the AwaitMessageSensor, it uses importlib.import_module(""util.my_package"") which does not work.

We have found a workaround for our problem. In ConsumeFromTopicOperator, we can use both dotted string and callable as the value of ""apply_function"". The apply_function of ConsumeFromTopicOperator works fine. Why not do the same for AwaitMessageSensor? When you compare ConsumeFromTopicOperator with AwaitMessageTrigger, you will see that the change to be made is only:

```
if isinstance(self.apply_function, str):
    process_message = import_string(self.apply_function)
else:
    process_message self.apply_function
```

Instead of:
```
process_message = import_string(self.apply_function)
```

And also need to change the type definition of apply_function in AwaitMessageSensor and AwaitMessageTrigger. Overall, the change is small.

### How to reproduce

Use AwaitMessageSensor in Gooogle Cloud Composer and the problem will produce.

### Anything else

The problem happens every time.

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",ghost,2024-10-22 09:53:30+00:00,[],2024-10-24 18:07:34+00:00,2024-10-24 18:07:33+00:00,https://github.com/apache/airflow/issues/43253,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2428825032, 'issue_id': 2604914151, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 22, 9, 53, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2431112973, 'issue_id': 2604914151, 'author': 'guillesd', 'body': 'I had the same issue. I do not consider this a bug, but it would be great if we can pass the callable to avoid this short of problem!', 'created_at': datetime.datetime(2024, 10, 23, 7, 14, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2431304357, 'issue_id': 2604914151, 'author': 'AnkurSarode', 'body': 'I also came across this issue. It is quite annoying. One thing that works is if you package your custom function `process_messages` as a PyPi package and install it into cloud composer as an external module. But this is unnecessary overwork and hardly a solution to this problem.', 'created_at': datetime.datetime(2024, 10, 23, 8, 33, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2436037060, 'issue_id': 2604914151, 'author': 'potiuk', 'body': 'I believe this is a composer issue only - you should report it there. Seems like DAG folder is not available for triggerer there.', 'created_at': datetime.datetime(2024, 10, 24, 18, 7, 34, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-22 09:53:33 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

guillesd on (2024-10-23 07:14:21 UTC): I had the same issue. I do not consider this a bug, but it would be great if we can pass the callable to avoid this short of problem!

AnkurSarode on (2024-10-23 08:33:02 UTC): I also came across this issue. It is quite annoying. One thing that works is if you package your custom function `process_messages` as a PyPi package and install it into cloud composer as an external module. But this is unnecessary overwork and hardly a solution to this problem.

potiuk on (2024-10-24 18:07:34 UTC): I believe this is a composer issue only - you should report it there. Seems like DAG folder is not available for triggerer there.

"
2604724062,issue,open,,Operator's extra link button is disabled,"### Apache Airflow version

2.10.2

### What happened?

When in get_link method I provide `None`, the button is **enabled** and when I provide link (internal link) it is **disabled** 

### What you think should happen instead?

In the following code, if url is empty or None, the result of isSanitised is True ! 
also if the url is internal then isSanitised is False (so internal url is not supported)

As a workaround for now I am providing a random non-rul string instead of None, and I replaced my internal url with absolute one (but I missed rendering my page in same window now)

https://github.com/apache/airflow/blob/2.10.2/airflow/www/static/js/dag/details/taskInstance/ExtraLinks.tsx
``` py
const isExternal = (url: string | null) =>
    url && /^(?:[a-z]+:)?\/\//.test(url);

  const isSanitised = (url: string | null) => {
    if (!url) {
      return true;
    }
    const urlRegex = /^(https?:)/i;
    return urlRegex.test(url);
  };

  return (
    <Box my={3}>
      <Text as=""strong"">Extra Links</Text>
      <Flex flexWrap=""wrap"" mt={3}>
        {links.map(({ name, url }) => (
          <Button
            key={name}
            as={Link}
            colorScheme=""blue""
            href={url}
            isDisabled={!isSanitised(url)}
            target={isExternal(url) ? ""_blank"" : undefined}
            mr={2}
          >
            {name}
          </Button>
        ))}
      </Flex>
      <Divider my={2} />
    </Box>
  );
};
```


### How to reproduce

- Create a simple plugin with an OperatorLink
- Return None in get_link() method of your OperatorLink
- You can see the button in Details page of corresponding task is enalbed.

### Operating System

Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

non related

### Deployment

Docker-Compose

### Deployment details

We are deploying on docker swarm

### Anything else?

It works in 2.8.1 
And It seems that internal url issue is fixed in **main** branch but None handling is not fixed.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",mfatemipour,2024-10-22 08:44:46+00:00,['aditya0yadav'],2025-01-12 17:53:21+00:00,,https://github.com/apache/airflow/issues/43252,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2466179205, 'issue_id': 2604724062, 'author': 'enisnazif', 'body': ""I'm happy to investigate this - will take a look now"", 'created_at': datetime.datetime(2024, 11, 9, 11, 24, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466222685, 'issue_id': 2604724062, 'author': 'enisnazif', 'body': ""If I understand you correctly, seems like this can be solved with:\r\n\r\n```\r\nconst isSanitised = (url: string | null) => {\r\n    if (!url) {\r\n      return false;\r\n    }\r\n    const urlRegex = /^(https?:)/i;\r\n    return urlRegex.test(url);\r\n  };\r\n```\r\n\r\ni.e, if the URL is '' or null, set isSanitised to false so that the button ends up having the isDisabled prop set to true"", 'created_at': datetime.datetime(2024, 11, 9, 13, 46, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466229721, 'issue_id': 2604724062, 'author': 'enisnazif', 'body': 'candidate PR here: https://github.com/apache/airflow/pull/43844', 'created_at': datetime.datetime(2024, 11, 9, 14, 3, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466661922, 'issue_id': 2604724062, 'author': 'mfatemipour', 'body': ""isSanitised should return `true` for internal urls too (urls that doesn't start with `http`)."", 'created_at': datetime.datetime(2024, 11, 10, 9, 39, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466698109, 'issue_id': 2604724062, 'author': 'eladkal', 'body': 'Fixed in https://github.com/apache/airflow/pull/43844', 'created_at': datetime.datetime(2024, 11, 10, 11, 33, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2497432445, 'issue_id': 2604724062, 'author': 'mfatemipour', 'body': ""Hi, Your fix isn't complete, as I mentioned in my previous command isSanitised should returns `true` also for internal urls. but now it returns `false`\r\n\r\n```\r\nconst urlRegex = /^(https?:)/i;\r\nreturn urlRegex.test(url);\r\n```"", 'created_at': datetime.datetime(2024, 11, 25, 9, 40, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2505182933, 'issue_id': 2604724062, 'author': 'potiuk', 'body': '> const urlRegex = /^(https?:)/i;\r\n\r\nCan you propose a better fix ?', 'created_at': datetime.datetime(2024, 11, 28, 3, 13, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2505489554, 'issue_id': 2604724062, 'author': 'mfatemipour', 'body': 'Yeah, first of all, following code to check that a url is external is buggy:\r\n```\r\nconst isExternal = (url: string | null) =>\r\n    url && /^(?:[a-z]+:)?\\/\\//.test(url);\r\n```\r\n\r\nThis regex will accept a url starting with ""//""\r\n\r\nso first let\'s fix this (removing `?`):\r\n```\r\nconst isExternal = (url: string | null) =>\r\n    url && /^(?:[a-z]+:)\\/\\//.test(url);\r\n```\r\n\r\nAnd for the isSantised method let\'s reuse this isExternal\r\nto check that if url is external or internal (for internal check, starts with a single `/` character)\r\n\r\n```\r\nconst isSanitised = (url: string | null) => {\r\n    if (!url) {\r\n      return true;\r\n    }\r\n    return isExternal(url) || /^\\/[^\\/]/.test(url);\r\n  };\r\n```\r\n\r\nI cannot test it, I don\'t have a ready airflow dev environment on my work laptop, otherwise I would be appreciated to contribute.', 'created_at': datetime.datetime(2024, 11, 28, 8, 8, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2506032276, 'issue_id': 2604724062, 'author': 'potiuk', 'body': ""> I cannot test it, I don't have a ready airflow dev environment on my work laptop, otherwise I would be appreciated to contribute.\r\n\r\n\r\nYou can use codespaces - which is entirely from the browser, supported by Airflow and free up to 80 hours a month for everyone in GitHub. Super easy to use - you click a button and you have working development environment with airflow in a matter of minutes. Quick start here: https://github.com/apache/airflow/blob/main/contributing-docs/quick-start-ide/contributors_quick_start_codespaces.rst\r\n\r\nIf you can create and coment on issues in GitHub, you can also run this development environment -as it is purely in-browser and runs Airflow's CI image remotely on GitHub's workers."", 'created_at': datetime.datetime(2024, 11, 28, 12, 42, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585844906, 'issue_id': 2604724062, 'author': 'aditya0yadav', 'body': 'If it is available, can I work on this', 'created_at': datetime.datetime(2025, 1, 12, 17, 35, 31, tzinfo=datetime.timezone.utc)}]","enisnazif on (2024-11-09 11:24:33 UTC): I'm happy to investigate this - will take a look now

enisnazif on (2024-11-09 13:46:54 UTC): If I understand you correctly, seems like this can be solved with:

```
const isSanitised = (url: string | null) => {
    if (!url) {
      return false;
    }
    const urlRegex = /^(https?:)/i;
    return urlRegex.test(url);
  };
```

i.e, if the URL is '' or null, set isSanitised to false so that the button ends up having the isDisabled prop set to true

enisnazif on (2024-11-09 14:03:06 UTC): candidate PR here: https://github.com/apache/airflow/pull/43844

mfatemipour (Issue Creator) on (2024-11-10 09:39:42 UTC): isSanitised should return `true` for internal urls too (urls that doesn't start with `http`).

eladkal on (2024-11-10 11:33:30 UTC): Fixed in https://github.com/apache/airflow/pull/43844

mfatemipour (Issue Creator) on (2024-11-25 09:40:49 UTC): Hi, Your fix isn't complete, as I mentioned in my previous command isSanitised should returns `true` also for internal urls. but now it returns `false`

```
const urlRegex = /^(https?:)/i;
return urlRegex.test(url);
```

potiuk on (2024-11-28 03:13:41 UTC): Can you propose a better fix ?

mfatemipour (Issue Creator) on (2024-11-28 08:08:26 UTC): Yeah, first of all, following code to check that a url is external is buggy:
```
const isExternal = (url: string | null) =>
    url && /^(?:[a-z]+:)?\/\//.test(url);
```

This regex will accept a url starting with ""//""

so first let's fix this (removing `?`):
```
const isExternal = (url: string | null) =>
    url && /^(?:[a-z]+:)\/\//.test(url);
```

And for the isSantised method let's reuse this isExternal
to check that if url is external or internal (for internal check, starts with a single `/` character)

```
const isSanitised = (url: string | null) => {
    if (!url) {
      return true;
    }
    return isExternal(url) || /^\/[^\/]/.test(url);
  };
```

I cannot test it, I don't have a ready airflow dev environment on my work laptop, otherwise I would be appreciated to contribute.

potiuk on (2024-11-28 12:42:10 UTC): You can use codespaces - which is entirely from the browser, supported by Airflow and free up to 80 hours a month for everyone in GitHub. Super easy to use - you click a button and you have working development environment with airflow in a matter of minutes. Quick start here: https://github.com/apache/airflow/blob/main/contributing-docs/quick-start-ide/contributors_quick_start_codespaces.rst

If you can create and coment on issues in GitHub, you can also run this development environment -as it is purely in-browser and runs Airflow's CI image remotely on GitHub's workers.

aditya0yadav (Assginee) on (2025-01-12 17:35:31 UTC): If it is available, can I work on this

"
2604454589,issue,open,,Databricks No scheme supplied error,"### Apache Airflow Provider(s)

databricks

### Versions of Apache Airflow Providers

6.10.0

### Apache Airflow version

2.8.0

### Operating System

linux/amd64

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### What happened

When we use the oauth authentication with a Databricks service principal, we provide the environment variable :
 - AIRFLOW_CONN_DATABRICKS_DEFAULT = ""databricks://SP_CLIENT_ID:SP_SECRET@DATABRICKS_WORKSPACE_ID.azuredatabricks.net?service_principal_oauth=true""

But we have this error :
<img width=""767"" alt=""databricks"" src=""https://github.com/user-attachments/assets/98728dc7-6e80-47ac-a424-ef3ddd581c4e"">


### What you think should happen instead

_No response_

### How to reproduce

Install the Databricks operator, set the environment variable and use DatabricksRunNowOperator function in your DAG to run a workflow in Databricks.

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",tbaert-prest,2024-10-22 06:55:07+00:00,[],2024-10-22 06:57:29+00:00,,https://github.com/apache/airflow/issues/43249,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('provider:databricks', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2428405537, 'issue_id': 2604454589, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 22, 6, 55, 10, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-22 06:55:10 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2604071998,issue,open,,GitSync Credentials Secret requires 2 different sets of data fields,"### Official Helm Chart version

1.15.0 (latest released)

### Apache Airflow version

chart default

### Kubernetes Version

1.28

### Helm Chart configuration

```yaml
targetRevision: 1.15.0
destinationNamespace: airflow
repoURL: 'https://airflow.apache.org'
chart: airflow
values:
  executor: LocalKubernetesExecutor
  data:
    metadataSecretName: postgresql-connection-string-secret
  postgresql:
    enabled: false
  dags:
    gitSync:
      enabled: true
      repo: 'https://github.com/johnstonmatt/demo-dag-bag'
      credentialsSecret: airflow-git-credentials
      branch: main
      wait: 40
      subPath: dags
    # less relevant but true
    enableBuiltInSecretEnvVars:
      AIRFLOW__CORE__FERNET_KEY: false
      AIRFLOW__WEBSERVER__SECRET_KEY: false
    fernetKeySecretName: airflow-fernet-key
    webserverSecretKeySecretName: airflow-webserver-key
    config:
      webserver:
        expose_config: 'True'
        instance_name: Polyseam
        enable_proxy_fix: 'True'
        base_url: 'https://airflow.bug-repro.mj.cndi.link'
      operators:
        default_owner: Polyseam
    logs:
      persistence:
        enabled: true
        size: 15Gi
        storageClassName: rwm
    createUserJob:
      useHelmHooks: false
      applyCustomEnv: false
    migrateDatabaseJob:
      applyCustomEnv: false
      useHelmHooks: false
      jobAnnotations:
        ""argocd.argoproj.io/hook"": Sync      
```
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: airflow-git-credentials
  namespace: airflow
stringData:
  GIT_SYNC_USERNAME: 'foousername'
  GIT_SYNC_PASSWORD: 'barpassword'
  # newly required by airflow-triggerer, in addition to the pre-existing keys above
  GITSYNC_USERNAME: 'foousername'
  GITSYNC_PASSWORD: 'barpassword'
```


### Docker Image customizations

_No response_

### What happened

`airflow-triggerer` needs the gitsync credentials to be formatted as `GITSYNC_USERNAME` and `GITSYNC_PASSWORD`, while the rest of the chart seems to require `GIT_SYNC_USERNAME` and `GIT_SYNC_PASSWORD`

### What you think should happen instead

the airflow helm chart should adhere to only one format rather than requiring both

### How to reproduce

If you need to reproduce:

1. Use [cndi](https://cndi.run/gh?utm_id=5111) to install `airflow` via ArgoCD: `cndi create ghusername/my-repo -t airflow`
2. after cluster is deployed and working, update from `targetRevision: '1.11.0'` to `targetRevision: '1.15.0'`

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",johnstonmatt,2024-10-22 02:41:02+00:00,[],2024-10-23 15:42:23+00:00,,https://github.com/apache/airflow/issues/43247,"[('kind:bug', 'This is a clearly a bug'), ('area:helm-chart', 'Airflow Helm Chart')]","[{'comment_id': 2428093536, 'issue_id': 2604071998, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 22, 2, 41, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428587946, 'issue_id': 2604071998, 'author': 'romsharon98', 'body': 'what git-sync version are u using?\r\nthe difference between this `GIT_SYNC_USERNAME` and this `GITSYNC_USERNAME` is the git-sync version.\r\nthis `GIT_SYNC_USERNAME` is supported by git-sync 3.6 and `GITSYNC_USERNAME` is supported by git-sync 4\r\nmaybe somehow you have different git-sync version on scheduler and triggerer?', 'created_at': datetime.datetime(2024, 10, 22, 8, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428747404, 'issue_id': 2604071998, 'author': 'tobimichael96', 'body': 'We encountered the same problem right now. I think this here is the problem: https://github.com/apache/airflow/blob/d122ebf3a6aae2c96c95a1d63db9a98aa52e4763/chart/templates/_helpers.yaml#L235C37-L235C54\r\n\r\nIf you specify the secret name, it requires both keys to be there.', 'created_at': datetime.datetime(2024, 10, 22, 9, 21, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428894576, 'issue_id': 2604071998, 'author': 'romsharon98', 'body': ""when upgrading from 1.11 to 1.15 you also upgrade from git-sync 3.6.9 to 4.1.0 therefore you need to add those environment to your secret `GITSYNC_USERNAME` `GITSYNC_PASSWORD`.\r\nit also documented here https://github.com/romsharon98/airflow/blob/main/chart/values.yaml#L2738.\r\nalso check that the scheduler and the triggerer are having the same git-sync version (maybe on of them didn't upgrade)\r\nLet me know if I missed something."", 'created_at': datetime.datetime(2024, 10, 22, 10, 23, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2429041211, 'issue_id': 2604071998, 'author': 'tobimichael96', 'body': 'Yes, but looking how the `git_sync_container` object is defined in the `_helpers.yaml`, it does not matter what git-sync version you are using. It requires `GITSYNC_*` and `GIT_SYNC_*` being available in the secret. \r\n\r\nSo what you are describing is what the application side expects, but that is not in sync with the helm-chart code.\r\n\r\nIn my opinion that requires a change in the helm chart.', 'created_at': datetime.datetime(2024, 10, 22, 11, 36, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2429113327, 'issue_id': 2604071998, 'author': 'tobimichael96', 'body': 'I created a PR to provide an example on how to solve this: https://github.com/apache/airflow/pull/43264', 'created_at': datetime.datetime(2024, 10, 22, 12, 12, 7, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-22 02:41:06 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

romsharon98 on (2024-10-22 08:21:00 UTC): what git-sync version are u using?
the difference between this `GIT_SYNC_USERNAME` and this `GITSYNC_USERNAME` is the git-sync version.
this `GIT_SYNC_USERNAME` is supported by git-sync 3.6 and `GITSYNC_USERNAME` is supported by git-sync 4
maybe somehow you have different git-sync version on scheduler and triggerer?

tobimichael96 on (2024-10-22 09:21:24 UTC): We encountered the same problem right now. I think this here is the problem: https://github.com/apache/airflow/blob/d122ebf3a6aae2c96c95a1d63db9a98aa52e4763/chart/templates/_helpers.yaml#L235C37-L235C54

If you specify the secret name, it requires both keys to be there.

romsharon98 on (2024-10-22 10:23:49 UTC): when upgrading from 1.11 to 1.15 you also upgrade from git-sync 3.6.9 to 4.1.0 therefore you need to add those environment to your secret `GITSYNC_USERNAME` `GITSYNC_PASSWORD`.
it also documented here https://github.com/romsharon98/airflow/blob/main/chart/values.yaml#L2738.
also check that the scheduler and the triggerer are having the same git-sync version (maybe on of them didn't upgrade)
Let me know if I missed something.

tobimichael96 on (2024-10-22 11:36:52 UTC): Yes, but looking how the `git_sync_container` object is defined in the `_helpers.yaml`, it does not matter what git-sync version you are using. It requires `GITSYNC_*` and `GIT_SYNC_*` being available in the secret. 

So what you are describing is what the application side expects, but that is not in sync with the helm-chart code.

In my opinion that requires a change in the helm chart.

tobimichael96 on (2024-10-22 12:12:07 UTC): I created a PR to provide an example on how to solve this: https://github.com/apache/airflow/pull/43264

"
2603689337,issue,open,,AWS GlueCatalogHook doesn't support custom CatalogId,"### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

[apache-airflow-providers-amazon](https://airflow.apache.org/docs/apache-airflow-providers-amazon/8.28.0)
- version  8.28.0

### Apache Airflow version

2.10.1

### Operating System

MWAA

### Deployment

Amazon (AWS) MWAA

### Deployment details

Vanilla Deployment

### What happened

The current GlueCatalogHook doesn't pass the CatalogId property during boto3 calls as seen from here:

[GlueCatalogHook](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/_modules/airflow/providers/amazon/aws/hooks/glue_catalog.html)

### What you think should happen instead

There should be a way to pass the CatalogId as there will be users that will need to pass the CatalogId.
- This happened to my use case at work.

### How to reproduce

Try to target a Glue database and table that has an associated CatalogId where the CatalogId is not the default AWS AccountId and all operations will fail.

### Anything else

I was able to have a workaround by copying the implementation of the actual GlueCatalogHook and changing our sensors to use this ExtendedGlueCatalogHook where we add the CatalogId to the calls, example:

```
 def get_partitions(
        self,
        catalog_id: str,
        database_name: str,
        table_name: str,
        expression: str = """",
        page_size: int | None = None,
        max_items: int | None = None,
    ) -> set[tuple]:
   ...

   response = paginator.paginate(
            CatalogId=catalog_id, <=============== This should be added as an optional parameter
            DatabaseName=database_name, TableName=table_name, Expression=expression, PaginationConfig=config
        )

        partitions = set()
        for page in response:
            for partition in page[""Partitions""]:
                partitions.add(tuple(partition[""Values""]))

        return partitions
...
```

If anyone from the AWS team is going to work on this one, I'm also part of Amazon and you reach reach me (keds@) and I can show you what we did on this one.

Thanks!

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",keeed,2024-10-21 21:21:33+00:00,['keeed'],2024-12-09 22:12:47+00:00,,https://github.com/apache/airflow/issues/43238,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('kind:feature', 'Feature Requests'), ('good first issue', '')]","[{'comment_id': 2427742601, 'issue_id': 2603689337, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 21, 21, 21, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2449454228, 'issue_id': 2603689337, 'author': 'gopidesupavan', 'body': 'go ahed.. :)', 'created_at': datetime.datetime(2024, 10, 31, 9, 52, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2504551716, 'issue_id': 2603689337, 'author': 'ferruzzi', 'body': ""I can have a look at this some time next week if @keeed doesn't?"", 'created_at': datetime.datetime(2024, 11, 27, 18, 28, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2529443622, 'issue_id': 2603689337, 'author': 'olsenbudanur', 'body': 'I asked Devin to take a look into this issue, the solution it came up with looks good to me. The old unit tests + new unit tests Devin wrote passed as well. Created a PR with the changes, would be interested in a review: https://github.com/apache/airflow/pull/44800\r\n\r\nLink to Devin run: https://app.devin.ai/sessions/f6e5706fdebf47cb8cafcb44e8dd3ccb', 'created_at': datetime.datetime(2024, 12, 9, 21, 4, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2529501669, 'issue_id': 2603689337, 'author': 'gopidesupavan', 'body': 'Prs should come from real person GitHub account, not from boats.\r\n\r\ncc: @potiuk', 'created_at': datetime.datetime(2024, 12, 9, 21, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2529522274, 'issue_id': 2603689337, 'author': 'potiuk', 'body': 'Actually it can. https://www.apache.org/legal/generative-tooling.html:\r\n\r\n> Given the above, code generated in whole or in part using AI can be contributed if the contributor ensures that:\r\n>\r\n> 1. The terms and conditions of the generative AI tool do not place any restrictions on use of the output that would be inconsistent with the [Open Source Definition](https://opensource.org/osd/).\r\n> 2. At least one of the following conditions is met:\r\n>    1. The output is not copyrightable subject matter (and would not be even if produced by a human).\r\n>    2. No third party materials are included in the output.\r\n>    3. Any third party materials that are included in the output are being used with permission (e.g., under a compatible open-source license) of the third party copyright holders and in compliance with the applicable license terms.\r\n> 3. A contributor obtains reasonable certainty that conditions 2.2 or 2.3 are met if the AI tool itself provides sufficient information about output that may be similar to training data, or from code scanning results.\r\n\r\n\r\n@olsenbudanur -> can you confirm (and somewhat explain how) those conditions are met ?', 'created_at': datetime.datetime(2024, 12, 9, 21, 24, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2529552363, 'issue_id': 2603689337, 'author': 'gopidesupavan', 'body': 'Ah okay :)', 'created_at': datetime.datetime(2024, 12, 9, 21, 29, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2529560310, 'issue_id': 2603689337, 'author': 'olsenbudanur', 'body': '@potiuk looking into these conditions right now, will send an update soon', 'created_at': datetime.datetime(2024, 12, 9, 21, 30, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2529621626, 'issue_id': 2603689337, 'author': 'olsenbudanur', 'body': ""I've contacted the Devin (Cognition) team, and they confirmed that they reviewed the [Apache terms](https://www.apache.org/legal/generative-tooling.html) & [open-source terms](https://opensource.org/osd). They say everything looks fine (can tag someone from their team if needed)\r\n\r\n**1- The terms and conditions of the generative AI tool do not place any restrictions on use of the output that would be inconsistent with the [Open Source Definition](https://opensource.org/osd/)**\r\n\r\nAccording to [Devin's terms](https://www.cognition.ai/pages/terms-of-service), all outputs are fully owned by the user. No copyright and no third party materials.\r\n\r\n\r\n**2- At least one of the following conditions is met**\r\nThe output is not copyrightable subject matter. Also no third party materials are used.\r\n\r\n**3-**\r\nI am certain that this is not applicable.\r\n\r\n@potiuk is there anything I'm missing?"", 'created_at': datetime.datetime(2024, 12, 9, 22, 3, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2529628264, 'issue_id': 2603689337, 'author': 'potiuk', 'body': '> According to [Devin\'s terms](https://www.cognition.ai/pages/terms-of-service), all outputs are fully owned by the user. No copyright and no third party materials.\r\n\r\nThis is wrong. For example If they are using GPL code to train their code GPL puts restriction on redistribution of that code. It\'s not the ""ownership"" of the code it\'s the licence restrictions that are put on the code.', 'created_at': datetime.datetime(2024, 12, 9, 22, 7, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2529638089, 'issue_id': 2603689337, 'author': 'olsenbudanur', 'body': ""Oh I see. I'll try to pull someone from the Cognition team in this thread, they should have a better understanding of this than I do"", 'created_at': datetime.datetime(2024, 12, 9, 22, 12, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2529638932, 'issue_id': 2603689337, 'author': 'potiuk', 'body': 'BTW. If Devin wants to check if they follow the licence, they can write a question and explain what they do and ask for clarification via mechanisms described at https://www.apache.org/legal/#communications', 'created_at': datetime.datetime(2024, 12, 9, 22, 12, 46, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-21 21:21:36 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

gopidesupavan on (2024-10-31 09:52:30 UTC): go ahed.. :)

ferruzzi on (2024-11-27 18:28:46 UTC): I can have a look at this some time next week if @keeed doesn't?

olsenbudanur on (2024-12-09 21:04:03 UTC): I asked Devin to take a look into this issue, the solution it came up with looks good to me. The old unit tests + new unit tests Devin wrote passed as well. Created a PR with the changes, would be interested in a review: https://github.com/apache/airflow/pull/44800

Link to Devin run: https://app.devin.ai/sessions/f6e5706fdebf47cb8cafcb44e8dd3ccb

gopidesupavan on (2024-12-09 21:19:00 UTC): Prs should come from real person GitHub account, not from boats.

cc: @potiuk

potiuk on (2024-12-09 21:24:52 UTC): Actually it can. https://www.apache.org/legal/generative-tooling.html:



@olsenbudanur -> can you confirm (and somewhat explain how) those conditions are met ?

gopidesupavan on (2024-12-09 21:29:02 UTC): Ah okay :)

olsenbudanur on (2024-12-09 21:30:54 UTC): @potiuk looking into these conditions right now, will send an update soon

olsenbudanur on (2024-12-09 22:03:56 UTC): I've contacted the Devin (Cognition) team, and they confirmed that they reviewed the [Apache terms](https://www.apache.org/legal/generative-tooling.html) & [open-source terms](https://opensource.org/osd). They say everything looks fine (can tag someone from their team if needed)

**1- The terms and conditions of the generative AI tool do not place any restrictions on use of the output that would be inconsistent with the [Open Source Definition](https://opensource.org/osd/)**

According to [Devin's terms](https://www.cognition.ai/pages/terms-of-service), all outputs are fully owned by the user. No copyright and no third party materials.


**2- At least one of the following conditions is met**
The output is not copyrightable subject matter. Also no third party materials are used.

**3-**
I am certain that this is not applicable.

@potiuk is there anything I'm missing?

potiuk on (2024-12-09 22:07:47 UTC): This is wrong. For example If they are using GPL code to train their code GPL puts restriction on redistribution of that code. It's not the ""ownership"" of the code it's the licence restrictions that are put on the code.

olsenbudanur on (2024-12-09 22:12:23 UTC): Oh I see. I'll try to pull someone from the Cognition team in this thread, they should have a better understanding of this than I do

potiuk on (2024-12-09 22:12:46 UTC): BTW. If Devin wants to check if they follow the licence, they can write a question and explain what they do and ask for clarification via mechanisms described at https://www.apache.org/legal/#communications

"
2603391861,issue,closed,completed,WTForms 3.2.0 broke Airflow main,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.x as well ass main/3.0

### What happened?

Update of WTForms 3.2.0 removed some deprecations and broke the current use of the forms in Airflow.

PR: https://github.com/pallets-eco/wtforms/pull/859
Release: https://github.com/pallets-eco/wtforms/releases/tag/3.2.0
Broken run: https://github.com/apache/airflow/actions/runs/11441304988/job/31829670288

### What you think should happen instead?

WTForms feature release should not break Airflow.

### How to reproduce

n/a

### Operating System

all

### Versions of Apache Airflow Providers

unrelevant

### Deployment

Other

### Deployment details

Github :-D

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jscheffl,2024-10-21 18:50:01+00:00,[],2024-10-24 22:21:22+00:00,2024-10-24 22:21:21+00:00,https://github.com/apache/airflow/issues/43228,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:dependencies', 'Issues related to dependencies problems'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2427478323, 'issue_id': 2603391861, 'author': 'jscheffl', 'body': 'Aaah, I see following other threads the problem should be fixed in 3.2.1...', 'created_at': datetime.datetime(2024, 10, 21, 18, 53, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2427515002, 'issue_id': 2603391861, 'author': 'jscheffl', 'body': '...and the fix 3.2.1 also has a regression as in https://github.com/pallets-eco/flask-wtf/issues/608 - re open this...', 'created_at': datetime.datetime(2024, 10, 21, 19, 12, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2436441476, 'issue_id': 2603391861, 'author': 'potiuk', 'body': 'This is fixed already via #43309', 'created_at': datetime.datetime(2024, 10, 24, 22, 21, 21, tzinfo=datetime.timezone.utc)}]","jscheffl (Issue Creator) on (2024-10-21 18:53:22 UTC): Aaah, I see following other threads the problem should be fixed in 3.2.1...

jscheffl (Issue Creator) on (2024-10-21 19:12:56 UTC): ...and the fix 3.2.1 also has a regression as in https://github.com/pallets-eco/flask-wtf/issues/608 - re open this...

potiuk on (2024-10-24 22:21:21 UTC): This is fixed already via #43309

"
2603286217,issue,closed,completed,"""breeze start-airflow"" failing on main branch ","### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

""breeze start-airflow"" is failing for main branch

```
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running upgrade 3a8972ecb8f9 -> 05234396c6fc, Rename dataset as asset.
Traceback (most recent call last):
  File ""/usr/local/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context
    self.dialect.do_execute(
  File ""/usr/local/lib/python3.12/site-packages/sqlalchemy/engine/default.py"", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.UndefinedObject: constraint ""dsdar_dataset_fkey"" of relation ""dag_schedule_asset_alias_reference"" does not exist


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/usr/local/bin/airflow"", line 8, in <module>
    sys.exit(main())
             ^^^^^^
```

Seems some issue with migrating from datasets to assets

### What you think should happen instead?

This PR could be a possible problem https://github.com/apache/airflow/pull/42023
Reverting this fix is fixing the issue.

### How to reproduce

1. Checkout main branch
2. Run breeze `start-airflow`

### Operating System

MacOS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",shubhamraj-git,2024-10-21 17:59:06+00:00,['Lee-W'],2024-10-23 06:40:30+00:00,2024-10-23 06:16:29+00:00,https://github.com/apache/airflow/issues/43226,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:db-migrations', 'PRs with DB migration'), ('AIP-74', 'Dataset -> Asset')]","[{'comment_id': 2427373848, 'issue_id': 2603286217, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 21, 17, 59, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2427375574, 'issue_id': 2603286217, 'author': 'bbovenzi', 'body': 'Cc : @Lee-W @uranusjr  looks like we missed something in the dataset - asset migration', 'created_at': datetime.datetime(2024, 10, 21, 18, 0, 4, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-21 17:59:09 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

bbovenzi on (2024-10-21 18:00:04 UTC): Cc : @Lee-W @uranusjr  looks like we missed something in the dataset - asset migration

"
2603018600,issue,closed,completed,Helm chart does not support multiple executors,"### Official Helm Chart version

1.15.0 (latest released)

### Apache Airflow version

2.10.2

### Kubernetes Version

1.30.4

### Helm Chart configuration

```yaml
executor: CeleryExecutor,KubernetesExecutor
```

### Docker Image customizations

n/a

### What happened

First, the JSON schema specification failed as it has an allowlist of all known supported executors. Then, when I skipped schema verification, the deployment failed because the executor value is used as a label on at least one deployment, and commas are not allowed in label values.

```
❯ echo 'executor: CeleryExecutor,KubernetesExecutor' > values.yaml
❯ helm upgrade --install airflow apache-airflow/airflow --values values.yaml                                
Error: UPGRADE FAILED: values don't meet the specifications of the schema(s) in the following chart(s):
airflow:
- executor: executor must be one of the following: ""LocalExecutor"", ""LocalKubernetesExecutor"", ""CeleryExecutor"", ""KubernetesExecutor"", ""CeleryKubernetesExecutor"", ""airflow.providers.amazon.aws.executors.batch.AwsBatchExecutor"", ""airflow.providers.amazon.aws.executors.ecs.AwsEcsExecutor""

❯ helm upgrade --install airflow apache-airflow/airflow --values values.yaml --skip-schema-validation
Release ""airflow"" does not exist. Installing it now.
Error: 1 error occurred:
	* Deployment.apps ""airflow-scheduler"" is invalid: metadata.labels: Invalid value: ""CeleryExecutor,KubernetesExecutor"": a valid label must be an empty string or consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyValue',  or 'my_value',  or '12345', regex used for validation is '(([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9])?')
```

### What you think should happen instead

The helm chart should deploy successfully when multiple executors are specified as described in [the docs](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/executor/index.html#using-multiple-executors-concurrently).

### How to reproduce

```shell
echo 'executor: CeleryExecutor,KubernetesExecutor' > values.yaml
helm upgrade --install airflow apache-airflow/airflow --values values.yaml
```

and

```shell
helm upgrade --install airflow apache-airflow/airflow --values values.yaml --skip-schema-validation
```

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rcheatham-q,2024-10-21 16:02:17+00:00,['jx2lee'],2024-11-24 15:11:45+00:00,2024-11-24 15:11:45+00:00,https://github.com/apache/airflow/issues/43224,"[('kind:feature', 'Feature Requests'), ('good first issue', ''), ('area:helm-chart', 'Airflow Helm Chart')]","[{'comment_id': 2435920794, 'issue_id': 2603018600, 'author': 'potiuk', 'body': 'Sure this is good feature to add. If you would liek to do @rcheatham-q - you can add it, otherwise it will have to wait for someone to implement it.', 'created_at': datetime.datetime(2024, 10, 24, 17, 32, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2435921614, 'issue_id': 2603018600, 'author': 'potiuk', 'body': 'Shall I assign you @rcheatham-q  ?', 'created_at': datetime.datetime(2024, 10, 24, 17, 32, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2436309015, 'issue_id': 2603018600, 'author': 'rcheatham-q', 'body': ""I don't have time to do this right now, but if the opportunity presents itself I'm willing to take this on."", 'created_at': datetime.datetime(2024, 10, 24, 20, 46, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2451403172, 'issue_id': 2603018600, 'author': 'amoghrajesh', 'body': '@rcheatham-q do you have some time to work on this? Otherwise I can pick this up', 'created_at': datetime.datetime(2024, 11, 1, 6, 59, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452804930, 'issue_id': 2603018600, 'author': 'rcheatham-q', 'body': '@amoghrajesh I do not have time to work on this', 'created_at': datetime.datetime(2024, 11, 2, 2, 19, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452973482, 'issue_id': 2603018600, 'author': 'jx2lee', 'body': 'Can I take this issue?', 'created_at': datetime.datetime(2024, 11, 2, 12, 21, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452973756, 'issue_id': 2603018600, 'author': 'amoghrajesh', 'body': '@jx2lee feel free. Assigning to you', 'created_at': datetime.datetime(2024, 11, 2, 12, 21, 54, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-10-24 17:32:27 UTC): Sure this is good feature to add. If you would liek to do @rcheatham-q - you can add it, otherwise it will have to wait for someone to implement it.

potiuk on (2024-10-24 17:32:44 UTC): Shall I assign you @rcheatham-q  ?

rcheatham-q (Issue Creator) on (2024-10-24 20:46:47 UTC): I don't have time to do this right now, but if the opportunity presents itself I'm willing to take this on.

amoghrajesh on (2024-11-01 06:59:57 UTC): @rcheatham-q do you have some time to work on this? Otherwise I can pick this up

rcheatham-q (Issue Creator) on (2024-11-02 02:19:35 UTC): @amoghrajesh I do not have time to work on this

jx2lee (Assginee) on (2024-11-02 12:21:01 UTC): Can I take this issue?

amoghrajesh on (2024-11-02 12:21:54 UTC): @jx2lee feel free. Assigning to you

"
2602219741,issue,open,reopened,RuntimeError when retrying DAG run with zero-length mapped tasks,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

- 2.9.2
- 2.10.4

### What happened?

The [Airflow docs](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/dynamic-task-mapping.html#automatically-skipping-zero-length-maps) state the following behaviour when encountering zero-length maps when using Dynamic Task Mapping:

> If the input is empty (zero length), no new tasks will be created and the mapped task will be marked as `SKIPPED`.

The abovementioned behaviour is indeed correctly observed when a mapped task is first executed as part of a new DAG run (i.e. `try_number = 1`).

However, on subsequent tries (i.e. `try_number > 1`), the mapped task will instead throw the following exception:

```
[2024-10-21, 10:54:51 UTC] {taskinstance.py:2306} INFO - Starting attempt 2 of 2
[2024-10-21, 10:54:51 UTC] {taskinstance.py:2330} INFO - Executing <Mapped(PythonOperator): print_args> on 2024-10-21 10:54:27.291897+00:00
[2024-10-21, 10:54:51 UTC] {standard_task_runner.py:63} INFO - Started process 341 to run task
[2024-10-21, 10:54:51 UTC] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'dtm_failure', 'print_args', 'manual__2024-10-21T10:54:27.291897+00:00', '--job-id', '13517', '--raw', '--subdir', 'DAGS_FOLDER/dtm_failure.py', '--cfg-path', '/tmp/tmpahwek3h8', '--map-index', '0']
[2024-10-21, 10:54:51 UTC] {standard_task_runner.py:91} INFO - Job 13517: Subtask print_args
[2024-10-21, 10:54:51 UTC] {task_command.py:426} INFO - Running <TaskInstance: dtm_failure.print_args manual__2024-10-21T10:54:27.291897+00:00 map_index=0 [running]> on host 172.21.174.13
[2024-10-21, 10:54:52 UTC] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2479, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2633, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context, jinja_env=jinja_env)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 3094, in render_templates
    original_task.render_template_fields(context, jinja_env)
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 829, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 688, in _expand_mapped_kwargs
    return self._get_specified_expand_input().resolve(context, session)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/expandinput.py"", line 202, in resolve
    data = {k: self._expand_mapped_field(k, v, context, session=session) for k, v in self.value.items()}
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/expandinput.py"", line 202, in <dictcomp>
    data = {k: self._expand_mapped_field(k, v, context, session=session) for k, v in self.value.items()}
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/expandinput.py"", line 182, in _expand_mapped_field
    found_index = _find_index_for_this_field(map_index)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/expandinput.py"", line 176, in _find_index_for_this_field
    raise RuntimeError(f""cannot expand field mapped to length {mapped_length!r}"")
RuntimeError: cannot expand field mapped to length 0
[2024-10-21, 10:54:52 UTC] {taskinstance.py:2953} ERROR - Unable to unmap task to determine if we need to send an alert email
[2024-10-21, 10:54:52 UTC] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=dtm_failure, task_id=print_args, run_id=manual__2024-10-21T10:54:27.291897+00:00, map_index=0, execution_date=20241021T105427, start_date=20241021T105451, end_date=20241021T105452
[2024-10-21, 10:54:52 UTC] {standard_task_runner.py:110} ERROR - Failed to execute job 13517 for task print_args (cannot expand field mapped to length 0; 341)
```

To emphasise, this issue only occurs when a zero-length map is encountered on `try_number > 1`. When `try_number = 1` and there exists a zero-length map, this issue does not occur.

### What you think should happen instead?

There shouldn't be an exception raised. Looking at the logs above, it seems to be a bug with the handling of mapped tasks as it relates to sending alerts to emails.

### How to reproduce

1. Create the following DAG with this code:
   ```python
   from airflow import DAG
   from airflow.operators.python import PythonOperator
   
   
   def generate_args() -> list[dict]:
      from airflow.operators.python import get_current_context
   
      context = get_current_context()
      is_first_try = context[""ti""].try_number == 1
   
      return [{""foo"": f""bar_{idx}""} for idx in range(5)] if is_first_try else []
   
   
   with DAG(
      ""dtm_failure"",
      description=""Demonstrate DAG failure with zero-length mapped tasks on subsequent tries"",
      schedule=None,
   ):
      task_generate_args = PythonOperator(
          task_id=""generate_args"",
          python_callable=generate_args,
      )
   
      task_print_args = PythonOperator.partial(
          task_id=""print_args"",
          python_callable=lambda foo: print(f""Arg: {foo}""),
      ).expand(op_kwargs=task_generate_args.output)
   ```
2. On your Airflow deployment, manually trigger a run of the `dtm_failure` DAG.  
3. The first try will succeed and you should see the following: <img width=""1452"" alt=""Screenshot 2024-10-21 at 7 13 53 PM"" src=""https://github.com/user-attachments/assets/1283732d-1f4b-4363-9f3c-ef13e65eabbd"">
4. Select `Clear > Clear existing tasks` to retry the entire DAG run.
5. The second try will fail and you should see the following: <img width=""1442"" alt=""Screenshot 2024-10-21 at 7 14 28 PM"" src=""https://github.com/user-attachments/assets/10a2e99d-5d8b-4d1b-8a58-d2a43309a64e"">
6. Select the failed mapped task to view its logs.


### Operating System

Debian 11

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other Docker-based deployment

### Deployment details

- **Python version:** 3.11.4

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",szeswee,2024-10-21 11:17:23+00:00,['kunaljubce'],2024-12-26 17:20:05+00:00,,https://github.com/apache/airflow/issues/43214,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:core', ''), ('area:dynamic-task-mapping', 'AIP-42')]","[{'comment_id': 2426383596, 'issue_id': 2602219741, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 21, 11, 17, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2426750573, 'issue_id': 2602219741, 'author': 'romsharon98', 'body': 'able to reproduce it also on the main branch', 'created_at': datetime.datetime(2024, 10, 21, 13, 52, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2447951185, 'issue_id': 2602219741, 'author': 'manojgosavi', 'body': 'Hi @romsharon98 , @potiuk , Can I work on this issue? Please assign it to me.Thanks.', 'created_at': datetime.datetime(2024, 10, 30, 17, 59, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2542987820, 'issue_id': 2602219741, 'author': 'shahar1', 'body': ""This issue seems to be resolved in latest 2.10 version.\r\nIf you somehow it again in any variation, please comment here and I'll reopen the issue."", 'created_at': datetime.datetime(2024, 12, 14, 7, 52, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2559256460, 'issue_id': 2602219741, 'author': 'szeswee', 'body': ""Hello @shahar1, I've managed to reproduce the same issue on 2.10.4. The exception and error logs emitted are identical."", 'created_at': datetime.datetime(2024, 12, 23, 9, 7, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2559644938, 'issue_id': 2602219741, 'author': 'potiuk', 'body': 'can you post your logs from 2.10.4 @szeswee -> even if they are ""nearly identical"" - seeing line numbers and all the other things from 2.10.4 might be helpful to address it.', 'created_at': datetime.datetime(2024, 12, 23, 12, 46, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562130635, 'issue_id': 2602219741, 'author': 'szeswee', 'body': '@potiuk Of course, here are the logs. No additional changes were made to the DAG code since the issue was first created.\r\n\r\n```\r\n[2024-12-26, 03:57:18 UTC] {taskinstance.py:2866} INFO - Starting attempt 2 of 2\r\n[2024-12-26, 03:57:18 UTC] {taskinstance.py:2889} INFO - Executing <Mapped(PythonOperator): print_args> on 2024-12-23 09:00:06.277179+00:00\r\n[2024-12-26, 03:57:18 UTC] {standard_task_runner.py:72} INFO - Started process 55 to run task\r\n[2024-12-26, 03:57:18 UTC] {standard_task_runner.py:104} INFO - Running: [\'airflow\', \'tasks\', \'run\', \'dtm_failure\', \'print_args\', \'manual__2024-12-23T09:00:06.277179+00:00\', \'--job-id\', \'51434\', \'--raw\', \'--subdir\', \'DAGS_FOLDER/dtm_failure.py\', \'--cfg-path\', \'/tmp/tmp8w_gf4cb\', \'--map-index\', \'4\']\r\n[2024-12-26, 03:57:18 UTC] {standard_task_runner.py:105} INFO - Job 51434: Subtask print_args\r\n[2024-12-26, 03:57:18 UTC] {task_command.py:467} INFO - Running <TaskInstance: dtm_failure.print_args manual__2024-12-23T09:00:06.277179+00:00 map_index=4 [running]> on host 172.21.248.156\r\n[2024-12-26, 03:57:18 UTC] {taskinstance.py:3311} ERROR - Task failed with exception\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 273, in _run_raw_task\r\n    TaskInstance._execute_task_with_callbacks(\r\n  File ""/usr/local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 3115, in _execute_task_with_callbacks\r\n    task_orig = self.render_templates(context=context, jinja_env=jinja_env)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 3534, in render_templates\r\n    original_task.render_template_fields(context, jinja_env)\r\n  File ""/usr/local/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 912, in render_template_fields\r\n    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session, include_xcom=True)\r\n                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 700, in _expand_mapped_kwargs\r\n    return self._get_specified_expand_input().resolve(context, session, include_xcom=include_xcom)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.11/site-packages/airflow/models/expandinput.py"", line 215, in resolve\r\n    data = {\r\n           ^\r\n  File ""/usr/local/lib/python3.11/site-packages/airflow/models/expandinput.py"", line 216, in <dictcomp>\r\n    k: self._expand_mapped_field(k, v, context, session=session, include_xcom=include_xcom)\r\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.11/site-packages/airflow/models/expandinput.py"", line 193, in _expand_mapped_field\r\n    found_index = _find_index_for_this_field(map_index)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.11/site-packages/airflow/models/expandinput.py"", line 187, in _find_index_for_this_field\r\n    raise RuntimeError(f""cannot expand field mapped to length {mapped_length!r}"")\r\nRuntimeError: cannot expand field mapped to length 0\r\n[2024-12-26, 03:57:18 UTC] {taskinstance.py:3359} ERROR - Unable to unmap task to determine if we need to send an alert email\r\n[2024-12-26, 03:57:18 UTC] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=dtm_failure, task_id=print_args, run_id=manual__2024-12-23T09:00:06.277179+00:00, map_index=4, execution_date=20241223T090006, start_date=20241226T035718, end_date=20241226T035718\r\n[2024-12-26, 03:57:18 UTC] {standard_task_runner.py:124} ERROR - Failed to execute job 51434 for task print_args (cannot expand field mapped to length 0; 55)\r\n[2024-12-26, 03:57:18 UTC] {local_task_job_runner.py:266} INFO - Task exited with return code 1\r\n```', 'created_at': datetime.datetime(2024, 12, 26, 4, 1, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562888152, 'issue_id': 2602219741, 'author': 'kunaljubce', 'body': '@szeswee Are you working on a fix for this? If not, @potiuk you can assign this to me. Thanks!', 'created_at': datetime.datetime(2024, 12, 26, 15, 28, 37, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-21 11:17:26 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

romsharon98 on (2024-10-21 13:52:26 UTC): able to reproduce it also on the main branch

manojgosavi on (2024-10-30 17:59:27 UTC): Hi @romsharon98 , @potiuk , Can I work on this issue? Please assign it to me.Thanks.

shahar1 on (2024-12-14 07:52:37 UTC): This issue seems to be resolved in latest 2.10 version.
If you somehow it again in any variation, please comment here and I'll reopen the issue.

szeswee (Issue Creator) on (2024-12-23 09:07:01 UTC): Hello @shahar1, I've managed to reproduce the same issue on 2.10.4. The exception and error logs emitted are identical.

potiuk on (2024-12-23 12:46:02 UTC): can you post your logs from 2.10.4 @szeswee -> even if they are ""nearly identical"" - seeing line numbers and all the other things from 2.10.4 might be helpful to address it.

szeswee (Issue Creator) on (2024-12-26 04:01:57 UTC): @potiuk Of course, here are the logs. No additional changes were made to the DAG code since the issue was first created.

```
[2024-12-26, 03:57:18 UTC] {taskinstance.py:2866} INFO - Starting attempt 2 of 2
[2024-12-26, 03:57:18 UTC] {taskinstance.py:2889} INFO - Executing <Mapped(PythonOperator): print_args> on 2024-12-23 09:00:06.277179+00:00
[2024-12-26, 03:57:18 UTC] {standard_task_runner.py:72} INFO - Started process 55 to run task
[2024-12-26, 03:57:18 UTC] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'dtm_failure', 'print_args', 'manual__2024-12-23T09:00:06.277179+00:00', '--job-id', '51434', '--raw', '--subdir', 'DAGS_FOLDER/dtm_failure.py', '--cfg-path', '/tmp/tmp8w_gf4cb', '--map-index', '4']
[2024-12-26, 03:57:18 UTC] {standard_task_runner.py:105} INFO - Job 51434: Subtask print_args
[2024-12-26, 03:57:18 UTC] {task_command.py:467} INFO - Running <TaskInstance: dtm_failure.print_args manual__2024-12-23T09:00:06.277179+00:00 map_index=4 [running]> on host 172.21.248.156
[2024-12-26, 03:57:18 UTC] {taskinstance.py:3311} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 3115, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context, jinja_env=jinja_env)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 3534, in render_templates
    original_task.render_template_fields(context, jinja_env)
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 912, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session, include_xcom=True)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 700, in _expand_mapped_kwargs
    return self._get_specified_expand_input().resolve(context, session, include_xcom=include_xcom)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/expandinput.py"", line 215, in resolve
    data = {
           ^
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/expandinput.py"", line 216, in <dictcomp>
    k: self._expand_mapped_field(k, v, context, session=session, include_xcom=include_xcom)
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/expandinput.py"", line 193, in _expand_mapped_field
    found_index = _find_index_for_this_field(map_index)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/expandinput.py"", line 187, in _find_index_for_this_field
    raise RuntimeError(f""cannot expand field mapped to length {mapped_length!r}"")
RuntimeError: cannot expand field mapped to length 0
[2024-12-26, 03:57:18 UTC] {taskinstance.py:3359} ERROR - Unable to unmap task to determine if we need to send an alert email
[2024-12-26, 03:57:18 UTC] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=dtm_failure, task_id=print_args, run_id=manual__2024-12-23T09:00:06.277179+00:00, map_index=4, execution_date=20241223T090006, start_date=20241226T035718, end_date=20241226T035718
[2024-12-26, 03:57:18 UTC] {standard_task_runner.py:124} ERROR - Failed to execute job 51434 for task print_args (cannot expand field mapped to length 0; 55)
[2024-12-26, 03:57:18 UTC] {local_task_job_runner.py:266} INFO - Task exited with return code 1
```

kunaljubce (Assginee) on (2024-12-26 15:28:37 UTC): @szeswee Are you working on a fix for this? If not, @potiuk you can assign this to me. Thanks!

"
2601838402,issue,closed,completed,"When the task is in the up_for_retry or rescheduler state, logs cannot be obtained","### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.3

### What happened?

When the task is in the up_for_retry or rescheduler state, logs cannot be obtained.
As shown in the figure, only the hostname is displayed
<img width=""1811"" alt=""iShot_2024-10-21_16 33 44"" src=""https://github.com/user-attachments/assets/035b0081-09b9-4b89-b20f-ef8b00cb441a"">


### What you think should happen instead?

Tasks in the up_for_retry and rescheduler states have been executed but failed and are in the retry phase. Logs should be displayed normally.

### How to reproduce

I deployed 3 nodes and did not enable remote logging.
This happens if the task is running on a non-web node and is in the up_for_retry or rescheduler state.


### Operating System

ubuntu 20.04

### Versions of Apache Airflow Providers

apache-airflow-providers-celery==3.7.3
apache-airflow-providers-common-io==1.3.2
apache-airflow-providers-common-sql==1.14.2
apache-airflow-providers-fab==1.2.2
apache-airflow-providers-ftp==3.10.0
apache-airflow-providers-http==4.12.0
apache-airflow-providers-imap==3.6.1
apache-airflow-providers-mysql==5.6.3
apache-airflow-providers-redis==3.7.1
apache-airflow-providers-smtp==1.7.1
apache-airflow-providers-sqlite==3.8.1

### Deployment

Virtualenv installation

### Deployment details

I use Python virtual environment deployment, the configuration is as attached
[airflow_new.txt](https://github.com/user-attachments/files/17457993/airflow_new.txt)


### Anything else?

code file :lib/python3.8/site-packages/airflow/utils/log/file_task_handler.py
This code will result in only getting the log of the node where the web is located when remote logging is not used, so only the host name will be output
<img width=""1471"" alt=""iShot_2024-10-21_16 51 24"" src=""https://github.com/user-attachments/assets/8f27a221-9394-4cab-bc10-753620c5feea"">

So, it should be changed to this
<img width=""1475"" alt=""iShot_2024-10-21_16 53 15"" src=""https://github.com/user-attachments/assets/feb61027-4a20-41f8-b87b-7eb0b264aa4b"">



### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",stursdsafdaf,2024-10-21 08:55:15+00:00,['stursdsafdaf'],2024-10-27 02:20:42+00:00,2024-10-26 14:39:43+00:00,https://github.com/apache/airflow/issues/43209,"[('kind:bug', 'This is a clearly a bug'), ('area:logging', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2426036111, 'issue_id': 2601838402, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 21, 8, 55, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2435869721, 'issue_id': 2601838402, 'author': 'potiuk', 'body': 'Looks playsible. WDYT @dstandish ?', 'created_at': datetime.datetime(2024, 10, 24, 17, 8, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439345915, 'issue_id': 2601838402, 'author': 'AllardQuek', 'body': ""Hi @stursdsafdaf, I am also facing a similar issue on v2.7.3. May I check if your proposed solution worked for you? \r\n\r\nI noticed there is a previously merged PR #39496 that seems to perform a similar fix, though the changes do not seem to be included in the latest version. As I'm new to Airflow, could anyone confirm if the changes from #39496 would solve this issue?"", 'created_at': datetime.datetime(2024, 10, 26, 5, 3, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439605116, 'issue_id': 2601838402, 'author': 'dstandish', 'body': 'I think I added a related fix here https://github.com/apache/airflow/pull/41272 which has milestone of 2.10.0.\r\n\r\nDid you try 2.10?', 'created_at': datetime.datetime(2024, 10, 26, 14, 38, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439605541, 'issue_id': 2601838402, 'author': 'dstandish', 'body': 'Yeah the reports here are from 2.9.3 and 2.7.3 so I assume that this is resolved by https://github.com/apache/airflow/pull/41272 in  2.10.0 so I will close this.', 'created_at': datetime.datetime(2024, 10, 26, 14, 39, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439625688, 'issue_id': 2601838402, 'author': 'AllardQuek', 'body': '> I think I added a related fix here #41272 which has milestone of 2.10.0.\r\n> \r\n> Did you try 2.10?\r\n\r\nWe are working with v2.7.3 in production. To implement the fix in an earlier version, which changes do we require from https://github.com/apache/airflow/pull/41272/files? The changes do not seem to mention the up for retry state. Thank you!', 'created_at': datetime.datetime(2024, 10, 26, 15, 42, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439719105, 'issue_id': 2601838402, 'author': 'potiuk', 'body': ""> We are working with v2.7.3 in production. To implement the fix in an earlier version, which changes do we require from https://github.com/apache/airflow/pull/41272/files? The changes do not seem to mention the up for retry state. Thank you!\r\n\r\nYou need to upgrade. We do not have a process to individually guide and explain how to take parts of the code and apply it to your version if you do not want to upgrade. If you want there are a number of 3rd-party commercial companies to assist you with it (and get paid for it), but if you want to rely on volunteer's effort here, the only way to apply fixes is to upgrade."", 'created_at': datetime.datetime(2024, 10, 26, 19, 55, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439808175, 'issue_id': 2601838402, 'author': 'AllardQuek', 'body': ""> You need to upgrade. We do not have a process to individually guide and explain how to take parts of the code and apply it to your version if you do not want to upgrade. If you want there are a number of 3rd-party commercial companies to assist you with it (and get paid for it), but if you want to rely on volunteer's effort here, the only way to apply fixes is to upgrade.\r\n\r\nGot it, thanks!"", 'created_at': datetime.datetime(2024, 10, 27, 2, 20, 41, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-21 08:55:18 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-10-24 17:08:27 UTC): Looks playsible. WDYT @dstandish ?

AllardQuek on (2024-10-26 05:03:04 UTC): Hi @stursdsafdaf, I am also facing a similar issue on v2.7.3. May I check if your proposed solution worked for you? 

I noticed there is a previously merged PR #39496 that seems to perform a similar fix, though the changes do not seem to be included in the latest version. As I'm new to Airflow, could anyone confirm if the changes from #39496 would solve this issue?

dstandish on (2024-10-26 14:38:03 UTC): I think I added a related fix here https://github.com/apache/airflow/pull/41272 which has milestone of 2.10.0.

Did you try 2.10?

dstandish on (2024-10-26 14:39:43 UTC): Yeah the reports here are from 2.9.3 and 2.7.3 so I assume that this is resolved by https://github.com/apache/airflow/pull/41272 in  2.10.0 so I will close this.

AllardQuek on (2024-10-26 15:42:17 UTC): We are working with v2.7.3 in production. To implement the fix in an earlier version, which changes do we require from https://github.com/apache/airflow/pull/41272/files? The changes do not seem to mention the up for retry state. Thank you!

potiuk on (2024-10-26 19:55:40 UTC): You need to upgrade. We do not have a process to individually guide and explain how to take parts of the code and apply it to your version if you do not want to upgrade. If you want there are a number of 3rd-party commercial companies to assist you with it (and get paid for it), but if you want to rely on volunteer's effort here, the only way to apply fixes is to upgrade.

AllardQuek on (2024-10-27 02:20:41 UTC): Got it, thanks!

"
2600579378,issue,closed,completed,RabbitMQOperator task stuck on queued when using CeleryExecutor as core executor ,"### Apache Airflow Provider(s)

redis

### Versions of Apache Airflow Providers

airflow-provider-rabbitmq 0.6.1

### Apache Airflow version

2.10.2

### Operating System

airflow bitnami image

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### What happened

When using CeleryExecutor as the core executor for Airflow and utilizing the RabbitMQOperator task from rabbitmq_provider.operators.rabbitmq, the task remains stuck in the queued state and is not recognized by Celery workers. The issue arises when attempting to use RabbitMQOperator to publish messages to an external RabbitMQ queue, which is not related to the Airflow instance.

The root cause of the conflict is that the RabbitMQOperator takes a queue field as a parameter, and this value is mistakenly treated as the name of the queue that the Celery workers will use to execute the task. This creates confusion, as the queue parameter is intended to represent the destination queue for message publishing, not the execution queue for Celery tasks.

### What you think should happen instead

_No response_

### How to reproduce

NA

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",abdalhalimalzohbi,2024-10-20 15:44:00+00:00,[],2024-10-24 14:08:44+00:00,2024-10-24 14:08:44+00:00,https://github.com/apache/airflow/issues/43203,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('pending-response', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2425052505, 'issue_id': 2600579378, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 20, 15, 44, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2425151558, 'issue_id': 2600579378, 'author': 'eladkal', 'body': '`airflow-provider-rabbitmq` is not official Airflow provider. There is no `RabbitMQOperator` in the Apache-Airflow project.\r\nUnless you can explain what is the bug in Airflow code base I suggest to submit this report to the maintainers of `airflow-provider-rabbitmq`', 'created_at': datetime.datetime(2024, 10, 20, 17, 51, 36, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-20 15:44:03 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2024-10-20 17:51:36 UTC): `airflow-provider-rabbitmq` is not official Airflow provider. There is no `RabbitMQOperator` in the Apache-Airflow project.
Unless you can explain what is the bug in Airflow code base I suggest to submit this report to the maintainers of `airflow-provider-rabbitmq`

"
2600153054,issue,closed,completed,Document local virtualenv setup with workspaces,"With #42505 and few follow-up issuse we've worked out how to use uv workspaces, to have the source tree split between airflow and providers. This means that it is possible to have airflow and providers development working fot both breeze and local virtual environment using `uv workspaces`

While with Breeze it works out-of-the-box, the local virtualenv setup and IDE setup for IntelliJ, VS Code requires some adjustments. Our documentation still describes how it works for the ""pre-providers-split"" and our contributors (both new and those who already contributed) are a bit confused what to do and how to set-it up - because there is not even a mentioning of uv and workspaces in the contributing documentation.

While a number of people who use the workspaces already had worked out their patterns, this is not explained and knowledge about it is not shared with other contributors, especially that `uv workspace` is a relatively new feature and by far not something we can assume that people know about.

We need to update https://github.com/apache/airflow/blob/main/contributing-docs/03_contributors_quick_start.rst https://github.com/apache/airflow/blob/main/contributing-docs/07_local_virtualenv.rst and related IDE guides in those chapters to explain it better.",potiuk,2024-10-20 07:54:15+00:00,['potiuk'],2024-11-02 19:32:25+00:00,2024-11-02 19:32:25+00:00,https://github.com/apache/airflow/issues/43200,"[('good first issue', ''), ('kind:documentation', '')]","[{'comment_id': 2426409297, 'issue_id': 2600153054, 'author': 'ashb', 'body': '`uv sync` ""should"" I think be the command we want to use, but it\'s not working right now and gives an error about unresolveable deps https://apache-airflow.slack.com/archives/C06K9Q5G2UA/p1729109616153099\r\n\r\n```\r\n  × No solution found when resolving dependencies for split (python_full_version == \'3.9.*\'):\r\n  ╰─▶ Because only the following versions of snowflake-snowpark-python{python_full_version < \'3.12\'} are available:\r\n          snowflake-snowpark-python{python_full_version < \'3.12\'}<=1.17.0\r\n          snowflake-snowpark-python{python_full_version < \'3.12\'}==1.18.0\r\n          snowflake-snowpark-python{python_full_version < \'3.12\'}==1.19.0\r\n          snowflake-snowpark-python{python_full_version < \'3.12\'}==1.20.0\r\n          snowflake-snowpark-python{python_full_version < \'3.12\'}==1.21.0\r\n          snowflake-snowpark-python{python_full_version < \'3.12\'}==1.21.1\r\n          snowflake-snowpark-python{python_full_version < \'3.12\'}==1.22.0\r\n          snowflake-snowpark-python{python_full_version < \'3.12\'}==1.22.1\r\n          snowflake-snowpark-python{python_full_version < \'3.12\'}==1.23.0\r\n      and all of:\r\n          snowflake-snowpark-python>=1.17.0,<=1.21.1\r\n          snowflake-snowpark-python>=1.22.1\r\n...\r\n```\r\n\r\n(Error trimmed)', 'created_at': datetime.datetime(2024, 10, 21, 11, 29, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452911918, 'issue_id': 2600153054, 'author': 'kunaljubce', 'body': '@potiuk Although this is not technically my first Airflow issue, can I take this up? Seems like a good way to get my hands dirty with the local installation stuff, and I have been meaning to get a good knowledge on that since I faced some challenges with my previous PR when trying to test it locally.', 'created_at': datetime.datetime(2024, 11, 2, 7, 56, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452974007, 'issue_id': 2600153054, 'author': 'potiuk', 'body': '> @potiuk Although this is not technically my first Airflow issue, can I take this up? Seems like a good way to get my hands dirty with the local installation stuff, and I have been meaning to get a good knowledge on that since I faced some challenges with my previous PR when trying to test it locally.\r\n\r\nAh... I have **just** proposed a PR to address it - and only now I see you commented on it (asynchronous checking). Maybe a good idea is to go through my PR @kunaljubce and follow it, check if things are clear and unambiguous, propose improvements etc :)', 'created_at': datetime.datetime(2024, 11, 2, 12, 22, 48, tzinfo=datetime.timezone.utc)}]","ashb on (2024-10-21 11:29:31 UTC): `uv sync` ""should"" I think be the command we want to use, but it's not working right now and gives an error about unresolveable deps https://apache-airflow.slack.com/archives/C06K9Q5G2UA/p1729109616153099

```
  × No solution found when resolving dependencies for split (python_full_version == '3.9.*'):
  ╰─▶ Because only the following versions of snowflake-snowpark-python{python_full_version < '3.12'} are available:
          snowflake-snowpark-python{python_full_version < '3.12'}<=1.17.0
          snowflake-snowpark-python{python_full_version < '3.12'}==1.18.0
          snowflake-snowpark-python{python_full_version < '3.12'}==1.19.0
          snowflake-snowpark-python{python_full_version < '3.12'}==1.20.0
          snowflake-snowpark-python{python_full_version < '3.12'}==1.21.0
          snowflake-snowpark-python{python_full_version < '3.12'}==1.21.1
          snowflake-snowpark-python{python_full_version < '3.12'}==1.22.0
          snowflake-snowpark-python{python_full_version < '3.12'}==1.22.1
          snowflake-snowpark-python{python_full_version < '3.12'}==1.23.0
      and all of:
          snowflake-snowpark-python>=1.17.0,<=1.21.1
          snowflake-snowpark-python>=1.22.1
...
```

(Error trimmed)

kunaljubce on (2024-11-02 07:56:07 UTC): @potiuk Although this is not technically my first Airflow issue, can I take this up? Seems like a good way to get my hands dirty with the local installation stuff, and I have been meaning to get a good knowledge on that since I faced some challenges with my previous PR when trying to test it locally.

potiuk (Issue Creator) on (2024-11-02 12:22:48 UTC): Ah... I have **just** proposed a PR to address it - and only now I see you commented on it (asynchronous checking). Maybe a good idea is to go through my PR @kunaljubce and follow it, check if things are clear and unambiguous, propose improvements etc :)

"
2598698691,issue,open,,Improve debug logging,"### Description

Identify and improve debug logging in core and providers. Debug logs should include all the information used to make the request/invocation, especially when interacting with external systems.  

### Use case/motivation

It helps identify the data used when interacting with external systems and response data. This can be very helpful in case of failures.  Many providers already do this. But, goal of this issue is to identify gaps and fill them

### Related issues

Parent Issue: https://github.com/apache/airflow/issues/40975

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rawwar,2024-10-19 03:35:58+00:00,['rawwar'],2024-12-20 16:30:00+00:00,,https://github.com/apache/airflow/issues/43186,"[('area:logging', ''), ('kind:feature', 'Feature Requests'), ('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2553044034, 'issue_id': 2598698691, 'author': 'Prab-27', 'body': ""@omkar-foss, I'd like to work on this issue. I would like to start with the scheduler"", 'created_at': datetime.datetime(2024, 12, 19, 8, 18, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2553053089, 'issue_id': 2598698691, 'author': 'rawwar', 'body': ""@Prab-27 , feel free to create issues and refer them to this one.  We can use this as an Issue to track progress. I'll start tracking them here once you link the issue."", 'created_at': datetime.datetime(2024, 12, 19, 8, 23, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2557322327, 'issue_id': 2598698691, 'author': 'omkar-foss', 'body': ""Sure thanks for picking it up @Prab-27, feel free to create a new issue for the Scheduler debug logging improvements and link it to this issue (as @rawwar suggested above).\r\n\r\nAlso in case you'd like to discuss or brainstorm something quickly, you can drop a message on `#sig-improving-debugging` channel on Airflow community Slack, link [here](https://apache-airflow.slack.com/archives/C07J87PK1BK)."", 'created_at': datetime.datetime(2024, 12, 20, 16, 28, 52, tzinfo=datetime.timezone.utc)}]","Prab-27 on (2024-12-19 08:18:46 UTC): @omkar-foss, I'd like to work on this issue. I would like to start with the scheduler

rawwar (Issue Creator) on (2024-12-19 08:23:14 UTC): @Prab-27 , feel free to create issues and refer them to this one.  We can use this as an Issue to track progress. I'll start tracking them here once you link the issue.

omkar-foss on (2024-12-20 16:28:52 UTC): Sure thanks for picking it up @Prab-27, feel free to create a new issue for the Scheduler debug logging improvements and link it to this issue (as @rawwar suggested above).

Also in case you'd like to discuss or brainstorm something quickly, you can drop a message on `#sig-improving-debugging` channel on Airflow community Slack, link [here](https://apache-airflow.slack.com/archives/C07J87PK1BK).

"
2598368012,issue,open,,Explore and implement task-level memory monitoring and display in UI,"### Description

Feedback from Airflow Debugging Survey 2024:
- Tools to accurately monitor memory usage on a task-by-task basis.

### Use case/motivation

**Goals for this issue are the following:**
- Explore feasibility of monitoring memory usage at task-level.
- Display the memory usage information in Airflow UI.

### Related issues

Parent Issue: https://github.com/apache/airflow/issues/40975

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-10-18 20:50:41+00:00,[],2024-10-23 13:26:28+00:00,,https://github.com/apache/airflow/issues/43182,"[('kind:feature', 'Feature Requests'), ('area:monitoring', ''), ('kind:meta', 'High-level information important to the community'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]",[],
2598351853,issue,open,,"Increase Airflow logging coverage, review existing logs and add missing essential logs","### Description

As per users' feedback in the Airflow Debugging Survey 2024, about 42% of respondents mentioned that logs are missing or non-existent in some cases.

### Use case/motivation

Goal for this issue is to review the existing Airflow logs coverage and enhance it by adding logs wherever appropriately required.


### Related issues

Parent Issue: https://github.com/apache/airflow/issues/40975

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-10-18 20:41:01+00:00,[],2024-10-29 08:24:56+00:00,,https://github.com/apache/airflow/issues/43180,"[('area:logging', ''), ('kind:feature', 'Feature Requests'), ('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2443547804, 'issue_id': 2598351853, 'author': 'Dev-iL', 'body': ""A good example of an issue with logging is when (fatal) errors from workers are not being forwarded to the scheduler, and it's very hard to trace what's actually wrong. In cases when a worker is exhibiting an early enough error (being unable to load the full Airflow machinery that records logs locally and forwards them to the scheduler/webserver), displaying a stack trace captured on the worker would help immensely.\n\nRef: https://github.com/apache/airflow/issues/42136"", 'created_at': datetime.datetime(2024, 10, 29, 8, 24, 54, tzinfo=datetime.timezone.utc)}]","Dev-iL on (2024-10-29 08:24:54 UTC): A good example of an issue with logging is when (fatal) errors from workers are not being forwarded to the scheduler, and it's very hard to trace what's actually wrong. In cases when a worker is exhibiting an early enough error (being unable to load the full Airflow machinery that records logs locally and forwards them to the scheduler/webserver), displaying a stack trace captured on the worker would help immensely.

Ref: https://github.com/apache/airflow/issues/42136

"
2598344802,issue,open,,Better readability and searchability of Airflow logs,"### Description

As per users' feedback in the Airflow Debugging Survey 2024:
- Enhanced readability and search features are needed
- More than 50% respondents find logs difficult to search and filter

### Use case/motivation

**Goals for this issue are the following:**
- Review existing log messages for verbosity and readability.
- Explore addition of a fuzzy search functionality in Airflow UI so user preferably doesn't have to leave Airflow window to search for Airflow logs (in other places like ES). This will enhance dev productivity.
- Explore keeping log verbosity configurable at DAG-level. For e.g. A user may want more verbosity for a model training DAG while very less verbosity for a scheduled DAG performing basic data transformations.

### Related issues

Parent Issue: https://github.com/apache/airflow/issues/40975

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-10-18 20:35:04+00:00,[],2024-10-18 20:37:22+00:00,,https://github.com/apache/airflow/issues/43179,"[('area:logging', ''), ('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]",[],
2598317853,issue,open,,Make stack traces more comprehensive yet simple,"### Description

As per users' feedback in the Airflow Debugging Survey 2024, user feedback around incomplete stack traces is as follows:

- Stack traces do not always show the full path of the source file, which can lead to confusion with files that have the same name across installed packages.
- Stack traces for failing DAGs often lack complete information when displayed in the UI's red error bar.
- Stack traces are generally helpful but can be misleading when they arise from dependency conflicts after upgrades.

### Use case/motivation

**Goals for this issue are the following:**
- Automatically dump relevant variables and environment states at the time of failure, including task parameters and Airflow configurations. Ensure to cleanup the dump to remove redundant content.
- Add time information to stack traces, detailing the time spent on different operations within a task.
- Include other DAG execution state information that could be useful to the user. Preferably keep it configurable to user can disable if not required.
- Attempt to always show the full path of the source file, in order to avoid confusion during debugging.

### Related issues

Parent Issue: https://github.com/apache/airflow/issues/40975

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-10-18 20:17:28+00:00,[],2024-10-23 13:29:15+00:00,,https://github.com/apache/airflow/issues/43178,"[('kind:feature', 'Feature Requests'), ('kind:meta', 'High-level information important to the community'), ('area:core', '')]",[],
