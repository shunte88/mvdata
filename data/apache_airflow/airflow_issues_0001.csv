id,type,state,state_reason,title,body,author,created_at,assignees,updated_at,closed_at,url,labels,comments_list,comment_thread
2839615728,issue,open,,Dynamic task mapping- Expand not working when provided value from a task,"### Apache Airflow version

3.0.0a2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

DAGRUN is failing with Dynamic Mapped task in the `upstream_failed` state even though upstream task is in success state

**Error in schedule logs**

```
[2025-02-08T05:18:20.469+0000] {scheduler_job_runner.py:744} INFO - Received executor event with state queued for task instance TaskInstanceKey(dag_id='context_ref', task_id='a_list', run_id='manual__2025-02-08T05:18:19.465557+00:00', try_number=1, map_index=-1)
[2025-02-08T05:18:20.479+0000] {scheduler_job_runner.py:776} INFO - Setting external_id for <TaskInstance: context_ref.a_list manual__2025-02-08T05:18:19.465557+00:00 [queued]> to e229edfa-f569-4e19-af4e-32f9a4c5b303
[2025-02-08T05:18:22.141+0000] {taskmap.py:148} ERROR - Cannot expand '\x1b[1m<Mapped(_PythonDecoratedOperator): ref_context>\x1b[22m' for run manual__2025-02-08T05:18:19.465557+00:00; missing upstream values: ['num']
[2025-02-08T05:18:22.165+0000] {scheduler_job_runner.py:744} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='context_ref', task_id='a_list', run_id='manual__2025-02-08T05:18:19.465557+00:00', try_number=1, map_index=-1)
```

<img width=""1319"" alt=""Image"" src=""https://github.com/user-attachments/assets/23bd5466-ef5a-4b22-967f-cf6d78810b5e"" />

### What you think should happen instead?

Dynamic mapped task DAG should execute fine as it used to be in AF2

### How to reproduce

Execute below DAG in with `3.0.0a2`

**DAG**
```

from airflow import DAG
from airflow.decorators import task
from datetime import datetime


@task
def a_list():
    return [3, 6, 9]


@task
def ref_context(num, **context):
    return num + context[""ti""].map_index * 5


@task
def assert_sum(nums, expect):
    print(nums)
    print(f""expecting sum: {' + '.join(map(str,nums))} == {expect}"")
    print(sum(nums))
    assert sum(nums) == expect


with DAG(
    dag_id=""context_ref"",
    start_date=datetime(1970, 1, 1),
    schedule=None,
    tags=[""taskmap""]
) as dag:
    assert_sum(ref_context.expand(num=a_list()), 33)
```

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vatsrahul1001,2025-02-08 05:26:17+00:00,['ashb'],2025-02-08 06:45:08+00:00,,https://github.com/apache/airflow/issues/46580,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:dynamic-task-mapping', 'AIP-42'), ('affected_version:3.0.0alpha', 'For all 3.0.0 alpha releases')]","[{'comment_id': 2644539300, 'issue_id': 2839615728, 'author': 'vatsrahul1001', 'body': 'Looks like `expand` is not working when list is provided from task\n**Not working**\n`assert_sum(ref_context.expand(num=a_list()), 33)`\n\n**working**\n`assert_sum(ref_context.expand(num=[3, 6, 9]), 33)`', 'created_at': datetime.datetime(2025, 2, 8, 6, 33, 11, tzinfo=datetime.timezone.utc)}]","vatsrahul1001 (Issue Creator) on (2025-02-08 06:33:11 UTC): Looks like `expand` is not working when list is provided from task
**Not working**
`assert_sum(ref_context.expand(num=a_list()), 33)`

**working**
`assert_sum(ref_context.expand(num=[3, 6, 9]), 33)`

"
2838912334,issue,open,,Limit and filters not applied correctly for dag runs API.,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

It seems the `limit` filter is not honored in the dagruns api as below. Though limit 14 is applied all dagruns are returned. This is slightly tricky to reproduce since the limit is passed correctly to API and doesn't work through UI but curl works. Ref https://github.com/apache/airflow/pull/46504#discussion_r1945247863

curl http://localhost:8000/public/dags/tutorial_taskflow_api/dagRuns\?limit\=14


### What you think should happen instead?

_No response_

### How to reproduce

1. Visit the dags page with more than 14 runs and the filter is not honored.
2. curl http://localhost:8000/public/dags/tutorial_taskflow_api/dagRuns\?limit\=14

### Operating System

Ubuntu 20.04.3 LTS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",tirkarthi,2025-02-07 19:18:48+00:00,[],2025-02-09 00:37:20+00:00,,https://github.com/apache/airflow/issues/46572,"[('kind:bug', 'This is a clearly a bug'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2645999623, 'issue_id': 2838912334, 'author': 'insomnes', 'body': 'From your initial PR conversation, I understood you are referencing the part where you extract `total_entries` from the response, and it shows all `total_entries` despite the limit set to 14.\n\nIf so I believe that this number is not affected by pagination limits and offsets, the `total_entries` would be based on count from `filters=` fields. \n\n```python\n    dag_run_select, total_entries = paginated_select(\n        statement=query,\n        filters=[logical_date, start_date_range, end_date_range, update_at_range, state],\n        order_by=order_by,\n        offset=offset,\n        limit=limit,\n        session=session,\n    )\n```\n\nAnd with curl it works as you mention:\n<details>\n\n  <summary>curl run for 2 entries</summary>\n  \n  \n  ```bash\ncurl -X \'GET\' \\\n        \'http://localhost:29091/public/dags/example_bash_decorator/dagRuns?limit=2&order_by=-start_date\' \\\n        -H \'accept: application/json\' | jq\n{\n  ""dag_runs"": [\n    {\n      ""dag_run_id"": ""manual__2025-02-08T23:20:51.665298+00:00"",\n      ""dag_id"": ""example_bash_decorator"",\n      ""logical_date"": ""2025-02-08T23:20:51.673703Z"",\n      ""queued_at"": ""2025-02-08T23:20:51.688648Z"",\n      ""start_date"": ""2025-02-08T23:38:06.692506Z"",\n      ""end_date"": ""2025-02-08T23:38:09.660110Z"",\n      ""data_interval_start"": ""2025-02-08T23:20:51.673703Z"",\n      ""data_interval_end"": ""2025-02-08T23:20:51.673703Z"",\n      ""last_scheduling_decision"": ""2025-02-08T23:38:09.656040Z"",\n      ""run_type"": ""manual"",\n      ""state"": ""failed"",\n      ""external_trigger"": true,\n      ""triggered_by"": ""rest_api"",\n      ""conf"": {},\n      ""note"": null\n    },\n    {\n      ""dag_run_id"": ""manual__2025-02-08T23:04:32.759689+00:00"",\n      ""dag_id"": ""example_bash_decorator"",\n      ""logical_date"": ""2025-02-08T23:04:32.759689Z"",\n      ""queued_at"": ""2025-02-08T23:04:32.784971Z"",\n      ""start_date"": ""2025-02-08T23:04:33.602888Z"",\n      ""end_date"": ""2025-02-08T23:04:36.819802Z"",\n      ""data_interval_start"": ""2025-02-08T23:04:32.759689Z"",\n      ""data_interval_end"": ""2025-02-08T23:04:32.759689Z"",\n      ""last_scheduling_decision"": ""2025-02-08T23:04:36.817639Z"",\n      ""run_type"": ""manual"",\n      ""state"": ""failed"",\n      ""external_trigger"": true,\n      ""triggered_by"": ""ui"",\n      ""conf"": {},\n      ""note"": null\n    }\n  ],\n  ""total_entries"": 127\n}\n  ```\n  \n</details>\n\nOr the problem is that API returns the wrong number of entries in `dag_runs` field?', 'created_at': datetime.datetime(2025, 2, 9, 0, 32, 10, tzinfo=datetime.timezone.utc)}]","insomnes on (2025-02-09 00:32:10 UTC): From your initial PR conversation, I understood you are referencing the part where you extract `total_entries` from the response, and it shows all `total_entries` despite the limit set to 14.

If so I believe that this number is not affected by pagination limits and offsets, the `total_entries` would be based on count from `filters=` fields. 

```python
    dag_run_select, total_entries = paginated_select(
        statement=query,
        filters=[logical_date, start_date_range, end_date_range, update_at_range, state],
        order_by=order_by,
        offset=offset,
        limit=limit,
        session=session,
    )
```

And with curl it works as you mention:
<details>

  <summary>curl run for 2 entries</summary>
  
  
  ```bash
curl -X 'GET' \
        'http://localhost:29091/public/dags/example_bash_decorator/dagRuns?limit=2&order_by=-start_date' \
        -H 'accept: application/json' | jq
{
  ""dag_runs"": [
    {
      ""dag_run_id"": ""manual__2025-02-08T23:20:51.665298+00:00"",
      ""dag_id"": ""example_bash_decorator"",
      ""logical_date"": ""2025-02-08T23:20:51.673703Z"",
      ""queued_at"": ""2025-02-08T23:20:51.688648Z"",
      ""start_date"": ""2025-02-08T23:38:06.692506Z"",
      ""end_date"": ""2025-02-08T23:38:09.660110Z"",
      ""data_interval_start"": ""2025-02-08T23:20:51.673703Z"",
      ""data_interval_end"": ""2025-02-08T23:20:51.673703Z"",
      ""last_scheduling_decision"": ""2025-02-08T23:38:09.656040Z"",
      ""run_type"": ""manual"",
      ""state"": ""failed"",
      ""external_trigger"": true,
      ""triggered_by"": ""rest_api"",
      ""conf"": {},
      ""note"": null
    },
    {
      ""dag_run_id"": ""manual__2025-02-08T23:04:32.759689+00:00"",
      ""dag_id"": ""example_bash_decorator"",
      ""logical_date"": ""2025-02-08T23:04:32.759689Z"",
      ""queued_at"": ""2025-02-08T23:04:32.784971Z"",
      ""start_date"": ""2025-02-08T23:04:33.602888Z"",
      ""end_date"": ""2025-02-08T23:04:36.819802Z"",
      ""data_interval_start"": ""2025-02-08T23:04:32.759689Z"",
      ""data_interval_end"": ""2025-02-08T23:04:32.759689Z"",
      ""last_scheduling_decision"": ""2025-02-08T23:04:36.817639Z"",
      ""run_type"": ""manual"",
      ""state"": ""failed"",
      ""external_trigger"": true,
      ""triggered_by"": ""ui"",
      ""conf"": {},
      ""note"": null
    }
  ],
  ""total_entries"": 127
}
  ```
  
</details>

Or the problem is that API returns the wrong number of entries in `dag_runs` field?

"
2838895768,issue,closed,completed,Fastapi crash on accessing task instance whose dagrun is in queued state,"### Apache Airflow version

3.0.0a1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

On trying to access a task instance page with a dagrun in queued state I am getting below error. `start_date` can be None for dagrun in queued state like `end_date` and the type needs to be changed. The files have to be regenerated. I tried doing this but getting unrelated change on running `uv run --group codegen --project apache-airflow-task-sdk --directory task_sdk datamodel-codegen`

https://github.com/apache/airflow/blob/45d37da572ead522d9fa64889ad5d12fedfd1a88/airflow/api_fastapi/execution_api/datamodels/taskinstance.py#L228-L229

http://localhost:8000/webapp/dags/read_asset_event_from_classic/runs/manual__2025-02-07T18:01:29.649946+00:00/tasks/read_asset_event_from_classic

```
[2025-02-08T00:30:05.038+0530] {httptools_impl.py:414} ERROR - Exception in ASGI application
  + Exception Group Traceback (most recent call last):
  |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/starlette/_utils.py"", line 76, in collapse_excgroups
  |     yield
  |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/starlette/middleware/base.py"", line 178, in __call__
  |     async with anyio.create_task_group() as task_group:
  |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py"", line 767, in __aexit__
  |     raise BaseExceptionGroup(
  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)
  +-+---------------- 1 ----------------
    | Traceback (most recent call last):
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py"", line 409, in run_asgi
    |     result = await app(  # type: ignore[func-returns-value]
    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py"", line 60, in __call__
    |     return await self.app(scope, receive, send)
    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/fastapi/applications.py"", line 1054, in __call__
    |     await super().__call__(scope, receive, send)
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/starlette/applications.py"", line 112, in __call__
    |     await self.middleware_stack(scope, receive, send)
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py"", line 187, in __call__
    |     raise exc
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py"", line 165, in __call__
    |     await self.app(scope, receive, _send)
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/starlette/middleware/gzip.py"", line 20, in __call__
    |     await responder(scope, receive, send)
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/starlette/middleware/gzip.py"", line 39, in __call__
    |     await self.app(scope, receive, self.send_with_gzip)
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py"", line 85, in __call__
    |     await self.app(scope, receive, send)
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/starlette/middleware/base.py"", line 177, in __call__
    |     with recv_stream, send_stream, collapse_excgroups():
    |   File ""/usr/lib/python3.11/contextlib.py"", line 158, in __exit__
    |     self.gen.throw(typ, value, traceback)
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/starlette/_utils.py"", line 82, in collapse_excgroups
    |     raise exc
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/starlette/middleware/base.py"", line 179, in __call__
    |     response = await self.dispatch_func(request, call_next)
    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/home/karthikeyan/stuff/python/airflow/airflow/api_fastapi/core_api/middleware.py"", line 28, in dispatch
    |     response = await call_next(request)
    |                ^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/starlette/middleware/base.py"", line 154, in call_next
    |     raise app_exc
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/starlette/middleware/base.py"", line 141, in coro
    |     await self.app(scope, receive_or_disconnect, send_no_error)
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/starlette/middleware/exceptions.py"", line 62, in __call__
    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py"", line 53, in wrapped_app
    |     raise exc
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py"", line 42, in wrapped_app
    |     await app(scope, receive, sender)
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/starlette/routing.py"", line 715, in __call__
    |     await self.middleware_stack(scope, receive, send)
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/starlette/routing.py"", line 735, in app
    |     await route.handle(scope, receive, send)
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/starlette/routing.py"", line 288, in handle
    |     await self.app(scope, receive, send)
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/starlette/routing.py"", line 76, in app
    |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py"", line 53, in wrapped_app
    |     raise exc
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py"", line 42, in wrapped_app
    |     await app(scope, receive, sender)
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/starlette/routing.py"", line 73, in app
    |     response = await f(request)
    |                ^^^^^^^^^^^^^^^^
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/fastapi/routing.py"", line 301, in app
    |     raw_response = await run_endpoint_function(
    |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/fastapi/routing.py"", line 214, in run_endpoint_function
    |     return await run_in_threadpool(dependant.call, **values)
    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/starlette/concurrency.py"", line 37, in run_in_threadpool
    |     return await anyio.to_thread.run_sync(func)
    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/anyio/to_thread.py"", line 56, in run_sync
    |     return await get_async_backend().run_sync_in_worker_thread(
    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py"", line 2461, in run_sync_in_worker_thread
    |     return await future
    |            ^^^^^^^^^^^^
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py"", line 962, in run
    |     result = context.run(func, *args)
    |              ^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/home/karthikeyan/stuff/python/airflow/airflow/api_fastapi/core_api/routes/public/log.py"", line 140, in get_log
    |     logs, metadata = task_log_reader.read_log_chunks(ti, try_number, metadata)
    |                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/home/karthikeyan/stuff/python/airflow/airflow/utils/log/log_reader.py"", line 65, in read_log_chunks
    |     logs, metadatas = self.log_handler.read(ti, try_number, metadata=metadata)
    |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/home/karthikeyan/stuff/python/airflow/airflow/utils/log/file_task_handler.py"", line 488, in read
    |     log, out_metadata = self._read(task_instance, try_number_element, metadata)
    |                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/home/karthikeyan/stuff/python/airflow/airflow/utils/log/file_task_handler.py"", line 369, in _read
    |     worker_log_rel_path = self._render_filename(ti, try_number)
    |                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/home/karthikeyan/stuff/python/airflow/airflow/utils/session.py"", line 101, in wrapper
    |     return func(*args, session=session, **kwargs)
    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/home/karthikeyan/stuff/python/airflow/airflow/utils/log/file_task_handler.py"", line 277, in _render_filename
    |     context = ti.get_template_context(session=session)
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/home/karthikeyan/stuff/python/airflow/airflow/models/taskinstance.py"", line 3212, in get_template_context
    |     return _get_template_context(
    |            ^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/home/karthikeyan/stuff/python/airflow/airflow/models/taskinstance.py"", line 926, in _get_template_context
    |     dag_run=DagRunSDK.model_validate(dag_run, from_attributes=True),
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/home/karthikeyan/stuff/python/airflow/.venv/lib/python3.11/site-packages/pydantic/main.py"", line 627, in model_validate
    |     return cls.__pydantic_validator__.validate_python(
    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    | pydantic_core._pydantic_core.ValidationError: 1 validation error for DagRun
    | start_date
    |   Input should be a valid datetime [type=datetime_type, input_value=None, input_type=NoneType]
    |     For further information visit https://errors.pydantic.dev/2.10/v/datetime_type
    +------------------------------------
```

### What you think should happen instead?

_No response_

### How to reproduce

1. Stop the scheduler.
2. Start a dagrun from UI.
3. Visit the task instance page of the dagrun in queued state.

### Operating System

Ubuntu 20.04.3

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",tirkarthi,2025-02-07 19:08:51+00:00,[],2025-02-07 19:23:11+00:00,2025-02-07 19:23:10+00:00,https://github.com/apache/airflow/issues/46570,"[('kind:bug', 'This is a clearly a bug'), ('area:API', ""Airflow's REST/HTTP API""), ('area:core', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK'), ('area:task-sdk', None), ('affected_version:3.0.0alpha', 'For all 3.0.0 alpha releases')]","[{'comment_id': 2643831967, 'issue_id': 2838895768, 'author': 'tirkarthi', 'body': 'Sorry, duplicate of https://github.com/apache/airflow/issues/46487', 'created_at': datetime.datetime(2025, 2, 7, 19, 23, 10, tzinfo=datetime.timezone.utc)}]","tirkarthi (Issue Creator) on (2025-02-07 19:23:10 UTC): Sorry, duplicate of https://github.com/apache/airflow/issues/46487

"
2838858214,issue,open,,Add validation for serverless job to check for environment_key in the job definition,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

https://github.com/apache/airflow/issues/45138 introduced support for serverless job. It also added a new parameter environment to be set for serverless jobs. In addition to this, the job definition should also contain the environment_key as without that the serverless job submission will fail

### What you think should happen instead?

Add validation logic to check:
 - if the submission is for serverless job execution
 - check the presence of environment_key in the job definition and raise error if not present
 - for non serverless job no check required

### How to reproduce

submit a databricks task operator to run serverless job (not passing cluster_key or existing_cluster_id but passing environment).
In the job definition, leave the environment_key empty and submit, the job should fail

### Operating System

macos

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",HariGS-DB,2025-02-07 18:52:08+00:00,[],2025-02-07 18:54:24+00:00,,https://github.com/apache/airflow/issues/46569,"[('kind:bug', 'This is a clearly a bug'), ('kind:feature', 'Feature Requests'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2838509641,issue,open,,AIP-66 Refactor DagRun to DagVersion association,"## Context
A DagRun can have multiple DagVersions associated. The reason for that is because the DagVersion can change in between tasks and even in between task tries.

To retrieve the DagVersions associated to a particular DagRun, we should explore the TI (current version) and TIH (history version for tries) and aggregate those.

Ideally this should be accessible through a property / association_proxy / relationship on the DagRun ORM instance so we can easily do `DagRun.dag_versions` to retrieve all of the related ORM DagVersion objects. (In addition we should be able to specify if we want to eagerload or lazyload that attribute).

The direct link between DagRun and DagVersion (via the FK) needs to be deleted.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2025-02-07 15:58:16+00:00,['ephraimbuddy'],2025-02-07 16:01:19+00:00,,https://github.com/apache/airflow/issues/46565,"[('kind:meta', 'High-level information important to the community'), ('area:core', ''), ('AIP-66: DAG Bundle/Manifest', '')]",[],
2838482115,issue,open,,Jinja templating capability is not working for the parameter (additional args) in spark job submit for DatabricksWorkflowTaskGroup,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

While creating workflow in Databricks using `DatabricksWorkflowTaskGroup`, I have a situation to pass some parameters to spark submit which are passed as jinja template. 

Sample task creation code with DatabricksWorkflowTaskGroup,
```
databricks_workflow = DatabricksWorkflowTaskGroup(
            group_id=""test_wf"",
            databricks_conn_id=DATABRICKS_CONNECTION_ID,
            add_suffix_on_collision=True,
            job_clusters=job_clusters_spec,
        )
        
with databricks_workflow:
    task1 = build_task_1(dag)
    task2 = build_task_2(dag)
    task1 >> task2
```

From the above sample workflow creation code, `build_task_1` and `build_task_2` returns task which is my custom build operator on extending `DatabricksTaskBaseOperator`.

Sample code for custom build operator,

```
class CustomDatabricksWorkflowTaskOperator(DatabricksTaskBaseOperator):
    """"""Custom operator wrapping DatabricksTaskOperator""""""

    template_fields = (
        ""application_args"",
        ""spark_conf"",
        ""driver_jvm_props"",
    )

    def __init__(
            self,
            *args,
            class_name: str = """",
            spark_conf: dict[str, str] | str = None,
            application_args: List[str] | str | None = None,
            driver_jvm_props: dict[str, str] | None = None,
            application_conf_file: str = ""test.conf"",
            libraries: List[dict[str, str]] = None,
            **kwargs,
    ):
        self.class_name = class_name
        self.application_conf_file = application_conf_file
        self.library = libraries if libraries is not None else _get_default_libraries()
        self.application_args = application_args
        super().__init__(*args, **kwargs)

        self.driver_jvm_props = driver_jvm_props
        self.spark_conf = spark_cone

    def _get_task_base_json(self) -> dict[str, Any]:
         return {
            ""run_if"": self.databricks_trigger_rule,
            'spark_jar_task': {
                'main_class_name': self.class_name,
                parameters"": [
                     self.application_args,
                     ""{{ get_test_template_value(ti,'test_key') }}"" // I have hard-coded here for ref. In ideal case, this comes as part of self.application_args which has been added in template_fields
               ] 
            },
            'libraries': self.library,
        }

    def execute(self, context):
        super().execute(context)
```


From the above code sample,  `{{ get_test_template_value(ti,'test_key') }}` is not getting resolved at the time of `launch` task from DatabricksWorkflowTaskGroup. But the same value got resolved at the time of task execution in airflow.

Sample json from `launch` task:
```
""tasks"": [
    {
      ""task_key"": ""task1"",
      ""depends_on"": [],
      ""run_if"": ""NONE_FAILED"",
      ""spark_jar_task"": {
        ""main_class_name"": ""com.sample.task1"",
        ""parameters"": [
          ""app.conf"",
          ""{{ get_test_template_value(ti,'test_key') }}""
        ]
      },
      ""libraries"": [
        {
          ""jar"": ""test.jar""
        }
      ],
      ""job_cluster_key"": ""job_cluster""
    },
```
Note: `get_test_template_value` function is added as a macro while DAG creation. 

Though I have included the `application_args` as `template_fields` section in the CustomDatabricksWorkflowTaskOperator, it is unable to resolve the template.

### What you think should happen instead?

Expectation is to jinja template resolution should happen while the launch tasks trigger API requests to Databricks to create the workflow. But since the template resolution is happening lazily here i.e., at the time of task execution in airflow which is late in this case as the respective task is already created in Databricks at the time of `launch` and Databricks doesn't have to way to get the template resolved value.


### How to reproduce

1. Create a DAG with `DatabricksWorkflowTaskGroup`
2. Add couple of tasks 
3. Create a custom Databricks Task operator by extending `DatabricksTaskBaseOperator`
4. Add one of the constructor parameter in `template_fields` section which needs to be tested
5. Override `_get_task_base_json` method to return a templated field/value as of the return dictionary
6. Trigger DAG
7. In launch task, you can notice from logs that the parameter property will pass the string as-is without resolving the actual value.
8. In the actual task execution, the actual value can be seen in the rendered template section.

### Operating System

PRETTY_NAME=""Debian GNU/Linux 12 (bookworm)"", NAME=""Debian GNU/Linux""

### Versions of Apache Airflow Providers

NA

### Deployment

Other Docker-based deployment

### Deployment details

We are building our own docker image with base image as `apache/airflow:2.10.4-python3.10` and use the same for deployment.

### Anything else?

Other approaches which I tried inside `_get_task_base_json` method of the custom operation which is responsible for returning the task JSON. 

1. Tried to force the render by using `super().render_template(self.application_args, context)` and `super().render_template_fields(context)` which unfortunately was not possible as the context was not part of the `_get_task_base_json` method scope.
2. As this is an abstract method and I had to override, unable to pass context object to the step1. 

Due to the above constraint, render templating as well as pulling from xcom also is not supported. (In other words, all the capabilities which we get from using context object is not possible).

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",karthi-keyan-n,2025-02-07 15:45:05+00:00,[],2025-02-07 15:47:27+00:00,,https://github.com/apache/airflow/issues/46563,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('area:core', ''), ('provider:databricks', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2643286467, 'issue_id': 2838482115, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 2, 7, 15, 45, 10, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-02-07 15:45:10 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2838191096,issue,open,,RdsExportTaskExistenceSensor Run indeterminate if snapshot export failed,"### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.10.0


### Apache Airflow version

2.7.3

### Operating System

Composer 2.8.5

### Deployment

Google Cloud Composer

### Deployment details

_No response_

### What happened

A few weeks ago, we encountered a problem while creating an RDS snapshot. Something changed in the validation of the snapshot configuration inputs, and it now checks the **s3_prefix** name.

### How We Discovered It ? 

We have several DAGs that create RDS database snapshots and export them to S3. On one fateful day (2025-01-24), we ran into a problem because all these DAGs, which run after hours, were stuck in the RdsExportTaskExistenceSensor (with no timeout defined).  Upon checking the AWS console, we saw that the exports had failed. We were using s3_prefix=""snapshots/{{ts_nodash}}/"" with a trailing slash, and this caused the failure, as shown in the screenshot.

![Image](https://github.com/user-attachments/assets/863e8678-dc5c-4beb-8fc2-b76f39f03aa6)

After updating s3_prefix removing the ended backslash, it work as expected.
```python
        - s3_prefix=""snapshots/{{ts_nodash}}/"",
        + s3_prefix=""snapshots/{{ts_nodash}}"",
```

### What you think should happen instead

RdsExportTaskExistenceSensor should raise and exception if an error statuses happens.

### How to reproduce

Our DAG uses the following logic

RdsCreateDbSnapshotOperator >> RdsSnapshotExistenceSensor >> RdsStartExportTaskOperator >> RdsExportTaskCompleteSensor >> .....

if **s3_prefix** in RdsStartExportTaskOperator has trailing backslash it will faill and RdsExportTaskCompleteSensor will not complete and  Run indeterminate if timeout is not defined.

### Anything else

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vargacypher,2025-02-07 13:40:14+00:00,['vargacypher'],2025-02-08 17:30:47+00:00,,https://github.com/apache/airflow/issues/46555,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('good first issue', '')]","[{'comment_id': 2644521605, 'issue_id': 2838191096, 'author': 'manojks1999', 'body': '@vargacypher can I work on this ?', 'created_at': datetime.datetime(2025, 2, 8, 5, 44, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2644825669, 'issue_id': 2838191096, 'author': 'vargacypher', 'body': ""I've  already had some work done here that adjust that.\n\nI'll will submit a PR today."", 'created_at': datetime.datetime(2025, 2, 8, 10, 22, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2645754438, 'issue_id': 2838191096, 'author': 'manya5', 'body': '""Hi, I am new to Apache Airflow and GSoC. I want to work on this issue. Could you please assign it to me?""', 'created_at': datetime.datetime(2025, 2, 8, 14, 56, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2645864792, 'issue_id': 2838191096, 'author': 'potiuk', 'body': '> ""Hi, I am new to Apache Airflow and GSoC. I want to work on this issue. Could you please assign it to me?""\n\nLook above: @vargacypher is going to submit a fix soon it looks - but You can contribute by reviewing the PR and testing it.', 'created_at': datetime.datetime(2025, 2, 8, 17, 30, 46, tzinfo=datetime.timezone.utc)}]","manojks1999 on (2025-02-08 05:44:38 UTC): @vargacypher can I work on this ?

vargacypher (Issue Creator) on (2025-02-08 10:22:53 UTC): I've  already had some work done here that adjust that.

I'll will submit a PR today.

manya5 on (2025-02-08 14:56:13 UTC): ""Hi, I am new to Apache Airflow and GSoC. I want to work on this issue. Could you please assign it to me?""

potiuk on (2025-02-08 17:30:46 UTC): Look above: @vargacypher is going to submit a fix soon it looks - but You can contribute by reviewing the PR and testing it.

"
2837879358,issue,closed,completed,Airflow 2.10.4 is significantly slower compared to 2.9.3,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Describe the slowness and when it occurs (e.g., loading the web UI, DAG execution, scheduler performance).  I have huge dag which was working very fine but now it is not loading and taking more time to load after i upgrade. I have changed nothing in the code.

### What you think should happen instead?

Airflow should perform similarly or better after the upgrade.


### How to reproduce

upgrade from `2.9.2` to `2.10.4`

### Operating System

linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Komalpawar22,2025-02-07 11:05:13+00:00,[],2025-02-07 12:23:50+00:00,2025-02-07 12:23:50+00:00,https://github.com/apache/airflow/issues/46551,"[('kind:bug', 'This is a clearly a bug'), ('area:performance', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2837854728,issue,open,,AIP-38 AIP-38 Allow specifying injectable server url (XHR Requests),"### Body

Related to: https://github.com/apache/airflow/issues/46548

We need a way to dynamically set the base url for the API requests.

Similarly to airflow 2 we can override the `OpenAPI.BASE` config based on a meta tag value. Something like this (if the meta tag value is not None):
```js
OpenAPI.BASE = getFromMetaTag();
```

Then the user can simply edit the meta tag in the production bundle to inject the desired server url, e.g:
```
<meta name=""backend-server-base-url"" content=""<your_host>/1234567"">
```

We can also check if the host is mandatory or if a path is enough to default to localhost, for instance if the user sets:
 ```
<meta name=""backend-server-base-url"" content=""/1234567"">
```
Ideally both use cases are supported (with and without host specified)


### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2025-02-07 10:52:58+00:00,[],2025-02-07 16:27:17+00:00,,https://github.com/apache/airflow/issues/46550,"[('area:API', ""Airflow's REST/HTTP API""), ('kind:meta', 'High-level information important to the community'), ('AIP-38', 'Modern Web Application'), ('affected_version:3.0.0alpha', 'For all 3.0.0 alpha releases')]",[],
2837837629,issue,open,,[Databricks provider] Add `deferrable` support to `DatabricksSqlOperator`,"### Description

Allow the `DatabricksSqlOperator` to be run using `deferrable` mode.

### Use case/motivation

I want to execute SQL code in a Databricks SQL endpoint or cluster using the `DatabricksSqlOperator` in deferrable mode.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",tatiana,2025-02-07 10:44:39+00:00,[],2025-02-09 06:41:32+00:00,,https://github.com/apache/airflow/issues/46549,"[('kind:feature', 'Feature Requests'), ('area:async-operators', 'AIP-40: Deferrable (""Async"") Operators'), ('provider:databricks', '')]",[],
2837817460,issue,open,,AIP-38 Allow specifying injectable base_url for the UI (Static files),"### Body

Some users might be deploying the UI to different locations and might need to change the base url dynamically (without rebuilding).

It would be great to be able to dynamically specify the base_url.

We can look at the vite experimental feature `renderBuiltUrl`. More information here:
https://stackoverflow.com/a/75238826


### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2025-02-07 10:35:09+00:00,[],2025-02-07 17:19:59+00:00,,https://github.com/apache/airflow/issues/46548,"[('kind:meta', 'High-level information important to the community'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application'), ('affected_version:3.0.0alpha', 'For all 3.0.0 alpha releases')]","[{'comment_id': 2643224844, 'issue_id': 2837817460, 'author': 'bbovenzi', 'body': 'Would we read the dynamic base url from a meta tag like in the other issue?', 'created_at': datetime.datetime(2025, 2, 7, 15, 17, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2643537063, 'issue_id': 2837817460, 'author': 'pierrejeambrun', 'body': 'In this particular instance I think we can get it from the page location. (Guess it based on the hosted path should be enough, but otherwise we can consider a meta tag as well)', 'created_at': datetime.datetime(2025, 2, 7, 17, 19, 57, tzinfo=datetime.timezone.utc)}]","bbovenzi on (2025-02-07 15:17:57 UTC): Would we read the dynamic base url from a meta tag like in the other issue?

pierrejeambrun (Issue Creator) on (2025-02-07 17:19:57 UTC): In this particular instance I think we can get it from the page location. (Guess it based on the hosted path should be enough, but otherwise we can consider a meta tag as well)

"
2837794722,issue,open,,Bug Report: Incorrect Log URL in Airflow 2.10.4,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Apache Airflow Version: 2.10.4
Previous Version (Worked as Expected): 2.9.3
Environment: Provide relevant details (e.g., deployment method, OS, executor type)

Description
After upgrading Airflow from 2.9.3 to 2.10.4, I observed that the log_url function is generating incorrect URLs. The returned URL does not match the log view page URL in the UI.
I am using Slack notifications; it was working fine until yesterday, but now the log URL redirects to the wrong URL path.
 using : ""url"": task_instance.log_url.replace(""localhost:8080"", ""prod.host.com"")
`airflow.models.taskinstance.TaskInstance => @property def log_url(self) -> str`

incorrect: https://host/dags/dag_id/grid?dag_run_id=manual__2025-02-07T03%3A25%3A19.637151%2B00%3A00&task_id=xyz&base_date=2025-02-07T03%3A25%3A19%2B0000&tab=logs

It should be:
https://host/dags/dag_id/grid?tab=logs&dag_run_id=manual__2025-02-07T03%3A25%3A19.637151%2B00%3A00&task_id=xyz

Would appreciate any insights from the maintainers on this issue. Thanks!

### What you think should happen instead?

https://host/dags/dag_id/grid?tab=logs&dag_run_id=manual__2025-02-07T03%3A25%3A19.637151%2B00%3A00&task_id=xyz

NOTE: Please verify once.

### How to reproduce

- Upgrade Airflow from 2.9.3 to 2.10.4.
- Retrieve log_url from Airflow task instance metadata.
- Compare the generated URL with the actual log view page in the UI.

### Operating System

Linux

### Versions of Apache Airflow Providers

2.10.4

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Komalpawar22,2025-02-07 10:24:49+00:00,[],2025-02-09 06:48:02+00:00,,https://github.com/apache/airflow/issues/46547,"[('kind:bug', 'This is a clearly a bug'), ('area:logging', ''), ('area:core', ''), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2642523504, 'issue_id': 2837794722, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 2, 7, 10, 24, 52, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-02-07 10:24:52 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2836836553,issue,closed,completed,401 Error when trying to trigger Airbyte Sync,"### Apache Airflow Provider(s)

airbyte

### Versions of Apache Airflow Providers

apache-airflow-providers-airbyte | 5.0.0



### Apache Airflow version

2.10.4

### Operating System

MacOS (local deployment via docker)

### Deployment

Docker-Compose

### Deployment details

_No response_

### What happened

When making a sync call using the AirbyteTriggerSyncOperator. I am receiving a 401 error but manual API calls work (to token URL)

During my manual testing i found without the content type header = application/json i would get a 401.

Is the call made in this provider done without this header as it will not work otherwise?

### What you think should happen instead

Should get a successful status code back

### How to reproduce

try to make a call to airbyte to trigger sync using the Aitbyte connection type with client id and secret

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",gmorrell,2025-02-06 23:29:16+00:00,[],2025-02-07 00:01:23+00:00,2025-02-07 00:01:23+00:00,https://github.com/apache/airflow/issues/46542,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:airbyte', '')]","[{'comment_id': 2641346270, 'issue_id': 2836836553, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 2, 6, 23, 29, 19, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-02-06 23:29:19 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2836374846,issue,open,,"With `useNavigate`, we can navigate through routes like `/webapp`, automatically using the correct domain.","### Description

When running code in Gitpod, we encountered an issue.

For example, when clicking on ""Check Out New UI,"" it redirects us to localhost:29091 instead of using the Gitpod port domain/route as expected

### Use case/motivation

for easy traversing for url When running code in Gitpod, we face an issue where navigation does not use the correct domain.

What Would You Like to Happen?

Ensure navigation within the application correctly adapts to Gitpod's port-based domain instead of defaulting to localhost.
When using useNavigate, routes like /webapp should automatically use the Gitpod domain without requiring manual adjustments.
Improve the development experience by preventing incorrect redirects and ensuring seamless navigation.

### Related issues

_No response_

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",aditya0yadav,2025-02-06 19:17:40+00:00,['aditya0yadav'],2025-02-06 22:15:47+00:00,,https://github.com/apache/airflow/issues/46533,"[('kind:feature', 'Feature Requests'), ('good first issue', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('affected_version:3.0.0alpha', 'For all 3.0.0 alpha releases')]","[{'comment_id': 2640897002, 'issue_id': 2836374846, 'author': 'potiuk', 'body': 'Using Chat GPT to create issues. Be careful, It creates far too chatty issues.', 'created_at': datetime.datetime(2025, 2, 6, 20, 12, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2640897749, 'issue_id': 2836374846, 'author': 'potiuk', 'body': 'But yes if you want to look at it - feel free.', 'created_at': datetime.datetime(2025, 2, 6, 20, 12, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2640898223, 'issue_id': 2836374846, 'author': 'potiuk', 'body': 'assigned you', 'created_at': datetime.datetime(2025, 2, 6, 20, 12, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2640902809, 'issue_id': 2836374846, 'author': 'aditya0yadav', 'body': '> Using Chat GPT to create issues. Be careful, It creates far too chatty issues.\n\nReal version have a lot of alphabetical mistake\nSo I will be careful next time \n\nThanks', 'created_at': datetime.datetime(2025, 2, 6, 20, 15, 9, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-02-06 20:12:03 UTC): Using Chat GPT to create issues. Be careful, It creates far too chatty issues.

potiuk on (2025-02-06 20:12:27 UTC): But yes if you want to look at it - feel free.

potiuk on (2025-02-06 20:12:41 UTC): assigned you

aditya0yadav (Issue Creator) on (2025-02-06 20:15:09 UTC): Real version have a lot of alphabetical mistake
So I will be careful next time 

Thanks

"
2836049447,issue,open,,Template job specs created from DatabricksWorklfowTaskGroup to use Jinja rendering,"### Description

Allow passing jinja syntax in task_config parameters of DatabricksTaskOperator that gets rendered at execution time.

### Use case/motivation

I should be able to make full use of Airflow context variables in a Databricks Job Run to ensure idempotency of my DAG such as `logical_date` or other `params` in case of occasional manual runs.

### Related issues

It is linked with https://github.com/apache/airflow/issues/42438 but this issue was never finished as the changes were not merged https://github.com/apache/airflow/pull/45066

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",arollet-decathlon,2025-02-06 16:42:19+00:00,[],2025-02-06 16:44:39+00:00,,https://github.com/apache/airflow/issues/46529,"[('area:providers', ''), ('kind:feature', 'Feature Requests'), ('provider:databricks', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2835952648,issue,open,,Clear tasks in a time range on UI,"### Description

Able to clear specific tasks in a DAG (selecting with checkboxes from the available ones), on a specified time range, input by hand in date/time input boxes. 
It would be event better if we could set the state not just to ""none"" (cleared), but to ""skipped"", ""failed"", ""success"", etc.

### Use case/motivation

I would like to be able to clear a given time range for specific tasks in a DAG.
Similar to the CLI command:
`airflow tasks clear -d -s <start_dat> -e <end_date> -t <task_regex> dag_id`

But configured on the UI.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Sly1024,2025-02-06 16:06:32+00:00,[],2025-02-06 16:08:50+00:00,,https://github.com/apache/airflow/issues/46526,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2640265539, 'issue_id': 2835952648, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 2, 6, 16, 6, 34, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-02-06 16:06:34 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2835437037,issue,open,,Backfill dry run CLI command is giving error,"### Apache Airflow version

3.0.0a1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Getting error when running backfill dry run CLI command `airflow backfill create --dag-id <dag_id> --dry-run --from-date <start_date> --to-date <end_date>`

Error:
```python
airflow backfill create --dag-id test_api_dag --dry-run --from-date 2025-02-01 --to-date 2025-02-06                        
Performing dry run of backfill.
Printing params:
    - dag_id = test_api_dag
    - from_date = 2025-02-01 00:00:00+00:00
    - to_date = 2025-02-06 00:00:00+00:00
    - max_active_runs = None
    - reverse = False
    - dag_run_conf = None
    - reprocess_behavior = None
Traceback (most recent call last):
  File ""/usr/local/bin/airflow"", line 10, in <module>
    sys.exit(main())
  File ""/opt/airflow/airflow/__main__.py"", line 58, in main
    args.func(args)
  File ""/opt/airflow/airflow/cli/cli_config.py"", line 49, in command
    return func(*args, **kwargs)
  File ""/opt/airflow/airflow/utils/cli.py"", line 111, in wrapper
    return f(*args, **kwargs)
  File ""/opt/airflow/airflow/utils/providers_configuration_loader.py"", line 55, in wrapped_function
    return func(*args, **kwargs)
  File ""/opt/airflow/airflow/cli/commands/remote_commands/backfill_command.py"", line 62, in create_backfill
    reverse=args.reverse,
AttributeError: 'Namespace' object has no attribute 'reverse'
``` 

### What you think should happen instead?

Command should work fine and return the backfills supposed to be created.

### How to reproduce

Steps:

1. Checkout to tag 3.0.0a1 and start airflow via breeze and have a test dag.
2. Run the backfill create command with --dry-run attribute.

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",atul-astronomer,2025-02-06 12:45:34+00:00,['vatsrahul1001'],2025-02-06 22:16:45+00:00,,https://github.com/apache/airflow/issues/46520,"[('kind:bug', 'This is a clearly a bug'), ('area:CLI', ''), ('area:core', ''), ('area:backfill', 'Specifically for backfill related'), ('affected_version:3.0.0alpha', 'For all 3.0.0 alpha releases')]",[],
2835380471,issue,open,,Legacy configuration not removed or updated,"### Apache Airflow version

3.0.0a1

### If ""Other Airflow 2 version"" selected, which one?

3.0.0a1

### What happened?

After running `airflow config lint`, we can still find the following config have not yet been migrated.


```
Found issues in your airflow.cfg:
  - Removed deprecated `cookie_samesite` configuration parameter from `webserver` section.
  - `dag_dir_list_interval` configuration parameter moved from `scheduler` section to `dag_processor` section as `refresh_interval`.

Please update your configuration file accordingly.
```

### What you think should happen instead?

`airflow config lint` should not raise any error.

### How to reproduce

```
uv venv --python 3.12

uv pip install \
    --find-links https://dist.apache.org/repos/dist/dev/airflow/3.0.0a1/ \
    apache-airflow==3.0.0a1 \
    apache-airflow-task-sdk==1.0.0a1 \
    apache-airflow-providers-standard==0.1.0a1 \
    apache-airflow-providers-fab==2.0.0a1 \
    apache-airflow-providers-celery==3.11.0a1
export AIRFLOW_HOME=`pwd`

uv run airflow db migrate
uv run airflow standalone

uv run airflow config lint
```

### Operating System

macOS 15.3

### Versions of Apache Airflow Providers

apache-airflow-providers-celery==3.11.0a1
apache-airflow-providers-common-compat==1.3.0
apache-airflow-providers-common-io==1.5.0
apache-airflow-providers-common-sql==1.21.0
apache-airflow-providers-fab==2.0.0a1
apache-airflow-providers-ftp==3.12.0
apache-airflow-providers-http==5.0.0
apache-airflow-providers-imap==3.8.0
apache-airflow-providers-smtp==1.9.0
apache-airflow-providers-sqlite==4.0.0
apache-airflow-providers-standard==0.1.0a1


### Deployment

Other

### Deployment details

`airflow standalone`

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2025-02-06 12:19:39+00:00,['jason810496'],2025-02-07 03:39:21+00:00,,https://github.com/apache/airflow/issues/46517,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:core', ''), ('affected_version:3.0.0alpha', 'For all 3.0.0 alpha releases')]","[{'comment_id': 2639691751, 'issue_id': 2835380471, 'author': 'jason810496', 'body': 'Hi @Lee-W, I can take this issue !', 'created_at': datetime.datetime(2025, 2, 6, 12, 27, 43, tzinfo=datetime.timezone.utc)}]","jason810496 (Assginee) on (2025-02-06 12:27:43 UTC): Hi @Lee-W, I can take this issue !

"
2835371496,issue,closed,completed,"""Legacy UI"" button in New UI does not work","### Apache Airflow version

3.0.0a1

### If ""Other Airflow 2 version"" selected, which one?

3.0.0a1

### What happened?

""Legacy UI"" button in New UI does not work (I think it links to 28080 instead of 8080)

### What you think should happen instead?

It should go back to the old UI

### How to reproduce


```sh
uv venv --python 3.12

uv pip install \
    --find-links https://dist.apache.org/repos/dist/dev/airflow/3.0.0a1/ \
    apache-airflow==3.0.0a1 \
    apache-airflow-task-sdk==1.0.0a1 \
    apache-airflow-providers-standard==0.1.0a1 \
    apache-airflow-providers-fab==2.0.0a1 \
    apache-airflow-providers-celery==3.11.0a1
export AIRFLOW_HOME=`pwd`

uv run airflow db migrate
uv run airflow standalone
```

1. Open `http://localhost:9091`
2. Click the ""Legacy UI"" button


### Operating System

macOS 15.3

### Versions of Apache Airflow Providers

apache-airflow-providers-celery==3.11.0a1
apache-airflow-providers-common-compat==1.3.0
apache-airflow-providers-common-io==1.5.0
apache-airflow-providers-common-sql==1.21.0
apache-airflow-providers-fab==2.0.0a1
apache-airflow-providers-ftp==3.12.0
apache-airflow-providers-http==5.0.0
apache-airflow-providers-imap==3.8.0
apache-airflow-providers-smtp==1.9.0
apache-airflow-providers-sqlite==4.0.0
apache-airflow-providers-standard==0.1.0a1

### Deployment

Other

### Deployment details

`airflow standalone`

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2025-02-06 12:15:42+00:00,['SaumilPatel03'],2025-02-08 20:00:21+00:00,2025-02-08 20:00:20+00:00,https://github.com/apache/airflow/issues/46516,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('affected_version:3.0.0alpha', 'For all 3.0.0 alpha releases')]","[{'comment_id': 2639829871, 'issue_id': 2835371496, 'author': 'SaumilPatel03', 'body': '@Lee-W Can you assign this issue to me.', 'created_at': datetime.datetime(2025, 2, 6, 13, 27, 15, tzinfo=datetime.timezone.utc)}]","SaumilPatel03 (Assginee) on (2025-02-06 13:27:15 UTC): @Lee-W Can you assign this issue to me.

"
2835333796,issue,open,,"""Check it out"" link to New UI does not work","### Apache Airflow version

3.0.0a1

### If ""Other Airflow 2 version"" selected, which one?

3.0.0a1

### What happened?

When click ""We have a new UI for Airflow 3.0 Check it out now!"", it goes to `http://localhost:29091/webapp` and thus finds nothing.

### What you think should happen instead?

It should go to `http://localhost:9091/webapp` instead. (or other ports depending on the setup)

### How to reproduce

```sh
uv venv --python 3.12

uv pip install \
    --find-links https://dist.apache.org/repos/dist/dev/airflow/3.0.0a1/ \
    apache-airflow==3.0.0a1 \
    apache-airflow-task-sdk==1.0.0a1 \
    apache-airflow-providers-standard==0.1.0a1 \
    apache-airflow-providers-fab==2.0.0a1 \
    apache-airflow-providers-celery==3.11.0a1
export AIRFLOW_HOME=`pwd`

uv run airflow db migrate
uv run airflow standalone
```

1. Open `http://localhost:8080`
2. Click the ""Check it out now!"" link of ""We have a new UI for Airflow 3.0 Check it out now!""

### Operating System

macOS 15.3

### Versions of Apache Airflow Providers

apache-airflow-providers-celery==3.11.0a1
apache-airflow-providers-common-compat==1.3.0
apache-airflow-providers-common-io==1.5.0
apache-airflow-providers-common-sql==1.21.0
apache-airflow-providers-fab==2.0.0a1
apache-airflow-providers-ftp==3.12.0
apache-airflow-providers-http==5.0.0
apache-airflow-providers-imap==3.8.0
apache-airflow-providers-smtp==1.9.0
apache-airflow-providers-sqlite==4.0.0
apache-airflow-providers-standard==0.1.0a1

### Deployment

Other

### Deployment details

`airflow standalone`

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2025-02-06 11:59:45+00:00,['Prab-27'],2025-02-07 01:25:09+00:00,,https://github.com/apache/airflow/issues/46514,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('affected_version:3.0.0alpha', 'For all 3.0.0 alpha releases')]","[{'comment_id': 2640133758, 'issue_id': 2835333796, 'author': 'Prab-27', 'body': ""@Lee-W I 'd like to work on this issue"", 'created_at': datetime.datetime(2025, 2, 6, 15, 24, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2640757475, 'issue_id': 2835333796, 'author': 'aditya0yadav', 'body': '[Lee-W](https://github.com/Lee-W) \nfor this issue \nit is working properly \nif we were to use breeze start-airflow and then click on check out new ui it is working properly', 'created_at': datetime.datetime(2025, 2, 6, 19, 6, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641623804, 'issue_id': 2835333796, 'author': 'Lee-W', 'body': '> [Lee-W](https://github.com/Lee-W) for this issue it is working properly if we were to use breeze start-airflow and then click on check out new ui it is working properly\n\nThis should work in all environment instead of breeze only', 'created_at': datetime.datetime(2025, 2, 7, 1, 24, 55, tzinfo=datetime.timezone.utc)}]","Prab-27 (Assginee) on (2025-02-06 15:24:09 UTC): @Lee-W I 'd like to work on this issue

aditya0yadav on (2025-02-06 19:06:08 UTC): [Lee-W](https://github.com/Lee-W) 
for this issue 
it is working properly 
if we were to use breeze start-airflow and then click on check out new ui it is working properly

Lee-W (Issue Creator) on (2025-02-07 01:24:55 UTC): This should work in all environment instead of breeze only

"
2835206611,issue,open,,Issues while reading Xcom values,"### Apache Airflow version

3.0.0a1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I see the below issue while reading Xcom

1.   `ti.xcom_pull(key=key)` used to work without task_id in AF2, however failing in AF3

**AF2**

<img width=""1353"" alt=""Image"" src=""https://github.com/user-attachments/assets/9dc08a59-3b83-4bbf-a71c-c3b5c779f025"" />


**AF3**
<img width=""1462"" alt=""Image"" src=""https://github.com/user-attachments/assets/d7ea1fdd-5b07-4ac8-87d3-63fa728def86"" />


1.  xcom values are showing in """"
**AF3**
![Image](https://github.com/user-attachments/assets/cd57567b-c982-4764-8f8e-6be3d5b8ad3e)
**AF2**
<img width=""1048"" alt=""Image"" src=""https://github.com/user-attachments/assets/3e764713-2d9b-4d72-b6f5-719ecde95ef0"" />

### What you think should happen instead?

There is be no deviations in retriving xcom from AF2

### How to reproduce


1. Execute below DAG
2. Verify Xcom values are with qoutes.
3. `val = ti.xcom_pull(key=key)` in **wait_pull** method was able to get the xcom value with only using key, however, in AF3 is not able to see . See xcom value of seperate.y task

```
from airflow.decorators import dag, task, task_group
from airflow.models.baseoperator import chain, cross_downstream
from time import sleep
from pendulum import today


def wait_pull(**kwargs):
    ti = kwargs[""ti""]
    key = ti.task_id.split(""."")[-2]
    val = None
    while val is None:
        val = ti.xcom_pull(key=key)
        if not val:
            print(""not yet"")
        sleep(5)
    print(f""got {val}"")
    return val


def push(val, **kwargs):
    ti = kwargs[""ti""]
    key = ti.task_id.split(""."")[-2]
    ti.xcom_push(key=key, value=val)


@task
def x(**kwargs):
    push(""X"", **kwargs)
    return ""x""


@task
def y(a, **kwargs):
    b = wait_pull(**kwargs)
    push(f""{b}Y"", **kwargs)
    return f""{a}y""


@task
def z(a, **kwargs):
    b = wait_pull(**kwargs)
    push(f""{b}Z"", **kwargs)
    return f""{a}z""


@task
def get(**kwargs):
    return wait_pull(**kwargs)


@task
def cat(a, b):
    if not a:
        return b
    if not b:
        return a
    return a + b


@task
def assert_one_of(to):
    @task
    def approved(val):
        print(val)
        assert int(val) in to

    return approved


@task_group()
def separate():
    a = x()
    y(a)
    c = z(a)
    d = get()
    return cat(c, d)


@task_group()
def chained_one_way():
    a = x()
    b = y(a)
    c = z(a)
    d = get()
    chain(a, b, c, d)
    return cat(c, d)


@task_group()
def chained_another_way():
    a = x()
    b = y(a)
    c = z(a)
    d = get()
    b >> d  # to remove race condition
    chain(a, c, b, d)
    return cat(c, d)


@task_group()
def crossed_one_way():
    a = x()
    b = y(a)
    c = z(a)
    d = get()
    c >> d  # to remove race condition
    cross_downstream([a, b], [c, d])
    return cat(c, d)


@task_group()
def crossed_another_way():
    a = x()
    b = y(a)
    c = z(a)
    d = get()
    cross_downstream([a, c], [b, d])
    return cat(c, d)


@task
def assert_equal(a, b):
    print(f""{a} == {b}"")
    assert a == b


@dag(
    schedule=None,
    start_date=today('UTC').add(days=-1),
    default_args={""owner"": ""airflow""},
    max_active_runs=1,
    catchup=False,
    tags=[""core""],
)
def cross_chain_structure_hash():
    """"""
    The patterns below aren't supposed to be meaningful, it's enough that a
    change in the underlying functionality will fail this test and attract
    human attention.
    """"""

    assert_equal(""xzX"", separate())
    assert_equal(""xzXYZ"", chained_one_way())
    assert_equal(""xzXZY"", chained_another_way())
    assert_equal(""xzXYZ"", crossed_one_way())
    assert_equal(""xzXZ"", crossed_another_way())


the_dag = cross_chain_structure_hash()
```


### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vatsrahul1001,2025-02-06 11:01:04+00:00,"['ashb', 'amoghrajesh']",2025-02-06 12:12:20+00:00,,https://github.com/apache/airflow/issues/46513,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('affected_version:3.0.0alpha', 'For all 3.0.0 alpha releases')]","[{'comment_id': 2639655214, 'issue_id': 2835206611, 'author': 'amoghrajesh', 'body': 'I will take a look at this soon', 'created_at': datetime.datetime(2025, 2, 6, 12, 12, 10, tzinfo=datetime.timezone.utc)}]","amoghrajesh (Assginee) on (2025-02-06 12:12:10 UTC): I will take a look at this soon

"
2835079516,issue,closed,completed,Task SDK not able to read asset events,"### Apache Airflow version

3.0.0a1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Task SDK not able to read asset events 


**Error**

```
[{\""name\"":\""s3://output/1.txt\"",\""uri\"":\""s3://output/1.txt\"",\""asset_type\"":\""Asset\""}],\""outlets\"":[],\""type\"":\""RuntimeCheckOnTask\""}\n"",""logger"":""task""}
{""timestamp"":""2025-02-06T09:59:33.498327Z"",""level"":""info"",""event"":""inlet_events are None"",""chan"":""stdout"",""logger"":""task""}
{""timestamp"":""2025-02-06T09:59:33.498183"",""level"":""error"",""event"":""Task failed with exception"",""logger"":""task"",""error_detail"":[{""exc_type"":""TypeError"",""exc_value"":""'NoneType' object is not subscriptable"",""syntax_error"":null,""is_cause"":false,""frames"":[{""filename"":""/opt/airflow/task_sdk/src/airflow/sdk/execution_time/task_runner.py"",""lineno"":545,""name"":""run""},{""filename"":""/opt/airflow/task_sdk/src/airflow/sdk/execution_time/task_runner.py"",""lineno"":645,""name"":""_execute_task""},{""filename"":""/opt/airflow/airflow/models/baseoperator.py"",""lineno"":173,""name"":""wrapper""},{""filename"":""/opt/airflow/airflow/decorators/base.py"",""lineno"":252,""name"":""execute""},{""filename"":""/opt/airflow/airflow/models/baseoperator.py"",""lineno"":173,""name"":""wrapper""},{""filename"":""/opt/airflow/providers/standard/src/airflow/providers/standard/operators/python.py"",""lineno"":196,""name"":""execute""},{""filename"":""/opt/airflow/providers/standard/src/airflow/providers/standard/operators/python.py"",""lineno"":222,""name"":""execute_callable""},{""filename"":""/opt/airflow/airflow/utils/operator_helpers.py"",""lineno"":261,""name"":""run""},{""filename"":""/files/dags/example_inlet_event_extra.py"",""lineno"":30,""name"":""read_dataset_event""}]}]}
{""timestamp"":""2025-02-06T09:59:33.500653"",""level"":""debug"",""event"":""Sending request"",""json"":""{\""state\"":\""failed\"",\""end_date\"":\""2025-02-06T09:59:33.500437Z\"",\""type\"":\""TaskState\""}\n"",""logger"":""task""}
```

### What you think should happen instead?

We should be able to fetch asset events in AF3 as we are able to do in AF2.

### How to reproduce

Create Asset events with below DAG

```
ds = Dataset(""s3://output/1.txt"")

with DAG(
    dag_id=""dataset_with_extra_by_yield"",
    catchup=False,
    start_date=datetime.datetime.min,
    schedule=""@daily"",
    tags=[""datasets""],
):

    @task(outlets=[ds])
    def dataset_with_extra_by_yield():
        yield Metadata(ds, {""hi"": ""bye""})

    dataset_with_extra_by_yield()

with DAG(
    dag_id=""dataset_with_extra_by_context"",
    catchup=False,
    start_date=datetime.datetime.min,
    schedule=""@daily"",
    tags=[""dataset""],
):

    @task(outlets=[ds])
    def dataset_with_extra_by_context(*, outlet_events=None):
        outlet_events[ds].extra = {""hi"": ""bye""}

    dataset_with_extra_by_context()
```

Now try to retrieve them with below DAG

```
ds = Dataset(""s3://output/1.txt"")

with DAG(
    dag_id=""read_dataset_event"",
    catchup=False,
    start_date=datetime.datetime.min,
    schedule=""@daily"",
    tags=[""datasets""],
):

    @task(inlets=[ds])
    def read_dataset_event(*, inlet_events=None):
        print(f""inlet_events are {inlet_events}"")
        for event in inlet_events[ds][:-2]:
            print(event.extra[""hi""])

    read_dataset_event()
```

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vatsrahul1001,2025-02-06 10:05:37+00:00,['Lee-W'],2025-02-07 02:12:03+00:00,2025-02-07 02:11:26+00:00,https://github.com/apache/airflow/issues/46508,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:datasets', 'Issues related to the datasets feature'), ('affected_version:3.0.0alpha', 'For all 3.0.0 alpha releases')]","[{'comment_id': 2639397275, 'issue_id': 2835079516, 'author': 'vatsrahul1001', 'body': '@wei is fixing this.', 'created_at': datetime.datetime(2025, 2, 6, 10, 18, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641762119, 'issue_id': 2835079516, 'author': 'Lee-W', 'body': ""Thanks @vatsrahul1001 for finding it. It's blocking all the tests related to assets. Just merged"", 'created_at': datetime.datetime(2025, 2, 7, 2, 12, 2, tzinfo=datetime.timezone.utc)}]","vatsrahul1001 (Issue Creator) on (2025-02-06 10:18:23 UTC): @wei is fixing this.

Lee-W (Assginee) on (2025-02-07 02:12:02 UTC): Thanks @vatsrahul1001 for finding it. It's blocking all the tests related to assets. Just merged

"
2834898240,issue,open,,Switch positional logical-date-or-run-id CLI argument to a flag?,"### Body

Raised during review of #46407.

We introduced `logical_date_or_run_id` back in 2.2 when we started enforcing the DR-TI relationship so people can select a TI with run_id instead of execution_date. The argument was implemented as a “smart” positional argument because we needed to maintain backward compatibility of the CLI. With 3.0, however, it is possible to get rid of the magic and require the user to pass in a named argument instead, so instead of

```
$ airflow tasks run <dag_id> <task_id> <logical_date_or_run_id>
```

the user must write

```
$ airflow tasks run <dag_id> <task_id> --run-id <run_id>
```

or

```
$ airflow tasks run <dag_id> <task_id> --logical-date <logical_date>
```

This feels like an improvement in a vacuum, but is it worthwhile as a breaking change?

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",uranusjr,2025-02-06 08:42:57+00:00,[],2025-02-06 08:42:57+00:00,,https://github.com/apache/airflow/issues/46506,"[('area:CLI', ''), ('airflow3.0:breaking', 'Candidates for Airflow 3.0 that contain breaking changes')]",[],
2834561679,issue,open,,Handle records with null logical date when downgrade db,"### Body

Split from https://github.com/apache/airflow/pull/46295#discussion_r1944088023

We used to have some code in upgrade and downgrade to check if the db is in a state suitable for migration
https://github.com/apache/airflow/commit/56a3987c3fca3a574b8f1c7faa8f08de2becee60

This can be done here. Basically we can check if there are any records with null logical date, and move them to a separate table.



### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",uranusjr,2025-02-06 05:10:23+00:00,[],2025-02-06 05:12:41+00:00,,https://github.com/apache/airflow/issues/46500,"[('area:db-migrations', 'PRs with DB migration'), ('AIP-83', 'Remove Execution Date Unique Constraint from DAG Run')]",[],
2833933539,issue,closed,completed,Add a parameter to specify expected return code in WinRMOperator,"### Description

Hello, currently WinRMOperator return is marked as success only if the launched command return 0. Adding an expected_return_code parameter could allow dealing with command that doesn't return 0 on success without making ""conversion"" in an englobing batch.

### Use case/motivation

When launching a [robocopy command](https://learn.microsoft.com/en-us/windows-server/administration/windows-commands/robocopy), return code from 0 to 7 may be considered as success.

### Related issues

_No response_

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",darkag,2025-02-05 20:48:53+00:00,[],2025-02-06 22:25:26+00:00,2025-02-06 22:25:26+00:00,https://github.com/apache/airflow/issues/46494,"[('area:providers', ''), ('kind:feature', 'Feature Requests'), ('good first issue', ''), ('provider:microsoft-winrm', '')]",[],
2833646088,issue,open,,Trying to view TaskInstance for a queued/not-yet-started DagRun gives a 500 error,"### Body

If you trigger a DagRun and the scheduler doesn't start it straight away (dag is paused, too many running or any other reason) and navigate to the TI in the UI the API server throws this 500 error:


```
    INFO   192.168.156.1:30738 - ""GET /public/dags/example_dynamic_task_mapping/dagRuns/manual__2025-02-05T17%3A49%3A12.401366%2B00%3A00/taskInstances/sum_it/logs/1?map_index=-1 HTTP/1.1"" 500
[2025-02-05T18:14:22.707+0000] {httptools_impl.py:414} ERROR - Exception in ASGI application

  + Exception Group Traceback (most recent call last):
  |   File ""/usr/local/lib/python3.12/site-packages/starlette/_utils.py"", line 76, in collapse_excgroups
  |     yield
  |   File ""/usr/local/lib/python3.12/site-packages/starlette/middleware/base.py"", line 178, in __call__
  |     async with anyio.create_task_group() as task_group:
  |                ^^^^^^^^^^^^^^^^^^^^^^^^^
  |   File ""/usr/local/lib/python3.12/site-packages/anyio/_backends/_asyncio.py"", line 767, in __aexit__
  |     raise BaseExceptionGroup(
  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)
  +-+---------------- 1 ----------------
    | Traceback (most recent call last):
    |   File ""/usr/local/lib/python3.12/site-packages/uvicorn/protocols/http/httptools_impl.py"", line 409, in run_asgi
    |     result = await app(  # type: ignore[func-returns-value]
    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/usr/local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py"", line 60, in __call__
    |     return await self.app(scope, receive, send)
    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/usr/local/lib/python3.12/site-packages/fastapi/applications.py"", line 1054, in __call__
    |     await super().__call__(scope, receive, send)
    |   File ""/usr/local/lib/python3.12/site-packages/starlette/applications.py"", line 112, in __call__
    |     await self.middleware_stack(scope, receive, send)
    |   File ""/usr/local/lib/python3.12/site-packages/starlette/middleware/errors.py"", line 187, in __call__
    |     raise exc
    |   File ""/usr/local/lib/python3.12/site-packages/starlette/middleware/errors.py"", line 165, in __call__
    |     await self.app(scope, receive, _send)
    |   File ""/usr/local/lib/python3.12/site-packages/starlette/middleware/gzip.py"", line 20, in __call__
    |     await responder(scope, receive, send)
    |   File ""/usr/local/lib/python3.12/site-packages/starlette/middleware/gzip.py"", line 39, in __call__
    |     await self.app(scope, receive, self.send_with_gzip)
    |   File ""/usr/local/lib/python3.12/site-packages/starlette/middleware/cors.py"", line 85, in __call__
    |     await self.app(scope, receive, send)
    |   File ""/usr/local/lib/python3.12/site-packages/starlette/middleware/base.py"", line 177, in __call__
    |     with recv_stream, send_stream, collapse_excgroups():
    |                                    ^^^^^^^^^^^^^^^^^^^^
    |   File ""/usr/local/lib/python3.12/contextlib.py"", line 158, in __exit__
    |     self.gen.throw(value)
    |   File ""/usr/local/lib/python3.12/site-packages/starlette/_utils.py"", line 82, in collapse_excgroups
    |     raise exc
    |   File ""/usr/local/lib/python3.12/site-packages/starlette/middleware/base.py"", line 179, in __call__
    |     response = await self.dispatch_func(request, call_next)
    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/opt/airflow/airflow/api_fastapi/core_api/middleware.py"", line 28, in dispatch
    |     response = await call_next(request)
    |                ^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/usr/local/lib/python3.12/site-packages/starlette/middleware/base.py"", line 154, in call_next
    |     raise app_exc
    |   File ""/usr/local/lib/python3.12/site-packages/starlette/middleware/base.py"", line 141, in coro
    |     await self.app(scope, receive_or_disconnect, send_no_error)
    |   File ""/usr/local/lib/python3.12/site-packages/starlette/middleware/exceptions.py"", line 62, in __call__
    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
    |   File ""/usr/local/lib/python3.12/site-packages/starlette/_exception_handler.py"", line 53, in wrapped_app
    |     raise exc
    |   File ""/usr/local/lib/python3.12/site-packages/starlette/_exception_handler.py"", line 42, in wrapped_app
    |     await app(scope, receive, sender)
    |   File ""/usr/local/lib/python3.12/site-packages/starlette/routing.py"", line 715, in __call__
    |     await self.middleware_stack(scope, receive, send)
    |   File ""/usr/local/lib/python3.12/site-packages/starlette/routing.py"", line 735, in app
    |     await route.handle(scope, receive, send)
    |   File ""/usr/local/lib/python3.12/site-packages/starlette/routing.py"", line 288, in handle
    |     await self.app(scope, receive, send)
    |   File ""/usr/local/lib/python3.12/site-packages/starlette/routing.py"", line 76, in app
    |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)
    |   File ""/usr/local/lib/python3.12/site-packages/starlette/_exception_handler.py"", line 53, in wrapped_app
    |     raise exc
    |   File ""/usr/local/lib/python3.12/site-packages/starlette/_exception_handler.py"", line 42, in wrapped_app
    |     await app(scope, receive, sender)
    |   File ""/usr/local/lib/python3.12/site-packages/starlette/routing.py"", line 73, in app
    |     response = await f(request)
    |                ^^^^^^^^^^^^^^^^
    |   File ""/usr/local/lib/python3.12/site-packages/fastapi/routing.py"", line 301, in app
    |     raw_response = await run_endpoint_function(
    |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/usr/local/lib/python3.12/site-packages/fastapi/routing.py"", line 214, in run_endpoint_function
    |     return await run_in_threadpool(dependant.call, **values)
    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/usr/local/lib/python3.12/site-packages/starlette/concurrency.py"", line 37, in run_in_threadpool
    |     return await anyio.to_thread.run_sync(func)
    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/usr/local/lib/python3.12/site-packages/anyio/to_thread.py"", line 56, in run_sync
    |     return await get_async_backend().run_sync_in_worker_thread(
    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/usr/local/lib/python3.12/site-packages/anyio/_backends/_asyncio.py"", line 2461, in run_sync_in_worker_thread
    |     return await future
    |            ^^^^^^^^^^^^
    |   File ""/usr/local/lib/python3.12/site-packages/anyio/_backends/_asyncio.py"", line 962, in run
    |     result = context.run(func, *args)
    |              ^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/opt/airflow/airflow/api_fastapi/core_api/routes/public/log.py"", line 140, in get_log
    |     logs, metadata = task_log_reader.read_log_chunks(ti, try_number, metadata)
    |                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/opt/airflow/airflow/utils/log/log_reader.py"", line 65, in read_log_chunks
    |     logs, metadatas = self.log_handler.read(ti, try_number, metadata=metadata)
    |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/opt/airflow/airflow/utils/log/file_task_handler.py"", line 488, in read
    |     log, out_metadata = self._read(task_instance, try_number_element, metadata)
    |                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/opt/airflow/airflow/utils/log/file_task_handler.py"", line 369, in _read
    |     worker_log_rel_path = self._render_filename(ti, try_number)
    |                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/opt/airflow/airflow/utils/session.py"", line 101, in wrapper
    |     return func(*args, session=session, **kwargs)
    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/opt/airflow/airflow/utils/log/file_task_handler.py"", line 277, in _render_filename
    |     context = ti.get_template_context(session=session)
    |               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/opt/airflow/airflow/models/taskinstance.py"", line 3212, in get_template_context
    |     return _get_template_context(
    |            ^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/opt/airflow/airflow/models/taskinstance.py"", line 926, in _get_template_context
    |     dag_run=DagRunSDK.model_validate(dag_run, from_attributes=True),
    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    |   File ""/usr/local/lib/python3.12/site-packages/pydantic/main.py"", line 627, in model_validate
    |     return cls.__pydantic_validator__.validate_python(
    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    | pydantic_core._pydantic_core.ValidationError: 1 validation error for DagRun
    | start_date
    |   Input should be a valid datetime [type=datetime_type, input_value=None, input_type=NoneType]
    |     For further information visit https://errors.pydantic.dev/2.10/v/datetime_type
    +------------------------------------

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.12/site-packages/uvicorn/protocols/http/httptools_impl.py"", line 409, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py"", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/fastapi/applications.py"", line 1054, in __call__
    await super().__call__(scope, receive, send)
  File ""/usr/local/lib/python3.12/site-packages/starlette/applications.py"", line 112, in __call__
    await self.middleware_stack(scope, receive, send)
  File ""/usr/local/lib/python3.12/site-packages/starlette/middleware/errors.py"", line 187, in __call__
    raise exc
  File ""/usr/local/lib/python3.12/site-packages/starlette/middleware/errors.py"", line 165, in __call__
    await self.app(scope, receive, _send)
  File ""/usr/local/lib/python3.12/site-packages/starlette/middleware/gzip.py"", line 20, in __call__
    await responder(scope, receive, send)
  File ""/usr/local/lib/python3.12/site-packages/starlette/middleware/gzip.py"", line 39, in __call__
    await self.app(scope, receive, self.send_with_gzip)
  File ""/usr/local/lib/python3.12/site-packages/starlette/middleware/cors.py"", line 85, in __call__
    await self.app(scope, receive, send)
  File ""/usr/local/lib/python3.12/site-packages/starlette/middleware/base.py"", line 177, in __call__
    with recv_stream, send_stream, collapse_excgroups():
                                   ^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/contextlib.py"", line 158, in __exit__
    self.gen.throw(value)
  File ""/usr/local/lib/python3.12/site-packages/starlette/_utils.py"", line 82, in collapse_excgroups
    raise exc
  File ""/usr/local/lib/python3.12/site-packages/starlette/middleware/base.py"", line 179, in __call__
    response = await self.dispatch_func(request, call_next)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/airflow/api_fastapi/core_api/middleware.py"", line 28, in dispatch
    response = await call_next(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/starlette/middleware/base.py"", line 154, in call_next
    raise app_exc
  File ""/usr/local/lib/python3.12/site-packages/starlette/middleware/base.py"", line 141, in coro
    await self.app(scope, receive_or_disconnect, send_no_error)
  File ""/usr/local/lib/python3.12/site-packages/starlette/middleware/exceptions.py"", line 62, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File ""/usr/local/lib/python3.12/site-packages/starlette/_exception_handler.py"", line 53, in wrapped_app
    raise exc
  File ""/usr/local/lib/python3.12/site-packages/starlette/_exception_handler.py"", line 42, in wrapped_app
    await app(scope, receive, sender)
  File ""/usr/local/lib/python3.12/site-packages/starlette/routing.py"", line 715, in __call__
    await self.middleware_stack(scope, receive, send)
  File ""/usr/local/lib/python3.12/site-packages/starlette/routing.py"", line 735, in app
    await route.handle(scope, receive, send)
  File ""/usr/local/lib/python3.12/site-packages/starlette/routing.py"", line 288, in handle
    await self.app(scope, receive, send)
  File ""/usr/local/lib/python3.12/site-packages/starlette/routing.py"", line 76, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File ""/usr/local/lib/python3.12/site-packages/starlette/_exception_handler.py"", line 53, in wrapped_app
    raise exc
  File ""/usr/local/lib/python3.12/site-packages/starlette/_exception_handler.py"", line 42, in wrapped_app
    await app(scope, receive, sender)
  File ""/usr/local/lib/python3.12/site-packages/starlette/routing.py"", line 73, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/fastapi/routing.py"", line 301, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/fastapi/routing.py"", line 214, in run_endpoint_function
    return await run_in_threadpool(dependant.call, **values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/starlette/concurrency.py"", line 37, in run_in_threadpool
    return await anyio.to_thread.run_sync(func)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/anyio/to_thread.py"", line 56, in run_sync
    return await get_async_backend().run_sync_in_worker_thread(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/anyio/_backends/_asyncio.py"", line 2461, in run_sync_in_worker_thread
    return await future
           ^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/anyio/_backends/_asyncio.py"", line 962, in run
    result = context.run(func, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/airflow/api_fastapi/core_api/routes/public/log.py"", line 140, in get_log
    logs, metadata = task_log_reader.read_log_chunks(ti, try_number, metadata)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/airflow/utils/log/log_reader.py"", line 65, in read_log_chunks
    logs, metadatas = self.log_handler.read(ti, try_number, metadata=metadata)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/airflow/utils/log/file_task_handler.py"", line 488, in read
    log, out_metadata = self._read(task_instance, try_number_element, metadata)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/airflow/utils/log/file_task_handler.py"", line 369, in _read
    worker_log_rel_path = self._render_filename(ti, try_number)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/airflow/utils/session.py"", line 101, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/airflow/utils/log/file_task_handler.py"", line 277, in _render_filename
    context = ti.get_template_context(session=session)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/airflow/models/taskinstance.py"", line 3212, in get_template_context
    return _get_template_context(
           ^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/airflow/models/taskinstance.py"", line 926, in _get_template_context
    dag_run=DagRunSDK.model_validate(dag_run, from_attributes=True),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/pydantic/main.py"", line 627, in model_validate
    return cls.__pydantic_validator__.validate_python(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 1 validation error for DagRun
start_date
  Input should be a valid datetime [type=datetime_type, input_value=None, input_type=NoneType]
    For further information visit https://errors.pydantic.dev/2.10/v/datetime_type
```

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",ashb,2025-02-05 18:17:32+00:00,[],2025-02-07 19:39:01+00:00,,https://github.com/apache/airflow/issues/46487,"[('kind:bug', 'This is a clearly a bug'), ('area:API', ""Airflow's REST/HTTP API""), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application'), ('affected_version:3.0.0alpha', 'For all 3.0.0 alpha releases')]","[{'comment_id': 2639049880, 'issue_id': 2833646088, 'author': 'phanikumv', 'body': 'cc @bbovenzi @pierrejeambrun', 'created_at': datetime.datetime(2025, 2, 6, 7, 38, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2643867545, 'issue_id': 2833646088, 'author': 'tirkarthi', 'body': 'I guess `start_date` should be set as nullable similar to `end_date` . `start_date` is nullable in `DAGRunResponse`. I tried this fix with manual edit but unfortunately couldn\'t regenerate the files `_generated.py` properly in local setup.\n\nhttps://github.com/apache/airflow/blob/45d37da572ead522d9fa64889ad5d12fedfd1a88/airflow/api_fastapi/execution_api/datamodels/taskinstance.py#L228-L229\n\nFix tried\n\n```\nstart_date: Annotated[datetime | None, Field(title=""Start Date"")] = None\n```\n\nhttps://github.com/apache/airflow/issues/46570', 'created_at': datetime.datetime(2025, 2, 7, 19, 31, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2643897870, 'issue_id': 2833646088, 'author': 'bbovenzi', 'body': ""Yes, `start_date` should be nullable. I ran into this issue when I was working on autorefresh and had to use logical_date in some places, although I guess that's also not the best practice anymore."", 'created_at': datetime.datetime(2025, 2, 7, 19, 39, tzinfo=datetime.timezone.utc)}]","phanikumv on (2025-02-06 07:38:21 UTC): cc @bbovenzi @pierrejeambrun

tirkarthi on (2025-02-07 19:31:20 UTC): I guess `start_date` should be set as nullable similar to `end_date` . `start_date` is nullable in `DAGRunResponse`. I tried this fix with manual edit but unfortunately couldn't regenerate the files `_generated.py` properly in local setup.

https://github.com/apache/airflow/blob/45d37da572ead522d9fa64889ad5d12fedfd1a88/airflow/api_fastapi/execution_api/datamodels/taskinstance.py#L228-L229

Fix tried

```
start_date: Annotated[datetime | None, Field(title=""Start Date"")] = None
```

https://github.com/apache/airflow/issues/46570

bbovenzi on (2025-02-07 19:39:00 UTC): Yes, `start_date` should be nullable. I ran into this issue when I was working on autorefresh and had to use logical_date in some places, although I guess that's also not the best practice anymore.

"
2833564484,issue,open,,Background page links are not correct when user click opens up a modal/dialog page,"### Apache Airflow version

3.0.0a1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

On airflow 3 UI, when user opens gantt/grid/graph modal page and inspects all the links on the DOM, the gantt/grid/graph links for the background page are not correct.

![Image](https://github.com/user-attachments/assets/24dad8cd-d89c-4f81-ab68-cc1b18135414)

### What you think should happen instead?

The links on the background page should not change.

### How to reproduce

Steps:
1. Navigate to Airflow 3 UI.
2. Navigate to a DAG overview tab via Dags sidebar nav.
3. Inspect all the link elements and take a note of gantt/graph/grid href attribute value.
4. Click on the Gantt link on the right side and inspect all the link elements again.
5. Notice the href value for gantt/grid/graph has changed.

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",atul-astronomer,2025-02-05 17:36:55+00:00,['pierrejeambrun'],2025-02-05 17:41:32+00:00,,https://github.com/apache/airflow/issues/46483,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('affected_version:3.0.0alpha', 'For all 3.0.0 alpha releases')]",[],
2833487530,issue,closed,completed,Move api server in helm chart back to beta with _ before it,"### Description

The apiServer is now a main entry in the helm chart. Make it beta with _ prefix and reinstate the warning. See https://github.com/apache/airflow/pull/44463 for reference.

### Use case/motivation

It may change in between upcoming 1.16.0 and Airflow 3 release, so better to keep it beta to allow more flexibility.

### Related issues

_No response_

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",andrii-korotkov-verkada,2025-02-05 17:00:00+00:00,['andrii-korotkov-verkada'],2025-02-06 15:21:50+00:00,2025-02-06 15:21:50+00:00,https://github.com/apache/airflow/issues/46477,"[('kind:feature', 'Feature Requests'), ('area:helm-chart', 'Airflow Helm Chart')]","[{'comment_id': 2637584705, 'issue_id': 2833487530, 'author': 'jedcunningham', 'body': '@andrii-korotkov-verkada - so you know in the future, issues are entirely optional, you can just open a PR directly.', 'created_at': datetime.datetime(2025, 2, 5, 17, 33, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2637591193, 'issue_id': 2833487530, 'author': 'andrii-korotkov-verkada', 'body': 'Ah okay, sounds good.', 'created_at': datetime.datetime(2025, 2, 5, 17, 35, 50, tzinfo=datetime.timezone.utc)}]","jedcunningham on (2025-02-05 17:33:11 UTC): @andrii-korotkov-verkada - so you know in the future, issues are entirely optional, you can just open a PR directly.

andrii-korotkov-verkada (Issue Creator) on (2025-02-05 17:35:50 UTC): Ah okay, sounds good.

"
2832943695,issue,open,,Status of testing of Apache Airflow 2.10.5rc1,"### Body

We are kindly requesting that contributors to [Apache Airflow RC 2.10.5rc1](https://pypi.org/project/apache-airflow/2.10.5rc1/) help test the RC.

Please let us know by commenting if the issue is addressed in the latest RC.

- [x] [[v2-10-test] Fix short circuit in mapped tasks (#44912)](https://github.com/apache/airflow/pull/44912): @shahar1 @Chais
     Linked issues:
     - [MappedTasks don't short-circuit (#43883)](https://github.com/apache/airflow/issues/43883)
     - [Fix short circuit in mapped tasks (#44925)](https://github.com/apache/airflow/pull/44925)
- [x] [[v2-10-test] Fix premature evaluation in mapped task group (#44937)](https://github.com/apache/airflow/pull/44937): @shahar1 @benbuckman
     Linked issues:
     - [Fix pre-mature evaluation of tasks in mapped task group (#40460)](https://github.com/apache/airflow/pull/40460)
     - [Trigger Rule ONE_FAILED does not work in task group with mapped tasks (#34023)](https://github.com/apache/airflow/issues/34023)
- [ ] [[v2-10-test] Fix task id validation in BaseOperator (#44938)](https://github.com/apache/airflow/pull/44938): @gopidesupavan @patniharshit
     Linked issues:
     - [Scheduler crash loop due to invalid task_id persisted in DagBag without pre-validation (#44738)](https://github.com/apache/airflow/issues/44738)
     - [Fix task id validation in BaseOperator (#44939)](https://github.com/apache/airflow/pull/44939)
- [ ] [[v2-10-test] Set Autocomplete Off on Login Form - Main (#44929) (#44940)](https://github.com/apache/airflow/pull/44940): @github-actions[bot] @MarBed190 @geraj1010
     Linked issues:
     - [Autocomplete Attribute Not Disabled for Password Fields in Login Forms (#44019)](https://github.com/apache/airflow/issues/44019)
- [x] [[v2-10-test] Bugfix some Doc urls in repo (#45007) (#45010)](https://github.com/apache/airflow/pull/45010): @jscheffl @github-actions[bot]
- [ ] [Add BigQuery job link (#45020)](https://github.com/apache/airflow/pull/45020): @nakamura1878
- [x] [Fix DB isolation tests on v2-10-test (#45021)](https://github.com/apache/airflow/pull/45021): @jscheffl
- [x] [Fix DB isolation tests on v2-10-test (#45029)](https://github.com/apache/airflow/pull/45029): @jscheffl
     Linked issues:
     - [Fix DB isolation tests on v2-10-test (#45021)](https://github.com/apache/airflow/pull/45021)
- [x] [[v2-10-test] Avoid 1.1.8 version of msgraph-core (#45044) (#45063)](https://github.com/apache/airflow/pull/45063): @potiuk
- [x] [[2-10-test] Bump hatch version in breeze and prevent ""get-workflow-info"" failure … (#45064)](https://github.com/apache/airflow/pull/45064): @potiuk
- [x] [[v2-10-test] Only run ARM collection tests in main branch (#45068) (#45076)](https://github.com/apache/airflow/pull/45076): @potiuk
- [ ] [[v2-10-test] Add traceback log output when sigterm was sent (#44880) (#45077)](https://github.com/apache/airflow/pull/45077): @VladaZakharova @potiuk
- [ ] [[v2-10-test] Handle relative paths when sanitizing URLs (#41995) (#45080)](https://github.com/apache/airflow/pull/45080): @potiuk @utkarsharma2
- [x] [[v2-10-test] Evaluate none in extended json type decorator (#45120)](https://github.com/apache/airflow/pull/45120): @shahar1
     Linked issues:
     - [Evaluate None in SQLAlchemy's extended JSON type decorator (#45119)](https://github.com/apache/airflow/pull/45119)
- [x] [Allow Dynamic Tasks to be Searchable Using map_index_template (#45109) (#45122)](https://github.com/apache/airflow/pull/45122): @jscheffl
     Linked issues:
     - [Allow Dynamic Tasks to be Searchable Using map_index_template (#45109)](https://github.com/apache/airflow/pull/45109)
- [x] [[v2-10-test] Allow fetching XCom with forward slash from the API and escape it in the UI (#45134) (#45137)](https://github.com/apache/airflow/pull/45137): @shahar1 @github-actions[bot]
- [x] [[v2-10-test] Fix breeze output static checks failure (#45142) (#45147)](https://github.com/apache/airflow/pull/45147): @potiuk
- [x] [[v2-10-test] Mark failing db isolation test in v2-10-test as skipped (#45151)](https://github.com/apache/airflow/pull/45151): @potiuk
- [x] [[v2-10-test] Sort ""opts"" element in click option dictionary before ha… (#45161)](https://github.com/apache/airflow/pull/45161): @potiuk
- [x] [[v2-10-test] Fixed the endless reschedule (#45224) (#45250)](https://github.com/apache/airflow/pull/45250): @jscheffl @morooshka
     Linked issues:
     - [Fixed the endless sensor reschedule (#45224)](https://github.com/apache/airflow/pull/45224)
- [ ] [Bump uv to 0.5.11 (#45105) (#45272)](https://github.com/apache/airflow/pull/45272): @kaxil @raphaelauv
- [x] [[v2-10-test] Fix update issues for object and advanced-arrays fields when empty default (#45313) (#45315)](https://github.com/apache/airflow/pull/45315): @jscheffl @github-actions[bot]
- [ ] [do not push stale update to related DagRun on TI update after task execution (#45348)](https://github.com/apache/airflow/pull/45348): @mobuchowski
- [ ] [[v2-10-test] Cease using ``InventoryFileReader`` (#45391) (#45538)](https://github.com/apache/airflow/pull/45538): @github-actions[bot] @AA-Turner
- [ ] [fix: log action get the correct request body (#45546) (#45560)](https://github.com/apache/airflow/pull/45560): @luoyuliuyin @pierrejeambrun @dstandish
- [x] [[v2-10-test] Ensure teardown tasks are executed when DAG run is set to failed (#45530) (#45581)](https://github.com/apache/airflow/pull/45581): @jscheffl @github-actions[bot]
- [x] [[v2-10-test] Update spelling wordlist (#45579) (#45582)](https://github.com/apache/airflow/pull/45582): @shahar1
- [x] [Protect against missing .uv cache (#45605)](https://github.com/apache/airflow/pull/45605): @shahar1 @potiuk
- [x] [[v2-10-test] Provide package write permissions to push-ci-image-cache job (#45573) (#45612)](https://github.com/apache/airflow/pull/45612): @gopidesupavan @potiuk
- [ ] [Backport #41832 - fix: rm skip_if and run_if in python source (#45680)](https://github.com/apache/airflow/pull/45680): @phi-friday @josix @pedro-cf
     Linked issues:
     - [fix: rm `skip_if` and `run_if` in python source (#41832)](https://github.com/apache/airflow/pull/41832)
     - [Bug: `@task.skip_if` decorator fails when used with `@task.virtualenv` in Airflow TaskFlow API (#43354)](https://github.com/apache/airflow/issues/43354)
- [x] [ Fix empty task instance for log (#45702) (#45703)](https://github.com/apache/airflow/pull/45703): @MishchenkoYuriy @jscheffl
- [ ] [Issue deprecation warning for plugins registering `ti_deps` (#45742)](https://github.com/apache/airflow/pull/45742): @ashb
     Linked issues:
     - [Removed the ability for Operators to specify their own ""scheduling deps"". (#45713)](https://github.com/apache/airflow/pull/45713)
- [ ] [[v2-10-test] Improve speed of tests by not creating connections at parse time (#45690) (#45826)](https://github.com/apache/airflow/pull/45826): @ashb @github-actions[bot]
- [ ] [[v2-10-test] Add ready_for_review to workflow pull_request types (#45855) (#45906)](https://github.com/apache/airflow/pull/45906): @gopidesupavan
     Linked issues:
     - [Add ready_for_review to workflow pull_request types (#45855)](https://github.com/apache/airflow/pull/45855)
- [ ] [[v2-10-test] Fix `FileTaskHandler` only read from default executor (#46000)](https://github.com/apache/airflow/pull/46000): @jason810496 @insomnes
     Linked issues:
     - [Allow `FileTaskHandler` get running tasks logs not only from default executor (#45529)](https://github.com/apache/airflow/issues/45529)
- [x] [[v2-10-test] Upgrade uv and pip (#46078)](https://github.com/apache/airflow/pull/46078): @potiuk
- [ ] [[v2-10-test] Add Webserver parameters: max_form_parts, max_form_memory_size (#45749) (#46243)](https://github.com/apache/airflow/pull/46243): @moiseenkov @github-actions[bot]
- [ ] [Fixed thread local _sentinel.callers defect and added test cases (#44… (#46280)](https://github.com/apache/airflow/pull/46280): @rahulgoyal2987 @utkarsharma2
- [ ] [Add map_index parameter to extra links API for Airflow 2.10 (#46337)](https://github.com/apache/airflow/pull/46337): @shubhamraj-git


Thanks to all who contributed to the release (probably not a complete list!):
@insomnes @moiseenkov @geraj1010 @josix @patniharshit @luoyuliuyin @utkarsharma2 @mobuchowski @dstandish @ashb @gopidesupavan @AA-Turner @VladaZakharova @MishchenkoYuriy @phi-friday @potiuk @jason810496 @morooshka @Chais @pedro-cf @kaxil @benbuckman @shubhamraj-git @shahar1 @rahulgoyal2987 @jscheffl @nakamura1878 @raphaelauv @MarBed190 @pierrejeambrun

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",utkarsharma2,2025-02-05 13:23:36+00:00,[],2025-02-06 22:38:36+00:00,,https://github.com/apache/airflow/issues/46466,"[('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2636880074, 'issue_id': 2832943695, 'author': 'raphaelauv', 'body': 'the container image are not yet present on dockerhub https://hub.docker.com/r/apache/airflow/tags?name=2.10.5', 'created_at': datetime.datetime(2025, 2, 5, 13, 38, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2636890454, 'issue_id': 2832943695, 'author': 'utkarsharma2', 'body': ""@raphaelauv Ya, I'm working on it. I will update once they are up."", 'created_at': datetime.datetime(2025, 2, 5, 13, 42, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2637136086, 'issue_id': 2832943695, 'author': 'utkarsharma2', 'body': '@raphaelauv The container images are now available.', 'created_at': datetime.datetime(2025, 2, 5, 15, 9, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2637886419, 'issue_id': 2832943695, 'author': 'potiuk', 'body': '> [@raphaelauv](https://github.com/raphaelauv) The container images are not available.\n\nI assume it was `are NOW available`', 'created_at': datetime.datetime(2025, 2, 5, 19, 50, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2637999598, 'issue_id': 2832943695, 'author': 'jscheffl', 'body': 'Checked the contributions/fixed that I merged, all looks good!', 'created_at': datetime.datetime(2025, 2, 5, 20, 53, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641282663, 'issue_id': 2832943695, 'author': 'potiuk', 'body': 'Checked all my changes - most of them breeze etc. looks good!', 'created_at': datetime.datetime(2025, 2, 6, 22, 38, 34, tzinfo=datetime.timezone.utc)}]","raphaelauv on (2025-02-05 13:38:25 UTC): the container image are not yet present on dockerhub https://hub.docker.com/r/apache/airflow/tags?name=2.10.5

utkarsharma2 (Issue Creator) on (2025-02-05 13:42:53 UTC): @raphaelauv Ya, I'm working on it. I will update once they are up.

utkarsharma2 (Issue Creator) on (2025-02-05 15:09:32 UTC): @raphaelauv The container images are now available.

potiuk on (2025-02-05 19:50:18 UTC): I assume it was `are NOW available`

jscheffl on (2025-02-05 20:53:15 UTC): Checked the contributions/fixed that I merged, all looks good!

potiuk on (2025-02-06 22:38:34 UTC): Checked all my changes - most of them breeze etc. looks good!

"
2832800121,issue,closed,completed,More meaningful name for `@task.kubernetes` pods by default,"### Description

Generate more meaningful default names for pods by `@task.kubernetes` with explicit randomness control


### Use case/motivation

Reference PR with tests showing the desired changes:
https://github.com/apache/airflow/pull/46462

Now pod created by decorator would have name `f""k8s_airflow_pod_{uuid.uuid4().hex}""` if name was not explicitly provided by the user.

I want more meaningful names by default (via python_callable name utilization) and to reuse already present randomness in name control option from `KubernetesPodOperator`: `random_name_suffix`.

### Related issues

_No response_

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",insomnes,2025-02-05 12:25:07+00:00,['insomnes'],2025-02-08 18:40:04+00:00,2025-02-08 18:25:04+00:00,https://github.com/apache/airflow/issues/46464,"[('area:providers', ''), ('kind:feature', 'Feature Requests'), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]","[{'comment_id': 2637523301, 'issue_id': 2832800121, 'author': 'RNHTTR', 'body': ""For future reference, there's no need to open an issue if you already have a PR submitted :)"", 'created_at': datetime.datetime(2025, 2, 5, 17, 6, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2637725048, 'issue_id': 2832800121, 'author': 'insomnes', 'body': ""Yeah, sorry for that, I thought that for features it's different from bug fixes. Thanks for clarifying this for me!"", 'created_at': datetime.datetime(2025, 2, 5, 18, 37, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2642474427, 'issue_id': 2832800121, 'author': 'insomnes', 'body': 'I think these changes also somewhat solve\n https://github.com/apache/airflow/issues/44779', 'created_at': datetime.datetime(2025, 2, 7, 10, 1, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2645852413, 'issue_id': 2832800121, 'author': 'insomnes', 'body': ""Can someone with the rights please set reviewers (or even take a look) for the new PR?\n\nIt is redo of the PR already approved by Ryan, but I've messed it up during rebase for providers transition and closes it. \n\nSo after re-creation it fell in a loophole when CODEOWNERS file was broken due to mentioned transition, and no reviewers were set automatically.\n\nhttps://github.com/apache/airflow/pull/46535"", 'created_at': datetime.datetime(2025, 2, 8, 17, 11, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2645888153, 'issue_id': 2832800121, 'author': 'potiuk', 'body': 'Please do not ""rush"" things. You seem to be pretty impatient to get reviews. But this is open-source and people review things (and generally react) when they feel like they have some time or when they are interested in certain features or where someone is persistent (but also patient) enough to remind about their PR. BUT those are mostly volunteers here. 72 HRS for ANY reaction time is an absolute minimum you can expect. People here work in their free time often. They have holidays, weekens, sometimes families they tend to. Or simply might be busy at work and decide for a while not to spend their FREE time on the project. \n\nAnd CODEOWNERS are not really well used so far - they were - in many cases not even used. I just - lterally an hour ago started a discussion about that on our devlist https://lists.apache.org/thread/xq7837nd2j99slb558ls7b6jqntq6y32 and if you would like to add your thinking on how things should be working there from a user perspective - you are absolutley free to joing the conversation (see `community` tab on our website to know how to join the devlist).\n\nGenerally speaking - you should be patient and wait for maintainers to pick an interest and if they don\'t, you should re-ping (again in -general - not individual people) after reasonable time - counted in several days, not hours.  This is also described in our contribution guides - particularly https://github.com/apache/airflow/blob/main/contributing-docs/16_contribution_workflow.rst  where you can learn more (and I suggest you generally read the whole contributor\'s guide - it\'s pretty good read).\n\nAlso - if I may suggest - open a few more PRs while you are waiting. One of the things here is that different areas are often tended to by different people and some of them might pick interested in different PRs faster of slower. By having several different things to work on you can account for the asynchronous -  and apparently longer that maybe you are used to -feedback time. Having several things in parallel helps with that enormously.', 'created_at': datetime.datetime(2025, 2, 8, 18, 14, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2645896042, 'issue_id': 2832800121, 'author': 'insomnes', 'body': 'My main concern was that the current approach for reviewing was not very clear to me, and missing the reviewers in PR was looking like it gonna stale as a lot of PRs here even with reviewers set. Thank you for pointing to the contribution workflow doc! I\'ve missed it. I will take a proper read now to clear all things. \n\nYou are right that I am not used to ""no reaction"" for 24h+ and I should be more patient. \nI didn\'t want to make any nuance and will try to implement your advice about multiple tasks to work on, thank you!\nI am just really interested in the development of this awesome system. \n\n>  I just - lterally an hour ago started a discussion about that on our devlist https://lists.apache.org/thread/xq7837nd2j99slb558ls7b6jqntq6y32 \n\nYeah, I am already subscribed to the list and will try to add my 2 cents on the improvement of the process', 'created_at': datetime.datetime(2025, 2, 8, 18, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2645897524, 'issue_id': 2832800121, 'author': 'potiuk', 'body': '> My main concern was that the current approach for reviewing was not very clear to me, and missing the reviewers in PR was looking like it gonna stale as a lot of PRs here even with reviewers set. Thank you for pointing to the contribution workflow doc! I\'ve missed it. I will take a proper read now to clear all things.\n> \n> You are right that I am not used to ""no reaction"" for 24h+ and I should be more patient. I didn\'t want to make any nuance and will try to implement your advice about multiple tasks to work on, thank you! I am just really interested in the development of this awesome system.\n> \n> > I just - lterally an hour ago started a discussion about that on our devlist https://lists.apache.org/thread/xq7837nd2j99slb558ls7b6jqntq6y32\n> \n> Yeah, I am already subscribed to the list and will try to add my 2 cents on the improvement of the process\n\nCool.. BTW. Sometimes even to ""think"" about the response will take time - and you also have to take into account that some people here handle multiple 100s PRs/Issues a week. \n\nPatience and opening several streams of work to contribute is key ingredient of success in OSS contributions :)', 'created_at': datetime.datetime(2025, 2, 8, 18, 40, 3, tzinfo=datetime.timezone.utc)}]","RNHTTR on (2025-02-05 17:06:23 UTC): For future reference, there's no need to open an issue if you already have a PR submitted :)

insomnes (Issue Creator) on (2025-02-05 18:37:03 UTC): Yeah, sorry for that, I thought that for features it's different from bug fixes. Thanks for clarifying this for me!

insomnes (Issue Creator) on (2025-02-07 10:01:14 UTC): I think these changes also somewhat solve
 https://github.com/apache/airflow/issues/44779

insomnes (Issue Creator) on (2025-02-08 17:11:33 UTC): Can someone with the rights please set reviewers (or even take a look) for the new PR?

It is redo of the PR already approved by Ryan, but I've messed it up during rebase for providers transition and closes it. 

So after re-creation it fell in a loophole when CODEOWNERS file was broken due to mentioned transition, and no reviewers were set automatically.

https://github.com/apache/airflow/pull/46535

potiuk on (2025-02-08 18:14:39 UTC): Please do not ""rush"" things. You seem to be pretty impatient to get reviews. But this is open-source and people review things (and generally react) when they feel like they have some time or when they are interested in certain features or where someone is persistent (but also patient) enough to remind about their PR. BUT those are mostly volunteers here. 72 HRS for ANY reaction time is an absolute minimum you can expect. People here work in their free time often. They have holidays, weekens, sometimes families they tend to. Or simply might be busy at work and decide for a while not to spend their FREE time on the project. 

And CODEOWNERS are not really well used so far - they were - in many cases not even used. I just - lterally an hour ago started a discussion about that on our devlist https://lists.apache.org/thread/xq7837nd2j99slb558ls7b6jqntq6y32 and if you would like to add your thinking on how things should be working there from a user perspective - you are absolutley free to joing the conversation (see `community` tab on our website to know how to join the devlist).

Generally speaking - you should be patient and wait for maintainers to pick an interest and if they don't, you should re-ping (again in -general - not individual people) after reasonable time - counted in several days, not hours.  This is also described in our contribution guides - particularly https://github.com/apache/airflow/blob/main/contributing-docs/16_contribution_workflow.rst  where you can learn more (and I suggest you generally read the whole contributor's guide - it's pretty good read).

Also - if I may suggest - open a few more PRs while you are waiting. One of the things here is that different areas are often tended to by different people and some of them might pick interested in different PRs faster of slower. By having several different things to work on you can account for the asynchronous -  and apparently longer that maybe you are used to -feedback time. Having several things in parallel helps with that enormously.

insomnes (Issue Creator) on (2025-02-08 18:36:00 UTC): My main concern was that the current approach for reviewing was not very clear to me, and missing the reviewers in PR was looking like it gonna stale as a lot of PRs here even with reviewers set. Thank you for pointing to the contribution workflow doc! I've missed it. I will take a proper read now to clear all things. 

You are right that I am not used to ""no reaction"" for 24h+ and I should be more patient. 
I didn't want to make any nuance and will try to implement your advice about multiple tasks to work on, thank you!
I am just really interested in the development of this awesome system. 


Yeah, I am already subscribed to the list and will try to add my 2 cents on the improvement of the process

potiuk on (2025-02-08 18:40:03 UTC): Cool.. BTW. Sometimes even to ""think"" about the response will take time - and you also have to take into account that some people here handle multiple 100s PRs/Issues a week. 

Patience and opening several streams of work to contribute is key ingredient of success in OSS contributions :)

"
2832483434,issue,closed,completed,Focus DAG search box on cmd/ctrl+K,"### Description

In many websites, pressing Cmd/Ctrl+K brings up search functionality. We can add the same to airflow. This would be especially useful on pages that contain the DAG search box, e.g. home, DAG details, etc.

I have no experience with front-end development, so I don't know the drawbacks/difficulties of trapping Cmd/Ctrl+K (although it looks straightforward).

### Use case/motivation

Allow the user to quickly search for a DAG, without needing to switch hands away from the keyboard after e,g logging in or opening the webpage from e.g. favourites.

Example flows:
- jump to dag after login: enter login credentials -> press enter -> press cmd+k -> type dag id -> press enter
    * this currently requires the user to press tab (a lot) or use the mouse
 - jump to dag after opening page: in browser: ctrl/cml + L -> ""airflow"" -> press enter -> (already logged in) -> press cmd+k -> type dag id -> press enter

### Related issues

Could not find any.

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Jari27,2025-02-05 10:08:02+00:00,[],2025-02-05 15:47:43+00:00,2025-02-05 15:47:43+00:00,https://github.com/apache/airflow/issues/46458,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2636285520, 'issue_id': 2832483434, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 2, 5, 10, 8, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2636453818, 'issue_id': 2832483434, 'author': 'tirkarthi', 'body': ""This was done in new UI in https://github.com/apache/airflow/pull/45908 for the quick search. The same can be done for home page to focus on search. I can take this up. The changes won't be done in Airflow 2 UI since it's deprecated for removal."", 'created_at': datetime.datetime(2025, 2, 5, 11, 20, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2636508266, 'issue_id': 2832483434, 'author': 'Jari27', 'body': ""That'd be really nice! I came as far as this: https://github.com/apache/airflow/compare/main...Jari27:airflow:focus-search-box-on-cmd-k; but I believe that's for 2.10?"", 'created_at': datetime.datetime(2025, 2, 5, 11, 47, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2636581259, 'issue_id': 2832483434, 'author': 'tirkarthi', 'body': ""I have created https://github.com/apache/airflow/pull/46463 for Airflow 3. I don't think features to 2.10 UI is accepted at this stage."", 'created_at': datetime.datetime(2025, 2, 5, 12, 21, 9, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-02-05 10:08:05 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

tirkarthi on (2025-02-05 11:20:22 UTC): This was done in new UI in https://github.com/apache/airflow/pull/45908 for the quick search. The same can be done for home page to focus on search. I can take this up. The changes won't be done in Airflow 2 UI since it's deprecated for removal.

Jari27 (Issue Creator) on (2025-02-05 11:47:27 UTC): That'd be really nice! I came as far as this: https://github.com/apache/airflow/compare/main...Jari27:airflow:focus-search-box-on-cmd-k; but I believe that's for 2.10?

tirkarthi on (2025-02-05 12:21:09 UTC): I have created https://github.com/apache/airflow/pull/46463 for Airflow 3. I don't think features to 2.10 UI is accepted at this stage.

"
2832276564,issue,open,,Change default logical date to null when manual trigger from UI,"### Body

Split from #46189 so we can track frontend and backend changes separately.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",uranusjr,2025-02-05 08:38:42+00:00,"['uranusjr', 'sunank200']",2025-02-06 10:30:45+00:00,,https://github.com/apache/airflow/issues/46455,"[('kind:feature', 'Feature Requests'), ('kind:meta', 'High-level information important to the community'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-83', 'Remove Execution Date Unique Constraint from DAG Run')]","[{'comment_id': 2639429986, 'issue_id': 2832276564, 'author': 'sunank200', 'body': 'PR: [46512](https://github.com/apache/airflow/pull/46512)', 'created_at': datetime.datetime(2025, 2, 6, 10, 30, 43, tzinfo=datetime.timezone.utc)}]","sunank200 (Assginee) on (2025-02-06 10:30:43 UTC): PR: [46512](https://github.com/apache/airflow/pull/46512)

"
2832073085,issue,open,,Get rid of *.zip in tests cases,"### Description

We should generate the zip file in our test case (i.e., include the files we want to zip but not the zip file itself) and then do the test instead of uploading it to the repo. Currently, we have the following 2 zip files.

* tests/dags/test_dag_warnings.zip
* tests/dags/test_zip.zip

In addition to it, we should have a pre-commit to avoid someone from commit a zip file.


### Use case/motivation

zip files are hard to review and could incur security issues. 



### Related issues

https://github.com/apache/airflow/pull/46231#issuecomment-2630114344

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2025-02-05 06:54:16+00:00,['josix'],2025-02-05 07:19:40+00:00,,https://github.com/apache/airflow/issues/46449,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:dev-tools', ''), ('kind:feature', 'Feature Requests'), ('good first issue', '')]","[{'comment_id': 2635870744, 'issue_id': 2832073085, 'author': 'josix', 'body': 'I can take this one.', 'created_at': datetime.datetime(2025, 2, 5, 7, 16, 10, tzinfo=datetime.timezone.utc)}]","josix (Assginee) on (2025-02-05 07:16:10 UTC): I can take this one.

"
2831532288,issue,open,,Add Selective Check for Caplog Usage,"### Description

Add a special check for caplog usage. It should ask for maintainer approval if it is not `Mocked`.

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2025-02-04 23:55:49+00:00,['bugraoz93'],2025-02-07 00:04:23+00:00,,https://github.com/apache/airflow/issues/46445,"[('kind:feature', 'Feature Requests'), ('area:CI', ""Airflow's tests and continious integration"")]","[{'comment_id': 2641420362, 'issue_id': 2831532288, 'author': 'potiuk', 'body': 'Nice!', 'created_at': datetime.datetime(2025, 2, 7, 0, 4, 22, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-02-07 00:04:22 UTC): Nice!

"
2831491954,issue,closed,completed,Add locking for bundle initialize on workers,"### Body

Right now it's possible for more than one task to be trying to initialize a bundle at the same time, which can lead to git failing, for example.

We need to use lock files, like we are building for ttl, to also serialize this.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2025-02-04 23:24:13+00:00,['jedcunningham'],2025-02-06 11:00:06+00:00,2025-02-06 11:00:06+00:00,https://github.com/apache/airflow/issues/46444,"[('kind:meta', 'High-level information important to the community'), ('area:core', ''), ('AIP-66: DAG Bundle/Manifest', '')]",[],
2831382959,issue,open,,"Status of testing Providers that were prepared on February 04, 2025","I have a kind request for all the contributors to the latest provider packages release.
Could you please help us to test the RC versions of the providers?

The guidelines on how to test providers can be found in

[Verify providers by contributors](https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDER_PACKAGES.md#verify-the-release-candidate-by-contributors)

Let us know in the comments, whether the issue is addressed.

These are providers that require testing as there were some substantial changes introduced:


## Provider [fab: 1.5.3rc1](https://pypi.org/project/apache-airflow-providers-fab/1.5.3rc1)
   - [ ] [[providers-fab/v1-5] Use different default algorithms for different werkzeug versions (#46384) (#46392)](https://github.com/apache/airflow/pull/46392): @potiuk
   - [ ] [[providers-fab/v1-5] Upgrade to FAB 4.5.3 (#45874) (#45918)](https://github.com/apache/airflow/pull/45918): @potiuk

<!--

NOTE TO RELEASE MANAGER:

You can move here the providers that have doc-only changes or for which changes are trivial, and
you could assess that they are OK.

-->
All users involved in the PRs:
@potiuk ",potiuk,2025-02-04 21:57:25+00:00,[],2025-02-04 21:59:41+00:00,,https://github.com/apache/airflow/issues/46442,"[('area:providers', ''), ('kind:meta', 'High-level information important to the community'), ('testing status', 'Status of testing releases'), ('provider:fab', '')]",[],
2830824998,issue,open,,Task logs readability in Airflow 3 is not good as compared to Airflow 2,"### Apache Airflow version

3.0.0a1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Task logs are much more readable in Airflow 2 as compared to Airflow 3

### What you think should happen instead?

_No response_

### How to reproduce

Run the below DAG and compare the logs in Airflow 2 and Airflow 3

```python
from airflow import DAG
from airflow.providers.standard.operators.bash import BashOperator

dag = DAG(
    'hello_world_dag',
    schedule=None,
    catchup=False,
)

hello_task = BashOperator(
    task_id='say_hello',
    bash_command='echo ""Hello World from Airflow!""',
    do_xcom_push = True,
    dag=dag,
)

hello_task

``` 

**Screenshots**:

Airflow 2.10

<img width=""1223"" alt=""Image"" src=""https://github.com/user-attachments/assets/14922eb2-437a-4842-afd5-232e1e53ad3b"" />

Airflow 3 (Tag 3.0.0a1)

<img width=""1703"" alt=""Image"" src=""https://github.com/user-attachments/assets/34d5d44d-03aa-4b4e-8eb6-0418a6d1e253"" />

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",atul-astronomer,2025-02-04 17:04:11+00:00,[],2025-02-07 10:30:20+00:00,,https://github.com/apache/airflow/issues/46435,"[('kind:bug', 'This is a clearly a bug'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('affected_version:3.0.0alpha', 'For all 3.0.0 alpha releases')]","[{'comment_id': 2636455912, 'issue_id': 2830824998, 'author': 'tirkarthi', 'body': 'cc: @ashb @kaxil @amoghrajesh', 'created_at': datetime.datetime(2025, 2, 5, 11, 21, 24, tzinfo=datetime.timezone.utc)}]","tirkarthi on (2025-02-05 11:21:24 UTC): cc: @ashb @kaxil @amoghrajesh

"
2830799182,issue,closed,completed,Cannot use the SQLExecuteQueryOperator with elastic connection type (e.g. ElasticsearchSQLHook ),"### Apache Airflow Provider(s)

elasticsearch

### Versions of Apache Airflow Providers

apache-airflow-providers-elasticsearch==6.0.0

### Apache Airflow version

2.10.4

### Operating System

Linux

### Deployment

Other 3rd-party Helm chart

### Deployment details

_No response_

### What happened

Today we tried to use the Elasticsearch connection type (e.g. ElasticsearchSQLHook ) through the SQLExecuteQueryOperator.
You would expect it to work as the ElasticsearchSQLHook extends the DbApiHook, unfortunately it doesn't as it is missing some implementations.

### What you think should happen instead

I would expect the Elasticsearch connection type to work with the SQLExecuteQueryOperator, as it supports executing SQL statements.

### How to reproduce

Just use the SQLExecuteQueryOperator with the Elasticsearch connection type.  Also in the TestElasticsearchSQLHook unit test you can see the cursor is mocked but the ESConnection wrapper class doesn't actually return a cursor.

### Anything else

We've patched the ElasticsearchSQLHook in our Airflow installation to make it work, so I created a [PR ](https://github.com/apache/airflow/pull/46439) for this to make it work in elasticsearch provider

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",dabla,2025-02-04 16:51:59+00:00,[],2025-02-06 16:08:28+00:00,2025-02-06 16:08:28+00:00,https://github.com/apache/airflow/issues/46434,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:elasticsearch', '')]",[],
2830774435,issue,open,,The `test_refresh_dags_dir_deactivates_deleted_zipped_dags` is flaky,"Flaky test in DAG zipped dags


```python
=================================== FAILURES ===================================
_ TestDagFileProcessorManager.test_refresh_dags_dir_deactivates_deleted_zipped_dags _
tests/dag_processing/test_manager.py:636: in test_refresh_dags_dir_deactivates_deleted_zipped_dags
    assert not dag.get_is_active()
E   assert not True
E    +  where True = get_is_active()
E    +    where get_is_active = <DAG: test_zip_dag>.get_is_active
----------------------------- Captured stdout call -----------------------------
[2025-02-04T16:02:01.361+0000] {dagbag.py:574} INFO - Filling up the DagBag from /tmp/pytest-of-root/pytest-0/test_refresh_dags_dir_deactiva0
[2025-02-04T16:02:01.364+0000] {dagbag.py:442} INFO - File /opt/airflow/tests/dags/test_zip.zip:file_no_airflow_dag.py assumed to contain no DAGs. Skipping.
[2025-02-04T16:02:01.371+0000] {collection.py:83} INFO - Creating ORM DAG for test_zip_dag
[2025-02-04T16:02:01.391+0000] {manager.py:231} INFO - Processing files using up to 2 processes at a time 
[2025-02-04T16:02:01.391+0000] {manager.py:232} INFO - Process each file at most once every 30 seconds
[2025-02-04T16:02:01.392+0000] {manager.py:98} INFO - DAG bundles loaded: testing
[2025-02-04T16:02:01.394+0000] {manager.py:98} INFO - DAG bundles loaded: testing
[2025-02-04T16:02:01.395+0000] {manager.py:443} INFO - Refreshing DAG bundles
[2025-02-04T16:02:01.398+0000] {manager.py:531} INFO - Searching for files in testing at /opt/airflow/tests/dags
[2025-02-04T16:02:01.416+0000] {manager.py:533} INFO - Found 41 files for bundle testing
[2025-02-04T16:02:01.466+0000] {manager.py:704} INFO - 
================================================================================
```

Example failure: https://github.com/apache/airflow/actions/runs/13139235278/job/36662824378?pr=46431#step:6:3443",potiuk,2025-02-04 16:40:32+00:00,[],2025-02-04 16:42:47+00:00,,https://github.com/apache/airflow/issues/46432,"[('area:CI', ""Airflow's tests and continious integration""), ('area:core', '')]",[],
2830589884,issue,closed,completed,Sometimes our tests fail with undefined symbol: GENERAL_NAME_free,"Sometimes our tests fail with this error that indicates mismatch between libSSL that is embedded in psycopg2 and the one available in the system. Given that this appears very rarely and randomly, and that we are using containers for all the runs, it looks like it is environmental and likely related to Kernel Version used at the Github Runners we get.

It should eventually go away (hopefully) - it's hard to think about remediation if that's the real root cause.

```python
==================================== ERRORS ====================================
____ ERROR collecting tests/provider_tests/postgres/assets/test_postgres.py ____
ImportError while importing test module '/opt/airflow/providers/postgres/tests/provider_tests/postgres/assets/test_postgres.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
/usr/local/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
providers/postgres/tests/provider_tests/postgres/assets/test_postgres.py:24: in <module>
    from airflow.providers.postgres.assets.postgres import sanitize_uri
airflow/__init__.py:78: in <module>
    settings.initialize()
airflow/settings.py:642: in initialize
    configure_orm()
airflow/settings.py:366: in configure_orm
    engine = create_engine(SQL_ALCHEMY_CONN, connect_args=connect_args, **engine_args, future=True)
/usr/local/lib/python3.12/site-packages/sqlalchemy/util/deprecations.py:309: in warned
    return fn(*args, **kwargs)
/usr/local/lib/python3.12/site-packages/sqlalchemy/engine/create.py:560: in create_engine
    dbapi = dialect_cls.dbapi(**dbapi_args)
/usr/local/lib/python3.12/site-packages/sqlalchemy/dialects/postgresql/psycopg2.py:782: in dbapi
    import psycopg2
/usr/local/lib/python3.12/site-packages/psycopg2/__init__.py:51: in <module>
    from psycopg2._psycopg import (                     # noqa
E   ImportError: /usr/local/lib/python3.12/site-packages/psycopg2/_psycopg.cpython-312-x86_64-linux-gnu.so: undefined symbol: GENERAL_NAME_free
___ ERROR collecting tests/provider_tests/postgres/dialects/test_postgres.py ___
/usr/local/lib/python3.12/logging/config.py:105: in _resolve
    found = getattr(found, n)
E   AttributeError: partially initialized module 'airflow' has no attribute 'utils' (most likely due to a circular import)
```",potiuk,2025-02-04 15:25:56+00:00,['potiuk'],2025-02-04 16:41:45+00:00,2025-02-04 16:41:45+00:00,https://github.com/apache/airflow/issues/46430,"[('area:CI', ""Airflow's tests and continious integration""), ('provider:postgres', '')]",[],
2830569474,issue,open,,Add support for preinjecting connection for bundles,"### Body

Some bundles will end up needing a connection - we should preinject it so we don't have to hit the execution api right away when starting a task. We will have to do it in a generic way though, since not all bundles will rely on connections.

We can send it as part of [this response](https://github.com/apache/airflow/blob/3c732fbc088b9baa849b7a53c307642a9f4328ec/airflow/api_fastapi/execution_api/datamodels/taskinstance.py#L230-L243) to [this request](https://github.com/apache/airflow/blob/3c732fbc088b9baa849b7a53c307642a9f4328ec/task_sdk/src/airflow/sdk/execution_time/supervisor.py#L611-L625).

This can happen post 3.0.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2025-02-04 15:17:52+00:00,[],2025-02-07 13:45:46+00:00,,https://github.com/apache/airflow/issues/46429,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('kind:meta', 'High-level information important to the community'), ('AIP-66: DAG Bundle/Manifest', '')]","[{'comment_id': 2642989143, 'issue_id': 2830569474, 'author': 'jx2lee', 'body': '@jedcunningham Can I take this? Please assign me. 🙏', 'created_at': datetime.datetime(2025, 2, 7, 13, 45, 45, tzinfo=datetime.timezone.utc)}]","jx2lee on (2025-02-07 13:45:45 UTC): @jedcunningham Can I take this? Please assign me. 🙏

"
2830384709,issue,open,,Improve error handling and logging in CommsDecoder interface,"Resurrect https://github.com/apache/airflow/pull/45927 and extend it.

Every request should have a response that the client reads.

We need the ability to pass/send errors from Supervisor to the task code (for instance if there is an error performing an action, or if the XCom can't be found) -- right now each end point handles this itself in a different way.

https://github.com/apache/airflow/blob/eb05869b6de611185b47539a443bf131ad7702bd/task_sdk/src/airflow/sdk/api/client.py#L267-L282 for example shouldn't be handled in the API Client, but it should expose/raise the 404 to the Task itself and it should be handled there",ashb,2025-02-04 14:09:48+00:00,[],2025-02-04 14:21:45+00:00,,https://github.com/apache/airflow/issues/46426,"[('area:logging', '')]",[],
2830125889,issue,closed,completed,snowflake_conn_id not a template_field in SnowflakeSqlApiOperator,"### Apache Airflow Provider(s)

snowflake

### Versions of Apache Airflow Providers

apache-airflow-providers-snowflake==5.8.1

### Apache Airflow version

2.10.4

### Operating System

Debian Bookwork

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### What happened

`SnowflakeSqlApiOperator`'s `snowflake_conn_id` field is not templatable, unlike the other Snowflake Operators where `snowflake_conn_id` is a template_field.

### What you think should happen instead

`SnowflakeSqlApiOperator`'s `snowflake_conn_id` field should be a `template_field` to be consistent with the other operators.

### How to reproduce

```
from airflow import DAG
from airflow.providers.snowflake.operators.snowflake import SnowflakeSqlApiOperator
import pendulum

with DAG(
  dag_id='snowflake',
  start_date=pendulum.datetime(2025,1,1),
  schedule=None,
  catchup=False,
):
  SnowflakeSqlApiOperator(
    task_id='snowflake',
    snowflake_conn_id='snowflake-{{var.value.some_var}}',
    sql='select 'hello' as hello;'
  )
```


### Anything else

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bbossy,2025-02-04 12:34:49+00:00,[],2025-02-06 13:12:28+00:00,2025-02-06 13:12:28+00:00,https://github.com/apache/airflow/issues/46421,"[('area:providers', ''), ('kind:feature', 'Feature Requests'), ('good first issue', ''), ('provider:snowflake', 'Issues related to Snowflake provider'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2633780356, 'issue_id': 2830125889, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 2, 4, 12, 34, 51, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-02-04 12:34:51 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2829902758,issue,open,,Env variables are not getting exported in the logs using PythonOperator,"### Apache Airflow version

Tag: 3.0.0a1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Env vars are not getting exported to the logs when user uses default_args parameter in the DAG with PythonOperator.

**Stack trace:**

[dag_id=default_args_owner_run_id=manual__2025-02-04T10_55_46.978360+00_00_task_id=check_for_owner_in_def_args_attempt=1.log](https://github.com/user-attachments/files/18656321/dag_id.default_args_owner_run_id.manual__2025-02-04T10_55_46.978360%2B00_00_task_id.check_for_owner_in_def_args_attempt.1.log)

### What you think should happen instead?

The logs should contain env variable logs like for BashOperator:

`{""timestamp"":""2025-02-04T10:54:46.602494"",""level"":""debug"",""event"":""Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='catchup_test' AIRFLOW_CTX_TASK_ID='catchup' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-02-04T10:54:43.944208+00:00'"",""logger"":""airflow.task.operators.airflow.providers.standard.operators.bash.BashOperator""}`

### How to reproduce

Use the below DAG to reproduce:

```python
from datetime import timedelta

from pendulum import today

from airflow.models import DAG
from dags.plugins.airflow_dag_introspection import log_checker
from providers.standard.src.airflow.providers.standard.operators.python import PythonOperator

def easy_return():
    pass


default_args = {""owner"": ""airflow""}

with DAG(
    dag_id=""default_args_owner"",
    start_date=today('UTC').add(days=-1),
    schedule=None,
    default_args=default_args,
    doc_md=docs,
    tags=[""dagparams""],
) as dag:

    py0 = PythonOperator(task_id=""dummy1"", python_callable=easy_return)

    py1 = PythonOperator(
        task_id=""check_for_owner_in_def_args"",
        python_callable=log_checker,
        op_args=[
            ""dummy1"",
            ""AIRFLOW_CTX_DAG_OWNER='airflow'"",
            ""AIRFLOW_CTX_DAG_OWNER='astro'"",
        ],
    )
py0 >> py1

``` 

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",atul-astronomer,2025-02-04 11:25:29+00:00,['Rakshars'],2025-02-06 08:00:23+00:00,,https://github.com/apache/airflow/issues/46418,"[('kind:feature', 'Feature Requests'), ('good first issue', ''), ('area:core', ''), ('provider:standard', ''), ('affected_version:3.0.0alpha', 'For all 3.0.0 alpha releases')]","[{'comment_id': 2637682005, 'issue_id': 2829902758, 'author': 'potiuk', 'body': 'Whoever will be importing that. You have to be really careful about variables printed in logs. There are some cases where printing variables migh reveal sensitive information so extra care should be used when implementing this feature.', 'created_at': datetime.datetime(2025, 2, 5, 18, 15, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2638865095, 'issue_id': 2829902758, 'author': 'Rakshars', 'body': 'Hey, this is my first time contributing, can I please try and solve this issue.', 'created_at': datetime.datetime(2025, 2, 6, 5, 21, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2639092750, 'issue_id': 2829902758, 'author': 'potiuk', 'body': 'Feel free.', 'created_at': datetime.datetime(2025, 2, 6, 8, 0, 22, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-02-05 18:15:49 UTC): Whoever will be importing that. You have to be really careful about variables printed in logs. There are some cases where printing variables migh reveal sensitive information so extra care should be used when implementing this feature.

Rakshars (Assginee) on (2025-02-06 05:21:59 UTC): Hey, this is my first time contributing, can I please try and solve this issue.

potiuk on (2025-02-06 08:00:22 UTC): Feel free.

"
2829851969,issue,open,,Xcom pull is failing when is key is None,"### Apache Airflow version

3.0.0a1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

` pulled_value_1 = ti.xcom_pull(key=None, task_ids=""push"")` in failing in AF3, however, same code works with `2.10.4`. Looks like when I provide the key it works fine. I am raising this issue as there is a deviation of behaviour from AF2 here. Maybe we can handle this in task-sdk

**3.0.0a1**

<img width=""1525"" alt=""Image"" src=""https://github.com/user-attachments/assets/b79e2124-43c3-4e98-a715-a5da7e55e0e4"" />

**2.10.4**

<img width=""1388"" alt=""Image"" src=""https://github.com/user-attachments/assets/b7006021-4179-402d-a639-8d42388bd9fc"" />

`{""timestamp"":""2025-02-04T11:01:20.886364"",""level"":""error"",""event"":""Task failed with exception"",""logger"":""task"",""error_detail"":[{""exc_type"":""ValidationError"",""exc_value"":""1 validation error for GetXCom\nkey\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    For further information visit [https://errors.pydantic.dev/2.10/v/string_type"",""syntax_error"":null,""is_cause"":false,""frames"":[](https://errors.pydantic.dev/2.10/v/string_type%22,%22syntax_error%22:null,%22is_cause%22:false,%22frames%22:[){""filename"":""/opt/airflow/task_sdk/src/airflow/sdk/execution_time/task_runner.py"",""lineno"":545,""name"":""run""},{""filename"":""/opt/airflow/task_sdk/src/airflow/sdk/execution_time/task_runner.py"",""lineno"":645,""name"":""_execute_task""},{""filename"":""/opt/airflow/airflow/models/baseoperator.py"",""lineno"":173,""name"":""wrapper""},{""filename"":""/opt/airflow/providers/standard/src/airflow/providers/standard/operators/python.py"",""lineno"":196,""name"":""execute""},{""filename"":""/opt/airflow/providers/standard/src/airflow/providers/standard/operators/python.py"",""lineno"":222,""name"":""execute_callable""},{""filename"":""/opt/airflow/airflow/utils/operator_helpers.py"",""lineno"":261,""name"":""run""},{""filename"":""/files/dags/example_xcom.py"",""lineno"":34,""name"":""puller""},{""filename"":""/opt/airflow/task_sdk/src/airflow/sdk/execution_time/task_runner.py"",""lineno"":266,""name"":""xcom_pull""},{""filename"":""/usr/local/lib/python3.9/site-packages/pydantic/main.py"",""lineno"":214,""name"":""__init__""}]}]}
`



### What you think should happen instead?

Same code working in AF2 should work in AF3 as well

### How to reproduce

Use below DAG to replicate
**DAG CODE** 



```
""""""Example DAG demonstrating the usage of XComs.""""""
from airflow import DAG
from airflow.providers.standard.operators.python import PythonOperator
from datetime import datetime, timedelta

dag = DAG(
    ""example_xcom"",
    start_date=datetime(2023, 11, 28),
    default_args={""owner"": ""airflow""},
    schedule=""@daily"",
    catchup=False,
    tags=[""core""],
)

value_1 = [1, 2, 3]
value_2 = {""a"": ""b""}


def push(**kwargs):
    """"""Pushes an XCom without a specific target""""""
    kwargs[""ti""].xcom_push(key=""value from pusher 1"", value=value_1)


def push_by_returning(**kwargs):
    """"""Pushes an XCom without a specific target, just by returning it""""""
    return value_2


def puller(**kwargs):
    """"""Pull all previously pushed XComs and check if the pushed values match the pulled values.""""""
    ti = kwargs[""ti""]

    # get value_1
    pulled_value_1 = ti.xcom_pull(key=None, task_ids=""push"")
    if pulled_value_1 != value_1:
        raise ValueError(f""The two values differ {pulled_value_1} and {value_1}"")

    # get value_2
    pulled_value_2 = ti.xcom_pull(task_ids=""push_by_returning"")
    if pulled_value_2 != value_2:
        raise ValueError(f""The two values differ {pulled_value_2} and {value_2}"")

    # get both value_1 and value_2
    pulled_value_1, pulled_value_2 = ti.xcom_pull(
        key=None, task_ids=[""push"", ""push_by_returning""]
    )
    print(f""pulled_value_1 is {pulled_value_1}"")
    print(f""pulled_value_2 is {pulled_value_2}"")
    if pulled_value_1 != value_1:
        raise ValueError(f""The two values differ {pulled_value_1} and {value_1}"")
    if pulled_value_2 != value_2:
        raise ValueError(f""The two values differ {pulled_value_2} and {value_2}"")


push1 = PythonOperator(
    task_id=""push"",
    dag=dag,
    python_callable=push,
    depends_on_past=True,
)

push2 = PythonOperator(
    task_id=""push_by_returning"",
    dag=dag,
    python_callable=push_by_returning,
)

pull = PythonOperator(
    task_id=""puller"",
    dag=dag,
    python_callable=puller,
)

pull << [push1, push2]
```

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vatsrahul1001,2025-02-04 11:10:43+00:00,['amoghrajesh'],2025-02-07 10:30:00+00:00,,https://github.com/apache/airflow/issues/46417,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('affected_version:3.0.0alpha', 'For all 3.0.0 alpha releases')]","[{'comment_id': 2633803742, 'issue_id': 2829851969, 'author': 'amoghrajesh', 'body': 'Nice observation. Will pick it up soon', 'created_at': datetime.datetime(2025, 2, 4, 12, 45, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2642532528, 'issue_id': 2829851969, 'author': 'ashb', 'body': ""I'd almost argue that explicitly passing `key=None` is a bug in the user code. If you don't want to specify a key don't pass the argument?\n\n(Because passing None as an argument is different to not passing the argument.)"", 'created_at': datetime.datetime(2025, 2, 7, 10, 29, 13, tzinfo=datetime.timezone.utc)}]","amoghrajesh (Assginee) on (2025-02-04 12:45:44 UTC): Nice observation. Will pick it up soon

ashb on (2025-02-07 10:29:13 UTC): I'd almost argue that explicitly passing `key=None` is a bug in the user code. If you don't want to specify a key don't pass the argument?

(Because passing None as an argument is different to not passing the argument.)

"
2829672345,issue,open,,Add `@task.kubernetes_cmd` decorator to run returned command,"### Description

There is bash decorator which allows neat and simple bash command (returned str) execution. I want to add the same ability as `kubernetes_cmd` decorator. So we can configure the command we want to run inside specified image and not the python function.


### Use case/motivation

Decorator flow is short and simple way to run airflow tasks, it's also superb in templating terms.
When you use airflow as orchestrator mostly to run commands in specific images it would be nice to have something similar to bash operator:

```python
@task.bash
def also_run_this() -> str:
    return 'echo ""ti_key={{ task_instance_key_str }}""'

also_this = also_run_this()
```


Which will allow us not to run some python function (as current `@task.kubernetes` do), but the command returned from function executed on airflow Scheduler / Worker side.

```python
@task.kubernetes_cmd
def prepare_cmd() -> str:
    return 'python /path/inside/image/main.py --some-option={{ task_instance_key_str }}""""'

```

I mean that `prepare_cmd` should be run inside executor / worker with access to airflow environment and this way we unleash full templating power of airflow for command generation in combination with python code without work around.
The generated string command is used as actual cmds argument for operator (we may decide to use `list[str]` as needed return value`. 


This may simplify tons of `KubernetesPodOperator` usages in the described flow of container orchestration without the need to provide code on the airflow side.

### Related issues

_No response_

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",insomnes,2025-02-04 10:01:34+00:00,[],2025-02-04 11:07:54+00:00,,https://github.com/apache/airflow/issues/46414,"[('kind:feature', 'Feature Requests'), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2829558600,issue,open,,Connection schema is returned as None when fetching via jinja template,"### Apache Airflow version

Tag: 3.0.0a1

### What happened?

Connection schema is returned as None when fetching using `f""{{{{ conn.{dag_name}_connection.schema }}}}""`

**Stack trace:**

[dag_id=test_jinja_connection_id_run_id=manual__2025-02-04T06_33_59.506710+00_00_task_id=check_jinja_conn_id_attempt=1.log](https://github.com/user-attachments/files/18654603/dag_id.test_jinja_connection_id_run_id.manual__2025-02-04T06_33_59.506710%2B00_00_task_id.check_jinja_conn_id_attempt.1.log)

### What you think should happen instead?

Connection schema value should be returned correct and assertion should pass.

### How to reproduce

Use the below DAG to reproduce:
The connection can be created using UI or API having below parameters
`request_body = {
            ""connection_id"": f""{dag_name}_connection"",
            ""conn_type"": ""postgres"",
            ""description"": ""postgres"",
            ""host"": ""database-1.cxmxicvi57az.us-east-2.rds.amazonaws.com"",
            ""login"": ""postgres"",
            ""schema"": ""postgres"",
            ""port"": 5432,
            ""password"": POSTGRES_PASS
        }`

```python
import os

from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator
from airflow.providers.standard.operators.python import PythonOperator
from pendulum import today

from airflow.exceptions import AirflowNotFoundException
from airflow.hooks.base import BaseHook
from airflow.models import DAG
from dags.plugins import api_utility

dag_name = ""test_jinja_connection_id""

def conn_id_test(**context):
    print(""CONTEXT: "", context)
    print(f""The connection type is: {context['get_conn_type']}"")
    print(f""The host is: {context['check_host']}"")
    print(f""The schema is: {context['get_schema']}"")
    print(f""The login is: {context['get_login']}"")
    print(f""The password is: {context['get_pass']}"")  # returns '***' hides the password
    print(f""The port is: {context['get_port']}"")
    print(f""The extras are: {context['get_extras']}"")

    print(""asserting the connection type"")
    assert context[""get_conn_type""] == ""postgres""
    print(""asserting the host"")
    assert context[""check_host""] == POSTGRES_HOST
    print(""asserting the schema"")
    assert context[""get_schema""] == ""postgres""
    print(""asserting the login"")
    assert context[""get_login""] == ""postgres""
    print(""asserting the port"")
    assert context[""get_port""] == ""5432""
    # print(""asserting the extras"")
    # assert context[""get_extras""] == ""{'key': 'value'}""


with DAG(
    dag_id=dag_name,
    schedule=None,
    start_date=today('UTC').add(days=-2),
    doc_md=docs,
    tags=[""core""],
) as dag:

    P0 = SQLExecuteQueryOperator(
        task_id=""create_table_define_cols"",
        conn_id=f""{dag_name}_connection"",
        sql=""""""
            CREATE TABLE IF NOT EXISTS jinja_connection_template_test(
            random_str varchar,
            herbs varchar,
            primary key(herbs));
            """""",
    )

    py1 = PythonOperator(
        task_id=""check_jinja_conn_id"",
        python_callable=conn_id_test,
        op_kwargs={
            ""get_conn_type"": f""{{{{ conn.{dag_name}_connection.conn_type }}}}"",
            ""check_host"": f""{{{{ conn.{dag_name}_connection.host }}}}"",
            ""get_schema"": f""{{{{ conn.{dag_name}_connection.schema }}}}"",
            ""get_login"": f""{{{{ conn.{dag_name}_connection.login }}}}"",
            ""get_pass"": f""{{{{ conn.{dag_name}_connection.password }}}}"",
            ""get_port"": f""{{{{ conn.{dag_name}_connection.port }}}}"",
            ""get_extras"": f""{{{{ conn.{dag_name}_connection.extra }}}}"",
        },
    )


P0 >> py1

``` 

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",atul-astronomer,2025-02-04 09:12:38+00:00,['amoghrajesh'],2025-02-04 09:15:23+00:00,,https://github.com/apache/airflow/issues/46412,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:task-sdk', None), ('affected_version:3.0.0alpha', 'For all 3.0.0 alpha releases')]",[],
2829495738,issue,closed,completed,'RuntimeTaskInstance' object has no attribute 'get_dagrun',"### Apache Airflow version

Tag: 3.0.0a1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

The DAG has failed on the BranchSQLOperator stating TI object does not have attribute get_dagrun

**Stack trace:**

[dag_id=sql_check_operator_run_id=manual__2025-02-04T05_54_46.258183+00_00_task_id=branch_sql_attempt=1.log](https://github.com/user-attachments/files/18654430/dag_id.sql_check_operator_run_id.manual__2025-02-04T05_54_46.258183%2B00_00_task_id.branch_sql_attempt.1.log)

### What you think should happen instead?

The BranchSQLOperator should work and excute the query

### How to reproduce

The following DAG can be used to reproduce.
**This dag requires a postgres connection to be configured before you run it**

```python
from datetime import date, timedelta

from airflow.models import DAG
from airflow.providers.standard.operators.python import PythonOperator
from airflow.providers.common.sql.operators.sql import (
    SQLCheckOperator,
    SQLIntervalCheckOperator,
    SQLThresholdCheckOperator,
    SQLValueCheckOperator,
    SQLExecuteQueryOperator
)
from airflow.operators.empty import EmptyOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.utils.trigger_rule import TriggerRule
from airflow.utils.state import TaskInstanceState
from pendulum import today

from dags.plugins.elephantsql_kashin import conn_id as CONN_ID

from dags.plugins.airflow_dag_introspection import assert_the_task_states
from providers.common.sql.src.airflow.providers.common.sql.operators.sql import BranchSQLOperator

docs = """"""
####Info
This dag requires a postgres connection to be configured before you run it
####Purpose
This dag tests BranchSQLOperator, SQLCheckOperator, SQLIntervalCheckOperator, SQLThresholdCheckOperator and SQLValueCheckOperator
####Expected Behavior
This dag has 11 tasks 9 of which are expected to succeed and 2 tasks that are expected to be skipped.\n
This dag should pass.

""""""

DATES = []
for i in range(6):
    DATES.append((date.today() - timedelta(days=i)).strftime(""%Y-%m-%d""))

TABLE = ""checktable""
DROP = f""DROP TABLE IF EXISTS {TABLE} CASCADE;""
CREATE = f""CREATE TABLE IF NOT EXISTS {TABLE}(state varchar, temp integer, date date)""
INSERT = f""""""
    INSERT INTO {TABLE}(state, temp, date)
    VALUES ('Lagos', 23, '{DATES[4]}'),
        ('Enugu', 25, '{DATES[3]}'),
        ('Delta', 25, '{DATES[2]}'),
        ('California', 28, '{DATES[1]}'),
        ('Abuja', 25, '{DATES[0]}')
    """"""

SQLBOOL_QUERY = f""""""
SELECT CAST(CASE WHEN COUNT(*) > 0 THEN 1 ELSE 0 END AS BIT)
FROM {TABLE} WHERE temp = 30;
""""""


def prepare_data():
    postgres = PostgresHook(CONN_ID)
    with postgres.get_conn() as conn:
        with conn.cursor() as cur:
            cur.execute(DROP)
            cur.execute(CREATE)
            cur.execute(INSERT)
        conn.commit()


with DAG(
    dag_id=""sql_check_operator"",
    default_args={""owner"": ""airflow"", ""start_date"": today('UTC').add(days=-2)},
    schedule=None,
    tags=[""core"", ""psql""],
    doc_md=docs,
) as dag:
    t1 = PythonOperator(task_id=""prepare_table"", python_callable=prepare_data)

    t2 = SQLCheckOperator(
        task_id=""sql_check"", sql=f""SELECT COUNT(*) FROM {TABLE}"", conn_id=CONN_ID
    )
    t3 = SQLValueCheckOperator(
        task_id=""sql_check_value"",
        sql=f""SELECT COUNT(*) FROM {TABLE}"",
        pass_value=5,
        conn_id=CONN_ID,
    )
    t4 = BranchSQLOperator(
        conn_id=CONN_ID,
        task_id=""branch_sql"",
        sql=SQLBOOL_QUERY,
        follow_task_ids_if_false=""add_state"",
        follow_task_ids_if_true=""remove_state"",
    )
    t5 = SQLExecuteQueryOperator(
        conn_id=CONN_ID,
        task_id=""remove_state"",
        sql=f""DELETE FROM {TABLE} WHERE name='Bob'"",
    )

    d0 = EmptyOperator(
        task_id=""dummy0""
    )

    t6 = SQLExecuteQueryOperator(
        conn_id=CONN_ID,
        task_id=""add_state"",
        sql=f""INSERT INTO {TABLE} (state, temp, date) VALUES ('Abia', 25, '{DATES[5]}')"",
    )
    t7 = SQLThresholdCheckOperator(
        conn_id=CONN_ID,
        task_id=""check_threshold"",
        min_threshold=5,
        max_threshold=7,
        sql=f""SELECT COUNT(*) FROM {TABLE}"",
    )
    t8 = SQLIntervalCheckOperator(
        conn_id=CONN_ID,
        task_id=""interval_check"",
        table=TABLE,
        days_back=3,
        date_filter_column=""date"",
        metrics_thresholds={""temp"": 24},
    )

    t9 = SQLExecuteQueryOperator(
        conn_id=CONN_ID,
        task_id=""drop_table_last"",
        sql=f""DROP TABLE IF EXISTS {TABLE} CASCADE;"",
    )

    t10 = PythonOperator(
        task_id=""check_task_states"",
        python_callable=assert_the_task_states,
        op_kwargs={""task_ids_and_assertions"": {
            ""prepare_table"": TaskInstanceState.SUCCESS,
            ""sql_check"": TaskInstanceState.SUCCESS,
            ""sql_check_value"": TaskInstanceState.SUCCESS,
            ""branch_sql"": TaskInstanceState.SUCCESS,
            ""add_state"": TaskInstanceState.SUCCESS,
            ""check_threshold"": TaskInstanceState.SUCCESS,
            ""interval_check"": TaskInstanceState.SUCCESS,
            ""drop_table_last"": TaskInstanceState.SUCCESS,
            ""remove_state"": TaskInstanceState.SKIPPED,
            ""dummy0"": TaskInstanceState.SKIPPED,
            ""check_threshold"": TaskInstanceState.SUCCESS,

        }},
        trigger_rule=TriggerRule.ALL_DONE
    )

    t1 >> t2 >> t3 >> t4 >> [t6, t5]
    t6 >> t7 >> t8 >> t9 >> t10
    t5 >> d0

``` 

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",atul-astronomer,2025-02-04 08:42:53+00:00,['amoghrajesh'],2025-02-07 10:28:27+00:00,2025-02-07 10:28:26+00:00,https://github.com/apache/airflow/issues/46411,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:core-operators', 'Operators, Sensors and hooks within Core Airflow'), ('affected_version:3.0.0alpha', 'For all 3.0.0 alpha releases')]","[{'comment_id': 2642530863, 'issue_id': 2829495738, 'author': 'ashb', 'body': 'Duplicate of #45823', 'created_at': datetime.datetime(2025, 2, 7, 10, 28, 26, tzinfo=datetime.timezone.utc)}]","ashb on (2025-02-07 10:28:26 UTC): Duplicate of #45823

"
2829466151,issue,closed,completed,'RuntimeTaskInstance' object has no attribute 'next_method',"### Apache Airflow version

Tag: 3.0.0a1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

The DAG has failed on the retry because TI object does not have attribute next_method

**Stack trace:**

{""timestamp"":""2025-02-04T08:16:24.757980Z"",""level"":""info"",""event"":""TI:  id=UUID('0194d007-ee4d-783f-8e5e-21710b26ce29') task_id='retry' dag_id='triggerer_retry' run_id='manual__2025-02-04T08:16:08.249908+00:00' try_number=2 map_index=-1 hostname='f3432247b682' task=<Task(RetryOperator): retry> max_tries=1"",""chan"":""stdout"",""logger"":""task""} {""timestamp"":""2025-02-04T08:16:24.757883"",""level"":""error"",""event"":""Task failed with exception"",""logger"":""task"",""error_detail"":[{""exc_type"":""AttributeError"",""exc_value"":""'RuntimeTaskInstance' object has no attribute 'next_method'"",""syntax_error"":null,""is_cause"":false,""frames"":[{""filename"":""/opt/airflow/task_sdk/src/airflow/sdk/execution_time/task_runner.py"",""lineno"":545,""name"":""run""},{""filename"":""/opt/airflow/task_sdk/src/airflow/sdk/execution_time/task_runner.py"",""lineno"":645,""name"":""_execute_task""},{""filename"":""/opt/airflow/airflow/models/baseoperator.py"",""lineno"":173,""name"":""wrapper""},{""filename"":""/files/dags/retry.py"",""lineno"":16,""name"":""execute""},{""filename"":""/usr/local/lib/python3.9/site-packages/pydantic/main.py"",""lineno"":891,""name"":""__getattr__""}]}]} {""timestamp"":""2025-02-04T08:16:24.758229"",""level"":""debug"",""event"":""Sending request"",""json"":""{\""state\"":\""failed\"",\""end_date\"":\""2025-02-04T08:16:24.758128Z\"",\""type\"":\""TaskState\""}\n"",""logger"":""task""}

### What you think should happen instead?

DAG should succeed on the second try.

### How to reproduce

The following DAG can be used to reproduce.

```python
class RetryOperator(BaseOperator):
    def execute(self, context):
        ti = context[""ti""]
        has_next_method = bool(ti.next_method)
        try_number = ti.try_number
        self.log.info(
            f""In `execute`: has_next_method: {has_next_method}, try_number:{try_number}""
        )

        self.defer(
            trigger=SuccessTrigger(),
            method_name=""next"",
            kwargs={""execute_try_number"": try_number},
        )

    def next(self, context, execute_try_number, event=None):
        self.log.info(""In next!"")
        ti = context[""ti""]
        has_next_method = bool(ti.next_method)
        try_number = ti.try_number
        self.log.info(
            f""In `next`: has_next_method: {has_next_method}, try_number:{try_number}, excute_try_number: {execute_try_number}""
        )

        if try_number == 1:
            # Force a retry
            raise AirflowException(""Force a retry"")
        # Did we run `execute`?
        if execute_try_number != try_number:
            raise AirflowException(""`execute` wasn't run during retry!"")
        return None  # Success!


with DAG(
    ""triggerer_retry"", schedule=None, start_date=datetime(2021, 9, 13), tags=['core']
) as dag:
    RetryOperator(task_id=""retry"", retries=1, retry_delay=timedelta(seconds=15))
``` 

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",atul-astronomer,2025-02-04 08:27:37+00:00,['rawwar'],2025-02-07 10:27:46+00:00,2025-02-07 10:27:45+00:00,https://github.com/apache/airflow/issues/46410,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:task-sdk', None), ('affected_version:3.0.0alpha', 'For all 3.0.0 alpha releases')]","[{'comment_id': 2633207942, 'issue_id': 2829466151, 'author': 'atul-astronomer', 'body': 'Stack trace:\n`{""timestamp"":""2025-02-04T08:16:24.757980Z"",""level"":""info"",""event"":""TI:  id=UUID(\'0194d007-ee4d-783f-8e5e-21710b26ce29\') task_id=\'retry\' dag_id=\'triggerer_retry\' run_id=\'manual__2025-02-04T08:16:08.249908+00:00\' try_number=2 map_index=-1 hostname=\'f3432247b682\' task=<Task(RetryOperator): retry> max_tries=1"",""chan"":""stdout"",""logger"":""task""}\n{""timestamp"":""2025-02-04T08:16:24.757883"",""level"":""error"",""event"":""Task failed with exception"",""logger"":""task"",""error_detail"":[{""exc_type"":""AttributeError"",""exc_value"":""\'RuntimeTaskInstance\' object has no attribute \'next_method\'"",""syntax_error"":null,""is_cause"":false,""frames"":[{""filename"":""/opt/airflow/task_sdk/src/airflow/sdk/execution_time/task_runner.py"",""lineno"":545,""name"":""run""},{""filename"":""/opt/airflow/task_sdk/src/airflow/sdk/execution_time/task_runner.py"",""lineno"":645,""name"":""_execute_task""},{""filename"":""/opt/airflow/airflow/models/baseoperator.py"",""lineno"":173,""name"":""wrapper""},{""filename"":""/files/dags/retry.py"",""lineno"":16,""name"":""execute""},{""filename"":""/usr/local/lib/python3.9/site-packages/pydantic/main.py"",""lineno"":891,""name"":""__getattr__""}]}]}\n{""timestamp"":""2025-02-04T08:16:24.758229"",""level"":""debug"",""event"":""Sending request"",""json"":""{\\""state\\"":\\""failed\\"",\\""end_date\\"":\\""2025-02-04T08:16:24.758128Z\\"",\\""type\\"":\\""TaskState\\""}\\n"",""logger"":""task""}`', 'created_at': datetime.datetime(2025, 2, 4, 8, 30, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2642529413, 'issue_id': 2829466151, 'author': 'ashb', 'body': 'Duplicate of #45429', 'created_at': datetime.datetime(2025, 2, 7, 10, 27, 45, tzinfo=datetime.timezone.utc)}]","atul-astronomer (Issue Creator) on (2025-02-04 08:30:21 UTC): Stack trace:
`{""timestamp"":""2025-02-04T08:16:24.757980Z"",""level"":""info"",""event"":""TI:  id=UUID('0194d007-ee4d-783f-8e5e-21710b26ce29') task_id='retry' dag_id='triggerer_retry' run_id='manual__2025-02-04T08:16:08.249908+00:00' try_number=2 map_index=-1 hostname='f3432247b682' task=<Task(RetryOperator): retry> max_tries=1"",""chan"":""stdout"",""logger"":""task""}
{""timestamp"":""2025-02-04T08:16:24.757883"",""level"":""error"",""event"":""Task failed with exception"",""logger"":""task"",""error_detail"":[{""exc_type"":""AttributeError"",""exc_value"":""'RuntimeTaskInstance' object has no attribute 'next_method'"",""syntax_error"":null,""is_cause"":false,""frames"":[{""filename"":""/opt/airflow/task_sdk/src/airflow/sdk/execution_time/task_runner.py"",""lineno"":545,""name"":""run""},{""filename"":""/opt/airflow/task_sdk/src/airflow/sdk/execution_time/task_runner.py"",""lineno"":645,""name"":""_execute_task""},{""filename"":""/opt/airflow/airflow/models/baseoperator.py"",""lineno"":173,""name"":""wrapper""},{""filename"":""/files/dags/retry.py"",""lineno"":16,""name"":""execute""},{""filename"":""/usr/local/lib/python3.9/site-packages/pydantic/main.py"",""lineno"":891,""name"":""__getattr__""}]}]}
{""timestamp"":""2025-02-04T08:16:24.758229"",""level"":""debug"",""event"":""Sending request"",""json"":""{\""state\"":\""failed\"",\""end_date\"":\""2025-02-04T08:16:24.758128Z\"",\""type\"":\""TaskState\""}\n"",""logger"":""task""}`

ashb on (2025-02-07 10:27:45 UTC): Duplicate of #45429

"
2828956575,issue,open,,Go through DAG processor metrics and make bundle aware,"### Body

We need to go through all the different metrics and make sure 1) they all still make sense to emit and 2) they are bundle aware.

This does mean some breaking changes for deployment managers, but we should just document what they are in our newsfragment so deployment managers can adjust to the changes.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2025-02-04 02:20:38+00:00,[],2025-02-04 02:22:54+00:00,,https://github.com/apache/airflow/issues/46403,"[('kind:meta', 'High-level information important to the community'), ('area:metrics', ''), ('AIP-66: DAG Bundle/Manifest', '')]",[],
2828784397,issue,open,,Enhancement: Support Dynamic dag_id Resolution in TriggerDagRunOperator for Clickable Links,"### Body

Currently, when using TriggerDagRunOperator with a dynamically determined dag_id (e.g., from XCom or templated fields), the Airflow web UI cannot generate a proper link to the triggered DAG at parse time. This results in a broken or invalid link (e.g., {{ task_instance.xcom_pull(...) }}) or a “DAG not found” error, even though the triggered DAG actually runs successfully.

Use Case / Motivation

    We have pipelines that need to trigger different DAGs dynamically, depending on runtime conditions (e.g., data or metadata that arrives in real-time).
    While the operator itself correctly triggers the desired DAG, the UI link to the triggered DAG is not usable.
    Users rely on the UI link to quickly navigate to the triggered DAG’s run details, so a broken link complicates monitoring and debugging.

Proposed Solution

    Allow the Airflow UI to update or determine the dag_id link after the task is executed, possibly by reading from XCom or storing the resolved dag_id in the metadata DB.
    Provide a mechanism (e.g., an extra link or a new operator parameter) to supply the final dag_id to the UI once it’s known at runtime.
    Alternatively, enhance TriggerDagRunOperator to set the correct link in the UI if dag_id is determined during the operator’s execution.

```
def decide_which_dag(**kwargs):
    # Some dynamic logic here
    return ""some_dynamic_dag_id""

trigger_dynamic = TriggerDagRunOperator(
    task_id=""trigger_dynamic_dag"",
    trigger_dag_id=""{{ python_callable_returned_id }}"",  # or from XCom
    wait_for_completion=True,
    dag=dag,
)

```

in the above scenario, the DAG is triggered correctly, but the link displayed in the UI is not valid.
",pauloenrique071,2025-02-03 23:33:50+00:00,[],2025-02-05 17:15:52+00:00,,https://github.com/apache/airflow/issues/46402,"[('kind:feature', 'Feature Requests'), ('good first issue', ''), ('kind:meta', 'High-level information important to the community'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2632415515, 'issue_id': 2828784397, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 2, 3, 23, 33, 52, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-02-03 23:33:52 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2828684477,issue,open,,Add release date to doc pages?,"### What do you see as an issue?

It would be a convenience to have the release dates on the docs pages when a new version is cut.   Two examples: [Here](https://airflow.apache.org/docs/apache-airflow/2.5.3/index.html) you can see the docs for Airflow 2.5.3, but there is no indication that this is from October 2023.  You can get that from going to the GitHub repo and search the Releases page, but that can be cumbersome.    Similarly, the provider package versions like [this](https://airflow.apache.org/docs/apache-airflow-providers-airbyte/4.0.0/index.html) say that it is release 4.0.0 and that it requires at least Airflow 2.8.0, both of which can be tracked down via the Releases page, but that's not entirely convenient.



### Solving the problem

Though obviously not a major thing,  it seems like putting the release date on the page could come in handy in various situations.

### Anything else

I have absolutely no idea how this might be implemented.  It seems like it should be easy to automate somewhere in the doc template or something, but maybe it's more effort than it is worth.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",ferruzzi,2025-02-03 22:24:07+00:00,[],2025-02-04 11:23:33+00:00,,https://github.com/apache/airflow/issues/46401,"[('kind:documentation', ''), ('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2633599459, 'issue_id': 2828684477, 'author': 'potiuk', 'body': 'Absolutely!', 'created_at': datetime.datetime(2025, 2, 4, 11, 15, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2633619416, 'issue_id': 2828684477, 'author': 'potiuk', 'body': 'This is a bit connected with the way how we are going to release the docs via S3 archive etc (something that @gopidesupavan  has mainly figured out but he is now on holidays - see #45621 release).\n\nBut we could - rather easily have an automated script retrieving the information about release time from simplejson interface of PyPI and post-process the generated documentation to update the release times (also for all the versions of all providers released in the past)\n\n\nExample json API: https://pypi.org/pypi/apache-airflow-providers-amazon/json', 'created_at': datetime.datetime(2025, 2, 4, 11, 23, 31, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-02-04 11:15:28 UTC): Absolutely!

potiuk on (2025-02-04 11:23:31 UTC): This is a bit connected with the way how we are going to release the docs via S3 archive etc (something that @gopidesupavan  has mainly figured out but he is now on holidays - see #45621 release).

But we could - rather easily have an automated script retrieving the information about release time from simplejson interface of PyPI and post-process the generated documentation to update the release times (also for all the versions of all providers released in the past)


Example json API: https://pypi.org/pypi/apache-airflow-providers-amazon/json

"
2828202005,issue,closed,completed,All operators are not able to handle tuple collection in parameter 'op_args' ,"### Apache Airflow version

Tag: 3.0.0a1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

User is passing op_args value as tuple in the PythonOperator but getting error `""error_detail"":[{""exc_type"":""ValidationError"",""exc_value"":""6 validation errors for SetRenderedFields\nrendered_fields.op_args.list.0.list.0\n  input was not a valid JSON value [type=invalid-json-value, input_value=('group1.dummy0', 'group1.dummy1', 'branch one'),`

### What you think should happen instead?

User should be able to pass a tuple as a collection in op_args

### How to reproduce

Issue found on Airflow 3 tag '3.0.0a1'

1. Create a DAG which utilises the below snippet
`PythonOperator(
        task_id=""tuples"",
        python_callable=some_callable,
        op_args=[(""a"", ""b"", ""c"")]
)`
2. Run the DAG and notice the error

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",atul-astronomer,2025-02-03 18:15:04+00:00,['amoghrajesh'],2025-02-04 13:35:33+00:00,2025-02-04 13:35:33+00:00,https://github.com/apache/airflow/issues/46393,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:core-operators', 'Operators, Sensors and hooks within Core Airflow'), ('affected_version:3.0.0alpha', 'For all 3.0.0 alpha releases')]","[{'comment_id': 2631727317, 'issue_id': 2828202005, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 2, 3, 18, 15, 6, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-02-03 18:15:06 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2827823596,issue,open,,AIP-84 | Add multisort to dags list request,"In the UI, we default the dags list to show dags with the latest dag run first , which we pass as `-last_dag_run_start_date`. Problem is that the minus symbol seems to make the default sorts also reverse so any dags without a dag run will be in reverse alphabetical order which is counterintuitive for users.
If you reverse the dag run start date order, it will also change the alphabetical order.

<img width=""1711"" alt=""Image"" src=""https://github.com/user-attachments/assets/91c38cb5-bc15-4f75-9305-9e5cea966a0b"" />",bbovenzi,2025-02-03 15:22:37+00:00,['Vishnu-sai-teja'],2025-02-04 11:23:00+00:00,,https://github.com/apache/airflow/issues/46383,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2631346480, 'issue_id': 2827823596, 'author': 'pierrejeambrun', 'body': 'Nice. For now the `dag_id` (name) is always used as the secondary sort criteria to ensure stability in the result set returned. (In case the primary criteria has identical values).\n\nWhen we use a descending sort, the secondary filter (which is the primary key of the table, here the `dag_id` is also `.desc()`).\n\nIn the following code `column` is the primary sort criteria, the one specified by the user. `primary_key_column` is the secondary sort criteria always used and retrieved from the models.\n\nhttps://github.com/apache/airflow/blob/ee6bd7ee162ff295b86d86fdd1b356c51b9bba78/airflow/api_fastapi/common/parameters.py#L202-L205\n\nI think we need to support multisort to be able to handle this case. Users would be able to specify that they want to sort on the primary criteria in asc(), but on a secondary criteria in desc() order for intance. (Or any other arbitrary combination).', 'created_at': datetime.datetime(2025, 2, 3, 15, 34, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2631878040, 'issue_id': 2827823596, 'author': 'Vishnu-sai-teja', 'body': 'Hey can work on this.', 'created_at': datetime.datetime(2025, 2, 3, 19, 24, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2633617113, 'issue_id': 2827823596, 'author': 'pierrejeambrun', 'body': 'Great, assigned !', 'created_at': datetime.datetime(2025, 2, 4, 11, 22, 45, tzinfo=datetime.timezone.utc)}]","pierrejeambrun on (2025-02-03 15:34:28 UTC): Nice. For now the `dag_id` (name) is always used as the secondary sort criteria to ensure stability in the result set returned. (In case the primary criteria has identical values).

When we use a descending sort, the secondary filter (which is the primary key of the table, here the `dag_id` is also `.desc()`).

In the following code `column` is the primary sort criteria, the one specified by the user. `primary_key_column` is the secondary sort criteria always used and retrieved from the models.

https://github.com/apache/airflow/blob/ee6bd7ee162ff295b86d86fdd1b356c51b9bba78/airflow/api_fastapi/common/parameters.py#L202-L205

I think we need to support multisort to be able to handle this case. Users would be able to specify that they want to sort on the primary criteria in asc(), but on a secondary criteria in desc() order for intance. (Or any other arbitrary combination).

Vishnu-sai-teja (Assginee) on (2025-02-03 19:24:37 UTC): Hey can work on this.

pierrejeambrun on (2025-02-04 11:22:45 UTC): Great, assigned !

"
2827708433,issue,closed,completed,Airflow on k8s: DagBag import timeout,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I am encountering unexpected failures while executing tasks using the KubernetesPodOperator. Specifically, I have a DAG with a task that is mapped to spawn hundreds of KubernetesPodOperator tasks, and some of these tasks are failing spontaneously. I don't think it's a problem about the DAG complexity, parsing time or so. I have 500 tasks and 10% of them fail.

I would like to report this issue and seek assistance in resolving it.


```
[2025-02-03T13:32:19.910+0000] {settings.py:475} DEBUG - Setting up DB connection pool (PID 7)
[2025-02-03T13:32:19.984+0000] {settings.py:579} DEBUG - settings.prepare_engine_args(): Using pool settings. pool_size=5, max_overflow=10, pool_recycle=1800, pid=7
[2025-02-03T13:32:20.814+0000] {configuration.py:862} DEBUG - Could not retrieve value from section core, for key dataset_manager_kwargs. Skipping redaction of this conf.
[2025-02-03T13:32:20.815+0000] {configuration.py:862} DEBUG - Could not retrieve value from section smtp, for key smtp_password. Skipping redaction of this conf.
[2025-02-03T13:32:20.816+0000] {configuration.py:862} DEBUG - Could not retrieve value from section database, for key sql_alchemy_engine_args. Skipping redaction of this conf.
[2025-02-03T13:32:20.833+0000] {cli_action_loggers.py:51} DEBUG - Adding <function default_action_log at 0x7f81367aa7a0> to pre execution callback
/home/airflow/.local/lib/python3.11/site-packages/airflow/metrics/statsd_logger.py:184 RemovedInAirflow3Warning: The basic metric validator will be deprecated in the future in favor of pattern-matching.  You can try this now by setting config option metrics_use_pattern_match to True.
[2025-02-03T13:32:39.119+0000] {serde.py:375} DEBUG - registering decimal.Decimal for serialization
[2025-02-03T13:32:39.121+0000] {serde.py:382} DEBUG - registering decimal.Decimal for deserialization
[2025-02-03T13:32:39.122+0000] {serde.py:375} DEBUG - registering builtins.frozenset for serialization
[2025-02-03T13:32:39.123+0000] {serde.py:375} DEBUG - registering builtins.set for serialization
[2025-02-03T13:32:39.123+0000] {serde.py:375} DEBUG - registering builtins.tuple for serialization
[2025-02-03T13:32:39.124+0000] {serde.py:382} DEBUG - registering builtins.frozenset for deserialization
[2025-02-03T13:32:39.124+0000] {serde.py:382} DEBUG - registering builtins.set for deserialization
[2025-02-03T13:32:39.125+0000] {serde.py:382} DEBUG - registering builtins.tuple for deserialization
[2025-02-03T13:32:39.125+0000] {serde.py:390} DEBUG - registering builtins.frozenset for stringifying
[2025-02-03T13:32:39.126+0000] {serde.py:390} DEBUG - registering builtins.set for stringifying
[2025-02-03T13:32:39.126+0000] {serde.py:390} DEBUG - registering builtins.tuple for stringifying
[2025-02-03T13:32:39.128+0000] {serde.py:375} DEBUG - registering datetime.date for serialization
[2025-02-03T13:32:39.128+0000] {serde.py:375} DEBUG - registering datetime.datetime for serialization
[2025-02-03T13:32:39.128+0000] {serde.py:375} DEBUG - registering datetime.timedelta for serialization
[2025-02-03T13:32:39.129+0000] {serde.py:375} DEBUG - registering pendulum.datetime.DateTime for serialization
[2025-02-03T13:32:39.129+0000] {serde.py:382} DEBUG - registering datetime.date for deserialization
[2025-02-03T13:32:39.130+0000] {serde.py:382} DEBUG - registering datetime.datetime for deserialization
[2025-02-03T13:32:39.130+0000] {serde.py:382} DEBUG - registering datetime.timedelta for deserialization
[2025-02-03T13:32:39.130+0000] {serde.py:382} DEBUG - registering pendulum.datetime.DateTime for deserialization
[2025-02-03T13:32:39.131+0000] {serde.py:375} DEBUG - registering deltalake.table.DeltaTable for serialization
[2025-02-03T13:32:39.133+0000] {serde.py:382} DEBUG - registering deltalake.table.DeltaTable for deserialization
[2025-02-03T13:32:39.133+0000] {serde.py:390} DEBUG - registering deltalake.table.DeltaTable for stringifying
[2025-02-03T13:32:39.134+0000] {serde.py:375} DEBUG - registering pyiceberg.table.Table for serialization
[2025-02-03T13:32:39.134+0000] {serde.py:382} DEBUG - registering pyiceberg.table.Table for deserialization
[2025-02-03T13:32:39.135+0000] {serde.py:390} DEBUG - registering pyiceberg.table.Table for stringifying
[2025-02-03T13:32:39.136+0000] {serde.py:375} DEBUG - registering kubernetes.client.models.v1_resource_requirements.V1ResourceRequirements for serialization
[2025-02-03T13:32:39.136+0000] {serde.py:375} DEBUG - registering kubernetes.client.models.v1_pod.V1Pod for serialization
[2025-02-03T13:32:39.137+0000] {serde.py:375} DEBUG - registering numpy.int8 for serialization
[2025-02-03T13:32:39.137+0000] {serde.py:375} DEBUG - registering numpy.int16 for serialization
[2025-02-03T13:32:39.138+0000] {serde.py:375} DEBUG - registering numpy.int32 for serialization
[2025-02-03T13:32:39.138+0000] {serde.py:375} DEBUG - registering numpy.int64 for serialization
[2025-02-03T13:32:39.139+0000] {serde.py:375} DEBUG - registering numpy.uint8 for serialization
[2025-02-03T13:32:39.139+0000] {serde.py:375} DEBUG - registering numpy.uint16 for serialization
[2025-02-03T13:32:39.139+0000] {serde.py:375} DEBUG - registering numpy.uint32 for serialization
[2025-02-03T13:32:39.140+0000] {serde.py:375} DEBUG - registering numpy.uint64 for serialization
[2025-02-03T13:32:39.140+0000] {serde.py:375} DEBUG - registering numpy.bool_ for serialization
[2025-02-03T13:32:39.141+0000] {serde.py:375} DEBUG - registering numpy.float64 for serialization
[2025-02-03T13:32:39.141+0000] {serde.py:375} DEBUG - registering numpy.float16 for serialization
[2025-02-03T13:32:39.142+0000] {serde.py:375} DEBUG - registering numpy.complex128 for serialization
[2025-02-03T13:32:39.142+0000] {serde.py:375} DEBUG - registering numpy.complex64 for serialization
[2025-02-03T13:32:39.142+0000] {serde.py:382} DEBUG - registering numpy.int8 for deserialization
[2025-02-03T13:32:39.143+0000] {serde.py:382} DEBUG - registering numpy.int16 for deserialization
[2025-02-03T13:32:39.143+0000] {serde.py:382} DEBUG - registering numpy.int32 for deserialization
[2025-02-03T13:32:39.144+0000] {serde.py:382} DEBUG - registering numpy.int64 for deserialization
[2025-02-03T13:32:39.144+0000] {serde.py:382} DEBUG - registering numpy.uint8 for deserialization
[2025-02-03T13:32:39.145+0000] {serde.py:382} DEBUG - registering numpy.uint16 for deserialization
[2025-02-03T13:32:39.145+0000] {serde.py:382} DEBUG - registering numpy.uint32 for deserialization
[2025-02-03T13:32:39.145+0000] {serde.py:382} DEBUG - registering numpy.uint64 for deserialization
[2025-02-03T13:32:39.146+0000] {serde.py:382} DEBUG - registering numpy.bool_ for deserialization
[2025-02-03T13:32:39.146+0000] {serde.py:382} DEBUG - registering numpy.float64 for deserialization
[2025-02-03T13:32:39.147+0000] {serde.py:382} DEBUG - registering numpy.float16 for deserialization
[2025-02-03T13:32:39.147+0000] {serde.py:382} DEBUG - registering numpy.complex128 for deserialization
[2025-02-03T13:32:39.147+0000] {serde.py:382} DEBUG - registering numpy.complex64 for deserialization
[2025-02-03T13:32:39.148+0000] {serde.py:375} DEBUG - registering pandas.core.frame.DataFrame for serialization
[2025-02-03T13:32:39.149+0000] {serde.py:382} DEBUG - registering pandas.core.frame.DataFrame for deserialization
[2025-02-03T13:32:39.149+0000] {serde.py:375} DEBUG - registering pendulum.tz.timezone.FixedTimezone for serialization
[2025-02-03T13:32:39.153+0000] {serde.py:375} DEBUG - registering pendulum.tz.timezone.Timezone for serialization
[2025-02-03T13:32:39.154+0000] {serde.py:375} DEBUG - registering zoneinfo.ZoneInfo for serialization
[2025-02-03T13:32:39.155+0000] {serde.py:382} DEBUG - registering pendulum.tz.timezone.FixedTimezone for deserialization
[2025-02-03T13:32:39.156+0000] {serde.py:382} DEBUG - registering pendulum.tz.timezone.Timezone for deserialization
[2025-02-03T13:32:39.156+0000] {serde.py:382} DEBUG - registering zoneinfo.ZoneInfo for deserialization
[2025-02-03T13:32:39.157+0000] {serde.py:393} DEBUG - loading serializers took 0.039 seconds
[2025-02-03T13:32:42.366+0000] {cli_action_loggers.py:79} DEBUG - Calling callbacks: [<function default_action_log at 0x7f81367aa7a0>]
[2025-02-03T13:32:42.829+0000] {plugins_manager.py:357} DEBUG - Loading plugins
[2025-02-03T13:32:42.829+0000] {plugins_manager.py:273} DEBUG - Loading plugins from directory: /opt/airflow/plugins
[2025-02-03T13:32:42.830+0000] {plugins_manager.py:253} DEBUG - Loading plugins from entrypoints
[2025-02-03T13:32:42.911+0000] {plugins_manager.py:256} DEBUG - Importing entry_point plugin openlineage
[2025-02-03T13:32:50.467+0000] {plugins_manager.py:375} DEBUG - Loading 1 plugin(s) took 7.64 seconds
[2025-02-03T13:32:50.468+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/repo/dags/csr-vss-XXX/XXX-airflow/dags/scan_XXX.py
[2025-02-03T13:32:50.470+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/repo/dags/csr-vss-XXX/XXX-airflow/dags/scan_XXX.py
[2025-02-03T13:33:26.444+0000] {timeout.py:68} ERROR - Process timed out, PID: 7
[2025-02-03T13:33:26.445+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/repo/dags/csr-vss-XXX/XXX-airflow/dags/scan_XXX.py
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py"", line 383, in parse
    loader.exec_module(new_module)
  File ""<frozen importlib._bootstrap_external>"", line 940, in exec_module
  File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed
  File ""/home/airflow/.local/lib/python3.11/site-packages/sds_provider/operators/kubernetes/pod.py"", line 1, in <module>
    from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py"", line 62, in <module>
    from airflow.providers.cncf.kubernetes.callbacks import ExecutionMode, KubernetesPodOperatorCallback
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/callbacks.py"", line 23, in <module>
    import kubernetes_asyncio.client as async_k8s
  File ""/home/airflow/.local/lib/python3.11/site-packages/kubernetes_asyncio/__init__.py"", line 19, in <module>
    import kubernetes_asyncio.client
  File ""/home/airflow/.local/lib/python3.11/site-packages/kubernetes_asyncio/client/__init__.py"", line 20, in <module>
    from kubernetes_asyncio.client.api.well_known_api import WellKnownApi
  File ""/home/airflow/.local/lib/python3.11/site-packages/kubernetes_asyncio/client/api/__init__.py"", line 6, in <module>
    from kubernetes_asyncio.client.api.well_known_api import WellKnownApi
  File ""/home/airflow/.local/lib/python3.11/site-packages/kubernetes_asyncio/client/api/well_known_api.py"", line 20, in <module>
    from kubernetes_asyncio.client.api_client import ApiClient
  File ""/home/airflow/.local/lib/python3.11/site-packages/kubernetes_asyncio/client/api_client.py"", line 28, in <module>
    import kubernetes_asyncio.client.models
  File ""/home/airflow/.local/lib/python3.11/site-packages/kubernetes_asyncio/client/models/__init__.py"", line 577, in <module>
    from kubernetes_asyncio.client.models.v1beta1_validating_admission_policy_list import V1beta1ValidatingAdmissionPolicyList
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/timeout.py"", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/repo/dags/csr-vss-XXX/XXX-airflow/dags/scan_XXX.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.10.4/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.10.4/best-practices.html#reducing-dag-complexity, PID: 7
[2025-02-03T13:33:26.939+0000] {cli.py:251} WARNING - Dag 'XXX_scan_XXX' not found in path /opt/airflow/dags/repo/dags/csr-vss-XXX/XXX-airflow/dags/scan_XXX.py; trying path /opt/airflow/dags/repo/dags/csr-vss-XXX/XXX-airflow/dags/scan_XXX.py
[2025-02-03T13:33:26.952+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/repo/dags/csr-vss-XXX/XXX-airflow/dags/scan_XXX.py
[2025-02-03T13:33:26.953+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/repo/dags/csr-vss-XXX/XXX-airflow/dags/scan_XXX.py
[2025-02-03T13:33:57.450+0000] {timeout.py:68} ERROR - Process timed out, PID: 7
[2025-02-03T13:33:57.451+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/repo/dags/csr-vss-XXX/XXX-airflow/dags/scan_XXX.py
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py"", line 383, in parse
    loader.exec_module(new_module)
  File ""<frozen importlib._bootstrap_external>"", line 940, in exec_module
  File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed
  File ""/home/airflow/.local/lib/python3.11/site-packages/sds_provider/operators/kubernetes/pod.py"", line 1, in <module>
    from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py"", line 62, in <module>
    from airflow.providers.cncf.kubernetes.callbacks import ExecutionMode, KubernetesPodOperatorCallback
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/callbacks.py"", line 23, in <module>
    import kubernetes_asyncio.client as async_k8s
  File ""/home/airflow/.local/lib/python3.11/site-packages/kubernetes_asyncio/__init__.py"", line 19, in <module>
    import kubernetes_asyncio.client
  File ""/home/airflow/.local/lib/python3.11/site-packages/kubernetes_asyncio/client/__init__.py"", line 20, in <module>
    from kubernetes_asyncio.client.api.well_known_api import WellKnownApi
  File ""/home/airflow/.local/lib/python3.11/site-packages/kubernetes_asyncio/client/api/__init__.py"", line 31, in <module>
    from kubernetes_asyncio.client.api.certificates_v1alpha1_api import CertificatesV1alpha1Api
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/timeout.py"", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/repo/dags/csr-vss-XXX/XXX-airflow/dags/scan_XXX.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.10.4/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.10.4/best-practices.html#reducing-dag-complexity, PID: 7
[2025-02-03T13:34:15.728+0000] {dagbag.py:369} DEBUG - Importing /opt/airflow/dags/repo/dags/csr-vss-XXX/XXX-airflow/dags/scan_XXX.py
[2025-02-03T13:34:47.089+0000] {timeout.py:68} ERROR - Process timed out, PID: 7
[2025-02-03T13:34:47.089+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/repo/dags/csr-vss-XXX/XXX-airflow/dags/scan_XXX.py
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py"", line 383, in parse
    loader.exec_module(new_module)
  File ""<frozen importlib._bootstrap_external>"", line 940, in exec_module
  File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed
  File ""/home/airflow/.local/lib/python3.11/site-packages/sds_provider/operators/kubernetes/pod.py"", line 1, in <module>
    from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py"", line 62, in <module>
    from airflow.providers.cncf.kubernetes.callbacks import ExecutionMode, KubernetesPodOperatorCallback
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/callbacks.py"", line 23, in <module>
    import kubernetes_asyncio.client as async_k8s
  File ""/home/airflow/.local/lib/python3.11/site-packages/kubernetes_asyncio/__init__.py"", line 19, in <module>
    import kubernetes_asyncio.client
  File ""/home/airflow/.local/lib/python3.11/site-packages/kubernetes_asyncio/client/__init__.py"", line 45, in <module>
    from kubernetes_asyncio.client.api.certificates_v1alpha1_api import CertificatesV1alpha1Api
  File ""/home/airflow/.local/lib/python3.11/site-packages/kubernetes_asyncio/client/api/__init__.py"", line 49, in <module>
    from kubernetes_asyncio.client.api.networking_v1alpha1_api import NetworkingV1alpha1Api
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/timeout.py"", line 69, in handle_timeout
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/repo/dags/csr-vss-XXX/XXX-airflow/dags/scan_XXX.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.10.4/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.10.4/best-practices.html#reducing-dag-complexity, PID: 7
[2025-02-03T13:34:47.282+0000] {cli_action_loggers.py:98} DEBUG - Calling callbacks: []
Traceback (most recent call last):
  File ""/home/airflow/.local/bin/airflow"", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/__main__.py"", line 62, in main
    args.func(args)
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/cli_config.py"", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/cli.py"", line 116, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/commands/task_command.py"", line 458, in task_run
    _dag = get_dag(args.subdir, args.dag_id, args.read_from_db)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/cli.py"", line 255, in get_dag
    raise AirflowException(
airflow.exceptions.AirflowException: Dag 'XXX_scan_XXX' could not be found; either it does not exist or it failed to parse.
[2025-02-03T13:34:47.588+0000] {settings.py:612} DEBUG - Disposing DB connection pool (PID 7)
```

### What you think should happen instead?

_No response_

### How to reproduce

Trigger a DAG with a mapped task of KubernetesPodOperator.

### Operating System

Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==9.1.0
apache-airflow-providers-celery==3.8.5
apache-airflow-providers-cncf-kubernetes==10.0.1
apache-airflow-providers-common-compat==1.2.2
apache-airflow-providers-common-io==1.4.2
apache-airflow-providers-common-sql==1.20.0
apache-airflow-providers-docker==3.14.1
apache-airflow-providers-elasticsearch==5.5.3
apache-airflow-providers-fab==1.5.1
apache-airflow-providers-ftp==3.11.1
apache-airflow-providers-google==11.0.0
apache-airflow-providers-grpc==3.6.0
apache-airflow-providers-hashicorp==3.8.0
apache-airflow-providers-http==4.13.3
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-microsoft-azure==11.1.0
apache-airflow-providers-mysql==5.7.4
apache-airflow-providers-odbc==4.8.1
apache-airflow-providers-openlineage==1.14.0
apache-airflow-providers-postgres==5.14.0
apache-airflow-providers-redis==3.8.0
apache-airflow-providers-sendgrid==3.6.0
apache-airflow-providers-sftp==4.11.1
apache-airflow-providers-slack==8.9.2
apache-airflow-providers-smtp==1.8.1
apache-airflow-providers-snowflake==5.8.1
apache-airflow-providers-sqlite==3.9.1
apache-airflow-providers-ssh==3.14.0

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",ruztomas,2025-02-03 14:38:58+00:00,[],2025-02-05 18:53:49+00:00,2025-02-05 18:53:49+00:00,https://github.com/apache/airflow/issues/46378,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2631188221, 'issue_id': 2827708433, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 2, 3, 14, 39, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2633880262, 'issue_id': 2827708433, 'author': 'Lee2532', 'body': 'Increase dagbag_import_timeout\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#dagbag-import-timeout', 'created_at': datetime.datetime(2025, 2, 4, 13, 17, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2633913937, 'issue_id': 2827708433, 'author': 'ruztomas', 'body': 'I just raised it to 100.0s and it seems to work fine. The fact that concerns me is that it takes a few seconds when it went fine. If all go ok today, i will close this issue. Thanks!', 'created_at': datetime.datetime(2025, 2, 4, 13, 25, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2633957816, 'issue_id': 2827708433, 'author': 'Lee2532', 'body': 'i think scheduler issue\n\nhttps://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#parsing-processes', 'created_at': datetime.datetime(2025, 2, 4, 13, 33, 49, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-02-03 14:39:02 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

Lee2532 on (2025-02-04 13:17:21 UTC): Increase dagbag_import_timeout

https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#dagbag-import-timeout

ruztomas (Issue Creator) on (2025-02-04 13:25:03 UTC): I just raised it to 100.0s and it seems to work fine. The fact that concerns me is that it takes a few seconds when it went fine. If all go ok today, i will close this issue. Thanks!

Lee2532 on (2025-02-04 13:33:49 UTC): i think scheduler issue

https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#parsing-processes

"
2827382261,issue,closed,completed,Changing apply_function in AwaitMessageSensor or AwaitMessageTrigger requires trigger restart,"### Apache Airflow Provider(s)

apache-kafka

### Versions of Apache Airflow Providers

apache-airflow-providers-apache-kafka==1.7.0

### Apache Airflow version

2.10.4

### Operating System

Debian GNU/Linux 12 (bookworm)

### Deployment

Docker-Compose

### Deployment details

Used ""Docker version 27.4.0, build bde2b89"", with official docker-compose from [airflow-website](https://airflow.apache.org/docs/apache-airflow/2.10.4/docker-compose.yaml) as basis. Some changes to install ""apache-airflow[apache-kafka]"" and include Kafka containers into compose-file.

### What happened

I am designing a workflow sending and receiving Kafka messages. For reception I am using AwaitMessageSensor where you have to pass an apply_function in dot-sting-notation as used by importlib. Directly after startup of the airflow system, everything works fine, but when the code of apply_function is changed, these changes are not reflected by the trigger actually being executed. The problem gets highlighted, if the function is renamed. The trigger crashes with an ImprtError ""Module ""myDAG"" does not define a ""my_apply_function"" attribute/class"", although my_apply_function is defined in the currently used DAG-file. I verified that the most recent version of the file is correctly synced to the triggerer-container (mounted into the container as per default) and it can also be found in the database.

### What you think should happen instead

Changing the apply function should be correctly reflected by the triggerer.

### How to reproduce

Take a working Kafka-Airflow-Setup, use an AwaitMessageSensor, rename its apply_function. (I think this problem is quite general, so I did not provide code, if someone really requires my docker-compose and a minimal DAG, I could provide it later).

### Anything else

I researched quite a while, to come to anything near a solution. The TriggererRunner uses a cache to get triggerer classes by their function name (see [source code](https://github.com/apache/airflow/blob/8a3757b99e247d2f09a86d12e6aff8d525a3f3e7/airflow/jobs/triggerer_job_runner.py#L759)). I read #43253 which has been closed saying that this would be a CloudComposer issue (which I am not using). Also issue #31743 has been closed after [updates](https://github.com/apache/airflow/issues/31743#issuecomment-1598786020) to the Documentation of deferrable operators. I also digged into the source code of AwaitMessageSensor and AwaitMessageTrigger where the string passed via apply_function is imported by importlib. So I expected my new code to be imported each time, I used the sensor.

My understanding of this problem is far from complete, but it seems to me that my previous expectation is not met, because the TriggererRunner caches the triggerers and therefore the import of my apply_function by importlib is only performed once. If that is not true, please correct me.

I propose that at least the documentation should be updated, explicitly pointing out that changing apply_function requires a restart of the triggerer. The aforementioned changes there where only in the section about writing your own triggerers, which I did not take into account, since I am not writing my own triggerer, but only using one from an offical provider. The way it is working now is very unexpected and cost me a lot of time figuring out. But from a user perspective it would be much more convenient, if the apply_function could be updated whenever code is updated across all containers, e.g. by sidestepping the caching either with the whole AwaitMessageTrigger or at least the apply_function.

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",StrawberryOwl,2025-02-03 12:32:16+00:00,[],2025-02-05 16:27:58+00:00,2025-02-05 16:27:58+00:00,https://github.com/apache/airflow/issues/46376,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:apache-kafka', '')]","[{'comment_id': 2630820068, 'issue_id': 2827382261, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 2, 3, 12, 32, 19, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-02-03 12:32:19 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2827246430,issue,closed,completed,"Task k8s pod spec is rendered using the default `pod_template_file`, even when an override was passed to the `KubernetesExecutor`","### Apache Airflow Provider(s)

cncf-kubernetes

### Versions of Apache Airflow Providers
```
apache-airflow-providers-amazon==9.2.0
apache-airflow-providers-apache-hdfs==4.7.0
apache-airflow-providers-apache-hive==9.0.0
apache-airflow-providers-apache-spark==4.8.1
apache-airflow-providers-cncf-kubernetes==10.1.0
apache-airflow-providers-common-compat==1.3.0
apache-airflow-providers-common-io==1.5.0
apache-airflow-providers-common-sql==1.21.0
apache-airflow-providers-fab==1.5.2
apache-airflow-providers-ftp==3.12.0
apache-airflow-providers-http==5.0.0
apache-airflow-providers-imap==3.8.0
apache-airflow-providers-postgres==6.0.0
apache-airflow-providers-smtp==1.9.0
apache-airflow-providers-sqlite==4.0.0
```
### Apache Airflow version

2.10.3

### Operating System

Debian 11 Bullseye

### Deployment

Other 3rd-party Helm chart

### Deployment details

Our airflow instance is running in Kubernetes, and uses the `KubernetesExecutor` to run tasks as Kubernetes pods. It uses the `inCluster` config setup, to get permissions from its serviceaccount token. 

### What happened

We deploy task as Kubernetes pods, using a `pod_template_file` configured in `airflow.cfg`. However, some tasks make use the of the `KubernetesPodOperator` to themselves create a pod to run an ad-hoc command (that might or might not be python code). We have defined a second pod template file that contains extra configuration. 

```diff
~ ❯ diff -u default_pod_template.yaml kubernetes_executor_pod_template_kubeapi_enabled.yaml
--- default_pod_template.yaml	2025-02-03 12:02:46
+++ kubernetes_executor_pod_template_kubeapi_enabled.yaml	2025-02-03 12:03:47
@@ -7,6 +7,7 @@
     release: production
     routed_via: production
     component: task-pod
+    kubeapi_enabled: 'True'
 spec:
   restartPolicy: Never
   hostAliases:
@@ -16,6 +17,7 @@
   - ip: 10.64.36.112
     hostnames:
     - an-test-master1002.eqiad.wmnet
+  serviceAccountName: airflow

   volumes:
   - configMap:
```

The label is used to allow egress to the Kubernetes API server, via a Calico Networkpolicy, and the `serviceAccount` is used to make sure that the task pod has the required RBAC to create and delete the pods via the `KubernetesPodOperator`.

We pass that non-default `pod_template_file` to the executor via `executor_options['pod_template_file']` ([link](https://gitlab.wikimedia.org/repos/data-engineering/airflow-dags/-/blob/96a54a46653073ddc1b5c6d95ce3eb01146d5c47/wmf_airflow_common/operators/kubernetes.py#L16))

I realized that while the label and serviceAccount appeared in the output of `kubectl get pod <pod-name> -oyaml`, they did _not_ appear in the `K8s Pod Spec` pane in the Airflow UI. See this example:

<img width=""868"" alt=""Image"" src=""https://github.com/user-attachments/assets/9c9a4b20-b68f-4fd1-a187-8043847bb405"" />

_The `kubeapi_enabled` label is missing_

I had a look at the rendered pod spec in the database (in the `rendered_task_instance_fields` table) and both the label and the service account were missing from there as well, which indicates that the code responsible for inserting that spec in the database in the first place is at fault. 



### What you think should happen instead

I tracked down the code responsible for the rendering of the task instance k8s pod spec to [`render_k8s_pod_yaml`](https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/cncf/kubernetes/template_rendering.py#L35), which includes the following line

```python
base_worker_pod=PodGenerator.deserialize_model_file(kube_config.pod_template_file),
```
itself defined [here](https://github.com/apache/airflow/blob/f871e015ce97423838fc17faca68ff1fc1fbed17/providers/src/airflow/providers/cncf/kubernetes/kube_config.py#L37) as 
```python
self.pod_template_file = conf.get(self.kubernetes_section, ""pod_template_file"", fallback=None)
```

Nowhere in that function are we looking at a potential `pod_template_file` override passed via the `executor_options`.

### How to reproduce

To reproduce this issue, you need to run airflow with `KubernetesExecutor`, set `kubernetes_executor.pod_template_file` as the path of an existing yaml pod template, and also provide another pod template (referenced later as `/path/to/custom_pod_template_file.yaml`). This custom template should differ from the default one in some way (added or removed fields, for example).

Then run the following DAG, and inspect its pod spec:

```python
from datetime import datetime

from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator
from airflow import DAG

with DAG(
    ...
) as dag:
    run_k8s_pod = KubernetesPodOperator(
        task_id=""run-cat-os-release"",
        name=""run-cat-os-release"",
        # the container can run anything, not just python code
        image=""debian:bookworm"",
        cmds=[""/usr/bin/cat""],
        arguments=[""/etc/os-release""],
        executor_config={""pod_template_file"": ""/path/to/custom_pod_template_file.yaml""}
    )

    run_k8s_pod
```

### Anything else

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",brouberol,2025-02-03 11:30:22+00:00,[],2025-02-09 06:18:59+00:00,2025-02-09 06:18:59+00:00,https://github.com/apache/airflow/issues/46373,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2827184875,issue,closed,completed,Spark kuberntes operator Issue,"### Apache Airflow Provider(s)

cncf-kubernetes

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes==10.0.1

### Apache Airflow version

2.10.4

### Operating System

os from the docker image 2.10.4-python3.9

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

airflow chart version is airflow-8.9.0

### What happened

Hi Team,
I am trying to run SparkKubernetesOperator based airflow dag.
This is my spark deployment file which is running fine on spark kubernetes operator.
```
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: spark-pi
  namespace: data-platform
spec:
  type: Scala
  mode: cluster
  image: spark:3.5.2
  imagePullPolicy: IfNotPresent
  mainClass: ""org.apache.spark.examples.SparkPi""
  mainApplicationFile: ""local:///opt/spark/examples/jars/spark-examples_2.12-3.5.2.jar""
  arguments:
    - ""5000""
  sparkVersion: 3.5.2
  driver:
    labels:
      version: 3.5.2
    cores: 1
    memory: 512m
    serviceAccount: spark-operator-spark
  executor:
    labels:
      version: 3.5.2
    instances: 1
    cores: 1
    memory: 512m
```
**airflow dag file,**
```
from datetime import timedelta
import os
from airflow import DAG
from airflow.providers.cncf.kubernetes.operators.spark_kubernetes import SparkKubernetesOperator
from airflow.utils.dates import days_ago
from airflow.models import Variable


default_args = {
    'owner': 'Mahesh',
    'retries': 3,
    'retry_delay': timedelta(minutes=5)
}


dag = DAG(
    'spark_on_k8s_airflow',
    default_args=default_args,  # Add default_args to the DAG
    start_date=days_ago(1),
    catchup=False,
    schedule_interval=timedelta(days=1)
)

spark_k8s_task = SparkKubernetesOperator(
    task_id='spark-on-k8s-airflow',
    trigger_rule=""all_success"",
    depends_on_past=False,
    retries=0,
    application_file='spark_jobs/pi.yaml',
    namespace=""data-platform"",
    kubernetes_conn_id=""dev-dp"",
    dag=dag
)

spark_k8s_task
```
But it fails with below yaml scanner error. But there is no syntax issue with my yaml file.
```
[2025-02-03, 06:21:10 UTC] {taskinstance.py:3311} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py"", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py"", line 733, in _execute_callable
    return ExecutionCallableRunner(
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py"", line 252, in run
    return self.func(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/models/baseoperator.py"", line 422, in wrapper
    return func(self, *args, **kwargs)
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py"", line 298, in execute
    kube_client=self.client,
  File ""/usr/local/lib/python3.9/functools.py"", line 993, in __get__
    val = self.func(instance)
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py"", line 285, in client
    return self.hook.core_v1_client
  File ""/usr/local/lib/python3.9/functools.py"", line 993, in __get__
    val = self.func(instance)
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/cncf/kubernetes/hooks/kubernetes.py"", line 302, in core_v1_client
    return client.CoreV1Api(api_client=self.api_client)
  File ""/usr/local/lib/python3.9/functools.py"", line 993, in __get__
    val = self.func(instance)
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/cncf/kubernetes/hooks/kubernetes.py"", line 298, in api_client
    return self.get_conn()
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/cncf/kubernetes/hooks/kubernetes.py"", line 259, in get_conn
    config.load_kube_config(
  File ""/home/airflow/.local/lib/python3.9/site-packages/kubernetes/config/kube_config.py"", line 819, in load_kube_config
    loader = _get_kube_config_loader(
  File ""/home/airflow/.local/lib/python3.9/site-packages/kubernetes/config/kube_config.py"", line 771, in _get_kube_config_loader
    kcfg = KubeConfigMerger(filename)
  File ""/home/airflow/.local/lib/python3.9/site-packages/kubernetes/config/kube_config.py"", line 686, in __init__
    self._load_config_from_file_path(paths)
  File ""/home/airflow/.local/lib/python3.9/site-packages/kubernetes/config/kube_config.py"", line 711, in _load_config_from_file_path
    self.load_config(path)
  File ""/home/airflow/.local/lib/python3.9/site-packages/kubernetes/config/kube_config.py"", line 716, in load_config
    config = yaml.safe_load(f)
  File ""/home/airflow/.local/lib/python3.9/site-packages/yaml/__init__.py"", line 125, in safe_load
    return load(stream, SafeLoader)
  File ""/home/airflow/.local/lib/python3.9/site-packages/yaml/__init__.py"", line 81, in load
    return loader.get_single_data()
  File ""/home/airflow/.local/lib/python3.9/site-packages/yaml/constructor.py"", line 49, in get_single_data
    node = self.get_single_node()
  File ""/home/airflow/.local/lib/python3.9/site-packages/yaml/composer.py"", line 36, in get_single_node
    document = self.compose_document()
  File ""/home/airflow/.local/lib/python3.9/site-packages/yaml/composer.py"", line 55, in compose_document
    node = self.compose_node(None, None)
  File ""/home/airflow/.local/lib/python3.9/site-packages/yaml/composer.py"", line 84, in compose_node
    node = self.compose_mapping_node(anchor)
  File ""/home/airflow/.local/lib/python3.9/site-packages/yaml/composer.py"", line 127, in compose_mapping_node
    while not self.check_event(MappingEndEvent):
  File ""/home/airflow/.local/lib/python3.9/site-packages/yaml/parser.py"", line 98, in check_event
    self.current_event = self.state()
  File ""/home/airflow/.local/lib/python3.9/site-packages/yaml/parser.py"", line 428, in parse_block_mapping_key
    if self.check_token(KeyToken):
  File ""/home/airflow/.local/lib/python3.9/site-packages/yaml/scanner.py"", line 116, in check_token
    self.fetch_more_tokens()
  File ""/home/airflow/.local/lib/python3.9/site-packages/yaml/scanner.py"", line 223, in fetch_more_tokens
    return self.fetch_value()
  File ""/home/airflow/.local/lib/python3.9/site-packages/yaml/scanner.py"", line 577, in fetch_value
    raise ScannerError(None, None,
yaml.scanner.ScannerError: mapping values are not allowed here
  in ""/tmp/tmpepozi80i"", line 1, column 24
```

### What you think should happen instead

_No response_

### How to reproduce

This is my spark deployment file which is running fine on spark kubernetes operator.
```
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: spark-pi
  namespace: data-platform
spec:
  type: Scala
  mode: cluster
  image: spark:3.5.2
  imagePullPolicy: IfNotPresent
  mainClass: ""org.apache.spark.examples.SparkPi""
  mainApplicationFile: ""local:///opt/spark/examples/jars/spark-examples_2.12-3.5.2.jar""
  arguments:
    - ""5000""
  sparkVersion: 3.5.2
  driver:
    labels:
      version: 3.5.2
    cores: 1
    memory: 512m
    serviceAccount: spark-operator-spark
  executor:
    labels:
      version: 3.5.2
    instances: 1
    cores: 1
    memory: 512m
```
**airflow dag file,**
```
from datetime import timedelta
import os
from airflow import DAG
from airflow.providers.cncf.kubernetes.operators.spark_kubernetes import SparkKubernetesOperator
from airflow.utils.dates import days_ago
from airflow.models import Variable


default_args = {
    'owner': 'Mahesh',
    'retries': 3,
    'retry_delay': timedelta(minutes=5)
}


dag = DAG(
    'spark_on_k8s_airflow',
    default_args=default_args,  # Add default_args to the DAG
    start_date=days_ago(1),
    catchup=False,
    schedule_interval=timedelta(days=1)
)

spark_k8s_task = SparkKubernetesOperator(
    task_id='spark-on-k8s-airflow',
    trigger_rule=""all_success"",
    depends_on_past=False,
    retries=0,
    application_file='spark_jobs/pi.yaml',
    namespace=""data-platform"",
    kubernetes_conn_id=""dev-dp"",
    dag=dag
)

spark_k8s_task
```

### Anything else

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",MaheshSankaran,2025-02-03 11:03:17+00:00,[],2025-02-04 07:39:27+00:00,2025-02-04 07:39:16+00:00,https://github.com/apache/airflow/issues/46370,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2630625678, 'issue_id': 2827184875, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 2, 3, 11, 3, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2633112300, 'issue_id': 2827184875, 'author': 'MaheshSankaran', 'body': 'problem with kubeconfig connection with airflow, we can close this.', 'created_at': datetime.datetime(2025, 2, 4, 7, 39, 16, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-02-03 11:03:20 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

MaheshSankaran (Issue Creator) on (2025-02-04 07:39:16 UTC): problem with kubeconfig connection with airflow, we can close this.

"
2826565512,issue,open,,Task SDK should support extra links for operators,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Currently, 
Task SDK does not support extra links yet, we need this in order to have extra_links working with mapped operator.

This was discovered while https://github.com/apache/airflow/pull/46107


### What you think should happen instead?

_No response_

### How to reproduce

Extra links for mapped operator won't work.

### Operating System

macOS-14.7.1

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",shubhamraj-git,2025-02-03 06:49:17+00:00,['amoghrajesh'],2025-02-04 10:57:36+00:00,,https://github.com/apache/airflow/issues/46363,"[('kind:bug', 'This is a clearly a bug'), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK'), ('affected_version:3.0.0alpha', 'For all 3.0.0 alpha releases')]","[{'comment_id': 2632145814, 'issue_id': 2826565512, 'author': 'ashb', 'body': ""It's not just mapped operators -- it's all operators :) This is a known issue in our backlog https://github.com/orgs/apache/projects/405/views/1?filterQuery=link&pane=issue&itemId=95531657 -- since that is only a draft issue/note I'll put this in the project in it's place."", 'created_at': datetime.datetime(2025, 2, 3, 21, 45, 50, tzinfo=datetime.timezone.utc)}]","ashb on (2025-02-03 21:45:50 UTC): It's not just mapped operators -- it's all operators :) This is a known issue in our backlog https://github.com/orgs/apache/projects/405/views/1?filterQuery=link&pane=issue&itemId=95531657 -- since that is only a draft issue/note I'll put this in the project in it's place.

"
2826543926,issue,open,,Make `dag_version_id` a non-nullable field in DR/TI,"### Body

This ensures there's no DagRun or TI without a DagVersion, and we can go from dag_version to the bundle version, such as dr.dag_version.bundle_name, etc.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",ephraimbuddy,2025-02-03 06:35:20+00:00,['ephraimbuddy'],2025-02-03 19:06:43+00:00,,https://github.com/apache/airflow/issues/46362,"[('kind:meta', 'High-level information important to the community'), ('area:core', ''), ('AIP-66: DAG Bundle/Manifest', '')]",[],
2826512447,issue,open,,Re-add feature to use `use_airflow_context` in venv operators,"### Body

The PR https://github.com/apache/airflow/pull/46306 reverts the feature https://github.com/apache/airflow/pull/41363. 
Reasoning:
https://github.com/apache/airflow/pull/46306#issuecomment-2626847574


We should consider re-adding it at a later point.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",amoghrajesh,2025-02-03 06:13:21+00:00,[],2025-02-03 06:15:39+00:00,,https://github.com/apache/airflow/issues/46361,"[('kind:feature', 'Feature Requests'), ('kind:meta', 'High-level information important to the community'), ('area:core-operators', 'Operators, Sensors and hooks within Core Airflow')]","[{'comment_id': 2630061976, 'issue_id': 2826512447, 'author': 'amoghrajesh', 'body': 'cc @phi-friday', 'created_at': datetime.datetime(2025, 2, 3, 6, 13, 30, tzinfo=datetime.timezone.utc)}]","amoghrajesh (Issue Creator) on (2025-02-03 06:13:30 UTC): cc @phi-friday

"
2825956160,issue,closed,completed,TypeError when importing operators from airflow.providers.google.cloud.operators.dataproc,"### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

12.0.0

### Apache Airflow version

2.10.4

### Operating System

Ubuntu 24.04.1 LTS

### Deployment

Virtualenv installation

### Deployment details

_No response_

### What happened

When importing any of the existing operators in airflow.providers.google.cloud.operators.dataproc (for example DataprocCreateClusterOperator, DataprocDeleteClusterOperator or DataprocSubmitJobOperator) at the beginning of a dag, a dag import error occurs when starting the airflow scheduler.

### What you think should happen instead

When I start the airflow scheduler the following error occurs:

[2025-02-02T19:12:46.029+0100] {logging_mixin.py:190} INFO - [2025-02-02T19:12:46.028+0100] {dagbag.py:387} ERROR - Failed to import: /home/iscipar/Projects/airflow-tutorial/dags/6_dataproc_airflow.py
Traceback (most recent call last):
 File ""/home/iscipar/Proyectos/airflow-tutorial/airflow_env/lib/python3.12/site-packages/airflow/models/dagbag.py"", line 383, in parse
 loader.exec_module(new_module)
 File ""<frozen importlib._bootstrap_external>"", line 995, in exec_module
 File ""<frozen importlib._bootstrap>"", line 488, in _call_with_frames_removed
 File ""/home/iscipar/Proyectos/airflow-tutorial/dags/6_dataproc_airflow.py"", line 5, in <module>
 from airflow.providers.google.cloud.operators.dataproc import (
 File ""/home/iscipar/Proyectos/airflow-tutorial/airflow_env/lib/python3.12/site-packages/airflow/providers/google/cloud/operators/dataproc.py"", line 57, in <module>
 from airflow.providers.google.cloud.openlineage.utils import (
 File ""/home/iscipar/Proyectos/airflow-tutorial/airflow_env/lib/python3.12/site-packages/airflow/providers/google/cloud/openlineage/utils.py"", line 204, in <module>
 class BigQueryJobRunFacet(RunFacet):
TypeError: function() argument 'code' must be code, not str

### How to reproduce

The error is reproduced simply by adding the following import inside the code of a dag without importing the source code of the tasks of said dag:

```
from airflow.providers.google.cloud.operators.dataproc import (
    DataprocCreateClusterOperator,
    DataprocDeleteClusterOperator,
    DataprocSubmitJobOperator,
)
```

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",iscipar,2025-02-02 18:57:14+00:00,[],2025-02-05 17:06:21+00:00,2025-02-05 17:06:21+00:00,https://github.com/apache/airflow/issues/46357,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2629513559, 'issue_id': 2825956160, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 2, 2, 18, 57, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629526524, 'issue_id': 2825956160, 'author': 'mohamedmeqlad99', 'body': ""I think that Airflow's Google provider package is using code that is incompatible with Python 3.12. This is likely due to changes in Python 3.12 affecting the exec() function or types.CodeType \nI suggest Downgrade to Python 3.10 or 3.11"", 'created_at': datetime.datetime(2025, 2, 2, 19, 36, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2631797875, 'issue_id': 2825956160, 'author': 'iscipar', 'body': 'Hi.\nThis is my first time opening an issue in this community.\nI see that there are two solutions:\n- downgrade my python version\n- update the package with the fix from the pull request\nAs I prefer the second option, would a new version of the package with the fix be available now or do I have to wait? What npm install command would I have to run?\nThanks,', 'created_at': datetime.datetime(2025, 2, 3, 18, 50, 25, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-02-02 18:57:16 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

mohamedmeqlad99 on (2025-02-02 19:36:38 UTC): I think that Airflow's Google provider package is using code that is incompatible with Python 3.12. This is likely due to changes in Python 3.12 affecting the exec() function or types.CodeType 
I suggest Downgrade to Python 3.10 or 3.11

iscipar (Issue Creator) on (2025-02-03 18:50:25 UTC): Hi.
This is my first time opening an issue in this community.
I see that there are two solutions:
- downgrade my python version
- update the package with the fix from the pull request
As I prefer the second option, would a new version of the package with the fix be available now or do I have to wait? What npm install command would I have to run?
Thanks,

"
2825893083,issue,open,,Detect when bundle methods called before `initialize`?,"### Body

#46355 has highlighted the fact that we don't check if `initialize` has been called before other methods that rely on it have been. For example, `get_current_version`. I'm not positive we should check that, but opening this issue so I can think it through later.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2025-02-02 16:41:14+00:00,[],2025-02-03 14:26:06+00:00,,https://github.com/apache/airflow/issues/46356,"[('kind:meta', 'High-level information important to the community'), ('AIP-66: DAG Bundle/Manifest', '')]","[{'comment_id': 2631153157, 'issue_id': 2825893083, 'author': 'jx2lee', 'body': ""If it doesn't use `initialize` externally, what about moving the `initialize` to internal method(`_intialize`) and running in the constructor? Is it difficult because the `initialize` operation too expensive?"", 'created_at': datetime.datetime(2025, 2, 3, 14, 26, 5, tzinfo=datetime.timezone.utc)}]","jx2lee on (2025-02-03 14:26:05 UTC): If it doesn't use `initialize` externally, what about moving the `initialize` to internal method(`_intialize`) and running in the constructor? Is it difficult because the `initialize` operation too expensive?

"
2825864094,issue,closed,completed,"SFTPOperator with dynamic user, fails.","### What do you see as an issue?

Hey there, This is my first post here so forgive me if this is not the correct place.

I have a unique need of Airflow, I need to integrate SFTP&SSH Operator with a dynamic client, meaning I  need that every trigger will also prompt for a user and a password, This means that I can't actually create a permanent Connection in the UI.

I find that using SSHHook is the correct approch.
I created a task that receives the params, and does:
```
def create_ssh_hook(**kwargs):
    ti = kwargs[""ti""]
    ti.xcom_push(key=""ssh_hook_host"", value=kwargs[""params""][""host""])
    ti.xcom_push(key=""ssh_hook_username"", value=kwargs[""params""][""username""])
    ti.xcom_push(key=""ssh_hook_password"", value=kwargs[""params""][""password""])


create_ssh_hook_task = PythonOperator(
     task_id=""create_ssh_hook"",
     provide_context=True,
     python_callable=create_ssh_hook,
    op_kwargs=params
)


add_file_to_vm = SFTPOperator(
      task_id='add_script_to_vm',
      sftp_hook=SFTPHook(ssh_hook=SSHHook(remote_host=""{{ ti.xcom_pull(key='ssh_hook_host', task_ids='create_ssh_hook') }}"",  # I know this is not supported after 4.9.0, but I can't find a different way to do it and this is the version we have.
                                          username=""{{ ti.xcom_pull(key='ssh_hook_username', task_ids='create_ssh_hook') }}""
                                          password=""{{ ti.xcom_pull(key='ssh_hook_password', task_ids='create_ssh_hook') }}"")),
     local_filepath='/a',
     remote_filepath='/tmp/a',
     operation='put',
     create_intermediate_dirs=True,
)

create_ssh_hoot_task >> add_file_to_vm
```
Now I recieve the following error:
`socket.gaierror: [Errno -2] Name or service not known`

Now when trying to just ssh from the same place that runs this dag, works just fine, also setting up a proper connection with a static user and a ssh_conn_id also works.


### Solving the problem

I think the correct way of working with SSHHook should still be implemented.
I tried every docs file but none specify about dynamic connection.

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",remchuk,2025-02-02 15:39:01+00:00,[],2025-02-03 10:28:34+00:00,2025-02-03 10:28:34+00:00,https://github.com/apache/airflow/issues/46354,"[('kind:bug', 'This is a clearly a bug'), ('kind:documentation', ''), ('area:core-operators', 'Operators, Sensors and hooks within Core Airflow'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2629444767, 'issue_id': 2825864094, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 2, 2, 15, 39, 5, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-02-02 15:39:05 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2824145719,issue,open,,error on _resolve_should_track_driver_status with spark standalone (cluster mode),"### Apache Airflow Provider(s)

apache-spark

### Versions of Apache Airflow Providers

apache-airflow-providers-apache-spark    5.0.0
pyspark                                                       3.5.4


### Apache Airflow version

2.10.3

### Operating System

ubuntu server 22.04

### Deployment

Other

### Deployment details

NA

### What happened

Hi

SparkSubmitOperator fails when interacting with a spark standalone server in deploy-mode=cluster.

In such condition, the _resolve_should_track_driver_status function in /usr/local/lib/python3.12/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py returns True... 

I have exactly the same pb as the one discussed here :
https://github.com/apache/airflow/discussions/21799

I did find a case opened for this issue.

### What you think should happen instead

I have an error when _resolve_should_track_driver_status returns True.
If I modify the function to return False, testing the task passes but in such case the final spark job status is not confirmed by the spark driver.

### How to reproduce

```
cat sparkoperator_test.py
import airflow
from datetime import datetime, timedelta

from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

default_args = {
    'owner': 'xxxxxxxx',
    'start_date': datetime(2025, 1, 31),
}

with airflow.DAG('sparkoperator_test',
                  default_args=default_args,
                  schedule_interval='@hourly',
                  description='test of sparkoperator in airflow by Omar and Regis',
                  ) as dag:

    spark_conf={
        'spark.standalone.submit.waitAppCompletion':'true',
    }

    spark_compute_pi = SparkSubmitOperator(
        task_id='spark-compute-pi',
        conn_id='spark_qa1',
        java_class='org.apache.spark.examples.SparkPi',
        application='/opt/spark/examples/jars/spark-examples_2.12-3.5.3.jar',
        name='compute_pi_from_airflow',
        application_args=[""30""],
        total_executor_cores=4,
        executor_memory='1g',
        executor_cores=1,
        verbose='true',
        conf=spark_conf,
        execution_timeout=timedelta(minutes=10),
        retries=0,
    )

    spark_compute_pi

```

spark version : 3.5.3
1 single node (hosting the master + 1 worker node)

The error happens when testing the task with `airflow tasks test ...`

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rlebreto,2025-01-31 18:29:27+00:00,[],2025-01-31 18:29:31+00:00,,https://github.com/apache/airflow/issues/46334,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2628041431, 'issue_id': 2824145719, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 31, 18, 29, 29, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-31 18:29:29 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2823604019,issue,open,,Quick Start does not work as written on Ubuntu 24.04,"### What do you see as an issue?

When following https://airflow.apache.org/docs/apache-airflow/stable/start.html on an Ubuntu 24.04 system the `pip install` step fails with `error: externally-managed-environment`.

I would expect it to fail in the same way on a Debian Bookworm system and probably on other current distribution versions.

### Solving the problem

The Debian/Ubuntu error message gives good hints how to solve the problem, but I think that a quickstart document should provide steps that work out of the box. So they should involve creating a venv, or at least, if you assume users to have some required level Python knowledge, the document should warn the user that this step may be needed.

### Anything else

The full error text:
```
error: externally-managed-environment

× This environment is externally managed
╰─> To install Python packages system-wide, try apt install
    python3-xyz, where xyz is the package you are trying to
    install.
    
    If you wish to install a non-Debian-packaged Python package,
    create a virtual environment using python3 -m venv path/to/venv.
    Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make
    sure you have python3-full installed.
    
    If you wish to install a non-Debian packaged Python application,
    it may be easiest to use pipx install xyz, which will manage a
    virtual environment for you. Make sure you have pipx installed.
    
    See /usr/share/doc/python3.12/README.venv for more information.

note: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.
hint: See PEP 668 for the detailed specification.
```

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",mpol,2025-01-31 14:07:26+00:00,['adan-shahid'],2025-02-02 09:54:52+00:00,,https://github.com/apache/airflow/issues/46316,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('kind:documentation', '')]","[{'comment_id': 2627439695, 'issue_id': 2823604019, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 31, 14, 7, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2627759905, 'issue_id': 2823604019, 'author': 'potiuk', 'body': 'That is a feature of new `pip` relased recently. If you would like to submit a PR correcting the documentation fixing it you are most welcome (It\'s just a matter of clicking ""suggest a change on this page"" and PR will be opened for you and you can join > 3200 contributors to Airlfow and give back to the community by contributing such doc update. Other than that I mark it as ""good first issue"" so maybe someone will pick it up and fix. But I encourage you to make a PR.', 'created_at': datetime.datetime(2025, 1, 31, 16, 32, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629036898, 'issue_id': 2823604019, 'author': 'adan-shahid', 'body': '@potiuk Is this issue open? If yes then kindly assign it to me and guide me to start it.\nThanks!', 'created_at': datetime.datetime(2025, 2, 1, 17, 25, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629139434, 'issue_id': 2823604019, 'author': 'potiuk', 'body': 'Feel free.', 'created_at': datetime.datetime(2025, 2, 1, 22, 18, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629299178, 'issue_id': 2823604019, 'author': 'Maryam3107', 'body': ""> ### What do you see as an issue?\n> When following https://airflow.apache.org/docs/apache-airflow/stable/start.html on an Ubuntu 24.04 system the `pip install` step fails with `error: externally-managed-environment`.\n> \n> I would expect it to fail in the same way on a Debian Bookworm system and probably on other current distribution versions.\n> \n> ### Solving the problem\n> The Debian/Ubuntu error message gives good hints how to solve the problem, but I think that a quickstart document should provide steps that work out of the box. So they should involve creating a venv, or at least, if you assume users to have some required level Python knowledge, the document should warn the user that this step may be needed.\n> \n> ### Anything else\n> The full error text:\n> \n> ```\n> error: externally-managed-environment\n> \n> × This environment is externally managed\n> ╰─> To install Python packages system-wide, try apt install\n>     python3-xyz, where xyz is the package you are trying to\n>     install.\n>     \n>     If you wish to install a non-Debian-packaged Python package,\n>     create a virtual environment using python3 -m venv path/to/venv.\n>     Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n>     sure you have python3-full installed.\n>     \n>     If you wish to install a non-Debian packaged Python application,\n>     it may be easiest to use pipx install xyz, which will manage a\n>     virtual environment for you. Make sure you have pipx installed.\n>     \n>     See /usr/share/doc/python3.12/README.venv for more information.\n> \n> note: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n> hint: See PEP 668 for the detailed specification.\n> ```\n> \n> ### Are you willing to submit PR?\n> * [x]  Yes I am willing to submit a PR!\n> \n> ### Code of Conduct\n> * [x]  I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\nI can solve this issue. kindly assign it to me?"", 'created_at': datetime.datetime(2025, 2, 2, 8, 36, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629311378, 'issue_id': 2823604019, 'author': 'adan-shahid', 'body': 'I am already working on this issue.\n@Maryam3107 kindly look for another issue.', 'created_at': datetime.datetime(2025, 2, 2, 9, 15, 30, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-31 14:07:29 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2025-01-31 16:32:32 UTC): That is a feature of new `pip` relased recently. If you would like to submit a PR correcting the documentation fixing it you are most welcome (It's just a matter of clicking ""suggest a change on this page"" and PR will be opened for you and you can join > 3200 contributors to Airlfow and give back to the community by contributing such doc update. Other than that I mark it as ""good first issue"" so maybe someone will pick it up and fix. But I encourage you to make a PR.

adan-shahid (Assginee) on (2025-02-01 17:25:56 UTC): @potiuk Is this issue open? If yes then kindly assign it to me and guide me to start it.
Thanks!

potiuk on (2025-02-01 22:18:31 UTC): Feel free.

Maryam3107 on (2025-02-02 08:36:06 UTC): I can solve this issue. kindly assign it to me?

adan-shahid (Assginee) on (2025-02-02 09:15:30 UTC): I am already working on this issue.
@Maryam3107 kindly look for another issue.

"
2823225191,issue,open,,Consider shorter / lesser verbose standard provider import path for users,"### Body

A discussion maybe for later, but the entire canonical path in dags, like:
```
from airflow.providers.standard.operators.empty import EmptyOperator
```

seems to be pretty verbose. Consider having something like:
```
from airflow.providers.standard import EmptyOperator
```

OR

```
from airflow.providers.standard.operators import EmptyOperator
```

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",amoghrajesh,2025-01-31 11:59:50+00:00,[],2025-02-01 18:20:43+00:00,,https://github.com/apache/airflow/issues/46309,"[('area:providers', ''), ('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2627350943, 'issue_id': 2823225191, 'author': 'potiuk', 'body': 'Great Idea. We could even automate it easily for all providers - especially now after they are moved to separate repos. \n\nOur ""validate provider yaml"" already has the code to read and import all the provider  ""entities"" created - so turning that into generated ""__all__"" imports at the top-level of the provider should be easy.', 'created_at': datetime.datetime(2025, 1, 31, 13, 32, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2627362106, 'issue_id': 2823225191, 'author': 'aditya0yadav', 'body': 'Can we do something like this?  \n\nExample:  \nfrom airflow.providers.standard.operators.empty import EmptyOperator  \n\nThe parent directory of ""empty"" could include an init file or another module that collectively imports and organizes the classes and functions used in the subdirectories under ""operators.""  \n\nThis approach could potentially resolve the issue, but we would need approval from maintainers or other contributors to implement it.  \n\nIf there\'s a better solution, please share your thoughts in the comments. Your insights would help improve my understanding.', 'created_at': datetime.datetime(2025, 1, 31, 13, 37, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2627366553, 'issue_id': 2823225191, 'author': 'aditya0yadav', 'body': 'if this work \nplease let me work on this', 'created_at': datetime.datetime(2025, 1, 31, 13, 38, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2627503462, 'issue_id': 2823225191, 'author': 'potiuk', 'body': '> Can we do something like this?\n> \n> Example: from airflow.providers.standard.operators.empty import EmptyOperator\n> \n> The parent directory of ""empty"" could include an init file or another module that collectively imports and organizes the classes and functions used in the subdirectories under ""operators.""\n> \n> This approach could potentially resolve the issue, but we would need approval from maintainers or other contributors to implement it.\n> \n> If there\'s a better solution, please share your thoughts in the comments. Your insights would help improve my understanding.\n\nThose __init__ files **SHOULD** be generated automatically and absolutely not maintained manually (they already ARE I think) - so we need to implement it as part of the pre-commit framework that should generate the right code for all providers.\n\nAnd yes - if done like that I am ok with it (we do not have provider maintainers - maintainers are just maintainers).', 'created_at': datetime.datetime(2025, 1, 31, 14, 36, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629057198, 'issue_id': 2823225191, 'author': 'Prab-27', 'body': ""Interesting! I'd like to get involved in configuring a pre-commit hook for this"", 'created_at': datetime.datetime(2025, 2, 1, 18, 20, 42, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-31 13:32:42 UTC): Great Idea. We could even automate it easily for all providers - especially now after they are moved to separate repos. 

Our ""validate provider yaml"" already has the code to read and import all the provider  ""entities"" created - so turning that into generated ""__all__"" imports at the top-level of the provider should be easy.

aditya0yadav on (2025-01-31 13:37:17 UTC): Can we do something like this?  

Example:  
from airflow.providers.standard.operators.empty import EmptyOperator  

The parent directory of ""empty"" could include an init file or another module that collectively imports and organizes the classes and functions used in the subdirectories under ""operators.""  

This approach could potentially resolve the issue, but we would need approval from maintainers or other contributors to implement it.  

If there's a better solution, please share your thoughts in the comments. Your insights would help improve my understanding.

aditya0yadav on (2025-01-31 13:38:22 UTC): if this work 
please let me work on this

potiuk on (2025-01-31 14:36:10 UTC): Those __init__ files **SHOULD** be generated automatically and absolutely not maintained manually (they already ARE I think) - so we need to implement it as part of the pre-commit framework that should generate the right code for all providers.

And yes - if done like that I am ok with it (we do not have provider maintainers - maintainers are just maintainers).

Prab-27 on (2025-02-01 18:20:42 UTC): Interesting! I'd like to get involved in configuring a pre-commit hook for this

"
2821736322,issue,closed,completed,joined task always skipped,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

The joning task after branching is always skipped with trigger policy=""none_failed_min_one_success"":

```
from airflow.utils.dates import days_ago
from airflow.decorators import dag, task, task_group

@task
def start():
    return [ 1, 2, 3 ]


@task_group
def expanding(n):

    @task.branch
    def branching(n):
        print(n)

        if n == 1:
            return ""expanding.foo""
        else:
            return ""expanding.bar""

    @task
    def foo(n):
        print(n)
        return n + 1

    @task
    def bar(n):
        print(n)
        return n + 3

    @task(trigger_rule='none_failed_min_one_success')
    def finalize(x, y):
        if x is not None:
            return x
        if y is not None:
            return y
        return None

    choose_tech_branch_result = branching(n)

    foo_result = foo(n)
    bar_result = bar(n)

    choose_tech_branch_result >> [ foo_result, bar_result ]

    finalize_result = finalize(foo_result, bar_result)
    [ foo_result, bar_result ] >> finalize_result

    return finalize_result


@task
def finalize_all(group_results):
    print(group_results)
    return group_results


@dag(dag_id=""branching"",
     start_date=days_ago(1),
     schedule_interval=None)
def branch_example():

    group_results = expanding.expand(n=start())
    finalizer_result = finalize_all(group_results)
    print(finalizer_result)

branch_example()

```

Workaround: pass argument `n` to task ´finalize` even if it's not used. seams to be an issue with upstream evaluation.
```
from airflow.utils.dates import days_ago
from airflow.decorators import dag, task, task_group

@task
def start():
    return [ 1, 2, 3 ]


@task_group
def expanding(n):

    @task.branch
    def branching(n):
        print(n)

        if n == 1:
            return ""expanding.foo""
        else:
            return ""expanding.bar""

    @task
    def foo(n):
        print(n)
        return n + 1

    @task
    def bar(n):
        print(n)
        return n + 3

    @task(trigger_rule='none_failed_min_one_success')
    def finalize(_, x, y):
        if x is not None:
            return x
        if y is not None:
            return y
        return None

    choose_tech_branch_result = branching(n)

    foo_result = foo(n)
    bar_result = bar(n)

    choose_tech_branch_result >> [ foo_result, bar_result ]

    finalize_result = finalize(n, foo_result, bar_result)
    [ foo_result, bar_result ] >> finalize_result

    return finalize_result


@task
def finalize_all(group_results):
    print(group_results)
    return group_results


@dag(dag_id=""branching"",
     start_date=days_ago(1),
     schedule_interval=None)
def branch_example():

    group_results = expanding.expand(n=start())
    finalizer_result = finalize_all(group_results)
    print(finalizer_result)

branch_example()
```


### What you think should happen instead?

_No response_

### How to reproduce

see comment/code

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",olk,2025-01-30 20:18:24+00:00,[],2025-01-31 13:47:53+00:00,2025-01-31 13:47:53+00:00,https://github.com/apache/airflow/issues/46297,"[('kind:bug', 'This is a clearly a bug'), ('area:core', '')]",[],
2821215959,issue,open,,Allow passing config_path as a parameter to KubernetesExecutor,"### Description

Currently, when using executor_config with KubernetesExecutor, the config_file path must be hardcoded. If we try to pass it dynamically using a Jinja template (""{{ params.config_path }}""), it results in an error.

Example of a working static configuration:
```python
executor_config={""KubernetesExecutor"": {""config_file"": ""/opt/airflow/config/kubernetes/docker-desktop-config.yaml""}}
```
However, if we attempt to pass it dynamically, it fails:
```python
executor_config={""KubernetesExecutor"": {""config_file"": ""{{ params.config_path }}""}}
```
This would be useful when deploying DAGs across different environments (e.g., multiple regions) without hardcoding paths.

**Proposed Solution:**

Modify executor_config to allow templating in KubernetesExecutor.config_file, similar to other templated parameters in Airflow.

### Use case/motivation

This would be useful when deploying DAGs across different environments (e.g., multiple regions) without hardcoding paths.
Airflow should allow config_file to accept templated values, resolving them at runtime.
This feature would improve flexibility when running KubernetesExecutor tasks that need different configurations based on dynamic parameters.
Would love to hear thoughts from the community on this! Thanks! 😊

### Related issues


This feature would improve Airflow’s flexibility in hybrid execution environments and align with the goals of[ AIP-61](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-61+Hybrid+Execution). Would love to hear thoughts from the community on this! 🚀

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",pablo-moreno-mancebo,2025-01-30 16:01:30+00:00,['Maryam3107'],2025-02-03 10:03:08+00:00,,https://github.com/apache/airflow/issues/46293,"[('kind:feature', 'Feature Requests'), ('good first issue', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]","[{'comment_id': 2624904177, 'issue_id': 2821215959, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 30, 16, 1, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2627389225, 'issue_id': 2821215959, 'author': 'potiuk', 'body': ""If you can take a look at propose a PR solving it, yes I think it would be useful. But it needs a deep look in the code whether it's possible, also with Airflow 3 changes / Task SDK changes in progress, likely doing it after this is completed, is a good idea."", 'created_at': datetime.datetime(2025, 1, 31, 13, 44, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629153797, 'issue_id': 2821215959, 'author': 'insomnes', 'body': 'Are you sure you can rewrite it with?\n ```\nexecutor_config={""KubernetesExecutor"": {""config_file"": ""/opt/airflow/config/kubernetes/docker-desktop-config.yaml""}}\n```\nI\'ve dug through the code and also experimented a bit, and couldn\'t find any way to rewrite the initial config file for `KubernetesExecutor`. Am I missing something?  Any  of the `executor_config` I\'ve tried to use with deprecated `KubernetesExecutor` variant except the empty one led to the error:\n```\n{kubernetes_executor.py:355} ERROR - Invalid executor_config for TaskInstanceKey\n```', 'created_at': datetime.datetime(2025, 2, 1, 23, 8, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629394496, 'issue_id': 2821215959, 'author': 'Maryam3107', 'body': '> ### Description\n> Currently, when using executor_config with KubernetesExecutor, the config_file path must be hardcoded. If we try to pass it dynamically using a Jinja template (""{{ params.config_path }}""), it results in an error.\n> \n> Example of a working static configuration:\n> \n> executor_config={""KubernetesExecutor"": {""config_file"": ""/opt/airflow/config/kubernetes/docker-desktop-config.yaml""}}\n> However, if we attempt to pass it dynamically, it fails:\n> \n> executor_config={""KubernetesExecutor"": {""config_file"": ""{{ params.config_path }}""}}\n> This would be useful when deploying DAGs across different environments (e.g., multiple regions) without hardcoding paths.\n> \n> **Proposed Solution:**\n> \n> Modify executor_config to allow templating in KubernetesExecutor.config_file, similar to other templated parameters in Airflow.\n> \n> ### Use case/motivation\n> This would be useful when deploying DAGs across different environments (e.g., multiple regions) without hardcoding paths. Airflow should allow config_file to accept templated values, resolving them at runtime. This feature would improve flexibility when running KubernetesExecutor tasks that need different configurations based on dynamic parameters. Would love to hear thoughts from the community on this! Thanks! 😊\n> \n> ### Related issues\n> This feature would improve Airflow’s flexibility in hybrid execution environments and align with the goals of[ AIP-61](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-61+Hybrid+Execution). Would love to hear thoughts from the community on this! 🚀\n> \n> ### Are you willing to submit a PR?\n> * [ ]  Yes I am willing to submit a PR!\n> \n> ### Code of Conduct\n> * [x]  I agree to follow this project\'s [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n\nI can solve this problem. kindly assign it to me', 'created_at': datetime.datetime(2025, 2, 2, 13, 21, 33, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-30 16:01:33 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2025-01-31 13:44:12 UTC): If you can take a look at propose a PR solving it, yes I think it would be useful. But it needs a deep look in the code whether it's possible, also with Airflow 3 changes / Task SDK changes in progress, likely doing it after this is completed, is a good idea.

insomnes on (2025-02-01 23:08:20 UTC): Are you sure you can rewrite it with?
 ```
executor_config={""KubernetesExecutor"": {""config_file"": ""/opt/airflow/config/kubernetes/docker-desktop-config.yaml""}}
```
I've dug through the code and also experimented a bit, and couldn't find any way to rewrite the initial config file for `KubernetesExecutor`. Am I missing something?  Any  of the `executor_config` I've tried to use with deprecated `KubernetesExecutor` variant except the empty one led to the error:
```
{kubernetes_executor.py:355} ERROR - Invalid executor_config for TaskInstanceKey
```

Maryam3107 (Assginee) on (2025-02-02 13:21:33 UTC): I can solve this problem. kindly assign it to me

"
2820851906,issue,open,,Complex conditional dataset scheduling does not displayed in DAG graph,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.2

### What happened?

I am trying to schedule datasets with the `&` and `|` condition to implement data lineage using Datasets. The list of datasets that depend on the DAG is as follows:

```
(
      (Dataset(""table1"") & Dataset(""table2"") & Dataset(""table3"")) |
      (Dataset(""tableA"") & Dataset(""tableB"") & Dataset(""tableC"") & Dataset(""dummy""))
)
```

The intention is that every day, `table1`, `table2`, and `table3 `will be updated, while `tableA`, `tableB`, and `tableC `will be updated on an ad-hoc basis -- and I do not want the DAG to be triggered. 

However, after deploying this DAG to Airflow, the DAG graph does not display the dependent datasets. It only displays tasks and outlet datasets.

I also tried other variations, such as:
```
(
    Dataset(""table1"") & Dataset(""table2"") & Dataset(""table3"") | Dataset(""tableA"")
)
```
and
```
(
    Dataset(""table1"") & Dataset(""table2"") & Dataset(""table3"") | Dataset(""tableA"") | Dataset(""tableB"")
)
```
Both of these display correctly. However, when using:
```
(
    Dataset(""table1"") & Dataset(""table2"") & Dataset(""table3"") | Dataset(""tableA"") & Dataset(""tableB"")
)
```
The DAG does not display the datasets. However, Dataset graph displays dataset relation correctly in every example.


### What you think should happen instead?

_No response_

### How to reproduce

Create a dag with conditonal dataset scheduling, complex &, | like in the description. Try deploying to Airflow and see DAG graph view.

### Operating System

N/A

### Versions of Apache Airflow Providers

_No response_

### Deployment

Google Cloud Composer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",kullachartp,2025-01-30 13:32:31+00:00,[],2025-02-03 19:07:43+00:00,,https://github.com/apache/airflow/issues/46288,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('area:datasets', 'Issues related to the datasets feature'), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2624528939, 'issue_id': 2820851906, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 30, 13, 32, 34, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-30 13:32:34 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2820748681,issue,open,,Export react-flow dag graph,"### Description

Is there a way to export the html content of a DAG graph to an image on the UI side?
Also I am thinking could we add a REST API endpoint that returns the html content.

### Use case/motivation

- A download image button like here [https://reactflow.dev/examples/misc/download-image](https://reactflow.dev/examples/misc/download-image)
- A rest API endpoint that returns the html content

### Related issues

_No response_

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",dominic-lcw,2025-01-30 12:47:36+00:00,[],2025-01-30 12:49:53+00:00,,https://github.com/apache/airflow/issues/46285,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2624418211, 'issue_id': 2820748681, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 30, 12, 47, 39, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-30 12:47:39 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2819989757,issue,open,,AIP-83 Amendment refactor reprocess behaviour in backfill,"### Description

We will encounter an issue when attempting to create a backfill with the reprocess behavior set to `completed` or `failed`, as it will trigger a unique constraint error on logical_date due to the restored uniqueness constraint.

To align with the old (2.x) behavior for existing runs that need to be reprocessed, we should ensure that when reprocessing results in a new run, the existing run is cleared. Instead of deleting the row, we update the necessary fields—similar to what was done in 2.x. This at least includes changing the state, but likely also updating start_date and possibly other relevant fields.

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vatsrahul1001,2025-01-30 06:10:11+00:00,['vatsrahul1001'],2025-01-30 06:12:24+00:00,,https://github.com/apache/airflow/issues/46278,"[('kind:feature', 'Feature Requests'), ('area:backfill', 'Specifically for backfill related')]",[],
2819153128,issue,closed,completed,sklearn is not a dependency and fails with google-cloud-aiplatform 1.79.0,"### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

From the last failure https://github.com/apache/airflow/actions/runs/13032455156/job/36355532907, looks like sklearn is not a dependency on google provider. Using the latest stable version: https://scikit-learn.org/stable/auto_examples/release_highlights/plot_release_highlights_1_6_0.html

See also https://github.com/apache/airflow/pull/46242/

### Apache Airflow version

main

### Operating System

any

### Deployment

Other

### Deployment details

CI

### What happened

_No response_

### What you think should happen instead

_No response_

### How to reproduce

See https://github.com/apache/airflow/actions/runs/13032455156/job/36355532907

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jscheffl,2025-01-29 20:03:19+00:00,[],2025-01-30 07:23:41+00:00,2025-01-30 07:23:41+00:00,https://github.com/apache/airflow/issues/46258,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('area:providers', ''), ('area:dependencies', 'Issues related to dependencies problems')]",[],
2818818895,issue,open,,Add API endpoint that lets you see what happened in the backfill,"### Body

We can see the backfill entity but it does not show what runs were trigger with it etc.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2025-01-29 17:26:47+00:00,['vatsrahul1001'],2025-01-29 17:29:04+00:00,,https://github.com/apache/airflow/issues/46250,"[('area:API', ""Airflow's REST/HTTP API""), ('kind:meta', 'High-level information important to the community'), ('area:backfill', 'Specifically for backfill related')]",[],
2818284423,issue,closed,completed,Long task IDs break Airflow UI: task blocks become hidden,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When a task ID has a very long task name, e.g. equal to `AIRFLOW_MAX_ID_LENGTH`, the UI breaks on smaller monitor resolutions with default browser zoom.

The way it breaks is by the task blocks becoming hidden, even if scrolling the task ID + task blocks div all the way to the right:

<img width=""1487"" alt=""Image"" src=""https://github.com/user-attachments/assets/a6c400af-2a23-47f7-858c-dc678a81e23e"" />


The only way to see the task blocks again is by either zooming out on the browser (which makes everything too small to read), or by using a bigger monitor, or by expanding the browser window over a multi-monitor setup.

### What you think should happen instead?

The blocks should not become hidden when scrolling all the way to the right. In fact, I think it's a bad thing that they disappear at all.

I think a better solution here would be to make the task instance blocks not live in the same div as the task IDs. That way, you would be able to resize/scroll through the task IDs, the task blocks, and the details/log divs each individually.

### How to reproduce

Put this DAG in your Airflow instance:

```
from airflow import DAG
from airflow.operators.empty import EmptyOperator
from datetime import datetime, timedelta


default_args = {
    ""owner"": ""airflow"",
    ""depends_on_past"": False,
    ""start_date"": datetime(2025, 1, 29),
    ""email_on_failure"": False,
    ""email_on_retry"": False,
    ""retries"": 1,
    ""retry_delay"": timedelta(minutes=5),
}

dag = DAG(
    ""dag_with_long_task_id"",
    default_args=default_args,
    description=""A DAG with a single task having a 250-character task ID"",
    schedule_interval=timedelta(days=1),
)

task_with_long_id = EmptyOperator(
    task_id=250 * ""a"",
    dag=dag,
)
```

### Operating System

macOS 15.2 (24C101)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",internetcoffeephone,2025-01-29 13:50:39+00:00,[],2025-01-29 23:45:32+00:00,2025-01-29 23:45:32+00:00,https://github.com/apache/airflow/issues/46239,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]",[],
2817759847,issue,open,,Upload task logs to remote storage in Task SDK supervisor,"### Description

If remote logging has been configured, the supervisor should attempt to upload the task logs to remote storage as part of its post-execute step. The flow should look something like the following:

execute -> upload remote storage -> report final state to API -> finalize (e.g. run listeners) -> supervisor uploads task logs

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",ianbuss,2025-01-29 10:09:28+00:00,[],2025-01-29 10:11:48+00:00,,https://github.com/apache/airflow/issues/46234,"[('area:logging', ''), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2817560338,issue,open,,Data-aware scheduling with time delay,"### Description

time delay args for data-aware scheduling dags

### Use case/motivation

I have 2 dags, `main_dag` and `status_dag`. `main_dag` outlets a dataset called `dataset1` and the `status_dag` should be triggered after 10 minutes of when `dataset1` is created or updated. Right now, I use time.sleep() method in `status_dag` to delay for 10 mins and then get executed.

Wouldn't this be helpful to add a time delay for data-aware scheduling dags?

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",lafirm,2025-01-29 08:36:04+00:00,[],2025-02-03 00:42:09+00:00,,https://github.com/apache/airflow/issues/46230,"[('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('area:datasets', 'Issues related to the datasets feature')]","[{'comment_id': 2620991664, 'issue_id': 2817560338, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 29, 8, 36, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2621008646, 'issue_id': 2817560338, 'author': 'eladkal', 'body': '>  should be triggered after 10 minutes of when dataset1 is created or updated\n\nCan you share reasoning why would you want that?', 'created_at': datetime.datetime(2025, 1, 29, 8, 45, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2621028797, 'issue_id': 2817560338, 'author': 'lafirm', 'body': ""> Can you share reasoning why would you want that?\n\nIn my current project where I use SQLMesh which uses intervals and I need to wait for some time to get the complete intervals where I don't want to use allow_partials. I said 10 mins is for an example. And I apologise that this is the max context I can provide, can't go more detail on this. Hope you understand!\n\nIt's not a blocker for me, however I thought that this feature would be beneficial for someone like me who works with status updates and SQLMesh intervals."", 'created_at': datetime.datetime(2025, 1, 29, 8, 54, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2621723511, 'issue_id': 2817560338, 'author': 'eladkal', 'body': 'I am not sure I follow on the scenario.\nAre you saying that some component reported finished successfully - thus downstream task is scheduled but the finish success report is not true until 10 min pass? Then why it reported success to begin with?', 'created_at': datetime.datetime(2025, 1, 29, 13, 55, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2622113409, 'issue_id': 2817560338, 'author': 'lafirm', 'body': ""Its not correct.\n\nMy use case is that an incremental SQLMesh model (called model_a) created by an airflow task which contains intervals, and I've another DAG with incremental model_b which has a where clause on start and end datetime of the model intervals and model_b needs the upto date value of model_a for which I need to wait for the interval to get completed because of the way SQLMesh works and it's interval, start and end datetime of a model"", 'created_at': datetime.datetime(2025, 1, 29, 16, 24, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629392679, 'issue_id': 2817560338, 'author': 'eladkal', 'body': '> for which I need to wait for the interval to get completed \n\nAirflow doesn\'t have SQLMesh provider/operators thus I assume you implemented your own.\nPlease explain why the task Airflow succeed before the interval completed.\nTo clarify, I assume you have something like this:\n\n```\nwith DAG(...):\n    MyOperator(\n        # this runs SqlMesh model\n        outlets=[Dataset(""my_dataset"")],\n        ...,\n    )\n```', 'created_at': datetime.datetime(2025, 2, 2, 13, 15, 51, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-29 08:36:07 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2025-01-29 08:45:32 UTC): Can you share reasoning why would you want that?

lafirm (Issue Creator) on (2025-01-29 08:54:23 UTC): In my current project where I use SQLMesh which uses intervals and I need to wait for some time to get the complete intervals where I don't want to use allow_partials. I said 10 mins is for an example. And I apologise that this is the max context I can provide, can't go more detail on this. Hope you understand!

It's not a blocker for me, however I thought that this feature would be beneficial for someone like me who works with status updates and SQLMesh intervals.

eladkal on (2025-01-29 13:55:22 UTC): I am not sure I follow on the scenario.
Are you saying that some component reported finished successfully - thus downstream task is scheduled but the finish success report is not true until 10 min pass? Then why it reported success to begin with?

lafirm (Issue Creator) on (2025-01-29 16:24:08 UTC): Its not correct.

My use case is that an incremental SQLMesh model (called model_a) created by an airflow task which contains intervals, and I've another DAG with incremental model_b which has a where clause on start and end datetime of the model intervals and model_b needs the upto date value of model_a for which I need to wait for the interval to get completed because of the way SQLMesh works and it's interval, start and end datetime of a model

eladkal on (2025-02-02 13:15:51 UTC): Airflow doesn't have SQLMesh provider/operators thus I assume you implemented your own.
Please explain why the task Airflow succeed before the interval completed.
To clarify, I assume you have something like this:

```
with DAG(...):
    MyOperator(
        # this runs SqlMesh model
        outlets=[Dataset(""my_dataset"")],
        ...,
    )
```

"
2817403729,issue,closed,not_planned,Add a Deferrable Version of MSGraphSensor,"### Description

This feature allows the sensor to relinquish control during periods of inactivity, introducing a deferrable version of the MSGraphSensor to maximize resource use. The efficiency of sensor operations within the Microsoft Graph integration for Apache Airflow is increased by this improvement.

### Use case/motivation

The purpose of this enhancement is to improve Apache Airflow's current Microsoft Graph integration by increasing the effectiveness of resources for lengthy jobs,facilitating resource-constrained workflows that are scalable, addressing any mistakes by using a strong response handler for non-JSON or empty responses and Users can more effectively manage workloads involving Microsoft Graph services by putting this deferrable sensor -into practice.

### Related issues

N/A

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Afreedhi,2025-01-29 07:01:02+00:00,[],2025-01-29 08:19:38+00:00,2025-01-29 08:19:31+00:00,https://github.com/apache/airflow/issues/46227,"[('kind:feature', 'Feature Requests'), ('area:async-operators', 'AIP-40: Deferrable (""Async"") Operators'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2620854630, 'issue_id': 2817403729, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 29, 7, 1, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2620962532, 'issue_id': 2817403729, 'author': 'eladkal', 'body': 'AI bot', 'created_at': datetime.datetime(2025, 1, 29, 8, 19, 31, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-29 07:01:04 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2025-01-29 08:19:31 UTC): AI bot

"
2817171804,issue,open,,retries and callbacks are not honoured for ending deferred task from trigger,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/deferring.html#exiting-deferred-task-from-triggers

On tasks with retry when a `TaskFailedEvent` is from trigger the task is marked as failed with the retries not executed for the failed tasks. Similar issue with callbacks and also email_on_failure, email_on_retry also not being honored. This happens in main and also 2.10.4 . PR related https://github.com/apache/airflow/pull/40084

### What you think should happen instead?

Retries, callbacks and emails on failure should be honored. 

### How to reproduce

1. Create following triggers and dag with retries and callback.
2. Run the dag with `TaskFailedEvent` emitted from the trigger.
3. The dag is marked as failed without retries and callbacks executed.
4. Uncomment yielding `TriggerEvent` and run the dag again with new code.
5. Retry happens and the failure callback is also executed.

```python
# plugins/custom_trigger.py
from __future__ import annotations

import asyncio
import logging

from airflow.triggers.base import BaseTrigger, TriggerEvent, TaskSuccessEvent, TaskFailedEvent
from airflow.utils import timezone

class StateTrigger(BaseTrigger):
    def __init__(self, state):
        super().__init__()
        self.state = state

    def serialize(self):
        return (""custom_trigger.StateTrigger"", {""state"": self.state})

    async def run(self):
        if self.state == ""success"":
            yield TaskSuccessEvent()
        else:
            yield TaskFailedEvent()

        # yield TriggerEvent(self.state)
```

```python

# dag_state_test.py

from __future__ import annotations

from datetime import datetime

from custom_trigger import StateTrigger

from airflow import DAG
from airflow.models.baseoperator import BaseOperator


class MultipleDeferTrigger(BaseOperator):
    """"""Multiple defer trigger.""""""

    def __init__(self, state=None, *args, **kwargs):
        self.state = state
        super().__init__(*args, **kwargs)

    def execute(self, context):
        self.defer(
            trigger=StateTrigger(self.state),
            method_name=""execute_complete"",
        )

    def execute_complete(self, context, event=None):
        raise Exception(event)

with DAG(
    dag_id=""state_defer"",
    start_date=datetime(2021, 1, 1),
    catchup=False,
    schedule=None,
    default_args = {
        ""on_success_callback"": lambda context: open(""/tmp/on_success_callback"", ""w+"").write(str(datetime.now())),
        ""on_failure_callback"": lambda context: open(""/tmp/on_failure_callback"", ""w+"").write(str(datetime.now())),
        ""retries"": 1,
        ""retry_delay"": 5.0
    }
) as dag:
    success = MultipleDeferTrigger(task_id=""success"", state=""success"", retry_delay=5.0)
    failed = MultipleDeferTrigger(task_id=""failed"", state=""failed"", retry_delay=5.0)

    success
    failed

```

### Operating System

Ubuntu 20.04.3 LTS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",tirkarthi,2025-01-29 03:43:09+00:00,[],2025-01-29 03:44:15+00:00,,https://github.com/apache/airflow/issues/46224,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:Triggerer', '')]","[{'comment_id': 2620662243, 'issue_id': 2817171804, 'author': 'tirkarthi', 'body': 'cc: @sunank200', 'created_at': datetime.datetime(2025, 1, 29, 3, 44, 14, tzinfo=datetime.timezone.utc)}]","tirkarthi (Issue Creator) on (2025-01-29 03:44:14 UTC): cc: @sunank200

"
2816760073,issue,open,,Pymongo 4.11 breaks Mongo provider,"### Apache Airflow Provider(s)

mongo

### Versions of Apache Airflow Providers

Failed CI run:
https://github.com/apache/airflow/actions/runs/13018061271/job/36312691670

Errors when using pymongo 4.11:
FAILED providers/mongo/tests/provider_tests/mongo/hooks/test_mongo.py::TestMongoHook::test_replace_many - TypeError: BulkOperationBuilder.add_replace() got an unexpected keyword argument 'sort'
FAILED providers/mongo/tests/provider_tests/mongo/hooks/test_mongo.py::TestMongoHook::test_replace_many_with_upsert - TypeError: BulkOperationBuilder.add_replace() got an unexpected keyword argument 'sort'

PR as workaround coming

### Apache Airflow version

main

### Operating System

any

### Deployment

Other

### Deployment details

_No response_

### What happened

_No response_

### What you think should happen instead

_No response_

### How to reproduce

breeze testing providers-tests --test-type ""Providers[mongo]"" --python 3.12 

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jscheffl,2025-01-28 21:35:07+00:00,[],2025-01-29 16:46:11+00:00,,https://github.com/apache/airflow/issues/46215,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('provider:mongo', '')]",[],
2816348141,issue,closed,completed,AIP-83 amendment -- update backfill,"### Body

Once we've restored uniqueness on logical date, backfill logic can be made a bit simpler.

We should go through, and in places where we guard against the possibility of multiple runs for a given logical date, we should simplify the logic.  

Additionally, in places where we do locking to avoid the creation of multiple concurrent runs for a given logical date, we should just rely on the fact that there is a unique constraint to protect us.

We also need to restore the old (2.x) behavior re existing runs that need to be reprocessed.  When the reprocess behavior would result in a new run, we clear the existing run.  We don't delete the row but update the fields that need to be updated -- whatever was done in 2.x.  At least this means changing the state, but probably also start_date and maybe other fields as well.

(reference: https://cwiki.apache.org/confluence/display/AIRFLOW/Option+2+clarification+doc+WIP)



### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2025-01-28 17:42:47+00:00,['prabhusneha'],2025-02-07 07:06:53+00:00,2025-02-07 07:06:53+00:00,https://github.com/apache/airflow/issues/46202,"[('kind:meta', 'High-level information important to the community'), ('area:backfill', 'Specifically for backfill related')]","[{'comment_id': 2620809093, 'issue_id': 2816348141, 'author': 'prabhusneha', 'body': 'I can pick this task.', 'created_at': datetime.datetime(2025, 1, 29, 6, 20, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2622102110, 'issue_id': 2816348141, 'author': 'prabhusneha', 'body': 'PR : #46248 \n#46187  needs to be merged before closing this one.', 'created_at': datetime.datetime(2025, 1, 29, 16, 19, 44, tzinfo=datetime.timezone.utc)}]","prabhusneha (Assginee) on (2025-01-29 06:20:51 UTC): I can pick this task.

prabhusneha (Assginee) on (2025-01-29 16:19:44 UTC): PR : #46248 
#46187  needs to be merged before closing this one.

"
2816340851,issue,open,,AIP-83 question 7 How do we deal with APIs that select a logical date range?,"### Body

For xcom pull, setting `include_prior_dates` to True will have no effect.  The reason why is, prior refers to prior logical date, and we do not propose to change this.

For clear, same thing – adding those filters will have no effect, for the same reason.

When you use a range, runs with null logical date will never be included in the result.

In other cases, when there existing filters that apply to logical date, we don't plan to change it.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2025-01-28 17:39:10+00:00,['sunank200'],2025-02-04 16:26:11+00:00,,https://github.com/apache/airflow/issues/46200,"[('area:API', ""Airflow's REST/HTTP API""), ('kind:meta', 'High-level information important to the community')]",[],
2816336650,issue,open,,AIP-83 question 6 run_id logic when no logical date,"### Body

to construct run_id, when null logical date, use run_after + random string

(depends on addition of run_after, which is in another ticket)

(question 6 https://cwiki.apache.org/confluence/display/AIRFLOW/Option+2+clarification+doc+WIP)

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2025-01-28 17:37:09+00:00,['prabhusneha'],2025-02-03 20:17:48+00:00,,https://github.com/apache/airflow/issues/46199,"[('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2621240559, 'issue_id': 2816336650, 'author': 'prabhusneha', 'body': 'I can work on this', 'created_at': datetime.datetime(2025, 1, 29, 10, 25, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2631984703, 'issue_id': 2816336650, 'author': 'prabhusneha', 'body': 'PR: #46398', 'created_at': datetime.datetime(2025, 2, 3, 20, 17, 46, tzinfo=datetime.timezone.utc)}]","prabhusneha (Assginee) on (2025-01-29 10:25:40 UTC): I can work on this

prabhusneha (Assginee) on (2025-02-03 20:17:46 UTC): PR: #46398

"
2816332943,issue,open,,AIP-83 question 5 template / context access behavior,"### Body

for runs with no logical date / data interval, there should be a key error if these vars are accessed from template / context.

(question 5 https://cwiki.apache.org/confluence/display/AIRFLOW/Option+2+clarification+doc+WIP)

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2025-01-28 17:35:36+00:00,['prabhusneha'],2025-02-04 11:24:15+00:00,,https://github.com/apache/airflow/issues/46198,"[('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2633621457, 'issue_id': 2816332943, 'author': 'prabhusneha', 'body': 'I can work on this. Please assign it to me.', 'created_at': datetime.datetime(2025, 2, 4, 11, 24, 9, tzinfo=datetime.timezone.utc)}]","prabhusneha (Assginee) on (2025-02-04 11:24:09 UTC): I can work on this. Please assign it to me.

"
2816328985,issue,open,,AIP-83 question 4.b. manual run data interval behavior,"### Manual runs, schedule-driven dag

If logical date null, there should be no data interval; if logical date not null, data interval should be as normal.

If logical date is null, accessing data interval in context or templates should result in a key error.

### Manual runs, dag is asset driven

There will be no data interval.

Accessing data interval in context or templates should result in a key error.
",dstandish,2025-01-28 17:33:47+00:00,['sunank200'],2025-02-05 06:07:46+00:00,,https://github.com/apache/airflow/issues/46197,"[('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2635782110, 'issue_id': 2816328985, 'author': 'sunank200', 'body': 'I am pushing changes to [manual-data-interval-behaviour](https://github.com/astronomer/airflow/tree/manual-data-interval-behaviour) branch.', 'created_at': datetime.datetime(2025, 2, 5, 6, 7, 45, tzinfo=datetime.timezone.utc)}]","sunank200 (Assginee) on (2025-02-05 06:07:45 UTC): I am pushing changes to [manual-data-interval-behaviour](https://github.com/astronomer/airflow/tree/manual-data-interval-behaviour) branch.

"
2816325804,issue,open,,AIP-83 question 4.a. Asset-triggered dags have no logical date and no data_interval_start / data_interval_end,"### Body

Asset-triggered dags will have neither logical date nor data_interval_start / data_interval_end

question 4.a. https://cwiki.apache.org/confluence/display/AIRFLOW/Option+2+clarification+doc+WIP

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2025-01-28 17:32:09+00:00,['sunank200'],2025-02-04 17:20:18+00:00,,https://github.com/apache/airflow/issues/46196,"[('kind:meta', 'High-level information important to the community'), ('area:datasets', 'Issues related to the datasets feature')]",[],
2816323127,issue,closed,completed,AIP-83 question 3 new field `run_after`,"### Body

we add a new date field run_after which means, the earliest time after which this dag run may be scheduled

in UI, Airflow should sort by run_after  by default

question 3 https://cwiki.apache.org/confluence/display/AIRFLOW/Option+2+clarification+doc+WIP

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2025-01-28 17:30:56+00:00,['uranusjr'],2025-02-03 10:47:17+00:00,2025-02-03 10:47:16+00:00,https://github.com/apache/airflow/issues/46195,"[('kind:meta', 'High-level information important to the community'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2627267245, 'issue_id': 2816323127, 'author': 'sunank200', 'body': 'Handled in [PR 45732](https://github.com/apache/airflow/pull/45732)', 'created_at': datetime.datetime(2025, 1, 31, 12, 59, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2630589757, 'issue_id': 2816323127, 'author': 'uranusjr', 'body': 'Done', 'created_at': datetime.datetime(2025, 2, 3, 10, 47, 16, tzinfo=datetime.timezone.utc)}]","sunank200 on (2025-01-31 12:59:42 UTC): Handled in [PR 45732](https://github.com/apache/airflow/pull/45732)

uranusjr (Assginee) on (2025-02-03 10:47:16 UTC): Done

"
2816319474,issue,open,,"AIP-83 2.c. For asset-triggered dags, using logical date in a template will result in key error","### Body

For asset-triggered dags, using logical date in a template will result in key error

(qeustion 2.c. https://cwiki.apache.org/confluence/display/AIRFLOW/Option+2+clarification+doc+WIP)

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2025-01-28 17:29:44+00:00,['Lee-W'],2025-02-03 06:37:20+00:00,,https://github.com/apache/airflow/issues/46194,"[('kind:bug', 'This is a clearly a bug'), ('kind:meta', 'High-level information important to the community'), ('area:datasets', 'Issues related to the datasets feature')]",[],
2816318332,issue,open,,AIP-83 2.b. Dags which are a schedule + asset triggered will have no logical date or data interval),"### Body

Dags which are a schedule + asset triggered will have the same behavior as just asset-triggered (i.e. no logical date or data interval)

question 2.b. https://cwiki.apache.org/confluence/display/AIRFLOW/Option+2+clarification+doc+WIP



### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2025-01-28 17:29:16+00:00,['Lee-W'],2025-02-06 05:49:54+00:00,,https://github.com/apache/airflow/issues/46193,"[('kind:meta', 'High-level information important to the community'), ('area:datasets', 'Issues related to the datasets feature')]","[{'comment_id': 2636352299, 'issue_id': 2816318332, 'author': 'Lee-W', 'body': 'hey @dstandish , would like to confirm: does it mean a dag with the following structure should never have logical date and data interval? Or just the dag runs that are triggered by assets?\n\n```python\nfrom airflow.timetables.datasets import DatasetOrTimeSchedule\nfrom airflow.timetables.trigger import CronTriggerTimetable\n\n\n@dag(\n    schedule=DatasetOrTimeSchedule(\n        timetable=CronTriggerTimetable(""0 1 * * 3"", timezone=""UTC""), datasets=(dag1_dataset & dag2_dataset)\n    )\n    # Additional arguments here, replace this comment with actual arguments\n)\ndef example_dag():\n    # DAG tasks go here\n    pass\n```', 'created_at': datetime.datetime(2025, 2, 5, 10, 35, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2638901038, 'issue_id': 2816318332, 'author': 'dstandish', 'body': '@Lee-W please confirm with @uranusjr  -- i think he was of the opinion it should be no logical date and no data interval. i did not have an opinion about it.', 'created_at': datetime.datetime(2025, 2, 6, 5, 49, 53, tzinfo=datetime.timezone.utc)}]","Lee-W (Assginee) on (2025-02-05 10:35:13 UTC): hey @dstandish , would like to confirm: does it mean a dag with the following structure should never have logical date and data interval? Or just the dag runs that are triggered by assets?

```python
from airflow.timetables.datasets import DatasetOrTimeSchedule
from airflow.timetables.trigger import CronTriggerTimetable


@dag(
    schedule=DatasetOrTimeSchedule(
        timetable=CronTriggerTimetable(""0 1 * * 3"", timezone=""UTC""), datasets=(dag1_dataset & dag2_dataset)
    )
    # Additional arguments here, replace this comment with actual arguments
)
def example_dag():
    # DAG tasks go here
    pass
```

dstandish (Issue Creator) on (2025-02-06 05:49:53 UTC): @Lee-W please confirm with @uranusjr  -- i think he was of the opinion it should be no logical date and no data interval. i did not have an opinion about it.

"
2816315223,issue,open,,AIP-83 2.a. Asset-triggered dag runs will not have a logical date or a data interval,"### Body

Asset-triggered dag runs will not have a logical date or a data interval

(question 2.a. https://cwiki.apache.org/confluence/display/AIRFLOW/Option+2+clarification+doc+WIP)

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2025-01-28 17:28:12+00:00,['Lee-W'],2025-02-03 06:38:44+00:00,,https://github.com/apache/airflow/issues/46192,"[('kind:meta', 'High-level information important to the community'), ('area:datasets', 'Issues related to the datasets feature')]",[],
2816312861,issue,open,,AIP-83 cli dag run trigger default null,"### Body

CLI dag run trigger will be default none, but you can supply a logical date if you like.

(question 1d https://cwiki.apache.org/confluence/display/AIRFLOW/Option+2+clarification+doc+WIP)

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2025-01-28 17:27:17+00:00,['uranusjr'],2025-02-04 16:11:23+00:00,,https://github.com/apache/airflow/issues/46191,"[('area:CLI', ''), ('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2620734041, 'issue_id': 2816312861, 'author': 'Prab-27', 'body': ""@dstandish , I'd like to be involved in AIP-83 by contributing. I can work on this issue. Could you please clarify? One can use a logical date if they want, and otherwise enter none in the remote DAG `dag trigger` command or `dag run trigger`"", 'created_at': datetime.datetime(2025, 1, 29, 5, 10, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2634435644, 'issue_id': 2816312861, 'author': 'phanikumv', 'body': '@uranusjr already created a PR for it #46407', 'created_at': datetime.datetime(2025, 2, 4, 16, 11, 22, tzinfo=datetime.timezone.utc)}]","Prab-27 on (2025-01-29 05:10:21 UTC): @dstandish , I'd like to be involved in AIP-83 by contributing. I can work on this issue. Could you please clarify? One can use a logical date if they want, and otherwise enter none in the remote DAG `dag trigger` command or `dag run trigger`

phanikumv on (2025-02-04 16:11:22 UTC): @uranusjr already created a PR for it #46407

"
2816310956,issue,open,,AIP-83 trigger dag run operator default null,"### Body

TriggerDagRunOperator will have a default of None for logical date, but user may supply

(question 1c https://cwiki.apache.org/confluence/display/AIRFLOW/Option+2+clarification+doc+WIP)

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2025-01-28 17:26:38+00:00,['vatsrahul1001'],2025-01-29 10:20:06+00:00,,https://github.com/apache/airflow/issues/46190,"[('kind:meta', 'High-level information important to the community'), ('area:core-operators', 'Operators, Sensors and hooks within Core Airflow')]",[],
2816306603,issue,open,,"AIP-83 when triggering run via new UI, default null, but give a date picker","### Body

In new UI, when triggering a run, it will be default None but you can select a date with date picker

reference: https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=337676722#Option2clarificationdocWIP-Resolutionofquestions


### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2025-01-28 17:24:28+00:00,"['uranusjr', 'pierrejeambrun']",2025-02-05 14:04:42+00:00,,https://github.com/apache/airflow/issues/46189,"[('kind:feature', 'Feature Requests'), ('kind:meta', 'High-level information important to the community'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]",[],
2816301874,issue,open,,AIP-83 Logical date should be required field when triggering run via API,"### Body

Logical date should be required field when triggering run via API

this is question 1b here https://cwiki.apache.org/confluence/display/AIRFLOW/Option+2+clarification+doc+WIP

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2025-01-28 17:22:13+00:00,['vatsrahul1001'],2025-02-05 12:52:48+00:00,,https://github.com/apache/airflow/issues/46188,"[('area:API', ""Airflow's REST/HTTP API""), ('good first issue', ''), ('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2631888734, 'issue_id': 2816301874, 'author': 'Vishnu-sai-teja', 'body': 'I can take this up', 'created_at': datetime.datetime(2025, 2, 3, 19, 29, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2636774524, 'issue_id': 2816301874, 'author': 'pierrejeambrun', 'body': 'Hello @Vishnu-sai-teja I think @vatsrahul1001 is already working on that.', 'created_at': datetime.datetime(2025, 2, 5, 12, 52, 46, tzinfo=datetime.timezone.utc)}]","Vishnu-sai-teja on (2025-02-03 19:29:57 UTC): I can take this up

pierrejeambrun on (2025-02-05 12:52:46 UTC): Hello @Vishnu-sai-teja I think @vatsrahul1001 is already working on that.

"
2816298461,issue,closed,completed,AIP-83 Restore uniqueness constraint on logical date and make logical date nullable,"If there is a migration that drops the uniqueness constraint, don't add another migration just edit that one.
",dstandish,2025-01-28 17:20:48+00:00,['vatsrahul1001'],2025-02-07 01:53:07+00:00,2025-02-07 01:53:05+00:00,https://github.com/apache/airflow/issues/46187,"[('area:db-migrations', 'PRs with DB migration')]","[{'comment_id': 2639329070, 'issue_id': 2816298461, 'author': 'raphaelauv', 'body': 'Hi , why do you restore uniqueness constraint on the logical_date ? thanks', 'created_at': datetime.datetime(2025, 2, 6, 9, 50, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2639491246, 'issue_id': 2816298461, 'author': 'potiuk', 'body': '> Hi , why do you restore uniqueness constraint on the logical_date ? thanks\n\nThat would take a lot to explain in a single comment but discussion here: https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-83+amendment+to+support+classic+Airflow+authoring+style and https://cwiki.apache.org/confluence/display/AIRFLOW/Option+2+clarification+doc+WIP and related discussions linked in those docs.\n\nIt took us several weeks of discussions and I think three dev calls to come to this conclusion so I think the only way to understand ""why"" is to read all those discussions to get the full contetxt @raphaelauv', 'created_at': datetime.datetime(2025, 2, 6, 10, 57, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2639677679, 'issue_id': 2816298461, 'author': 'raphaelauv', 'body': 'thanks for the context @potiuk', 'created_at': datetime.datetime(2025, 2, 6, 12, 21, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641740153, 'issue_id': 2816298461, 'author': 'Lee-W', 'body': 'Close as https://github.com/apache/airflow/pull/46295 is merged', 'created_at': datetime.datetime(2025, 2, 7, 1, 53, 5, tzinfo=datetime.timezone.utc)}]","raphaelauv on (2025-02-06 09:50:50 UTC): Hi , why do you restore uniqueness constraint on the logical_date ? thanks

potiuk on (2025-02-06 10:57:40 UTC): That would take a lot to explain in a single comment but discussion here: https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-83+amendment+to+support+classic+Airflow+authoring+style and https://cwiki.apache.org/confluence/display/AIRFLOW/Option+2+clarification+doc+WIP and related discussions linked in those docs.

It took us several weeks of discussions and I think three dev calls to come to this conclusion so I think the only way to understand ""why"" is to read all those discussions to get the full contetxt @raphaelauv

raphaelauv on (2025-02-06 12:21:21 UTC): thanks for the context @potiuk

Lee-W on (2025-02-07 01:53:05 UTC): Close as https://github.com/apache/airflow/pull/46295 is merged

"
2815162441,issue,closed,not_planned,Support more Database Backends,"### Description

According to the [docs](https://airflow.apache.org/docs/apache-airflow/stable/howto/set-up-database.html#choosing-database-backend) the only supported Database backends are SQLite, MySQL and Postgres, and SQLite is not meant for production.

Some companies have already setup Infrastructures, possibly using other database systems, so this limitation makes it harder to integrate Airflow since it requires the installation and integration of either MySQL or Postgres.

For example, in our company we use Oracle.
Even though the installation of an extra DB is possible, it is a measurable overhead and will involve multiple teams.

I think the flexibility to choose between more DB systems would be a relief to many users.
Example besides my case: https://stackoverflow.com/questions/61195252/airflow-with-oracle-backend

### Use case/motivation

We have an Oracle DB. In order to use airflow we need to setup a second DB. It would be awsome if we did not have to do that.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!
Would happily do, but unfortunatelly my python knowledge is very limited.

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",alkismavridis,2025-01-28 10:00:45+00:00,[],2025-01-29 16:47:32+00:00,2025-01-29 16:47:32+00:00,https://github.com/apache/airflow/issues/46175,"[('area:MetaDB', 'Meta Database related issues.'), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2618513383, 'issue_id': 2815162441, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 28, 10, 0, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2618680131, 'issue_id': 2815162441, 'author': 'eladkal', 'body': 'Not going to happen. We had negative experience with MsSQL and we are not going to expand on other databases.', 'created_at': datetime.datetime(2025, 1, 28, 11, 5, 1, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-28 10:00:47 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2025-01-28 11:05:01 UTC): Not going to happen. We had negative experience with MsSQL and we are not going to expand on other databases.

"
2815016677,issue,closed,completed,sqlalchemy 2 set in a virtualenv breaks variable access,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

_Note: just adding this to be tracked for future reference (I haven't seen this specific issue with `sqlalchemy` 2 in the roadmap toward airflow 3). If you feel this is redondant, please just close this issue._

Running airflow 2.10.4 from the docker image with python 3.10.

Setting a virtualenv config with ""sqlalchemy>2.0.0"" breaks Variable retrieval (but otherwise works fine).

### What you think should happen instead?

Up to now, I had'nt seen side effects linked to using sqlalchemy 2 in virtualenv (that's even something recommented on some SO's answers like [here](https://stackoverflow.com/questions/76365797/how-do-i-get-airflow-to-work-with-sqlalchemy-2-0-2-when-it-has-a-1-4-48-version)). Maybe that's just something that needs to be displayed explicitly until airflow 3 is out?

### How to reproduce

Dag example:

```python
from airflow.decorators import dag, task


@dag()
def sqlalchemy_test():

    from airflow.models import Variable

    Variable.set(""eggs"", ""spam"")

    @task
    def get_variable():
        from airflow.models import Variable

        print(Variable.get(""eggs""))

    @task.virtualenv(requirements=[""sqlalchemy>2.0.0""])
    def get_variable2():
        from airflow.models import Variable

        print(Variable.get(""eggs""))

    get_variable()
    get_variable2()


sqlalchemy_test()

```

### Operating System

Ubuntu 22.04.3 LTS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

Docker image derived from apache/airflow:2.10.4-python3.10

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",tgrandje,2025-01-28 08:58:50+00:00,[],2025-01-29 23:42:40+00:00,2025-01-29 23:42:40+00:00,https://github.com/apache/airflow/issues/46173,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:dependencies', 'Issues related to dependencies problems'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2618340892, 'issue_id': 2815016677, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 28, 8, 58, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2618391355, 'issue_id': 2815016677, 'author': 'eladkal', 'body': 'Please share also traceback so we can see the error', 'created_at': datetime.datetime(2025, 1, 28, 9, 18, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2618402222, 'issue_id': 2815016677, 'author': 'eladkal', 'body': ""Ah OK I get your issue. it's not about virtual env it's about variables.\n\nYou are doing it wrong. You can not do `Variable.get()` inside virtual env. You should use `op_args` / `op_kwargs` and pass the variable to the environment using Jinja macro `{{ var.value.<variable_name> }}`"", 'created_at': datetime.datetime(2025, 1, 28, 9, 23, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2618405466, 'issue_id': 2815016677, 'author': 'tgrandje', 'body': 'get_variable2\'s full log. The first errors seems to be linked to the usage of sqlalchemy 2 (failed to import plugin openlinage and the like). I\'m not sure there\'s much usable traceback in the final error, though...\n\n```\n[2025-01-28, 10:20:20 CET] {local_task_job_runner.py:123} ▶ Pre task execution logs\n[2025-01-28, 10:20:20 CET] {process_utils.py:186} INFO - Executing cmd: /home/airflow/.local/bin/python -m virtualenv /tmp/venvjh12i7vg --system-site-packages --python=python\n[2025-01-28, 10:20:20 CET] {process_utils.py:190} INFO - Output:\n[2025-01-28, 10:20:21 CET] {process_utils.py:194} INFO - created virtual environment CPython3.10.16.final.0-64 in 214ms\n[2025-01-28, 10:20:21 CET] {process_utils.py:194} INFO -   creator CPython3Posix(dest=/tmp/venvjh12i7vg, clear=False, no_vcs_ignore=False, global=True)\n[2025-01-28, 10:20:21 CET] {process_utils.py:194} INFO -   seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/home/airflow/.local/share/virtualenv)\n[2025-01-28, 10:20:21 CET] {process_utils.py:194} INFO -     added seed packages: pip==24.3.1, setuptools==75.6.0, wheel==0.45.1\n[2025-01-28, 10:20:21 CET] {process_utils.py:194} INFO -   activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n[2025-01-28, 10:20:21 CET] {process_utils.py:186} INFO - Executing cmd: /tmp/venvjh12i7vg/bin/pip install -r /tmp/venvjh12i7vg/requirements.txt\n[2025-01-28, 10:20:21 CET] {process_utils.py:190} INFO - Output:\n[2025-01-28, 10:20:22 CET] {process_utils.py:194} INFO - Collecting sqlalchemy>2.0.0 (from -r /tmp/venvjh12i7vg/requirements.txt (line 1))\n[2025-01-28, 10:20:22 CET] {process_utils.py:194} INFO -   Using cached SQLAlchemy-2.0.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n[2025-01-28, 10:20:22 CET] {process_utils.py:194} INFO - Requirement already satisfied: greenlet!=0.4.17 in /home/airflow/.local/lib/python3.10/site-packages (from sqlalchemy>2.0.0->-r /tmp/venvjh12i7vg/requirements.txt (line 1)) (3.1.1)\n[2025-01-28, 10:20:22 CET] {process_utils.py:194} INFO - Requirement already satisfied: typing-extensions>=4.6.0 in /home/airflow/.local/lib/python3.10/site-packages (from sqlalchemy>2.0.0->-r /tmp/venvjh12i7vg/requirements.txt (line 1)) (4.12.2)\n[2025-01-28, 10:20:22 CET] {process_utils.py:194} INFO - Using cached SQLAlchemy-2.0.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n[2025-01-28, 10:20:23 CET] {process_utils.py:194} INFO - Installing collected packages: sqlalchemy\n[2025-01-28, 10:20:23 CET] {process_utils.py:194} INFO -   Attempting uninstall: sqlalchemy\n[2025-01-28, 10:20:23 CET] {process_utils.py:194} INFO -     Found existing installation: SQLAlchemy 1.4.54\n[2025-01-28, 10:20:23 CET] {process_utils.py:194} INFO -     Not uninstalling sqlalchemy at /home/airflow/.local/lib/python3.10/site-packages, outside environment /tmp/venvjh12i7vg\n[2025-01-28, 10:20:23 CET] {process_utils.py:194} INFO -     Can\'t uninstall \'SQLAlchemy\'. No files were found to uninstall.\n[2025-01-28, 10:20:24 CET] {process_utils.py:194} INFO - ERROR: pip\'s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n[2025-01-28, 10:20:24 CET] {process_utils.py:194} INFO - flask-appbuilder 4.5.2 requires SQLAlchemy<1.5, but you have sqlalchemy 2.0.37 which is incompatible.\n[2025-01-28, 10:20:24 CET] {process_utils.py:194} INFO - apache-airflow 2.10.4 requires sqlalchemy<2.0,>=1.4.36, but you have sqlalchemy 2.0.37 which is incompatible.\n[2025-01-28, 10:20:24 CET] {process_utils.py:194} INFO - marshmallow-sqlalchemy 0.28.2 requires SQLAlchemy<2.0,>=1.3.0, but you have sqlalchemy 2.0.37 which is incompatible.\n[2025-01-28, 10:20:24 CET] {process_utils.py:194} INFO - Successfully installed sqlalchemy-2.0.37\n[2025-01-28, 10:20:24 CET] {process_utils.py:194} INFO - \n[2025-01-28, 10:20:24 CET] {process_utils.py:194} INFO - [notice] A new release of pip is available: 24.3.1 -> 25.0\n[2025-01-28, 10:20:24 CET] {process_utils.py:194} INFO - [notice] To update, run: python -m pip install --upgrade pip\n[2025-01-28, 10:20:24 CET] {process_utils.py:186} INFO - Executing cmd: /tmp/venvjh12i7vg/bin/python /tmp/venv-calld1zaldnp/script.py /tmp/venv-calld1zaldnp/script.in /tmp/venv-calld1zaldnp/script.out /tmp/venv-calld1zaldnp/string_args.txt /tmp/venv-calld1zaldnp/termination.log\n[2025-01-28, 10:20:24 CET] {process_utils.py:190} INFO - Output:\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO - [2025-01-28T10:20:25.517+0100] {plugins_manager.py:266} ERROR - Failed to import plugin openlineage\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO - Traceback (most recent call last):\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/plugins_manager.py"", line 258, in load_entrypoint_plugins\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     plugin_class = entry_point.load()\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/importlib_metadata/__init__.py"", line 211, in load\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     module = import_module(match.group(\'module\'))\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/usr/local/lib/python3.10/importlib/__init__.py"", line 126, in import_module\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return _bootstrap._gcd_import(name[level:], package, level)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap>"", line 1050, in _gcd_import\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap>"", line 1006, in _find_and_load_unlocked\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap>"", line 688, in _load_unlocked\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap_external>"", line 883, in exec_module\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/openlineage/plugins/openlineage.py"", line 21, in <module>\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     from airflow.providers.openlineage.plugins.listener import get_openlineage_listener\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/openlineage/plugins/listener.py"", line 30, in <module>\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     from airflow.models import DagRun\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap>"", line 1075, in _handle_fromlist\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/__init__.py"", line 79, in __getattr__\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     val = import_string(f""{path}.{name}"")\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/module_loading.py"", line 39, in import_string\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     module = import_module(module_path)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/usr/local/lib/python3.10/importlib/__init__.py"", line 126, in import_module\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return _bootstrap._gcd_import(name[level:], package, level)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagrun.py"", line 60, in <module>\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     from airflow.models.taskinstance import TaskInstance as TI\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 1799, in <module>\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     class TaskInstance(Base, LoggingMixin):\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/decl_api.py"", line 198, in __init__\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     _as_declarative(reg, cls, dict_)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/decl_base.py"", line 244, in _as_declarative\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return _MapperConfig.setup_mapping(registry, cls, dict_, None, {})\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/decl_base.py"", line 325, in setup_mapping\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return _ClassScanMapperConfig(\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/decl_base.py"", line 561, in __init__\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     self._scan_attributes()\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/decl_base.py"", line 1021, in _scan_attributes\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     collected_annotation = self._collect_annotation(\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/decl_base.py"", line 1303, in _collect_annotation\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     extracted = _extract_mapped_subtype(\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/util.py"", line 2365, in _extract_mapped_subtype\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     raise sa_exc.ArgumentError(\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO - sqlalchemy.exc.ArgumentError: Type annotation for ""TaskInstance.dag_model"" can\'t be correctly interpreted for Annotated Declarative Table form.  ORM annotations should normally make use of the ``Mapped[]`` generic type, or other ORM-compatible generic type, as a container for the actual type, which indicates the intent that the attribute is mapped. Class variables that are not intended to be mapped by the ORM should use ClassVar[].  To allow Annotated Declarative to disregard legacy annotations which don\'t use Mapped[] to pass, set ""__allow_unmapped__ = True"" on the class or a superclass this class. (Background on this error at: https://sqlalche.me/e/20/zlpr)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO - [2025-01-28T10:20:25.607+0100] {variable.py:357} ERROR - Unable to retrieve variable from secrets backend (MetastoreBackend). Checking subsequent secrets backend.\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO - Traceback (most recent call last):\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/variable.py"", line 353, in get_variable_from_secrets\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     var_val = secrets_backend.get_variable(key=key)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py"", line 97, in wrapper\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return func(*args, session=session, **kwargs)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/secrets/metastore.py"", line 66, in get_variable\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return MetastoreBackend._fetch_variable(key=key, session=session)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py"", line 166, in wrapper\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return func(*args, **kwargs)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py"", line 94, in wrapper\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return func(*args, **kwargs)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/secrets/metastore.py"", line 84, in _fetch_variable\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     var_value = session.scalar(select(Variable).where(Variable.key == key).limit(1))\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 2413, in scalar\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return self._execute_internal(\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 2251, in _execute_internal\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     result: Result[Any] = compile_state_cls.orm_execute_statement(\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/context.py"", line 305, in orm_execute_statement\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     result = conn.execute(\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 1416, in execute\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return meth(\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/sql/elements.py"", line 515, in _execute_on_connection\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return connection._execute_clauseelement(\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 1630, in _execute_clauseelement\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     compiled_sql, extracted_params, cache_hit = elem._compile_w_cache(\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/sql/elements.py"", line 703, in _compile_w_cache\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     compiled_sql = self._compiler(\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/sql/elements.py"", line 316, in _compiler\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return dialect.statement_compiler(dialect, self, **kw)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/sql/compiler.py"", line 1429, in __init__\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     Compiled.__init__(self, dialect, statement, **kwargs)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/sql/compiler.py"", line 870, in __init__\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     self.string = self.process(self.statement, **compile_kwargs)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/sql/compiler.py"", line 915, in process\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return obj._compiler_dispatch(self, **kwargs)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/sql/visitors.py"", line 141, in _compiler_dispatch\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return meth(self, **kw)  # type: ignore  # noqa: E501\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/sql/compiler.py"", line 4680, in visit_select\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     compile_state = select_stmt._compile_state_factory(\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/sql/base.py"", line 683, in create_for_statement\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return klass.create_for_statement(statement, compiler, **kw)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/context.py"", line 1110, in create_for_statement\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     _QueryEntity.to_compile_state(\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/context.py"", line 2565, in to_compile_state\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     _MapperEntity(\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/context.py"", line 2645, in __init__\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     entity._post_inspect\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 1257, in __get__\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     obj.__dict__[self.__name__] = result = self.fget(obj)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/mapper.py"", line 2724, in _post_inspect\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     self._check_configure()\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/mapper.py"", line 2401, in _check_configure\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     _configure_registries({self.registry}, cascade=True)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/mapper.py"", line 4207, in _configure_registries\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     Mapper.dispatch._for_class(Mapper).before_configured()  # type: ignore # noqa: E501\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/event/attr.py"", line 378, in __call__\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     fn(*args, **kw)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/events.py"", line 893, in wrap\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     fn(*arg, **kw)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 1916, in go\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return once_fn(*arg, **kw)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/__init__.py"", line 60, in import_all_models\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     __getattr__(name)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/__init__.py"", line 79, in __getattr__\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     val = import_string(f""{path}.{name}"")\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/module_loading.py"", line 39, in import_string\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     module = import_module(module_path)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/usr/local/lib/python3.10/importlib/__init__.py"", line 126, in import_module\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return _bootstrap._gcd_import(name[level:], package, level)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap>"", line 1050, in _gcd_import\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap>"", line 1006, in _find_and_load_unlocked\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap>"", line 688, in _load_unlocked\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap_external>"", line 883, in exec_module\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dag.py"", line 103, in <module>\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     from airflow.models.baseoperator import BaseOperator\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py"", line 83, in <module>\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     from airflow.models.mappedoperator import OperatorPartial, validate_mapping_kwargs\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/mappedoperator.py"", line 54, in <module>\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     from airflow.triggers.base import StartTriggerArgs\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/triggers/base.py"", line 27, in <module>\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     from airflow.models.taskinstance import SimpleTaskInstance\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 1799, in <module>\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     class TaskInstance(Base, LoggingMixin):\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/decl_api.py"", line 198, in __init__\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     _as_declarative(reg, cls, dict_)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/decl_base.py"", line 244, in _as_declarative\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return _MapperConfig.setup_mapping(registry, cls, dict_, None, {})\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/decl_base.py"", line 325, in setup_mapping\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return _ClassScanMapperConfig(\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/decl_base.py"", line 561, in __init__\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     self._scan_attributes()\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/decl_base.py"", line 1021, in _scan_attributes\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     collected_annotation = self._collect_annotation(\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/decl_base.py"", line 1303, in _collect_annotation\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     extracted = _extract_mapped_subtype(\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/util.py"", line 2365, in _extract_mapped_subtype\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     raise sa_exc.ArgumentError(\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO - sqlalchemy.exc.ArgumentError: Type annotation for ""TaskInstance.dag_model"" can\'t be correctly interpreted for Annotated Declarative Table form.  ORM annotations should normally make use of the ``Mapped[]`` generic type, or other ORM-compatible generic type, as a container for the actual type, which indicates the intent that the attribute is mapped. Class variables that are not intended to be mapped by the ORM should use ClassVar[].  To allow Annotated Declarative to disregard legacy annotations which don\'t use Mapped[] to pass, set ""__allow_unmapped__ = True"" on the class or a superclass this class. (Background on this error at: https://sqlalche.me/e/20/zlpr)\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO - Traceback (most recent call last):\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venv-calld1zaldnp/script.py"", line 48, in <module>\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     res = get_variable2(*arg_dict[""args""], **arg_dict[""kwargs""])\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venv-calld1zaldnp/script.py"", line 21, in get_variable2\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     print(Variable.get(""eggs""))\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/variable.py"", line 145, in get\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     raise KeyError(f""Variable {key} does not exist"")\n[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO - KeyError: \'Variable eggs does not exist\'\n[2025-01-28, 10:20:25 CET] {taskinstance.py:3311} ERROR - Task failed with exception\nTraceback (most recent call last):\n  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 767, in _execute_task\n    result = _execute_callable(context=context, **execute_callable_kwargs)\n  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 733, in _execute_callable\n    return ExecutionCallableRunner(\n  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py"", line 252, in run\n    return self.func(*args, **kwargs)\n  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py"", line 422, in wrapper\n    return func(self, *args, **kwargs)\n  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/decorators/base.py"", line 266, in execute\n    return_value = super().execute(context)\n  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py"", line 422, in wrapper\n    return func(self, *args, **kwargs)\n  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py"", line 505, in execute\n    return super().execute(context=serializable_context)\n  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py"", line 422, in wrapper\n    return func(self, *args, **kwargs)\n  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py"", line 238, in execute\n    return_value = self.execute_callable()\n  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py"", line 870, in execute_callable\n    result = self._execute_python_callable_in_subprocess(python_path)\n  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py"", line 588, in _execute_python_callable_\n```in_subprocess\n    raise AirflowException(error_msg) from None\nairflow.exceptions.AirflowException: Process returned non-zero exit status 1.\n\'Variable eggs does not exist\'\n[2025-01-28, 10:20:25 CET] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=sqlalchemy_test, task_id=get_variable2, run_id=manual__2025-01-28T09:20:19.722028+00:00, execution_date=20250128T092019, start_date=20250128T092020, end_date=20250128T092025\n[2025-01-28, 10:20:25 CET] {taskinstance.py:340} ▶ Post task execution logs', 'created_at': datetime.datetime(2025, 1, 28, 9, 24, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2618413961, 'issue_id': 2815016677, 'author': 'tgrandje', 'body': '> Ah OK I get your issue. it\'s not about virtual env it\'s about variables.\n> \n> You are doing it wrong. You can not do `Variable.get()` inside virtual env. You should use `op_args` / `op_kwargs` and pass the variable to the environment using Jinja macro `{{ var.value.<variable_name> }}`\n\n@eladkal You sure about that? I hadn\'t had any trouble up to now. For instance, that dag will run get_variable3 just fine:\n```\nfrom airflow.decorators import dag, task\n\n\n@dag()\ndef sqlalchemy_test():\n\n    from airflow.models import Variable\n\n    Variable.set(""eggs"", ""spam"")\n\n    @task\n    def get_variable():\n        from airflow.models import Variable\n\n        print(Variable.get(""eggs""))\n\n    @task.virtualenv(requirements=[""sqlalchemy>2.0.0""])\n    def get_variable2():\n        from airflow.models import Variable\n\n        print(Variable.get(""eggs""))\n\n    @task.virtualenv(requirements=[""geopandas""])\n    def get_variable3():\n        from airflow.models import Variable\n\n        print(Variable.get(""eggs""))\n\n    get_variable()\n    get_variable2()\n    get_variable3()\n\n\nsqlalchemy_test()\n```', 'created_at': datetime.datetime(2025, 1, 28, 9, 28, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2621252344, 'issue_id': 2815016677, 'author': 'laksh-krishna-sharma', 'body': 'Hello sir may i work on this issue with a bit of your guidance', 'created_at': datetime.datetime(2025, 1, 29, 10, 31, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2621454029, 'issue_id': 2815016677, 'author': 'tgrandje', 'body': '@laksh-krishna-sharma Sure! Do you need anything beside what I had already posted?', 'created_at': datetime.datetime(2025, 1, 29, 12, 2, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2623179641, 'issue_id': 2815016677, 'author': 'potiuk', 'body': ""> apache-airflow 2.10.4 requires sqlalchemy<2.0,>=1.4.36, but you have sqlalchemy 2.0.37 which is incompatible. \n\nThis is clear I think. Aoache Airflow does not work with sqlalchemy > 2.0 - mainly because some of the dependencies do not work with it. \n\nI thin it's clear because airflow has sqlalchemy < 2.0 as requirement."", 'created_at': datetime.datetime(2025, 1, 29, 23, 42, 32, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-28 08:58:53 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2025-01-28 09:18:07 UTC): Please share also traceback so we can see the error

eladkal on (2025-01-28 09:23:20 UTC): Ah OK I get your issue. it's not about virtual env it's about variables.

You are doing it wrong. You can not do `Variable.get()` inside virtual env. You should use `op_args` / `op_kwargs` and pass the variable to the environment using Jinja macro `{{ var.value.<variable_name> }}`

tgrandje (Issue Creator) on (2025-01-28 09:24:48 UTC): get_variable2's full log. The first errors seems to be linked to the usage of sqlalchemy 2 (failed to import plugin openlinage and the like). I'm not sure there's much usable traceback in the final error, though...

```
[2025-01-28, 10:20:20 CET] {local_task_job_runner.py:123} ▶ Pre task execution logs
[2025-01-28, 10:20:20 CET] {process_utils.py:186} INFO - Executing cmd: /home/airflow/.local/bin/python -m virtualenv /tmp/venvjh12i7vg --system-site-packages --python=python
[2025-01-28, 10:20:20 CET] {process_utils.py:190} INFO - Output:
[2025-01-28, 10:20:21 CET] {process_utils.py:194} INFO - created virtual environment CPython3.10.16.final.0-64 in 214ms
[2025-01-28, 10:20:21 CET] {process_utils.py:194} INFO -   creator CPython3Posix(dest=/tmp/venvjh12i7vg, clear=False, no_vcs_ignore=False, global=True)
[2025-01-28, 10:20:21 CET] {process_utils.py:194} INFO -   seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/home/airflow/.local/share/virtualenv)
[2025-01-28, 10:20:21 CET] {process_utils.py:194} INFO -     added seed packages: pip==24.3.1, setuptools==75.6.0, wheel==0.45.1
[2025-01-28, 10:20:21 CET] {process_utils.py:194} INFO -   activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator
[2025-01-28, 10:20:21 CET] {process_utils.py:186} INFO - Executing cmd: /tmp/venvjh12i7vg/bin/pip install -r /tmp/venvjh12i7vg/requirements.txt
[2025-01-28, 10:20:21 CET] {process_utils.py:190} INFO - Output:
[2025-01-28, 10:20:22 CET] {process_utils.py:194} INFO - Collecting sqlalchemy>2.0.0 (from -r /tmp/venvjh12i7vg/requirements.txt (line 1))
[2025-01-28, 10:20:22 CET] {process_utils.py:194} INFO -   Using cached SQLAlchemy-2.0.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)
[2025-01-28, 10:20:22 CET] {process_utils.py:194} INFO - Requirement already satisfied: greenlet!=0.4.17 in /home/airflow/.local/lib/python3.10/site-packages (from sqlalchemy>2.0.0->-r /tmp/venvjh12i7vg/requirements.txt (line 1)) (3.1.1)
[2025-01-28, 10:20:22 CET] {process_utils.py:194} INFO - Requirement already satisfied: typing-extensions>=4.6.0 in /home/airflow/.local/lib/python3.10/site-packages (from sqlalchemy>2.0.0->-r /tmp/venvjh12i7vg/requirements.txt (line 1)) (4.12.2)
[2025-01-28, 10:20:22 CET] {process_utils.py:194} INFO - Using cached SQLAlchemy-2.0.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)
[2025-01-28, 10:20:23 CET] {process_utils.py:194} INFO - Installing collected packages: sqlalchemy
[2025-01-28, 10:20:23 CET] {process_utils.py:194} INFO -   Attempting uninstall: sqlalchemy
[2025-01-28, 10:20:23 CET] {process_utils.py:194} INFO -     Found existing installation: SQLAlchemy 1.4.54
[2025-01-28, 10:20:23 CET] {process_utils.py:194} INFO -     Not uninstalling sqlalchemy at /home/airflow/.local/lib/python3.10/site-packages, outside environment /tmp/venvjh12i7vg
[2025-01-28, 10:20:23 CET] {process_utils.py:194} INFO -     Can't uninstall 'SQLAlchemy'. No files were found to uninstall.
[2025-01-28, 10:20:24 CET] {process_utils.py:194} INFO - ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
[2025-01-28, 10:20:24 CET] {process_utils.py:194} INFO - flask-appbuilder 4.5.2 requires SQLAlchemy<1.5, but you have sqlalchemy 2.0.37 which is incompatible.
[2025-01-28, 10:20:24 CET] {process_utils.py:194} INFO - apache-airflow 2.10.4 requires sqlalchemy<2.0,>=1.4.36, but you have sqlalchemy 2.0.37 which is incompatible.
[2025-01-28, 10:20:24 CET] {process_utils.py:194} INFO - marshmallow-sqlalchemy 0.28.2 requires SQLAlchemy<2.0,>=1.3.0, but you have sqlalchemy 2.0.37 which is incompatible.
[2025-01-28, 10:20:24 CET] {process_utils.py:194} INFO - Successfully installed sqlalchemy-2.0.37
[2025-01-28, 10:20:24 CET] {process_utils.py:194} INFO - 
[2025-01-28, 10:20:24 CET] {process_utils.py:194} INFO - [notice] A new release of pip is available: 24.3.1 -> 25.0
[2025-01-28, 10:20:24 CET] {process_utils.py:194} INFO - [notice] To update, run: python -m pip install --upgrade pip
[2025-01-28, 10:20:24 CET] {process_utils.py:186} INFO - Executing cmd: /tmp/venvjh12i7vg/bin/python /tmp/venv-calld1zaldnp/script.py /tmp/venv-calld1zaldnp/script.in /tmp/venv-calld1zaldnp/script.out /tmp/venv-calld1zaldnp/string_args.txt /tmp/venv-calld1zaldnp/termination.log
[2025-01-28, 10:20:24 CET] {process_utils.py:190} INFO - Output:
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO - [2025-01-28T10:20:25.517+0100] {plugins_manager.py:266} ERROR - Failed to import plugin openlineage
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO - Traceback (most recent call last):
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/plugins_manager.py"", line 258, in load_entrypoint_plugins
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     plugin_class = entry_point.load()
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/importlib_metadata/__init__.py"", line 211, in load
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     module = import_module(match.group('module'))
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/usr/local/lib/python3.10/importlib/__init__.py"", line 126, in import_module
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return _bootstrap._gcd_import(name[level:], package, level)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap>"", line 1050, in _gcd_import
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap>"", line 1006, in _find_and_load_unlocked
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap>"", line 688, in _load_unlocked
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap_external>"", line 883, in exec_module
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/openlineage/plugins/openlineage.py"", line 21, in <module>
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     from airflow.providers.openlineage.plugins.listener import get_openlineage_listener
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/openlineage/plugins/listener.py"", line 30, in <module>
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     from airflow.models import DagRun
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap>"", line 1075, in _handle_fromlist
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/__init__.py"", line 79, in __getattr__
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     val = import_string(f""{path}.{name}"")
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/module_loading.py"", line 39, in import_string
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     module = import_module(module_path)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/usr/local/lib/python3.10/importlib/__init__.py"", line 126, in import_module
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return _bootstrap._gcd_import(name[level:], package, level)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dagrun.py"", line 60, in <module>
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     from airflow.models.taskinstance import TaskInstance as TI
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 1799, in <module>
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     class TaskInstance(Base, LoggingMixin):
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/decl_api.py"", line 198, in __init__
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     _as_declarative(reg, cls, dict_)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/decl_base.py"", line 244, in _as_declarative
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return _MapperConfig.setup_mapping(registry, cls, dict_, None, {})
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/decl_base.py"", line 325, in setup_mapping
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return _ClassScanMapperConfig(
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/decl_base.py"", line 561, in __init__
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     self._scan_attributes()
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/decl_base.py"", line 1021, in _scan_attributes
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     collected_annotation = self._collect_annotation(
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/decl_base.py"", line 1303, in _collect_annotation
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     extracted = _extract_mapped_subtype(
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/util.py"", line 2365, in _extract_mapped_subtype
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     raise sa_exc.ArgumentError(
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO - sqlalchemy.exc.ArgumentError: Type annotation for ""TaskInstance.dag_model"" can't be correctly interpreted for Annotated Declarative Table form.  ORM annotations should normally make use of the ``Mapped[]`` generic type, or other ORM-compatible generic type, as a container for the actual type, which indicates the intent that the attribute is mapped. Class variables that are not intended to be mapped by the ORM should use ClassVar[].  To allow Annotated Declarative to disregard legacy annotations which don't use Mapped[] to pass, set ""__allow_unmapped__ = True"" on the class or a superclass this class. (Background on this error at: https://sqlalche.me/e/20/zlpr)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO - [2025-01-28T10:20:25.607+0100] {variable.py:357} ERROR - Unable to retrieve variable from secrets backend (MetastoreBackend). Checking subsequent secrets backend.
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO - Traceback (most recent call last):
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/variable.py"", line 353, in get_variable_from_secrets
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     var_val = secrets_backend.get_variable(key=key)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py"", line 97, in wrapper
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return func(*args, session=session, **kwargs)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/secrets/metastore.py"", line 66, in get_variable
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return MetastoreBackend._fetch_variable(key=key, session=session)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py"", line 166, in wrapper
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return func(*args, **kwargs)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py"", line 94, in wrapper
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return func(*args, **kwargs)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/secrets/metastore.py"", line 84, in _fetch_variable
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     var_value = session.scalar(select(Variable).where(Variable.key == key).limit(1))
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 2413, in scalar
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return self._execute_internal(
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 2251, in _execute_internal
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     result: Result[Any] = compile_state_cls.orm_execute_statement(
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/context.py"", line 305, in orm_execute_statement
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     result = conn.execute(
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 1416, in execute
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return meth(
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/sql/elements.py"", line 515, in _execute_on_connection
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return connection._execute_clauseelement(
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 1630, in _execute_clauseelement
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     compiled_sql, extracted_params, cache_hit = elem._compile_w_cache(
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/sql/elements.py"", line 703, in _compile_w_cache
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     compiled_sql = self._compiler(
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/sql/elements.py"", line 316, in _compiler
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return dialect.statement_compiler(dialect, self, **kw)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/sql/compiler.py"", line 1429, in __init__
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     Compiled.__init__(self, dialect, statement, **kwargs)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/sql/compiler.py"", line 870, in __init__
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     self.string = self.process(self.statement, **compile_kwargs)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/sql/compiler.py"", line 915, in process
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return obj._compiler_dispatch(self, **kwargs)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/sql/visitors.py"", line 141, in _compiler_dispatch
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return meth(self, **kw)  # type: ignore  # noqa: E501
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/sql/compiler.py"", line 4680, in visit_select
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     compile_state = select_stmt._compile_state_factory(
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/sql/base.py"", line 683, in create_for_statement
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return klass.create_for_statement(statement, compiler, **kw)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/context.py"", line 1110, in create_for_statement
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     _QueryEntity.to_compile_state(
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/context.py"", line 2565, in to_compile_state
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     _MapperEntity(
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/context.py"", line 2645, in __init__
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     entity._post_inspect
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 1257, in __get__
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     obj.__dict__[self.__name__] = result = self.fget(obj)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/mapper.py"", line 2724, in _post_inspect
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     self._check_configure()
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/mapper.py"", line 2401, in _check_configure
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     _configure_registries({self.registry}, cascade=True)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/mapper.py"", line 4207, in _configure_registries
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     Mapper.dispatch._for_class(Mapper).before_configured()  # type: ignore # noqa: E501
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/event/attr.py"", line 378, in __call__
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     fn(*args, **kw)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/events.py"", line 893, in wrap
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     fn(*arg, **kw)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 1916, in go
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return once_fn(*arg, **kw)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/__init__.py"", line 60, in import_all_models
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     __getattr__(name)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/__init__.py"", line 79, in __getattr__
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     val = import_string(f""{path}.{name}"")
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/module_loading.py"", line 39, in import_string
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     module = import_module(module_path)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/usr/local/lib/python3.10/importlib/__init__.py"", line 126, in import_module
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return _bootstrap._gcd_import(name[level:], package, level)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap>"", line 1050, in _gcd_import
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap>"", line 1006, in _find_and_load_unlocked
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap>"", line 688, in _load_unlocked
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap_external>"", line 883, in exec_module
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/dag.py"", line 103, in <module>
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     from airflow.models.baseoperator import BaseOperator
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py"", line 83, in <module>
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     from airflow.models.mappedoperator import OperatorPartial, validate_mapping_kwargs
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/mappedoperator.py"", line 54, in <module>
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     from airflow.triggers.base import StartTriggerArgs
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/triggers/base.py"", line 27, in <module>
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     from airflow.models.taskinstance import SimpleTaskInstance
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 1799, in <module>
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     class TaskInstance(Base, LoggingMixin):
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/decl_api.py"", line 198, in __init__
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     _as_declarative(reg, cls, dict_)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/decl_base.py"", line 244, in _as_declarative
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return _MapperConfig.setup_mapping(registry, cls, dict_, None, {})
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/decl_base.py"", line 325, in setup_mapping
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     return _ClassScanMapperConfig(
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/decl_base.py"", line 561, in __init__
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     self._scan_attributes()
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/decl_base.py"", line 1021, in _scan_attributes
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     collected_annotation = self._collect_annotation(
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/decl_base.py"", line 1303, in _collect_annotation
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     extracted = _extract_mapped_subtype(
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venvjh12i7vg/lib/python3.10/site-packages/sqlalchemy/orm/util.py"", line 2365, in _extract_mapped_subtype
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     raise sa_exc.ArgumentError(
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO - sqlalchemy.exc.ArgumentError: Type annotation for ""TaskInstance.dag_model"" can't be correctly interpreted for Annotated Declarative Table form.  ORM annotations should normally make use of the ``Mapped[]`` generic type, or other ORM-compatible generic type, as a container for the actual type, which indicates the intent that the attribute is mapped. Class variables that are not intended to be mapped by the ORM should use ClassVar[].  To allow Annotated Declarative to disregard legacy annotations which don't use Mapped[] to pass, set ""__allow_unmapped__ = True"" on the class or a superclass this class. (Background on this error at: https://sqlalche.me/e/20/zlpr)
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO - Traceback (most recent call last):
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venv-calld1zaldnp/script.py"", line 48, in <module>
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     res = get_variable2(*arg_dict[""args""], **arg_dict[""kwargs""])
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/tmp/venv-calld1zaldnp/script.py"", line 21, in get_variable2
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     print(Variable.get(""eggs""))
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -   File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/variable.py"", line 145, in get
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO -     raise KeyError(f""Variable {key} does not exist"")
[2025-01-28, 10:20:25 CET] {process_utils.py:194} INFO - KeyError: 'Variable eggs does not exist'
[2025-01-28, 10:20:25 CET] {taskinstance.py:3311} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 733, in _execute_callable
    return ExecutionCallableRunner(
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/operator_helpers.py"", line 252, in run
    return self.func(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py"", line 422, in wrapper
    return func(self, *args, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/decorators/base.py"", line 266, in execute
    return_value = super().execute(context)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py"", line 422, in wrapper
    return func(self, *args, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py"", line 505, in execute
    return super().execute(context=serializable_context)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py"", line 422, in wrapper
    return func(self, *args, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py"", line 238, in execute
    return_value = self.execute_callable()
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py"", line 870, in execute_callable
    result = self._execute_python_callable_in_subprocess(python_path)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py"", line 588, in _execute_python_callable_
```in_subprocess
    raise AirflowException(error_msg) from None
airflow.exceptions.AirflowException: Process returned non-zero exit status 1.
'Variable eggs does not exist'
[2025-01-28, 10:20:25 CET] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=sqlalchemy_test, task_id=get_variable2, run_id=manual__2025-01-28T09:20:19.722028+00:00, execution_date=20250128T092019, start_date=20250128T092020, end_date=20250128T092025
[2025-01-28, 10:20:25 CET] {taskinstance.py:340} ▶ Post task execution logs

tgrandje (Issue Creator) on (2025-01-28 09:28:41 UTC): @eladkal You sure about that? I hadn't had any trouble up to now. For instance, that dag will run get_variable3 just fine:
```
from airflow.decorators import dag, task


@dag()
def sqlalchemy_test():

    from airflow.models import Variable

    Variable.set(""eggs"", ""spam"")

    @task
    def get_variable():
        from airflow.models import Variable

        print(Variable.get(""eggs""))

    @task.virtualenv(requirements=[""sqlalchemy>2.0.0""])
    def get_variable2():
        from airflow.models import Variable

        print(Variable.get(""eggs""))

    @task.virtualenv(requirements=[""geopandas""])
    def get_variable3():
        from airflow.models import Variable

        print(Variable.get(""eggs""))

    get_variable()
    get_variable2()
    get_variable3()


sqlalchemy_test()
```

laksh-krishna-sharma on (2025-01-29 10:31:07 UTC): Hello sir may i work on this issue with a bit of your guidance

tgrandje (Issue Creator) on (2025-01-29 12:02:36 UTC): @laksh-krishna-sharma Sure! Do you need anything beside what I had already posted?

potiuk on (2025-01-29 23:42:32 UTC): This is clear I think. Aoache Airflow does not work with sqlalchemy > 2.0 - mainly because some of the dependencies do not work with it. 

I thin it's clear because airflow has sqlalchemy < 2.0 as requirement.

"
2814687927,issue,open,,Permanent link icon is broken after Sphinx upgrade,"### What do you see as an issue?

After upgrading our version of Sphinx in #45563, the permanent link icon is now broken.

Before:

<img width=""373"" alt=""Image"" src=""https://github.com/user-attachments/assets/cd3bc955-f960-421a-b6f2-5622816bdc37"" />

Currently:

<img width=""354"" alt=""Image"" src=""https://github.com/user-attachments/assets/0e338c7f-6310-4ad4-9904-f999ef7e30d7"" />



### Solving the problem

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jedcunningham,2025-01-28 05:43:39+00:00,[],2025-02-02 16:59:44+00:00,,https://github.com/apache/airflow/issues/46170,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('kind:documentation', '')]","[{'comment_id': 2618045563, 'issue_id': 2814687927, 'author': 'eladkal', 'body': 'cc @shahar1', 'created_at': datetime.datetime(2025, 1, 28, 6, 36, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2618063733, 'issue_id': 2814687927, 'author': 'shahar1', 'body': '> cc @shahar1 \n\n@jscheffl do you know how to handle it?', 'created_at': datetime.datetime(2025, 1, 28, 6, 52, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2619935057, 'issue_id': 2814687927, 'author': 'jscheffl', 'body': 'In older Sphinx versions (or any extra, have no deep insight) there was an SVG rendering the link. After upgrading the SVG was ""gone"" so could not adjust the template to the icon/image. Therefore to make it to main I replaced it with the text.\n\nFeel free to find the bug in the theme or extend it, Otherwise we need to live with it.', 'created_at': datetime.datetime(2025, 1, 28, 19, 56, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2622326896, 'issue_id': 2814687927, 'author': 'potiuk', 'body': 'I think the SVG was gone because we tested it in test environment where CSP (Content Security Policy) was set - and we had to fix few other things (for example we had to download all images of committers from GitHub to be embedded in our website. A ND we should do the same in this case. this SVG has to come from our static folder.  That should fix it', 'created_at': datetime.datetime(2025, 1, 29, 17, 22, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629283051, 'issue_id': 2814687927, 'author': 'hariomkhonde108', 'body': 'where is the link icon broken i  can see it properley on the website and if it is still  there can i solve this issue\n\n![Image](https://github.com/user-attachments/assets/4728ab38-93ba-4602-b78a-8191f95680c5)\n\nso if the isssue is still there can you tell me where', 'created_at': datetime.datetime(2025, 2, 2, 7, 40, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629472431, 'issue_id': 2814687927, 'author': 'jedcunningham', 'body': '@hariomkhonde108, this is in main, not the ""stable"" docs on the website. So you either need to [build the docs locally](https://github.com/apache/airflow/tree/main/docs#building-documentation) or look at the [dev docs](http://apache-airflow-docs.s3-website.eu-central-1.amazonaws.com/docs/apache-airflow/stable/administration-and-deployment/scheduler.html).', 'created_at': datetime.datetime(2025, 2, 2, 16, 59, 44, tzinfo=datetime.timezone.utc)}]","eladkal on (2025-01-28 06:36:43 UTC): cc @shahar1

shahar1 on (2025-01-28 06:52:15 UTC): @jscheffl do you know how to handle it?

jscheffl on (2025-01-28 19:56:25 UTC): In older Sphinx versions (or any extra, have no deep insight) there was an SVG rendering the link. After upgrading the SVG was ""gone"" so could not adjust the template to the icon/image. Therefore to make it to main I replaced it with the text.

Feel free to find the bug in the theme or extend it, Otherwise we need to live with it.

potiuk on (2025-01-29 17:22:37 UTC): I think the SVG was gone because we tested it in test environment where CSP (Content Security Policy) was set - and we had to fix few other things (for example we had to download all images of committers from GitHub to be embedded in our website. A ND we should do the same in this case. this SVG has to come from our static folder.  That should fix it

hariomkhonde108 on (2025-02-02 07:40:40 UTC): where is the link icon broken i  can see it properley on the website and if it is still  there can i solve this issue

![Image](https://github.com/user-attachments/assets/4728ab38-93ba-4602-b78a-8191f95680c5)

so if the isssue is still there can you tell me where

jedcunningham (Issue Creator) on (2025-02-02 16:59:44 UTC): @hariomkhonde108, this is in main, not the ""stable"" docs on the website. So you either need to [build the docs locally](https://github.com/apache/airflow/tree/main/docs#building-documentation) or look at the [dev docs](http://apache-airflow-docs.s3-website.eu-central-1.amazonaws.com/docs/apache-airflow/stable/administration-and-deployment/scheduler.html).

"
2814637428,issue,closed,not_planned,Fix `org.apache.spark.SparkException: Could not parse Master URL: 'spark-master:7077'`,"### Apache Airflow Provider(s)

apache-spark

### Versions of Apache Airflow Providers

apache-airflow-providers-apache-spark==5.0.0


### Apache Airflow version

2.10.4

### Operating System

Microsoft Windows 11

### Deployment

Docker-Compose

### Deployment details

Docker version 27.4.0, build `bde2b89`
Docker Compose version v2.31.0-desktop.2
Docker Desktop 4.37.1 (178610)

### What happened

When using Docker Compose to configure Apache Airflow with Spark, the `SparkSubmitOperator` fails to execute the Spark job due to an incorrect `--master` URL format in the `spark-submit` command.

With docker compose
```yml
services:
  airflow-webserver:
    build:
      context: .
      dockerfile: ./airflow/Dockerfile # installed java and apache-airflow-providers-apache-spark
    container_name: airflow-webserver
    environment:
      - AIRFLOW_CONN_SPARK_DEFAULT=spark://spark-master:7077
      ...

  airflow-scheduler:
    build:
      context: .
      dockerfile: ./airflow/Dockerfile # installed java and apache-airflow-providers-apache-spark
    container_name: airflow-scheduler
    environment:
      - AIRFLOW_CONN_SPARK_DEFAULT=spark://spark-master:7077
      ...
```

for DAG with 
```python
SparkSubmitOperator(
        ...
        conn_id='spark_default',  
        ...
)
```

failed with log 
```log
INFO - Spark-Submit cmd: spark-submit --master spark-master:7077 --conf ... /opt/spark/jobs/test_simple_job.py
...
INFO - : org.apache.spark.SparkException: Could not parse Master URL: 'spark-master:7077'
...
ERROR - Failed to execute job 2 for task spark_submit_task (Cannot execute: spark-submit --master spark-master:7077 --conf ... /opt/spark/jobs/test_simple_job.py. Error code is: 1.; 59)
```

### What you think should happen instead

The `spark-submit` command should use the correct `--master` URL format:
`--master spark://spark-master:7077`

The expected command should look like this:
```bash
spark-submit --master spark://spark-master:7077 /opt/spark/jobs/my_spark_job.py
```

### How to reproduce

1. Clone the repository and switch to the issue-report branch:
   ```bash
   git clone -b issue-report https://github.com/yehoon17/data-analysis.git
   ```
2. Start the Docker Compose setup:
   ```bash
   docker compose -f docker-compose.issue.yml up -d
   ```
3. Trigger the DAG with `dag_id=spark_job_dag`.

The error described above will occur when the DAG attempts to run the Spark job.

### Details
`docker-compose.issue.yml`:
```
services:
  postgres:
    image: postgres:17.2
    container_name: postgres
    env_file:
      - ./.env
    ports:
      - ""5433:5432""
    volumes:
      - ./postgres/init.sql:/docker-entrypoint-initdb.d/init.sql

  airflow-webserver:
    build:
      context: .
      dockerfile: ./airflow/Dockerfile
    container_name: airflow-webserver
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      - AIRFLOW_CONN_POSTGRES_DEFAULT=${AIRFLOW_CONN_POSTGRES_DEFAULT}
      - AIRFLOW_CONN_SPARK_DEFAULT=spark://spark-master:7077
    depends_on:
      - postgres
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./spark/jobs:/opt/spark/jobs
      - ./raw_data:/opt/airflow/raw_data
    ports:
      - ""8080:8080""
    command: >
      bash -c ""airflow db migrate &&
                airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com &&
                airflow webserver""

  airflow-scheduler:
    build:
      context: .
      dockerfile: ./airflow/Dockerfile
    container_name: airflow-scheduler
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      - AIRFLOW_CONN_POSTGRES_DEFAULT=${AIRFLOW_CONN_POSTGRES_DEFAULT}
      - AIRFLOW_CONN_SPARK_DEFAULT=spark://spark-master:7077
    depends_on:
      - postgres
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./spark/jobs:/opt/spark/jobs
      - ./raw_data:/opt/airflow/raw_data
    command: >
      bash -c ""airflow scheduler""
    restart: always

  spark-master:
    image: bitnami/spark:3.5.4
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_IP=spark-master
    ports:
      - ""7077:7077""  # Spark Master Port
      - ""18080:8080""  # Spark Web UI
    volumes:
      - ./raw_data:/opt/spark/raw_data
      - ./spark/jobs:/opt/spark/jobs

  spark-worker:
    image: bitnami/spark:3.5.4
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
    depends_on:
      - spark-master
    ports:
      - ""18081:8081""  # Spark Worker UI
    volumes:
      - ./raw_data:/opt/spark/raw_data
```

`airflow/Dockerfile`:
```FROM apache/airflow:2.10.4

# Switch to root user to install system-level dependencies
USER root

# Install required libraries and clean up in a single layer
RUN apt-get update && apt-get install -y --no-install-recommends \
    gcc \
    # g++ \
    heimdal-dev \
    # libsasl2-dev \
    openjdk-17-jre-headless \
  && apt-get autoremove -yqq --purge \
  && apt-get clean \
  && rm -rf /var/lib/apt/lists/*

# Set JAVA_HOME environment variable for Spark compatibility
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
ENV PATH=$JAVA_HOME/bin:$PATH

# Switch back to airflow user
USER airflow

# Copy and install Python dependencies
COPY requirements.txt /requirements.txt
RUN pip install --no-cache-dir -r /requirements.txt \
  && pip uninstall -y argparse
```

`airflow/dags/test_spark_dag.py`:
```python
from airflow import DAG
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from datetime import datetime, timedelta

# Default arguments for the DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Define the DAG
with DAG(
    dag_id='spark_job_dag',
    default_args=default_args,
    description='Run a Spark job using Airflow',
    schedule_interval=None,  
    start_date=datetime(2023, 1, 1),
    catchup=False,
) as dag:

    # SparkSubmitOperator to run the Spark job
    run_spark_job = SparkSubmitOperator(
        task_id='spark_submit_task',
        application='/opt/spark/jobs/test_simple_job.py',  
        conn_id='spark_default',  
        name='example_spark_job',
        execution_timeout=timedelta(minutes=10),
        conf={
            'spark.cores.max': '2',
            'spark.executor.memory': '1G',
            'spark.executor.instances': '1',
        },
        executor_cores=1,
        executor_memory='1G',
        driver_memory='512M',
        verbose=True
    )

    run_spark_job
```

`spark/jobs/test_simple_job.py'`:
```python
from pyspark.sql import SparkSession

def main():
    # Initialize a Spark session
    spark = SparkSession.builder \
        .appName(""SimplePySparkJob"") \
        .getOrCreate()

    # Create a DataFrame
    data = [(""Alice"", 1), (""Bob"", 2), (""Cathy"", 3)]
    df = spark.createDataFrame(data, [""name"", ""value""])

    # Show the DataFrame
    df.show()

    # Perform some transformations and actions
    result = df.groupBy(""value"").count()
    result.show()

    # Stop the Spark session
    spark.stop()

if __name__ == ""__main__"":
    main()
```

### Anything else

#### **Resolution**
To fix the issue, update the `conn_data[""master""]` assignment in the `SparkSubmitHook`:
1. Access the relevant containers:
   ```bash
   docker exec -it airflow-webserver /bin/bash
   docker exec -it airflow-scheduler /bin/bash
   ```
2. Edit the file:
   for `apache-airflow-providers-apache-spark==5.0.0`:
   ```bash
   sed -i '271s|conn_data\[""master""\] = f""{conn.host}:{conn.port}""|conn_data[""master""] = f""{conn.conn_type}://{conn.host}:{conn.port}""|' /home/airflow/.local/lib/python3.12/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py
   ```
   Change this line:
   ```python
   conn_data[""master""] = f""{conn.host}:{conn.port}""
   ```
   To this:
   ```python
   conn_data[""master""] = f""{conn.conn_type}://{conn.host}:{conn.port}""
   ```

#### **Outcome**
After making this change, the `SparkSubmitOperator` correctly constructs the `--master` argument with the `spark://` prefix, resolving the error `org.apache.spark.SparkException: Could not parse Master URL: 'spark-master:7077'`

Updated Airflow log output:
```
INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --name example_spark_job /opt/airflow/spark_jobs/my_spark_job.py
...
INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
...
INFO - 25/01/27 21:04:20 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.5:7077 after 121 ms (0 ms spent in bootstraps)
...
INFO - Marking task as SUCCESS. dag_id=spark_job_dag, task_id=spark_submit_task, run_id=manual__2025-01-27T21:03:55.131353+00:00, execution_date=20250127T210355, start_date=20250127T210402, end_date=20250127T211001
...
```


Full story: https://github.com/yehoon17/data-analysis/issues/5

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",yehoon17,2025-01-28 05:05:29+00:00,[],2025-01-29 08:25:47+00:00,2025-01-29 08:25:45+00:00,https://github.com/apache/airflow/issues/46169,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:apache-spark', '')]","[{'comment_id': 2617855153, 'issue_id': 2814637428, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 28, 5, 5, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2618095587, 'issue_id': 2814637428, 'author': 'nevcohen', 'body': 'See [here](https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html#uri-format-example) how to use URI to store connction in environment variables.', 'created_at': datetime.datetime(2025, 1, 28, 7, 17, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2618101517, 'issue_id': 2814637428, 'author': 'nevcohen', 'body': 'Maybe for this purpose it is better for you to put [json](https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html#json-format-example) in the environment variable.', 'created_at': datetime.datetime(2025, 1, 28, 7, 21, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2618330527, 'issue_id': 2814637428, 'author': 'yehoon17', 'body': 'Thank you for your response and suggestion.\n\nI think I need to look into managing connections further to fully understand the implementations.\nI am open and eager to learn more about this topic. Any additional suggestions or improvements, not just regarding this issue but also in other areas, would be greatly appreciated.\n\nThank you again for your time and input!', 'created_at': datetime.datetime(2025, 1, 28, 8, 54, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2620973462, 'issue_id': 2814637428, 'author': 'eladkal', 'body': 'Closing the issue as no bug reported', 'created_at': datetime.datetime(2025, 1, 29, 8, 25, 45, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-28 05:05:32 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

nevcohen on (2025-01-28 07:17:22 UTC): See [here](https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html#uri-format-example) how to use URI to store connction in environment variables.

nevcohen on (2025-01-28 07:21:41 UTC): Maybe for this purpose it is better for you to put [json](https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html#json-format-example) in the environment variable.

yehoon17 (Issue Creator) on (2025-01-28 08:54:25 UTC): Thank you for your response and suggestion.

I think I need to look into managing connections further to fully understand the implementations.
I am open and eager to learn more about this topic. Any additional suggestions or improvements, not just regarding this issue but also in other areas, would be greatly appreciated.

Thank you again for your time and input!

eladkal on (2025-01-29 08:25:45 UTC): Closing the issue as no bug reported

"
2814511379,issue,closed,not_planned,Fix BigQueryInsertJobOperator error handling in deferrable mode,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

I am using Apache Airflow version 2.6.2.

### What happened?

When using the BigQueryInsertJobOperator in deferrable mode, if the job fails without being deferred, the task does not raise an exception or fail as expected. Instead, it continues execution without proper error handling, leading to incorrect task states and potential data inconsistencies.

### What you think should happen instead?

The task should raise an exception and fail immediately if the job encounters an error in non-deferred mode, ensuring proper error handling and task state management.

### How to reproduce

1. Set up a DAG with the BigQueryInsertJobOperator in deferrable mode.  
2. Configure the operator to execute a job that will fail (invalid SQL query).  
3. Trigger the DAG and observe the task behavior.

### Operating System

Ubuntu 20.04 LTS

### Versions of Apache Airflow Providers

8.10.0

### Deployment

Docker-Compose

### Deployment details

- Airflow version: 2.6.2  
- Docker Compose version: 2.17.2  
- Kubernetes version: N/A  
- Custom configurations: None

### Anything else?

This issue occurs every time a non-deferred job fails in the BigQueryInsertJobOperator. Below are the relevant logs:  

log
[2025-01-28T12:34:56.789Z] ERROR - Task failed but did not raise an exception. Task state: SUCCESS

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Mohammed-Karim226,2025-01-28 02:58:45+00:00,[],2025-01-28 05:29:58+00:00,2025-01-28 05:25:21+00:00,https://github.com/apache/airflow/issues/46160,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2617920775, 'issue_id': 2814511379, 'author': 'eladkal', 'body': '@Mohammed-Karim226 You have track record of posting spam issues in this project and in others. I am closing this issue for the same reason.\n\nIf you are a real person with real intent to improve the project please interact like people do. We do not appreciate wasting our time. If you are a real Google user with a real problem do the extra mile to prove this is a real issue. Show actual logs with images proving the failure show the actual real code you ran to face the error and present it both in Airflow and Google cloud env.', 'created_at': datetime.datetime(2025, 1, 28, 5, 25, 21, tzinfo=datetime.timezone.utc)}]","eladkal on (2025-01-28 05:25:21 UTC): @Mohammed-Karim226 You have track record of posting spam issues in this project and in others. I am closing this issue for the same reason.

If you are a real person with real intent to improve the project please interact like people do. We do not appreciate wasting our time. If you are a real Google user with a real problem do the extra mile to prove this is a real issue. Show actual logs with images proving the failure show the actual real code you ran to face the error and present it both in Airflow and Google cloud env.

"
2814478076,issue,closed,completed,Could not read served logs Caused by NameResolutionError,"### Official Helm Chart version

1.15.0 (latest released)

### Apache Airflow version

2.10.4

### Kubernetes Version

1.29.8

### Helm Chart configuration

executor: ""CeleryKubernetesExecutor""

config:
  celery_kubernetes_executor:
    kubernetes_queue: k8s

airflowVersion: 2.10.4

images:
    airflow:
        repository: apache/airflow
        tag: 2.10.4


### Docker Image customizations

None.

### What happened

When running any task on the `k8s` queue, the logs portion of the DAGs displays the message `Could not read served logs: HTTPConnectionPool(host='hello-world-task1-8n9u83p0', port=8793): Max retries exceeded with url: /log/dag_id=hello_world/run_id=manual__2025-01-28T02:16:39.751530+00:00/task_id=task1/attempt=1.log (Caused by NameResolutionError(""<urllib3.connection.HTTPConnection object at 0x7f89fd5e6b70>: Failed to resolve 'hello-world-task1-8n9u83p0' ([Errno -3] Temporary failure in name resolution)""))`

### What you think should happen instead

Logs should be available on the web interface.

### How to reproduce

Deploy helm chart with config above, and run a simple hello world dag

```python
@dag(
    schedule=None,
    start_date=pendulum.datetime(2001, 1, 1, tz=""UTC""),
    catchup=False,
)
def hello_world():
    
    @task(queue='k8s')
    def task1():
        print(""Hello World"")
    
    task1()

hello_world()
```

### Anything else

I just noticed that if you open the logs tab just as the flow is running, you can catch a glimpse of the logs. But then the error message appears, apparently just as the pod is deleted.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",carlosjourdan,2025-01-28 02:24:27+00:00,[],2025-01-28 03:19:55+00:00,2025-01-28 03:19:54+00:00,https://github.com/apache/airflow/issues/46159,"[('kind:bug', 'This is a clearly a bug'), ('area:logging', ''), ('area:helm-chart', 'Airflow Helm Chart'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2617624509, 'issue_id': 2814478076, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 28, 2, 24, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2617761743, 'issue_id': 2814478076, 'author': 'carlosjourdan', 'body': 'Fixed it with \n\n```yaml\nlogs:\n  persistence:\n    enabled: true\n```', 'created_at': datetime.datetime(2025, 1, 28, 3, 19, 54, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-28 02:24:29 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

carlosjourdan (Issue Creator) on (2025-01-28 03:19:54 UTC): Fixed it with 

```yaml
logs:
  persistence:
    enabled: true
```

"
2814431541,issue,closed,completed,Croniter (and other) dependency risks (due to EU CRA): Package unmaintained and scheduled for potential removal,"### What do you see as an issue?

## Description:
The croniter package, which Airflow uses for parsing cron expressions (particularly extended syntax like `0 0 * * MON#1`), has been officially declared unmaintained as of December 10, 2024. The maintainer announced potential removal of all resources by March 15, 2025 due to EU Cyber Resilience Act regulations.

(Was helping an internal Airflow user on their cron expressions today and found out this news through browsing the Airflow User doc links)

Airflow Doc:
https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/cron.html#cron-presets

## Impact:
- Extended cron syntax functionality in Airflow depends on this package
- No future security patches or updates will be provided
- Package could be removed from PyPI in March 2025


## Details
From https://github.com/corpusops/croniter
```
While this repository has been inactive for some time, this formal notice, issued on December 10, 2024, serves as the official declaration to clarify the situation. Consequently, this repository and all associated resources (including related projects, code, documentation, and distributed packages such as Docker images, PyPI packages, etc.) are now explicitly declared unmaintained and abandoned.

I would like to remind everyone that this project’s free license has always been based on the principle that the software is provided ""AS-IS"", without any warranty or expectation of liability or maintenance from the maintainer. As such, it is used solely at the user's own risk, with no warranty or liability from the maintainer, including but not limited to any damages arising from its use.

Due to the enactment of the Cyber Resilience Act (EU Regulation 2024/2847), which significantly alters the regulatory framework, including penalties of up to €15M, combined with its demands for unpaid and indefinite liability, it has become untenable for me to continue maintaining all my Open Source Projects as a natural person. The new regulations impose personal liability risks and create an unacceptable burden, regardless of my personal situation now or in the future, particularly when the work is done voluntarily and without compensation.

No further technical support, updates (including security patches), or maintenance, of any kind, will be provided.

These resources may remain online, but solely for public archiving, documentation, and educational purposes.

Users are strongly advised not to use these resources in any active or production-related projects, and to seek alternative solutions that comply with the new legal requirements (EU CRA).

Using these resources outside of these contexts is strictly prohibited and is done at your own risk.

Regarding the potential transfer of the project to another entity, discussions are ongoing, but no final decision has been made yet. As a last resort, if the project and its associated resources are not transferred, I may begin removing any published resources related to this project (e.g., from PyPI, Docker Hub, GitHub, etc.) starting March 15, 2025, especially if the CRA’s risks remain disproportionate.
```

### Solving the problem

## Potential Solutions:
1. Evaluate alternative cron parsing libraries
2. Consider forking and maintaining croniter within Apache Airflow
3. Implement native support for extended cron syntax

### Open questions
- It is likely that it's not just this one OSS tool that Airflow depends on is reacting to EU CRA, might need a scalable solution to handle this
- Would be helpful to get broader community input on preferred approach before the March 2025 deadline.

### FYI: Links a few things I read about ASF's reaction (TG)
https://news.apache.org/foundation/entry/update-on-eu-software-regulation-lots-of-improvements-good-news
https://news.apache.org/foundation/entry/open-source-community-unites-to-build-cra-compliant-cybersecurity-processes

### Anything else

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",1fanwang,2025-01-28 01:42:01+00:00,[],2025-01-28 08:36:08+00:00,2025-01-28 08:36:08+00:00,https://github.com/apache/airflow/issues/46158,"[('kind:bug', 'This is a clearly a bug'), ('kind:documentation', ''), ('area:dependencies', 'Issues related to dependencies problems'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2617474051, 'issue_id': 2814431541, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 28, 1, 42, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2617763852, 'issue_id': 2814431541, 'author': 'jedcunningham', 'body': 'Thanks @1fanwang for making sure this was on our radar! The Airflow PMC is aware of this and we have been working behind the scenes on some options. We will have more to share soon.', 'created_at': datetime.datetime(2025, 1, 28, 3, 22, 15, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-28 01:42:04 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jedcunningham on (2025-01-28 03:22:15 UTC): Thanks @1fanwang for making sure this was on our radar! The Airflow PMC is aware of this and we have been working behind the scenes on some options. We will have more to share soon.

"
2814248824,issue,closed,completed,Add support for additional Weaviate v4 features in the Airflow Weaviate provider,"### Description

The recent PR (#40194) upgraded the Weaviate provider in Apache Airflow to support Weaviate v4. While this PR introduced the necessary changes to migrate from v3 to v4, there are additional features and improvements in Weaviate v4 that could be leveraged to enhance the Airflow provider further. This feature request proposes adding support for these new capabilities to improve functionality and user experience.

### Use case/motivation

1. *Enhanced functionality*: Weaviate v4 introduces new features such as improved schema management, advanced query capabilities, and better performance optimizations. Supporting these features in the Airflow provider would allow users to take full advantage of Weaviate's capabilities.
2. *Better user experience*: By aligning the provider with the latest Weaviate features, users can benefit from a more intuitive and powerful integration.
3. *Future-proofing*: As Weaviate continues to evolve, ensuring the Airflow provider stays up-to-date will reduce the need for frequent breaking changes and simplify maintenance.


### Related issues

- *PR #40194*: The original PR that upgraded the Weaviate provider to v4.
- *PR #40663*: Fixes to the Weaviate changelog related to the v4 upgrade.
- *PR #40670*: Minor fixes for typos in the Weaviate provider code.


### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Mohammed-Karim226,2025-01-27 23:10:40+00:00,[],2025-01-29 14:24:51+00:00,2025-01-27 23:56:40+00:00,https://github.com/apache/airflow/issues/46153,"[('area:providers', ''), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:weaviate', ''), ('AI Spam', '')]","[{'comment_id': 2617217866, 'issue_id': 2814248824, 'author': 'potiuk', 'body': ""AI spam. Please stop doing it I am repporting you to GitHub - they already started to close such user's accounts after our reports, so you can expect this to happen soon."", 'created_at': datetime.datetime(2025, 1, 27, 23, 56, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2617555736, 'issue_id': 2814248824, 'author': 'Mohammed-Karim226', 'body': 'It is not fully AI bot usage, I only use the AI bot for the best MD formatting and writing styles that save time for me, organize the content, and make me more productive.\n\nRegards,\nMohammed Karim.', 'created_at': datetime.datetime(2025, 1, 28, 2, 5, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2621798540, 'issue_id': 2814248824, 'author': 'potiuk', 'body': ""> It is not fully AI bot usage, I only use the AI bot for the best MD formatting and writing styles that save time for me, organize the content, and make me more productive.\n> \n> Regards, Mohammed Karim.\n\nWhich leads to all your issues being closed. It means it makes you far less productive. Try without those tools, they don't work."", 'created_at': datetime.datetime(2025, 1, 29, 14, 23, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2621801391, 'issue_id': 2814248824, 'author': 'potiuk', 'body': 'This is how your ""more productive"" looks like. Just face reality.\n\n![Image](https://github.com/user-attachments/assets/3648d538-6c68-4c98-9700-51136ad151a0)', 'created_at': datetime.datetime(2025, 1, 29, 14, 24, 49, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-27 23:56:40 UTC): AI spam. Please stop doing it I am repporting you to GitHub - they already started to close such user's accounts after our reports, so you can expect this to happen soon.

Mohammed-Karim226 (Issue Creator) on (2025-01-28 02:05:17 UTC): It is not fully AI bot usage, I only use the AI bot for the best MD formatting and writing styles that save time for me, organize the content, and make me more productive.

Regards,
Mohammed Karim.

potiuk on (2025-01-29 14:23:40 UTC): Which leads to all your issues being closed. It means it makes you far less productive. Try without those tools, they don't work.

potiuk on (2025-01-29 14:24:49 UTC): This is how your ""more productive"" looks like. Just face reality.

![Image](https://github.com/user-attachments/assets/3648d538-6c68-4c98-9700-51136ad151a0)

"
2814080557,issue,closed,completed,"Hard to tell if ""Include"" buttons are selected (under ""Clear Task"")","### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When retrying a task via the ""Clear Task"" dialog, the button bar for ""Include"" options only slightly change color when selected, making it difficult to visually determine which options are enabled:

![Image](https://github.com/user-attachments/assets/af8f6953-b408-4146-beb3-5a4d90df119e)

Depending on the user's screen/color settings, the selected options above either show up as light gray and reasonably obvious, or very light gray that is almost indistinguishable from the unselected white color. This makes it very easy to clear tasks using the wrong options.

### What you think should happen instead?

Rather than use `gray.100` for selected options in the button bar, use `blue.500` or another brighter and more obvious color (per the [Chakra UI colors](https://www.chakra-ui.com/docs/theming/colors)

### How to reproduce

1. Select a task in the Airflow UI, click the ""Clear task"" button
2. Click the buttons underneath ""Include""; depending on your screen's color settings it may be very hard to tell what is selected.

### Operating System

Windows 10

### Versions of Apache Airflow Providers

```
apache-airflow-providers-common-compat==1.3.0
apache-airflow-providers-common-io==1.5.0
apache-airflow-providers-common-sql==1.21.0
apache-airflow-providers-fab==1.5.2
apache-airflow-providers-ftp==3.12.0
apache-airflow-providers-http==5.0.0
apache-airflow-providers-imap==3.8.0
apache-airflow-providers-smtp==1.9.0
apache-airflow-providers-sqlite==4.0.0
```

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",kilbasar,2025-01-27 21:19:12+00:00,[],2025-01-27 23:42:53+00:00,2025-01-27 23:42:53+00:00,https://github.com/apache/airflow/issues/46151,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2616916811, 'issue_id': 2814080557, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 27, 21, 19, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2617143919, 'issue_id': 2814080557, 'author': 'potiuk', 'body': 'We are unlikely to make any changes to the UI - Airflow 3 will have completely new / rewritten UI. Converting it into a discussion - if you would like to work on a patch still - feel free to open PR, but it will only be accepted if it is really a bugfix and classified as such.', 'created_at': datetime.datetime(2025, 1, 27, 23, 42, 47, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-27 21:19:15 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2025-01-27 23:42:47 UTC): We are unlikely to make any changes to the UI - Airflow 3 will have completely new / rewritten UI. Converting it into a discussion - if you would like to work on a patch still - feel free to open PR, but it will only be accepted if it is really a bugfix and classified as such.

"
2813497450,issue,closed,completed,AIP-38 Mark TI as success / failed,"### Body

Related to https://github.com/apache/airflow/issues/43712

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2025-01-27 16:36:45+00:00,['pierrejeambrun'],2025-01-28 16:44:22+00:00,2025-01-28 16:44:22+00:00,https://github.com/apache/airflow/issues/46137,"[('kind:meta', 'High-level information important to the community')]",[],
2813275697,issue,open,,Add way to remove old bundle versions from worker/processor,"### Body

Now that we have multiple versions of a bundle on different Airflow components, we also need a way to clean them up once they are no longer being actively used.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2025-01-27 15:06:27+00:00,['dstandish'],2025-01-27 15:08:47+00:00,,https://github.com/apache/airflow/issues/46134,"[('kind:feature', 'Feature Requests'), ('kind:meta', 'High-level information important to the community'), ('AIP-66: DAG Bundle/Manifest', '')]",[],
2813141343,issue,open,,"Add control over bundle ""version in db"" check in pasring loop","### Body

Right now, in order to protect against multiple DAG processors becoming out of sync and parsing different versions of a bundle at the same time, we are constantly querying the db for the currently known bundle version. We, however, need a way to turn that down a bit, both for single DAG processor and multiple DAG processors.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2025-01-27 14:12:45+00:00,['jedcunningham'],2025-01-27 14:15:03+00:00,,https://github.com/apache/airflow/issues/46130,"[('kind:meta', 'High-level information important to the community'), ('area:core', ''), ('AIP-66: DAG Bundle/Manifest', '')]",[],
2813130931,issue,closed,completed,AIP-38 | Add auto-refreshing to new UI,"- Dag Run page: https://github.com/apache/airflow/pull/46213
- Task Instance page: https://github.com/apache/airflow/pull/46213
- Graph view https://github.com/apache/airflow/pull/46330
- Grid view https://github.com/apache/airflow/pull/46330
- Dag page https://github.com/apache/airflow/pull/46296
- Dags list https://github.com/apache/airflow/pull/46326",bbovenzi,2025-01-27 14:08:30+00:00,['bbovenzi'],2025-01-31 18:09:32+00:00,2025-01-31 18:09:32+00:00,https://github.com/apache/airflow/issues/46129,"[('area:UI', 'Related to UI/UX. For Frontend Developers.')]",[],
2811595687,issue,closed,completed,"Status of testing Providers that were prepared on January 26, 2025","### Body

I have a kind request for all the contributors to the latest provider packages release.
Could you please help us to test the RC versions of the providers?

The guidelines on how to test providers can be found in

[Verify providers by contributors](https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDER_PACKAGES.md#verify-the-release-candidate-by-contributors)

Let us know in the comments, whether the issue is addressed.

**NOTE Changes tested on [RC1](https://github.com/apache/airflow/issues/45957) were automatically marked as tested for RC2**


These are providers that require testing as there were some substantial changes introduced:


## Provider [celery: 3.10.0rc2](https://pypi.org/project/apache-airflow-providers-celery/3.10.0rc2)
   - [x] [Add support for custom celery configs (#45038)](https://github.com/apache/airflow/pull/45038): @arorasachin9
   - [x] [Fix Version Check for CLI Imports in Providers (#45255)](https://github.com/apache/airflow/pull/45255): @bugraoz93
   - [x] [AIP-72: Support DAG parsing context in Task SDK (#45694)](https://github.com/apache/airflow/pull/45694): @kaxil
   - [x] [AIP-72: Support better type-hinting for Context dict in SDK  (#45583)](https://github.com/apache/airflow/pull/45583): @kaxil

<!--

NOTE TO RELEASE MANAGER:

You can move here the providers that have doc-only changes or for which changes are trivial, and
you could assess that they are OK.

-->
All users involved in the PRs:
@arorasachin9 @kaxil @bugraoz93




### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2025-01-26 14:36:48+00:00,[],2025-01-27 15:09:39+00:00,2025-01-27 15:09:38+00:00,https://github.com/apache/airflow/issues/46072,"[('area:providers', ''), ('kind:meta', 'High-level information important to the community'), ('testing status', 'Status of testing releases')]","[{'comment_id': 2614603825, 'issue_id': 2811595687, 'author': 'potiuk', 'body': 'All good!', 'created_at': datetime.datetime(2025, 1, 26, 21, 47, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2616020047, 'issue_id': 2811595687, 'author': 'eladkal', 'body': 'Thank you everyone. Providers are released.\n\nI invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).', 'created_at': datetime.datetime(2025, 1, 27, 15, 9, 38, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-26 21:47:28 UTC): All good!

eladkal (Issue Creator) on (2025-01-27 15:09:38 UTC): Thank you everyone. Providers are released.

I invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).

"
2811418095,issue,open,,Generate database ERD using chartdb,"### Body

https://github.com/chartdb/chartdb is very popular project and generate nice ERD schemas (~though not sure if we can use it due to AGPL-3.0 license ?~)
We should update our [generation script](https://github.com/apache/airflow/blob/3f6f04868d09a8766e4652417b7c0e8082ca2fd7/scripts/in_container/run_prepare_er_diagram.py) to use it



Note: This library is to be used only for development. We are not distributing it to users thus we are OK with having it despite  AGPL-3.0 license https://www.apache.org/legal/resolved.html#prohibited (Thanks to Jarek for this clarification)
### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2025-01-26 09:19:27+00:00,['jx2lee'],2025-01-26 14:18:20+00:00,,https://github.com/apache/airflow/issues/46057,"[('area:MetaDB', 'Meta Database related issues.'), ('good first issue', ''), ('kind:meta', 'High-level information important to the community'), ('kind:task', 'A task that needs to be completed as part of a larger issue')]","[{'comment_id': 2614439655, 'issue_id': 2811418095, 'author': 'jx2lee', 'body': '@eladkal Can i take this?', 'created_at': datetime.datetime(2025, 1, 26, 14, 7, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614444061, 'issue_id': 2811418095, 'author': 'eladkal', 'body': '> [@eladkal](https://github.com/eladkal) Can i take this?\n\nAssigned', 'created_at': datetime.datetime(2025, 1, 26, 14, 18, 19, tzinfo=datetime.timezone.utc)}]","jx2lee (Assginee) on (2025-01-26 14:07:51 UTC): @eladkal Can i take this?

eladkal (Issue Creator) on (2025-01-26 14:18:19 UTC): Assigned

"
2811302888,issue,open,,Airflow dag processor exits with too many open files after sometime,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

It seems airflow dag processor opens sockets under the hood that are not closed properly leading too many open files after it runs for sometime. To reproduce this please follow below steps since it takes sometime for the ulimit to be reached in case it's unlimited or very high.

There is comment on reading the source code

https://github.com/apache/airflow/blob/29b9e8ea10de7a82ad40a7a2160c64a84004a45e/task_sdk/src/airflow/sdk/execution_time/supervisor.py#L345-L361


### What you think should happen instead?

The open files should not increase and the files opened should be closed. The socket warnings should be fixed which could indicate the problem.

```
2025-01-26 09:20:05 [debug    ] Workload process exited 1      [supervisor] exit_code=0
[2025-01-26T09:20:05.502+0530] {dag.py:1841} INFO - Sync 1 DAGs
[2025-01-26T09:20:05.510+0530] {dag.py:2398} INFO - Setting next_dagrun for example_setup_teardown_taskflow to None, run_after=None
/usr/lib/python3.11/socket.py:789 ResourceWarning: unclosed <socket.socket fd=320, family=1, type=1, proto=0>
```

### How to reproduce

1. set `min_file_process_interval = 10` in airflow.cfg to trigger frequent reparsing.
2. Run `PYTHONWARNINGS=always python -X dev -m airflow dag-processor`
3. Use `ps aux | grep -i dag-processor` to get the pid
4. Run `watch ""ls -1 /proc/<pid>/fd | wc""` which keeps increasing
5. In another tab set open files limit for the process using `prlimit --pid <pid> --nofile=1024:1024`
Once the limit is reached dag processor exits with following stack trace

```
/usr/lib/python3.11/socket.py:789 ResourceWarning: unclosed <socket.socket fd=1010, family=1, type=1, proto=0>
[2025-01-26T09:12:56.322+0530] {dag_processor_job_runner.py:63} ERROR - Exception when executing DagProcessorJob
Traceback (most recent call last):
  File ""/home/karthikeyan/stuff/python/airflow/airflow/jobs/dag_processor_job_runner.py"", line 61, in _execute
    self.processor.run()
  File ""/home/karthikeyan/stuff/python/airflow/airflow/dag_processing/manager.py"", line 231, in run
    return self._run_parsing_loop()
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/karthikeyan/stuff/python/airflow/airflow/dag_processing/manager.py"", line 315, in _run_parsing_loop
    self._start_new_processes()
  File ""/home/karthikeyan/stuff/python/airflow/airflow/dag_processing/manager.py"", line 779, in _start_new_processes
    processor = self._create_process(file_path)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/karthikeyan/stuff/python/airflow/airflow/dag_processing/manager.py"", line 763, in _create_process
    return DagFileProcessorProcess.start(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/karthikeyan/stuff/python/airflow/airflow/dag_processing/processor.py"", line 212, in start
    proc: Self = super().start(target=target, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/karthikeyan/stuff/python/airflow/.env/lib/python3.11/site-packages/airflow/sdk/execution_time/supervisor.py"", line 343, in start
    child_logs, read_logs = mkpipe()
                            ^^^^^^^^
  File ""/home/karthikeyan/stuff/python/airflow/.env/lib/python3.11/site-packages/airflow/sdk/execution_time/supervisor.py"", line 132, in mkpipe
    rsock, wsock = socketpair()
                   ^^^^^^^^^^^^
  File ""/usr/lib/python3.11/socket.py"", line 657, in socketpair
    a, b = _socket.socketpair(family, type, proto)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 24] Too many open files
[2025-01-26T09:12:56.332+0530] {process_utils.py:266} INFO - Waiting up to 5 seconds for processes to exit...
Traceback (most recent call last):
  File ""<frozen runpy>"", line 198, in _run_module_as_main
  File ""<frozen runpy>"", line 88, in _run_code
  File ""/home/karthikeyan/stuff/python/airflow/airflow/__main__.py"", line 62, in <module>
    main()
  File ""/home/karthikeyan/stuff/python/airflow/airflow/__main__.py"", line 58, in main
    args.func(args)
  File ""/home/karthikeyan/stuff/python/airflow/airflow/cli/cli_config.py"", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/karthikeyan/stuff/python/airflow/airflow/utils/cli.py"", line 111, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/home/karthikeyan/stuff/python/airflow/airflow/utils/providers_configuration_loader.py"", line 55, in wrapped_function
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/karthikeyan/stuff/python/airflow/airflow/cli/commands/local_commands/dag_processor_command.py"", line 54, in dag_processor
    run_command_with_daemon_option(
  File ""/home/karthikeyan/stuff/python/airflow/airflow/cli/commands/local_commands/daemon_utils.py"", line 86, in run_command_with_daemon_option
    callback()
  File ""/home/karthikeyan/stuff/python/airflow/airflow/cli/commands/local_commands/dag_processor_command.py"", line 57, in <lambda>
    callback=lambda: run_job(job=job_runner.job, execute_callable=job_runner._execute),
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/karthikeyan/stuff/python/airflow/airflow/utils/session.py"", line 101, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/karthikeyan/stuff/python/airflow/airflow/jobs/job.py"", line 342, in run_job
    return execute_job(job, execute_callable=execute_callable)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/karthikeyan/stuff/python/airflow/airflow/jobs/job.py"", line 371, in execute_job
    ret = execute_callable()
          ^^^^^^^^^^^^^^^^^^
  File ""/home/karthikeyan/stuff/python/airflow/airflow/jobs/dag_processor_job_runner.py"", line 61, in _execute
    self.processor.run()
  File ""/home/karthikeyan/stuff/python/airflow/airflow/dag_processing/manager.py"", line 231, in run
    return self._run_parsing_loop()
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/karthikeyan/stuff/python/airflow/airflow/dag_processing/manager.py"", line 315, in _run_parsing_loop
    self._start_new_processes()
  File ""/home/karthikeyan/stuff/python/airflow/airflow/dag_processing/manager.py"", line 779, in _start_new_processes
    processor = self._create_process(file_path)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/karthikeyan/stuff/python/airflow/airflow/dag_processing/manager.py"", line 763, in _create_process
    return DagFileProcessorProcess.start(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/karthikeyan/stuff/python/airflow/airflow/dag_processing/processor.py"", line 212, in start
    proc: Self = super().start(target=target, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/karthikeyan/stuff/python/airflow/.env/lib/python3.11/site-packages/airflow/sdk/execution_time/supervisor.py"", line 343, in start
    child_logs, read_logs = mkpipe()
                            ^^^^^^^^
  File ""/home/karthikeyan/stuff/python/airflow/.env/lib/python3.11/site-packages/airflow/sdk/execution_time/supervisor.py"", line 132, in mkpipe
    rsock, wsock = socketpair()
                   ^^^^^^^^^^^^
  File ""/usr/lib/python3.11/socket.py"", line 657, in socketpair
    a, b = _socket.socketpair(family, type, proto)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 24] Too many open files

```

### Operating System

Ubuntu 20.04.3 LTS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",tirkarthi,2025-01-26 03:51:03+00:00,['jedcunningham'],2025-02-06 23:07:10+00:00,,https://github.com/apache/airflow/issues/46048,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('affected_version:main_branch', 'Issues Reported for main branch'), ('area:DAG-processing', ''), ('affected_version:3.0.0alpha', 'For all 3.0.0 alpha releases')]","[{'comment_id': 2614198634, 'issue_id': 2811302888, 'author': 'tirkarthi', 'body': 'cc: @kaxil @ashb @jedcunningham', 'created_at': datetime.datetime(2025, 1, 26, 3, 52, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614705629, 'issue_id': 2811302888, 'author': 'jedcunningham', 'body': ""Thanks @tirkarthi. I'll look into it this coming week."", 'created_at': datetime.datetime(2025, 1, 27, 1, 39, 22, tzinfo=datetime.timezone.utc)}]","tirkarthi (Issue Creator) on (2025-01-26 03:52:10 UTC): cc: @kaxil @ashb @jedcunningham

jedcunningham (Assginee) on (2025-01-27 01:39:22 UTC): Thanks @tirkarthi. I'll look into it this coming week.

"
2811212785,issue,closed,completed,Convert remaining providers to the new structure,"We have now a number of providers converted to the new structure. We have almost 90 of them to be converted. And we have automated scripts that do bulk of the work. This was part of #44511

Process of the conversion of each of the provider is easy - and described in https://github.com/apache/airflow/blob/main/dev/moving_providers/README.md

The devlist announcement: https://lists.apache.org/thread/dzbj5yx5kwpbwyr5yscp4wnlsp6p9v8l

The list below keeps track of the conversion. The list is generated via:

```
find providers/src -name 'provider.yaml' | sed 's/providers\/src\/airflow\/providers/ - [ ] /' | sed s'/\/provider.yaml//'  | sor
```

 - [x] /amazon
 - [x] /apache/beam
 - [x] /apache/cassandra
 - [x] /apache/drill
 - [x] /apache/druid
 - [x] /apache/flink
 - [x] /apache/hdfs
 - [x] /apache/hive
 - [x] /apache/impala
 - [x] /apache/kafka
 - [x] /apache/kylin
 - [x] /apache/livy
 - [x] /apache/pig
 - [x] /apache/pinot
 - [x] /apache/spark
 - [x] /apprise
 - [x] /arangodb
 - [x] /asana
 - [x] /atlassian/jira
 - [x] /cloudant
 - [x] /cncf/kubernetes
 - [x] /cohere
 - [x] /common/compat
 - [x] /common/io
 - [x] /databricks
 - [x] /datadog
 - [x] /dbt/cloud
 - [x] /dingding
 - [x] /discord
 - [x] /docker
 - [x] /elasticsearch
 - [x] /exasol
 - [x] /fab
 - [x] /facebook
 - [x] /ftp
 - [x] /github
 - [x] /google
 - [x] /grpc
 - [x] /hashicorp
 - [x] /http
 - [x] /imap
 - [x] /influxdb
 - [x] /jdbc
 - [x] /jenkins
 - [x] /microsoft/azure
 - [x] /microsoft/mssql
 - [x] /microsoft/psrp
 - [x] /microsoft/winrm
 - [x] /mongo
 - [x] /mysql
 - [x] /neo4j
 - [x] /odbc
 - [x] /openai
 - [x] /openfaas
 - [x] /openlineage
 - [x] /opensearch
 - [x] /opsgenie
 - [x] /oracle
 - [x] /pagerduty
 - [x] /papermill
 - [x] /pgvector
 - [x] /pinecone
 - [x] /postgres
 - [x] /presto
 - [x] /qdrant
 - [x] /redis
 - [x] /salesforce
 - [x] /samba
 - [x] /segment
 - [x] /sendgrid
 - [x] /sftp
 - [x] /singularity
 - [x] /slack
 - [x] /smtp
 - [x] /snowflake
 - [x] /sqlite
 - [x] /ssh
 - [x] /tableau
 - [x] /telegram
 - [x] /teradata
 - [x] /trino
 - [x] /vertica
 - [x] /weaviate
 - [x] /yandex
 - [x] /ydb
 - [x] /zendesk


",potiuk,2025-01-25 22:59:55+00:00,"['dabla', 'potiuk', 'ferruzzi', 'sunank200', 'bugraoz93', 'josix', 'rawwar', 'kunaljubce', 'amoghrajesh', 'vatsrahul1001']",2025-02-09 08:34:39+00:00,2025-02-09 08:17:20+00:00,https://github.com/apache/airflow/issues/46045,"[('area:providers', ''), ('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2614176844, 'issue_id': 2811212785, 'author': 'rawwar', 'body': ""I'll work on Zendesk, ydb"", 'created_at': datetime.datetime(2025, 1, 26, 2, 18, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614206567, 'issue_id': 2811212785, 'author': 'Prab-27', 'body': ""I'm working on Trino, Apprise"", 'created_at': datetime.datetime(2025, 1, 26, 4, 28, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614210109, 'issue_id': 2811212785, 'author': 'eladkal', 'body': 'Lets please make sure we merge open PRs for the providers (if we can) before the conversion.', 'created_at': datetime.datetime(2025, 1, 26, 4, 46, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614220982, 'issue_id': 2811212785, 'author': 'vatsrahul1001', 'body': '[PR](https://github.com/apache/airflow/pull/46049) for Weaviate.', 'created_at': datetime.datetime(2025, 1, 26, 5, 36, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614281077, 'issue_id': 2811212785, 'author': 'potiuk', 'body': '> Lets please make sure we merge open PRs for the providers (if we can) before the conversion.\n\nYeah before attempting to convert a particular provider It would be great to check if there are outsanding PRs. I am going through the PRs from the last few weeks to see if they apply - but if anyone will want to take on a provider - just checking if there are no related PRs before will do the job :)', 'created_at': datetime.datetime(2025, 1, 26, 8, 53, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614561045, 'issue_id': 2811212785, 'author': 'josix', 'body': ""I'm working on openai and mongodb"", 'created_at': datetime.datetime(2025, 1, 26, 19, 29, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614572695, 'issue_id': 2811212785, 'author': 'potiuk', 'body': 'Unfortunately a log of those result in conflicts if they are not done in sequence - so you will have to rebase those that have conflicts :(', 'created_at': datetime.datetime(2025, 1, 26, 20, 6, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614606975, 'issue_id': 2811212785, 'author': 'potiuk', 'body': 'We really need to batch the providers now - seems that many of them can be fully automated - so I suggest to select a range of those providers (you can provide multiple providers as input to the move script) and submit them together. That will make resolving of the conflict a bit easier.', 'created_at': datetime.datetime(2025, 1, 26, 21, 58, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614723192, 'issue_id': 2811212785, 'author': 'rawwar', 'body': 'Working on sqlite, redis, vertica - https://github.com/apache/airflow/pull/46101', 'created_at': datetime.datetime(2025, 1, 27, 2, 2, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614830486, 'issue_id': 2811212785, 'author': 'vatsrahul1001', 'body': '> We really need to batch the providers now - seems that many of them can be fully automated - so I suggest to select a range of those providers (you can provide multiple providers as input to the move script) and submit them together. That will make resolving of the conflict a bit easier.\n\nYes I have merged `mysql odbc jenkins pagerduty` providers in [PR](https://github.com/apache/airflow/pull/46102)', 'created_at': datetime.datetime(2025, 1, 27, 4, 33, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614845077, 'issue_id': 2811212785, 'author': 'amoghrajesh', 'body': ""There should not really be an issue related to the migration but I'd prefer not batching them. In case there is an issue, reverting the batch would be uglier than individually isolating and debugging"", 'created_at': datetime.datetime(2025, 1, 27, 4, 52, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614845545, 'issue_id': 2811212785, 'author': 'jason810496', 'body': 'Working on\n- /apache/flink https://github.com/apache/airflow/pull/46132\n- /apache/hdfs https://github.com/apache/airflow/pull/46140\n- /apache/hive https://github.com/apache/airflow/pull/46312\n- /apache/kafka https://github.com/apache/airflow/pull/46110\n- /apache/kylin https://github.com/apache/airflow/pull/46108\n- /apache/livy https://github.com/apache/airflow/pull/46131\n- /apache/pig https://github.com/apache/airflow/pull/46105\n- /apache/pinot https://github.com/apache/airflow/pull/46106\n- /apache/spark https://github.com/apache/airflow/pull/46108\n- sendgrid https://github.com/apache/airflow/pull/46313\n- oracle https://github.com/apache/airflow/pull/46314', 'created_at': datetime.datetime(2025, 1, 27, 4, 52, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614987744, 'issue_id': 2811212785, 'author': 'potiuk', 'body': '> There should not really be an issue related to the migration but I\'d prefer not batching them. In case there is an issue, reverting the batch would be uglier than individually isolating and debugging\n\nYeah. Resolving the conflicts is not **that** bad and actually the more providers we migrate, the less likely conflicts will be - becase all the conflicting stuff is only when you have adjacent changes in the file - i.e. two providers migrated separatelyt that are ""neighbours"". \n\nI think our aim should be to migrate providers semi-randomly :)', 'created_at': datetime.datetime(2025, 1, 27, 7, 8, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614992223, 'issue_id': 2811212785, 'author': 'Prab-27', 'body': 'Working on \n/exasol\n/influxdb\n/apache/impala\n/dingding\n/apache/cassandra\n/yandex\narangodb', 'created_at': datetime.datetime(2025, 1, 27, 7, 12, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2615018004, 'issue_id': 2811212785, 'author': 'josix', 'body': 'working on \n- /common/io - #46111 \n- /databricks - #46207 \n- /fab - #46144 \n- /elasticsearch - #46146 \n- /dbt cloud - #46208\n- /slack - #46209\n- /opensearch - #46210 \n- /github - #46211', 'created_at': datetime.datetime(2025, 1, 27, 7, 31, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2615070781, 'issue_id': 2811212785, 'author': 'kunaljubce', 'body': 'Working on:\n* /apache/beam #46071\n* /microsoft/azure #46254 \n* yandex #46525', 'created_at': datetime.datetime(2025, 1, 27, 8, 6, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2615216345, 'issue_id': 2811212785, 'author': 'amoghrajesh', 'body': '> > There should not really be an issue related to the migration but I\'d prefer not batching them. In case there is an issue, reverting the batch would be uglier than individually isolating and debugging\n> \n> Yeah. Resolving the conflicts is not **that** bad and actually the more providers we migrate, the less likely conflicts will be - becase all the conflicting stuff is only when you have adjacent changes in the file - i.e. two providers migrated separatelyt that are ""neighbours"". \n> \n> I think our aim should be to migrate providers semi-randomly :) \n\n\n\nOkay yeah. No neighbours and semi random is something that will almost always work!!', 'created_at': datetime.datetime(2025, 1, 27, 9, 17, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2622620302, 'issue_id': 2811212785, 'author': 'o-nikolas', 'body': ""Hey folks, I'm going to tackle apache/druid and apache/drill"", 'created_at': datetime.datetime(2025, 1, 29, 19, 16, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2625341372, 'issue_id': 2811212785, 'author': 'o-nikolas', 'body': ""I've just gone through and audited the list and updated the checklist to be accurate of what's complete. Some things are not complete but claimed/in progress. Here is the current list of unclaimed providers. I have crossed out a couple that I'm going to claim and work on as of this message 😃 So there are 5 left unclaimed as far as I can tell. \n- sendgrid\n- salesforce\n- oracle\n- google\n- cncf.kubernetes\n- ~~cloudant~~\n- ~~amazon~~"", 'created_at': datetime.datetime(2025, 1, 30, 19, 4, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626395823, 'issue_id': 2811212785, 'author': 'josix', 'body': 'Working on `salesforce`', 'created_at': datetime.datetime(2025, 1, 31, 6, 19, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626589022, 'issue_id': 2811212785, 'author': 'rawwar', 'body': 'Will pick on google', 'created_at': datetime.datetime(2025, 1, 31, 8, 16, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626683707, 'issue_id': 2811212785, 'author': 'hardeybisey', 'body': 'Will be working on cncf.kubernetes.', 'created_at': datetime.datetime(2025, 1, 31, 8, 55, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626715887, 'issue_id': 2811212785, 'author': 'jason810496', 'body': 'Working on\n- sendgrid\n- oracle', 'created_at': datetime.datetime(2025, 1, 31, 9, 12, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626913762, 'issue_id': 2811212785, 'author': 'dabla', 'body': ""@kunaljubce I don't see the microsoft provider mentioned here, but found it [here](https://github.com/apache/airflow/pull/46254), but apparently it only relates to azure, while I was already moving all 4 of them."", 'created_at': datetime.datetime(2025, 1, 31, 10, 50, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2627414478, 'issue_id': 2811212785, 'author': 'kunaljubce', 'body': ""Hi David,\r\n\r\nYeah, so the providers and their respective PRs are being marked in the\r\nissue created to track this move. I have mentioned that I am taking it up\r\nin the issue comments.\r\n\r\nAlso, I initially planned to do all microsoft provider moves in 1 PR. But\r\nmicrosoft/azure itself is changing 200+ files, so it didn't seem like a\r\ngood idea to punch more providers into it. I am actively working on this\r\none, so feel free to take up the remaining microsoft providers. I will\r\nremove them from my comment in the corresponding issue.\r\n\r\nHope that clarifies :)\r\n\r\nRegards,\r\nKunal\r\n\r\nOn Fri, 31 Jan 2025, 16:21 David Blain, ***@***.***> wrote:\r\n\r\n> @kunaljubce <https://github.com/kunaljubce> I don't see the microsoft\r\n> provider mentioned here, but found it here\r\n> <https://github.com/apache/airflow/pull/46254>, but apparently it only\r\n> relates to azure, while I was already moving all 4 of them.\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/apache/airflow/issues/46045#issuecomment-2626913762>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AIG3THSPYXCGNIEDVWV3MPD2NNIR3AVCNFSM6AAAAABV3Y7SMWVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDMMRWHEYTGNZWGI>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>"", 'created_at': datetime.datetime(2025, 1, 31, 13, 55, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2627467473, 'issue_id': 2811212785, 'author': 'sunank200', 'body': 'Is the change for `/arangodb` done? I can pick that up', 'created_at': datetime.datetime(2025, 1, 31, 14, 19, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2627484959, 'issue_id': 2811212785, 'author': 'kunaljubce', 'body': 'Ankit, check out the issue https://github.com/apache/airflow/issues/46045\r\nand see if anyone has commented regarding taking this up or linked any PRs\r\nfor it. If not, just drop a comment and take it up.\r\n\r\nHope that helps!\r\n\r\nThanks,\r\nKunal\r\n\r\n\r\nOn Fri, 31 Jan 2025, 19:50 Ankit Chaurasia, ***@***.***>\r\nwrote:\r\n\r\n> Is the change for /arangodb done? I can pick that up\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/apache/airflow/issues/46045#issuecomment-2627467473>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AIG3THTGRGUJ7PVUFTMR7K32NOBBVAVCNFSM6AAAAABV3Y7SMWVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDMMRXGQ3DONBXGM>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>', 'created_at': datetime.datetime(2025, 1, 31, 14, 27, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2627858726, 'issue_id': 2811212785, 'author': 'Prab-27', 'body': '> Is the change for `/arangodb` done? I can pick that up\n\nYa. Need to rebase my PR', 'created_at': datetime.datetime(2025, 1, 31, 17, 25, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2627867175, 'issue_id': 2811212785, 'author': 'dabla', 'body': ""> Hi David, Yeah, so the providers and their respective PRs are being marked in the issue created to track this move. I have mentioned that I am taking it up in the issue comments. Also, I initially planned to do all microsoft provider moves in 1 PR. But microsoft/azure itself is changing 200+ files, so it didn't seem like a good idea to punch more providers into it. I am actively working on this one, so feel free to take up the remaining microsoft providers. I will remove them from my comment in the corresponding issue. Hope that clarifies :) Regards, Kunal\n> […](#)\n\nHello Kunal,\n\nYes thank you for the explanation was a bit confused thanks once again ;-)\n\nIndeed would have been too much changes for one PR, I will take over the other ones then.\n\nKind regards,\nDavid"", 'created_at': datetime.datetime(2025, 1, 31, 17, 30, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2628108440, 'issue_id': 2811212785, 'author': 'dabla', 'body': ""I've created [PR ](https://github.com/apache/airflow/pull/46338) to move remaining Microsoft providers"", 'created_at': datetime.datetime(2025, 1, 31, 19, 10, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2628957514, 'issue_id': 2811212785, 'author': 'bugraoz93', 'body': ""I couldn't find any orphan provider, hope I am not missing any 🤞If someone needs extra hands, please let me know. Amazing progress! Kudos all!"", 'created_at': datetime.datetime(2025, 2, 1, 13, 39, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2628957960, 'issue_id': 2811212785, 'author': 'potiuk', 'body': ""> I couldn't find any orphan provider, hope I am not missing any 🤞If someone needs extra hands, please let me know. Amazing progress! Kudos all!\n\nIndeed !!! I am super impressed :)"", 'created_at': datetime.datetime(2025, 2, 1, 13, 40, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2631629176, 'issue_id': 2811212785, 'author': 'ferruzzi', 'body': 'This is amazing, I only noticed the list this morning and it\'s already ""done"".', 'created_at': datetime.datetime(2025, 2, 3, 17, 28, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2634282202, 'issue_id': 2811212785, 'author': 'potiuk', 'body': '> This is amazing, I only noticed the list this morning and it\'s already ""done"".\n\nALMOST', 'created_at': datetime.datetime(2025, 2, 4, 15, 16, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2634555086, 'issue_id': 2811212785, 'author': 'dabla', 'body': ""@potiuk wasn't winrm already converted with my [PR](https://github.com/apache/airflow/pull/46338)?"", 'created_at': datetime.datetime(2025, 2, 4, 16, 59, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2637795215, 'issue_id': 2811212785, 'author': 'potiuk', 'body': ""> [@potiuk](https://github.com/potiuk) wasn't winrm already converted with my [PR](https://github.com/apache/airflow/pull/46338)?\n\nYep. Looks like not."", 'created_at': datetime.datetime(2025, 2, 5, 19, 6, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641019395, 'issue_id': 2811212785, 'author': 'insomnes', 'body': ""@potiuk I don't know if it's already planned, at least I didn't find any mention in this and open issues. But this structure change broke `CODEOWNERS`  (and auto reviewers setting), maybe it should be added to check-list or as a new issue?"", 'created_at': datetime.datetime(2025, 2, 6, 21, 5, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641021408, 'issue_id': 2811212785, 'author': 'potiuk', 'body': ""> [@potiuk](https://github.com/potiuk) I don't know if it's already planned, at least I didn't find any mention in this and open issues. But this structure change broke `CODEOWNERS` (and auto reviewers setting), maybe it should be added to check-list or as a new issue?\n\nAh. Great point :). Will fix it soon. Thanks @insomnes"", 'created_at': datetime.datetime(2025, 2, 6, 21, 6, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641034620, 'issue_id': 2811212785, 'author': 'potiuk', 'body': '@insomnes  https://github.com/apache/airflow/pull/46538 here -> one great thing about the provider move is that it removes a lot of duplication from this file.', 'created_at': datetime.datetime(2025, 2, 6, 21, 11, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641070513, 'issue_id': 2811212785, 'author': 'potiuk', 'body': '5 (!) to go !', 'created_at': datetime.datetime(2025, 2, 6, 21, 20, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2643220173, 'issue_id': 2811212785, 'author': 'potiuk', 'body': 'Three to go: \n\n- [x] amazon - @o-nikolas \n- [x] databricks\n- [x] microsoft.azure - @kunaljubce', 'created_at': datetime.datetime(2025, 2, 7, 15, 16, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2643398527, 'issue_id': 2811212785, 'author': 'potiuk', 'body': 'Two to go !', 'created_at': datetime.datetime(2025, 2, 7, 16, 25, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2643402117, 'issue_id': 2811212785, 'author': 'Prab-27', 'body': ""I 'll work on Amazon"", 'created_at': datetime.datetime(2025, 2, 7, 16, 26, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2643404101, 'issue_id': 2811212785, 'author': 'potiuk', 'body': '@o-nikolas already works on Amazon', 'created_at': datetime.datetime(2025, 2, 7, 16, 27, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2643405238, 'issue_id': 2811212785, 'author': 'kunaljubce', 'body': 'For anyone coming in new, `microsoft.azure` PR is in the works :)', 'created_at': datetime.datetime(2025, 2, 7, 16, 28, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2644049263, 'issue_id': 2811212785, 'author': 'ferruzzi', 'body': ""@potiuk I didn't think it would actually let me do that, but I edited your last comment with the names of the person working on the last two so people don't keep pinging you that they can do them."", 'created_at': datetime.datetime(2025, 2, 7, 20, 20, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2645824792, 'issue_id': 2811212785, 'author': 'potiuk', 'body': ""> [@potiuk](https://github.com/potiuk) I didn't think it would actually let me do that, but I edited your last comment with the names of the person working on the last two so people don't keep pinging you that they can do them.\n\ncool"", 'created_at': datetime.datetime(2025, 2, 8, 16, 28, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2645943098, 'issue_id': 2811212785, 'author': 'potiuk', 'body': 'Just Amazon left  (and almost there: @o-nikolas ) :)', 'created_at': datetime.datetime(2025, 2, 8, 21, 12, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2646121170, 'issue_id': 2811212785, 'author': 'potiuk', 'body': 'DONE ! 🎉 🎉 🎉 🎉 🎉 🎉 🎉 🎉 🎉 🎉 🎉 🎉 🎉 🎉 🎉', 'created_at': datetime.datetime(2025, 2, 9, 8, 18, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2646126236, 'issue_id': 2811212785, 'author': 'jscheffl', 'body': 'Fantastic! Now the cleanup can start!', 'created_at': datetime.datetime(2025, 2, 9, 8, 34, 38, tzinfo=datetime.timezone.utc)}]","rawwar (Assginee) on (2025-01-26 02:18:10 UTC): I'll work on Zendesk, ydb

Prab-27 on (2025-01-26 04:28:20 UTC): I'm working on Trino, Apprise

eladkal on (2025-01-26 04:46:09 UTC): Lets please make sure we merge open PRs for the providers (if we can) before the conversion.

vatsrahul1001 (Assginee) on (2025-01-26 05:36:42 UTC): [PR](https://github.com/apache/airflow/pull/46049) for Weaviate.

potiuk (Issue Creator) on (2025-01-26 08:53:30 UTC): Yeah before attempting to convert a particular provider It would be great to check if there are outsanding PRs. I am going through the PRs from the last few weeks to see if they apply - but if anyone will want to take on a provider - just checking if there are no related PRs before will do the job :)

josix (Assginee) on (2025-01-26 19:29:59 UTC): I'm working on openai and mongodb

potiuk (Issue Creator) on (2025-01-26 20:06:56 UTC): Unfortunately a log of those result in conflicts if they are not done in sequence - so you will have to rebase those that have conflicts :(

potiuk (Issue Creator) on (2025-01-26 21:58:31 UTC): We really need to batch the providers now - seems that many of them can be fully automated - so I suggest to select a range of those providers (you can provide multiple providers as input to the move script) and submit them together. That will make resolving of the conflict a bit easier.

rawwar (Assginee) on (2025-01-27 02:02:16 UTC): Working on sqlite, redis, vertica - https://github.com/apache/airflow/pull/46101

vatsrahul1001 (Assginee) on (2025-01-27 04:33:39 UTC): Yes I have merged `mysql odbc jenkins pagerduty` providers in [PR](https://github.com/apache/airflow/pull/46102)

amoghrajesh (Assginee) on (2025-01-27 04:52:01 UTC): There should not really be an issue related to the migration but I'd prefer not batching them. In case there is an issue, reverting the batch would be uglier than individually isolating and debugging

jason810496 on (2025-01-27 04:52:36 UTC): Working on
- /apache/flink https://github.com/apache/airflow/pull/46132
- /apache/hdfs https://github.com/apache/airflow/pull/46140
- /apache/hive https://github.com/apache/airflow/pull/46312
- /apache/kafka https://github.com/apache/airflow/pull/46110
- /apache/kylin https://github.com/apache/airflow/pull/46108
- /apache/livy https://github.com/apache/airflow/pull/46131
- /apache/pig https://github.com/apache/airflow/pull/46105
- /apache/pinot https://github.com/apache/airflow/pull/46106
- /apache/spark https://github.com/apache/airflow/pull/46108
- sendgrid https://github.com/apache/airflow/pull/46313
- oracle https://github.com/apache/airflow/pull/46314

potiuk (Issue Creator) on (2025-01-27 07:08:50 UTC): Yeah. Resolving the conflicts is not **that** bad and actually the more providers we migrate, the less likely conflicts will be - becase all the conflicting stuff is only when you have adjacent changes in the file - i.e. two providers migrated separatelyt that are ""neighbours"". 

I think our aim should be to migrate providers semi-randomly :)

Prab-27 on (2025-01-27 07:12:20 UTC): Working on 
/exasol
/influxdb
/apache/impala
/dingding
/apache/cassandra
/yandex
arangodb

josix (Assginee) on (2025-01-27 07:31:05 UTC): working on 
- /common/io - #46111 
- /databricks - #46207 
- /fab - #46144 
- /elasticsearch - #46146 
- /dbt cloud - #46208
- /slack - #46209
- /opensearch - #46210 
- /github - #46211

kunaljubce (Assginee) on (2025-01-27 08:06:01 UTC): Working on:
* /apache/beam #46071
* /microsoft/azure #46254 
* yandex #46525

amoghrajesh (Assginee) on (2025-01-27 09:17:52 UTC): Okay yeah. No neighbours and semi random is something that will almost always work!!

o-nikolas on (2025-01-29 19:16:36 UTC): Hey folks, I'm going to tackle apache/druid and apache/drill

o-nikolas on (2025-01-30 19:04:28 UTC): I've just gone through and audited the list and updated the checklist to be accurate of what's complete. Some things are not complete but claimed/in progress. Here is the current list of unclaimed providers. I have crossed out a couple that I'm going to claim and work on as of this message 😃 So there are 5 left unclaimed as far as I can tell. 
- sendgrid
- salesforce
- oracle
- google
- cncf.kubernetes
- ~~cloudant~~
- ~~amazon~~

josix (Assginee) on (2025-01-31 06:19:09 UTC): Working on `salesforce`

rawwar (Assginee) on (2025-01-31 08:16:04 UTC): Will pick on google

hardeybisey on (2025-01-31 08:55:51 UTC): Will be working on cncf.kubernetes.

jason810496 on (2025-01-31 09:12:36 UTC): Working on
- sendgrid
- oracle

dabla (Assginee) on (2025-01-31 10:50:46 UTC): @kunaljubce I don't see the microsoft provider mentioned here, but found it [here](https://github.com/apache/airflow/pull/46254), but apparently it only relates to azure, while I was already moving all 4 of them.

kunaljubce (Assginee) on (2025-01-31 13:55:54 UTC): Hi David,

Yeah, so the providers and their respective PRs are being marked in the
issue created to track this move. I have mentioned that I am taking it up
in the issue comments.

Also, I initially planned to do all microsoft provider moves in 1 PR. But
microsoft/azure itself is changing 200+ files, so it didn't seem like a
good idea to punch more providers into it. I am actively working on this
one, so feel free to take up the remaining microsoft providers. I will
remove them from my comment in the corresponding issue.

Hope that clarifies :)

Regards,
Kunal

On Fri, 31 Jan 2025, 16:21 David Blain, ***@***.***> wrote:

sunank200 (Assginee) on (2025-01-31 14:19:46 UTC): Is the change for `/arangodb` done? I can pick that up

kunaljubce (Assginee) on (2025-01-31 14:27:52 UTC): Ankit, check out the issue https://github.com/apache/airflow/issues/46045
and see if anyone has commented regarding taking this up or linked any PRs
for it. If not, just drop a comment and take it up.

Hope that helps!

Thanks,
Kunal


On Fri, 31 Jan 2025, 19:50 Ankit Chaurasia, ***@***.***>
wrote:

Prab-27 on (2025-01-31 17:25:04 UTC): Ya. Need to rebase my PR

dabla (Assginee) on (2025-01-31 17:30:11 UTC): Hello Kunal,

Yes thank you for the explanation was a bit confused thanks once again ;-)

Indeed would have been too much changes for one PR, I will take over the other ones then.

Kind regards,
David

dabla (Assginee) on (2025-01-31 19:10:25 UTC): I've created [PR ](https://github.com/apache/airflow/pull/46338) to move remaining Microsoft providers

bugraoz93 (Assginee) on (2025-02-01 13:39:19 UTC): I couldn't find any orphan provider, hope I am not missing any 🤞If someone needs extra hands, please let me know. Amazing progress! Kudos all!

potiuk (Issue Creator) on (2025-02-01 13:40:41 UTC): Indeed !!! I am super impressed :)

ferruzzi (Assginee) on (2025-02-03 17:28:31 UTC): This is amazing, I only noticed the list this morning and it's already ""done"".

potiuk (Issue Creator) on (2025-02-04 15:16:24 UTC): ALMOST

dabla (Assginee) on (2025-02-04 16:59:57 UTC): @potiuk wasn't winrm already converted with my [PR](https://github.com/apache/airflow/pull/46338)?

potiuk (Issue Creator) on (2025-02-05 19:06:22 UTC): Yep. Looks like not.

insomnes on (2025-02-06 21:05:40 UTC): @potiuk I don't know if it's already planned, at least I didn't find any mention in this and open issues. But this structure change broke `CODEOWNERS`  (and auto reviewers setting), maybe it should be added to check-list or as a new issue?

potiuk (Issue Creator) on (2025-02-06 21:06:52 UTC): Ah. Great point :). Will fix it soon. Thanks @insomnes

potiuk (Issue Creator) on (2025-02-06 21:11:13 UTC): @insomnes  https://github.com/apache/airflow/pull/46538 here -> one great thing about the provider move is that it removes a lot of duplication from this file.

potiuk (Issue Creator) on (2025-02-06 21:20:41 UTC): 5 (!) to go !

potiuk (Issue Creator) on (2025-02-07 15:16:03 UTC): Three to go: 

- [x] amazon - @o-nikolas 
- [x] databricks
- [x] microsoft.azure - @kunaljubce

potiuk (Issue Creator) on (2025-02-07 16:25:01 UTC): Two to go !

Prab-27 on (2025-02-07 16:26:39 UTC): I 'll work on Amazon

potiuk (Issue Creator) on (2025-02-07 16:27:33 UTC): @o-nikolas already works on Amazon

kunaljubce (Assginee) on (2025-02-07 16:28:05 UTC): For anyone coming in new, `microsoft.azure` PR is in the works :)

ferruzzi (Assginee) on (2025-02-07 20:20:25 UTC): @potiuk I didn't think it would actually let me do that, but I edited your last comment with the names of the person working on the last two so people don't keep pinging you that they can do them.

potiuk (Issue Creator) on (2025-02-08 16:28:24 UTC): cool

potiuk (Issue Creator) on (2025-02-08 21:12:26 UTC): Just Amazon left  (and almost there: @o-nikolas ) :)

potiuk (Issue Creator) on (2025-02-09 08:18:06 UTC): DONE ! 🎉 🎉 🎉 🎉 🎉 🎉 🎉 🎉 🎉 🎉 🎉 🎉 🎉 🎉 🎉

jscheffl on (2025-02-09 08:34:38 UTC): Fantastic! Now the cleanup can start!

"
2810831787,issue,open,,ASF enforcing content policy as of end Feb 2025,The ASF is enforcing (as of end of Feb) content policy (for privacy reasons) and we need to convert our website to only load stuff from our page.,potiuk,2025-01-25 07:42:21+00:00,"['potiuk', 'gopidesupavan']",2025-01-25 07:50:08+00:00,,https://github.com/apache/airflow/issues/46036,[],"[{'comment_id': 2613828900, 'issue_id': 2810831787, 'author': 'potiuk', 'body': ""Already done for most Javascript, Fonts and GitHub images.\n\nWhat's left are videos from youtube in meetup: \n\nHere: https://airflow.apache.org/meetups/\n\nWhich should be done according to the example shown https://privacy.apache.org/examples/youtube-html/with-youtube-embeds.html"", 'created_at': datetime.datetime(2025, 1, 25, 7, 45, 11, tzinfo=datetime.timezone.utc)}]","potiuk (Issue Creator) on (2025-01-25 07:45:11 UTC): Already done for most Javascript, Fonts and GitHub images.

What's left are videos from youtube in meetup: 

Here: https://airflow.apache.org/meetups/

Which should be done according to the example shown https://privacy.apache.org/examples/youtube-html/with-youtube-embeds.html

"
2810212631,issue,closed,not_planned,Missing Bug Report Template in Apache Airflow Repository,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

N/A – This issue pertains to the repository’s configuration and does not depend on a specific Airflow version.

### What happened?

Currently, the Apache Airflow repository does not include a bug report template. This makes it difficult for contributors to provide detailed and consistent information when reporting issues. As a result, maintainers may face delays in reproducing and resolving reported bugs due to incomplete or unclear submissions.



### What you think should happen instead?

The repository should include a standardized bug report template that:  
- Guides contributors to provide detailed and consistent information.  
- Includes structured sections like ""Bug Description,"" ""Steps to Reproduce,"" ""Expected vs. Actual Behavior,"" and ""Environment Details.""  

This would improve issue triage efficiency and reduce the back-and-forth communication required to clarify incomplete bug reports.  


### How to reproduce

1. Go to the Apache Airflow repository.  
2. Navigate to the **Issues** tab and click on **New Issue**.  
3. Observe that there is no pre-existing template specifically for bug reporting.  


### Operating System

N/A

### Versions of Apache Airflow Providers

2.10.4

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Shinkhal,2025-01-24 19:10:36+00:00,[],2025-01-25 06:46:57+00:00,2025-01-25 06:46:50+00:00,https://github.com/apache/airflow/issues/46027,"[('kind:bug', 'This is a clearly a bug'), ('kind:meta', 'High-level information important to the community'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2613210104, 'issue_id': 2810212631, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 24, 19, 10, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2613277037, 'issue_id': 2810212631, 'author': 'Shinkhal', 'body': 'Sure , Will do.', 'created_at': datetime.datetime(2025, 1, 24, 19, 54, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2613813621, 'issue_id': 2810212631, 'author': 'eladkal', 'body': 'Spam', 'created_at': datetime.datetime(2025, 1, 25, 6, 46, 50, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-24 19:10:40 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

Shinkhal (Issue Creator) on (2025-01-24 19:54:48 UTC): Sure , Will do.

eladkal on (2025-01-25 06:46:50 UTC): Spam

"
2810079109,issue,open,,HIVE SSL mTLS capability,"### Description

Add HIVE provider capability to use SSL and perform mTLS handshake with connecting host


### Use case/motivation

Airflow runs in a different network from HIVE due to policy and company support.
Require mTLS encrypted connection between the two instances to securely run HIVE jobs remotely.

### Related issues

None that I am aware of

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",alexio215,2025-01-24 18:10:05+00:00,['alexio215'],2025-02-05 17:45:51+00:00,,https://github.com/apache/airflow/issues/46023,"[('area:providers', ''), ('kind:feature', 'Feature Requests'), ('provider:apache-hive', '')]","[{'comment_id': 2613115204, 'issue_id': 2810079109, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 24, 18, 10, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2613116453, 'issue_id': 2810079109, 'author': 'alexio215', 'body': 'Hello, thank you for having me. It is my first time contributing to any open source project, so please bear with me.\n\nHappy to learn from any wisdom shared', 'created_at': datetime.datetime(2025, 1, 24, 18, 10, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2613128436, 'issue_id': 2810079109, 'author': 'alexio215', 'body': 'My current inclination in solving this problem is to use [jpype](https://pypi.org/project/jpype1/) to funnel python requests through a JVM running the JDBC Driver adjacent to Airflow.\n\nThe goal with this is to use the Python natively to write HIVE DAGs but communicate in Java which is more native to HIVE and supports the JKS key format, which is the default to HIVE', 'created_at': datetime.datetime(2025, 1, 24, 18, 18, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2613250678, 'issue_id': 2810079109, 'author': 'alexio215', 'body': 'On pause for now and considering ramifications of this in my environment', 'created_at': datetime.datetime(2025, 1, 24, 19, 37, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2622767231, 'issue_id': 2810079109, 'author': 'nevcohen', 'body': ""The `hive cli` or `hive server 2` [connections](https://airflow.apache.org/docs/apache-airflow-providers-apache-hive/stable/connections/index.html) don't work for you?"", 'created_at': datetime.datetime(2025, 1, 29, 20, 25, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2626422301, 'issue_id': 2810079109, 'author': 'alexio215', 'body': ""So I'm currently looking at using two capabilities. The first, is to connect to an NGINX proxy that requires SSL certs and expects mTLS to serve HIVE commands locally through our cluster, into the HIVE2SERVER running right behind it. The second, down the line that I am hoping for, is to find or create support for direct connection with pyHIVE to HIVE2SERVER running with SSL, and to perform mTLS. The problem with this however is that I notice that python does not natively support the .jks format that HIVE2SERVER expects, hence the use of an NGINX proxy. However, looking at pyHIVE, and its most recent issues, to me it seems that pyHIVE as well does not support SSL connection:\nhttps://github.com/dropbox/PyHive/issues/257\n\nForgive me for any misunderstanding as well, this is all a learning process to me at the same time. Thank you for the patience and help @nevcohen"", 'created_at': datetime.datetime(2025, 1, 31, 6, 41, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629484716, 'issue_id': 2810079109, 'author': 'nevcohen', 'body': ""> So I'm currently looking at using two capabilities. The first, is to connect to an NGINX proxy that requires SSL certs and expects mTLS to serve HIVE commands locally through our cluster, into the HIVE2SERVER running right behind it. The second, down the line that I am hoping for, is to find or create support for direct connection with pyHIVE to HIVE2SERVER running with SSL, and to perform mTLS. The problem with this however is that I notice that python does not natively support the .jks format that HIVE2SERVER expects, hence the use of an NGINX proxy. However, looking at pyHIVE, and its most recent issues, to me it seems that pyHIVE as well does not support SSL connection:\n> https://github.com/dropbox/PyHive/issues/257\n> \n> Forgive me for any misunderstanding as well, this is all a learning process to me at the same time. Thank you for the patience and help @nevcohen \n\nSo today how do you connect to hive using a code?"", 'created_at': datetime.datetime(2025, 2, 2, 17, 36, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2632079316, 'issue_id': 2810079109, 'author': 'alexio215', 'body': '> > So I\'m currently looking at using two capabilities. The first, is to connect to an NGINX proxy that requires SSL certs and expects mTLS to serve HIVE commands locally through our cluster, into the HIVE2SERVER running right behind it. The second, down the line that I am hoping for, is to find or create support for direct connection with pyHIVE to HIVE2SERVER running with SSL, and to perform mTLS. The problem with this however is that I notice that python does not natively support the .jks format that HIVE2SERVER expects, hence the use of an NGINX proxy. However, looking at pyHIVE, and its most recent issues, to me it seems that pyHIVE as well does not support SSL connection:\n> > [dropbox/PyHive#257](https://github.com/dropbox/PyHive/issues/257)\n> > Forgive me for any misunderstanding as well, this is all a learning process to me at the same time. Thank you for the patience and help [@nevcohen](https://github.com/nevcohen)\n> \n> So today how do you connect to hive using a code?\n\nThank you for the patience, this has taken some digging on my end, getting accustomed to what is currently practiced in my org. Currently our pyHive queries are written a more manual script and sent to a NGINX server that redirects appropriate traffic to a Hive2Server proxy. The Thrift communication is wrapped in HTTPS using the THTTPClient module from the Thrift library. I have found this to exist within pyHive as well.\n\nThis lives and is made accessible within the Connection method of pyHive\n`if scheme in (""https"", ""http"") and thrift_transport is None:\n            port = port or 1000\n            ssl_context = None\n            if scheme == ""https"":\n                ssl_context = create_default_context()\n                ssl_context.check_hostname = check_hostname == ""true""\n                ssl_cert = ssl_cert or ""none""\n                ssl_context.verify_mode = ssl_cert_parameter_map.get(ssl_cert, CERT_NONE)\n            thrift_transport = thrift.transport.THttpClient.THttpClient(\n                uri_or_host=""{scheme}://{host}:{port}/cliservice/"".format(\n                    scheme=scheme, host=host, port=port\n                ),\n                ssl_context=ssl_context,\n            )`\n\nMy goal is to add a method using the ssl library that creates ssl context using the extras provided and appends them to the connection being created if a ""use_https_proxy"" boolean is specified within the proxy. Further, a ""enable_mtls"" boolean option will be included to allow for cases where someone needs to use mTLS.', 'created_at': datetime.datetime(2025, 2, 3, 21, 7, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2632478556, 'issue_id': 2810079109, 'author': 'alexio215', 'body': '> > > So I\'m currently looking at using two capabilities. The first, is to connect to an NGINX proxy that requires SSL certs and expects mTLS to serve HIVE commands locally through our cluster, into the HIVE2SERVER running right behind it. The second, down the line that I am hoping for, is to find or create support for direct connection with pyHIVE to HIVE2SERVER running with SSL, and to perform mTLS. The problem with this however is that I notice that python does not natively support the .jks format that HIVE2SERVER expects, hence the use of an NGINX proxy. However, looking at pyHIVE, and its most recent issues, to me it seems that pyHIVE as well does not support SSL connection:\n> > > [dropbox/PyHive#257](https://github.com/dropbox/PyHive/issues/257)\n> > > Forgive me for any misunderstanding as well, this is all a learning process to me at the same time. Thank you for the patience and help [@nevcohen](https://github.com/nevcohen)\n> > \n> > \n> > So today how do you connect to hive using a code?\n> \n> Thank you for the patience, this has taken some digging on my end, getting accustomed to what is currently practiced in my org. Currently our pyHive queries are written a more manual script and sent to a NGINX server that redirects appropriate traffic to a Hive2Server proxy. The Thrift communication is wrapped in HTTPS using the THTTPClient module from the Thrift library. I have found this to exist within pyHive as well.\n> \n> This lives and is made accessible within the Connection method of pyHive `if scheme in (""https"", ""http"") and thrift_transport is None: port = port or 1000 ssl_context = None if scheme == ""https"": ssl_context = create_default_context() ssl_context.check_hostname = check_hostname == ""true"" ssl_cert = ssl_cert or ""none"" ssl_context.verify_mode = ssl_cert_parameter_map.get(ssl_cert, CERT_NONE) thrift_transport = thrift.transport.THttpClient.THttpClient( uri_or_host=""{scheme}://{host}:{port}/cliservice/"".format( scheme=scheme, host=host, port=port ), ssl_context=ssl_context, )`\n> \n> My goal is to add a method using the ssl library that creates ssl context using the extras provided and appends them to the connection being created if a ""use_https_proxy"" boolean is specified within the proxy. Further, a ""enable_mtls"" boolean option will be included to allow for cases where someone needs to use mTLS.\n\nFinding a way to do this through the pyHive scheme and default constructor parameters', 'created_at': datetime.datetime(2025, 2, 4, 0, 22, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2637612311, 'issue_id': 2810079109, 'author': 'nevcohen', 'body': ""I think I understand what you want to do, the way I see it there are two options. \n\n1. Open a PR and if it's something simple and relevant it will also be promoted in the open source (I would love to do a CR for you).\n2. Implement your own wrapper for the operator in your organization."", 'created_at': datetime.datetime(2025, 2, 5, 17, 45, 50, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-24 18:10:07 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

alexio215 (Issue Creator) on (2025-01-24 18:10:49 UTC): Hello, thank you for having me. It is my first time contributing to any open source project, so please bear with me.

Happy to learn from any wisdom shared

alexio215 (Issue Creator) on (2025-01-24 18:18:18 UTC): My current inclination in solving this problem is to use [jpype](https://pypi.org/project/jpype1/) to funnel python requests through a JVM running the JDBC Driver adjacent to Airflow.

The goal with this is to use the Python natively to write HIVE DAGs but communicate in Java which is more native to HIVE and supports the JKS key format, which is the default to HIVE

alexio215 (Issue Creator) on (2025-01-24 19:37:17 UTC): On pause for now and considering ramifications of this in my environment

nevcohen on (2025-01-29 20:25:04 UTC): The `hive cli` or `hive server 2` [connections](https://airflow.apache.org/docs/apache-airflow-providers-apache-hive/stable/connections/index.html) don't work for you?

alexio215 (Issue Creator) on (2025-01-31 06:41:54 UTC): So I'm currently looking at using two capabilities. The first, is to connect to an NGINX proxy that requires SSL certs and expects mTLS to serve HIVE commands locally through our cluster, into the HIVE2SERVER running right behind it. The second, down the line that I am hoping for, is to find or create support for direct connection with pyHIVE to HIVE2SERVER running with SSL, and to perform mTLS. The problem with this however is that I notice that python does not natively support the .jks format that HIVE2SERVER expects, hence the use of an NGINX proxy. However, looking at pyHIVE, and its most recent issues, to me it seems that pyHIVE as well does not support SSL connection:
https://github.com/dropbox/PyHive/issues/257

Forgive me for any misunderstanding as well, this is all a learning process to me at the same time. Thank you for the patience and help @nevcohen

nevcohen on (2025-02-02 17:36:32 UTC): So today how do you connect to hive using a code?

alexio215 (Issue Creator) on (2025-02-03 21:07:59 UTC): Thank you for the patience, this has taken some digging on my end, getting accustomed to what is currently practiced in my org. Currently our pyHive queries are written a more manual script and sent to a NGINX server that redirects appropriate traffic to a Hive2Server proxy. The Thrift communication is wrapped in HTTPS using the THTTPClient module from the Thrift library. I have found this to exist within pyHive as well.

This lives and is made accessible within the Connection method of pyHive
`if scheme in (""https"", ""http"") and thrift_transport is None:
            port = port or 1000
            ssl_context = None
            if scheme == ""https"":
                ssl_context = create_default_context()
                ssl_context.check_hostname = check_hostname == ""true""
                ssl_cert = ssl_cert or ""none""
                ssl_context.verify_mode = ssl_cert_parameter_map.get(ssl_cert, CERT_NONE)
            thrift_transport = thrift.transport.THttpClient.THttpClient(
                uri_or_host=""{scheme}://{host}:{port}/cliservice/"".format(
                    scheme=scheme, host=host, port=port
                ),
                ssl_context=ssl_context,
            )`

My goal is to add a method using the ssl library that creates ssl context using the extras provided and appends them to the connection being created if a ""use_https_proxy"" boolean is specified within the proxy. Further, a ""enable_mtls"" boolean option will be included to allow for cases where someone needs to use mTLS.

alexio215 (Issue Creator) on (2025-02-04 00:22:24 UTC): Finding a way to do this through the pyHive scheme and default constructor parameters

nevcohen on (2025-02-05 17:45:50 UTC): I think I understand what you want to do, the way I see it there are two options. 

1. Open a PR and if it's something simple and relevant it will also be promoted in the open source (I would love to do a CR for you).
2. Implement your own wrapper for the operator in your organization.

"
2809840889,issue,closed,not_planned,openlineage: readd few removed Task properties in AirflowRunFacet #40371,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

In PR #38264, a few Task properties in the AirflowRunFacet were accidentally removed. This caused missing serialization of these properties, which are necessary for proper functionality in the OpenLineage provider.


### What you think should happen instead?

The removed Task properties should be serialized in the AirflowRunFacet as they were before PR #38264. This ensures compatibility and correct functionality for OpenLineage integration.


### How to reproduce

Use an Airflow deployment with OpenLineage provider enabled.

Check the serialized output of AirflowRunFacet for Task properties.

Observe that certain properties are missing after PR #38264.

### Operating System

N/A (This is a code-level issue, not OS-specific.)

### Versions of Apache Airflow Providers

[providers-ydb/1.0.0rc1](https://github.com/apache/airflow/releases/tag/providers-ydb%2F1.0.0rc1)

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

N/A  (This is a code-level issue, not deployment-specific.)

### Anything else?

### Frequency
This issue occurs consistently for any Task using the affected AirflowRunFacet properties.

### Fix
The issue was resolved in PR #40371, which reintroduced the missing properties.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Mohammed-Karim226,2025-01-24 16:25:29+00:00,[],2025-01-25 08:35:39+00:00,2025-01-25 08:35:32+00:00,https://github.com/apache/airflow/issues/46016,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:openlineage', 'AIP-53'), ('AI Spam', '')]","[{'comment_id': 2613842862, 'issue_id': 2809840889, 'author': 'eladkal', 'body': 'Ai bot', 'created_at': datetime.datetime(2025, 1, 25, 8, 35, 32, tzinfo=datetime.timezone.utc)}]","eladkal on (2025-01-25 08:35:32 UTC): Ai bot

"
2809726114,issue,closed,completed,Deferred operator causing deadlock and Airflow falled down,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.6.3

### What happened?

We used defered operator waiting for something in several instances during high mysql database server load.
We got following issue of airflow core two times:

```
Exception when executing TriggererJobRunner._run_trigger_loop
Traceback (most recent call last):
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context
    self.dialect.do_execute(
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/sqlalchemy/engine/default.py"", line 736, in do_execute
    cursor.execute(statement, parameters)
  File ""/opt/gts/3pp/python/lib/python3.9/site-packages/pymysql/cursors.py"", line 153, in execute
    result = self._query(query)
  File ""/opt/gts/3pp/python/lib/python3.9/site-packages/pymysql/cursors.py"", line 322, in _query
    conn.query(q)
  File ""/opt/gts/3pp/python/lib/python3.9/site-packages/pymysql/connections.py"", line 558, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File ""/opt/gts/3pp/python/lib/python3.9/site-packages/pymysql/connections.py"", line 822, in _read_query_result
    result.read()
  File ""/opt/gts/3pp/python/lib/python3.9/site-packages/pymysql/connections.py"", line 1200, in read
    first_packet = self.connection._read_packet()
  File ""/opt/gts/3pp/python/lib/python3.9/site-packages/pymysql/connections.py"", line 772, in _read_packet
    packet.raise_for_error()
  File ""/opt/gts/3pp/python/lib/python3.9/site-packages/pymysql/protocol.py"", line 221, in raise_for_error
    err.raise_mysql_exception(self._data)
  File ""/opt/gts/3pp/python/lib/python3.9/site-packages/pymysql/err.py"", line 143, in raise_mysql_exception
    raise errorclass(errno, errval)
pymysql.err.OperationalError: (1213, 'Deadlock found when trying to get lock; try restarting transaction')

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/airflow/jobs/triggerer_job_runner.py"", line 333, in _execute
    self._run_trigger_loop()
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/airflow/jobs/triggerer_job_runner.py"", line 362, in _run_trigger_loop
    self.handle_events()
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/airflow/jobs/triggerer_job_runner.py"", line 390, in handle_events
    Trigger.submit_event(trigger_id=trigger_id, event=event)
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/airflow/api_internal/internal_api_call.py"", line 112, in wrapper
    return func(*args, **kwargs)
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/airflow/utils/session.py"", line 76, in wrapper
    return func(*args, session=session, **kwargs)
  File ""/opt/gts/3pp/python/lib/python3.9/contextlib.py"", line 126, in __exit__
    next(self.gen)
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/airflow/utils/session.py"", line 37, in create_session
    session.commit()
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 832, in commit
    self._prepare_impl()
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 811, in _prepare_impl
    self.session.flush()
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 3449, in flush
    self._flush(objects)
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 3589, in _flush
    transaction.rollback(_capture_exception=True)
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/sqlalchemy/util/compat.py"", line 211, in raise_
    raise exception
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/sqlalchemy/orm/session.py"", line 3549, in _flush
    flush_context.execute()
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/sqlalchemy/orm/unitofwork.py"", line 456, in execute
    rec.execute(self)
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/sqlalchemy/orm/unitofwork.py"", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/sqlalchemy/orm/persistence.py"", line 237, in save_obj
    _emit_update_statements(
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/sqlalchemy/orm/persistence.py"", line 1001, in _emit_update_statements
    c = connection._execute_20(
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/sqlalchemy/sql/elements.py"", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 2134, in _handle_dbapi_exception
    util.raise_(
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/sqlalchemy/util/compat.py"", line 211, in raise_
    raise exception
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context
    self.dialect.do_execute(
  File ""/opt/gts/3pp/airflow/lib/python3.9/site-packages/sqlalchemy/engine/default.py"", line 736, in do_execute
    cursor.execute(statement, parameters)
  File ""/opt/gts/3pp/python/lib/python3.9/site-packages/pymysql/cursors.py"", line 153, in execute
    result = self._query(query)
  File ""/opt/gts/3pp/python/lib/python3.9/site-packages/pymysql/cursors.py"", line 322, in _query
    conn.query(q)
  File ""/opt/gts/3pp/python/lib/python3.9/site-packages/pymysql/connections.py"", line 558, in query
    self._affected_rows = self._read_query_result(unbuffered=unbuffered)
  File ""/opt/gts/3pp/python/lib/python3.9/site-packages/pymysql/connections.py"", line 822, in _read_query_result
    result.read()
  File ""/opt/gts/3pp/python/lib/python3.9/site-packages/pymysql/connections.py"", line 1200, in read
    first_packet = self.connection._read_packet()
  File ""/opt/gts/3pp/python/lib/python3.9/site-packages/pymysql/connections.py"", line 772, in _read_packet
    packet.raise_for_error()
  File ""/opt/gts/3pp/python/lib/python3.9/site-packages/pymysql/protocol.py"", line 221, in raise_for_error
    err.raise_mysql_exception(self._data)
  File ""/opt/gts/3pp/python/lib/python3.9/site-packages/pymysql/err.py"", line 143, in raise_mysql_exception
    raise errorclass(errno, errval)
sqlalchemy.exc.OperationalError: (pymysql.err.OperationalError) (1213, 'Deadlock found when trying to get lock; try restarting transaction')

[SQL: UPDATE task_instance SET state=%(state)s, updated_at=%(updated_at)s, 
      trigger_id=%(trigger_id)s, next_kwargs=%(next_kwargs)s 
      WHERE task_instance.dag_id = %(task_instance_dag_id)s 
        AND task_instance.task_id = %(task_instance_task_id)s 
        AND task_instance.run_id = %(task_instance_run_id)s 
        AND task_instance.map_index = %(task_instance_map_index)s]
[parameters: {
    'state': <TaskInstanceState.SCHEDULED: 'scheduled'>, 
    'updated_at': datetime.datetime(2024, 12, 12, 10, 37, 41, 847579), 
    'trigger_id': None, 
    'next_kwargs': '{""__var"": {""event"": {""__var"": 1733999860.954081, ""__type"": ""datetime""}}, ""__type"": ""dict""}', 
    'task_instance_dag_id': 'AE_executor', 
    'task_instance_task_id': 'RFF.RFF_WAIT_FOR_IVS_DATA_CLEANSING_STATUS', 
    'task_instance_run_id': 'manual__2024-12-12T08:01:19+00:00', 
    'task_instance_map_index': -1
}]
```


### What you think should happen instead?

Recommended from mysql is re-trigger the operation as is already implemented in airflow for example when fixing: https://github.com/apache/airflow/issues/41428

### How to reproduce

I created code to trigger the function same way as the code raised the issue:

```
from operators.reschedule import RescheduleOperator
from airflow.operators.python import BaseOperator
from airflow.operators.bash import BashOperator
from operators.prisma_empty import PrismaEmptyOperator
from airflow import DAG
from datetime import datetime, timedelta
from common.logger import task_logger
from airflow.triggers.temporal import TimeDeltaTrigger

class EmptyOperator(BashOperator):
    '''
    Empty operator does literally nothing. To be used for example as placeholder for
    not-yet-implemented job.
    '''

    def __init__(self, *args, **kwargs):
        super().__init__(
            bash_command=""echo 'No action'"",
            *args, **kwargs)


class WaitForOperator(BaseOperator):
    def __init__(self, *,
                 poll_interval = 0.5,
                 **kwargs):

        self.timeout_deadline = datetime.now() + timedelta(days = 1)
        self.poll_interval = timedelta(seconds=poll_interval)

        super().__init__(**kwargs)

    def execute(self, context):
        now = datetime.now()
        deadline = self.timeout_deadline
        if deadline < now:
            task_logger.info(f""Deadline time is in the past. Exiting."")
            return

        task_logger.info(f""Setting deadline to: {deadline}"")
        time_diff = (deadline - now).total_seconds()
        self.xcom_push(context, key=""timeout_rerun_sec"", value=time_diff)
        self.xcom_push(context, key=""start_time"", value=now.isoformat())
        self.xcom_push(context, key=""deadline"", value=deadline.isoformat())

        self.deferred(context)

    def evaluate_condition(self):
        return False

    # pylint: disable=unused-argument
    def deferred(self, context, event=None):
        """"""deferred""""""

        if self.evaluate_condition():
            return

        now = datetime.now()
        deadline = datetime.fromisoformat(self.xcom_pull(context, key=""deadline""))
        task_logger.info(f""Woken from deferral; deadline: {deadline}"")
        diff = deadline - now
        if diff >= timedelta(seconds=0):
            task_logger.info(f""Remaining time to deadline: {diff}"")
        elif self.fail_on_timeout:
            raise RuntimeError(self.message_on_timeout)
        else:
            task_logger.info(""Deadline reached, considering as SUCCESS"")
            return

        defer_interval = min(self.poll_interval, deadline - now)
        task_logger.info(f""Deferring for {defer_interval} (until {now + defer_interval})"")
        self.defer(trigger=TimeDeltaTrigger(defer_interval), method_name=""deferred"")


# This sample is to be deleted soon
with DAG(
    dag_id=""wait_for_drill"",
    default_args=None,
    description=""Spawns several instances of Wait for in order to investigate PRISMA-10686"",
    tags=[""prisma-samples""],
) as dag:
    beg = EmptyOperator(dag=dag, task_id=""BEGIN"")
    end = EmptyOperator(dag=dag, task_id=""END"")

    for i in range(30):
        wait_for = WaitForOperator(dag=dag, task_id=f""WAITFOR_{i}"")
        beg >> wait_for

        wait_for >> end



```

But even with thise example I was not able to reproduce it.
The issue probably relates huge complexity of our application involve more than 300 tasks spread in 30 dags.

### Operating System

Red Hat Enterprise Linux 8.10 (Ootpa)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

Local executor + Airflow is installed on Rhel machine.

### Anything else?

We see the issue two times during one day of testing.
We are sharing mysql server for Airflow and application itself. 
We are using two airflow instances (each one is having itself database) over SQL alchemy.


### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",nesp159de,2025-01-24 15:29:15+00:00,[],2025-01-26 13:35:32+00:00,2025-01-26 13:35:32+00:00,https://github.com/apache/airflow/issues/46015,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:Triggerer', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2612875502, 'issue_id': 2809726114, 'author': 'bugraoz93', 'body': 'Seems like AI, closing.', 'created_at': datetime.datetime(2025, 1, 24, 16, 3, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614422617, 'issue_id': 2809726114, 'author': 'potiuk', 'body': 'I don\'t think so @bugraoz93  :). This one looks pretty legitimate (and sorry @nesp159de - we recently have been flooded with almost good looking issues generated by AI so we are pretty ""sensitive"" now).\n\nBut I am not sure we can do anything about it regardless. There are a number of places where mysql can deadlock  - where it is completely unnecessary and we know it - the deadlocks are far less often happening on Postgres and our recommendation - for now - would be to switch to postgres.\n\nThere are few other things you can do:\n\n*  try to disable ""schedule_after_task_execution"" - which is an often source for deadlock\n*  upgrade to latest Airflow 2.10.* - you are running pretty old version of Airlfow and you miss more or less 1000 fixes and new features that have been implemented since. Among them - especially in some of the latest versions of Airlfow was limiting a number of deadlocks that you might have. If you look at the changelog you will be able to find them. Generally speaking ""upgrade to latest version"" and ""check if the problems are still there"" is absolutely recommended action before anyones time is spent on looking at this issue.\n* Airlfow 3 (coming in 2-3 months) will have completely revamped architecture where removing database deadlocs and generally database pressure is one of the most important changes that drove the re-architecting. Stay tuned and upgrede/test it when ready\n\nConverting it into discussion if more discussion is needed.', 'created_at': datetime.datetime(2025, 1, 26, 13, 35, 26, tzinfo=datetime.timezone.utc)}]","bugraoz93 on (2025-01-24 16:03:42 UTC): Seems like AI, closing.

potiuk on (2025-01-26 13:35:26 UTC): I don't think so @bugraoz93  :). This one looks pretty legitimate (and sorry @nesp159de - we recently have been flooded with almost good looking issues generated by AI so we are pretty ""sensitive"" now).

But I am not sure we can do anything about it regardless. There are a number of places where mysql can deadlock  - where it is completely unnecessary and we know it - the deadlocks are far less often happening on Postgres and our recommendation - for now - would be to switch to postgres.

There are few other things you can do:

*  try to disable ""schedule_after_task_execution"" - which is an often source for deadlock
*  upgrade to latest Airflow 2.10.* - you are running pretty old version of Airlfow and you miss more or less 1000 fixes and new features that have been implemented since. Among them - especially in some of the latest versions of Airlfow was limiting a number of deadlocks that you might have. If you look at the changelog you will be able to find them. Generally speaking ""upgrade to latest version"" and ""check if the problems are still there"" is absolutely recommended action before anyones time is spent on looking at this issue.
* Airlfow 3 (coming in 2-3 months) will have completely revamped architecture where removing database deadlocs and generally database pressure is one of the most important changes that drove the re-architecting. Stay tuned and upgrede/test it when ready

Converting it into discussion if more discussion is needed.

"
2809363426,issue,open,,backfill should not create runs for future dates,"### Body


 backfill shouldn't be created for future dates I believe but if I pass run_backwards=true then its creating backfill for future date also
reverse block is missing the condition if x.data_interval.end < now
Also we can reuse the infos in the reverse block
def _get_info_list(
    *,
    from_date,
    to_date,
    reverse,
    dag,
):
    infos = dag.iter_dagrun_infos_between(from_date, to_date)
    now = timezone.utcnow()
    dagrun_info_list = (x for x in infos if x.data_interval.end < now)
    if reverse:
        dagrun_info_list = reversed([x for x in dag.iter_dagrun_infos_between(from_date, to_date)])
    return dagrun_info_list

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2025-01-24 12:38:45+00:00,['vatsrahul1001'],2025-02-07 15:26:32+00:00,,https://github.com/apache/airflow/issues/46012,"[('good first issue', ''), ('kind:meta', 'High-level information important to the community'), ('area:backfill', 'Specifically for backfill related')]","[{'comment_id': 2636941074, 'issue_id': 2809363426, 'author': '123Soham-bhatia', 'body': 'i would like to work on this issue, please assign it to me.', 'created_at': datetime.datetime(2025, 2, 5, 14, 3, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2636951969, 'issue_id': 2809363426, 'author': '123Soham-bhatia', 'body': '@potiuk  can i work on this one?', 'created_at': datetime.datetime(2025, 2, 5, 14, 8, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2637884325, 'issue_id': 2809363426, 'author': 'potiuk', 'body': 'I think @vatsrahul1001 already works on it', 'created_at': datetime.datetime(2025, 2, 5, 19, 49, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2643244239, 'issue_id': 2809363426, 'author': 'vatsrahul1001', 'body': 'Already have [PR](https://github.com/apache/airflow/pull/46017) up for this', 'created_at': datetime.datetime(2025, 2, 7, 15, 26, 30, tzinfo=datetime.timezone.utc)}]","123Soham-bhatia on (2025-02-05 14:03:41 UTC): i would like to work on this issue, please assign it to me.

123Soham-bhatia on (2025-02-05 14:08:07 UTC): @potiuk  can i work on this one?

potiuk on (2025-02-05 19:49:13 UTC): I think @vatsrahul1001 already works on it

vatsrahul1001 (Assginee) on (2025-02-07 15:26:30 UTC): Already have [PR](https://github.com/apache/airflow/pull/46017) up for this

"
2808838761,issue,closed,completed,HttpToS3Operator OOM when downloading large file,"### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

main is affected

### Apache Airflow version

2.10

### Operating System

Linux

### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### What happened

Whole file is attempted to be loaded to memory.
Task exited with return code -9.

https://github.com/apache/airflow/blob/bb77ebf1a2384169c4cd3132b36c54d779473886/providers/src/airflow/providers/amazon/aws/transfers/http_to_s3.py#L164-L175

In this code response.content is in-memory representation of file content (as bytes).

### What you think should happen instead

Lazy load via `stream=True`, `response.raw` and `S3Hook.load_fileobj()`

### How to reproduce

Run `HttpToS3Operator` with file larger than RAM available (~20GB in my setup was enough).

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rogalski,2025-01-24 08:24:12+00:00,[],2025-01-26 13:16:59+00:00,2025-01-26 13:16:59+00:00,https://github.com/apache/airflow/issues/46008,"[('area:providers', '')]","[{'comment_id': 2612677645, 'issue_id': 2808838761, 'author': 'raphaelauv', 'body': 'hi, HttpToS3Operator is an airflow operator doing the data transfer thanks to the code you linked.\n\nSo data is going through the airflow_worker ( if using celery executor )\n\nyou have multiple options: \n\n1) use the kubernetes Executor ( or any other executor that let you custom the RAM context of the task ) for this task and the code of the operator ( but still not doing stream transfer ) will not suffer of the airflow_worker hardware limits\n\nor\n\n2) use the KubernetesPodOperator and trigger an efficient specialist tool to execute the transfer , like  RCLONE -> https://rclone.org/docs/', 'created_at': datetime.datetime(2025, 1, 24, 14, 35, 6, tzinfo=datetime.timezone.utc)}]","raphaelauv on (2025-01-24 14:35:06 UTC): hi, HttpToS3Operator is an airflow operator doing the data transfer thanks to the code you linked.

So data is going through the airflow_worker ( if using celery executor )

you have multiple options: 

1) use the kubernetes Executor ( or any other executor that let you custom the RAM context of the task ) for this task and the code of the operator ( but still not doing stream transfer ) will not suffer of the airflow_worker hardware limits

or

2) use the KubernetesPodOperator and trigger an efficient specialist tool to execute the transfer , like  RCLONE -> https://rclone.org/docs/

"
2808525278,issue,open,,Investigate if we can optimise register_asset_changes_in_db from task runner,"### Body

Right now, for registering asset events on task completion, we send across a lot of ""events"" for AssetNameRef and AssetUriRef types. Maybe we can optimise it by retrieving the Asset directly using client APIs and sending those.

This will also simplify `register_asset_changes_in_db`

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",amoghrajesh,2025-01-24 05:19:25+00:00,['amoghrajesh'],2025-01-24 05:21:41+00:00,,https://github.com/apache/airflow/issues/46002,"[('area:datasets', 'Issues related to the datasets feature'), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2808354862,issue,closed,completed,Support https repos in GitHook,"### Body

We should support pointing at repos via https - both public and private repos using auth tokens. [This check in initialize blocks it](https://github.com/apache/airflow/blob/ce2c891e55b5dbb290c1109d47c6a17a4a04acac/airflow/dag_processing/bundles/git.py#L139-L142).

Also, while we are at it, we need to update the [example in config](https://github.com/apache/airflow/blob/ce2c891e55b5dbb290c1109d47c6a17a4a04acac/airflow/config_templates/config.yml#L2659-L2667).


### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2025-01-24 02:33:49+00:00,['jx2lee'],2025-01-27 01:33:24+00:00,2025-01-27 01:33:24+00:00,https://github.com/apache/airflow/issues/45998,"[('kind:feature', 'Feature Requests'), ('kind:meta', 'High-level information important to the community'), ('AIP-66: DAG Bundle/Manifest', '')]","[{'comment_id': 2614442696, 'issue_id': 2808354862, 'author': 'jx2lee', 'body': '@jedcunningham Can i take this?\nI fixed view_url method to enable https, along with the point you mentioned!', 'created_at': datetime.datetime(2025, 1, 26, 14, 14, 46, tzinfo=datetime.timezone.utc)}]","jx2lee (Assginee) on (2025-01-26 14:14:46 UTC): @jedcunningham Can i take this?
I fixed view_url method to enable https, along with the point you mentioned!

"
2808180498,issue,closed,completed,AssignmentsManager retries requests even when the broker is no longer a replica,,Mahmoud-Riad00,2025-01-23 23:53:34+00:00,[],2025-01-25 22:05:41+00:00,2025-01-25 22:03:55+00:00,https://github.com/apache/airflow/issues/45997,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:apache-kafka', ''), ('AI Spam', '')]","[{'comment_id': 2611249713, 'issue_id': 2808180498, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 23, 23, 53, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614112057, 'issue_id': 2808180498, 'author': 'potiuk', 'body': 'Please stop repoting AI-generated nonsense issues - I am reporting your account to GitHub and you might expect your account to be blocked.', 'created_at': datetime.datetime(2025, 1, 25, 21, 56, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614114243, 'issue_id': 2808180498, 'author': 'Mahmoud-Riad00', 'body': ""Ok I'll try doing all work my self and thank for telling it's noticable and for your reporting me I think I deserve that ."", 'created_at': datetime.datetime(2025, 1, 25, 22, 5, 40, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-23 23:53:37 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2025-01-25 21:56:43 UTC): Please stop repoting AI-generated nonsense issues - I am reporting your account to GitHub and you might expect your account to be blocked.

Mahmoud-Riad00 (Issue Creator) on (2025-01-25 22:05:40 UTC): Ok I'll try doing all work my self and thank for telling it's noticable and for your reporting me I think I deserve that .

"
2808151459,issue,closed,not_planned,Add support for converting dictionaries to boto-style key-value lists for AWS API parameters,"### Description

This feature request proposes adding a utility method to convert Python dictionaries into the boto-style key-value list format required by AWS APIs. This will simplify the process of working with AWS services in Airflow by allowing users to pass dictionaries (a more intuitive and user-friendly format) instead of manually formatting key-value lists.

### Use case/motivation

When interacting with AWS services (e.g., S3, RDS, SageMaker), many APIs expect parameters in a specific key-value list format ([{""Key"": ""name"", ""Value"": ""example""}]). Currently, users must manually format their data into this structure, which is error-prone and less intuitive.  

By introducing a utility method to handle this conversion, users can simply pass a Python dictionary ({""name"": ""example""}), and the method will automatically transform it into the required format. This improves usability, reduces boilerplate code, and makes Airflow's AWS integration more accessible to users unfamiliar with boto3's quirks.

### Related issues

- *PR #28816*: Introduced a method to convert dictionaries to boto-style key-value lists.  
- *Issue #18938*: Discusses limitations in rendering dictionary keys in templated values, which is relevant to this feature.

### Are you willing to submit a PR?

- [x] No

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Mohammed-Karim226,2025-01-23 23:33:35+00:00,[],2025-01-25 08:36:10+00:00,2025-01-25 08:36:09+00:00,https://github.com/apache/airflow/issues/45996,"[('area:providers', ''), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2613843344, 'issue_id': 2808151459, 'author': 'eladkal', 'body': 'AI spam', 'created_at': datetime.datetime(2025, 1, 25, 8, 36, 9, tzinfo=datetime.timezone.utc)}]","eladkal on (2025-01-25 08:36:09 UTC): AI spam

"
2808148814,issue,open,,AIP-65 | Add dag versions to DagRunResponse,"### Body

Add `dag_versions` as an array of uuids to the DagRunResponse in the public REST API. Most of the time this will be an array of length 1, but some dag runs can have multiple versions.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",bbovenzi,2025-01-23 23:30:50+00:00,['jason810496'],2025-02-04 14:12:08+00:00,,https://github.com/apache/airflow/issues/45995,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-65: DAG history in UI', '')]","[{'comment_id': 2613838154, 'issue_id': 2808148814, 'author': 'jason810496', 'body': 'Hi @bbovenzi , I can work on this issue, could you assign to me ? Thanks !', 'created_at': datetime.datetime(2025, 1, 25, 8, 19, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2622050665, 'issue_id': 2808148814, 'author': 'bbovenzi', 'body': '@jason810496 Assigned!', 'created_at': datetime.datetime(2025, 1, 29, 16, 0, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2622538375, 'issue_id': 2808148814, 'author': 'jason810496', 'body': 'Hi @bbovenzi, may I ask when the case of ""but some DAG runs can have multiple versions"" would occur?  \n\nSince there is a `dag_version_id` in the `DagRun` model, shouldn\'t there be only one `dag_version` per `DagRun`?  \nhttps://github.com/apache/airflow/blob/main/airflow/models/dagrun.py#L169-L170\n\nOr is making `dag_versions` a list mainly for UI purposes?\nFor example, `latest_dag_runs: list[DAGRunResponse]` is used in `DAGWithLatestDagRunsResponse`. However, even in this case, each item in `latest_dag_runs` still corresponds to a `DagRun` model, which should have only one `dag_version` per `DagRun`.  \n\nOr is this related to `DagModel` instead of `DagRun`? Since `DagModel` has a `dag_versions` relationship, there can be multiple `dag_versions` for a `DagModel`.  \n\nThanks!  \ncc @pierrejeambrun', 'created_at': datetime.datetime(2025, 1, 29, 18, 40, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2622638757, 'issue_id': 2808148814, 'author': 'bbovenzi', 'body': 'Oh yeah this one is quite complicated. Sometimes a dag version can change in the middle of a dag run. We may need to wait until that schema is updated to allow a many to many relationship cc: @jedcunningham @ephraimbuddy', 'created_at': datetime.datetime(2025, 1, 29, 19, 24, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2623140389, 'issue_id': 2808148814, 'author': 'pierrejeambrun', 'body': 'There is a FK from version to TI and from dagrun to versions.\n\nDagrun to version needs to be deleted. And we need to retrieve dagrun versions as an aggregate of the dagrun TIs versions. (All different versions used for all TIs are the version of the dagrun)\n\nIf I understand correctly.', 'created_at': datetime.datetime(2025, 1, 29, 23, 17, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2623142607, 'issue_id': 2808148814, 'author': 'pierrejeambrun', 'body': 'I can open a PR to remove that direct link between DagRun and DagVersion.', 'created_at': datetime.datetime(2025, 1, 29, 23, 18, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2627098266, 'issue_id': 2808148814, 'author': 'jason810496', 'body': 'Hi @pierrejeambrun,  \n\nAfter some POC work and deeper investigation, I found that even when **""Sometimes a dag version can change in the middle of a dag run. ""** happens, all `TaskInstance`s of the same `DagRun` will still have the same `dag_version`. This means that the idea **""Retrieve DagRun versions as an aggregate of the DagRun TIs versions (i.e., all different versions used for all TIs are the version of the DagRun)""** is unlikely to happen.  \n\nHere’s my breakdown:  \n\n1. `dag_version` is passed as an argument to `dag.create_dagrun` and then passed down to `_create_orm_dagrun`.  \n2. The `DagRun` instance is created as `run` in `_create_orm_dagrun`, which then calls `run.verify_integrity`.  \n3. The `task_creator` factory, obtained from `self._get_task_creator`, creates instances of the `TaskInstance` model (let’s simplify these as `TIs`). Importantly, the **`dag_version_id` of all `TIs` is always taken from `self.dag_version_id`**, meaning all `dag_versions` of `TIs` will be the same as that of the `DagRun`.  \n4. All `TIs` created by `task_creator` are persisted using `self._create_task_instances`.  \n\nIn summary, I believe we don’t need to remove the **direct link between `DagRun` and `DagVersion`**, and the return type of `dag_version` in this API will always be a single `UUID`.  \n\nAm I correct in this assumption?  \nOr is there actually a scenario where **different `dag_versions` can be found among `TIs` in the same `DagRun`** that I haven\'t considered?  \n\nThanks!  \n\ncc @ephraimbuddy', 'created_at': datetime.datetime(2025, 1, 31, 12, 24, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2627399959, 'issue_id': 2808148814, 'author': 'jedcunningham', 'body': 'You are spot on how it behaves now, but we need to refactor that so it _will_ be possible at some point in the future.', 'created_at': datetime.datetime(2025, 1, 31, 13, 49, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2628996472, 'issue_id': 2808148814, 'author': 'jason810496', 'body': 'After reading through AIP-63, 64, and 65 and reviewing the related codebase today, this endpoint should also consider `TaskInstanceHistory`, right?  \nIt should join `DagRun`, `TaskInstance`, `TaskInstanceHistory`, and `DagVersion`, somewhat like `get_task_instance_tries`, but with granularity not down to the `task_id` level.\n\nhttps://github.com/apache/airflow/blob/260a988292855569e55777bcd1c7a085be3db0c1/airflow/api_fastapi/core_api/routes/public/task_instances.py#L260-L293', 'created_at': datetime.datetime(2025, 2, 1, 15, 30, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2631236399, 'issue_id': 2808148814, 'author': 'pierrejeambrun', 'body': 'Ideally I think you are right. Version could have changed in between tries and we need to explore the TaskInstanceHistory to get that information.', 'created_at': datetime.datetime(2025, 2, 3, 14, 57, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2634087976, 'issue_id': 2808148814, 'author': 'jason810496', 'body': '> Ideally I think you are right. Version could have changed in between tries and we need to explore the TaskInstanceHistory to get that information.\n\nThanks for confirming !', 'created_at': datetime.datetime(2025, 2, 4, 14, 12, 6, tzinfo=datetime.timezone.utc)}]","jason810496 (Assginee) on (2025-01-25 08:19:08 UTC): Hi @bbovenzi , I can work on this issue, could you assign to me ? Thanks !

bbovenzi (Issue Creator) on (2025-01-29 16:00:12 UTC): @jason810496 Assigned!

jason810496 (Assginee) on (2025-01-29 18:40:04 UTC): Hi @bbovenzi, may I ask when the case of ""but some DAG runs can have multiple versions"" would occur?  

Since there is a `dag_version_id` in the `DagRun` model, shouldn't there be only one `dag_version` per `DagRun`?  
https://github.com/apache/airflow/blob/main/airflow/models/dagrun.py#L169-L170

Or is making `dag_versions` a list mainly for UI purposes?
For example, `latest_dag_runs: list[DAGRunResponse]` is used in `DAGWithLatestDagRunsResponse`. However, even in this case, each item in `latest_dag_runs` still corresponds to a `DagRun` model, which should have only one `dag_version` per `DagRun`.  

Or is this related to `DagModel` instead of `DagRun`? Since `DagModel` has a `dag_versions` relationship, there can be multiple `dag_versions` for a `DagModel`.  

Thanks!  
cc @pierrejeambrun

bbovenzi (Issue Creator) on (2025-01-29 19:24:29 UTC): Oh yeah this one is quite complicated. Sometimes a dag version can change in the middle of a dag run. We may need to wait until that schema is updated to allow a many to many relationship cc: @jedcunningham @ephraimbuddy

pierrejeambrun on (2025-01-29 23:17:08 UTC): There is a FK from version to TI and from dagrun to versions.

Dagrun to version needs to be deleted. And we need to retrieve dagrun versions as an aggregate of the dagrun TIs versions. (All different versions used for all TIs are the version of the dagrun)

If I understand correctly.

pierrejeambrun on (2025-01-29 23:18:46 UTC): I can open a PR to remove that direct link between DagRun and DagVersion.

jason810496 (Assginee) on (2025-01-31 12:24:24 UTC): Hi @pierrejeambrun,  

After some POC work and deeper investigation, I found that even when **""Sometimes a dag version can change in the middle of a dag run. ""** happens, all `TaskInstance`s of the same `DagRun` will still have the same `dag_version`. This means that the idea **""Retrieve DagRun versions as an aggregate of the DagRun TIs versions (i.e., all different versions used for all TIs are the version of the DagRun)""** is unlikely to happen.  

Here’s my breakdown:  

1. `dag_version` is passed as an argument to `dag.create_dagrun` and then passed down to `_create_orm_dagrun`.  
2. The `DagRun` instance is created as `run` in `_create_orm_dagrun`, which then calls `run.verify_integrity`.  
3. The `task_creator` factory, obtained from `self._get_task_creator`, creates instances of the `TaskInstance` model (let’s simplify these as `TIs`). Importantly, the **`dag_version_id` of all `TIs` is always taken from `self.dag_version_id`**, meaning all `dag_versions` of `TIs` will be the same as that of the `DagRun`.  
4. All `TIs` created by `task_creator` are persisted using `self._create_task_instances`.  

In summary, I believe we don’t need to remove the **direct link between `DagRun` and `DagVersion`**, and the return type of `dag_version` in this API will always be a single `UUID`.  

Am I correct in this assumption?  
Or is there actually a scenario where **different `dag_versions` can be found among `TIs` in the same `DagRun`** that I haven't considered?  

Thanks!  

cc @ephraimbuddy

jedcunningham on (2025-01-31 13:49:15 UTC): You are spot on how it behaves now, but we need to refactor that so it _will_ be possible at some point in the future.

jason810496 (Assginee) on (2025-02-01 15:30:38 UTC): After reading through AIP-63, 64, and 65 and reviewing the related codebase today, this endpoint should also consider `TaskInstanceHistory`, right?  
It should join `DagRun`, `TaskInstance`, `TaskInstanceHistory`, and `DagVersion`, somewhat like `get_task_instance_tries`, but with granularity not down to the `task_id` level.

https://github.com/apache/airflow/blob/260a988292855569e55777bcd1c7a085be3db0c1/airflow/api_fastapi/core_api/routes/public/task_instances.py#L260-L293

pierrejeambrun on (2025-02-03 14:57:19 UTC): Ideally I think you are right. Version could have changed in between tries and we need to explore the TaskInstanceHistory to get that information.

jason810496 (Assginee) on (2025-02-04 14:12:06 UTC): Thanks for confirming !

"
2808144418,issue,closed,completed,AIP-65 | Filter dag structure by dag version,"### Body

Add dag_version as an optional parameter to the /structure_data endpoint in the UI rest api to return the dag structure from that version. By default we should always return the structure of the latest version

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",bbovenzi,2025-01-23 23:26:37+00:00,['jason810496'],2025-01-30 17:02:11+00:00,2025-01-30 17:02:11+00:00,https://github.com/apache/airflow/issues/45994,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-65: DAG history in UI', '')]","[{'comment_id': 2613838466, 'issue_id': 2808144418, 'author': 'jason810496', 'body': 'Hi @bbovenzi , I can work on this issue, could you assign to me ? Thanks !', 'created_at': datetime.datetime(2025, 1, 25, 8, 20, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2622050818, 'issue_id': 2808144418, 'author': 'bbovenzi', 'body': '@jason810496 Assigned!', 'created_at': datetime.datetime(2025, 1, 29, 16, 0, 16, tzinfo=datetime.timezone.utc)}]","jason810496 (Assginee) on (2025-01-25 08:20:02 UTC): Hi @bbovenzi , I can work on this issue, could you assign to me ? Thanks !

bbovenzi (Issue Creator) on (2025-01-29 16:00:16 UTC): @jason810496 Assigned!

"
2808135284,issue,closed,completed,AIP-65 | Add Dag Version to TaskInstanceResponse in rest api,"### Body

Add `dag_version` to the REST API's TaskInstanceResponse and TaskInstanceHistoryResponse.

It will also be helpful to add dag_version as a field to filter the list task instances endpoint too.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",bbovenzi,2025-01-23 23:18:55+00:00,"['ephraimbuddy', 'pierrejeambrun']",2025-02-05 21:46:13+00:00,2025-02-05 21:46:13+00:00,https://github.com/apache/airflow/issues/45993,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-65: DAG history in UI', '')]","[{'comment_id': 2613838619, 'issue_id': 2808135284, 'author': 'jason810496', 'body': 'Hi @bbovenzi , I can work on this issue, could you assign to me ? Thanks !', 'created_at': datetime.datetime(2025, 1, 25, 8, 20, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2622050971, 'issue_id': 2808135284, 'author': 'bbovenzi', 'body': '@jason810496 Assigned!', 'created_at': datetime.datetime(2025, 1, 29, 16, 0, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2622089240, 'issue_id': 2808135284, 'author': 'pierrejeambrun', 'body': ""@jason810496 I'll take this one so we can move in parallel on API related stuff for AIP-65. (so you'r not working alone on that and we can get to the front-end side as fast as possible.)"", 'created_at': datetime.datetime(2025, 1, 29, 16, 15, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2622103540, 'issue_id': 2808135284, 'author': 'jason810496', 'body': ""> [@jason810496](https://github.com/jason810496) I'll take this one so we can move in parallel on API related stuff for AIP-65. (so you'r not working alone on that and we can get to the front-end side as fast as possible.)\n\nSure 🚀"", 'created_at': datetime.datetime(2025, 1, 29, 16, 20, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2636976562, 'issue_id': 2808135284, 'author': 'phanikumv', 'body': '@ephraimbuddy is working on the TaskInstanceHistory part of the story', 'created_at': datetime.datetime(2025, 2, 5, 14, 17, 37, tzinfo=datetime.timezone.utc)}]","jason810496 on (2025-01-25 08:20:41 UTC): Hi @bbovenzi , I can work on this issue, could you assign to me ? Thanks !

bbovenzi (Issue Creator) on (2025-01-29 16:00:19 UTC): @jason810496 Assigned!

pierrejeambrun (Assginee) on (2025-01-29 16:15:01 UTC): @jason810496 I'll take this one so we can move in parallel on API related stuff for AIP-65. (so you'r not working alone on that and we can get to the front-end side as fast as possible.)

jason810496 on (2025-01-29 16:20:21 UTC): Sure 🚀

phanikumv on (2025-02-05 14:17:37 UTC): @ephraimbuddy is working on the TaskInstanceHistory part of the story

"
2808118105,issue,open,,AIP-65 | Add List Dag Versions to Rest API,"Add a new GET `/{dag_id}/versions` endpoint to the public rest api to return a paginated list of dag versions with fields like:

```
{
  version_name,
  created_at,
  bundle_version,
  bundle_link
}
```

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",bbovenzi,2025-01-23 23:03:36+00:00,['Prab-27'],2025-02-07 17:15:50+00:00,,https://github.com/apache/airflow/issues/45992,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-65: DAG history in UI', '')]","[{'comment_id': 2612061141, 'issue_id': 2808118105, 'author': 'Prab-27', 'body': ""@bbovenzi, I'd like to work on this issue."", 'created_at': datetime.datetime(2025, 1, 24, 9, 29, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2623583455, 'issue_id': 2808118105, 'author': 'phanikumv', 'body': 'Assigned to you @Prab-27', 'created_at': datetime.datetime(2025, 1, 30, 5, 58, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2639239595, 'issue_id': 2808118105, 'author': 'pierrejeambrun', 'body': 'Hello @Prab-27,\n\nAny progress on this issue ? Do you need help getting started ?', 'created_at': datetime.datetime(2025, 2, 6, 9, 16, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2639490823, 'issue_id': 2808118105, 'author': 'Prab-27', 'body': '@pierrejeambrun Thanks for your help. Could you please tell me how I can get `bundle_link` from the model ?', 'created_at': datetime.datetime(2025, 2, 6, 10, 57, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2643459021, 'issue_id': 2808118105, 'author': 'pierrejeambrun', 'body': 'You can retrieve the bundle url using the `DagBundlesManager` and calling the `view_url`.', 'created_at': datetime.datetime(2025, 2, 7, 16, 49, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2643527698, 'issue_id': 2808118105, 'author': 'Prab-27', 'body': '@pierrejeambrun Thanks , I will raise a PR very soon', 'created_at': datetime.datetime(2025, 2, 7, 17, 15, 49, tzinfo=datetime.timezone.utc)}]","Prab-27 (Assginee) on (2025-01-24 09:29:39 UTC): @bbovenzi, I'd like to work on this issue.

phanikumv on (2025-01-30 05:58:10 UTC): Assigned to you @Prab-27

pierrejeambrun on (2025-02-06 09:16:24 UTC): Hello @Prab-27,

Any progress on this issue ? Do you need help getting started ?

Prab-27 (Assginee) on (2025-02-06 10:57:29 UTC): @pierrejeambrun Thanks for your help. Could you please tell me how I can get `bundle_link` from the model ?

pierrejeambrun on (2025-02-07 16:49:18 UTC): You can retrieve the bundle url using the `DagBundlesManager` and calling the `view_url`.

Prab-27 (Assginee) on (2025-02-07 17:15:49 UTC): @pierrejeambrun Thanks , I will raise a PR very soon

"
2808105158,issue,closed,completed,Scheduler performance with large number of mapped task instances,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Hi! I'm still evaluating if Airflow is suitable for our needs, and it seems it is experiencing hiccups when used with bigger mapped task instances.

Even for the minimized example (see below), that produces the following DAG structure, the performance of the system (effective parallelism) drops significantly when switching from 100x to 500x task Dynamic Task Mapping size:

Here, one `group` of tasks sleeps for 6s on average:
- 100x case takes ~70s -> effective parallelism is ~9
- 500x case takes ~560s -> total effective parallelism is ~5 (it catches up later in the run, so the value is overstated)

![Image](https://github.com/user-attachments/assets/1c523c76-25ae-46a0-96cc-b029d4ea3bdd)

### What you think should happen instead?

Debug messages reveal, that the scheduler spends most time in evaluating task instance dependencies of the DagRun in
[TaskInstance.are_dependencies_met](https://github.com/apache/airflow/blob/2.10.4/airflow/models/taskinstance.py#L2603) (being CPU-bound, not by the DB), scheduling only one batch (16 by default) per 8s loop:

```
{scheduler_job_runner.py:2069} DEBUG - Finding 'running' jobs without a recent heartbeat
{event_scheduler.py:39} DEBUG - Calling <bound method SchedulerJobRunner._emit_pool_metrics of <airflow.jobs.scheduler_job_runner.SchedulerJobRunner object at 0x75ee2ad3d0a0>>
{scheduler_job_runner.py:1174} DEBUG - Next timed event is in 4.999933
{scheduler_job_runner.py:1176} DEBUG - Ran scheduling loop in 8.49 seconds
```

In particular, the 100x case has most of the task instances scheduled by the mini-scheduler, while initially none of the 500x run.
In that case, the mini-scheduler is skipped due to [locked rows](https://github.com/apache/airflow/blob/2.10.4/airflow/models/taskinstance.py#L3859), but it wasn't immediately clear to me where the lock happens in the scheduler.

Are there any architectural limitations preventing Airflow from managing thousands of fairly short-living tasks? The scheduler seems to spend excessively long time in evaluating the DagRun while holding database locks.

### How to reproduce

Use the docker-compose quick-start with the following DAG:

<details><summary>Source code</summary>

```python
from time import sleep

from airflow.models.dag import DAG
from airflow.decorators import task, task_group
import numpy as np

with DAG(
    ""minimal"",
    default_args={
        # We want to maximize use of the limited hardware
        'weight_rule': ""upstream"",
    },
    schedule=None,
    start_date=None,
    catchup=False,
    tags=[""bugs""],
) as dag:

    @task
    def generator():
        return list(range(500))

    @task
    def part1(x):
        rng = np.random.default_rng()
        #assert rng.random() > 0.001
        sleep(1 + rng.random())
        return str(x)

    @task
    def part2():
        sleep(0.5)
        return ""s""

    @task
    def combiner(part1, part2):
        rng = np.random.default_rng()
        #assert rng.random() > 0.001
        sleep(1 + rng.random())
        return part1 + part2

    @task
    def reporter(x):
        print(x)

    @task  # pool = 'limited_hardware'
    def evaluate(what):
        #assert np.random.default_rng().random() > 0.001
        sleep(3)
        return what, len(what)

    @task_group
    def group(num, part2):
        s1 = part1(num)
        comb = combiner(s1, part2)
        rep = reporter(comb)
        res = evaluate(comb)
        return res, comb, s1, rep


    @task(trigger_rule=""one_failed"", priority_weight=1000,)
    def skipper(ti):
        assert False  # this normally skips all unfinished tasks


    @task(trigger_rule=""all_done"",)
    def collect(ins):
        print(list(ins))

    t1 = generator()
    s = skipper()

    parts = group.partial(part2=part2()).expand(num=t1)
    parts >> s
    collect(ins=parts[0])
```
</details>

### Operating System

Ubuntu 24.04.1 LTS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

Vanilla docker-compose setup from docs.  20 core Xeon W-1290 CPU.

### Anything else?



### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",matrach,2025-01-23 22:52:12+00:00,[],2025-01-25 21:50:22+00:00,2025-01-25 21:50:22+00:00,https://github.com/apache/airflow/issues/45991,"[('kind:bug', 'This is a clearly a bug'), ('area:performance', ''), ('area:core', ''), ('area:dynamic-task-mapping', 'AIP-42'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2611178226, 'issue_id': 2808105158, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 23, 22, 52, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2611334138, 'issue_id': 2808105158, 'author': 'matrach', 'body': ""I've mentioned the mini-scheduler above, but if I understand correctly, it resolved the scheduling state for ALL task instances, but then ignored the ones not downstream...?\n\nIt appears that the scheduler for mapped tasks takes at least quadratic time in their quantity, as [here](https://github.com/apache/airflow/blob/577985821e4e3c8cfbc2797feb19cc32fe2aaac0/airflow/ti_deps/deps/trigger_rule_dep.py#L255) it loops over all finished task instances, and does so for all schedulable task instances."", 'created_at': datetime.datetime(2025, 1, 24, 1, 10, 28, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-23 22:52:16 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

matrach (Issue Creator) on (2025-01-24 01:10:28 UTC): I've mentioned the mini-scheduler above, but if I understand correctly, it resolved the scheduling state for ALL task instances, but then ignored the ones not downstream...?

It appears that the scheduler for mapped tasks takes at least quadratic time in their quantity, as [here](https://github.com/apache/airflow/blob/577985821e4e3c8cfbc2797feb19cc32fe2aaac0/airflow/ti_deps/deps/trigger_rule_dep.py#L255) it loops over all finished task instances, and does so for all schedulable task instances.

"
2808092472,issue,open,,AIP-38 | Add Mapped Task Instance Summary page,"From the graph or (WIP) grid view, if you click on a mapped task you will be directed to a broken page. This is because we only support the details of a single mapped task instance but have no summary page.

We should create a Mapped Task Instance summary page that allows a user to select a mapped task instance without specifying a map_index and then seeing a summary of the the instances as well as a table of all the mapped instances. See the old UI for reference:

Old UI:
<img width=""1087"" alt=""Image"" src=""https://github.com/user-attachments/assets/72780b9b-85f0-4072-94ad-d229d45ef2e7"" />",bbovenzi,2025-01-23 22:43:51+00:00,[],2025-01-23 22:48:03+00:00,,https://github.com/apache/airflow/issues/45990,"[('area:UI', 'Related to UI/UX. For Frontend Developers.')]",[],
2808079915,issue,closed,completed,test,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

test

### What you think should happen instead?

test

### How to reproduce

test

### Operating System

test

### Versions of Apache Airflow Providers

test

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

test

### Anything else?

test

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Sabbir02,2025-01-23 22:38:36+00:00,[],2025-01-23 22:38:43+00:00,2025-01-23 22:38:43+00:00,https://github.com/apache/airflow/issues/45989,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2807881646,issue,open,,smtp_mail_from Always Overwrites from_email in Airflow Email Sending Logic,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

2.10.3

### What happened?

#### Description:

The current implementation of email sending in Airflow does not allow the `from_email` parameter to function as intended due to the behavior of `smtp_mail_from`. 

The code snippet from send_email_smtp function in airflow/utils/email.py below highlights the issue:

```python
smtp_mail_from = conf.get(""smtp"", ""SMTP_MAIL_FROM"")

if smtp_mail_from is not None:
    mail_from = smtp_mail_from
else:
    if from_email is None:
        raise ValueError(
            ""You should set from email - either by smtp/smtp_mail_from config or `from_email` parameter""
        )
    mail_from = from_email
```

### Issue:
- **`smtp_mail_from` is always a string.** Even if the configuration in `airflow.cfg` leaves it empty, `conf.get(""smtp"", ""SMTP_MAIL_FROM"")` defaults to an empty string (`""""`), which is not `None`.
- As a result, the `from_email` parameter is **never used** because the `if smtp_mail_from is not None` condition is always `True`.
- This behavior renders the `from_email` parameter in various email-related functionalities effectively useless.

### What you think should happen instead?


### Expected Behavior:
The `from_email` parameter should take precedence if explicitly provided, but it currently cannot because `smtp_mail_from` is never `None`.

### Proposed Solution:
Modify the condition to properly handle empty strings and prioritize `from_email` when explicitly set:

```python
smtp_mail_from = conf.get(""smtp"", ""SMTP_MAIL_FROM"")

if smtp_mail_from:  # Check for non-empty string
    mail_from = smtp_mail_from
else:
    if from_email is None:
        raise ValueError(
            ""You should set from email - either by smtp/smtp_mail_from config or `from_email` parameter""
        )
    mail_from = from_email
```

This change ensures:
1. `smtp_mail_from` is only used when it is explicitly set to a non-empty value.
2. The `from_email` parameter is respected when provided.

### How to reproduce

#### Prerequisites:
- An Airflow instance is running.
- Access to the Airflow UI to create an SMTP connection.

---

#### Steps to Reproduce:

1. **Clear SMTP Configuration in `airflow.cfg`:**
   In the `airflow.cfg` file, ensure all SMTP-related parameter in the `[smtp]` section are left blank or commented out:
   ```ini
   [smtp]
   smtp_mail_from =
   ```

2. **Restart Airflow:**
   Restart the Airflow services to apply the changes:
   ```bash
   airflow scheduler restart
   airflow webserver restart
   ```

3. **Create an SMTP Connection in the Airflow UI:**
   - Navigate to the **Admin** -> **Connections** page.
   - Click **+ Add a new record**.
   - Fill out the connection details:
     - **Conn Id**: `smtp_test`
     - **Conn Type**: `SMTP`
     - **Host**: `<SMTP server>` (e.g., `smtp.example.com`)
     - **Port**: `25`
     - **Email From**: `test@example.com`
   - Leave other fields empty or as defaults.
   - Save the connection.

4. **Create a DAG with an `EmailOperator`:**
   Use the following sample DAG to send an email:

   ```python
   from airflow import DAG
   from airflow.operators.email import EmailOperator
   from datetime import datetime

   with DAG(
       dag_id='test_smtp_from_email',
       start_date=datetime(2025, 1, 1),
       schedule_interval=None,
       catchup=False,
   ) as dag:

       email_task = EmailOperator(
           task_id='send_email',
           conn_id='smtp_test',
           to='recipient@example.com',
           subject='Test Email',
           html_content='This is a test email.',
           from_email='custom_sender@example.com',  # Explicit from_email
       )
   ```

5. **Trigger the DAG:**
   - Navigate to the Airflow UI.
   - Trigger the `test_smtp_from_email` DAG.

---

#### Expected Behavior:
- The email should be sent with `custom_sender@example.com` as the `From` address when `from_email` is explicitly provided.

---

#### Actual Behavior:
- The email is sent using the default `smtp_mail_from` from `airflow.cfg` (likely an empty string or a misconfigured value).
- The `from_email` parameter is ignored completely.

---

This demonstrates that the `from_email` parameter is not respected due to the logic prioritizing `smtp_mail_from`, even when it is blank.

### Operating System

Debian GNU/Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.10.3/docker-compose.yaml'

### Anything else?

### Impact:
This bug impacts all email-sending functionalities, such as email alerts, `EmailOperator`, and any custom email utilities relying on Airflow's email sending methods.

### Environment:
- Airflow version: 2.10.3

Let me know if more details or examples are needed!

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",opqi,2025-01-23 20:52:55+00:00,[],2025-02-09 00:16:27+00:00,,https://github.com/apache/airflow/issues/45983,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('pending-response', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2610994397, 'issue_id': 2807881646, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 23, 20, 52, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614109831, 'issue_id': 2807881646, 'author': 'potiuk', 'body': 'Can you please propose PR to solve problems. It will be easier to validate the claims by looking at changes in the code that fix it.', 'created_at': datetime.datetime(2025, 1, 25, 21, 47, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2645994248, 'issue_id': 2807881646, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2025, 2, 9, 0, 16, 26, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-23 20:52:58 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2025-01-25 21:47:11 UTC): Can you please propose PR to solve problems. It will be easier to validate the claims by looking at changes in the code that fix it.

github-actions[bot] on (2025-02-09 00:16:26 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

"
2807671540,issue,closed,completed,Support inline ssh key in `GitHook`,"### Body

Instead of requiring keys to be on disk somewhere, we should also support the key being in the connection itself.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2025-01-23 18:51:08+00:00,['jx2lee'],2025-02-05 15:34:06+00:00,2025-02-05 15:34:06+00:00,https://github.com/apache/airflow/issues/45979,"[('kind:feature', 'Feature Requests'), ('kind:meta', 'High-level information important to the community'), ('area:core-operators', 'Operators, Sensors and hooks within Core Airflow'), ('AIP-66: DAG Bundle/Manifest', '')]","[{'comment_id': 2614977475, 'issue_id': 2807671540, 'author': 'jx2lee', 'body': '@jedcunningham Can i take this? 🚀', 'created_at': datetime.datetime(2025, 1, 27, 7, 0, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2616466142, 'issue_id': 2807671540, 'author': 'jx2lee', 'body': '@jedcunningham I have question! To support key in connection itself, does it need to implement like [this](https://airflow.apache.org/docs/apache-airflow-providers-ssh/stable/connections/ssh.html#:~:text=private_key%20%2D%20Content%20of,the%20private%20key.), include private_key/passphrase?', 'created_at': datetime.datetime(2025, 1, 27, 17, 35, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2616590869, 'issue_id': 2807671540, 'author': 'jedcunningham', 'body': 'I\'d keep it simple and not cover the passphrase path. If folks need ""more"", having them go down the ssh-agent path seems reasonable to me. We can always expand what we support later.', 'created_at': datetime.datetime(2025, 1, 27, 18, 31, 22, tzinfo=datetime.timezone.utc)}]","jx2lee (Assginee) on (2025-01-27 07:00:57 UTC): @jedcunningham Can i take this? 🚀

jx2lee (Assginee) on (2025-01-27 17:35:31 UTC): @jedcunningham I have question! To support key in connection itself, does it need to implement like [this](https://airflow.apache.org/docs/apache-airflow-providers-ssh/stable/connections/ssh.html#:~:text=private_key%20%2D%20Content%20of,the%20private%20key.), include private_key/passphrase?

jedcunningham (Issue Creator) on (2025-01-27 18:31:22 UTC): I'd keep it simple and not cover the passphrase path. If folks need ""more"", having them go down the ssh-agent path seems reasonable to me. We can always expand what we support later.

"
2807458572,issue,open,,Add `breeze doctor` sub-command for troubleshooting,"### Description

add `doctor` subcommand which will help automate troubleshooting steps - https://github.com/apache/airflow/blob/main/dev/breeze/doc/04_troubleshooting.rst

### Use case/motivation

slack conv: https://apache-airflow.slack.com/archives/CQ9QHSFQX/p1737638955864009?thread_ts=1737634915.083709&cid=CQ9QHSFQX

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rawwar,2025-01-23 17:08:15+00:00,['kunaljubce'],2025-01-27 04:44:47+00:00,,https://github.com/apache/airflow/issues/45975,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('kind:feature', 'Feature Requests')]","[{'comment_id': 2614416582, 'issue_id': 2807458572, 'author': 'kunaljubce', 'body': '@rawwar Can you assign this to me?', 'created_at': datetime.datetime(2025, 1, 26, 13, 19, 41, tzinfo=datetime.timezone.utc)}]","kunaljubce (Assginee) on (2025-01-26 13:19:41 UTC): @rawwar Can you assign this to me?

"
2807231656,issue,closed,completed,Ability to Pin DAG's to the top,"### Description

Sometimes, there will be a DAG that we constantly look into. Having them pinned to the top of the list will make it easier to access.

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rawwar,2025-01-23 15:34:36+00:00,[],2025-01-23 15:37:43+00:00,2025-01-23 15:36:17+00:00,https://github.com/apache/airflow/issues/45972,"[('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2610141506, 'issue_id': 2807231656, 'author': 'rawwar', 'body': 'I clicked ""create"" accidentally. I was searching for a similar issue and found one - https://github.com/apache/airflow/issues/45207', 'created_at': datetime.datetime(2025, 1, 23, 15, 37, 41, tzinfo=datetime.timezone.utc)}]","rawwar (Issue Creator) on (2025-01-23 15:37:41 UTC): I clicked ""create"" accidentally. I was searching for a similar issue and found one - https://github.com/apache/airflow/issues/45207

"
2806973711,issue,closed,completed,Port `_validate_inlet_outlet_assets_activeness` into Task SDK,"### Body

Port this behaviour https://github.com/apache/airflow/blob/main/airflow/models/taskinstance.py#L262 into Task Sdk. Ideally it should go into the `run` endpoint for task instances.
",amoghrajesh,2025-01-23 13:54:26+00:00,['amoghrajesh'],2025-01-30 06:34:40+00:00,2025-01-30 06:34:40+00:00,https://github.com/apache/airflow/issues/45969,"[('area:core', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2806747654,issue,closed,completed,Add edit/create Xcom for task instance with RestAPI,"### Body

Currently we have no way to manually create or edit existed Xcom.
This is needed mostly for the use case of fixing/manually recover from problems with workflows.
Ideally, later on the endpoint will be accesable from the UI as well

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2025-01-23 12:12:46+00:00,['shubhamraj-git'],2025-02-06 14:30:24+00:00,2025-02-06 14:30:22+00:00,https://github.com/apache/airflow/issues/45966,"[('area:webserver', 'Webserver related Issues'), ('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('good first issue', '')]","[{'comment_id': 2609675575, 'issue_id': 2806747654, 'author': 'shubhamraj-git', 'body': '@eladkal plz assign this to me.', 'created_at': datetime.datetime(2025, 1, 23, 12, 23, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2639987942, 'issue_id': 2806747654, 'author': 'eladkal', 'body': 'Solved by https://github.com/apache/airflow/pull/46457 and https://github.com/apache/airflow/pull/46042', 'created_at': datetime.datetime(2025, 2, 6, 14, 30, 22, tzinfo=datetime.timezone.utc)}]","shubhamraj-git (Assginee) on (2025-01-23 12:23:22 UTC): @eladkal plz assign this to me.

eladkal (Issue Creator) on (2025-02-06 14:30:22 UTC): Solved by https://github.com/apache/airflow/pull/46457 and https://github.com/apache/airflow/pull/46042

"
2806278417,issue,closed,not_planned,"Status of testing Providers that were prepared on January 23, 2025","### Body


I have a kind request for all the contributors to the latest provider packages release.
Could you please help us to test the RC versions of the providers?

The guidelines on how to test providers can be found in

[Verify providers by contributors](https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDER_PACKAGES.md#verify-the-release-candidate-by-contributors)

Let us know in the comments, whether the issue is addressed.

These are providers that require testing as there were some substantial changes introduced:


## Provider [celery: 3.10.0rc1](https://pypi.org/project/apache-airflow-providers-celery/3.10.0rc1)
   - [x] [Add support for custom celery configs (#45038)](https://github.com/apache/airflow/pull/45038): @arorasachin9
   - [x] [Fix Version Check for CLI Imports in Providers (#45255)](https://github.com/apache/airflow/pull/45255): @bugraoz93
   - [ ] [AIP-72: Support DAG parsing context in Task SDK (#45694)](https://github.com/apache/airflow/pull/45694): @kaxil
   - [ ] [AIP-72: Support better type-hinting for Context dict in SDK  (#45583)](https://github.com/apache/airflow/pull/45583): @kaxil

<!--

NOTE TO RELEASE MANAGER:

You can move here the providers that have doc-only changes or for which changes are trivial, and
you could assess that they are OK.

-->
All users involved in the PRs:
@bugraoz93 @kaxil @arorasachin9



### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2025-01-23 08:44:07+00:00,[],2025-01-26 14:35:47+00:00,2025-01-26 09:48:24+00:00,https://github.com/apache/airflow/issues/45957,"[('area:providers', ''), ('kind:meta', 'High-level information important to the community'), ('testing status', 'Status of testing releases'), ('provider:celery', '')]","[{'comment_id': 2611019684, 'issue_id': 2806278417, 'author': 'bugraoz93', 'body': 'Thanks @eladkal! I tested the changes in the earlier versions. They look good.', 'created_at': datetime.datetime(2025, 1, 23, 21, 7, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614069946, 'issue_id': 2806278417, 'author': 'arorasachin9', 'body': 'Thanks @eladkal I have tested the changes in airflow 2.7.3. These are non breaking changes. They are some extra configurations of celery. They looks good.', 'created_at': datetime.datetime(2025, 1, 25, 19, 11, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614301838, 'issue_id': 2806278417, 'author': 'eladkal', 'body': 'Closing as we are having RC2', 'created_at': datetime.datetime(2025, 1, 26, 9, 48, 23, tzinfo=datetime.timezone.utc)}]","bugraoz93 on (2025-01-23 21:07:39 UTC): Thanks @eladkal! I tested the changes in the earlier versions. They look good.

arorasachin9 on (2025-01-25 19:11:48 UTC): Thanks @eladkal I have tested the changes in airflow 2.7.3. These are non breaking changes. They are some extra configurations of celery. They looks good.

eladkal (Issue Creator) on (2025-01-26 09:48:23 UTC): Closing as we are having RC2

"
2805757191,issue,closed,completed,Let user specify repo url in git bundle kwargs,,jedcunningham,2025-01-23 02:31:21+00:00,['jx2lee'],2025-02-02 16:48:10+00:00,2025-02-02 16:48:10+00:00,https://github.com/apache/airflow/issues/45952,"[('kind:feature', 'Feature Requests'), ('AIP-66: DAG Bundle/Manifest', '')]","[{'comment_id': 2622517781, 'issue_id': 2805757191, 'author': 'ephraimbuddy', 'body': 'If repo url is in kwargs and also in connection.host, which one will override the other?', 'created_at': datetime.datetime(2025, 1, 29, 18, 29, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2622727461, 'issue_id': 2805757191, 'author': 'jedcunningham', 'body': ""I'd do kwargs wins - but this is a little different use of connections than we typically have, so either option could be argued."", 'created_at': datetime.datetime(2025, 1, 29, 20, 5, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2628845215, 'issue_id': 2805757191, 'author': 'jx2lee', 'body': '@jedcunningham I’d like to work on this. Could you assign me? 🙏', 'created_at': datetime.datetime(2025, 2, 1, 8, 11, 39, tzinfo=datetime.timezone.utc)}]","ephraimbuddy on (2025-01-29 18:29:34 UTC): If repo url is in kwargs and also in connection.host, which one will override the other?

jedcunningham (Issue Creator) on (2025-01-29 20:05:35 UTC): I'd do kwargs wins - but this is a little different use of connections than we typically have, so either option could be argued.

jx2lee (Assginee) on (2025-02-01 08:11:39 UTC): @jedcunningham I’d like to work on this. Could you assign me? 🙏

"
2805270936,issue,closed,completed,How can I connect my organization databases (using JDBC driver connection) with Airflow,"
### Discussed in https://github.com/apache/airflow/discussions/45757

<div type='discussions-op-text'>

<sup>Originally posted by **priyank-stack** January 17, 2025</sup>
Hi,
I am trying to use latest Airflow version installation (2.10.4) and setting up a databases connection.

Issue:
Once Airflow is hosted on local, I am trying to establish a JDBC connection , but Test button is disabled.
What can I do to make it enable?

![Image](https://github.com/user-attachments/assets/49a703df-4244-40c0-a4ca-7fcba3f62b40)

Purpose:
I want to connect with my Vector and Ingres Databases.

Please provide some steps to check the connectivity?
Is it possible or not to connect my ingres DB with airflow? using own JDBC driver?
</div>",priyank-stack,2025-01-22 20:20:43+00:00,[],2025-01-22 21:03:02+00:00,2025-01-22 21:03:02+00:00,https://github.com/apache/airflow/issues/45944,"[('provider:jdbc', '')]","[{'comment_id': 2608187534, 'issue_id': 2805270936, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 22, 20, 20, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2608199962, 'issue_id': 2805270936, 'author': 'priyank-stack', 'body': 'CC @hab6', 'created_at': datetime.datetime(2025, 1, 22, 20, 28, 21, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-22 20:20:46 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

priyank-stack (Issue Creator) on (2025-01-22 20:28:21 UTC): CC @hab6

"
2805218580,issue,closed,completed,OOM Protection for Multi-Stage Queries in Pinot,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

OOM protection in Pinot relies on tracking resource usage of threads that execute a query. Currently, resource accounting is only supported for Single-Stage queries. This feature adds instrumentation for the Multi-Stage query engine, allowing automatic OOM protection as resource usage is accounted for in the same data structures checked by the OOM protection thread.

### What you think should happen instead?

_No response_

### How to reproduce

NA

### Operating System

NA

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",xauravww,2025-01-22 19:50:28+00:00,[],2025-01-25 21:42:45+00:00,2025-01-22 19:51:20+00:00,https://github.com/apache/airflow/issues/45940,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2608131925, 'issue_id': 2805218580, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 22, 19, 50, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2608253889, 'issue_id': 2805218580, 'author': 'potiuk', 'body': 'This looks totally AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:\n\na) you explain how you generated reports\nb) prove you are human\nc) explain why you created the issue', 'created_at': datetime.datetime(2025, 1, 22, 20, 58, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2608307111, 'issue_id': 2805218580, 'author': 'xauravww', 'body': 'Hi ,\r\n\r\nI sincerely apologize for the issue report I created. It was not\r\nintentional.\r\n\r\nI was exploring issue templates and assumed that I would be prompted to\r\nconfirm before the issue was created. I also thought that if it got created\r\nby mistake, I would be able to delete it. This is my first time doing this,\r\nand I wasn’t familiar with the process.\r\n\r\nI assure you this won’t happen again. If there is any additional\r\ninformation or clarification you need from me, please let me know.\r\n\r\nThank you for understanding, and I’m sorry for the inconvenience caused.\r\n\r\nBest regards,\r\nSaurav Maheshwari\r\n\r\nOn Thu, Jan 23, 2025, 2:29\u202fAM Jarek Potiuk ***@***.***> wrote:\r\n\r\n> This looks totally AI-generated. useless issue report that brings no value\r\n> and makes no sense. We are generally blocking users that sends a lot of\r\n> spam AI reports generated by bots.. as of yesterday so we will report your\r\n> account and block it unless:\r\n>\r\n> a) you explain how you generated reports\r\n> b) prove you are human\r\n> c) explain why you created the issue\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/apache/airflow/issues/45940#issuecomment-2608253889>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/A2NSSBP775O6RLJP2FP34TL2MABBNAVCNFSM6AAAAABVVXIPJGVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDMMBYGI2TGOBYHE>\r\n> .\r\n> You are receiving this because you modified the open/close state.Message\r\n> ID: ***@***.***>\r\n>', 'created_at': datetime.datetime(2025, 1, 22, 21, 28, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2609119074, 'issue_id': 2805218580, 'author': 'potiuk', 'body': '@xauravww -> What kind of issue templating did you use? We seem to get a LOT of issues recently created with similar tooling and I would love to know WHAT tooling and WHY ""airflow"" and ""iceberg"" were particularly targeted by those people who were using it. Can you please share what is the tooling you used - it can be private - you will find my email in GitHub profile of mine.', 'created_at': datetime.datetime(2025, 1, 23, 8, 12, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2610480728, 'issue_id': 2805218580, 'author': 'xauravww', 'body': ""Okay I'll let you know via email."", 'created_at': datetime.datetime(2025, 1, 23, 17, 22, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614108807, 'issue_id': 2805218580, 'author': 'potiuk', 'body': 'Thanks!', 'created_at': datetime.datetime(2025, 1, 25, 21, 42, 45, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-22 19:50:32 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2025-01-22 20:58:40 UTC): This looks totally AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:

a) you explain how you generated reports
b) prove you are human
c) explain why you created the issue

xauravww (Issue Creator) on (2025-01-22 21:28:40 UTC): Hi ,

I sincerely apologize for the issue report I created. It was not
intentional.

I was exploring issue templates and assumed that I would be prompted to
confirm before the issue was created. I also thought that if it got created
by mistake, I would be able to delete it. This is my first time doing this,
and I wasn’t familiar with the process.

I assure you this won’t happen again. If there is any additional
information or clarification you need from me, please let me know.

Thank you for understanding, and I’m sorry for the inconvenience caused.

Best regards,
Saurav Maheshwari

On Thu, Jan 23, 2025, 2:29 AM Jarek Potiuk ***@***.***> wrote:

potiuk on (2025-01-23 08:12:32 UTC): @xauravww -> What kind of issue templating did you use? We seem to get a LOT of issues recently created with similar tooling and I would love to know WHAT tooling and WHY ""airflow"" and ""iceberg"" were particularly targeted by those people who were using it. Can you please share what is the tooling you used - it can be private - you will find my email in GitHub profile of mine.

xauravww (Issue Creator) on (2025-01-23 17:22:48 UTC): Okay I'll let you know via email.

potiuk on (2025-01-25 21:42:45 UTC): Thanks!

"
2804779832,issue,open,,Replace references to `airflow users`,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

In main, we removed the `airflow users` command since that was connected to FAB. But we didn't actually remove that command from docker and so when a user does something like `breeze start-airflow` they will get an error and airflow won't run.

### What you think should happen instead?

We need to ctrl+f for `airflow users` and remove all references to it in our dockerfile, scripts and documentation.

### How to reproduce

run `breeze start-airflow`

### Operating System

mac

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bbovenzi,2025-01-22 16:05:24+00:00,['jx2lee'],2025-02-05 15:05:13+00:00,,https://github.com/apache/airflow/issues/45936,"[('kind:bug', 'This is a clearly a bug'), ('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('kind:documentation', ''), ('area:core', ''), ('AIP-79', '')]","[{'comment_id': 2607652040, 'issue_id': 2804779832, 'author': 'vincbeck', 'body': 'As part of this task we also need to make sure there is an equivalent command provided by the FAB auth manager like: `airflow fab users`', 'created_at': datetime.datetime(2025, 1, 22, 16, 6, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2634210621, 'issue_id': 2804779832, 'author': 'jx2lee', 'body': '@vincbeck I’d like to work on this.\nCould you assign me?', 'created_at': datetime.datetime(2025, 2, 4, 14, 50, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2634238213, 'issue_id': 2804779832, 'author': 'vincbeck', 'body': 'For sure!', 'created_at': datetime.datetime(2025, 2, 4, 15, 0, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2637120061, 'issue_id': 2804779832, 'author': 'jx2lee', 'body': ""Umm, I didn't get error(run `breeze start-airflow`) In current `main` branch (`0a61ac80aa66835ce4db3b28f829256a33ed8590`).\nCould you explain how to reproduce this issue? When removing line `airflow users ~` in Dockerfile/ci and run breeze command, user(admin) was not found. 😢"", 'created_at': datetime.datetime(2025, 2, 5, 15, 5, 11, tzinfo=datetime.timezone.utc)}]","vincbeck on (2025-01-22 16:06:57 UTC): As part of this task we also need to make sure there is an equivalent command provided by the FAB auth manager like: `airflow fab users`

jx2lee (Assginee) on (2025-02-04 14:50:09 UTC): @vincbeck I’d like to work on this.
Could you assign me?

vincbeck on (2025-02-04 15:00:01 UTC): For sure!

jx2lee (Assginee) on (2025-02-05 15:05:11 UTC): Umm, I didn't get error(run `breeze start-airflow`) In current `main` branch (`0a61ac80aa66835ce4db3b28f829256a33ed8590`).
Could you explain how to reproduce this issue? When removing line `airflow users ~` in Dockerfile/ci and run breeze command, user(admin) was not found. 😢

"
2804463046,issue,open,,AIP-84 Allow specifying an injectable path_prefix,"### Body

Allow via a configuration parameter a way for user to specify a `path_prefix` on the fastapi application. (FastAPI app also serves the webapplication, both will be under that prefix).

We can leverage the `root_path` param of fastapi and can build something similar to what we had for the legacy_api here:
https://github.com/apache/airflow/blob/051e617e0d7d0ebb995cb98063709350f279963c/airflow/www/extensions/init_wsgi_middlewares.py#L45-L51

This will also most likely require some adjustment in the front-end part. (vite build + redirection etc... to use the appropriate prefix, otherwise we might end-up with 404 responses there).

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2025-01-22 13:55:57+00:00,['rawwar'],2025-02-06 10:35:49+00:00,,https://github.com/apache/airflow/issues/45933,"[('area:API', ""Airflow's REST/HTTP API""), ('good first issue', ''), ('kind:meta', 'High-level information important to the community'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2607334499, 'issue_id': 2804463046, 'author': 'kaxil', 'body': 'For whoever picks this issue: To know about `root_path`, `path_prefix` and the use-case check https://fastapi.tiangolo.com/advanced/behind-a-proxy/', 'created_at': datetime.datetime(2025, 1, 22, 14, 3, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2639246551, 'issue_id': 2804463046, 'author': 'phanikumv', 'body': '@rawwar are you able to progress on this issue? Do you need any help', 'created_at': datetime.datetime(2025, 2, 6, 9, 19, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2639438581, 'issue_id': 2804463046, 'author': 'rawwar', 'body': ""I will work on the issue this weekend. I'll ask for help if I get stuck."", 'created_at': datetime.datetime(2025, 2, 6, 10, 35, 48, tzinfo=datetime.timezone.utc)}]","kaxil on (2025-01-22 14:03:31 UTC): For whoever picks this issue: To know about `root_path`, `path_prefix` and the use-case check https://fastapi.tiangolo.com/advanced/behind-a-proxy/

phanikumv on (2025-02-06 09:19:30 UTC): @rawwar are you able to progress on this issue? Do you need any help

rawwar (Assginee) on (2025-02-06 10:35:48 UTC): I will work on the issue this weekend. I'll ask for help if I get stuck.

"
2804456939,issue,open,,Replace `external_trigger` check with DagRunType,"Now that we have explicit Dag Run types, we should not need `DagRun.external_trigger` too.


Example, the following code can be changed

https://github.com/apache/airflow/blob/49581b38499d0e00742b289ae177913493499fe6/providers/src/airflow/providers/standard/operators/latest_only.py#L52-L58

to

```py
    def choose_branch(self, context: Context) -> str | Iterable[str]:
        # If the DAG Run is manually triggered, then return without
        # skipping downstream tasks
        dag_run: DagRun = context[""dag_run""]  # type: ignore[assignment]
        if dag_run.run_type == DagRunType.MANUAL:
            self.log.info(""Externally triggered DAG_Run: allowing execution to proceed."")
            return list(context[""task""].get_direct_relative_ids(upstream=False))

```


We should replace all usages of `DagRun.external_trigger` and remove it from DB too.



",kaxil,2025-01-22 13:53:25+00:00,['jason810496'],2025-01-22 14:19:15+00:00,,https://github.com/apache/airflow/issues/45932,"[('area:core', '')]","[{'comment_id': 2607352181, 'issue_id': 2804456939, 'author': 'jason810496', 'body': 'Hi @kaxil, I can work on this issue, could you assign to me ? Thanks !', 'created_at': datetime.datetime(2025, 1, 22, 14, 10, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2607373912, 'issue_id': 2804456939, 'author': 'kaxil', 'body': 'Awesome, assigned it to you', 'created_at': datetime.datetime(2025, 1, 22, 14, 19, 14, tzinfo=datetime.timezone.utc)}]","jason810496 (Assginee) on (2025-01-22 14:10:36 UTC): Hi @kaxil, I can work on this issue, could you assign to me ? Thanks !

kaxil (Issue Creator) on (2025-01-22 14:19:14 UTC): Awesome, assigned it to you

"
2804276313,issue,open,,SQLExecuteQueryOperator not timing out within expected timeframe,"### Apache Airflow Provider(s)

common-sql

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==9.0.0
apache-airflow-providers-common-sql==1.19.0
apache-airflow-providers-mysql==5.7.3

### Apache Airflow version

2.10.3

### Operating System

Amazon Linux 2023

### Deployment

Amazon (AWS) MWAA

### Deployment details

The issue happens on a production Amazon (AWS) MWAA environment, and is also reproducible locally using the [MWAA local runner](https://github.com/aws/aws-mwaa-local-runner) (deployed on a Mac).

### What happened

The `SQLExecuteQueryOperator` fails with timeout, but only _after_ the query has completed.

This is the workflow currently happening: `T0: Start -> T1: Run query -> T2: Expected timeout failure -> T3: Query finishes -> T4: Operator fails on timeout`

### What you think should happen instead

Once the execution timeout is reached, the `SQLExecuteQueryOperator` should kill the query and then fail.

This is the expected workflow: `T0: Start -> T1: Run query -> T2: Expected timeout, operator kills query and fails`

### How to reproduce

The issue is reproducible both on a production Amazon (AWS) MWAA environment, and locally using the [MWAA local runner](https://github.com/aws/aws-mwaa-local-runner) (deployed on a Mac).

The DAG below contains two tasks: 
1. The first one executes a long-running query on a MySQL database using the `SQLExecuteQueryOperator`.
2. The second one executes a long-running process using the `PythonOperator`.

The `PythonOperator` fails its execution exactly after one minute (as expected), the `SQLExecuteQueryOperator` takes a few minutes until the query completes (not as expected).

```
import os
import time

from datetime import timedelta

from airflow.decorators import dag
from airflow.models.connection import Connection
from airflow.operators.python import PythonOperator
from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator


@dag(schedule=None)
def test_sql_operator_timeout_dag():

    # SQL operator test
    connection = Connection(
        conn_id=""mysql_connection"",
        conn_type=""mysql"",
        host=""xyz"",
        port=""3306"",
        schema=""test"",
        login=""xyz"",
        password=""xyz"",
    )

    env_key = f""AIRFLOW_CONN_{connection.conn_id.upper()}""
    connection_uri = connection.get_uri()
    os.environ[env_key] = connection_uri
    
    SQLExecuteQueryOperator(
        task_id=""mysql_long_query"",
        conn_id=connection.conn_id,
        sql=""select benchmark(1200000000, md5('when will it end?'));"",
        execution_timeout=timedelta(minutes=1),
    )


    # Python operator test
    def sleeping_function():
        time.sleep(120)

    PythonOperator(
        task_id=""python_long_task"",
        python_callable=sleeping_function,
        execution_timeout=timedelta(minutes=1),
    )

test_sql_operator_timeout_dag = test_sql_operator_timeout_dag()
```


### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bentorb,2025-01-22 12:35:09+00:00,[],2025-01-22 20:58:06+00:00,,https://github.com/apache/airflow/issues/45930,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('provider:common-sql', '')]","[{'comment_id': 2607133874, 'issue_id': 2804276313, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 22, 12, 35, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2607141623, 'issue_id': 2804276313, 'author': 'eladkal', 'body': 'cc @dabla maybe you will have time to look into this? Seems inconsistent behavior of SQLExecuteQueryOperator', 'created_at': datetime.datetime(2025, 1, 22, 12, 38, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2607352971, 'issue_id': 2804276313, 'author': 'dabla', 'body': 'I\'ve also encountered the same behaviour, as the connection is blocking the thread or something and thus the SQLExecuteQueryOperator task only stops once the connection returns back, at least that\'s what I experienced.  \n\nSo to avoid that issue, what I\'ve done when using the JdbcHook, was by specifying a connection timeout on the JDBC connection, that way when the task has to timeout, the connection will also (even though the query will continue to run on the database) and thus not block the thread of the Airflow worker, but this was a specific solution with JDBC connections.\n\nBellow the code:\n\n```\nhook = DbApiHook.get_hook(conn_id=""conn_id"")\n    with hook.get_conn() as conn:\n        with closing(conn.cursor()) as cur:\n            stmt = conn.jconn.createStatement()\n            if timeout:\n                stmt.setQueryTimeout(\n                    int(timeout.total_seconds())\n                )  # Set the timeout in seconds\n            stmt.executeQuery(sql)  # Execute the SQL query\n            return fetch_one_handler(cur)\n```\n\nSo maybe in the hook some kind of timeout should also be specified on the connection being used, or if you want a more finegrained approach per operator, the you could specify the connection timeout through the hook_params of the SQLExecuteQueryOperator?  Still, this al depends if the underlying connection supports it of course.', 'created_at': datetime.datetime(2025, 1, 22, 14, 10, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2607455558, 'issue_id': 2804276313, 'author': 'eladkal', 'body': 'Should we apply this logic for the underlying get hook function that we are using? While not generic to all if we can provide workaround for connections that support it I think we should.\nYet the underlying issue is not clear to me. The kill signal from timeout comes from the scheduler. Why would the task not respond to it? The `exectuion_timeout` is for Airflow, not for the underlying SQL engine.', 'created_at': datetime.datetime(2025, 1, 22, 14, 51, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2607539721, 'issue_id': 2804276313, 'author': 'dabla', 'body': ""> Should we apply this logic for the underlying get hook function that we are using? While not generic to all if we can provide workaround for connections that support it I think we should. Yet the underlying issue is not clear to me. The kill signal from timeout comes from the scheduler. Why would the task not respond to it? The `exectuion_timeout` is for Airflow, not for the underlying SQL engine.\n\nWe could, but that possibly would not work for all cases and would be a workaround of the real issue.\nIt's indeed a weird issue and I was also surprised by that behaviour.  Back then I thought it was related to the Jdbc connection and maybe there was some glitch between the Python and Java process, but now we can assume this is a generic issue related to database connections. So yes it's weird that worker doesn't get killed."", 'created_at': datetime.datetime(2025, 1, 22, 15, 23, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2607703764, 'issue_id': 2804276313, 'author': 'eladkal', 'body': ""> but now we can assume this is a generic issue related to database connections. So yes it's weird that worker doesn't get killed.\n\nI am not sure it explains it. Airflow runs a process (that happens to be database connection) Airflow should be able to kill the process. The fact that the issue happens for Sql but not for Python is really odd. From the description it sounds like something is blocking the scheduler from killing the task (because it tried to kill it after its finished). I think that the issue is with the scheduler itself."", 'created_at': datetime.datetime(2025, 1, 22, 16, 27, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2607756533, 'issue_id': 2804276313, 'author': 'dabla', 'body': ""> > but now we can assume this is a generic issue related to database connections. So yes it's weird that worker doesn't get killed.\n> \n> I am not sure it explains it. Airflow runs a process (that happens to be database connection) Airflow should be able to kill the process. The fact that the issue happens for Sql but not for Python is really odd. From the description it sounds like something is blocking the schedule from killing the task (because it tried to kill it after its finished). I think that the issue is with the scheduler itself.\n\nQuestion, at least for me, is how do you debug something like this?"", 'created_at': datetime.datetime(2025, 1, 22, 16, 49, 14, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-22 12:35:13 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2025-01-22 12:38:44 UTC): cc @dabla maybe you will have time to look into this? Seems inconsistent behavior of SQLExecuteQueryOperator

dabla on (2025-01-22 14:10:55 UTC): I've also encountered the same behaviour, as the connection is blocking the thread or something and thus the SQLExecuteQueryOperator task only stops once the connection returns back, at least that's what I experienced.  

So to avoid that issue, what I've done when using the JdbcHook, was by specifying a connection timeout on the JDBC connection, that way when the task has to timeout, the connection will also (even though the query will continue to run on the database) and thus not block the thread of the Airflow worker, but this was a specific solution with JDBC connections.

Bellow the code:

```
hook = DbApiHook.get_hook(conn_id=""conn_id"")
    with hook.get_conn() as conn:
        with closing(conn.cursor()) as cur:
            stmt = conn.jconn.createStatement()
            if timeout:
                stmt.setQueryTimeout(
                    int(timeout.total_seconds())
                )  # Set the timeout in seconds
            stmt.executeQuery(sql)  # Execute the SQL query
            return fetch_one_handler(cur)
```

So maybe in the hook some kind of timeout should also be specified on the connection being used, or if you want a more finegrained approach per operator, the you could specify the connection timeout through the hook_params of the SQLExecuteQueryOperator?  Still, this al depends if the underlying connection supports it of course.

eladkal on (2025-01-22 14:51:10 UTC): Should we apply this logic for the underlying get hook function that we are using? While not generic to all if we can provide workaround for connections that support it I think we should.
Yet the underlying issue is not clear to me. The kill signal from timeout comes from the scheduler. Why would the task not respond to it? The `exectuion_timeout` is for Airflow, not for the underlying SQL engine.

dabla on (2025-01-22 15:23:03 UTC): We could, but that possibly would not work for all cases and would be a workaround of the real issue.
It's indeed a weird issue and I was also surprised by that behaviour.  Back then I thought it was related to the Jdbc connection and maybe there was some glitch between the Python and Java process, but now we can assume this is a generic issue related to database connections. So yes it's weird that worker doesn't get killed.

eladkal on (2025-01-22 16:27:43 UTC): I am not sure it explains it. Airflow runs a process (that happens to be database connection) Airflow should be able to kill the process. The fact that the issue happens for Sql but not for Python is really odd. From the description it sounds like something is blocking the scheduler from killing the task (because it tried to kill it after its finished). I think that the issue is with the scheduler itself.

dabla on (2025-01-22 16:49:14 UTC): Question, at least for me, is how do you debug something like this?

"
2804262636,issue,open,,Make asset compilation part of the `build_wheel` hatchling build_hook,"Currently, we have `custom` build hook in hatchling that performs two things:

* retrieval of .git commit (needs git to be available on path)
* compiling www-assets (needs pre-commit to be available on path)

We are not using `npm` but `pnpm` in airflow `ui` - new Airflow 3.0 UI and we are going to drop the old `www` - `npm` driven environment. PNPM is way more portable and easy to install and 2x faster than `npm` so we could potentially move both `git` and asset compilation to  `build_wheel` build hook, so that asset compilation happens always when airlfow is installed for editable installation or when wheel is built. 

We could install pnpm via https://pnpm.io/installation#on-posix-systems and do the `git` bit might be optionally skipped if `git` is not available. 

This way the contributors will not have to worry about compiling assets when installing airlfow locally for development or building wheels - the compilation would happen automatically when `pip install -e . ` or `uv sync` or and PEP compliant ""build wheel` happens.

This has the drawback of longer first-time installation and needs a bit smart determination on how to inform that the package needs to be reinstalled / buiild_hook run during the development.


",potiuk,2025-01-22 12:28:54+00:00,[],2025-01-22 12:31:09+00:00,,https://github.com/apache/airflow/issues/45929,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]",[],
2803995534,issue,open,,Tasks finish despite marked as failed,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Having following DAG:

```python
from datetime import datetime
from airflow.operators.python import PythonOperator
from airflow import DAG
import time


def wait_callable():
    for i in range(1, 30):
        print(f'sleep #{0}', i)
        time.sleep(1)


default_args = {
    'owner': 'airflow',
    'start_date': datetime(2020, 3, 4)
}

dag = DAG(""wait_dag_python"",
          default_args=default_args,
          schedule=None)


wait_task = PythonOperator(
    task_id='wait_task',
    python_callable=wait_callable,
    dag=dag
)
```

This task is made to finish after 30s and it prints output every 1s.
On Airflow UI when I mark the task as failed, it is marked as red and the DAG finishes in failed state. However when I refresh the page later (after 30s), the DAG is green and the logfile shows that the task finished successfully with all logs (one per second)

![Image](https://github.com/user-attachments/assets/d79fd74e-31a1-4fac-a3a5-87716c8329d2)

![Image](https://github.com/user-attachments/assets/95178e6d-cf20-4ca8-94f4-43d47ef4ab16)

scheduler log:

[scheduler_log.txt](https://github.com/user-attachments/files/18503709/scheduler_log.txt)

same happens in 2.7.3 and 2.10.4

### What you think should happen instead?

UI should be more consistent if this is as designed. Now it is confusing.
The user should have an option to terminate the task if it got stuck. This is especially needed for KubernetesPodOperator tasks where this could happen easily.

### How to reproduce

Run the DAG above. Mark the task as failed while it is running. Refresh the page in 30s

### Operating System

Ubuntu

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==9.0.0
apache-airflow-providers-celery==3.8.3
apache-airflow-providers-cncf-kubernetes==9.0.1
apache-airflow-providers-common-compat==1.2.1
apache-airflow-providers-common-io==1.4.2
apache-airflow-providers-common-sql==1.19.0
apache-airflow-providers-elasticsearch==5.5.2
apache-airflow-providers-fab==1.5.0
apache-airflow-providers-ftp==3.11.1
apache-airflow-providers-google==10.25.0
apache-airflow-providers-grpc==3.6.0
apache-airflow-providers-hashicorp==3.8.0
apache-airflow-providers-http==4.13.2
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-microsoft-azure==11.0.0
apache-airflow-providers-microsoft-mssql==3.9.1
apache-airflow-providers-mysql==5.7.3
apache-airflow-providers-odbc==4.8.0
apache-airflow-providers-opsgenie==5.7.0
apache-airflow-providers-oracle==3.12.0
apache-airflow-providers-postgres==5.13.1
apache-airflow-providers-redis==3.8.0
apache-airflow-providers-sftp==4.11.1
apache-airflow-providers-smtp==1.8.0
apache-airflow-providers-sqlite==3.9.0
apache-airflow-providers-ssh==3.14.0

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

default executor is CeleryKubernetesExecutor

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",pavelpi,2025-01-22 10:27:12+00:00,[],2025-01-22 12:22:53+00:00,,https://github.com/apache/airflow/issues/45925,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2606871051, 'issue_id': 2803995534, 'author': 'potiuk', 'body': 'I am not sure @kaxil @ashb, @amoghrajesh  but I believe that is the current behaviour in Airflow 2 (though I would expect that marking task as failed shoudl also make the supervisor to kill it **nicely** - but since you are close to that part now with TaskSDK work - is that the current behaviour and maybe we can change it in Airflow 3/fix in Airlfow 2 if so ?', 'created_at': datetime.datetime(2025, 1, 22, 10, 35, 28, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-22 10:35:28 UTC): I am not sure @kaxil @ashb, @amoghrajesh  but I believe that is the current behaviour in Airflow 2 (though I would expect that marking task as failed shoudl also make the supervisor to kill it **nicely** - but since you are close to that part now with TaskSDK work - is that the current behaviour and maybe we can change it in Airflow 3/fix in Airlfow 2 if so ?

"
2803921586,issue,closed,completed,Task documentation should be visible when selecting a task,"### Description

Hello,

I think that task documentation should be visible when selecting a task and not only when selecting a task instance. 
Currently when you create a new dag or add a new task, it is not possible to check documentation rendering if task has not run.

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",darkag,2025-01-22 09:55:23+00:00,[],2025-01-22 10:29:53+00:00,2025-01-22 10:29:53+00:00,https://github.com/apache/airflow/issues/45923,"[('kind:feature', 'Feature Requests'), ('good first issue', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2606858280, 'issue_id': 2803921586, 'author': 'potiuk', 'body': 'Can you please check if it is already better in the new UI of Airflow 3 ? We are changing the UI heavily now so you can contribute your ides there - as a PR for example\n\nFor now I am converting it into a discussion', 'created_at': datetime.datetime(2025, 1, 22, 10, 29, 48, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-22 10:29:48 UTC): Can you please check if it is already better in the new UI of Airflow 3 ? We are changing the UI heavily now so you can contribute your ides there - as a PR for example

For now I am converting it into a discussion

"
2803872826,issue,closed,completed,"""Task Logs Not Displaying in Airflow UI for DAGs Running on Kubernetes""","### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

airflow version

### What happened?

When I trigger a DAG manually via the UI, it fails to start, and I receive an error message in the logs: TaskInstance not found.
This happens even though the DAG is marked as active and has no syntax errors.

### What you think should happen instead?

Triggering a DAG should run it without issues, and tasks should execute as defined in the DAG configuration.

### How to reproduce

1. Create a simple DAG with one PythonOperator.
2. Deploy it to an Airflow environment running on Docker.
3. Manually trigger the DAG via the Airflow UI.
4. Observe that the task does not start and see error logs.

### Operating System

Ubuntu 20.04 LTS

### Versions of Apache Airflow Providers

Use the command pip freeze | grep apache-airflow-providers to get the provider versions.

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Deployed using Docker Compose with the default Airflow image.
No customizations made to the environment.

### Anything else?

The issue occurs consistently on every manual DAG trigger.
Log snippet:

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",manaswini003,2025-01-22 09:34:54+00:00,[],2025-01-22 10:08:48+00:00,2025-01-22 10:08:47+00:00,https://github.com/apache/airflow/issues/45922,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2606726395, 'issue_id': 2803872826, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 22, 9, 34, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2606809545, 'issue_id': 2803872826, 'author': 'potiuk', 'body': 'This looks totally AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:\n\na) you explain how you generated reports\nb) prove you are human\nc) explain why you created the issue', 'created_at': datetime.datetime(2025, 1, 22, 10, 8, 36, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-22 09:34:58 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2025-01-22 10:08:36 UTC): This looks totally AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:

a) you explain how you generated reports
b) prove you are human
c) explain why you created the issue

"
2803803855,issue,closed,completed,KubernetesPodOperator dry_run failure,"### Apache Airflow Provider(s)

cncf-kubernetes

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes 4.3.0

### Apache Airflow version

2.3.4

### Operating System

Linux

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### What happened

We are upgrading from apache-airflow-providers-cncf-kubernetes version 3.0.0 to 4.3.0, gradually moving through releases.

Our current setup includes a custom script that, during the Docker image build for Airflow, tests all DAGs and tasks in dry_run mode. This script is essential for detecting issues such as Python syntax errors, DAG cycles, duplicate tasks, import errors, and template rendering issues.

This process was functioning seamlessly with our existing Airflow version. However, as part of our upgrade to a newer version of Airflow (and its providers), we encountered an issue with the KubernetesPodOperator's dry_run functionality after addressing several other compatibility challenges.

Problem Description
The new dry_run implementation introduced in [this commit](https://github.com/apache/airflow/commit/d56ff765e15f9fcd582bc6d1ec0e83b0fedf476a) invokes the build_pod_request_obj method. This method calls the self.hook.is_in_cluster property, which attempts to construct a Kubernetes API client object and requires kube client credentials.

```
pod.metadata.labels.update(
    {
        'airflow_version': airflow_version.replace('+', '-'),
        'airflow_kpo_in_cluster': str(self.hook.is_in_cluster),
    }
)
```
The issue lies in the is_in_cluster property:
```
@property
def is_in_cluster(self):
    """"""Expose whether the hook is configured with ``load_incluster_config`` or not""""""
    if self._is_in_cluster is not None:
        return self._is_in_cluster
    self.api_client  # so we can determine if we are in_cluster or not
    return self._is_in_cluster

```
This behavior causes the dry_run to fail in an isolated test environment without valid kube config or credentials.

Error Traceback
When running in this environment, the following error occurs:

`kubernetes.config.config_exception.ConfigException: Invalid kube-config file. No configuration found.
`
Desired Behavior
We want to continue using dry_run to test DAGs without requiring Kubernetes credentials or kube config. The tests don't need to be fully accurate but should bypass unnecessary configurations.

Proposed Solutions
Introduce an Environment Variable
Add an environment variable to bypass the airflow_kpo_in_cluster label setting in dry_run mode when explicitly requested by the user.

Modify build_pod_request_obj Signature
Change the method signature of build_pod_request_obj to include a dry_run: bool = False keyword argument.

In the KubernetesPodOperator.dry_run method, invoke build_pod_request_obj(dry_run=True).
Skip setting the airflow_kpo_in_cluster label if dry_run=True.
Implement Both Options
Combine both approaches to provide flexibility.

These changes would enable dry_run functionality to work in isolated environments without requiring kube config or credentials while maintaining compatibility with the operator's primary functionality.

### What you think should happen instead

The KubernetesPodOperator.dry_run should allow testing DAGs and tasks without requiring Kubernetes credentials or kube config. This would ensure that the dry_run functionality can execute in isolated or non-production environments without unnecessary dependencies. It does not need to fully mimic runtime behavior but should bypass features dependent on Kubernetes API calls, such as the airflow_kpo_in_cluster label.

What Went Wrong
The new dry_run implementation attempts to evaluate self.hook.is_in_cluster, which requires constructing a Kubernetes API client. This step fails when no kube config or credentials are provided in the test environment, leading to a ConfigException.

Supporting Evidence
Error Log Fragment

`kubernetes.config.config_exception.ConfigException: Invalid kube-config file. No configuration found.
`
Root Cause
The build_pod_request_obj method in dry_run mode invokes self.hook.is_in_cluster as part of adding metadata labels:

`'airflow_kpo_in_cluster': str(self.hook.is_in_cluster),`
This property requires access to Kubernetes credentials, which are not available in the testing environment.

### How to reproduce

Set Up the Environment

Use Apache Airflow 2.10.4 and upgrade the apache-airflow-providers-cncf-kubernetes package to version 4.3.0.
Ensure Kubernetes credentials (kube config) are not provided in the environment.
Prepare a DAG with KubernetesPodOperator

Create a DAG that uses the KubernetesPodOperator.

Run Dry Run Test

Verify the Error
Confirm that the dry run fails with a kubernetes.config.config_exception.ConfigException error due to missing kube config or credentials.

### Anything else

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Arjit-Sharma001,2025-01-22 09:05:40+00:00,[],2025-01-22 20:43:32+00:00,2025-01-22 20:43:32+00:00,https://github.com/apache/airflow/issues/45916,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('pending-response', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2607730258, 'issue_id': 2803803855, 'author': 'nathadfield', 'body': '@Arjit-Sharma001 Given that the current version of the cncf provider is [10.1](https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/stable/changelog.html), there have clearly been many bug fixes and improvements since 4.3.0.  There will not be any fixes to old versions so, if you are able to, can you test your problem against a newer version?', 'created_at': datetime.datetime(2025, 1, 22, 16, 38, 10, tzinfo=datetime.timezone.utc)}]","nathadfield on (2025-01-22 16:38:10 UTC): @Arjit-Sharma001 Given that the current version of the cncf provider is [10.1](https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/stable/changelog.html), there have clearly been many bug fixes and improvements since 4.3.0.  There will not be any fixes to old versions so, if you are able to, can you test your problem against a newer version?

"
2803797552,issue,closed,completed,Airflow DAG scheduler fails to trigger tasks on version 2.4.1,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

Apache Airflow 2.4.1

### What happened?

The scheduler fails to trigger tasks on certain DAGs. The tasks remain in the ‘queued’ state indefinitely without executing. This started after upgrading to Airflow 2.4.1.

### What you think should happen instead?

The tasks should transition from the ‘queued’ state to the ‘running’ state, and be executed as per the DAG schedule.

### How to reproduce

Provide clear, step-by-step instructions to reproduce the problem. Example:

Install Apache Airflow 2.4.1.
Create a simple DAG with two tasks.
Trigger the DAG and observe that the tasks stay in the ‘queued’ state without execution.

### Operating System

Ubuntu 20.04

### Versions of Apache Airflow Providers

Ubuntu 20.04

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

apache-airflow-providers-google 2.0.1

### Anything else?

Deployed using Helm chart on Kubernetes with 3 replicas
This issue occurs every time the DAG is triggered after the upgrade to 2.4.1. Logs indicate the scheduler is not picking up the task in the ‘queued’ state

[2025-01-22 12:34:56] ERROR task_scheduler: Task stuck in 'queued' state  


### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Umesh7987,2025-01-22 09:03:02+00:00,[],2025-01-22 09:18:30+00:00,2025-01-22 09:16:19+00:00,https://github.com/apache/airflow/issues/45915,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2606654464, 'issue_id': 2803797552, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 22, 9, 3, 5, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-22 09:03:05 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2803780351,issue,closed,completed,Add a Deferrable Version of MSGraphSensor,"### Apache Airflow Provider(s)

microsoft-mssql, microsoft-azure

### Versions of Apache Airflow Providers

## Description  
This feature lets the sensor give up control when not needed, making a deferrable version of MSGraphSensor. This will help use resources better. It will also make the Microsoft Graph connection in Apache Airflow work more efficiently.

## Use Case / Motivation  
This change will improve Apache Airflow's Microsoft Graph connection by:  
- Using resources better for long tasks.  
- Helping workflows with limited resources that can grow.  
- Handling mistakes well, especially for responses that are not in JSON format or are empty.

With this new sensor, users can better manage their work with Microsoft Graph services.



### Apache Airflow version

It is Airflow 2.

### Operating System

Windows

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

The deployment involves updating Apache Airflow with the new version of the MSGraphSensor. This version allows the sensor to give up control when not needed, making it more efficient. It will be used in workflows that connect with Microsoft Graph services. The goal is to save resources and handle errors better. Once deployed, users will see better performance and fewer problems with long tasks.

### What happened

What happened is that a new feature was added to Apache Airflow. This feature allows the MSGraphSensor to give up control when it is unnecessary. This helps save resources and makes the system more efficient. The new version of the sensor will also handle errors better. It will be useful for managing tasks that use Microsoft Graph services.









### What you think should happen instead

What should happen is that the MSGraphSensor should work more efficiently. It should give up control when not needed and only use resources when necessary. If there is an error, the system should handle it correctly. This would make it easier for users to manage their tasks. The sensor should help improve the system, not make it harder to use.

### How to reproduce

To reproduce the problem, you need to use the MSGraphSensor in Apache Airflow. Try running a task that uses Microsoft Graph services. Then, see if the sensor is not giving up control when it should. You might notice it using too many resources. If the sensor does not work correctly, you can then check for errors.

### Anything else

Yes, it is important to know that this new feature will help Apache Airflow run better. The MSGraphSensor will use resources more efficiently, especially for long tasks. It will also handle errors well, such as when responses are empty or not in the right format. This will help users save time and avoid problems when working with Microsoft Graph services.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Vijaykumar069,2025-01-22 08:55:21+00:00,[],2025-01-24 06:00:45+00:00,2025-01-22 12:31:38+00:00,https://github.com/apache/airflow/issues/45913,"[('kind:bug', 'This is a clearly a bug'), ('provider:microsoft-azure', 'Azure-related issues'), ('area:providers', ''), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:microsoft-mssql', ''), ('AI Spam', '')]","[{'comment_id': 2606637593, 'issue_id': 2803780351, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 22, 8, 55, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2606654630, 'issue_id': 2803780351, 'author': 'eladkal', 'body': '@Vijaykumar069 your GitHub account looks of a real user.\nplease explain if you are related to the opening of https://github.com/apache/airflow/issues/45912 ?', 'created_at': datetime.datetime(2025, 1, 22, 9, 3, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2607126716, 'issue_id': 2803780351, 'author': 'potiuk', 'body': 'This looks totally AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:\n\na) you explain how you generated reports\nb) prove you are human\nc) explain why you created the issue', 'created_at': datetime.datetime(2025, 1, 22, 12, 31, 38, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-22 08:55:23 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2025-01-22 09:03:09 UTC): @Vijaykumar069 your GitHub account looks of a real user.
please explain if you are related to the opening of https://github.com/apache/airflow/issues/45912 ?

potiuk on (2025-01-22 12:31:38 UTC): This looks totally AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:

a) you explain how you generated reports
b) prove you are human
c) explain why you created the issue

"
2803749531,issue,closed,not_planned,Add a Deferrable Version of MSGraphSensor,"### Description

This feature allows the sensor to relinquish control during periods of inactivity, introducing a deferrable version of the MSGraphSensor to maximize resource use. The efficiency of sensor operations within the Microsoft Graph integration for Apache Airflow is increased by this improvement.




### Use case/motivation

The purpose of this enhancement is to improve Apache Airflow's current Microsoft Graph integration by:

increasing the effectiveness of resources for lengthy jobs.
facilitating resource-constrained workflows that are scalable.
addressing any mistakes by using a strong response handler for non-JSON or empty responses.
Users can more effectively manage workloads involving Microsoft Graph services by putting this deferrable sensor into practice.


### Related issues

N/A

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",svrajesh21,2025-01-22 08:41:24+00:00,[],2025-01-22 08:48:40+00:00,2025-01-22 08:48:32+00:00,https://github.com/apache/airflow/issues/45912,"[('kind:feature', 'Feature Requests'), ('area:async-operators', 'AIP-40: Deferrable (""Async"") Operators'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2606623710, 'issue_id': 2803749531, 'author': 'eladkal', 'body': 'AI spam ticket', 'created_at': datetime.datetime(2025, 1, 22, 8, 48, 32, tzinfo=datetime.timezone.utc)}]","eladkal on (2025-01-22 08:48:32 UTC): AI spam ticket

"
2803721929,issue,closed,not_planned,Task Instance Fails to Update State in Airflow 2.5.0,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

The task instances in Airflow 2.5.0 fail to correctly update their state during execution. Even when tasks complete successfully, their status is not updated in the Airflow metadata database, which causes incorrect DAG runs and monitoring issues.

### What happened?

The state of task instances should be updated accurately in the database to reflect their true execution status. This ensures that downstream processes and monitoring tools work as expected.

### What you think should happen instead?

_No response_

### How to reproduce

Create a simple DAG with one or more tasks.
Execute the DAG in Airflow 2.5.0.
Monitor the state of the task instances in the Airflow UI or the metadata database.
Observe that the task states do not update correctly, even after successful execution.

### Operating System

Linux (Ubuntu 22.04)

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==6.1.0  
apache-airflow-providers-google==9.0.0  
apache-airflow-providers-cncf-kubernetes==7.0.0  


### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Docker Compose



### Anything else?

This issue occurs intermittently but is more frequent under high-concurrency scenarios.
Relevant log excerpts:
[2025-01-22 10:32:15,890] {taskinstance.py:1194} ERROR - Task state update failed  
[2025-01-22 10:32:15,890] {scheduler_job.py:1822} INFO - DAG 'example_dag' remains in a running state despite task completion  


### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",missaarohi,2025-01-22 08:28:47+00:00,[],2025-01-22 08:52:40+00:00,2025-01-22 08:52:35+00:00,https://github.com/apache/airflow/issues/45911,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2606631812, 'issue_id': 2803721929, 'author': 'eladkal', 'body': 'This is spam. Also exact same title from another spam user https://github.com/apache/airflow/issues/45903', 'created_at': datetime.datetime(2025, 1, 22, 8, 52, 35, tzinfo=datetime.timezone.utc)}]","eladkal on (2025-01-22 08:52:35 UTC): This is spam. Also exact same title from another spam user https://github.com/apache/airflow/issues/45903

"
2803534851,issue,closed,not_planned,feature update,"### Description

Summary: Provide a short and clear description of the feature you are requesting.

Motivation: Explain why this feature is needed. Describe the problem that the feature will solve or the improvement it will bring to the current system.

Detailed Description: Provide a detailed explanation of the feature. Include specifics about how the feature should behave, what new functionality it will provide, and any user flows or processes that need to be taken into consideration. If relevant, provide examples or use cases.

Expected Behavior: Describe what the expected outcome or behavior of the feature should be once it is implemented. This may include details on how the feature should be integrated with existing functionality.

Additional Context: Add any other context that might be helpful for understanding the feature request. This might include links to related issues, possible challenges or considerations, and technical constraints.

Possible Solution (Optional): If you have a potential solution in mind, describe it here. You may also include any code snippets, diagrams, or architectural changes that would be involved.

Alternative Approaches (Optional): If applicable, explain any alternative approaches that have been considered, or why this particular solution is preferred.



### Use case/motivation

Summary: Provide a short and clear description of the feature you are requesting.

Motivation: Explain why this feature is needed. Describe the problem that the feature will solve or the improvement it will bring to the current system.

Detailed Description: Provide a detailed explanation of the feature. Include specifics about how the feature should behave, what new functionality it will provide, and any user flows or processes that need to be taken into consideration. If relevant, provide examples or use cases.

Expected Behavior: Describe what the expected outcome or behavior of the feature should be once it is implemented. This may include details on how the feature should be integrated with existing functionality.

Additional Context: Add any other context that might be helpful for understanding the feature request. This might include links to related issues, possible challenges or considerations, and technical constraints.

Possible Solution (Optional): If you have a potential solution in mind, describe it here. You may also include any code snippets, diagrams, or architectural changes that would be involved.

Alternative Approaches (Optional): If applicable, explain any alternative approaches that have been considered, or why this particular solution is preferred.



### Related issues

_No response_

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",r2000030871,2025-01-22 06:44:45+00:00,[],2025-01-22 06:58:38+00:00,2025-01-22 06:58:38+00:00,https://github.com/apache/airflow/issues/45909,"[('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2606422106, 'issue_id': 2803534851, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 22, 6, 44, 47, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-22 06:44:47 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2803451155,issue,closed,not_planned,"Briefly describe the issue in one sentence (e.g., ""Task fails to execute on retry with custom operator"").","### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Provide a clear and concise description of the issue.
For example:
""The task fails on retry when using a custom operator due to an unexpected error in the logs.""

### What you think should happen instead?

Explain the expected behavior.
For example:
""The task should successfully retry when using a custom operator without throwing errors.""

### How to reproduce

1.Install Apache Airflow version [mention version].
2.Create a DAG with [specific steps or code snippet].
3.Trigger the DAG and observe [specific outcome].

### Operating System

Windows

### Versions of Apache Airflow Providers

Run pip freeze | grep apache-airflow-providers and paste relevant results here.



### Deployment

Virtualenv installation

### Deployment details

Include details about your deployment (e.g., Docker, Helm, Kubernetes, etc.), customization, and configuration.



### Anything else?

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",atharv9017,2025-01-22 05:51:11+00:00,[],2025-01-22 06:28:54+00:00,2025-01-22 06:28:53+00:00,https://github.com/apache/airflow/issues/45907,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2606402310, 'issue_id': 2803451155, 'author': 'eladkal', 'body': 'Issue doesnt make sense and looks like a bot report', 'created_at': datetime.datetime(2025, 1, 22, 6, 28, 53, tzinfo=datetime.timezone.utc)}]","eladkal on (2025-01-22 06:28:53 UTC): Issue doesnt make sense and looks like a bot report

"
2803352888,issue,closed,completed,Airflow Bug Report,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

Airflow Bug Report

### What happened?

While running a DAG with a task configured to have max_retries=3, the task did not stop retrying after 3 failed attempts. Instead, it kept retrying indefinitely, ignoring the configured limit.

Context in which the problem occurred:
I encountered this issue after upgrading Apache Airflow from version 2.7.2 to 2.7.3. The DAG is set up to use the CeleryExecutor, and the task in question is a PythonOperator that calls a simple Python function. The problem is reproducible every time the task fails.

### What you think should happen instead?

The task should fail permanently after exhausting the configured max_retries=3 attempts. The system should respect the max_retries parameter and not continue retrying indefinitely.

What went wrong?
It appears that the max_retries parameter is either being ignored or overridden. This may be due to a regression introduced in the upgrade from version 2.7.2 to 2.7.3, or it could be related to the CeleryExecutor not correctly interpreting the max_retries setting.

### How to reproduce

Steps to Reproduce the Problem:

Environment Setup:

Airflow Version: 2.7.3
Executor: CeleryExecutor
Scheduler: Default Airflow Scheduler
Python Version: 3.10
Database: PostgreSQL 13
Operating System: Ubuntu 22.04
DAG Configuration:
Create a minimal DAG with the following structure:

python
Copy
Edit
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

def fail_task():
    raise ValueError(""This task is designed to fail."")

with DAG(
    dag_id='retry_issue_demo',
    start_date=datetime(2025, 1, 1),
    schedule_interval=None,
    catchup=False,
) as dag:
    failing_task = PythonOperator(
        task_id='failing_task',
        python_callable=fail_task,
        retries=3,  # Explicitly setting max_retries
    )
Execution:

Deploy the DAG to your Airflow environment.
Trigger the DAG manually via the Airflow UI or CLI.
Observation:

Monitor the task failing_task in the Airflow UI.
Observe the task retry behavior and note whether it respects the configured retries=3.
Expected Behavior:
The task should fail permanently after three retry attempts.

Actual Behavior:
The task continues to retry indefinitely, disregarding the retries parameter.

### Operating System

Ubuntu 22.04 LTS

### Versions of Apache Airflow Providers

apache-airflow-providers-google==7.0.0
apache-airflow-providers-amazon==3.0.0
apache-airflow-providers-slack==2.0.0


### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Abhimanuchauhan01,2025-01-22 04:35:04+00:00,[],2025-01-22 08:56:54+00:00,2025-01-22 08:56:52+00:00,https://github.com/apache/airflow/issues/45905,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:celery', '')]","[{'comment_id': 2606275233, 'issue_id': 2803352888, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 22, 4, 35, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2606640598, 'issue_id': 2803352888, 'author': 'potiuk', 'body': ""We are recently flodded by a number of AI -generated issues and I am not sure if that's not one as well. I will close it for now - but if you are human and it's a legitimate one. please update your report with some screenshots showing the failed task and correct formatting of your example to be python-formatted."", 'created_at': datetime.datetime(2025, 1, 22, 8, 56, 52, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-22 04:35:06 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2025-01-22 08:56:52 UTC): We are recently flodded by a number of AI -generated issues and I am not sure if that's not one as well. I will close it for now - but if you are human and it's a legitimate one. please update your report with some screenshots showing the failed task and correct formatting of your example to be python-formatted.

"
2803341505,issue,closed,not_planned,Task Instance Fails to Update State in Airflow 2.5.0,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

2.4.2

### What happened?

While running a DAG containing multiple tasks with dependencies, one of the tasks failed to update its state to success after successful execution. This issue caused downstream tasks to remain in the queued state indefinitely, effectively halting the DAG execution. Logs from the Airflow scheduler indicated potential anomalies in the TaskInstance.set_state method, which may not be handling state updates correctly under specific conditions. Testing with the latest main branch showed the issue still persists.

### What you think should happen instead?

The affected task should update its state to success immediately upon completion. This state transition should unblock and trigger the execution of dependent downstream tasks. Logs should also confirm the successful state update.

### How to reproduce

	1.	Define a DAG with a sequence of dependent tasks.
	2.	Deploy the DAG on Apache Airflow 2.5.0.
	3.	Trigger the DAG execution.
	4.	Monitor task states via the Airflow UI or CLI.
	5.	Check logs for any errors related to TaskInstance.set_state.

### Operating System

Ubuntu 20.04 (Command: cat /etc/os-release output)

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==6.1.1
apache-airflow-providers-apache-hive==4.0.0
apache-airflow-providers-google==8.2.0
apache-airflow-providers-slack==7.0.0

### Deployment

Docker-Compose

### Deployment details

	•	Using docker-compose.yaml for a standalone Airflow deployment.
	•	PostgreSQL as metadata database.
	•	Redis as Celery backend.
	•	Versioned configuration with minor customizations in airflow.cfg.

### Anything else?

	•	This issue occurs intermittently, especially under heavy task loads or during high parallelism.
	•	Relevant logs:
[2025-01-22 10:12:33,123] {{scheduler_job.py:139}} ERROR - Failed to update task state to success for task_id=""example_task_1"". Exception: Connection timed out.

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Bhhsaurabh,2025-01-22 04:26:03+00:00,[],2025-01-22 08:52:17+00:00,2025-01-22 08:52:11+00:00,https://github.com/apache/airflow/issues/45903,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2606266511, 'issue_id': 2803341505, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 22, 4, 26, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2606631014, 'issue_id': 2803341505, 'author': 'eladkal', 'body': 'This is spam. Also exact same title from another spam user https://github.com/apache/airflow/issues/45911', 'created_at': datetime.datetime(2025, 1, 22, 8, 52, 11, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-22 04:26:06 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2025-01-22 08:52:11 UTC): This is spam. Also exact same title from another spam user https://github.com/apache/airflow/issues/45911

"
2803338842,issue,closed,completed,Workflow Test Commit Causes Skipped Checks or CI Failures,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

The PR created for testing workflows resulted in some checks being skipped during the GitHub Actions pipeline execution. Specifically, checks such as ""React UI tests,"" ""Static checks: basic checks only,"" and ""Build documentation (--docs-only)"" were skipped, which could indicate issues with the CI configuration or dependencies missing in the environment. This is not expected behavior, as all checks should run unless explicitly excluded or intentionally skipped.

### What you think should happen instead?

All the tests, including UI, static checks, and documentation checks, should run successfully without being skipped unless intentionally configured otherwise. There shouldn't be any skipped tests unless they are non-essential or have been excluded for a valid reason.

### How to reproduce

1. Create a PR with test commits designed to check the workflow (e.g., for testing CI behavior).
2. Monitor the GitHub Actions pipeline.
3. Observe that certain tests (e.g., React UI tests, Static checks) are skipped while others complete successfully.
4. Investigate the CI configuration or dependencies to find potential causes.

### Operating System

Ubuntu

### Versions of Apache Airflow Providers

#output
$ pip freeze | grep apache-airflow-providers
apache-airflow-providers-celery==2.0.0
apache-airflow-providers-amazon==2.0.0
apache-airflow-providers-google==2.0.0


### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Deployment Type: GitHub Actions (CI pipeline)
Deployment Details: Running on Ubuntu with Python 3.9 in GitHub Actions CI pipeline.

### Anything else?

The problem seems to occur consistently for all workflows triggered by this PR. Skipped checks, particularly those related to static analysis and UI tests, suggest a potential configuration issue or misconfigured dependencies.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",missaarohi,2025-01-22 04:24:01+00:00,[],2025-01-22 08:30:24+00:00,2025-01-22 08:30:16+00:00,https://github.com/apache/airflow/issues/45902,"[('kind:bug', 'This is a clearly a bug'), ('area:CI', ""Airflow's tests and continious integration""), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2606587805, 'issue_id': 2803338842, 'author': 'potiuk', 'body': 'No. We have https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md that implement deliberate skipping of some parts of test workflow where the heuristics analysis in `selective checks` determines that some of the tests can be skipped. This is to make the tests faster and take less resources. \n\nThere are sometimes false-negatives - i.e. skipping tests where they should be run, but we are looking at those cases and fixing them as we find them, also we have ""canary"" builds in `main` run every 6 hours that run full set of tests in main to see if we accidentally merged something that should have not passed the tests in PR.\n\nIf you have concrete example where tests did not run when they should, feel free to report them individually in separate issues - or just explain it in`#internal-airflow-ci-cd` slack channel on Airlfow Slack.', 'created_at': datetime.datetime(2025, 1, 22, 8, 30, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2606588043, 'issue_id': 2803338842, 'author': 'potiuk', 'body': 'Closing as invalid.', 'created_at': datetime.datetime(2025, 1, 22, 8, 30, 23, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-22 08:30:16 UTC): No. We have https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/04_selective_checks.md that implement deliberate skipping of some parts of test workflow where the heuristics analysis in `selective checks` determines that some of the tests can be skipped. This is to make the tests faster and take less resources. 

There are sometimes false-negatives - i.e. skipping tests where they should be run, but we are looking at those cases and fixing them as we find them, also we have ""canary"" builds in `main` run every 6 hours that run full set of tests in main to see if we accidentally merged something that should have not passed the tests in PR.

If you have concrete example where tests did not run when they should, feel free to report them individually in separate issues - or just explain it in`#internal-airflow-ci-cd` slack channel on Airlfow Slack.

potiuk on (2025-01-22 08:30:23 UTC): Closing as invalid.

"
2803336627,issue,closed,completed,Add support for filtering S3 keys by last modified time in S3DeleteObjectsOperator,"### Description

This feature introduces the ability to filter S3 keys based on their last modified time when using the S3DeleteObjectsOperator. Users can specify a time range or timestamp criteria, enabling more precise control over which objects are deleted from an S3 bucket.

### Use case/motivation

The primary use case for this feature is to allow users to delete objects in an S3 bucket selectively, based on their last modified time. This is particularly useful in scenarios where users need to manage object lifecycle policies programmatically or clean up outdated files without affecting recent data.

For example, users could configure the operator to delete objects older than 30 days, ensuring that only stale data is removed. This adds flexibility and efficiency to S3 object management tasks in Apache Airflow.

### Related issues

No existing issue directly addresses this feature. However, this addition enhances the functionality of S3DeleteObjectsOperator.

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",svrajesh21,2025-01-22 04:22:03+00:00,[],2025-01-22 08:36:16+00:00,2025-01-22 08:21:30+00:00,https://github.com/apache/airflow/issues/45901,"[('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2606571001, 'issue_id': 2803336627, 'author': 'potiuk', 'body': 'This looks totally AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:\n\na) you explain how you generated reports\nb) prove you are human\nc) explain why you created the issue', 'created_at': datetime.datetime(2025, 1, 22, 8, 21, 30, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-22 08:21:30 UTC): This looks totally AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:

a) you explain how you generated reports
b) prove you are human
c) explain why you created the issue

"
2803326095,issue,closed,completed,Import Future Annotations in Virtual Environment Jinja Template Causes Errors,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When using the python_virtualenv_script.jinja2 template in Apache Airflow, the absence of from __future__ import annotations caused errors in tasks with type annotations.

### What you think should happen instead?

The template should include from __future__ import annotations by default to support forward references in type annotations, ensuring compatibility and preventing errors. The template did not include the necessary import statement for future annotations, leading to type errors when annotations were used.

### How to reproduce

1. Create a task using the virtual environment decorator with type annotations in Airflow.
2. Run the task and observe the type errors due to missing from __future__ import annotations.

### Operating System

Windows 10

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other 3rd-party Helm chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jjavieralonso,2025-01-22 04:14:03+00:00,[],2025-01-22 08:36:15+00:00,2025-01-22 07:46:14+00:00,https://github.com/apache/airflow/issues/45900,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2606510414, 'issue_id': 2803326095, 'author': 'potiuk', 'body': 'Your reports are AI spam and create a lot of effort for maintainers.\n\nI am closing the reports and blocking you. If you wish to get unblocked explain here:\n\n1) how you generated those reports\n2) prove you are human not bot\n3) explain why are you doing it?', 'created_at': datetime.datetime(2025, 1, 22, 7, 46, 14, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-22 07:46:14 UTC): Your reports are AI spam and create a lot of effort for maintainers.

I am closing the reports and blocking you. If you wish to get unblocked explain here:

1) how you generated those reports
2) prove you are human not bot
3) explain why are you doing it?

"
2803323284,issue,closed,completed,Implement start/end/debug for multiple executors in scheduler job,"### Description

The current Airflow scheduler job does not support managing multiple executors simultaneously. This limitation prevents users from debugging, starting, and stopping tasks across multiple executors efficiently. This PR aims to address this by introducing the ability to start, stop, and debug tasks across multiple executors within the scheduler job.

### Use case/motivation

This feature is crucial for users managing complex Airflow setups with multiple executors. It improves the flexibility of managing workflows by allowing users to start, stop, and debug tasks across different executors directly from the scheduler job.

### Related issues

_No response_

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",sivatenneti,2025-01-22 04:11:23+00:00,[],2025-01-22 08:36:15+00:00,2025-01-22 08:13:35+00:00,https://github.com/apache/airflow/issues/45899,"[('area:Scheduler', 'including HA (high availability) scheduler'), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2606566373, 'issue_id': 2803323284, 'author': 'potiuk', 'body': 'This looks totally AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:\n\na) you explain how you generated reports\nb) prove you are human\nc) explain why you created the issue', 'created_at': datetime.datetime(2025, 1, 22, 8, 19, 15, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-22 08:19:15 UTC): This looks totally AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:

a) you explain how you generated reports
b) prove you are human
c) explain why you created the issue

"
2803296990,issue,closed,completed,Practice sample,"### Description

During the implementation of authentication protection for AIP-69, I realized that the internal API does not carry support for authentication access. The internal API lacks token-based authentication. Implementing token authentication improves the security of the API by requiring a token in the JSON request body. This will align the internal API with other system areas that already implement authentication mechanisms (AIP-69).

### Use case/motivation

This change addresses a gap in authentication and enhances the security of the internal API without adding unnecessary complexity.

### Related issues

Since https://github.com/apache/airflow/pull/40897—an alternate PR utilizing token authentication on internal API—has been reviewed

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Sabbir02,2025-01-22 03:46:24+00:00,[],2025-01-22 08:36:14+00:00,2025-01-22 03:46:38+00:00,https://github.com/apache/airflow/issues/45897,"[('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2606564555, 'issue_id': 2803296990, 'author': 'potiuk', 'body': 'This looks totally AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:\n\na) you explain how you generated reports\nb) prove you are human\nc) explain why you created the issue', 'created_at': datetime.datetime(2025, 1, 22, 8, 18, 19, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-22 08:18:19 UTC): This looks totally AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:

a) you explain how you generated reports
b) prove you are human
c) explain why you created the issue

"
2803286247,issue,closed,completed,HTTP 500 Error when triggering DAG with invalid parameters,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When attempting to trigger a DAG through the stable API with invalid parameters, the API returns an HTTP 500 Internal Server Error. The error message shows 'Uuups'. This happens due to missing validation for the parameters before triggering the DAG.

### What you think should happen instead?

The API should return a 400 Bad Request with appropriate error details instead of a 500 error.

### How to reproduce

1. Use the stable API to trigger a DAG.
2. Send invalid parameters (e.g., missing required fields or incorrect values).
3. Observe the returned HTTP 500 error.

### Operating System

Ubuntu 20.04

### Versions of Apache Airflow Providers

apache-airflow-providers-google==5.0.0

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [ ] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",missaarohi,2025-01-22 03:36:42+00:00,[],2025-01-22 08:17:56+00:00,2025-01-22 08:17:48+00:00,https://github.com/apache/airflow/issues/45896,"[('kind:bug', 'This is a clearly a bug'), ('area:API', ""Airflow's REST/HTTP API""), (""won't fix"", ''), ('area:core', '')]","[{'comment_id': 2606216661, 'issue_id': 2803286247, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 22, 3, 36, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2606563596, 'issue_id': 2803286247, 'author': 'potiuk', 'body': ""Won't fix - we are rewriting the APIs in Airlfow 3 with FastAPI and Pydantic validation - so validation will be built in. There are no plans to enhance the behaviour and make any changes to the API framework used in Airflow 2."", 'created_at': datetime.datetime(2025, 1, 22, 8, 17, 48, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-22 03:36:44 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2025-01-22 08:17:48 UTC): Won't fix - we are rewriting the APIs in Airlfow 3 with FastAPI and Pydantic validation - so validation will be built in. There are no plans to enhance the behaviour and make any changes to the API framework used in Airflow 2.

"
2803286208,issue,closed,not_planned,Add PowerBI Dataset Refresh Operator to Apache Airflow,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Currently, there is no operator in Apache Airflow to trigger a Power BI dataset refresh. This creates limitations for workflows that depend on real-time or scheduled Power BI dataset updates.

### What you think should happen instead?

An operator should be introduced to enable users to refresh Power BI datasets from Apache Airflow. This operator should allow functionality such as specifying dataset IDs, workspace IDs, force refresh, and termination options.

### How to reproduce

N/A

### Operating System

Windows

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

This operator should include XCom integration for downstream tasks, provide monitoring links to Power BI, and allow custom connection forms for easy configuration.



### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",atharv9017,2025-01-22 03:36:40+00:00,[],2025-01-22 08:36:14+00:00,2025-01-22 06:34:25+00:00,https://github.com/apache/airflow/issues/45895,"[('kind:bug', 'This is a clearly a bug'), ('kind:feature', 'Feature Requests'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2606409039, 'issue_id': 2803286208, 'author': 'eladkal', 'body': 'AI generated issue. Closing.', 'created_at': datetime.datetime(2025, 1, 22, 6, 34, 25, tzinfo=datetime.timezone.utc)}]","eladkal on (2025-01-22 06:34:25 UTC): AI generated issue. Closing.

"
2803277281,issue,closed,completed,Refactor Azure Synapse Pipeline Hook and Add Managed Identity Support,"### Description

This feature introduces a new base class BaseAzureSynapseHook to manage common connection forms and functionalities for Azure Synapse Hooks. It also adds managed identity support for DefaultAzureCredential and deprecates the old default connection name azure_synapse_connection.

### Use case/motivation

The refactoring improves code maintainability by centralizing common logic in a base class, reducing duplication, and making it easier to manage changes. The addition of managed identity support enhances security by allowing Azure Synapse connections to use managed identities instead of client secrets.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jjavieralonso,2025-01-22 03:28:14+00:00,[],2025-01-22 08:36:13+00:00,2025-01-22 07:47:25+00:00,https://github.com/apache/airflow/issues/45894,"[('provider:microsoft-azure', 'Azure-related issues'), ('area:providers', ''), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2606512432, 'issue_id': 2803277281, 'author': 'potiuk', 'body': 'This looks like AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:\n\na) you explain how you generated reports\nb) prove you are human\nc) explain why you created the issue', 'created_at': datetime.datetime(2025, 1, 22, 7, 47, 25, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-22 07:47:25 UTC): This looks like AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:

a) you explain how you generated reports
b) prove you are human
c) explain why you created the issue

"
2803260939,issue,closed,completed,The Helm tests failed because of namespace conflicts in parallel test run,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.6.3

### What happened?

While executing Helm tests simultaneously or within a common Kubernetes cluster, tests encounter failures because of namespace clashes. For instance, several test executions try to generate resources within the same namespace, resulting in errors such as:

Error: cannot re-use a name that is still in use

### What you think should happen instead?

Each Helm test run should use a unique namespace to avoid conflicts. This would allow parallel test execution and improve reliability.

### How to reproduce

Run two instances of the Helm tests simultaneously in the same Kubernetes cluster.

Observe namespace conflicts and test failures.

### Operating System

Airflow version: 2.6.3  Kubernetes version: v1.21  Helm version: v3.7.0

### Versions of Apache Airflow Providers

Airflow version: 2.6.3

Kubernetes version: v1.21

Helm version: v3.7.0



### Deployment

Official Apache Airflow Helm Chart

### Deployment details

The issue occurs in a Kubernetes cluster deployed using Helm v3.7.0. The cluster is running on GKE (Google Kubernetes Engine) with the following configuration:

Cluster version: v1.21.12-gke.2200

Node pool: 3 nodes (n1-standard-4)

Helm chart: Apache Airflow 2.6.3

### Anything else?

This issue is particularly problematic in CI/CD environments where multiple test runs may execute concurrently. The current setup does not account for parallel execution, leading to namespace conflicts and test failures.

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",deveshkul,2025-01-22 03:10:20+00:00,[],2025-01-22 08:13:17+00:00,2025-01-22 08:13:17+00:00,https://github.com/apache/airflow/issues/45892,"[('kind:bug', 'This is a clearly a bug'), ('area:helm-chart', 'Airflow Helm Chart'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2606192974, 'issue_id': 2803260939, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 22, 3, 10, 22, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-22 03:10:22 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2803254101,issue,closed,completed,Enhance TriggerDagRunOperator to Avoid Failure When DAG Run Already Exists,"### Description

When multiple TriggerDagRunOperator tasks are in a failed state due to various reasons (e.g., unclean worker stops, zombie tasks), clearing them results in inconsistent behavior:

Some tasks succeed as expected.
Others fail immediately because the dag_run already exists and was previously triggered.
This makes managing TriggerDagRunOperator at scale (e.g., thousands of runs) challenging.

### Use case/motivation

This feature is critical for users running large-scale Airflow deployments where thousands of TriggerDagRunOperator tasks need to be cleared efficiently. The enhancement will reduce manual overhead and improve task reliability.

### Related issues

_No response_

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",sivatenneti,2025-01-22 03:03:16+00:00,[],2025-01-22 08:36:13+00:00,2025-01-22 08:10:05+00:00,https://github.com/apache/airflow/issues/45891,"[('kind:feature', 'Feature Requests'), ('area:core-operators', 'Operators, Sensors and hooks within Core Airflow'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2606186250, 'issue_id': 2803254101, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 22, 3, 3, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2606549557, 'issue_id': 2803254101, 'author': 'potiuk', 'body': 'This looks totally AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:\n\na) you explain how you generated reports\nb) prove you are human\nc) explain why you created the issue', 'created_at': datetime.datetime(2025, 1, 22, 8, 10, 5, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-22 03:03:18 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2025-01-22 08:10:05 UTC): This looks totally AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:

a) you explain how you generated reports
b) prove you are human
c) explain why you created the issue

"
2803253607,issue,closed,completed,Fix the logging of the dag-processor,"### Body

- We need to take care of a few logging-related duties.

1. Obtain processor logs for each file writing again; at the moment, they don't land anywhere on the disk.
2. We no longer use the separate log file, thus move the dag processor manager logs to stdout.


### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",Hossam-Samin,2025-01-22 03:02:46+00:00,[],2025-01-22 08:36:12+00:00,2025-01-22 03:03:02+00:00,https://github.com/apache/airflow/issues/45890,"[('kind:meta', 'High-level information important to the community'), ('AI Spam', '')]","[{'comment_id': 2606547728, 'issue_id': 2803253607, 'author': 'potiuk', 'body': 'This looks totally AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:\n\na) you explain how you generated reports\nb) prove you are human\nc) explain why you created the issue', 'created_at': datetime.datetime(2025, 1, 22, 8, 9, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-22 08:09:00 UTC): This looks totally AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:

a) you explain how you generated reports
b) prove you are human
c) explain why you created the issue

"
2803251201,issue,closed,completed,[OpenLineage] fix: disabled_for_operators now stops whole event emission #38033,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.8.0

### What happened?

The current implementation of OpenLineage in Airflow does not fully respect the disabled_for_operators configuration. While this setting excludes metadata for specified operators, it does not prevent event emission entirely. 

### What you think should happen instead?

No events should be emitted for operators listed in the disabled_for_operators configuration.

### How to reproduce

Set up a working Airflow instance with the latest version. Create a basic workflow (DAG) using the feature or operator in question.
Trigger the workflow manually or on a schedule.


### Operating System

Windows 10

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon 3.4.0

### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### Anything else?

This issue may affect workflows requiring strict control over event emission, especially for operators that should not trigger lineage events. 

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Sabbir02,2025-01-22 03:01:04+00:00,[],2025-01-22 08:36:12+00:00,2025-01-22 03:01:09+00:00,https://github.com/apache/airflow/issues/45889,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2606547046, 'issue_id': 2803251201, 'author': 'potiuk', 'body': 'This looks totally AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:\n\na) you explain how you generated reports\nb) prove you are human\nc) explain why you created the issue', 'created_at': datetime.datetime(2025, 1, 22, 8, 8, 40, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-22 08:08:40 UTC): This looks totally AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:

a) you explain how you generated reports
b) prove you are human
c) explain why you created the issue

"
2803229852,issue,closed,completed,Enhancement Request: Improve Documentation for the pyspark Decorator,"### What do you see as an issue?

The current documentation for the pyspark decorator is incomplete and lacks sufficient examples to demonstrate its full potential. Specifically:

It does not clearly explain how to handle errors when setting up the Spark context or connection (conn_id).
There is limited guidance on how to use the config_kwargs parameter to customize Spark configurations.
Advanced use cases, such as managing large datasets or integrating with existing Spark workflows, are missing.

### Solving the problem

The problem can be solved by:

Adding Detailed Examples:

Provide examples that show how to handle errors during Spark session setup or misconfigured connection IDs.
Include use cases for leveraging config_kwargs to set specific Spark configurations like executor memory or core allocation.
Expanding Advanced Use Cases:

Document how the decorator can handle large datasets efficiently.
Include examples of integrating pyspark tasks with existing Spark jobs.
Highlighting Edge Cases:

Add scenarios where the Spark context might fail to initialize and how to debug these issues.
Improving Parameter Documentation:

Clearly describe each parameter (conn_id, config_kwargs, multiple_outputs) and their potential values with examples.


### Anything else

This problem occurs every time a user refers to the documentation but cannot find sufficient information to address specific questions or scenarios. Enhancing the documentation will benefit all users and encourage wider adoption of the pyspark decorator.

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",svrajesh21,2025-01-22 02:42:28+00:00,[],2025-01-22 08:36:12+00:00,2025-01-22 07:49:11+00:00,https://github.com/apache/airflow/issues/45887,"[('kind:bug', 'This is a clearly a bug'), ('kind:documentation', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2606515093, 'issue_id': 2803229852, 'author': 'potiuk', 'body': 'This looks like AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:\n\na) you explain how you generated reports\nb) prove you are human\nc) explain why you created the issue', 'created_at': datetime.datetime(2025, 1, 22, 7, 49, 11, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-22 07:49:11 UTC): This looks like AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:

a) you explain how you generated reports
b) prove you are human
c) explain why you created the issue

"
2803225135,issue,closed,completed,Add skip_on_exit_code to SSHOperator,"### Description

This feature adds a new parameter skip_on_exit_code to the SSHOperator class. This parameter allows the task to be skipped if the SSH command exits with a specified exit code.

### Use case/motivation

The addition of the skip_on_exit_code parameter makes SSHOperator more flexible by allowing specific exit codes to signify a skipped state, similar to other operators like BashOperator and PythonVirtualenvOperator. This is useful for workflows where certain exit codes should not be treated as failures but as indicators to skip the task.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jjavieralonso,2025-01-22 02:37:42+00:00,[],2025-01-22 08:36:11+00:00,2025-01-22 07:47:35+00:00,https://github.com/apache/airflow/issues/45886,"[('kind:feature', 'Feature Requests'), ('area:core-operators', 'Operators, Sensors and hooks within Core Airflow'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2606512720, 'issue_id': 2803225135, 'author': 'potiuk', 'body': 'This looks like AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:\n\na) you explain how you generated reports\nb) prove you are human\nc) explain why you created the issue', 'created_at': datetime.datetime(2025, 1, 22, 7, 47, 36, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-22 07:47:36 UTC): This looks like AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:

a) you explain how you generated reports
b) prove you are human
c) explain why you created the issue

"
2803222311,issue,closed,not_planned,Native CLI Support for Managing AWS Identity Center Resources,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Apache Airflow does not provide native CLI commands for managing AWS Identity Center resources, such as users, groups, and permissions. Users currently need to rely on external scripts or manual operations, leading to inefficiencies and increased error rates in workflows requiring robust identity management.

### What you think should happen instead?

Airflow should provide CLI commands for:

Creating, listing, modifying, and deleting AWS Identity Center resources (users, groups, permissions).
Improved error handling and validation for AWS Identity Center operations.

### How to reproduce

Try creating, listing, modifying, or deleting AWS Identity Center resources using Airflow CLI.
Notice that no commands are available for AWS Identity Center management.

### Operating System

Windows

### Versions of Apache Airflow Providers

pip freeze

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",atharv9017,2025-01-22 02:34:52+00:00,[],2025-01-22 08:36:11+00:00,2025-01-22 06:34:08+00:00,https://github.com/apache/airflow/issues/45885,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:CLI', ''), ('kind:feature', 'Feature Requests'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2606408727, 'issue_id': 2803222311, 'author': 'eladkal', 'body': 'AI generated issue. Closing.', 'created_at': datetime.datetime(2025, 1, 22, 6, 34, 8, tzinfo=datetime.timezone.utc)}]","eladkal on (2025-01-22 06:34:08 UTC): AI generated issue. Closing.

"
2803192030,issue,closed,not_planned,clear and concise title of the bug,"### Apache Airflow Provider(s)

apache-hive

### Versions of Apache Airflow Providers

google-cloud-providers:9.0

### Apache Airflow version

apache airflow version:2.4

### Operating System

Hp,windows 10

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

-update provider package
identify the fixed version check the release notesor the bugreport for the specific version of the provider package that includes the fix.
-update requirement
modify your requirements .txt to use the updated version of the provider package.

### What happened

a bug was discovered and subsequently fixed in one of the provider packages used by your apache airflow.
the bug caused issues with workflow such as tasks within your dags might have been faiing due to the bug in the provider.
the bug could have led to incorrect data being processed or transformed impacting the accuracy.

### What you think should happen instead

pririotize minimizing downtime
enhance monitoring and logging 
improve rollback procedures
through testing and validatin

### How to reproduce

identify the exact airflow provideer package expereincing the bug.
review the bug report throughly.
if possible create a minimal example that demonstrates the bug.this makes it easier to debug and isolate the root cause.
ensure your local environment matches the environment described in the bug report as closely as possible.

### Anything else

N/A

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [ ] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vamshi583,2025-01-22 02:06:56+00:00,[],2025-01-22 15:11:59+00:00,2025-01-22 15:11:59+00:00,https://github.com/apache/airflow/issues/45883,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:apache-hive', ''), ('AI Spam', '')]","[{'comment_id': 2606125320, 'issue_id': 2803192030, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 22, 2, 6, 58, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-22 02:06:58 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2803170481,issue,closed,not_planned,Improper Type Validation in transfer_config_args of S3Hook,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When using the S3Hook in a DAG with a transfer operation, providing incorrect types for transfer_config_args does not trigger the appropriate error. Instead, it causes improper handling of transfer configurations and can result in unexpected errors during the S3 file transfer.

### What you think should happen instead?

transfer_config_args should be properly validated by checking the types of the provided arguments.
A TypeError should be raised when invalid types are provided, instead of the previously raised ValueError.

### How to reproduce

Use the S3Hook in a DAG with a transfer operation.
Provide incorrect types for transfer_config_args.
Observe that the error raised does not properly reflect the type mismatch and leads to improper behavior

### Operating System

Windows

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

Installed via pip or using a virtual environment.
Custom configuration may include setting up the executor (e.g., LocalExecutor or CeleryExecutor) and modifying Airflow's configuration file (airflow.cfg) to suit local system requirements.
For file transfer operations (like S3 integration), using AWS CLI and configuring S3Hook as per Airflow documentation.

### Anything else?

This issue has been consistently observed when invalid types are passed as transfer_config_args in the S3Hook, which causes unexpected behavior during file transfers.

A possible enhancement would be improving the error handling to raise a TypeError instead of the current ValueError, ensuring better error clarity and handling.

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",atharv9017,2025-01-22 01:44:30+00:00,[],2025-01-22 08:36:09+00:00,2025-01-22 06:33:48+00:00,https://github.com/apache/airflow/issues/45881,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2606408369, 'issue_id': 2803170481, 'author': 'eladkal', 'body': 'AI generated issue. Closing.', 'created_at': datetime.datetime(2025, 1, 22, 6, 33, 48, tzinfo=datetime.timezone.utc)}]","eladkal on (2025-01-22 06:33:48 UTC): AI generated issue. Closing.

"
2803159212,issue,closed,completed,Incorrect validation for transfer_config_args in S3Hook,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.2.3

### What happened?

The validation of transfer_config_args in te S3Hook was not correctly checking the types of the provided arguments. This led to improper handling of transfer configurations, potentially causing unexpected errors during file transfer to/from S3.

### What you think should happen instead?

transfer_config_args should be validated properly by checking the types of the provided arguments. Specifiically, a TypeError should be raised when invalid types are provided instead of the previously raised ValueError.

### How to reproduce

1. Use the S3Hook in a DAG with transfer operation, providing incorrect types of transfer_config_args.
2. Observe the incorrect validation behavior and the error raised.

### Operating System

Linux Ubuntu 20.04

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jjavieralonso,2025-01-22 01:33:35+00:00,[],2025-01-22 13:45:12+00:00,2025-01-22 07:37:54+00:00,https://github.com/apache/airflow/issues/45880,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2606497282, 'issue_id': 2803159212, 'author': 'potiuk', 'body': 'This looks like AI-generated. useless issue report that brings no value and makes no sense.  We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:\n\na) you explain how you generated reports\nb) prove you are human\nc) explain why you created the issue', 'created_at': datetime.datetime(2025, 1, 22, 7, 37, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2607261298, 'issue_id': 2803159212, 'author': 'jjavieralonso', 'body': 'Hi Jarek, I was assigned some issues that were originally created by other contributors who didnt know how to properly create them in GitHub and responded through our platform instead. So, the titles and descriptions weren’t written by me, I just presented the issues.', 'created_at': datetime.datetime(2025, 1, 22, 13, 32, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2607290851, 'issue_id': 2803159212, 'author': 'eladkal', 'body': ""We do not accept reports that are not coming directly from the user who experience the issue. issues are not submit and forget. It's a discussion with the reporter and always requires testing from the reporter when the issue is fixed.\nIf you are owner of a platform that hides the real user and do so automation to report issues to Airflow - all traffic from it will be closed.\n\n> I was assigned some issues that were originally created by other contributors who didnt know how to properly create them in GitHub\n\nClicking New issue button is something that they can't do?"", 'created_at': datetime.datetime(2025, 1, 22, 13, 45, 10, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-22 07:37:54 UTC): This looks like AI-generated. useless issue report that brings no value and makes no sense.  We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:

a) you explain how you generated reports
b) prove you are human
c) explain why you created the issue

jjavieralonso (Issue Creator) on (2025-01-22 13:32:29 UTC): Hi Jarek, I was assigned some issues that were originally created by other contributors who didnt know how to properly create them in GitHub and responded through our platform instead. So, the titles and descriptions weren’t written by me, I just presented the issues.

eladkal on (2025-01-22 13:45:10 UTC): We do not accept reports that are not coming directly from the user who experience the issue. issues are not submit and forget. It's a discussion with the reporter and always requires testing from the reporter when the issue is fixed.
If you are owner of a platform that hides the real user and do so automation to report issues to Airflow - all traffic from it will be closed.


Clicking New issue button is something that they can't do?

"
2803152427,issue,closed,completed,<>,"### Body

_No response_

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",3bdofathe,2025-01-22 01:26:54+00:00,[],2025-01-22 06:48:24+00:00,2025-01-22 06:47:47+00:00,https://github.com/apache/airflow/issues/45879,"[('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2606084005, 'issue_id': 2803152427, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 22, 1, 26, 56, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-22 01:26:56 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2803122964,issue,closed,completed,Optimize deferrable execution for AzureDataFactoryRunPipelineOperator,"### Description

Currently, the AzureDataFactoryRunPipelineOperator defers tasks even if they have already been completed, leading to unnecessary deferring and potential inefficiencies. This can result in increased execution times and resource usage. The proposed optimization involves checking if a submitted job has already been completed before deferring it, thereby skipping deferral for already finished tasks.


### Use case/motivation

This change will improve the efficiency and performance of the AzureDataFactoryRunPipelineOperator by reducing unnecessary deferring of tasks. It will help optimize resource usage and execution times, especially in environments with a high volume of tasks.


### Related issues

N/A


### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Mohammed-Karim226,2025-01-22 00:58:34+00:00,[],2025-01-28 17:22:14+00:00,2025-01-28 17:22:14+00:00,https://github.com/apache/airflow/issues/45878,"[('provider:microsoft-azure', 'Azure-related issues'), ('area:providers', ''), ('kind:feature', 'Feature Requests'), ('good first issue', '')]","[{'comment_id': 2606053568, 'issue_id': 2803122964, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 22, 0, 58, 37, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-22 00:58:37 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2802946653,issue,closed,completed,Add pagination to HttpOperator and make it more modular #34669,"### Description

In the event that I must know the page count beforehand. Two inefficient calls to the API are required for this, and neither one guarantees that every page will be viewed on the second call. HttpOperator by enabling it to handle custom pagination logic, providing users with more flexibility in managing API pagination. The goal is to replace the deprecated SimpleHttpOperator, offering a more efficient and adaptable way to process paginated API data, which improves how workflows interact with APIs within Airflow. 

### Use case/motivation

For APIs where the pagination logic is not fixed (e.g., using a cursor or other complex structures), this update allows users to implement their custom logic for paging through results. Support for dynamic pagination in APIs is critical for many use cases. By allowing users to pass their own pagination functions, the operator can handle different API structures and pagination methods without modifications.

### Related issues

Included in Airflow version 2.8.0
Reference PR: [#34606]

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Sabbir02,2025-01-21 22:28:27+00:00,[],2025-01-22 08:36:08+00:00,2025-01-21 22:56:04+00:00,https://github.com/apache/airflow/issues/45872,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2605907867, 'issue_id': 2802946653, 'author': 'potiuk', 'body': 'Another AI generated issue.', 'created_at': datetime.datetime(2025, 1, 21, 22, 56, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2605968913, 'issue_id': 2802946653, 'author': 'Sabbir02', 'body': ""Sorry for the inconvenience. I was trying to get familiar with the GitHub\r\nPR , issue template etc. I didn't know it will create a real time issue, i\r\nwas just trying random things. Thanks for notifying me.\r\n\r\nOn Wed, 22 Jan 2025, 4:56 am Jarek Potiuk, ***@***.***> wrote:\r\n\r\n> Another AI generated issue.\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/apache/airflow/issues/45872#issuecomment-2605907867>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/ANADYRQAIWG3JDVZBRKVYC32L3GBVAVCNFSM6AAAAABVTR4C2GVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDMMBVHEYDOOBWG4>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>"", 'created_at': datetime.datetime(2025, 1, 21, 23, 45, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2606472867, 'issue_id': 2802946653, 'author': 'potiuk', 'body': 'Please absolutely stop doing it. Due to high amount of AI generated spam like yours we block all accounts that created them. Blocking your account from interacting with our repo. If you want to get it restored, do some legitimated issues / PRs and let me know, then we can unblock you.', 'created_at': datetime.datetime(2025, 1, 22, 7, 21, 41, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-21 22:56:04 UTC): Another AI generated issue.

Sabbir02 (Issue Creator) on (2025-01-21 23:45:14 UTC): Sorry for the inconvenience. I was trying to get familiar with the GitHub
PR , issue template etc. I didn't know it will create a real time issue, i
was just trying random things. Thanks for notifying me.

On Wed, 22 Jan 2025, 4:56 am Jarek Potiuk, ***@***.***> wrote:

potiuk on (2025-01-22 07:21:41 UTC): Please absolutely stop doing it. Due to high amount of AI generated spam like yours we block all accounts that created them. Blocking your account from interacting with our repo. If you want to get it restored, do some legitimated issues / PRs and let me know, then we can unblock you.

"
2802857600,issue,closed,completed,Fix and deprecate source object's in  GCS,"### Description

In the past, Airflow's GCS integration included the use of wildcards in source object paths and included the delimiter parameter for object filtering. Nevertheless, GCS did not natively enable these features, which frequently led to misunderstandings or abuse. To address this, the following adjustments have been proposed:

- Delimiter parameter deprecation: Because of its non-standard implementation and lack of native GCS support, this parameter is being phased out.
- Match_glob Parameter Introduction: A new match_glob parameter will enable native GCS filtering based on pattern matching that is more accurate.



### Use case/motivation

Making ensuring Airflow's GCS integration is in line with GCS's inherent capabilities is the main driving force for this modification, which will enhance user experience and dependability. In GCS, the match_glob option will offer a simple, supported, and intuitive method of filtering items, lowering the possibility of mistakes and misunderstandings.


### Related issues

In order to enhance Airflow's GCS integration, this PR expands upon Issue #29115. For improved, native pattern-based filtering, it eliminates the perplexing delimiter parameter and substitutes it with a new match_glob option. These modifications simplify and improve the use of GCS integration.


### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Hossam-Samin,2025-01-21 21:33:55+00:00,[],2025-01-22 08:36:08+00:00,2025-01-21 22:45:48+00:00,https://github.com/apache/airflow/issues/45867,"[('provider:google', 'Google (including GCP) related issues'), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2605894100, 'issue_id': 2802857600, 'author': 'potiuk', 'body': 'Another AI generated issue', 'created_at': datetime.datetime(2025, 1, 21, 22, 45, 48, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-21 22:45:48 UTC): Another AI generated issue

"
2802837868,issue,closed,completed,Extend papermill operator to support remote kernels #34840,"### Description

The current implementation of the Papermill operator lacks the capability to connect with externally managed kernels, limiting its use to local kernel execution. This enhancement enables the operator to support running notebooks with externally managed kernels like Spark or Scala, improving flexibility and integration with Kubernetes for a smoother experience in Airflow.

### Use case/motivation


This update allows Apache Airflow's Papermill operator to run Jupyter notebooks with various kernels, such as Spark or Scala, in addition to Python. This flexibility is crucial for teams working with diverse data processing tasks and multi-language environments, enabling smoother workflows and better integration between different systems. It enhances Airflow's versatility and ensures that users can execute complex tasks without being limited to just Python notebooks.

### Related issues

The implementation has been verified as functional (see issue #35845).
Included in Airflow version 2.8.0 

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Sabbir02,2025-01-21 21:22:11+00:00,[],2025-01-22 08:36:07+00:00,2025-01-21 22:37:59+00:00,https://github.com/apache/airflow/issues/45866,"[('area:providers', ''), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:papermill', ''), ('AI Spam', '')]","[{'comment_id': 2605882975, 'issue_id': 2802837868, 'author': 'potiuk', 'body': 'Another AI generated issue.', 'created_at': datetime.datetime(2025, 1, 21, 22, 37, 59, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-21 22:37:59 UTC): Another AI generated issue.

"
2802647032,issue,closed,completed,"Deprecate delimiter param and source object's wildcards in GCS, introduce match_glob param. #31261","### Description

Airflow's GCS integration previously included the delimiter parameter for filtering objects and allowed the use of wildcards in source object paths. However, these features were not natively supported by GCS and often caused confusion or misuse. To address this, the following changes have been proposed:
1. Deprecation of delimiter parameter: This parameter is being deprecated due to its non-standard implementation and lack of native GCS support.
2. Introduction of match_glob Parameter: A new match_glob parameter will allow for more accurate, native GCS filtering based on pattern matching.

### Use case/motivation

The primary motivation for this change is to ensure Airflow's GCS integration aligns with GCS's native capabilities, improving reliability and user experience. The match_glob parameter will provide a straightforward, intuitive, and supported way to filter objects in GCS, reducing the potential for errors and confusion.

### Related issues

This PR builds on [Issue #29115](https://github.com/apache/airflow/issues/29115) to improve Airflow's GCS integration. It removes the confusing delimiter parameter and replaces it with a new match_glob option for better, native pattern-based filtering. These changes make GCS integration simpler and more user-friendly.

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Sabbir02,2025-01-21 19:31:42+00:00,[],2025-01-22 08:36:07+00:00,2025-01-21 22:35:05+00:00,https://github.com/apache/airflow/issues/45861,"[('provider:google', 'Google (including GCP) related issues'), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2605878861, 'issue_id': 2802647032, 'author': 'potiuk', 'body': ""This looks like AI generated issue. Please prove it's not"", 'created_at': datetime.datetime(2025, 1, 21, 22, 35, 5, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-21 22:35:05 UTC): This looks like AI generated issue. Please prove it's not

"
2802553854,issue,closed,completed,Add support for dynamic task grouping based on execution date,"### Description


## Repository Version
Apache Airflow version 2.10.4
## Search terms / keywords
dynamic task grouping, execution date, task dependencies, workflow optimization
## Description
Currently, Airflow does not support dynamic task grouping based on execution dates. Users are unable to programmatically group tasks according to the specific execution date of a DAG run. This enhancement would enable workflows to have better task dependency management and reduce the complexity of DAG definitions.
## What happened (current behavior)
In the current Airflow version, users have to manually define task dependencies or use complex logic to group tasks dynamically. This results in more rigid and harder-to-manage DAGs, especially in cases where workflows need to adapt to different execution dates.
## What do you think should happen instead (expected behavior)
We propose adding the ability to dynamically group tasks based on the execution date. This would allow users to create more flexible and adaptable DAGs by grouping tasks automatically according to the specific date of execution, reducing boilerplate code and making the workflow easier to maintain.




### Use case/motivation

## Use case and motivating examples (for feature requests)
This feature would benefit users working with large, time-sensitive workflows where tasks need to be grouped based on the specific run date. For example:
- Financial reports that must be generated for specific dates.
- ETL processes that need different data sources based on the execution date.

### Related issues

n/a

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",farhan-labib09,2025-01-21 18:38:30+00:00,[],2025-01-21 22:33:51+00:00,2025-01-21 22:33:51+00:00,https://github.com/apache/airflow/issues/45858,"[('kind:feature', 'Feature Requests'), ('area:dynamic-task-mapping', 'AIP-42'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2605477217, 'issue_id': 2802553854, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 21, 18, 38, 33, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-21 18:38:33 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2802515743,issue,closed,completed,Issue with Deferrable Sensor Documentation in PubSub Integration,"### What do you see as an issue?

The documentation for the new deferrable sensor in docs/apache-airflow-providers-google/operators/cloud/pubsub.rst does not provide sufficient context on the usage and benefits of the deferrable mode. It also lacks detailed examples and explanations on how to configure and use the deferrable sensor effectively.

### Solving the problem

_To address this issue, the documentation should be updated to include:_

**1**. A detailed description of the deferrable mode and its advantages.
**2.** Comprehensive examples demonstrating the configuration and usage of the deferrable sensor in various scenarios.
**3.** Clear guidelines on when to use the deferrable mode and potential pitfalls to avoid.


### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jjavieralonso,2025-01-21 18:16:32+00:00,[],2025-01-22 08:36:17+00:00,2025-01-22 07:47:55+00:00,https://github.com/apache/airflow/issues/45857,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('kind:documentation', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2606513231, 'issue_id': 2802515743, 'author': 'potiuk', 'body': 'This looks like AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:\n\na) you explain how you generated reports\nb) prove you are human\nc) explain why you created the issue', 'created_at': datetime.datetime(2025, 1, 22, 7, 47, 55, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-22 07:47:55 UTC): This looks like AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:

a) you explain how you generated reports
b) prove you are human
c) explain why you created the issue

"
2802488776,issue,closed,not_planned,"openlineage, postgres: add OpenLineage support for Postgres #31617","### Description

This issue adds OpenLineage support for the PostgresOperator to enable metadata tracking for PostgreSQL operations. The feature aims to enhance data pipeline observability by integrating OpenLineage capabilities with Apache Airflow's PostgreSQL provider.

Current Behavior
PostgresOperator currently lacks built-in OpenLineage support for tracking metadata.
Users cannot define both the database and schema for PostgreSQL connections directly, limiting flexibility.
Expected Behavior
Add support for OpenLineage in the PostgresOperator to enable metadata tracking for queries and tasks.
Allow users to specify both the database and schema explicitly when using PostgreSQL connections in Airflow.

### Use case/motivation

This feature improves Apache Airflow by adding OpenLineage support to the PostgresOperator, making it easier to track and understand how data flows through your workflows. With this integration, users can capture detailed metadata about SQL queries and their inputs and outputs, enhancing visibility and compliance in data pipelines.

It also simplifies connection configurations by letting users explicitly set the database and schema in PostgreSQL connections. This is especially useful in complex setups with multiple databases or schemas.

By adopting OpenLineage, Airflow aligns with modern data engineering standards, helping teams monitor, debug, and scale their workflows with confidence. This feature makes managing and maintaining data pipelines more transparent and user-friendly.

### Related issues

This feature builds on PR #31398, which lays the groundwork for integrating OpenLineage with Apache Airflow. To ensure everything works smoothly, thorough testing is needed to verify that lineage tracking and database/schema configurations are correctly implemented.



### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Sabbir02,2025-01-21 18:01:39+00:00,[],2025-01-22 08:36:06+00:00,2025-01-21 22:22:22+00:00,https://github.com/apache/airflow/issues/45856,"[('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:openlineage', 'AIP-53'), ('provider:postgres', ''), ('AI Spam', '')]","[{'comment_id': 2605860288, 'issue_id': 2802488776, 'author': 'potiuk', 'body': 'This also looks like ai generated issue. Please respond  if closing it is premature. I see no value in creating such issues', 'created_at': datetime.datetime(2025, 1, 21, 22, 22, 13, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-21 22:22:13 UTC): This also looks like ai generated issue. Please respond  if closing it is premature. I see no value in creating such issues

"
2802417384,issue,closed,completed,Permission Errors When Specifying a Destination Folder in Google Drive Integration (PR #31885),"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

2.10.1

### What happened?

When configuring an Airflow task to upload a file to Google Drive with a destination folder specified, the task intermittently fails with a permission error. This issue occurs even when the service account used has appropriate access rights to the target folder.

The error was not observed before the introduction of the destination folder feature in [PR #31885](https://github.com/apache/airflow/pull/31885).

### What you think should happen instead?

The task should upload the file to the specified destination folder without encountering permission errors, provided the service account has the necessary access rights.

### How to reproduce

Configure an Airflow task using GoogleDriveToGCSOperator or similar.
Specify a valid destination folder ID for the task.
Ensure the service account used has editor permissions for the destination folder.
Execute the task.
Expected Outcome:
The file should upload successfully to the specified folder.

Actual Outcome:
The task fails with a permission error.



### Operating System

Ubuntu 22.04.1 LTS

### Versions of Apache Airflow Providers

apache-airflow-providers-google==9.1.0


### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Helm chart version: 1.7.0
Kubernetes version: v1.26.2
Custom configurations:
Service account with editor permissions for Google Drive.
DAGs mounted via NFS.

### Anything else?

Adding detailed logs captured from the Airflow task and Google Drive API might help pinpoint the issue. The error appears to be intermittent, so further investigation into the conditions leading to the failure is necessary.



### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",svrajesh21,2025-01-21 17:23:31+00:00,[],2025-01-28 16:25:03+00:00,2025-01-28 16:25:03+00:00,https://github.com/apache/airflow/issues/45853,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('pending-response', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2605846509, 'issue_id': 2802417384, 'author': 'potiuk', 'body': 'There is lack of details - logs etc. - and then maybe @stiak might help', 'created_at': datetime.datetime(2025, 1, 21, 22, 13, 5, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-21 22:13:05 UTC): There is lack of details - logs etc. - and then maybe @stiak might help

"
2802393763,issue,closed,completed,Enhance DataFusion Integration with Asynchronous and Deferrable Support,"### Description

This feature enhances the DataFusion integration in Apache Airflow by adding asynchronous and deferrable support to DataFusion operators and hooks. Additionally, it introduces new DataFusion links and improves error handling.

### Use case/motivation

The addition of asynchronous and deferrable support allows users to run DataFusion pipelines more efficiently by leveraging non-blocking I/O operations. This is particularly useful for long-running pipelines, reducing the need for active waiting and improving resource utilization. The new DataFusion links provide easier navigation to specific DataFusion resources within the Google Cloud Console.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jjavieralonso,2025-01-21 17:12:04+00:00,[],2025-01-22 08:36:05+00:00,2025-01-22 07:48:41+00:00,https://github.com/apache/airflow/issues/45850,"[('provider:google', 'Google (including GCP) related issues'), ('kind:feature', 'Feature Requests'), ('area:async-operators', 'AIP-40: Deferrable (""Async"") Operators'), ('AI Spam', '')]","[{'comment_id': 2606514366, 'issue_id': 2802393763, 'author': 'potiuk', 'body': 'This looks like AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:\n\na) you explain how you generated reports\nb) prove you are human\nc) explain why you created the issue', 'created_at': datetime.datetime(2025, 1, 22, 7, 48, 42, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-22 07:48:42 UTC): This looks like AI-generated. useless issue report that brings no value and makes no sense. We are generally blocking users that sends a lot of spam AI reports generated by bots.. as of yesterday so we will report your account and block it unless:

a) you explain how you generated reports
b) prove you are human
c) explain why you created the issue

"
2802386569,issue,closed,completed,.,.,shreyashdwivedi36,2025-01-21 17:08:19+00:00,[],2025-01-29 03:34:55+00:00,2025-01-27 22:17:22+00:00,https://github.com/apache/airflow/issues/45849,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('pending-response', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2605832389, 'issue_id': 2802386569, 'author': 'potiuk', 'body': 'You report is confusing and ambiguous. You need to add the logs and dag example to make it more concrete and reproducible.', 'created_at': datetime.datetime(2025, 1, 21, 22, 3, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2617014080, 'issue_id': 2802386569, 'author': 'potiuk', 'body': 'Closing as AI spam,', 'created_at': datetime.datetime(2025, 1, 27, 22, 17, 22, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-21 22:03:50 UTC): You report is confusing and ambiguous. You need to add the logs and dag example to make it more concrete and reproducible.

potiuk on (2025-01-27 22:17:22 UTC): Closing as AI spam,

"
2802319302,issue,closed,completed,Issue with task retries not behaving as expected in certain conditions,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When executing DAGs that involve retries, the task retries do not behave as expected in certain edge cases. Specifically, the task does not retry when the retry condition should be met, or it retries beyond the expected number of retries.

### What you think should happen instead?

The task should retry exactly as per the defined retry parameters in the DAG definition, not exceeding the maximum retries, and retrying when the conditions trigger.

### How to reproduce

Define a task in a DAG with a retry policy (e.g., max retries and retry delay).
Induce a failure scenario where the task would fail and should retry.
Observe the task’s behavior in the UI or logs, particularly in cases where retries should trigger but do not.
Check the retry count to see if it exceeds the allowed maximum retries or doesn’t retry when it should.

### Operating System

Ubuntu 20.04 

### Versions of Apache Airflow Providers

apache-airflow-providers-google==6.0.0
apache-airflow-providers-s3==7.0.0

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Managed deployment on Kubernetes via Helm
Airflow version deployed via Helm Chart 2.0.0

### Anything else?

This issue occurs every time when the retry conditions are met, but retries either do not happen or exceed the maximum retry limit set in the DAG.
logs:[2025-01-20 12:34:56,789] {taskinstance.py:1200} INFO - Task failed. Retrying...
[2025-01-20 12:35:01,002] {taskinstance.py:1230} INFO - Task retry exceeded max retries.


### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",svrajesh21,2025-01-21 16:36:47+00:00,[],2025-01-28 16:25:11+00:00,2025-01-28 16:25:10+00:00,https://github.com/apache/airflow/issues/45846,"[('kind:bug', 'This is a clearly a bug'), ('pending-response', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2605221255, 'issue_id': 2802319302, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 21, 16, 36, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2605827759, 'issue_id': 2802319302, 'author': 'potiuk', 'body': 'No evidences show that things are wrong. I am confused by the description\n\nI think you should specify a detailed reproduction scenario and show what you expected vs. What you observed and show more than  ambiguous and unrelated logs', 'created_at': datetime.datetime(2025, 1, 21, 22, 0, 45, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-21 16:36:50 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2025-01-21 22:00:45 UTC): No evidences show that things are wrong. I am confused by the description

I think you should specify a detailed reproduction scenario and show what you expected vs. What you observed and show more than  ambiguous and unrelated logs

"
2802316681,issue,closed,completed,Memory leak when using AWS Glue Job Console in Airflow,"### Description

When running an AWS Glue job via Airflow, there appears to be a memory leak in the task rate monitoring component. After the job runs for a few hours, memory usage steadily increases, leading to crashes.


### Use case/motivation

Improved memory management would help ensure long-running jobs, such as large data processing tasks, can run smoothly without crashes.




### Related issues

No specific stack trace, just system logs indicating out-of-memory (OOM) errors.

Files and functions to be edited (if known):
Potentially related to the AWSGlueJobOperator or task rate monitoring functions in Airflow.


### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",hazemAmr0,2025-01-21 16:35:32+00:00,['hazemAmr0'],2025-01-28 16:49:56+00:00,2025-01-28 16:49:56+00:00,https://github.com/apache/airflow/issues/45845,"[('provider:amazon', 'AWS/Amazon - related issues'), ('area:performance', ''), ('kind:feature', 'Feature Requests'), ('good first issue', ''), ('AI Spam', '')]","[{'comment_id': 2605821452, 'issue_id': 2802316681, 'author': 'potiuk', 'body': 'Feel free to look at this', 'created_at': datetime.datetime(2025, 1, 21, 21, 56, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2611087084, 'issue_id': 2802316681, 'author': 'bugraoz93', 'body': '#45837 I closed this one just a moment ago. Could you please explain in more detail about the issue you are experiencing @hazemAmr0? How much heap you are using to run Airflow components? How many tasks you are running with AWSGlueJobOperator? Are you experiencing this problem in each task? How we can reproduce this locally? Is the memory leak on the AWS side or in the Apache Airflow component?', 'created_at': datetime.datetime(2025, 1, 23, 21, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2613919787, 'issue_id': 2802316681, 'author': 'potiuk', 'body': 'I think this is another case of AI generated spam. @hazemAmr0 -> if you are using some tools to generate such issues automatically, please stop.', 'created_at': datetime.datetime(2025, 1, 25, 10, 32, 49, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-21 21:56:41 UTC): Feel free to look at this

bugraoz93 on (2025-01-23 21:49:00 UTC): #45837 I closed this one just a moment ago. Could you please explain in more detail about the issue you are experiencing @hazemAmr0? How much heap you are using to run Airflow components? How many tasks you are running with AWSGlueJobOperator? Are you experiencing this problem in each task? How we can reproduce this locally? Is the memory leak on the AWS side or in the Apache Airflow component?

potiuk on (2025-01-25 10:32:49 UTC): I think this is another case of AI generated spam. @hazemAmr0 -> if you are using some tools to generate such issues automatically, please stop.

"
2802309736,issue,closed,completed,"Create Inventory of Triggers, Hooks, and Operators Using DB Directly","### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Several Triggers, Hooks, and Operators in the Providers directly interact with the database. This creates maintenance challenges and hinders modularity.

### What you think should happen instead?

An inventory should be created listing all such cases to enable updates aligned with AIP-72. This will help streamline the update process and improve the codebase's maintainability.

### How to reproduce

1.Review the Providers' codebase.
2. ID entify Triggers, Hooks, and Operators with direct database calls.
3.Compile these cases into an inventory.

### Operating System

Windows

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

The issue occurs consistently as the DB access pattern is embedded in the Providers. Logs and traces are not directly applicable to this task but may arise during AIP-72 updates.

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",atharv9017,2025-01-21 16:32:18+00:00,[],2025-01-21 19:55:26+00:00,2025-01-21 19:55:26+00:00,https://github.com/apache/airflow/issues/45844,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2605614641, 'issue_id': 2802309736, 'author': 'potiuk', 'body': 'this is a bot . closing', 'created_at': datetime.datetime(2025, 1, 21, 19, 55, 13, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-21 19:55:13 UTC): this is a bot . closing

"
2802302872,issue,closed,completed, .,.,shreyashdwivedi36,2025-01-21 16:29:28+00:00,[],2025-01-29 03:33:07+00:00,2025-01-27 22:17:41+00:00,https://github.com/apache/airflow/issues/45842,"[('kind:bug', 'This is a clearly a bug'), ('pending-response', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('area:db-migrations', 'PRs with DB migration'), ('AI Spam', '')]","[{'comment_id': 2605204028, 'issue_id': 2802302872, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 21, 16, 29, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2616178198, 'issue_id': 2802302872, 'author': 'RNHTTR', 'body': 'Can you please provide more detailed reproduction steps? `last_scheduler_run` was [removed several years ago](https://github.com/apache/airflow/pull/14581), in Airflow 2.0.2', 'created_at': datetime.datetime(2025, 1, 27, 16, 8, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2617014606, 'issue_id': 2802302872, 'author': 'potiuk', 'body': 'This is another AI spam generated.', 'created_at': datetime.datetime(2025, 1, 27, 22, 17, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2617019194, 'issue_id': 2802302872, 'author': 'potiuk', 'body': 'Reported you @shreyashdwivedi36 to GitHub', 'created_at': datetime.datetime(2025, 1, 27, 22, 20, 33, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-21 16:29:32 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

RNHTTR on (2025-01-27 16:08:51 UTC): Can you please provide more detailed reproduction steps? `last_scheduler_run` was [removed several years ago](https://github.com/apache/airflow/pull/14581), in Airflow 2.0.2

potiuk on (2025-01-27 22:17:41 UTC): This is another AI spam generated.

potiuk on (2025-01-27 22:20:33 UTC): Reported you @shreyashdwivedi36 to GitHub

"
2802284965,issue,open,,Add Support for client_tags in TrinoHook Connections,"### Description

This feature introduces the client_tags parameter to the TrinoHook connections in Apache Airflow. It allows users to specify a list of client tags when establishing a connection to Trino.

### Use case/motivation

The addition of the client_tags parameter enables users to tag their Trino clients with specific identifiers, which can be useful for tracking and managing queries, especially in multi-tenant environments or for monitoring purposes. This feature enhances the flexibility and usability of Trino connections within Apache Airflow.

### Related issues

Key Changes:
File: airflow/providers/trino/hooks/trino.py

Added client_tags parameter in the get_conn method.
Updated the connection initialization to include the client_tags parameter.
File: docs/apache-airflow-providers-trino/connections.rst

Updated documentation to include client_tags in the list of extra connection parameters.
File: tests/providers/trino/hooks/test_trino.py

Added test_get_conn_client_tags to verify the correct handling of client_tags.
Updated assert_connection_called_with to check for client_tags in connection parameters.

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jjavieralonso,2025-01-21 16:21:12+00:00,['wei-juncheng'],2025-01-25 12:00:41+00:00,,https://github.com/apache/airflow/issues/45841,"[('area:providers', ''), ('kind:feature', 'Feature Requests'), ('good first issue', ''), ('provider:trino', '')]","[{'comment_id': 2605184349, 'issue_id': 2802284965, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 21, 16, 21, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2608988318, 'issue_id': 2802284965, 'author': 'wei-juncheng', 'body': ""Hi @potiuk ,\ncould you please assign this issue to me? I am also using TrinoHook and interested in this issue. I'm currently investigating it."", 'created_at': datetime.datetime(2025, 1, 23, 6, 45, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2613942585, 'issue_id': 2802284965, 'author': 'potiuk', 'body': 'Sure', 'created_at': datetime.datetime(2025, 1, 25, 12, 0, 40, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-21 16:21:15 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

wei-juncheng (Assginee) on (2025-01-23 06:45:59 UTC): Hi @potiuk ,
could you please assign this issue to me? I am also using TrinoHook and interested in this issue. I'm currently investigating it.

potiuk on (2025-01-25 12:00:40 UTC): Sure

"
2802261351,issue,open,,"Create inventory of Trigger, Hook & Operator's using DB directly","A good amount of Trigger, Hooks & Operators in the Provider are calling DB directly. Create a list of inventory of those providers, so once AIP-72 is in a good shape, the community & provider's stakeholders can lead the effort to update those.",kaxil,2025-01-21 16:10:42+00:00,['potiuk'],2025-01-22 20:54:27+00:00,,https://github.com/apache/airflow/issues/45839,"[('area:providers', '')]","[{'comment_id': 2605600751, 'issue_id': 2802261351, 'author': 'potiuk', 'body': 'It should also be automated/verified with tests -> as described in Stage 3 of https://github.com/apache/airflow/issues/42632#issuecomment-2449671014', 'created_at': datetime.datetime(2025, 1, 21, 19, 47, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2608067322, 'issue_id': 2802261351, 'author': 'kaxil', 'body': 'I created a very basic list:\n\nThe following providers utilize\'s Airflow\'s session object:\n\n- Databricks\n- Edge\n- Elasticsearch\n- FAB\n- Google\n- Kubernetes\n- OpenLineage\n- OpenSearch\n- Standard\n\n[This GitHub search](https://github.com/search?q=repo%3Aapache%2Fairflow+%22from+airflow.utils.session%22+OR+%22from+airflow.settings+import+Session%22+path%3A%2F%5Eproviders%5C%2Fsrc%5C%2Fairflow%5C%2Fproviders%5C%2F%2F&type=code) should show the results too:\n\n\n-----\n\n**`Databricks`**\n- Uses it for the [Workflow plugin](https://github.com/apache/airflow/blob/6751c9635f86838086bbdfc52a17ad6818fbd03a/providers/src/airflow/providers/databricks/plugins/databricks_workflow.py)\n\n\n**`Edge`**\n- Expected for now since it is an executor.\n\n**`Elasticsearch`**\n- Uses it in the log handler. Needs to change\n\nhttps://github.com/apache/airflow/blob/6751c9635f86838086bbdfc52a17ad6818fbd03a/providers/src/airflow/providers/elasticsearch/log/es_task_handler.py#L225-L245\n\n**`FAB`**\n- For Auth, stores data in FAB related tables in Airflow DB\n\n**`Google`**\n\nUses it for getting current task instance in the **Triggerer** for BigQuery & Dataproc. Needs to change\n\nhttps://github.com/apache/airflow/blob/6751c9635f86838086bbdfc52a17ad6818fbd03a/providers/src/airflow/providers/google/cloud/triggers/dataproc.py#L246-L255\n\n**`K8s`**\n\nUses it for:\n- Rendering K8s Pod Spec YAML to show in Airflow UI for KPO & KE. ([Code](https://github.com/apache/airflow/blob/6751c9635f86838086bbdfc52a17ad6818fbd03a/providers/src/airflow/providers/cncf/kubernetes/template_rendering.py)\n- For KubernetesExecutor. Expected for now, needs to change via https://github.com/apache/airflow/issues/45427\n- For helper function to look [up the run_id from the TI table!](https://github.com/apache/airflow/blob/6751c9635f86838086bbdfc52a17ad6818fbd03a/providers/src/airflow/providers/cncf/kubernetes/kubernetes_helper_functions.py#L105C15-L105C52)\n\n**`OpenLineage`**\nUses it for checking if a TI is rescheduled or not but might be using it for more things. Needs to change!\n `is_ti_rescheduled_already` ([Code](https://github.com/apache/airflow/blob/6751c9635f86838086bbdfc52a17ad6818fbd03a/providers/src/airflow/providers/openlineage/utils/utils.py#L186-L205))\n\n**`OpenSearch`**\n- Same as ES and needs to change. ([Code](https://github.com/apache/airflow/blob/6751c9635f86838086bbdfc52a17ad6818fbd03a/providers/src/airflow/providers/opensearch/log/os_task_handler.py#L278))\n\n**`Standard`**\n- Lot of changes needed here.\n- ExternalTaskSensor and its Triggers ([code](https://github.com/apache/airflow/blob/6751c9635f86838086bbdfc52a17ad6818fbd03a/providers/src/airflow/providers/standard/sensors/external_task.py))\n- `TriggerDagRunOperator` and its ExtraOperatorLinks ([code](https://github.com/apache/airflow/blob/6751c9635f86838086bbdfc52a17ad6818fbd03a/providers/src/airflow/providers/standard/operators/trigger_dagrun.py#L45))\n- Bash Task decorator ([code](https://github.com/apache/airflow/blob/6751c9635f86838086bbdfc52a17ad6818fbd03a/providers/src/airflow/providers/standard/operators/bash.py#L201-L220))\n\n\n----\n\nThere might be other usage which I haven\'t dug into. I only checked for presence of the following in a file:\n\n- `from airflow.utils.session`\n- `from airflow.settings import Session`\n\nThere might be other cases where a Provider code might be importing things from `airflow.models` to import `DagRun` or `TaskInstance` and such directly, that will need to be found out and changed too.\n\n---\n\nApart from this task itself, we will need to separately change all the provider code to not import any code from `airflow` core and instead strictly depend on the Task SDK.\n\n\nA small script to get a similar inventory:\n\n```py\nimport os\nimport re\nimport json\n\n# Directories to scan\nDIRECTORIES = [""providers"", ""airflow/operators""]\n\n# Patterns to identify Airflow metadata DB access\nDB_PATTERNS = [\n    r""from airflow\\.utils\\.session"",\n    r""from airflow\\.settings import Session"",\n    # r""from airflow\\.models""\n]\n\n# Exclude tests directory\nEXCLUDED_DIRS = [""tests""]\n\nOUTPUT_FILE = ""metadata_db_call_inventory.json""\n\ndef matches_db_pattern(line):\n    """"""Check if a line matches any metadata DB access pattern.""""""\n    return any(re.search(pattern, line) for pattern in DB_PATTERNS)\n\ndef scan_file(filepath):\n    """"""Scan a single file for metadata DB access patterns.""""""\n    file_inventory = []\n    with open(filepath, ""r"", encoding=""utf-8"") as f:\n        try:\n            lines = f.readlines()\n        except Exception as e:\n            print(f""Error reading {filepath}: {e}"")\n            return []\n\n        for i, line in enumerate(lines, start=1):\n            if matches_db_pattern(line):\n                file_inventory.append({""line"": i, ""content"": line.strip()})\n    return file_inventory\n\ndef scan_directory(directory):\n    """"""Scan the given directory for metadata DB access patterns.""""""\n    inventory = {}\n    for root, dirs, files in os.walk(directory):\n        dirs[:] = [d for d in dirs if d not in EXCLUDED_DIRS]\n\n        for file in files:\n            if file.endswith("".py""):\n                filepath = os.path.join(root, file)\n                file_inventory = scan_file(filepath)\n                if file_inventory:\n                    rel_path = os.path.relpath(filepath)\n                    inventory[rel_path] = file_inventory\n    return inventory\n\ndef main():\n    """"""Main function to generate the inventory.""""""\n    full_inventory = {}\n\n    for directory in DIRECTORIES:\n        if os.path.exists(directory):\n            print(f""Scanning {directory}..."")\n            inventory = scan_directory(directory)\n            full_inventory.update(inventory)\n        else:\n            print(f""Directory not found: {directory}"")\n\n    with open(OUTPUT_FILE, ""w"", encoding=""utf-8"") as f:\n        json.dump(full_inventory, f, indent=4)\n\n    print(f""Inventory saved to {OUTPUT_FILE}"")\n\nif __name__ == ""__main__"":\n    main()\n\n\n```\n\n[metadata_db_call_inventory.json](https://github.com/user-attachments/files/18510555/metadata_db_call_inventory.json)', 'created_at': datetime.datetime(2025, 1, 22, 19, 14, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2608069641, 'issue_id': 2802261351, 'author': 'kaxil', 'body': '@potiuk Would you be able to run this program some point next month?', 'created_at': datetime.datetime(2025, 1, 22, 19, 15, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2608242748, 'issue_id': 2802261351, 'author': 'potiuk', 'body': '> [@potiuk](https://github.com/potiuk) Would you be able to run this program some point next month?\n\nSure', 'created_at': datetime.datetime(2025, 1, 22, 20, 54, 25, tzinfo=datetime.timezone.utc)}]","potiuk (Assginee) on (2025-01-21 19:47:17 UTC): It should also be automated/verified with tests -> as described in Stage 3 of https://github.com/apache/airflow/issues/42632#issuecomment-2449671014

kaxil (Issue Creator) on (2025-01-22 19:14:41 UTC): I created a very basic list:

The following providers utilize's Airflow's session object:

- Databricks
- Edge
- Elasticsearch
- FAB
- Google
- Kubernetes
- OpenLineage
- OpenSearch
- Standard

[This GitHub search](https://github.com/search?q=repo%3Aapache%2Fairflow+%22from+airflow.utils.session%22+OR+%22from+airflow.settings+import+Session%22+path%3A%2F%5Eproviders%5C%2Fsrc%5C%2Fairflow%5C%2Fproviders%5C%2F%2F&type=code) should show the results too:


-----

**`Databricks`**
- Uses it for the [Workflow plugin](https://github.com/apache/airflow/blob/6751c9635f86838086bbdfc52a17ad6818fbd03a/providers/src/airflow/providers/databricks/plugins/databricks_workflow.py)


**`Edge`**
- Expected for now since it is an executor.

**`Elasticsearch`**
- Uses it in the log handler. Needs to change

https://github.com/apache/airflow/blob/6751c9635f86838086bbdfc52a17ad6818fbd03a/providers/src/airflow/providers/elasticsearch/log/es_task_handler.py#L225-L245

**`FAB`**
- For Auth, stores data in FAB related tables in Airflow DB

**`Google`**

Uses it for getting current task instance in the **Triggerer** for BigQuery & Dataproc. Needs to change

https://github.com/apache/airflow/blob/6751c9635f86838086bbdfc52a17ad6818fbd03a/providers/src/airflow/providers/google/cloud/triggers/dataproc.py#L246-L255

**`K8s`**

Uses it for:
- Rendering K8s Pod Spec YAML to show in Airflow UI for KPO & KE. ([Code](https://github.com/apache/airflow/blob/6751c9635f86838086bbdfc52a17ad6818fbd03a/providers/src/airflow/providers/cncf/kubernetes/template_rendering.py)
- For KubernetesExecutor. Expected for now, needs to change via https://github.com/apache/airflow/issues/45427
- For helper function to look [up the run_id from the TI table!](https://github.com/apache/airflow/blob/6751c9635f86838086bbdfc52a17ad6818fbd03a/providers/src/airflow/providers/cncf/kubernetes/kubernetes_helper_functions.py#L105C15-L105C52)

**`OpenLineage`**
Uses it for checking if a TI is rescheduled or not but might be using it for more things. Needs to change!
 `is_ti_rescheduled_already` ([Code](https://github.com/apache/airflow/blob/6751c9635f86838086bbdfc52a17ad6818fbd03a/providers/src/airflow/providers/openlineage/utils/utils.py#L186-L205))

**`OpenSearch`**
- Same as ES and needs to change. ([Code](https://github.com/apache/airflow/blob/6751c9635f86838086bbdfc52a17ad6818fbd03a/providers/src/airflow/providers/opensearch/log/os_task_handler.py#L278))

**`Standard`**
- Lot of changes needed here.
- ExternalTaskSensor and its Triggers ([code](https://github.com/apache/airflow/blob/6751c9635f86838086bbdfc52a17ad6818fbd03a/providers/src/airflow/providers/standard/sensors/external_task.py))
- `TriggerDagRunOperator` and its ExtraOperatorLinks ([code](https://github.com/apache/airflow/blob/6751c9635f86838086bbdfc52a17ad6818fbd03a/providers/src/airflow/providers/standard/operators/trigger_dagrun.py#L45))
- Bash Task decorator ([code](https://github.com/apache/airflow/blob/6751c9635f86838086bbdfc52a17ad6818fbd03a/providers/src/airflow/providers/standard/operators/bash.py#L201-L220))


----

There might be other usage which I haven't dug into. I only checked for presence of the following in a file:

- `from airflow.utils.session`
- `from airflow.settings import Session`

There might be other cases where a Provider code might be importing things from `airflow.models` to import `DagRun` or `TaskInstance` and such directly, that will need to be found out and changed too.

---

Apart from this task itself, we will need to separately change all the provider code to not import any code from `airflow` core and instead strictly depend on the Task SDK.


A small script to get a similar inventory:

```py
import os
import re
import json

# Directories to scan
DIRECTORIES = [""providers"", ""airflow/operators""]

# Patterns to identify Airflow metadata DB access
DB_PATTERNS = [
    r""from airflow\.utils\.session"",
    r""from airflow\.settings import Session"",
    # r""from airflow\.models""
]

# Exclude tests directory
EXCLUDED_DIRS = [""tests""]

OUTPUT_FILE = ""metadata_db_call_inventory.json""

def matches_db_pattern(line):
    """"""Check if a line matches any metadata DB access pattern.""""""
    return any(re.search(pattern, line) for pattern in DB_PATTERNS)

def scan_file(filepath):
    """"""Scan a single file for metadata DB access patterns.""""""
    file_inventory = []
    with open(filepath, ""r"", encoding=""utf-8"") as f:
        try:
            lines = f.readlines()
        except Exception as e:
            print(f""Error reading {filepath}: {e}"")
            return []

        for i, line in enumerate(lines, start=1):
            if matches_db_pattern(line):
                file_inventory.append({""line"": i, ""content"": line.strip()})
    return file_inventory

def scan_directory(directory):
    """"""Scan the given directory for metadata DB access patterns.""""""
    inventory = {}
    for root, dirs, files in os.walk(directory):
        dirs[:] = [d for d in dirs if d not in EXCLUDED_DIRS]

        for file in files:
            if file.endswith("".py""):
                filepath = os.path.join(root, file)
                file_inventory = scan_file(filepath)
                if file_inventory:
                    rel_path = os.path.relpath(filepath)
                    inventory[rel_path] = file_inventory
    return inventory

def main():
    """"""Main function to generate the inventory.""""""
    full_inventory = {}

    for directory in DIRECTORIES:
        if os.path.exists(directory):
            print(f""Scanning {directory}..."")
            inventory = scan_directory(directory)
            full_inventory.update(inventory)
        else:
            print(f""Directory not found: {directory}"")

    with open(OUTPUT_FILE, ""w"", encoding=""utf-8"") as f:
        json.dump(full_inventory, f, indent=4)

    print(f""Inventory saved to {OUTPUT_FILE}"")

if __name__ == ""__main__"":
    main()


```

[metadata_db_call_inventory.json](https://github.com/user-attachments/files/18510555/metadata_db_call_inventory.json)

kaxil (Issue Creator) on (2025-01-22 19:15:57 UTC): @potiuk Would you be able to run this program some point next month?

potiuk (Assginee) on (2025-01-22 20:54:25 UTC): Sure

"
2802214655,issue,closed,completed,Get Operators subclassing SkipMixin working with Task SDK,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

The following classes subclass airflow.models.skipmixin.SkipMixin, which directly accesses the DB:

BranchMixIn (used in BaseBranchOperator, BranchPythonOperator, BranchDateTimeOperator, LatestOnlyOperator, etc.)
BaseSensorOperator
BranchSQLOperator
ShortCircuitOperator
These operators need to be refactored to work with Task SDK, as the current implementation directly accesses the database. The operations should be routed to the API server from the worker instead of the current DB access.

### What you think should happen instead?

Remove the inheritance of these operators from SkipMixin.
Route the DB-related operations to the API Server from the worker.

### How to reproduce

Use any of the affected operators (e.g., BaseBranchOperator, BranchPythonOperator, etc.).
Observe that DB operations are being performed directly by the operators, causing a dependency on DB access in the Task SDK context.

### Operating System

Windows

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

This issue occurs whenever the affected operators are used, causing issues with Task SDK integration due to direct DB access.

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",atharv9017,2025-01-21 15:50:51+00:00,[],2025-01-21 19:56:29+00:00,2025-01-21 19:51:52+00:00,https://github.com/apache/airflow/issues/45838,"[('kind:bug', 'This is a clearly a bug'), ('area:MetaDB', 'Meta Database related issues.'), ('area:core', ''), ('area:core-operators', 'Operators, Sensors and hooks within Core Airflow'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2605102859, 'issue_id': 2802214655, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 21, 15, 50, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2605181960, 'issue_id': 2802214655, 'author': 'tirkarthi', 'body': 'Is this a duplicate of https://github.com/apache/airflow/issues/45823 ?', 'created_at': datetime.datetime(2025, 1, 21, 16, 20, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2605608884, 'issue_id': 2802214655, 'author': 'potiuk', 'body': 'This is likely a bot repeating 2 of the issues created earier by @kaxil. I am reporting the bot to GitHub', 'created_at': datetime.datetime(2025, 1, 21, 19, 51, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2605616718, 'issue_id': 2802214655, 'author': 'potiuk', 'body': 'Reported', 'created_at': datetime.datetime(2025, 1, 21, 19, 56, 27, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-21 15:50:54 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

tirkarthi on (2025-01-21 16:20:18 UTC): Is this a duplicate of https://github.com/apache/airflow/issues/45823 ?

potiuk on (2025-01-21 19:51:52 UTC): This is likely a bot repeating 2 of the issues created earier by @kaxil. I am reporting the bot to GitHub

potiuk on (2025-01-21 19:56:27 UTC): Reported

"
2802188625,issue,closed,completed,solvig the issue,"### Description


Transition of Remote Commands, below are remote commands. We should integrate CLI API client and call API endpoints to eliminate sessions and direct database calls from remote commands.

asset
backfill
config
connection
dag
jobs
pool
provider
task (This will be decided after discussion, that's why there is no sub-issue for it, it will be updated)
variable
version
Dependency
https://github.com/apache/airflow/pull/45300 should be merged.


### Use case/motivation


[AIP-81](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-81+Enhanced+Security+in+CLI+via+Integration+of+API)



### Related issues


No response

Are you willing to submit a PR?

Yes I am willing to submit a PR!
Code of Conduct

I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",hazemAmr0,2025-01-21 15:40:23+00:00,[],2025-01-28 16:48:02+00:00,2025-01-23 21:33:48+00:00,https://github.com/apache/airflow/issues/45837,"[('area:CLI', ''), ('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('needs-triage', ""label for new issues that we didn't triage yet""), ('AI Spam', '')]","[{'comment_id': 2605076362, 'issue_id': 2802188625, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 21, 15, 40, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2611062592, 'issue_id': 2802188625, 'author': 'bugraoz93', 'body': 'Hey @hazemAmr0, there is already an issue for this purpose which was created earlier. I think you just copy pasted the description and created this one from #45661. There are already issues created and assigned for each of the commands. So I would say if you like to work on AIPs. Follow their project board and ping them you want to work on or similar. First please read [contributing](https://github.com/apache/airflow/blob/main/contributing-docs/README.rst) docs.\n\nI am closing this in favour of #45661.', 'created_at': datetime.datetime(2025, 1, 23, 21, 33, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2613919826, 'issue_id': 2802188625, 'author': 'potiuk', 'body': 'I think this is another case of AI generated spam. @hazemAmr0 -> if you are using some tools to generate such issues automatically, please stop.', 'created_at': datetime.datetime(2025, 1, 25, 10, 32, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2619541118, 'issue_id': 2802188625, 'author': 'hazemAmr0', 'body': ""I'm really sorry there was a misunderstanding @potiuk"", 'created_at': datetime.datetime(2025, 1, 28, 16, 48, 1, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-21 15:40:26 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

bugraoz93 on (2025-01-23 21:33:48 UTC): Hey @hazemAmr0, there is already an issue for this purpose which was created earlier. I think you just copy pasted the description and created this one from #45661. There are already issues created and assigned for each of the commands. So I would say if you like to work on AIPs. Follow their project board and ping them you want to work on or similar. First please read [contributing](https://github.com/apache/airflow/blob/main/contributing-docs/README.rst) docs.

I am closing this in favour of #45661.

potiuk on (2025-01-25 10:32:57 UTC): I think this is another case of AI generated spam. @hazemAmr0 -> if you are using some tools to generate such issues automatically, please stop.

hazemAmr0 (Issue Creator) on (2025-01-28 16:48:01 UTC): I'm really sorry there was a misunderstanding @potiuk

"
2801287157,issue,open,,Get Operators subclassing `SkipMixin` working with Task SDK,"The following classes sub-class `airflow.models.skipmixin:SkipMixin` which directly access DB. 
- BranchMixIn (which is further used in `BaseBranchOperator`, `BranchPythonOperator`,`BranchDateTimeOperator`, `LatestOnlyOperator` etc
- BaseSensorOperator
- BranchSQLOperator
- ShortCircuitOperator

To make it work with Task SDK and isolate the DB, we will have to remove the inheritance of these operators from `SkipMixin` and instead route these operations to the API Server from the worker.

<img width=""638"" alt=""Image"" src=""https://github.com/user-attachments/assets/791f3d47-25a4-4884-a675-5dbd330818f8"" />

",kaxil,2025-01-21 09:55:42+00:00,['shahar1'],2025-02-07 18:16:24+00:00,,https://github.com/apache/airflow/issues/45823,"[('area:API', ""Airflow's REST/HTTP API""), ('area:core-operators', 'Operators, Sensors and hooks within Core Airflow'), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2800323635,issue,closed,completed,test_login_submit fails randomly,"The TestSimpleAuthManagerAuthenticationViews::test_login_submit fails randomly from time to time:

```python
=================================== FAILURES ===================================
_ TestSimpleAuthManagerAuthenticationViews.test_login_submit[test-test-True-query_params0-None] _
tests/auth/managers/simple/views/test_auth.py:82: in test_login_submit
    assert response.location == expected_redirect
E   AssertionError: assert equals failed
E     '/login?error=1'     '/home?token=token'
---------------------------- Captured stdout setup -----------------------------
```

Example: https://github.com/apache/airflow/actions/runs/12869146077/job/35877996667#step:6:3386",potiuk,2025-01-20 21:55:07+00:00,['vincbeck'],2025-01-20 22:44:34+00:00,2025-01-20 22:43:57+00:00,https://github.com/apache/airflow/issues/45818,"[('area:CI', ""Airflow's tests and continious integration"")]",[],
2800277588,issue,closed,completed,Unify Data Models for Bulk Actions in Rest API (FastAPI),"### Description

Unify and use data models for Bulk Operations. The goal is to limit code duplication.

Also we can Leverage [Union in Pydantic](https://docs.pydantic.dev/latest/concepts/unions/#left-to-right-mode), and add a pydantic discriminator to the union on the `action` field to save time at the serialization time. The action field tells us what model to deserialize instead of using a left to right resolution.

### Use case/motivation

To eliminate duplicate code fragments for similar purposes in bulk operations for models such as connections, pools and variables.

### Related issues

#45601

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2025-01-20 21:12:56+00:00,[],2025-01-26 20:09:58+00:00,2025-01-26 20:09:58+00:00,https://github.com/apache/airflow/issues/45816,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('area:core', '')]","[{'comment_id': 2603265664, 'issue_id': 2800277588, 'author': 'potiuk', 'body': 'Nice!', 'created_at': datetime.datetime(2025, 1, 20, 21, 42, 3, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-20 21:42:03 UTC): Nice!

"
2800232117,issue,closed,completed,Port `prev_*_success` Task Context keys to Task SDK,"Port the following keys to Task SDK

https://github.com/apache/airflow/blob/b081f66d52cc20ba0cd8eb37b8984ed3d9805277/airflow/models/taskinstance.py#L963-L986

https://github.com/apache/airflow/blob/b081f66d52cc20ba0cd8eb37b8984ed3d9805277/airflow/models/taskinstance.py#L1030-L1033

It should be added here:

https://github.com/apache/airflow/blob/b081f66d52cc20ba0cd8eb37b8984ed3d9805277/task_sdk/src/airflow/sdk/execution_time/task_runner.py#L103-L106",kaxil,2025-01-20 20:34:05+00:00,['kaxil'],2025-01-21 05:44:26+00:00,2025-01-21 05:44:26+00:00,https://github.com/apache/airflow/issues/45814,"[('area:core', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2800197114,issue,open,,KubernetesPodOperator dry_run failure,"### Apache Airflow Provider(s)

cncf-kubernetes

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes 4.3.0


### Apache Airflow version

2.3.4

### Operating System

Linux

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

n/a

### What happened

We are upgrading from apache-airflow-providers-cncf-kubernetes 3.0.0 to 4.3.0 (going slowly through releases).

We have a custom script, that during docker image build of our airflow, tests all dags and all dag tasks in dry_run mode. Mostly to detect Python syntax errors, dag cycles duplicate tasks, wrong imports, ntemplating errors etc.

This was working all fine with our existing airflow, but we decided to upgrade airflow to newer version, and that also means updating airflow providers. After fixing bunch of other issues, I found the issues with KubernetedPodOperator dry run.

New dry_run added in https://github.com/apache/airflow/commit/d56ff765e15f9fcd582bc6d1ec0e83b0fedf476a invokes `KubernetesPodOperator` `build_pod_request_obj()` method which has a call to a property `self.hook.is_in_cluster`:

```python
        pod.metadata.labels.update(
            {
                'airflow_version': airflow_version.replace('+', '-'),
                'airflow_kpo_in_cluster': str(self.hook.is_in_cluster),
            }
        )
```

Unfortunately this property constructs a Kube API client object which requires kube client config / credentials to work.

```python
    @property
    def is_in_cluster(self):
        """"""Expose whether the hook is configured with ``load_incluster_config`` or not""""""
        if self._is_in_cluster is not None:
            return self._is_in_cluster
        self.api_client  # so we can determine if we are in_cluster or not
        return self._is_in_cluster```
```

This causes dry_run to not able to execute in isolated test environment:

```
Traceback (most recent call last):
  File ""<stdin>"", line 97, in <module>
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dag.py"", line 2307, in cli
    args.func(args, self)
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_parser.py"", line 51, in command
    return func(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py"", line 99, in wrapper
    return f(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py"", line 545, in task_test
    ti.dry_run()
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py"", line 1815, in dry_run
    self.task.dry_run()
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py"", line 607, in dry_run
    pod = self.build_pod_request_obj()
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/cncf/kubernetes/operators/kubernetes_pod.py"", line 595, in build_pod_request_obj
    'airflow_kpo_in_cluster': str(self.hook.is_in_cluster),
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/cncf/kubernetes/hooks/kubernetes.py"", line 283, in is_in_cluster
    self.api_client  # so we can determine if we are in_cluster or not
  File ""/usr/local/lib/python3.9/functools.py"", line 993, in __get__
    val = self.func(instance)
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/cncf/kubernetes/hooks/kubernetes.py"", line 291, in api_client
    return self.get_conn()
  File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/providers/cncf/kubernetes/hooks/kubernetes.py"", line 239, in get_conn
    config.load_kube_config(
  File ""/home/airflow/.local/lib/python3.9/site-packages/kubernetes/config/kube_config.py"", line 808, in load_kube_config
    loader = _get_kube_config_loader(
  File ""/home/airflow/.local/lib/python3.9/site-packages/kubernetes/config/kube_config.py"", line 767, in _get_kube_config_loader
    raise ConfigException(
kubernetes.config.config_exception.ConfigException: Invalid kube-config file. No configuration found.
```


We would like to continue using dry_run, but be able to run it without providing credentials or kube config. It does not need to be 100% accurate.

Two options:

* env var to bypass setting of `airflow_kpo_in_cluster` label in dry run mode, if user requests to do so.
* never populate it in dry_run mode. (change signature of `build_pod_request_obj` to have `dry_run: bool = False` kwarg and invoke it with `dry_run=True` in KubernetesPodOperator.dry_run()` method.

(or both) 

### What you think should happen instead

n/a

### How to reproduce

n/a

### Anything else

n/a

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",baryluk,2025-01-20 20:05:26+00:00,['baryluk'],2025-01-29 20:47:17+00:00,,https://github.com/apache/airflow/issues/45812,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]","[{'comment_id': 2603264801, 'issue_id': 2800197114, 'author': 'potiuk', 'body': 'Feel free to attempt to fix it and provide PR', 'created_at': datetime.datetime(2025, 1, 20, 21, 41, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2604891270, 'issue_id': 2800197114, 'author': 'baryluk', 'body': '@potiuk Sure, I can, but I wanted to get some feedback from kubernetes provider maintainers first what would they prefer.', 'created_at': datetime.datetime(2025, 1, 21, 14, 29, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2605030578, 'issue_id': 2800197114, 'author': 'potiuk', 'body': 'There are no ""kubernetes provider maintainers"" here. It\'s all ""airflow"" maintainers - anyone can comment here, and any maintainer can approve PR that is created. Since we are in the hottest part of Airlfow 3 building, it\'s not very likely that you will get more feedback than that, so creating a good PR with proposal how you would like to solve it is the best way to grab attention of maintainers who could approve it.', 'created_at': datetime.datetime(2025, 1, 21, 15, 22, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2622812645, 'issue_id': 2800197114, 'author': 'nevcohen', 'body': ""If I'm not mistaken, the purpose of `dry run` is to simulate running in a real environment, so you must give a credential (or kube config), if not, then where do you want it to run?"", 'created_at': datetime.datetime(2025, 1, 29, 20, 47, 16, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-20 21:41:08 UTC): Feel free to attempt to fix it and provide PR

baryluk (Issue Creator) on (2025-01-21 14:29:42 UTC): @potiuk Sure, I can, but I wanted to get some feedback from kubernetes provider maintainers first what would they prefer.

potiuk on (2025-01-21 15:22:54 UTC): There are no ""kubernetes provider maintainers"" here. It's all ""airflow"" maintainers - anyone can comment here, and any maintainer can approve PR that is created. Since we are in the hottest part of Airlfow 3 building, it's not very likely that you will get more feedback than that, so creating a good PR with proposal how you would like to solve it is the best way to grab attention of maintainers who could approve it.

nevcohen on (2025-01-29 20:47:16 UTC): If I'm not mistaken, the purpose of `dry run` is to simulate running in a real environment, so you must give a credential (or kube config), if not, then where do you want it to run?

"
2800038240,issue,open,,Fix all existing docs external references (js/fonts/images),"### Body

_No response_

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",gopidesupavan,2025-01-20 18:06:36+00:00,['gopidesupavan'],2025-01-20 18:08:52+00:00,,https://github.com/apache/airflow/issues/45808,"[('kind:documentation', ''), ('kind:meta', 'High-level information important to the community')]",[],
2800036550,issue,open,,Move all external url references and images to repository in airflow-site,"### Body

_No response_

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",gopidesupavan,2025-01-20 18:05:37+00:00,['gopidesupavan'],2025-01-20 18:05:48+00:00,,https://github.com/apache/airflow/issues/45807,"[('kind:meta', 'High-level information important to the community')]",[],
2798767157,issue,closed,completed,"Airflow web UI shows a task is still running, but in fact it is not","### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.5.3

### What happened?

# Background 
- Airflow is deployed in kuberenetes (k8s) cluster
- The DAG consists of daily tasks that uses `KubernetesPodOperator`
- The DAG is set with `max_active_runs=1` (because we don't have concurrent write)

# Issue
When we backfill the data, we go into the `airflow-scheduler` bash shell in k8s, and execute `airflow backfill` command, hoping that the backfill would complete over the weekend.

At some point, the shell is exited. Then the next Monday, I still find the the airflow web UI shows the DAG is still running. However, when I check the kubernetes, no such Pod is running. Since DAG is set with `max_active_runs=1`, it cannot keep consuming the latest data.

# Question
How could I prevent such inconsistency between Airflow and the actual running tasks in Kubernetes?


### What you think should happen instead?

_No response_

### How to reproduce

- Airflow running in Kubernetes cluster
- Have a DAG (`max_active_run = 1`)  with tasks using `KubernetesPodOperator`
- Execute into `airflow-scheduler` bash shell, and run `airflow backfill` command (Note that it is expected that the backfill takes a long time to run, say > 30 minutes)
- Then, at some point during backfill, you exit the bash shell

It is expected that the Airflow web UI is still observing a running task, but no such Pod in kubernetes cluster

### Operating System

Debian GNU/Linux 11 (bullseye)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",newTypeGeek,2025-01-20 09:54:43+00:00,[],2025-01-20 10:13:22+00:00,2025-01-20 10:13:21+00:00,https://github.com/apache/airflow/issues/45796,"[('kind:bug', 'This is a clearly a bug'), (""won't fix"", ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2601936075, 'issue_id': 2798767157, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 20, 9, 54, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2601982878, 'issue_id': 2798767157, 'author': 'potiuk', 'body': ""Backfill behaviour has been completely rewritten in the upcoming Airlfow 3 - the CLI will not work the way it worked so far - backfill will be also runnable via API and you will be able ot track it's progress via UI, there will be no need to leave the CLI running. We are not going to fix any problems resulting from that in Airflow 2"", 'created_at': datetime.datetime(2025, 1, 20, 10, 13, 21, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-20 09:54:45 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2025-01-20 10:13:21 UTC): Backfill behaviour has been completely rewritten in the upcoming Airlfow 3 - the CLI will not work the way it worked so far - backfill will be also runnable via API and you will be able ot track it's progress via UI, there will be no need to leave the CLI running. We are not going to fix any problems resulting from that in Airflow 2

"
2798143424,issue,closed,completed,[Critical Issue] Session Table Auto-Increment Mechanism Leads to ID Exhaustion in Airflow Database,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

airflow-web failed with error logs as follow：

[SQL: INSERT INTO session (session_id, data, expiry) VALUES (%(session_id)s, %(data)s, %(expiry)s) RETURNING session.id]
sqlalchemy.exc.DataError: (psycopg2.errors.NumericValueOutOfRange) integer out of range



airflow_2=> SELECT MAX(id) FROM session;
    max     
------------
 2147483647
(1 row)


airflow_2=> \d session
Table ""public.session""
   Column   |            Type             |                      Modifiers                       
------------+-----------------------------+------------------------------------------------------
 id         | integer                     | not null default nextval('session_id_seq'::regclass)
 session_id | character varying(255)      | 
 data       | bytea                       | 
 expiry     | timestamp without time zone | 
Indexes:
    ""session_pkey"" PRIMARY KEY, btree (id)
    ""session_session_id_key"" UNIQUE CONSTRAINT, btree (session_id)



airflow_2=> SELECT MAX(id) FROM session;
    max     
------------
 2147483647
(1 row)






### What you think should happen instead?

_No response_

### How to reproduce

running airflow for a long time with a lot of tasks

### Operating System

centos

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### Anything else?

[SQL: INSERT INTO session (session_id, data, expiry) VALUES (%(session_id)s, %(data)s, %(expiry)s) RETURNING session.id]
[parameters: {'session_id': 'b8a8eb6a-2f7a-496f-b409-1e2b43a06eb2', 'data': <psycopg2.extensions.Binary object at 0x7f153e16a100>, 'expiry': datetime.datetime(2025, 2, 14, 2, 19, 47, 825437, tzinfo=datetime.timezone.utc)}]
(Background on this error at: https://sqlalche.me/e/14/9h9h)
[2025-01-15T10:19:47.826+0800] {app.py:1744} ERROR - Exception on / [HEAD]
Traceback (most recent call last):
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context
    self.dialect.do_execute(
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py"", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.NumericValueOutOfRange: integer out of range


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/flask/app.py"", line 2529, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/flask/app.py"", line 1826, in full_dispatch_request
    return self.finalize_request(rv)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/flask/app.py"", line 1847, in finalize_request
    response = self.process_response(response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/flask/app.py"", line 2344, in process_response
    self.session_interface.save_session(self, ctx.session, response)
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/airflow/www/session.py"", line 33, in save_session
    return super().save_session(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/flask_session/sessions.py"", line 568, in save_session
    self.db.session.commit()
  File ""<string>"", line 2, in commit
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/orm/session.py"", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/orm/session.py"", line 832, in commit
    self._prepare_impl()
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/orm/session.py"", line 811, in _prepare_impl
    self.session.flush()
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/orm/session.py"", line 3449, in flush
    self._flush(objects)
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/orm/session.py"", line 3588, in _flush
    with util.safe_reraise():
         ^^^^^^^^^^^^^^^^^^^
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/util/compat.py"", line 211, in raise_
    raise exception
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/orm/session.py"", line 3549, in _flush
    flush_context.execute()
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py"", line 456, in execute
    rec.execute(self)
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py"", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py"", line 245, in save_obj
    _emit_insert_statements(
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py"", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/sql/elements.py"", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 2134, in _handle_dbapi_exception
    util.raise_(
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/util/compat.py"", line 211, in raise_
    raise exception
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context
    self.dialect.do_execute(
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py"", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.DataError: (psycopg2.errors.NumericValueOutOfRange) integer out of range

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)

_Originally posted by @claude89757 in https://github.com/apache/airflow/discussions/45782_",claude89757,2025-01-20 04:22:33+00:00,[],2025-01-20 05:18:32+00:00,2025-01-20 05:18:31+00:00,https://github.com/apache/airflow/issues/45793,"[('area:MetaDB', 'Meta Database related issues.')]","[{'comment_id': 2601300065, 'issue_id': 2798143424, 'author': 'claude89757', 'body': ""Through this issue, I believe there is a problem with the use of the auto-increment mechanism for the session table in Airflow's database. The specific reasons are as follows:\n\nI already have a scheduled task to execute from airflow.utils.db_cleanup import run_cleanup to clean up the database cache.\nHowever, this cache cleanup operation does not resolve the issue of the id in the session table continuously increasing and eventually exhausting.\nI urge the Airflow project developers to address this issue, as it could affect the long-term stability of the Airflow project."", 'created_at': datetime.datetime(2025, 1, 20, 4, 23, 38, tzinfo=datetime.timezone.utc)}]","claude89757 (Issue Creator) on (2025-01-20 04:23:38 UTC): Through this issue, I believe there is a problem with the use of the auto-increment mechanism for the session table in Airflow's database. The specific reasons are as follows:

I already have a scheduled task to execute from airflow.utils.db_cleanup import run_cleanup to clean up the database cache.
However, this cache cleanup operation does not resolve the issue of the id in the session table continuously increasing and eventually exhausting.
I urge the Airflow project developers to address this issue, as it could affect the long-term stability of the Airflow project.

"
2797623302,issue,closed,completed,DockerOperator return value not usable in dyn. task mapping,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

```
@task
def hash_article_url(article_data):
          return article_data

article_data = DockerOperator.partial(
          task_id=""fetch"",
          image=""xyz_image:0.0.1"",
          auto_remove=True,
          docker_url=""tcp://socket-proxy:2375"",
          network_mode=""default"",
          #tty=True,
          xcom_all=False,
          mount_tmp_dir=True,
).expand(command=docker_commands)
hash_article_url.expand(article_data=article_data)

```

results in error:

> Broken DAG: [/opt/airflow/dags/mediumcom_from_list_dag/dag.py]
> Traceback (most recent call last):
>   File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/decorators/base.py"", line 403, in _validate_arg_names
>     super()._validate_arg_names(func, kwargs)
>   File ""/home/airflow/.local/lib/python3.9/site-packages/airflow/decorators/base.py"", line 116, in _validate_arg_names
>     raise ValueError(
> ValueError: expand() got an unexpected type 'MappedOperator' for keyword argument 'article_data'

### What you think should happen instead?

no error - should simply compile

### How to reproduce

execute a script via DockerOperator on docker image that returns a dict

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",olk,2025-01-19 13:21:06+00:00,[],2025-01-19 13:31:26+00:00,2025-01-19 13:31:26+00:00,https://github.com/apache/airflow/issues/45787,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:dynamic-task-mapping', 'AIP-42'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2600859186, 'issue_id': 2797623302, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 19, 13, 21, 9, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-19 13:21:09 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2796858317,issue,open,,TestDbApiHook::test_run_no_log  randomly fails,"Sometimes TestDbApiHook::test_run_no_log  randomly fails (another caplog case) - usually in compat tests:

```python
________________________ TestDbApiHook.test_run_no_log _________________________
providers/tests/common/sql/hooks/test_dbapi.py:516: in test_run_no_log
    assert len(caplog.messages) == 1
E   assert 2 == 1
E    +  where 2 = len(['Rows affected: 0', ""Task was destroyed but it is pending!\ntask: <Task pending name='Task-113' coro=<<async_generator_asend without __name__>()> wait_for=<Future pending cb=[<TaskWakeupMethWrapper object at 0x7f8d90c493a0>()]>>""])
E    +    where ['Rows affected: 0', ""Task was destroyed but it is pending!\ntask: <Task pending name='Task-113' coro=<<async_generator_asend without __name__>()> wait_for=<Future pending cb=[<TaskWakeupMethWrapper object at 0x7f8d90c493a0>()]>>""] = <_pytest.logging.LogCaptureFixture object at 0x7f8d72fc7cd0>.messages
----------------------------- Captured stdout call -----------------------------
```

For example : https://github.com/apache/airflow/actions/runs/12835627081/job/35795783211#step:11:2417",potiuk,2025-01-18 08:39:51+00:00,[],2025-01-18 08:42:05+00:00,,https://github.com/apache/airflow/issues/45774,"[('area:logging', '')]",[],
2796457953,issue,closed,completed,The xcom sidecar container to be in the running state before we exec against it.,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

There is no guarantee that the xcom sidecar container will have been running by the time the extract_xcom is used by KPO's execute even if we wait for pod to be a ""Running"" state - this further means if the base container has completed before the xcom sidecar container is running, the entire operator fails because you cannot exec against a non-running container.

### What you think should happen instead?

Before we exec against it, it needs to wait for the Xcom sidecar container to be in ""running"" state.

### How to reproduce

Have a k8s cluster that can reproduce the behavior of the Xcom container not starting when the base container has been completed, and check that Xcom extraction was successful.

### Operating System

Windows

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lechufane,2025-01-17 23:15:59+00:00,[],2025-01-28 19:01:47+00:00,2025-01-28 19:01:47+00:00,https://github.com/apache/airflow/issues/45772,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('pending-response', ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]","[{'comment_id': 2599359216, 'issue_id': 2796457953, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 17, 23, 16, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2603349353, 'issue_id': 2796457953, 'author': 'insomnes', 'body': '> There is no guarantee that the xcom sidecar container will have been running by the time the extract_xcom is used by KPO\'s execute even if we wait for pod to be a ""Running"" state \n\n> Before we exec against it, it needs to wait for the Xcom sidecar container to be in ""running"" state.\n\nCan you please clarify a bit?\nThe Pod manager is now waiting for the sidecar pod to be in ""running"" state before executing xcom extraction commands,  isn\'t it? What do you mean hear?', 'created_at': datetime.datetime(2025, 1, 20, 23, 18, 39, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-17 23:16:02 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

insomnes on (2025-01-20 23:18:39 UTC): Can you please clarify a bit?
The Pod manager is now waiting for the sidecar pod to be in ""running"" state before executing xcom extraction commands,  isn't it? What do you mean hear?

"
2796446451,issue,closed,completed,Fix dag-processor logging,"### Body

There are a couple logging related tasks that we need to tackle

- [ ] Get processor logs for individual files writing again - right now they don't land on disk anywhere
- [ ] Move the dag processor manager logs to stdout - we don't need the separate log file any longer

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2025-01-17 23:03:20+00:00,['jedcunningham'],2025-02-04 23:27:42+00:00,2025-02-04 23:27:42+00:00,https://github.com/apache/airflow/issues/45771,"[('area:logging', ''), ('kind:meta', 'High-level information important to the community'), ('area:core', ''), ('AIP-66: DAG Bundle/Manifest', '')]","[{'comment_id': 2635290123, 'issue_id': 2796446451, 'author': 'jedcunningham', 'body': 'This was done by #46355 and #45888.', 'created_at': datetime.datetime(2025, 2, 4, 23, 27, 42, tzinfo=datetime.timezone.utc)}]","jedcunningham (Issue Creator) on (2025-02-04 23:27:42 UTC): This was done by #46355 and #45888.

"
2796393229,issue,open,,first run after turning on will be the latest scheduled run,"### Body

Description
Currently, catchup=False means ""first run after turning on will be the latest scheduled run"" and there is no way to produce the behavior where turning on a DAG means ""make the first run after turning on occur at the next schedule time instead of the previous one"".

Proposal: make a new flag (call it something to the effect of no_catchup_means_no_past_runs) which when set to True means that turning on a DAG that has the setting catchup=False will result in the first run being the one on the schedule that occurs next in time (rather than the one that occurs in the most recent past).

Use case/motivation
The use case is whenever you want to turn on a DAG without it running right away.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",Arjit-Sharma001,2025-01-17 22:15:35+00:00,[],2025-01-18 09:12:07+00:00,,https://github.com/apache/airflow/issues/45768,"[('kind:feature', 'Feature Requests'), ('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2599301531, 'issue_id': 2796393229, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 17, 22, 15, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2599639652, 'issue_id': 2796393229, 'author': 'potiuk', 'body': 'Are you really committer? Should you use new feature request for that one?', 'created_at': datetime.datetime(2025, 1, 18, 9, 7, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2599641026, 'issue_id': 2796393229, 'author': 'potiuk', 'body': 'I doubt we want to add even more complexity and new flag. You can attempt to make a PR to add that and see how complex it would be - I guess pretty complex, but what you should do is to set start_date properly on your DAG when you create it - to start at the date you want it.', 'created_at': datetime.datetime(2025, 1, 18, 9, 12, 7, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-17 22:15:37 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2025-01-18 09:07:44 UTC): Are you really committer? Should you use new feature request for that one?

potiuk on (2025-01-18 09:12:07 UTC): I doubt we want to add even more complexity and new flag. You can attempt to make a PR to add that and see how complex it would be - I guess pretty complex, but what you should do is to set start_date properly on your DAG when you create it - to start at the date you want it.

"
2796386380,issue,closed,completed,Software Hossam,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

1.Steps to reproduce the problem.
2.Expected behavior versus actual behavior.
3.Any error messages or logs (if available).

### What you think should happen instead?

- What should the system do if the issue did not exist?
- How does this align with the expected functionality of Airflow?
- Was there an incorrect configuration, bug, or unexpected behavior?
- Were there any recent changes or updates to your system, environment, or Airflow?

### How to reproduce

1.Environment Setup

- Mention the operating system, Python version, Airflow version, and any other relevant environment details.
- Include any specific configurations, dependencies, or plugins used.

2.Steps to Reproduce

- List the actions step-by-step to recreate the issue.
Example:
1.Start the Airflow web server.
2.Create a DAG with specific configurations.
3.Trigger the DAG and observe the logs or UI behavior.

3.Expected Outcome

- What should have happened if the issue did not exist?

4.Actual Outcome

- Describe the incorrect or unexpected behavior you observed.

5.Supporting Evidence

- Attach logs, screenshots, or any other data to help diagnose the issue.

### Operating System

Windows 10

### Versions of Apache Airflow Providers

pip list | grep apache-airflow-providers

### Deployment

Other

### Deployment details

I use to Development Mobile Application and Web Application.

### Anything else?

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Hossam-Samin,2025-01-17 22:09:48+00:00,[],2025-01-17 22:27:12+00:00,2025-01-17 22:27:12+00:00,https://github.com/apache/airflow/issues/45767,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2599295139, 'issue_id': 2796386380, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 17, 22, 9, 51, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-17 22:09:51 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2796376791,issue,closed,completed,Pass connection extra parameters to Wasb BlobServiceClient,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.30

### What happened?

Currently, the WasbHook does not pass the extra parameters from the connection to the BlobServiceClient, limiting flexibility in configuration.

### What you think should happen instead?

The WasbHook should read the extra field from connection configurations and pass these parameters to the underlying BlobServiceClient. This allows users to define configurations such as proxies or other client-specific settings in their connection setup.

### How to reproduce

Not applicable as this is a feature request.

### Operating System

Any

### Versions of Apache Airflow Providers

apache-airflow-providers-microsoft-azure

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Sabbir02,2025-01-17 22:01:15+00:00,[],2025-01-22 01:19:58+00:00,2025-01-22 01:19:58+00:00,https://github.com/apache/airflow/issues/45766,"[('kind:bug', 'This is a clearly a bug'), ('provider:microsoft-azure', 'Azure-related issues'), ('kind:feature', 'Feature Requests'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2599285517, 'issue_id': 2796376791, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 17, 22, 1, 18, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-17 22:01:18 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2795729343,issue,closed,completed,Port Registering of Asset Changes to Task SDK,"Assets are stored in DB after a Task completes execution with `SUCCESS` state as shown in the following code.

https://github.com/apache/airflow/blob/051e617e0d7d0ebb995cb98063709350f279963c/airflow/models/taskinstance.py#L359

This includes extracting outlets from the task and storing them in the DB.

We should port this and related changes to the Task SDK's task_runner.py

https://github.com/apache/airflow/blob/051e617e0d7d0ebb995cb98063709350f279963c/task_sdk/src/airflow/sdk/execution_time/task_runner.py#L600

",kaxil,2025-01-17 15:23:09+00:00,['amoghrajesh'],2025-01-24 06:41:40+00:00,2025-01-24 06:41:39+00:00,https://github.com/apache/airflow/issues/45752,"[('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]","[{'comment_id': 2601936280, 'issue_id': 2795729343, 'author': 'kaxil', 'body': '@amoghrajesh To test this you could run, https://github.com/apache/airflow/blob/main/airflow/example_dags/example_asset_alias.py example DAG', 'created_at': datetime.datetime(2025, 1, 20, 9, 54, 50, tzinfo=datetime.timezone.utc)}]","kaxil (Issue Creator) on (2025-01-20 09:54:50 UTC): @amoghrajesh To test this you could run, https://github.com/apache/airflow/blob/main/airflow/example_dags/example_asset_alias.py example DAG

"
2795446181,issue,closed,completed,"Update release process to include ""versioned"" link in relase mail instead ""stable""","Our release emails contain ""stable"" links instead of versioned links.

> The released sources and packages can be downloaded via https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-sources.html

This is wrong and we should change our templates to contain version instead - the release mails should point to ""versioned"" URLs.",potiuk,2025-01-17 13:09:19+00:00,['potiuk'],2025-01-19 15:30:09+00:00,2025-01-17 13:32:10+00:00,https://github.com/apache/airflow/issues/45747,"[('kind:documentation', '')]",[],
2795286894,issue,open,,Move BaseSensor out of core and in to TaskSDK definitions,,ashb,2025-01-17 11:44:34+00:00,['ashb'],2025-01-21 09:56:03+00:00,,https://github.com/apache/airflow/issues/45743,"[('area:core-operators', 'Operators, Sensors and hooks within Core Airflow')]","[{'comment_id': 2604215500, 'issue_id': 2795286894, 'author': 'kaxil', 'body': 'Related: https://github.com/apache/airflow/issues/45823', 'created_at': datetime.datetime(2025, 1, 21, 9, 56, 3, tzinfo=datetime.timezone.utc)}]","kaxil on (2025-01-21 09:56:03 UTC): Related: https://github.com/apache/airflow/issues/45823

"
2795028627,issue,closed,not_planned,Get `use_airflow_context` working on `_BasePythonVirtualenvOperator` with Task SDK,"
https://github.com/apache/airflow/blob/16eaa5e761fe8b163185f370302750d413ef2164/providers/src/airflow/providers/standard/operators/python.py#L533-L546",kaxil,2025-01-17 09:54:11+00:00,['amoghrajesh'],2025-02-03 09:36:31+00:00,2025-02-03 09:36:30+00:00,https://github.com/apache/airflow/issues/45739,"[('kind:feature', 'Feature Requests'), ('area:core-operators', 'Operators, Sensors and hooks within Core Airflow')]","[{'comment_id': 2627204284, 'issue_id': 2795028627, 'author': 'amoghrajesh', 'body': 'This feature is being reverted by https://github.com/apache/airflow/pull/46306', 'created_at': datetime.datetime(2025, 1, 31, 12, 42, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2630422850, 'issue_id': 2795028627, 'author': 'amoghrajesh', 'body': 'We do not need to handle this as of now, the feature has been removed by https://github.com/apache/airflow/pull/46306\nWill revisit with a later ticket', 'created_at': datetime.datetime(2025, 2, 3, 9, 36, 30, tzinfo=datetime.timezone.utc)}]","amoghrajesh (Assginee) on (2025-01-31 12:42:56 UTC): This feature is being reverted by https://github.com/apache/airflow/pull/46306

amoghrajesh (Assginee) on (2025-02-03 09:36:30 UTC): We do not need to handle this as of now, the feature has been removed by https://github.com/apache/airflow/pull/46306
Will revisit with a later ticket

"
2794975127,issue,open,,DatabricksWorkflowOperator do not update ACL on workflow reset,"### Apache Airflow Provider(s)

databricks

### Versions of Apache Airflow Providers

apache-airflow-providers-databricks==7.0.0

### Apache Airflow version

2.10.4

### Operating System

Linux

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

official Helm Chart

### What happened

DatabricksWorkflowOperator do not update ACL on workflow reset, this is done only during creation of workflow (Databricks API 2.0 implementation)
one more call is needed to Databricks API 2.0

### What you think should happen instead

DatabricksWorkflowOperator on  workflow reset should also update ACL in next api call

### How to reproduce

try to modify ACL after creation of workflow

### Anything else

Proposed solution:
after this line:
https://github.com/apache/airflow/blob/6cde25f1ac005e217b32f19a8bf1be75bf8e7af6/providers/src/airflow/providers/databricks/operators/databricks_workflow.py#L179
add this code:
```
            if ""access_control_list"" in job_spec.keys():
                access_control_list = {""access_control_list"": job_spec[""access_control_list""]}
                self.log.info(
                    ""Updating ACL of Databricks workflow job %s with spec %s"",
                    self.job_name,
                    json.dumps(access_control_list, indent=2),
                )
                self._hook.update_job_permission(job_id, access_control_list)
```

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",adamgorkaextbi,2025-01-17 09:27:32+00:00,[],2025-01-17 10:04:20+00:00,,https://github.com/apache/airflow/issues/45738,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('provider:databricks', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2794874654,issue,closed,completed,Airflow CLI 'task clear' does not contains filter for run_id,"### Description

I try to call task: airflow task clear ecm_cutoff -t MANAGE_JOBS, but the command cleared all instances of task, but I want to clear only the latest one.
I did not find filter for run id, only the following parameters:

Options:
  -h, --help            show this help message and exit
  -R, --dag-regex       Search dag_id as regex instead of exact string
  -d, --downstream      Include downstream tasks
  -e, --end-date END_DATE
                        Override end_date YYYY-MM-DD
  -X, --exclude-parentdag
                        Exclude ParentDAGS if the task cleared is a part of a SubDAG
  -x, --exclude-subdags
                        Exclude subdags
  -f, --only-failed     Only failed jobs
  -r, --only-running    Only running jobs
  -s, --start-date START_DATE
                        Override start_date YYYY-MM-DD
  -S, --subdir SUBDIR   File location or directory from which to look for the dag. Defaults to '[AIRFLOW_HOME]/dags' where [AIRFLOW_HOME] is the value you set for 'AIRFLOW_HOME' config you set in 'airflow.cfg'
  -t, --task-regex TASK_REGEX
                        The regex to filter specific task_ids (optional)
  -u, --upstream        Include upstream tasks
  -v, --verbose         Make logging output more verbose
  -y, --yes             Do not prompt to confirm. Use with care!

In the sources I found logical_date_or_run_id, that filter would be exactly what I need, but for this CLI commnad is not involved.

### Use case/motivation

Our airflow operators needs to re-executed task instances for a particular day. Currently there are many runs in history and we need to touch only the current (newest one) run.

### Related issues

_No response_

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",nesp159de,2025-01-17 08:33:15+00:00,[],2025-01-17 18:04:55+00:00,2025-01-17 18:04:55+00:00,https://github.com/apache/airflow/issues/45734,"[('area:CLI', ''), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2597696860, 'issue_id': 2794874654, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 17, 8, 33, 18, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-17 08:33:18 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2794657421,issue,closed,completed,Compatibility issues with polars: Any calculation of polars out of a task will block calculation in the task,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.0

### What happened?

When I create a constant out of tasks using some polars calculation like df.unique or df.with_columns, any polars calculation in tasks will be stuck. The tasks including it will show running and never end.

my polars version is 0.20.19

### What you think should happen instead?

The task should run as expected

### How to reproduce

```
from airflow.decorators import task
from airflow.models.dag import DAG
import polars as pl
import pendulum    
S1_TS = pl.DataFrame({""ticktime"": [1, 2, 2, 3]}).unique()

with DAG(
    f""test"",
    schedule=""00 20 * * 1-5"",
    start_date=pendulum.datetime(2024, 6, 18),
) as dag:
    # S1_TS = pl.DataFrame({""ticktime"": [1, 2, 2, 3]}).unique() # created constant in the dag context will have the same stucks
    @task
    def test():
        x = pl.DataFrame({""a"": [1, 2]}).filter(a=1)
    test()
```

### Operating System

Ubuntu 20.04.6 LTS (Focal Fossa)

### Versions of Apache Airflow Providers

![Image](https://github.com/user-attachments/assets/70cb7db2-3a1b-42bf-8ebe-c73e2d9075d2)

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

apache-airflow-providers-cncf-kubernetes 8.3.0
apache-airflow-providers-common-compat   1.2.0
apache-airflow-providers-common-io       1.3.1
apache-airflow-providers-common-sql      1.13.0
apache-airflow-providers-fab             1.1.0
apache-airflow-providers-ftp             3.9.0
apache-airflow-providers-http            4.11.0
apache-airflow-providers-imap            3.6.0
apache-airflow-providers-mysql           5.6.1
apache-airflow-providers-smtp            1.7.0
apache-airflow-providers-sqlite          3.8.0

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",WiC-htao,2025-01-17 06:06:53+00:00,[],2025-01-17 17:52:20+00:00,2025-01-17 17:52:20+00:00,https://github.com/apache/airflow/issues/45730,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2793565293,issue,closed,completed,DAG Processing Processor attempts to parse PPTX (and other non-code based ZIP archives) based on zipfile.is_zipfile design,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.3

### What happened?

Dags Processing Manager attempts to parse PPTX files causing it to place corrupted data in the metadata database.

### What you think should happen instead?

Ideally it should not be parsing any none-Python files from the /usr/local/airflow/dags directory.

### How to reproduce

Copy text from an Airflow DAG code and place it inside a PPTX slide. Upload the PPTX to `/usr/local/airflow/dags`, wait for errors to appear in dags processing manager logs (may take little bit of time), for reference I have attached what I used and screenshot:

![Image](https://github.com/user-attachments/assets/4e7413a1-fb67-4ad9-8cc2-38e230f5fb35)

[Testing AIrflow Bug.pptx](https://github.com/user-attachments/files/18444241/Testing.AIrflow.Bug.pptx)

### Operating System

Amazon Linux 2023

### Versions of Apache Airflow Providers

Connection type | Package|
|--|--| 
|AWS Connection |[apache-airflow-providers-amazon[aiobotocore]==9.0.0](https://airflow.apache.org/docs/apache-airflow-providers-amazon/9.0.0/index.html)|
| Postgres Connection|[apache-airflow-providers-postgres==5.13.1](https://airflow.apache.org/docs/apache-airflow-providers-postgres/5.13.1/index.html)| 
|FTP Connection|[apache-airflow-providers-ftp==3.11.1](https://airflow.apache.org/docs/apache-airflow-providers-ftp/3.11.1/index.html)|
|Fab Connection|[apache-airflow-providers-fab==1.5.0](https://airflow.apache.org/docs/apache-airflow-providers-fab/1.5.0/index.html)|
|Celery Connection|[apache-airflow-providers-celery==3.8.3](https://airflow.apache.org/docs/apache-airflow-providers-celery/3.8.3/index.html)|
|HTTP Connection|[apache-airflow-providers-http==4.13.2](https://airflow.apache.org/docs/apache-airflow-providers-http/4.13.2/index.html)|
|IMAP Connection|[apache-airflow-providers-imap==3.7.0](https://airflow.apache.org/docs/apache-airflow-providers-imap/3.7.0/index.html)|
|Common SQL|[apache-airflow-providers-common-sql==1.19.0](https://airflow.apache.org/docs/apache-airflow-providers-common-sql/1.19.0/index.html)|
|SQLite Connection|[apache-airflow-providers-sqlite==3.9.0](https://airflow.apache.org/docs/apache-airflow-providers-sqlite/3.9.0/index.html)|
|SMTP Connection|[apache-airflow-providers-smtp==1.8.0](https://airflow.apache.org/docs/apache-airflow-providers-smtp/1.8.0/index.html)|

### Deployment

Amazon (AWS) MWAA

### Deployment details

Nothing special, default MWAA deployment no additional configurations or requirements.

### Anything else?

Based on how zipfile works --> https://github.com/apache/airflow/blob/main/airflow/utils/file.py#L276

It appears the list of items that could be parsed extends beyond PPTX [[1]](https://en.wikipedia.org/wiki/List_of_file_signatures)

| Hex signature | ISO 8859-1 | Offset | Extension | Description |
|--|--|--|--|--|
|50 4B 03 04, 50 4B 05 06 (empty archive), 50 4B 07 08 (spanned archive)|PK␃␄,PK␅␆,PK␇␈ 	|0| 	zip, aar, apk, docx, epub, [ipa](https://en.wikipedia.org/wiki/.ipa), jar, kmz, [maff](https://en.wikipedia.org/wiki/Mozilla_Archive_Format), msix, odp, ods, odt, pk3, pk4, pptx, usdz, vsdx, xlsx, [xpi](https://en.wikipedia.org/wiki/XPInstall) |[zip file format](https://en.wikipedia.org/wiki/ZIP_(file_format)) and formats based on it, such as [EPUB](https://en.wikipedia.org/wiki/EPUB), [JAR](https://en.wikipedia.org/wiki/JAR_(file_format)), [ODF](https://en.wikipedia.org/wiki/OpenDocument), [OOXML](https://en.wikipedia.org/wiki/Office_Open_XML)


### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",url54,2025-01-16 19:13:33+00:00,[],2025-01-16 20:46:39+00:00,2025-01-16 20:46:37+00:00,https://github.com/apache/airflow/issues/45718,"[('kind:bug', 'This is a clearly a bug'), ('invalid', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('area:DAG-processing', '')]","[{'comment_id': 2596592121, 'issue_id': 2793565293, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 16, 19, 13, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2596857527, 'issue_id': 2793565293, 'author': 'potiuk', 'body': 'No. The issue is badly diagnosed. Seems like the issue is that spaces are not allowed in preparing the metrics, it has nothing to do with parsing .pptx as zipfile. Can you please create a new issue with better description of this issue, please? \n\nClosing that one as invalid. Ideally also sunmiting a fix with sanitizing the stat name could be done even without raising an issue, so you are most welcome to submit such a fix.', 'created_at': datetime.datetime(2025, 1, 16, 20, 46, 37, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-16 19:13:36 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2025-01-16 20:46:37 UTC): No. The issue is badly diagnosed. Seems like the issue is that spaces are not allowed in preparing the metrics, it has nothing to do with parsing .pptx as zipfile. Can you please create a new issue with better description of this issue, please? 

Closing that one as invalid. Ideally also sunmiting a fix with sanitizing the stat name could be done even without raising an issue, so you are most welcome to submit such a fix.

"
2793557712,issue,open,,AIP-72: Allow retrieving Assets via API Server & Task Context,"To support `outlet_events` & `inlet_events` in Context dict within the Task SDK, we will need an endpoint on the API Server which we can fetch when `outlet_events` & `inlet_events` are accessed.

https://github.com/apache/airflow/blob/a63b652f835b859a9ddb811a2b54e894bb5c4f84/airflow/models/taskinstance.py#L1024

https://github.com/apache/airflow/blob/a63b652f835b859a9ddb811a2b54e894bb5c4f84/airflow/models/taskinstance.py#L1019",kaxil,2025-01-16 19:10:13+00:00,['Lee-W'],2025-02-03 06:48:57+00:00,,https://github.com/apache/airflow/issues/45717,"[('area:API', ""Airflow's REST/HTTP API""), ('area:core', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK'), ('area:task-sdk', None)]","[{'comment_id': 2608182610, 'issue_id': 2793557712, 'author': 'kaxil', 'body': 'I have done the `OutletEventAccessors` in [this PR](https://github.com/apache/airflow/pull/45727), could either of you do `InletEventsAccessors` @Lee-W @uranusjr ?\n\nWill help spread the knowledge about Task SDK too.', 'created_at': datetime.datetime(2025, 1, 22, 20, 17, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2609375868, 'issue_id': 2793557712, 'author': 'Lee-W', 'body': 'Draft PR https://github.com/apache/airflow/pull/45960 (still super early stage)', 'created_at': datetime.datetime(2025, 1, 23, 10, 0, 39, tzinfo=datetime.timezone.utc)}]","kaxil (Issue Creator) on (2025-01-22 20:17:53 UTC): I have done the `OutletEventAccessors` in [this PR](https://github.com/apache/airflow/pull/45727), could either of you do `InletEventsAccessors` @Lee-W @uranusjr ?

Will help spread the knowledge about Task SDK too.

Lee-W (Assginee) on (2025-01-23 10:00:39 UTC): Draft PR https://github.com/apache/airflow/pull/45960 (still super early stage)

"
2790817744,issue,closed,completed,"Support the ""magic loop"" with Task SDK","Support https://airflow.apache.org/docs/apache-airflow/stable/howto/dynamic-dag-generation.html#optimizing-dag-parsing-delays-during-execution with Task SDK

https://github.com/apache/airflow/blob/e164f3cd73a6d2d8381d6bf3c5b301d83210772f/task_sdk/src/airflow/sdk/execution_time/task_runner.py#L408-L411",kaxil,2025-01-15 20:35:10+00:00,['kaxil'],2025-01-15 22:23:04+00:00,2025-01-15 22:23:04+00:00,https://github.com/apache/airflow/issues/45693,"[('kind:feature', 'Feature Requests'), ('area:dynamic-task-mapping', 'AIP-42'), ('area:DAG-processing', '')]",[],
2790609260,issue,closed,completed,"extra flag to make catchup=False mean ""first run is next scheduled""","### Description

Currently, `catchup=False` means ""first run after turning on will be the latest scheduled run"" and there is no way to produce the behavior where turning on a DAG means ""make the first run after turning on occur at the next schedule time instead of the previous one"".

Proposal: make a new flag (call it something to the effect of `no_catchup_means_no_past_runs`) which when set to `True` means that turning on a DAG that has the setting `catchup=False` will result in the first run being the one on the schedule that occurs next in time (rather than the one that occurs in the most recent past).



### Use case/motivation

The use case is whenever you want to turn on a DAG without it running right away.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",seth-olsen,2025-01-15 18:56:43+00:00,[],2025-01-18 10:20:40+00:00,2025-01-18 10:20:40+00:00,https://github.com/apache/airflow/issues/45691,"[('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2593714123, 'issue_id': 2790609260, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 15, 18, 56, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2595356010, 'issue_id': 2790609260, 'author': 'eladkal', 'body': ""Related: https://lists.apache.org/thread/j2n8h308ffq46sx0vcfnl61snh7tyjlo\nand https://github.com/apache/airflow/pull/38168, https://github.com/apache/airflow/pull/35392\n\nThe current behavior is consistent with how data pipeline works.\nToday you are processing yesterday data. Thus, if you set catchup=False and you'd like to avoid creating the first run then your `start_date` needs to be adjusted with +1 day.\n\nGenerally speaking, this behavior is not so suitable for other patterns. The backfill pain will be resolved with [AIP-78 Scheduler-managed backfill](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-78+Scheduler-managed+backfill) the rest will be handled in Airflow 3.1+"", 'created_at': datetime.datetime(2025, 1, 16, 12, 6, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2599539169, 'issue_id': 2790609260, 'author': 'seth-olsen', 'body': 'As the thread and stale/closed PRs mention, there are many cases where the workaround with start_date is undesirable or even impossible. I\'m not sure why neither of those PRs was accepted, seemed like a lot of the opposition was along the lines of ""eh, I don\'t want to have to look at another argument in the config"". Very glad I didn\'t waste time writing up a PR!', 'created_at': datetime.datetime(2025, 1, 18, 5, 21, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2599659352, 'issue_id': 2790609260, 'author': 'potiuk', 'body': '> ""eh, I don\'t want to have to look at another argument in the config""\n\nYes. that\'s a very good reason in fact. Adding more confusion and options is not desireable ""product"" property. Sometimes even at the expense not handling all cases. You can have a product with million configurable parameters that is useless and far too generic. So ""I do not want to have yet another knob to turn"" is quite a good reason for not accepting it - from product point of view, even if individual cases are not happy. Generally it\'s impossible to make everyone happy, some people will still be somewhat unhappy.\n\nThis looks like an important change in behaviour that also might impact some of the other discussions we have about Airlfow 3 - namely a lot of discussions about https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-83+amendment+to+support+classic+Airflow+authoring+style and resulting in https://cwiki.apache.org/confluence/display/AIRFLOW/Option+2+clarification+doc+WIP . While not 100% related, this PR and the backfill change mentioned by Elad are very much related to the catchup behaviour and show how you should approach such discussions.\n\nMy suggestion is @seth-olsen if you feel very strongly about this one, start a discussion on devlist and put forth your arguments. Analyse all the past dicussions that @eladkal so helpfully provided, read them in detail, anylyse why thigns were rejected - try to understand other\'s arguments (even if you do not agree with them, trying to understand what others are saying is a good idea), and come up with a concrete proposal how you think your case should be addressed and justify it. Eventually everything we do is consensus driven (i.e. we want to get to consensus where generally we agree to a direction) but if we cannot reach consensus, the last resort is voting https://www.apache.org/foundation/voting.html\n\nNote that - similarly to what Daniel did, when presenting your proposal you shoudl consider all the cases and combinations in your proposal, what it means what consequences it has when introduced, what it means for backwards compatibility etc.. Just thorough thinking followed by discussion, reaching consensus and if not possible, defining the outcome and calling for a vote. \n\nYour case is way simpler than what Daniel discussed, but the mechanism is very similar.\n\nSo I propose you start a `[DISCUSS]` thread on our devlist, where you explain rationale and refer to the past dicussions - but make sure it covers all cases and earlier arguments , let it run for a while and have people discuss it, drive it to consensus if possible and then call for a vote when you think general direction is set.', 'created_at': datetime.datetime(2025, 1, 18, 10, 19, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2599659501, 'issue_id': 2790609260, 'author': 'potiuk', 'body': 'For now - I convert it to a discussion as this certainly not an issue or a simple feature that is clear whether or if we should follow it.', 'created_at': datetime.datetime(2025, 1, 18, 10, 20, 34, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-15 18:56:47 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2025-01-16 12:06:24 UTC): Related: https://lists.apache.org/thread/j2n8h308ffq46sx0vcfnl61snh7tyjlo
and https://github.com/apache/airflow/pull/38168, https://github.com/apache/airflow/pull/35392

The current behavior is consistent with how data pipeline works.
Today you are processing yesterday data. Thus, if you set catchup=False and you'd like to avoid creating the first run then your `start_date` needs to be adjusted with +1 day.

Generally speaking, this behavior is not so suitable for other patterns. The backfill pain will be resolved with [AIP-78 Scheduler-managed backfill](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-78+Scheduler-managed+backfill) the rest will be handled in Airflow 3.1+

seth-olsen (Issue Creator) on (2025-01-18 05:21:17 UTC): As the thread and stale/closed PRs mention, there are many cases where the workaround with start_date is undesirable or even impossible. I'm not sure why neither of those PRs was accepted, seemed like a lot of the opposition was along the lines of ""eh, I don't want to have to look at another argument in the config"". Very glad I didn't waste time writing up a PR!

potiuk on (2025-01-18 10:19:57 UTC): Yes. that's a very good reason in fact. Adding more confusion and options is not desireable ""product"" property. Sometimes even at the expense not handling all cases. You can have a product with million configurable parameters that is useless and far too generic. So ""I do not want to have yet another knob to turn"" is quite a good reason for not accepting it - from product point of view, even if individual cases are not happy. Generally it's impossible to make everyone happy, some people will still be somewhat unhappy.

This looks like an important change in behaviour that also might impact some of the other discussions we have about Airlfow 3 - namely a lot of discussions about https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-83+amendment+to+support+classic+Airflow+authoring+style and resulting in https://cwiki.apache.org/confluence/display/AIRFLOW/Option+2+clarification+doc+WIP . While not 100% related, this PR and the backfill change mentioned by Elad are very much related to the catchup behaviour and show how you should approach such discussions.

My suggestion is @seth-olsen if you feel very strongly about this one, start a discussion on devlist and put forth your arguments. Analyse all the past dicussions that @eladkal so helpfully provided, read them in detail, anylyse why thigns were rejected - try to understand other's arguments (even if you do not agree with them, trying to understand what others are saying is a good idea), and come up with a concrete proposal how you think your case should be addressed and justify it. Eventually everything we do is consensus driven (i.e. we want to get to consensus where generally we agree to a direction) but if we cannot reach consensus, the last resort is voting https://www.apache.org/foundation/voting.html

Note that - similarly to what Daniel did, when presenting your proposal you shoudl consider all the cases and combinations in your proposal, what it means what consequences it has when introduced, what it means for backwards compatibility etc.. Just thorough thinking followed by discussion, reaching consensus and if not possible, defining the outcome and calling for a vote. 

Your case is way simpler than what Daniel discussed, but the mechanism is very similar.

So I propose you start a `[DISCUSS]` thread on our devlist, where you explain rationale and refer to the past dicussions - but make sure it covers all cases and earlier arguments , let it run for a while and have people discuss it, drive it to consensus if possible and then call for a vote when you think general direction is set.

potiuk on (2025-01-18 10:20:34 UTC): For now - I convert it to a discussion as this certainly not an issue or a simple feature that is clear whether or if we should follow it.

"
2790256247,issue,closed,completed,"""Unknown column 'dag.last_scheduler_run' in 'field list'"" - Is this related to some DB upgrade?","### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

My airflow UI page is down due to missing columns and I am not sure why is that so we have not upgraded airflow anythime but we did upgraded airflow DB few weeks back from mysql 5.7 to 8.

### What you think should happen instead?

_No response_

### How to reproduce

.

### Operating System

.

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",aksoni007,2025-01-15 16:10:11+00:00,['utkarsharma2'],2025-01-25 21:27:13+00:00,2025-01-25 21:27:13+00:00,https://github.com/apache/airflow/issues/45685,"[('kind:bug', 'This is a clearly a bug'), ('pending-response', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('area:db-migrations', 'PRs with DB migration'), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2593343756, 'issue_id': 2790256247, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 15, 16, 10, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2609378318, 'issue_id': 2790256247, 'author': 'utkarsharma2', 'body': '@aksoni007 Can you provider a stack trace for the issue?', 'created_at': datetime.datetime(2025, 1, 23, 10, 1, 42, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-15 16:10:14 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

utkarsharma2 (Assginee) on (2025-01-23 10:01:42 UTC): @aksoni007 Can you provider a stack trace for the issue?

"
2790192087,issue,open,,Local Executor test_execution[unlimited] seems to randomly fail,"Recently test_execution started to fail with:

```
FAILED tests/executors/test_local_executor.py::TestLocalExecutor::test_execution[unlimited] - Failed: Timeout >60.0s
```

Stacktrace:

```python
_________________ TestLocalExecutor.test_execution[unlimited] __________________
tests/executors/test_local_executor.py:129: in test_execution
    self._test_execute(parallelism=parallelism)
/usr/local/lib/python3.10/unittest/mock.py:1379: in patched
    return func(*newargs, **newkeywargs)
tests/executors/test_local_executor.py:106: in _test_execute
    executor.end()
airflow/executors/local_executor.py:234: in end
    proc.join()
/usr/local/lib/python3.10/multiprocessing/process.py:149: in join
    res = self._popen.wait(timeout)
/usr/local/lib/python3.10/multiprocessing/popen_fork.py:43: in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
/usr/local/lib/python3.10/multiprocessing/popen_fork.py:27: in poll
    pid, sts = os.waitpid(self.pid, flag)
E   Failed: Timeout >60.0s
----------------------------- Captured stdout call -----------------------------
[2025-01-15T02:12:46.747+0000] {local_executor.py:63} INFO - Worker starting up pid=91
[2025-01-15T02:12:46.761+0000] {local_executor.py:63} INFO - Worker starting up pid=92
[2025-01-15T02:12:46.769+0000] {local_executor.py:99} ERROR - uhoh
Traceback (most recent call last):
  File ""/opt/airflow/airflow/executors/local_executor.py"", line 95, in _run_worker
    _execute_work(log, workload)
  File ""/opt/airflow/airflow/executors/local_executor.py"", line 116, in _execute_work
    supervise(
  File ""/usr/local/lib/python3.10/unittest/mock.py"", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File ""/usr/local/lib/python3.10/unittest/mock.py"", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File ""/usr/local/lib/python3.10/unittest/mock.py"", line 1179, in _execute_mock_call
    result = effect(*args, **kwargs)
  File ""/opt/airflow/tests/executors/test_local_executor.py"", line 74, in fake_supervise
    raise RuntimeError(""fake failure"")
RuntimeError: fake failure
[2025-01-15T02:12:46.769+0000] {local_executor.py:219} INFO - Shutting down LocalExecutor; waiting for running tasks to finish.  Signal again if you don't want to wait.
[2025-01-15T02:12:46.778+0000] {local_executor.py:63} INFO - Worker starting up pid=93
```

Example failure: https://github.com/apache/airflow/actions/runs/12779948905/job/35625787707#step:6:7344",potiuk,2025-01-15 15:43:16+00:00,[],2025-01-17 18:44:31+00:00,,https://github.com/apache/airflow/issues/45683,"[('area:CI', ""Airflow's tests and continious integration""), ('area:Executors-core', 'LocalExecutor & SequentialExecutor')]","[{'comment_id': 2598954399, 'issue_id': 2790192087, 'author': 'ajitg25', 'body': '@potiuk Tested this locally, looks like its working now\n\nRan `breeze testing core-tests --run-in-parallel --run-db-tests-only`\n\n![Image](https://github.com/user-attachments/assets/87cb8f6d-9348-46ad-8e26-d66b9e499030)', 'created_at': datetime.datetime(2025, 1, 17, 18, 26, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2598987675, 'issue_id': 2790192087, 'author': 'potiuk', 'body': 'Yeah. It works ""in general"" - just fails randomly (flaky test)', 'created_at': datetime.datetime(2025, 1, 17, 18, 44, 30, tzinfo=datetime.timezone.utc)}]","ajitg25 on (2025-01-17 18:26:28 UTC): @potiuk Tested this locally, looks like its working now

Ran `breeze testing core-tests --run-in-parallel --run-db-tests-only`

![Image](https://github.com/user-attachments/assets/87cb8f6d-9348-46ad-8e26-d66b9e499030)

potiuk (Issue Creator) on (2025-01-17 18:44:30 UTC): Yeah. It works ""in general"" - just fails randomly (flaky test)

"
2788804380,issue,closed,completed,Update existing significant newsfragments with the later introduced template format,"### What do you see as an issue?

Since https://github.com/apache/airflow/pull/44378/, we have implemented a newsfragments template to track the breaking changes we have made. However, this template was introduced after many of our earlier breaking changes, which means that most of the existing news fragments do not conform to this format. By updating the existing entries to match the template, we will be able to create a script that summarizes the breaking changes made in Airflow 3.0.

### Solving the problem

This will need everyone's help on updating the existing newsfragments with the latest template. 

### Anything else

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2025-01-15 04:23:16+00:00,['Lee-W'],2025-01-24 03:19:58+00:00,2025-01-24 03:19:58+00:00,https://github.com/apache/airflow/issues/45673,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('kind:documentation', ''), ('airflow3.0:candidate', 'Potential candidates for Airflow 3.0')]",[],
2788704094,issue,closed,completed,Error on Inserting Session Data: psycopg2.errors.NumericValueOutOfRange: integer out of range,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

airflow-web failed with error logs as follow：

[SQL: INSERT INTO session (session_id, data, expiry) VALUES (%(session_id)s, %(data)s, %(expiry)s) RETURNING session.id]
sqlalchemy.exc.DataError: (psycopg2.errors.NumericValueOutOfRange) integer out of range



airflow_2=> SELECT MAX(id) FROM session;
    max     
------------
 2147483647
(1 row)


airflow_2=> \d session
Table ""public.session""
   Column   |            Type             |                      Modifiers                       
------------+-----------------------------+------------------------------------------------------
 id         | integer                     | not null default nextval('session_id_seq'::regclass)
 session_id | character varying(255)      | 
 data       | bytea                       | 
 expiry     | timestamp without time zone | 
Indexes:
    ""session_pkey"" PRIMARY KEY, btree (id)
    ""session_session_id_key"" UNIQUE CONSTRAINT, btree (session_id)



airflow_2=> SELECT MAX(id) FROM session;
    max     
------------
 2147483647
(1 row)






### What you think should happen instead?

_No response_

### How to reproduce

running airflow for a long time with a lot of tasks

### Operating System

centos

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### Anything else?

[SQL: INSERT INTO session (session_id, data, expiry) VALUES (%(session_id)s, %(data)s, %(expiry)s) RETURNING session.id]
[parameters: {'session_id': 'b8a8eb6a-2f7a-496f-b409-1e2b43a06eb2', 'data': <psycopg2.extensions.Binary object at 0x7f153e16a100>, 'expiry': datetime.datetime(2025, 2, 14, 2, 19, 47, 825437, tzinfo=datetime.timezone.utc)}]
(Background on this error at: https://sqlalche.me/e/14/9h9h)
[2025-01-15T10:19:47.826+0800] {app.py:1744} ERROR - Exception on / [HEAD]
Traceback (most recent call last):
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context
    self.dialect.do_execute(
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py"", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.NumericValueOutOfRange: integer out of range


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/flask/app.py"", line 2529, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/flask/app.py"", line 1826, in full_dispatch_request
    return self.finalize_request(rv)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/flask/app.py"", line 1847, in finalize_request
    response = self.process_response(response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/flask/app.py"", line 2344, in process_response
    self.session_interface.save_session(self, ctx.session, response)
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/airflow/www/session.py"", line 33, in save_session
    return super().save_session(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/flask_session/sessions.py"", line 568, in save_session
    self.db.session.commit()
  File ""<string>"", line 2, in commit
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/orm/session.py"", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/orm/session.py"", line 832, in commit
    self._prepare_impl()
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/orm/session.py"", line 811, in _prepare_impl
    self.session.flush()
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/orm/session.py"", line 3449, in flush
    self._flush(objects)
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/orm/session.py"", line 3588, in _flush
    with util.safe_reraise():
         ^^^^^^^^^^^^^^^^^^^
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/util/compat.py"", line 211, in raise_
    raise exception
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/orm/session.py"", line 3549, in _flush
    flush_context.execute()
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py"", line 456, in execute
    rec.execute(self)
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py"", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py"", line 245, in save_obj
    _emit_insert_statements(
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py"", line 1238, in _emit_insert_statements
    result = connection._execute_20(
             ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/sql/elements.py"", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 2134, in _handle_dbapi_exception
    util.raise_(
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/util/compat.py"", line 211, in raise_
    raise exception
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context
    self.dialect.do_execute(
  File ""/opt/bitnami/airflow/venv/lib/python3.12/site-packages/sqlalchemy/engine/default.py"", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.DataError: (psycopg2.errors.NumericValueOutOfRange) integer out of range

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",claude89757,2025-01-15 02:37:01+00:00,[],2025-01-18 18:51:40+00:00,2025-01-18 18:51:40+00:00,https://github.com/apache/airflow/issues/45671,"[('kind:bug', 'This is a clearly a bug'), ('area:MetaDB', 'Meta Database related issues.'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2591574085, 'issue_id': 2788704094, 'author': 'claude89757', 'body': '### Fix for `airflow-web` Issue\n\n1. **Check the Maximum ID**:\n   - Run the query:  \n     `SELECT MAX(id) FROM session;`  \n     Result: `2147483647`\n\n2. **Delete Recent Entries**:\n   - Execute the query:  \n     `DELETE FROM session WHERE id IN (SELECT id FROM session ORDER BY id DESC LIMIT 1000000);`\n\n3. **Verify the New Maximum ID**:\n   - Run the query:  \n     `SELECT MAX(id) FROM session;`  \n     Result: `2146483647`\n\nThis procedure temporarily resolves the issue with `airflow-web` by clearing a large number of session records.', 'created_at': datetime.datetime(2025, 1, 15, 3, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2591575010, 'issue_id': 2788704094, 'author': 'claude89757', 'body': 'The final optimization plan requires optimizing the SQL command used to initialize Airflow.', 'created_at': datetime.datetime(2025, 1, 15, 3, 38, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2599838661, 'issue_id': 2788704094, 'author': 'potiuk', 'body': 'I think you have something seriously, serously wrong with your setup: having `2147483647` sessions means more than 2 billion(!) sessions. .  \n\n\nThis is by no means possible to be generated by human traffic - this is likely something that is generated by some badly designed API calls  - where each API call is logging in, running (or even not) a call and logging out immediately. Which means that you never reuse sessions to run multiple API calls (which I would expect you shoudl do if you want to use APIS).\n\nJust to put it in perspective. There are 31 Million seconds in a year. So if you start running 10 API requessts /s with Airlfow doing login/logout you would need more than 6 years of continuous 10/req/s calls 24hrs/day 7 days a week to get to 2 billion sessions.\n\nI think you should review what you are doing with your API calls, because you are doing something seriously wrong.', 'created_at': datetime.datetime(2025, 1, 18, 18, 50, 11, tzinfo=datetime.timezone.utc)}]","claude89757 (Issue Creator) on (2025-01-15 03:37:00 UTC): ### Fix for `airflow-web` Issue

1. **Check the Maximum ID**:
   - Run the query:  
     `SELECT MAX(id) FROM session;`  
     Result: `2147483647`

2. **Delete Recent Entries**:
   - Execute the query:  
     `DELETE FROM session WHERE id IN (SELECT id FROM session ORDER BY id DESC LIMIT 1000000);`

3. **Verify the New Maximum ID**:
   - Run the query:  
     `SELECT MAX(id) FROM session;`  
     Result: `2146483647`

This procedure temporarily resolves the issue with `airflow-web` by clearing a large number of session records.

claude89757 (Issue Creator) on (2025-01-15 03:38:04 UTC): The final optimization plan requires optimizing the SQL command used to initialize Airflow.

potiuk on (2025-01-18 18:50:11 UTC): I think you have something seriously, serously wrong with your setup: having `2147483647` sessions means more than 2 billion(!) sessions. .  


This is by no means possible to be generated by human traffic - this is likely something that is generated by some badly designed API calls  - where each API call is logging in, running (or even not) a call and logging out immediately. Which means that you never reuse sessions to run multiple API calls (which I would expect you shoudl do if you want to use APIS).

Just to put it in perspective. There are 31 Million seconds in a year. So if you start running 10 API requessts /s with Airlfow doing login/logout you would need more than 6 years of continuous 10/req/s calls 24hrs/day 7 days a week to get to 2 billion sessions.

I think you should review what you are doing with your API calls, because you are doing something seriously wrong.

"
2788606691,issue,open,,AIP-81 Transition of Version Command,"### Description

_No response_

### Use case/motivation

#45661

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2025-01-15 01:16:35+00:00,['Prab-27'],2025-01-15 07:35:56+00:00,,https://github.com/apache/airflow/issues/45670,"[('area:CLI', ''), ('kind:feature', 'Feature Requests'), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]","[{'comment_id': 2591541930, 'issue_id': 2788606691, 'author': 'Prab-27', 'body': ""@bugraoz93 , I 'd like to work on this issue"", 'created_at': datetime.datetime(2025, 1, 15, 3, 3, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2591835805, 'issue_id': 2788606691, 'author': 'bugraoz93', 'body': 'Thanks @Prab-27 !! Assigned', 'created_at': datetime.datetime(2025, 1, 15, 7, 35, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2591836694, 'issue_id': 2788606691, 'author': 'bugraoz93', 'body': 'There is dependency at the moment you can see it from the parent issue', 'created_at': datetime.datetime(2025, 1, 15, 7, 35, 55, tzinfo=datetime.timezone.utc)}]","Prab-27 (Assginee) on (2025-01-15 03:03:26 UTC): @bugraoz93 , I 'd like to work on this issue

bugraoz93 (Issue Creator) on (2025-01-15 07:35:21 UTC): Thanks @Prab-27 !! Assigned

bugraoz93 (Issue Creator) on (2025-01-15 07:35:55 UTC): There is dependency at the moment you can see it from the parent issue

"
2788606512,issue,open,,AIP-81 Transition of Variable Command,"### Description

_No response_

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2025-01-15 01:16:23+00:00,['jason810496'],2025-01-15 07:36:47+00:00,,https://github.com/apache/airflow/issues/45669,"[('area:CLI', ''), ('kind:feature', 'Feature Requests'), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]","[{'comment_id': 2591623644, 'issue_id': 2788606512, 'author': 'jason810496', 'body': 'Hi @bugraoz93 I can work on this issue, could you assign to me ? Thanks !', 'created_at': datetime.datetime(2025, 1, 15, 4, 27, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2591837853, 'issue_id': 2788606512, 'author': 'bugraoz93', 'body': 'Hey @jason810496 , sure, thanks! There is dependency at the moment, you can see it from the parent issue. It is the central communication PR', 'created_at': datetime.datetime(2025, 1, 15, 7, 36, 41, tzinfo=datetime.timezone.utc)}]","jason810496 (Assginee) on (2025-01-15 04:27:02 UTC): Hi @bugraoz93 I can work on this issue, could you assign to me ? Thanks !

bugraoz93 (Issue Creator) on (2025-01-15 07:36:41 UTC): Hey @jason810496 , sure, thanks! There is dependency at the moment, you can see it from the parent issue. It is the central communication PR

"
2788606349,issue,open,,AIP-81 Transition of Provider Command,"### Description

_No response_

### Use case/motivation

#45661

### Related issues

_No response_

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2025-01-15 01:16:12+00:00,['bugraoz93'],2025-01-23 21:30:42+00:00,,https://github.com/apache/airflow/issues/45668,"[('area:CLI', ''), ('kind:feature', 'Feature Requests'), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]",[],
2788606152,issue,open,,AIP-81 Transition of Pools Command,"### Description

_No response_

### Use case/motivation

#45661

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2025-01-15 01:15:58+00:00,['jason810496'],2025-01-15 07:37:05+00:00,,https://github.com/apache/airflow/issues/45667,"[('area:CLI', ''), ('kind:feature', 'Feature Requests'), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]","[{'comment_id': 2591623756, 'issue_id': 2788606152, 'author': 'jason810496', 'body': 'Hi @bugraoz93 I can work on this issue, could you assign to me ? Thanks !', 'created_at': datetime.datetime(2025, 1, 15, 4, 27, 9, tzinfo=datetime.timezone.utc)}]","jason810496 (Assginee) on (2025-01-15 04:27:09 UTC): Hi @bugraoz93 I can work on this issue, could you assign to me ? Thanks !

"
2788606006,issue,open,,AIP-81 Transition of Jobs Command,"### Description

_No response_

### Use case/motivation

#45661

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2025-01-15 01:15:48+00:00,['Prab-27'],2025-01-15 08:10:52+00:00,,https://github.com/apache/airflow/issues/45666,"[('area:CLI', ''), ('kind:feature', 'Feature Requests'), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]","[{'comment_id': 2591887415, 'issue_id': 2788606006, 'author': 'Prab-27', 'body': ""@bugraoz93 , I'd like to work on this issue"", 'created_at': datetime.datetime(2025, 1, 15, 8, 6, 56, tzinfo=datetime.timezone.utc)}]","Prab-27 (Assginee) on (2025-01-15 08:06:56 UTC): @bugraoz93 , I'd like to work on this issue

"
2788605749,issue,open,,AIP-81 Transition of DAG Command,"### Description

_No response_

### Use case/motivation

#45661

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2025-01-15 01:15:30+00:00,['jx2lee'],2025-01-15 16:12:21+00:00,,https://github.com/apache/airflow/issues/45665,"[('area:CLI', ''), ('kind:feature', 'Feature Requests'), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]","[{'comment_id': 2592441234, 'issue_id': 2788605749, 'author': 'jx2lee', 'body': '@bugraoz93 Hi, Can I take this issue? Please assign it to me 🙏🏽', 'created_at': datetime.datetime(2025, 1, 15, 11, 23, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2593348589, 'issue_id': 2788605749, 'author': 'bugraoz93', 'body': 'Hey @jx2lee , for sure, assigning now. Please check the dependency on the parent issue. Thanks!', 'created_at': datetime.datetime(2025, 1, 15, 16, 12, 15, tzinfo=datetime.timezone.utc)}]","jx2lee (Assginee) on (2025-01-15 11:23:43 UTC): @bugraoz93 Hi, Can I take this issue? Please assign it to me 🙏🏽

bugraoz93 (Issue Creator) on (2025-01-15 16:12:15 UTC): Hey @jx2lee , for sure, assigning now. Please check the dependency on the parent issue. Thanks!

"
2788605480,issue,open,,AIP-81 Transition of Config Command,"### Description

_No response_

### Use case/motivation

#45661

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2025-01-15 01:15:11+00:00,['jason810496'],2025-01-15 07:37:18+00:00,,https://github.com/apache/airflow/issues/45664,"[('area:CLI', ''), ('kind:feature', 'Feature Requests'), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]","[{'comment_id': 2591623830, 'issue_id': 2788605480, 'author': 'jason810496', 'body': 'Hi @bugraoz93 I can work on this issue, could you assign to me ? Thanks !', 'created_at': datetime.datetime(2025, 1, 15, 4, 27, 14, tzinfo=datetime.timezone.utc)}]","jason810496 (Assginee) on (2025-01-15 04:27:14 UTC): Hi @bugraoz93 I can work on this issue, could you assign to me ? Thanks !

"
2788605069,issue,open,,AIP-81 Transition of Backfill Command,"### Description

_No response_

### Use case/motivation

#45661

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2025-01-15 01:14:42+00:00,['jx2lee'],2025-01-15 16:12:34+00:00,,https://github.com/apache/airflow/issues/45663,"[('area:CLI', ''), ('kind:feature', 'Feature Requests'), ('area:backfill', 'Specifically for backfill related'), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]","[{'comment_id': 2592461654, 'issue_id': 2788605069, 'author': 'jx2lee', 'body': '@bugraoz93 I would like to work on this as well!', 'created_at': datetime.datetime(2025, 1, 15, 11, 33, 53, tzinfo=datetime.timezone.utc)}]","jx2lee (Assginee) on (2025-01-15 11:33:53 UTC): @bugraoz93 I would like to work on this as well!

"
2788604690,issue,open,,AIP-81 Transition of Assets Command,"### Description

_No response_

### Use case/motivation

_No response_

### Related issues

#45661

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2025-01-15 01:14:16+00:00,['josix'],2025-01-15 16:14:11+00:00,,https://github.com/apache/airflow/issues/45662,"[('area:CLI', ''), ('kind:feature', 'Feature Requests'), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]","[{'comment_id': 2592468352, 'issue_id': 2788604690, 'author': 'josix', 'body': 'Hello @bugraoz93, may i take this one, thanks!', 'created_at': datetime.datetime(2025, 1, 15, 11, 36, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2593353357, 'issue_id': 2788604690, 'author': 'bugraoz93', 'body': 'Hello @josix , for sure. Thanks! Also thanks again for your previous contrbutions on this AIP!', 'created_at': datetime.datetime(2025, 1, 15, 16, 14, 6, tzinfo=datetime.timezone.utc)}]","josix (Assginee) on (2025-01-15 11:36:34 UTC): Hello @bugraoz93, may i take this one, thanks!

bugraoz93 (Issue Creator) on (2025-01-15 16:14:06 UTC): Hello @josix , for sure. Thanks! Also thanks again for your previous contrbutions on this AIP!

"
2788603101,issue,open,,AIP-81 Trasition of Remote Commands,"### Description

Transition of Remote Commands, below are remote commands. We should integrate CLI API client and call API endpoints to eliminate sessions and direct database calls from remote commands.

- asset
- backfill
- config
- connection
- dag
- jobs
- pool
- provider
- task (This will be decided after discussion, that's why there is no sub-issue for it, it will be updated)
- variable
- version

### Dependency
#45300 should be merged.

### Use case/motivation

[AIP-81](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-81+Enhanced+Security+in+CLI+via+Integration+of+API)

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2025-01-15 01:12:37+00:00,[],2025-01-15 07:34:25+00:00,,https://github.com/apache/airflow/issues/45661,"[('area:CLI', ''), ('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]",[],
2787908501,issue,closed,not_planned,EmrCreateJobFlowOperator returns early when `wait_for_completion=True`,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

The EmrCreateJobFlowOperator returns early when `wait_for_completion=True` due improperly treating `Running` as `success` in the acceptor. When a step is running that doesn't necessarily mean the job is complete, only waiting for new jobs or terminated should the cluster be considered idle and the job completed. 

This was introduced in https://github.com/apache/airflow/pull/37667, https://github.com/apache/airflow/pull/37667/files#diff-f4ade57b13bf58042c4171d225b9b28c7610072f8b33b722bb138db900feb7fcR65-R70


I added some printf debugging to find this issue

```
[2025-01-14, 17:22:34 UTC] {emr.py:706} INFO - Job flow with id j-YEXH6CUHAEDF created
[2025-01-14, 17:22:34 UTC] {emr.py:737} INFO - job_flow_waiting
[2025-01-14, 17:22:34 UTC] {waiter.py:394} INFO - here: wait
[2025-01-14, 17:23:34 UTC] {waiter.py:394} INFO - here: wait
[2025-01-14, 17:24:35 UTC] {waiter.py:394} INFO - here: wait
[2025-01-14, 17:25:35 UTC] {waiter.py:369} INFO - here: current_state=success
```

![Image](https://github.com/user-attachments/assets/2a4c0698-0d93-41c6-8eb9-730921afd307)

### What you think should happen instead?

The EmrCreateJobFlowOperator should continue to wait_for_completion while there are steps still running in the EMR cluster

### How to reproduce

Use the EmrCreateJobFlowOperator to create a cluster with one or more steps. Optionally set `""KeepJobFlowAliveWhenNoSteps"": False,`

### Operating System

Amazon Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### Anything else?

Every time

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",enewnham,2025-01-14 17:46:02+00:00,[],2025-01-14 18:49:29+00:00,2025-01-14 18:49:29+00:00,https://github.com/apache/airflow/issues/45651,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2590702535, 'issue_id': 2787908501, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 14, 17, 46, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2590860058, 'issue_id': 2787908501, 'author': 'enewnham', 'body': 'Just realizing we relied on an undefined behavior of `EmrCreateJobFlowOperator` in that before it would wait for the cluster to terminate when `KeepJobFlowAliveWhenNoSteps=False`. The new behavior is that is no longer the case. \n\nFor others who may of stubbed their toe, we wrapped the operator with our own operator and added the following code.\n\n```\n    def wait_for_terminate(self, job_flow_id: str):\n        # We need to wait for the job to complete before we can get the status of the group by step\n        # Since KeepJobFlowAliveWhenNoSteps is set to False, the cluster will terminate after the job is done\n        if self.deferrable:\n            self.defer(\n                trigger=EmrTerminateJobFlowTrigger(\n                    job_flow_id=job_flow_id,\n                    waiter_delay=self.waiter_delay,\n                    waiter_max_attempts=self.waiter_max_attempts,\n                    aws_conn_id=self.aws_conn_id,\n                ),\n                method_name=""execute_complete"",\n                # timeout is set to ensure that if a trigger dies, the timeout does not restart\n                # 60 seconds is added to allow the trigger to exit gracefully (i.e. yield TriggerEvent)\n                timeout=timedelta(seconds=self.waiter_max_attempts * self.waiter_delay + 60),\n            )\n        else:\n            super()._emr_hook.get_waiter(""job_flow_terminated"").wait(\n                ClusterId=self._job_flow_id,\n                WaiterConfig=prune_dict(\n                    {\n                        ""Delay"": self.waiter_delay,\n                        ""MaxAttempts"": self.waiter_max_attempts,\n                    }\n                ),\n            )\n```', 'created_at': datetime.datetime(2025, 1, 14, 18, 49, 9, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-14 17:46:05 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

enewnham (Issue Creator) on (2025-01-14 18:49:09 UTC): Just realizing we relied on an undefined behavior of `EmrCreateJobFlowOperator` in that before it would wait for the cluster to terminate when `KeepJobFlowAliveWhenNoSteps=False`. The new behavior is that is no longer the case. 

For others who may of stubbed their toe, we wrapped the operator with our own operator and added the following code.

```
    def wait_for_terminate(self, job_flow_id: str):
        # We need to wait for the job to complete before we can get the status of the group by step
        # Since KeepJobFlowAliveWhenNoSteps is set to False, the cluster will terminate after the job is done
        if self.deferrable:
            self.defer(
                trigger=EmrTerminateJobFlowTrigger(
                    job_flow_id=job_flow_id,
                    waiter_delay=self.waiter_delay,
                    waiter_max_attempts=self.waiter_max_attempts,
                    aws_conn_id=self.aws_conn_id,
                ),
                method_name=""execute_complete"",
                # timeout is set to ensure that if a trigger dies, the timeout does not restart
                # 60 seconds is added to allow the trigger to exit gracefully (i.e. yield TriggerEvent)
                timeout=timedelta(seconds=self.waiter_max_attempts * self.waiter_delay + 60),
            )
        else:
            super()._emr_hook.get_waiter(""job_flow_terminated"").wait(
                ClusterId=self._job_flow_id,
                WaiterConfig=prune_dict(
                    {
                        ""Delay"": self.waiter_delay,
                        ""MaxAttempts"": self.waiter_max_attempts,
                    }
                ),
            )
```

"
2787884540,issue,open,,Docs Reorganization for Airflow 3.0 [WIP],"## Epic 
Overhaul the Airflow documentation structure and content for the 3.0 release to improve usability, clarity, and consistency.

## Background
We're reorganizing and updating the docs to:
- Make them more intuitive for both new and experienced users.
- Separate the quickstart from installation details.
- Clearly highlight core concepts, common how-to guide and advanced usage.
- Align with a new docs style guide to ensure consistent tone, structure and formatting

**Proposed Docs Structure** (High-Level)

1. Introduction
2. Quickstart
3. Installation
4. Core Concepts & Basic Usage
5. Common How-to Guides
6. Advanced Usage & Best Practices
7. Deployment & Administration
8. Reference
9. Tutorials & Examples
10. Community & Contributing

## Goals

1. **Restructure** existing content into the new hierarchy.
2. **Rewrite** or **streamline** key pages (Quickstart, Installation, Core Concepts) for clarity.
3. **Document** any new or changed features specific to Airflow 3.0.
4. **Apply** consistent style/formatting using the agreed-upon style guide.
5. **Provide** placeholders or notes where full rewrites aren't feasible before launch.

## Scope & Deliverables

- **Create new folders / ToC entries** reflecting the new structure.
- **Migrate old content** from the current documentation site into the new structure.
- **Revise** top-priority pages (Quickstart, Installation, Core Concepts) to align with 3.0.
- **Mark** outdated or deprecated content for removal or archival.
- **Review & update** references to deprecated features.
- **Incorporate style guide** best practices, including consistent tone, headings and formatting.
- **Add placeholders** for topics we can't fully document right now (with an invitation for community help!)

## Style Guide

A style guide will be created to maintain consistency across the docs. 

High-level draft guidelines:
- **Tone:** Friendly yet professional.
- **Structure:** Short paragraphs, active voice, bullet lists where possible.
- **Code & Examples:** Realistic examples, tested snippets.
- **Links & References:** Descriptive link text.
- **Terminology:** Use official Airflow terms and versions correctly. 

## Tasks & Checklist
Below is a suggested breakdown of tasks. Each item can be further broken down or moved into individual issues that reference this epic. 

**1. Planning & Setup**
  - [ ] Finalize the new docs outline
  - [ ] Finalize the style guide
  - [ ] Validate local build environment for docs

**2. Audit Existing Pages**
  - [ ] Create a master spreadsheet or confluence page with a list of all current doc pages (title, URL/file path)
  - [ ] Tag each page with ""Keep"", ""Rewrite"", ""Merge"" or ""Remove""
  - [ ] Identify new or changed features in Airflow 3.0 that need coverage

**3. Create New File Structure**
  - [ ] Create folders/sections for each high-level heading (Introduction, Quickstart, etc.)
  - [ ] Move existing docs into corresponding sections with minimal edits
  - [ ] Update cross-links where possible

**4. Rewrite High-Priority Sections**
  - [ ] Quickstart: Provide a simple, minimal path to bootstrap an Airflow project and get Airflow running with a first DAG
  - [ ] Installation: Separate from Quickstart, describe multiple install methods
  - [ ] Core Concepts: Clarify DAGs, tasks, operators, assets, scheduling, and any 3.0 changes

**5. Apply Style Guide & Clean Up**
  - [ ] Ensure headings, bullet points, and code formatting adhere to the style guide
  - [ ] Add warnings/notes for deprecated features
  - [ ] Remove or consolidate redundant explanations

**6. Placeholder & Advanced Topics**
  - [ ] Create stubs for Advanced Usage, Best Practices, Deployment Administration, etc.
  - [ ] Invite community contributions for in-depth guides or operator-specific docs

**7. Final Review & Launch**
  - [ ] Test-build the docs in a staging environment
  - [ ] Solicit feedback from the community and QA testers
  - [ ] Fix broken links, remove ""TODO"" placeholders
  - [ ] Merge & publish as the 3.0 documentation set

## Contributing
Interested in helping? Comment below or create a pull request referencing this epic. We welcome contributions like:
- Revising a specific doc page
- Adding examples or tutorials
- Fixing typos or broken links
- Providing diagrams or visuals

We want to make Airflow 3.0 docs as user-friendly as possible, and community input is invaluable! 

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",cmarteepants,2025-01-14 17:38:08+00:00,['cmarteepants'],2025-01-14 18:11:25+00:00,,https://github.com/apache/airflow/issues/45649,"[('kind:documentation', ''), ('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2590762974, 'issue_id': 2787884540, 'author': 'potiuk', 'body': ""Nice! It's great to think of the UX for the docs of ours :) We often miss it."", 'created_at': datetime.datetime(2025, 1, 14, 18, 11, 24, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-14 18:11:24 UTC): Nice! It's great to think of the UX for the docs of ours :) We often miss it.

"
2787791706,issue,open,,Remove `subdir` arg support from cli commands,"### Body

A number of cli commands still support the `subdir` arg, but this should be removed in favor of support for `bundle` instead.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2025-01-14 17:05:38+00:00,['ambika-garg'],2025-01-16 03:47:34+00:00,,https://github.com/apache/airflow/issues/45647,"[('area:CLI', ''), ('kind:meta', 'High-level information important to the community'), ('AIP-66: DAG Bundle/Manifest', '')]","[{'comment_id': 2592911276, 'issue_id': 2787791706, 'author': 'ambika-garg', 'body': 'Hey @jedcunningham, I would like to take this up, can you please elaborate on what would be the requirements.', 'created_at': datetime.datetime(2025, 1, 15, 13, 50, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2594418743, 'issue_id': 2787791706, 'author': 'jedcunningham', 'body': 'Great! So the subdir arg is used in a bunch of different commands, like `dags list` for example. Every instance of that will need to be changed to filter by bundles instead. I imagine this will be a handful of PRs - just find the right balance of reviewability vs number of PRs :)\n\nIf you have further questions, please reach out.', 'created_at': datetime.datetime(2025, 1, 16, 3, 47, 29, tzinfo=datetime.timezone.utc)}]","ambika-garg (Assginee) on (2025-01-15 13:50:57 UTC): Hey @jedcunningham, I would like to take this up, can you please elaborate on what would be the requirements.

jedcunningham (Issue Creator) on (2025-01-16 03:47:29 UTC): Great! So the subdir arg is used in a bunch of different commands, like `dags list` for example. Every instance of that will need to be changed to filter by bundles instead. I imagine this will be a handful of PRs - just find the right balance of reviewability vs number of PRs :)

If you have further questions, please reach out.

"
2787789523,issue,open,,Add `bundle_name` and `latest_bundle_version` to `dags list` command,"### Body

We need to add `bundle_name` and `latest_bundle_version` to the `dags list` command

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2025-01-14 17:04:40+00:00,['ambika-garg'],2025-02-02 16:57:04+00:00,,https://github.com/apache/airflow/issues/45646,"[('area:CLI', ''), ('kind:meta', 'High-level information important to the community'), ('AIP-66: DAG Bundle/Manifest', '')]","[{'comment_id': 2594275967, 'issue_id': 2787789523, 'author': 'ambika-garg', 'body': 'Hey @jedcunningham, I can take this up. You can assign it to me.', 'created_at': datetime.datetime(2025, 1, 16, 1, 27, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2598573168, 'issue_id': 2787789523, 'author': 'ambika-garg', 'body': 'Hey @jedcunningham, I have a quick question about creating DAG Bundles. I noticed several classes under dag_processing/bundles. How can we use them to create bundles locally?', 'created_at': datetime.datetime(2025, 1, 17, 15, 9, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2598715365, 'issue_id': 2787789523, 'author': 'jedcunningham', 'body': 'You set them in [config](https://github.com/apache/airflow/blob/721fa2746d06fa778dffc6b13a723259e6b210ac/airflow/config_templates/config.yml#L2675-L2708). The default and example there should get you going, but feel free to reach back out if you get stuck!', 'created_at': datetime.datetime(2025, 1, 17, 16, 20, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2599708062, 'issue_id': 2787789523, 'author': 'ambika-garg', 'body': ""I have created a draft PR (https://github.com/apache/airflow/pull/45779) that adds the bundle name as an argument to the `dags list` command. Could you please review it to ensure it's on the right track? Once confirmed, I will proceed to add the `bundle_version` argument as well."", 'created_at': datetime.datetime(2025, 1, 18, 12, 59, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2600559339, 'issue_id': 2787789523, 'author': 'jedcunningham', 'body': 'Couple things:\n\n- This issue was more intended to show the bundle name and version in the output. But adding support for the flag is what #45647 was meant for, which you also grabbed :)\n- As for the display part of this issue, we have 2 options. First would be to wait for #45665 and #45644 to be done, and make sure these are added to the output. The other, which I like better (if you want to move fast) is to switch this cli command to use `DagBag(read_dags_from_db=True)` and get those columns added. The approach of using `dag_schema` from api_connexion for the list of columns also will need to go in #45665, so feel free to ""just make it work"".', 'created_at': datetime.datetime(2025, 1, 19, 3, 11, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2601031540, 'issue_id': 2787789523, 'author': 'ambika-garg', 'body': ""Thankyou @jedcunningham for reviewing it,  I'll go with the approach enabling `DagBag(read_dags_from_db=True)` and using the added columns in the output of cli command."", 'created_at': datetime.datetime(2025, 1, 19, 21, 39, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2606282005, 'issue_id': 2787789523, 'author': 'ambika-garg', 'body': 'Hi @jedcunningham,\n\nI have a few questions:\n\n1. I was able to display the bundle name and version in the output as shown in the image. Let me know if this aligns with your expectations. You can also review the changes in the [draft PR.](https://github.com/apache/airflow/pull/45779)\n\n<img width=""714"" alt=""Image"" src=""https://github.com/user-attachments/assets/2772b1ff-ea7b-4ea0-902a-e3719dc01962"" />\n\n2. I tested this on LocalBundle, but since it doesn’t support versions, I wanted to test it on GitBundle. However, I encountered an error. I updated the config.yml with the following changes but ran into issues. Could you guide me on whether any specific configurations are required?\n\n3. As per your suggestion, I attempted to use Dagbag(read_dags_from_db=True). While exploring this approach, I tried using the collect_dags_from_db function from [dagbag](https://github.com/apache/airflow/blob/main/airflow/models/dagbag.py) file, It was unable to load any file from the db.\n\nLooking forward to your guidance!', 'created_at': datetime.datetime(2025, 1, 22, 4, 41, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2607354561, 'issue_id': 2787789523, 'author': 'ambika-garg', 'body': 'Once we are good with `list_dags` command, we can extend the output functionality to the other CLI commands as well.', 'created_at': datetime.datetime(2025, 1, 22, 14, 11, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614723538, 'issue_id': 2787789523, 'author': 'jedcunningham', 'body': '>     1. I was able to display the bundle name and version in the output as shown in the image. Let me know if this aligns with your expectations. You can also review the changes in the [draft PR.](https://github.com/apache/airflow/pull/45779)\n> \n> \n> <img alt=""Image"" width=""714"" src=""https://private-user-images.githubusercontent.com/70703123/405475075-2772b1ff-ea7b-4ea0-902a-e3719dc01962.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mzc5NDI5NDEsIm5iZiI6MTczNzk0MjY0MSwicGF0aCI6Ii83MDcwMzEyMy80MDU0NzUwNzUtMjc3MmIxZmYtZWE3Yi00ZWEwLTkwMmEtZTM3MTlkYzAxOTYyLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTAxMjclMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwMTI3VDAxNTA0MVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWRlZDFlZGQxNWQ2ZmZlNzViYWMyYzc5OGFiYWM3NTNlOTRjM2M1OWM3NDEyYTljYzg4YTZkZWUxZjZkYjIyNDAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.jrq2T1IAEo6m8pu4hXklLkcZYXEQcfoGvk7TUVdT6m0"">\n\nYep, exactly!\n\n>     2. I tested this on LocalBundle, but since it doesn’t support versions, I wanted to test it on GitBundle. However, I encountered an error. I updated the config.yml with the following changes but ran into issues. Could you guide me on whether any specific configurations are required?\n\nWithout the error message, it\'s tough to say. There has been a bit of churn in GitHook and GitDagBundle recently. If you need a working config, let me know and I\'ll craft one to share.\n\n>     3. As per your suggestion, I attempted to use Dagbag(read_dags_from_db=True). While exploring this approach, I tried using the collect_dags_from_db function from [dagbag](https://github.com/apache/airflow/blob/main/airflow/models/dagbag.py) file, It was unable to load any file from the db.\n\nHmm, in theory, you can just do:\n\n```\n>>> from airflow.models.dagbag import DagBag\n>>> bag = DagBag(read_dags_from_db=True)\n>>> bag.collect_dags_from_db()\n[2025-01-26T19:00:50.726-0700] {dagbag.py:612} INFO - Filling up the DagBag from database\n...\n>>> bag.dag_ids\n[\'hello\', \'mapping_always_short\', \'ke_image_override\']\n```\n\nDid you get any errors? Or it was just empty?', 'created_at': datetime.datetime(2025, 1, 27, 2, 2, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2617121312, 'issue_id': 2787789523, 'author': 'ambika-garg', 'body': '> Hmm, in theory, you can just do:\n> \n> ```\n> >>> from airflow.models.dagbag import DagBag\n> >>> bag = DagBag(read_dags_from_db=True)\n> >>> bag.collect_dags_from_db()\n> [2025-01-26T19:00:50.726-0700] {dagbag.py:612} INFO - Filling up the DagBag from database\n> ...\n> >>> bag.dag_ids\n> [\'hello\', \'mapping_always_short\', \'ke_image_override\']\n> ```\n> \n> Did you get any errors? Or it was just empty?\n\nHey @jedcunningham ,  I get the following error upon trying it. \n\n<img width=""951"" alt=""Image"" src=""https://github.com/user-attachments/assets/d17d782f-6b4f-40b9-b23d-cba5a2e9aebb"" />', 'created_at': datetime.datetime(2025, 1, 27, 23, 25, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2617128824, 'issue_id': 2787789523, 'author': 'ambika-garg', 'body': 'Could you also please share a working configuration for the GithubDagBundle?', 'created_at': datetime.datetime(2025, 1, 27, 23, 30, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2617885733, 'issue_id': 2787789523, 'author': 'jedcunningham', 'body': 'Sounds like your db isn\'t migrated.\n\nHere is an example. First you need to set up a connection:\n\n```\nAIRFLOW_CONN_AIRFLOW_GIT_CONN=\'{""host"": ""https://github.com/apache/airflow.git""}\'\n```\n\nThen, add this to your config:\n\n```\n[dag_processor]\ndag_bundle_config_list = [\n        {\n            ""name"": ""airflow_example_dags"",\n            ""classpath"": ""airflow.dag_processing.bundles.git.GitDagBundle"",\n            ""kwargs"": {""tracking_ref"": ""main"", ""git_conn_id"": ""airflow_git_conn"", subdir=""airflow/example_dags""}\n        }\n    ]\n```\n\nHowever, something is broken right now and the processor is getting stuck on servicing the sockets. I\'ll have to dig into this tomorrow. In the meantime, you can safely go into the db and just put a string in the `dag.bundle_version` column. That will unblock you for now.\n\n(edit: added `.git` to the host - we need that!)', 'created_at': datetime.datetime(2025, 1, 28, 5, 13, 12, tzinfo=datetime.timezone.utc)}]","ambika-garg (Assginee) on (2025-01-16 01:27:42 UTC): Hey @jedcunningham, I can take this up. You can assign it to me.

ambika-garg (Assginee) on (2025-01-17 15:09:18 UTC): Hey @jedcunningham, I have a quick question about creating DAG Bundles. I noticed several classes under dag_processing/bundles. How can we use them to create bundles locally?

jedcunningham (Issue Creator) on (2025-01-17 16:20:45 UTC): You set them in [config](https://github.com/apache/airflow/blob/721fa2746d06fa778dffc6b13a723259e6b210ac/airflow/config_templates/config.yml#L2675-L2708). The default and example there should get you going, but feel free to reach back out if you get stuck!

ambika-garg (Assginee) on (2025-01-18 12:59:17 UTC): I have created a draft PR (https://github.com/apache/airflow/pull/45779) that adds the bundle name as an argument to the `dags list` command. Could you please review it to ensure it's on the right track? Once confirmed, I will proceed to add the `bundle_version` argument as well.

jedcunningham (Issue Creator) on (2025-01-19 03:11:38 UTC): Couple things:

- This issue was more intended to show the bundle name and version in the output. But adding support for the flag is what #45647 was meant for, which you also grabbed :)
- As for the display part of this issue, we have 2 options. First would be to wait for #45665 and #45644 to be done, and make sure these are added to the output. The other, which I like better (if you want to move fast) is to switch this cli command to use `DagBag(read_dags_from_db=True)` and get those columns added. The approach of using `dag_schema` from api_connexion for the list of columns also will need to go in #45665, so feel free to ""just make it work"".

ambika-garg (Assginee) on (2025-01-19 21:39:33 UTC): Thankyou @jedcunningham for reviewing it,  I'll go with the approach enabling `DagBag(read_dags_from_db=True)` and using the added columns in the output of cli command.

ambika-garg (Assginee) on (2025-01-22 04:41:22 UTC): Hi @jedcunningham,

I have a few questions:

1. I was able to display the bundle name and version in the output as shown in the image. Let me know if this aligns with your expectations. You can also review the changes in the [draft PR.](https://github.com/apache/airflow/pull/45779)

<img width=""714"" alt=""Image"" src=""https://github.com/user-attachments/assets/2772b1ff-ea7b-4ea0-902a-e3719dc01962"" />

2. I tested this on LocalBundle, but since it doesn’t support versions, I wanted to test it on GitBundle. However, I encountered an error. I updated the config.yml with the following changes but ran into issues. Could you guide me on whether any specific configurations are required?

3. As per your suggestion, I attempted to use Dagbag(read_dags_from_db=True). While exploring this approach, I tried using the collect_dags_from_db function from [dagbag](https://github.com/apache/airflow/blob/main/airflow/models/dagbag.py) file, It was unable to load any file from the db.

Looking forward to your guidance!

ambika-garg (Assginee) on (2025-01-22 14:11:34 UTC): Once we are good with `list_dags` command, we can extend the output functionality to the other CLI commands as well.

jedcunningham (Issue Creator) on (2025-01-27 02:02:35 UTC): Yep, exactly!


Without the error message, it's tough to say. There has been a bit of churn in GitHook and GitDagBundle recently. If you need a working config, let me know and I'll craft one to share.


Hmm, in theory, you can just do:

```
[2025-01-26T19:00:50.726-0700] {dagbag.py:612} INFO - Filling up the DagBag from database
...
['hello', 'mapping_always_short', 'ke_image_override']
```

Did you get any errors? Or it was just empty?

ambika-garg (Assginee) on (2025-01-27 23:25:45 UTC): Hey @jedcunningham ,  I get the following error upon trying it. 

<img width=""951"" alt=""Image"" src=""https://github.com/user-attachments/assets/d17d782f-6b4f-40b9-b23d-cba5a2e9aebb"" />

ambika-garg (Assginee) on (2025-01-27 23:30:15 UTC): Could you also please share a working configuration for the GithubDagBundle?

jedcunningham (Issue Creator) on (2025-01-28 05:13:12 UTC): Sounds like your db isn't migrated.

Here is an example. First you need to set up a connection:

```
AIRFLOW_CONN_AIRFLOW_GIT_CONN='{""host"": ""https://github.com/apache/airflow.git""}'
```

Then, add this to your config:

```
[dag_processor]
dag_bundle_config_list = [
        {
            ""name"": ""airflow_example_dags"",
            ""classpath"": ""airflow.dag_processing.bundles.git.GitDagBundle"",
            ""kwargs"": {""tracking_ref"": ""main"", ""git_conn_id"": ""airflow_git_conn"", subdir=""airflow/example_dags""}
        }
    ]
```

However, something is broken right now and the processor is getting stuck on servicing the sockets. I'll have to dig into this tomorrow. In the meantime, you can safely go into the db and just put a string in the `dag.bundle_version` column. That will unblock you for now.

(edit: added `.git` to the host - we need that!)

"
2787785977,issue,closed,completed,Add `bundle_version` to dagrun api endpoint,"### Body

Once #44752 is merged, we need to add the `bundle_version` to the dagrun fastapi api endpoint.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2025-01-14 17:03:25+00:00,['Prab-27'],2025-02-07 15:53:04+00:00,2025-02-07 15:53:03+00:00,https://github.com/apache/airflow/issues/45645,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('kind:meta', 'High-level information important to the community'), ('AIP-66: DAG Bundle/Manifest', '')]","[{'comment_id': 2595559992, 'issue_id': 2787785977, 'author': 'Prab-27', 'body': ""@jedcunningham , I 'd like to work on this issue"", 'created_at': datetime.datetime(2025, 1, 16, 12, 57, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2643316176, 'issue_id': 2787785977, 'author': 'jedcunningham', 'body': 'Closing this in favor of #45995, which is a bit more fleshed out and more of what we need as things have evolved. Thanks for offering to help @Prab-27.', 'created_at': datetime.datetime(2025, 2, 7, 15, 53, 3, tzinfo=datetime.timezone.utc)}]","Prab-27 (Assginee) on (2025-01-16 12:57:01 UTC): @jedcunningham , I 'd like to work on this issue

jedcunningham (Issue Creator) on (2025-02-07 15:53:03 UTC): Closing this in favor of #45995, which is a bit more fleshed out and more of what we need as things have evolved. Thanks for offering to help @Prab-27.

"
2787782169,issue,open,,Add `bundle_name` and `latest_bundle_version` to DAG api endpoint,"### Body

We need to add `bundle_name` and `latest_bundle_version` to DAG fastapi api endpoint.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2025-01-14 17:01:41+00:00,['Prab-27'],2025-02-03 01:49:17+00:00,,https://github.com/apache/airflow/issues/45644,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('kind:meta', 'High-level information important to the community'), ('AIP-66: DAG Bundle/Manifest', '')]","[{'comment_id': 2594769601, 'issue_id': 2787782169, 'author': 'Prab-27', 'body': ""@jedcunningham ,I 'd like to work on this issue but I'm confused about the file path where to create these endpoint  Could you please clarify ?"", 'created_at': datetime.datetime(2025, 1, 16, 8, 3, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2596177970, 'issue_id': 2787782169, 'author': 'jedcunningham', 'body': ""The endpoints live [here](https://github.com/apache/airflow/blob/main/airflow/api_fastapi/core_api/routes/public/dags.py), and you'll need to add those fields into the datamodels [here](https://github.com/apache/airflow/blob/main/airflow/api_fastapi/core_api/datamodels/dags.py)."", 'created_at': datetime.datetime(2025, 1, 16, 16, 29, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629480666, 'issue_id': 2787782169, 'author': 'Prab-27', 'body': '@jedcunningham Could you please tell me \n how I can get `latest_bundle_version` ? \nAS you explained  **Parsing will always happen on the latest version of the DAG bundles (and the execution environment configured by the manifest), as is the case today**\n\nDo I need to create a method to get `latest_bundle_version` from manifest file ?', 'created_at': datetime.datetime(2025, 2, 2, 17, 25, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2629717427, 'issue_id': 2787782169, 'author': 'jedcunningham', 'body': ""It's [these 2 columns from the DagModel](https://github.com/apache/airflow/blob/cb4335248a0b01e5e09a60dff8a467a758511c9b/airflow/models/dag.py#L2035-L2037)."", 'created_at': datetime.datetime(2025, 2, 3, 1, 49, 16, tzinfo=datetime.timezone.utc)}]","Prab-27 (Assginee) on (2025-01-16 08:03:03 UTC): @jedcunningham ,I 'd like to work on this issue but I'm confused about the file path where to create these endpoint  Could you please clarify ?

jedcunningham (Issue Creator) on (2025-01-16 16:29:20 UTC): The endpoints live [here](https://github.com/apache/airflow/blob/main/airflow/api_fastapi/core_api/routes/public/dags.py), and you'll need to add those fields into the datamodels [here](https://github.com/apache/airflow/blob/main/airflow/api_fastapi/core_api/datamodels/dags.py).

Prab-27 (Assginee) on (2025-02-02 17:25:29 UTC): @jedcunningham Could you please tell me 
 how I can get `latest_bundle_version` ? 
AS you explained  **Parsing will always happen on the latest version of the DAG bundles (and the execution environment configured by the manifest), as is the case today**

Do I need to create a method to get `latest_bundle_version` from manifest file ?

jedcunningham (Issue Creator) on (2025-02-03 01:49:16 UTC): It's [these 2 columns from the DagModel](https://github.com/apache/airflow/blob/cb4335248a0b01e5e09a60dff8a467a758511c9b/airflow/models/dag.py#L2035-L2037).

"
2787610513,issue,open,,Optimize git bundle for ephemeral workers,"### Body

When running in KE, we could likely do a single shallow clone, instead of our ""clone to bare repo, then clone the right version"" approach we take now. This would make the worker startup process a bit cheaper.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2025-01-14 16:08:03+00:00,[],2025-01-17 16:24:01+00:00,,https://github.com/apache/airflow/issues/45643,"[('kind:meta', 'High-level information important to the community'), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('AIP-66: DAG Bundle/Manifest', '')]","[{'comment_id': 2591120945, 'issue_id': 2787610513, 'author': 'dstandish', 'body': 'perhaps we can just make available to bundles the info about what executor the TI is running with and the bundle can make its own decisions about how to behave', 'created_at': datetime.datetime(2025, 1, 14, 21, 18, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2598311758, 'issue_id': 2787610513, 'author': 'IKholopov', 'body': 'Is there any solid reason to do non-shallow clone in the first place? Even in dag-processor the required data is only of the latest bound version, it will not really need the full history of the branch.', 'created_at': datetime.datetime(2025, 1, 17, 12, 57, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2598722835, 'issue_id': 2787610513, 'author': 'jedcunningham', 'body': ""The dag processor will end up needing more than 1 version around in order to run callbacks on the right version. Thus, the full bare repo is cloned.\n\nThe clone from the bare repo could be shallow though! Good call. (fwiw, we haven't done any optimization yet 😄)"", 'created_at': datetime.datetime(2025, 1, 17, 16, 24, 1, tzinfo=datetime.timezone.utc)}]","dstandish on (2025-01-14 21:18:24 UTC): perhaps we can just make available to bundles the info about what executor the TI is running with and the bundle can make its own decisions about how to behave

IKholopov on (2025-01-17 12:57:25 UTC): Is there any solid reason to do non-shallow clone in the first place? Even in dag-processor the required data is only of the latest bound version, it will not really need the full history of the branch.

jedcunningham (Issue Creator) on (2025-01-17 16:24:01 UTC): The dag processor will end up needing more than 1 version around in order to run callbacks on the right version. Thus, the full bare repo is cloned.

The clone from the bare repo could be shallow though! Good call. (fwiw, we haven't done any optimization yet 😄)

"
2787606728,issue,closed,completed,apt repository keys no longer functioning,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

The documentation at https://airflow.apache.org/docs/docker-stack/build.html#build-build-image suggests that it should be possible to run `apt-get update` in the context of the upstream `apache/airflow:2.10.4` docker image.  However, this no longer appears to be the case:

```
$ docker run -it --entrypoint=/bin/bash --user root apache/airflow:2.10.4
root@fffa6fd0bf12:/opt/airflow# apt-get update
Get:1 http://deb.debian.org/debian bookworm InRelease [151 kB]
Get:2 http://deb.debian.org/debian bookworm-updates InRelease [55.4 kB]
Get:3 https://download.docker.com/linux/debian bookworm InRelease [43.3 kB]
Get:4 https://packages.microsoft.com/debian/12/prod bookworm InRelease [3618 B]
Get:5 http://deb.debian.org/debian-security bookworm-security InRelease [48.0 kB]
Err:1 http://deb.debian.org/debian bookworm InRelease
  At least one invalid signature was encountered.
Get:6 https://archive.mariadb.org/mariadb-10.11/repo/debian bookworm InRelease [4625 B]
Err:2 http://deb.debian.org/debian bookworm-updates InRelease
  At least one invalid signature was encountered.
Get:7 https://apt.postgresql.org/pub/repos/apt bookworm-pgdg InRelease [129 kB]
Err:3 https://download.docker.com/linux/debian bookworm InRelease
  At least one invalid signature was encountered.
Err:4 https://packages.microsoft.com/debian/12/prod bookworm InRelease
  At least one invalid signature was encountered.
Err:5 http://deb.debian.org/debian-security bookworm-security InRelease
  At least one invalid signature was encountered.
Err:6 https://archive.mariadb.org/mariadb-10.11/repo/debian bookworm InRelease
  At least one invalid signature was encountered.
Err:7 https://apt.postgresql.org/pub/repos/apt bookworm-pgdg InRelease
  At least one invalid signature was encountered.
Reading package lists... Done
W: GPG error: http://deb.debian.org/debian bookworm InRelease: At least one invalid signature was encountered.
E: The repository 'http://deb.debian.org/debian bookworm InRelease' is not signed.
N: Updating from such a repository can't be done securely, and is therefore disabled by default.
N: See apt-secure(8) manpage for repository creation and user configuration details.
W: GPG error: http://deb.debian.org/debian bookworm-updates InRelease: At least one invalid signature was encountered.
E: The repository 'http://deb.debian.org/debian bookworm-updates InRelease' is not signed.
N: Updating from such a repository can't be done securely, and is therefore disabled by default.
N: See apt-secure(8) manpage for repository creation and user configuration details.
W: GPG error: https://download.docker.com/linux/debian bookworm InRelease: At least one invalid signature was encountered.
E: The repository 'https://download.docker.com/linux/debian bookworm InRelease' is not signed.
N: Updating from such a repository can't be done securely, and is therefore disabled by default.
N: See apt-secure(8) manpage for repository creation and user configuration details.
W: GPG error: https://packages.microsoft.com/debian/12/prod bookworm InRelease: At least one invalid signature was encountered.
E: The repository 'https://packages.microsoft.com/debian/12/prod bookworm InRelease' is not signed.
N: Updating from such a repository can't be done securely, and is therefore disabled by default.
N: See apt-secure(8) manpage for repository creation and user configuration details.
W: GPG error: http://deb.debian.org/debian-security bookworm-security InRelease: At least one invalid signature was encountered.
E: The repository 'http://deb.debian.org/debian-security bookworm-security InRelease' is not signed.
N: Updating from such a repository can't be done securely, and is therefore disabled by default.
N: See apt-secure(8) manpage for repository creation and user configuration details.
W: GPG error: https://archive.mariadb.org/mariadb-10.11/repo/debian bookworm InRelease: At least one invalid signature was encountered.
E: The repository 'https://archive.mariadb.org/mariadb-10.11/repo/debian bookworm InRelease' is not signed.
N: Updating from such a repository can't be done securely, and is therefore disabled by default.
N: See apt-secure(8) manpage for repository creation and user configuration details.
W: GPG error: https://apt.postgresql.org/pub/repos/apt bookworm-pgdg InRelease: At least one invalid signature was encountered.
E: The repository 'https://apt.postgresql.org/pub/repos/apt bookworm-pgdg InRelease' is not signed.
N: Updating from such a repository can't be done securely, and is therefore disabled by default.
N: See apt-secure(8) manpage for repository creation and user configuration details.
```

### What you think should happen instead?

`apt-get update` and/or `apt update` should return successfully, having downloaded the apt server lists from all sources.

### How to reproduce

1. `docker run -it --entrypoint=/bin/bash --user root apache/airflow:2.10.4 -c 'apt-get update'`

That's really it: you'll see the ""invalid signature"" errors and a return code of 100.

### Operating System

PRETTY_NAME=""Debian GNU/Linux 12 (bookworm)"" NAME=""Debian GNU/Linux"" VERSION_ID=""12"" VERSION=""12 (bookworm)"" VERSION_CODENAME=bookworm ID=debian HOME_URL=""https://www.debian.org/"" SUPPORT_URL=""https://www.debian.org/support"" BUG_REPORT_URL=""https://bugs.debian.org/""

### Versions of Apache Airflow Providers

n/a

### Deployment

Other Docker-based deployment

### Deployment details

n/a -- this is an issue with the docker image itself

### Anything else?

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",n-oden,2025-01-14 16:06:26+00:00,[],2025-01-14 17:19:10+00:00,2025-01-14 17:19:10+00:00,https://github.com/apache/airflow/issues/45642,"[('kind:bug', 'This is a clearly a bug'), ('area:production-image', 'Production image improvements and fixes'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2787601337,issue,open,,"Run doc publishing workflow from ""airflow"" repo","Currently doc publishing is a complex process - it involves checking two repos and copying code between them, just to publish generated code from one repo to the other and then push it so that it can be published. That requires not only to have both repos checked out at the same time locally but also some linking between them and it only works from local machine.

We could get rid of all that completely if we have just one publishing workflow in ""airflow"" CI. Once we move to ""s3"" only storage and exposing as website (via #45621), there is no need to copy the files to the separate repo any more, and with #45613 where we can move sphinx airflow theme to the main reapo as well, the whole publishing framework could be done as a set of workflow publishing the docuementation - straight from ""airlfow"" repository.

",potiuk,2025-01-14 16:05:00+00:00,[],2025-01-14 16:07:15+00:00,,https://github.com/apache/airflow/issues/45641,"[('kind:documentation', ''), ('area:CI', ""Airflow's tests and continious integration"")]",[],
2787259074,issue,open,,Support `@task.bash` with Task SDK,"`@task.bash` code currently requires DB interaction as below, we need to remove that and use Task SDK somehow

https://github.com/apache/airflow/blob/f0079293456835acb68721e54d88c7de1c570a11/providers/src/airflow/providers/standard/operators/bash.py#L198-L222",kaxil,2025-01-14 14:16:31+00:00,[],2025-02-05 16:39:46+00:00,,https://github.com/apache/airflow/issues/45639,"[('area:core', ''), ('area:core-operators', 'Operators, Sensors and hooks within Core Airflow'), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2787054824,issue,open,,Tasks are starved due to incorrect scheduler prioritization,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

2.9.3

### What happened?

We have a problem with our airflow cluster.
When the scheduler extracts and filters the tasks for scheduling even though the pools are not in a state of starvation then certain tasks in certain pools get stuck in scheduled.

This happens due to the following situation:

There are two pools, each with 20 slots, on the first there are 1000 tasks ready for scheduling and each of them has a priority of 2, on the second pool there are 100 tasks ready for scheduling and each of them with a priority of 1.
The first pool all the slots are currently occupied  except for 1 and the second, all slots are free.

What happens is, the scheduler sorts the 1100 tasks that are waiting in scheduled and each time pulls 32 (we set max_ti=32), but all the first 32 that the scheduler actually pulled after filtering belong to the first pool (it is not starving because some of its tasks are short and therefore always has a few single slots free) so it turns out that the tasks of the second pool are stuck in scheduled without being able to move forward, and in the first pool he gets 32 tasks each time, but in practice he can run a few individual tasks (2/3 tasks).

The same can be said about dags with lots of tasks but with very low concurrency.

### What you think should happen instead?

Don't [filter](https://github.com/apache/airflow/blob/main/airflow%2Fjobs%2Fscheduler_job_runner.py#L360) the tasks according to the pools/dags/tasks that are **starving**, but check how many **free slots** there are in each and filter (starved pool/dag/task) the amount of tasks according to each of them, [before](https://github.com/apache/airflow/blob/main/airflow%2Fjobs%2Fscheduler_job_runner.py#L374) `query.limit(max_tis)`

### How to reproduce

Pool PA: 3 slots

Pool PB: 20 slots

Dag DA: with 1000 shorts tasks, each task with priority 2 on pool PA.

Dag DB: with 100 shorts tasks, each task with priority 1 on pool PB.

Max active tasks = 32


### Operating System

Debian GNU/Linux 12 (bookworn)

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes==8.4.2

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Python 3.8

executor: ""KubernetesExecutor""

scheduler:

- max_tis_per_query: 32
- schedule_after_task_execution: 'False'
- task_queued_timeout_chheck_interval: 600

kubernetes_executor:

- worker_pods_ceartion_batch_size: 16


### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",nevcohen,2025-01-14 12:50:46+00:00,[],2025-01-15 09:33:41+00:00,,https://github.com/apache/airflow/issues/45636,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('area:core', ''), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2589833635, 'issue_id': 2787054824, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 14, 12, 50, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2590604544, 'issue_id': 2787054824, 'author': 'potiuk', 'body': ""Indeed. looks plausible. While this is an edge case, I think yes, the logic we have for pool filtering works as described. I vaguely recall we've been discussing about similar scenario in the past. Maybe someone could take a look how to optimise it for Airflow 3.\n\n@ashb -> maybe there wrere some other ideas for this pool filtering ?"", 'created_at': datetime.datetime(2025, 1, 14, 17, 13, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2592113479, 'issue_id': 2787054824, 'author': 'nevcohen', 'body': ""Something we thought about here and maybe it could solve the problem.\n\nWe'll add the following variable below the line [here](https://github.com/apache/airflow/blob/main/airflow%2Fjobs%2Fscheduler_job_runner.py#L324):\n\n```py\nmax_tis_loop = max_tis\n```\n\nAnd [this](https://github.com/apache/airflow/blob/main/airflow%2Fjobs%2Fscheduler_job_runner.py#L342) `for` will become:\n\n```py\nfor loop_count in range(1, max_tis_loop):\n    ...\n```\n\nAnd [the](https://github.com/apache/airflow/blob/main/airflow%2Fjobs%2Fscheduler_job_runner.py#L580) `is_done` variable will also be fixed:\n\n```py\nmax_tis -= len(executable_tis)\nis_done = len(executable_tis) == max_tis_loop or len(task_instances_to_examine) < max_tis_loop\n```\n\nHope that covers all use cases..."", 'created_at': datetime.datetime(2025, 1, 15, 9, 30, 21, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-14 12:50:49 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2025-01-14 17:13:29 UTC): Indeed. looks plausible. While this is an edge case, I think yes, the logic we have for pool filtering works as described. I vaguely recall we've been discussing about similar scenario in the past. Maybe someone could take a look how to optimise it for Airflow 3.

@ashb -> maybe there wrere some other ideas for this pool filtering ?

nevcohen (Issue Creator) on (2025-01-15 09:30:21 UTC): Something we thought about here and maybe it could solve the problem.

We'll add the following variable below the line [here](https://github.com/apache/airflow/blob/main/airflow%2Fjobs%2Fscheduler_job_runner.py#L324):

```py
max_tis_loop = max_tis
```

And [this](https://github.com/apache/airflow/blob/main/airflow%2Fjobs%2Fscheduler_job_runner.py#L342) `for` will become:

```py
for loop_count in range(1, max_tis_loop):
    ...
```

And [the](https://github.com/apache/airflow/blob/main/airflow%2Fjobs%2Fscheduler_job_runner.py#L580) `is_done` variable will also be fixed:

```py
max_tis -= len(executable_tis)
is_done = len(executable_tis) == max_tis_loop or len(task_instances_to_examine) < max_tis_loop
```

Hope that covers all use cases...

"
2786596044,issue,open,,"Overflow issue with the action button when clicked in the 'Dag Run' and 'Task Instances' sections, as well as other menu items in the navbar's browse section.","### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

There is an overflow issue occurring with the action button in the user interface when it is clicked, particularly in the 'Dag Run' and 'Task Instances' sections. This issue also extends to other menu items in the navbar's browse section. When the action button is triggered, the content that is supposed to appear may exceed the designated space or container, causing the layout to break or display improperly. This results in a poor user experience, as the button and associated elements may become inaccessible, overlapping with other content, or pushing elements outside of their intended boundaries. The overflow problem may affect both the visibility and usability of the interface, especially when the action button is used frequently or in a dynamic context where the content size fluctuates. Additionally, this issue may not be isolated to the aforementioned sections but could affect other areas of the application where similar buttons or menus are implemented, leading to inconsistent behavior throughout the site or application. Resolving this overflow issue is critical to ensure a smooth and visually coherent user experience across all areas of the interface.

### What you think should happen instead?

When the dropdown is clicked, it should appear above the list section (such as ""List Dag Run"" or others) to prevent any display issues. Currently, only two options are visible at a time, but for a better user experience, the dropdown should display at least six options at once.

### How to reproduce


This problem occurs due to the incorrect use of the CSS overflow property.

### Operating System

mac-os, codespace 

### Versions of Apache Airflow Providers

pip freeze | grep apache-airflow-provider

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",aditya0yadav,2025-01-14 08:57:12+00:00,[],2025-01-14 16:04:16+00:00,,https://github.com/apache/airflow/issues/45633,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2589356992, 'issue_id': 2786596044, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 14, 8, 57, 14, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-14 08:57:14 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2786060024,issue,open,,"Backport ""airflow config lint"" to airflow 2.11","### Description

Backport comment ""airflow config lint"" to airflow 2

https://github.com/apache/airflow/issues/41641#issuecomment-2576860156

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2025-01-14 02:36:31+00:00,['Lee-W'],2025-01-14 02:36:31+00:00,,https://github.com/apache/airflow/issues/45632,"[('area:dev-tools', ''), ('kind:feature', 'Feature Requests')]",[],
2785479267,issue,closed,completed,POC s3 redirects for airflow documentation,"### Body

_No response_

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",gopidesupavan,2025-01-13 21:33:49+00:00,['gopidesupavan'],2025-01-20 18:31:20+00:00,2025-01-20 18:04:14+00:00,https://github.com/apache/airflow/issues/45628,"[('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2603008082, 'issue_id': 2785479267, 'author': 'gopidesupavan', 'body': 'Tested with proxying s3 static bucket.', 'created_at': datetime.datetime(2025, 1, 20, 18, 4, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2603044336, 'issue_id': 2785479267, 'author': 'potiuk', 'body': 'Cool', 'created_at': datetime.datetime(2025, 1, 20, 18, 31, 18, tzinfo=datetime.timezone.utc)}]","gopidesupavan (Issue Creator) on (2025-01-20 18:04:14 UTC): Tested with proxying s3 static bucket.

potiuk on (2025-01-20 18:31:18 UTC): Cool

"
2785390204,issue,closed,completed,PagerDuty PagerdutyNotifier Class Uses a Removed Method and Fails to Send Notifications,"### Apache Airflow Provider(s)

pagerduty

### Versions of Apache Airflow Providers

apache-airflow-providers-pagerduty **4.0**

### Apache Airflow version

2.10.4

### Operating System

Ubuntu

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### What happened

PagerDuty alerts sent via `PagerdutyNotifier` are not sent. Looking at the source code, it looks like the `notify` method is using the `create_event` method which no longer exists.


```
    def notify(self, context):
        """"""Send a alert to a pagerduty event v2 API.""""""
        self.hook.**create_event**(
            summary=self.summary,
            severity=self.severity,
            source=self.source,
            action=self.action,
            dedup_key=self.dedup_key,
            custom_details=self.custom_details,
            group=self.group,
            component=self.component,
            class_type=self.class_type,
            images=self.images,
            links=self.links,
        )
```

### What you think should happen instead

As noted in the most recent change log, https://airflow.apache.org/docs/apache-airflow-providers-pagerduty/stable/changelog.html, switch the `create_event` method to `send_event`.

### How to reproduce

Trigger an alert with `PagerdutyNotifier` 

### Anything else

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bshea5,2025-01-13 20:59:01+00:00,[],2025-01-15 18:56:31+00:00,2025-01-15 18:56:31+00:00,https://github.com/apache/airflow/issues/45626,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:pagerduty', '')]","[{'comment_id': 2588186241, 'issue_id': 2785390204, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 13, 20, 59, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2588510273, 'issue_id': 2785390204, 'author': 'bshea5', 'body': 'Submitted a PR, https://github.com/apache/airflow/pull/45630', 'created_at': datetime.datetime(2025, 1, 14, 0, 40, 44, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-13 20:59:04 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

bshea5 (Issue Creator) on (2025-01-14 00:40:44 UTC): Submitted a PR, https://github.com/apache/airflow/pull/45630

"
2784721198,issue,open,,Remove relative_fileloc,"With the introduction of dag bundles, there's no longer a need for the fileloc / relative fileloc disctinction.  _All_ dags are defined through bundles, so we can always just store the path relative to the bundle root.

Note that even if a bundle stores dags in a subfolder, the fileloc on the dag object should always be relative to the bundle root.",dstandish,2025-01-13 17:29:32+00:00,['IKholopov'],2025-01-24 23:27:25+00:00,,https://github.com/apache/airflow/issues/45623,"[('area:core', '')]","[{'comment_id': 2599404500, 'issue_id': 2784721198, 'author': 'IKholopov', 'body': ""I can pick this one up, if it hasn't been started yet (you can assign it to me)"", 'created_at': datetime.datetime(2025, 1, 18, 0, 17, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2599641903, 'issue_id': 2784721198, 'author': 'potiuk', 'body': 'Assigned', 'created_at': datetime.datetime(2025, 1, 18, 9, 15, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2613233702, 'issue_id': 2784721198, 'author': 'dstandish', 'body': ""Hi @IKholopov just wondering if you've done any work on this?"", 'created_at': datetime.datetime(2025, 1, 24, 19, 26, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2613587435, 'issue_id': 2784721198, 'author': 'IKholopov', 'body': ""I've started working on it yesterday (I was offline for most of the last week), but nothing mergeable yet. I am starting with making fileloc storing a relative location. From what I saw, I should be able to close this one next week.\n\nIf this is currently on the blocking path and 1 week is not fast enough, please feel free to take over :)"", 'created_at': datetime.datetime(2025, 1, 24, 23, 27, 24, tzinfo=datetime.timezone.utc)}]","IKholopov (Assginee) on (2025-01-18 00:17:52 UTC): I can pick this one up, if it hasn't been started yet (you can assign it to me)

potiuk on (2025-01-18 09:15:23 UTC): Assigned

dstandish (Issue Creator) on (2025-01-24 19:26:04 UTC): Hi @IKholopov just wondering if you've done any work on this?

IKholopov (Assginee) on (2025-01-24 23:27:24 UTC): I've started working on it yesterday (I was offline for most of the last week), but nothing mergeable yet. I am starting with making fileloc storing a relative location. From what I saw, I should be able to close this one next week.

If this is currently on the blocking path and 1 week is not fast enough, please feel free to take over :)

"
2784683957,issue,open,,Unhandled botocore.exceptions.NoCredentialsError in async_wait,"### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==9.2.0
apache-airflow-providers-common-compat==1.3.0
apache-airflow-providers-common-io==1.5.0
apache-airflow-providers-common-sql==1.21.0
apache-airflow-providers-fab==1.5.2
apache-airflow-providers-ftp==3.12.0
apache-airflow-providers-http==5.0.0
apache-airflow-providers-imap==3.8.0
apache-airflow-providers-postgres==6.0.0
apache-airflow-providers-sendgrid==4.0.0
apache-airflow-providers-smtp==1.9.0
apache-airflow-providers-snowflake==6.0.0
apache-airflow-providers-sqlite==4.0.0

### Apache Airflow version

2.10.4

### Operating System

Amazon Linux 2023.6.20241212

### Deployment

Virtualenv installation

### Deployment details

Custom CDK stack with:

- EC2 instance running Airflow, managed by systemd
- IAM role granting permissions to AWS services
- RDS instance running Postgres

The Airflow virtualenv is managed by uv.

### What happened

When running a DAG with a deferrable BatchOperator and using boto3 credential strategy (`{base_aws.py:180} INFO - No connection ID provided. Fallback on boto3 credential strategy (region_name='us-east-1')`) a deferrable BatchOperator task can have its trigger immediately fail after submitting a batch job.

Although the trigger fails immediately, the batch job had launched successfully, and executes until successful exit, unbeknownst to Airflow.

Due to the scheduling of the DAG, there currently have not been any overlaps with the failed task's batch job and a subsequent task run yet, but having overlapping runs would be undesirable.

This error happens about once a week. I believe it has something to do with amazon-ssm-agent not rotating the credentials quickly enough.

### What you think should happen instead

`async_wait()` should catch the `NoCredentialsError` and continue to the next waiter attempt.

https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/amazon/aws/utils/waiter_with_logging.py#L133

### How to reproduce

Hard to reproduce, but invalidating AWS credentials right before the trigger initializes would likely produce a similar traceback.

### Anything else

Traceback from task log:

```
[2025-01-10, 20:00:19 EST] {baseoperator.py:1806} ERROR - Trigger failed:
Traceback (most recent call last):
  File ""/opt/airflow/git/.venv/lib/python3.12/site-packages/airflow/jobs/triggerer_job_runner.py"", line 558, in cleanup_finished_triggers
    result = details[""task""].result()
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/git/.venv/lib/python3.12/site-packages/airflow/jobs/triggerer_job_runner.py"", line 630, in run_trigger
    async for event in trigger.run():
  File ""/opt/airflow/git/.venv/lib/python3.12/site-packages/airflow/providers/amazon/aws/triggers/base.py"", line 143, in run
    await async_wait(
  File ""/opt/airflow/git/.venv/lib/python3.12/site-packages/airflow/providers/amazon/aws/utils/waiter_with_logging.py"", line 133, in async_wait
    await waiter.wait(**args, WaiterConfig={""MaxAttempts"": 1})
  File ""/opt/airflow/git/.venv/lib/python3.12/site-packages/aiobotocore/waiter.py"", line 49, in wait
    return await AIOWaiter.wait(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/git/.venv/lib/python3.12/site-packages/aiobotocore/waiter.py"", line 95, in wait
    response = await self._operation_method(**kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/git/.venv/lib/python3.12/site-packages/aiobotocore/waiter.py"", line 78, in __call__
    return await self._client_method(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/git/.venv/lib/python3.12/site-packages/aiobotocore/client.py"", line 394, in _make_api_call
    http, parsed_response = await self._make_request(
                            ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/git/.venv/lib/python3.12/site-packages/aiobotocore/client.py"", line 420, in _make_request
    return await self._endpoint.make_request(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/git/.venv/lib/python3.12/site-packages/aiobotocore/endpoint.py"", line 96, in _send_request
    request = await self.create_request(request_dict, operation_model)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/git/.venv/lib/python3.12/site-packages/aiobotocore/endpoint.py"", line 84, in create_request
    await self._event_emitter.emit(
  File ""/opt/airflow/git/.venv/lib/python3.12/site-packages/aiobotocore/hooks.py"", line 68, in _emit
    response = await resolve_awaitable(handler(**kwargs))
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/git/.venv/lib/python3.12/site-packages/aiobotocore/_helpers.py"", line 6, in resolve_awaitable
    return await obj
           ^^^^^^^^^
  File ""/opt/airflow/git/.venv/lib/python3.12/site-packages/aiobotocore/signers.py"", line 24, in handler
    return await self.sign(operation_name, request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/git/.venv/lib/python3.12/site-packages/aiobotocore/signers.py"", line 90, in sign
    auth.add_auth(request)
  File ""/opt/airflow/git/.venv/lib/python3.12/site-packages/botocore/auth.py"", line 423, in add_auth
    raise NoCredentialsError()
botocore.exceptions.NoCredentialsError: Unable to locate credentials
[2025-01-10, 20:00:19 EST] {taskinstance.py:3311} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/opt/airflow/git/.venv/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/git/.venv/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/git/.venv/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/git/.venv/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 1807, in resume_execution
    raise TaskDeferralError(next_kwargs.get(""error"", ""Unknown""))
airflow.exceptions.TaskDeferralError: Trigger failure
```

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",nrobinson-intelycare,2025-01-13 17:16:48+00:00,['Anurag-Kumar-01'],2025-01-14 15:32:13+00:00,,https://github.com/apache/airflow/issues/45622,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('good first issue', '')]","[{'comment_id': 2587715444, 'issue_id': 2784683957, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 13, 17, 17, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2588051749, 'issue_id': 2784683957, 'author': 'eladkal', 'body': 'cc @ferruzzi @vincbeck', 'created_at': datetime.datetime(2025, 1, 13, 19, 47, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2588086727, 'issue_id': 2784683957, 'author': 'vincbeck', 'body': 'I am trying to understand the whole picture. When using `BatchOperator`, it can successfully create the job but when the trigger gets initialized (because the operator is run in deferrable mode) it fails instantly? Does it fail right away when initializing the trigger or later while pooling the batch job status?\n\nSince the deployment is ""Virtualenv installation"", I assume the triggerer and the scheduler are the same machine? If so, I cannot explain why the job gets created successfully created but the trigger fails to fetch AWS creds when getting created. If the job gets created successfully it means AWS creds are there and valid. \n\nIf the scheduler and triggerer are different machines, I could find explanations.', 'created_at': datetime.datetime(2025, 1, 13, 20, 6, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2588124655, 'issue_id': 2784683957, 'author': 'nrobinson-intelycare', 'body': '> I am trying to understand the whole picture. When using `BatchOperator`, it can successfully create the job but when the trigger gets initialized (because the operator is run in deferrable mode) it fails instantly? Does it fail right away when initializing the trigger or later while pooling the batch job status?\n> \n> Since the deployment is ""Virtualenv installation"", I assume the triggerer and the scheduler are the same machine? If so, I cannot explain why the job gets created successfully created but the trigger fails to fetch AWS creds when getting created. If the job gets created successfully it means AWS creds are there and valid.\n> \n> If the scheduler and triggerer are different machines, I could find explanations.\n\nHey @vincbeck , thanks for looking into this.\n\nThe scheduler and triggerer are on the same machine, and are both using the IAM role of the EC2 instance.\n\nI think the trigger is failing right away.\n\nHere is the full task log from another BatchOperator failing in the same way, coincidentally on the same day.\n\n[dag_id=foxy_pendo_run_id=scheduled__2025-01-10T20_30_00+00_00_task_id=foxy_pendo_batch_job_attempt=1.log](https://github.com/user-attachments/files/18401616/dag_id.foxy_pendo_run_id.scheduled__2025-01-10T20_30_00%2B00_00_task_id.foxy_pendo_batch_job_attempt.1.log)', 'created_at': datetime.datetime(2025, 1, 13, 20, 25, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2588153092, 'issue_id': 2784683957, 'author': 'nrobinson-intelycare', 'body': '@vincbeck Could IMDSv2 being briefly unavailable or amazon-ssm-agent doing something cause this?', 'created_at': datetime.datetime(2025, 1, 13, 20, 41, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2589104604, 'issue_id': 2784683957, 'author': 'Anurag-Kumar-01', 'body': 'i want to take up this issue', 'created_at': datetime.datetime(2025, 1, 14, 6, 7, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2590255740, 'issue_id': 2784683957, 'author': 'vincbeck', 'body': '> i want to take up this issue\n\nSuper :)', 'created_at': datetime.datetime(2025, 1, 14, 15, 32, 7, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-13 17:17:18 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2025-01-13 19:47:04 UTC): cc @ferruzzi @vincbeck

vincbeck on (2025-01-13 20:06:14 UTC): I am trying to understand the whole picture. When using `BatchOperator`, it can successfully create the job but when the trigger gets initialized (because the operator is run in deferrable mode) it fails instantly? Does it fail right away when initializing the trigger or later while pooling the batch job status?

Since the deployment is ""Virtualenv installation"", I assume the triggerer and the scheduler are the same machine? If so, I cannot explain why the job gets created successfully created but the trigger fails to fetch AWS creds when getting created. If the job gets created successfully it means AWS creds are there and valid. 

If the scheduler and triggerer are different machines, I could find explanations.

nrobinson-intelycare (Issue Creator) on (2025-01-13 20:25:50 UTC): Hey @vincbeck , thanks for looking into this.

The scheduler and triggerer are on the same machine, and are both using the IAM role of the EC2 instance.

I think the trigger is failing right away.

Here is the full task log from another BatchOperator failing in the same way, coincidentally on the same day.

[dag_id=foxy_pendo_run_id=scheduled__2025-01-10T20_30_00+00_00_task_id=foxy_pendo_batch_job_attempt=1.log](https://github.com/user-attachments/files/18401616/dag_id.foxy_pendo_run_id.scheduled__2025-01-10T20_30_00%2B00_00_task_id.foxy_pendo_batch_job_attempt.1.log)

nrobinson-intelycare (Issue Creator) on (2025-01-13 20:41:53 UTC): @vincbeck Could IMDSv2 being briefly unavailable or amazon-ssm-agent doing something cause this?

Anurag-Kumar-01 (Assginee) on (2025-01-14 06:07:18 UTC): i want to take up this issue

vincbeck on (2025-01-14 15:32:07 UTC): Super :)

"
2784606458,issue,open,,Split our html out for Airflow site,"We are currently hosting all historical versions of all the docs we build in ""airlfow-site"" GitHub. The size of that repo is ginormous. Just checking it out takes 20 minutes.  And it takes 30 or so GB to push the repo to ASF publishing tool.

It seems that we can move all the historical versions to S3 ""archive"" bucket, however and host them from there, while only some (latest?) versions of the docs will be hosted via airlfow.apache.org - this can be done via sub-domain (such as `docs.airflow.apache.org` and redirecting the versions in our repo via `.htaccess` mechanism which we already use here https://github.com/apache/airflow-site/blob/main/landing-pages/site/static/.htaccess 


The goal should be to have a very small and lean `airflow-site` repo that we will be able to work with as ""usual""",potiuk,2025-01-13 16:51:04+00:00,['gopidesupavan'],2025-01-13 21:29:07+00:00,,https://github.com/apache/airflow/issues/45621,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:dev-tools', '')]","[{'comment_id': 2587652662, 'issue_id': 2784606458, 'author': 'potiuk', 'body': 'cc: @gopidesupavan @ashb', 'created_at': datetime.datetime(2025, 1, 13, 16, 51, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2588176482, 'issue_id': 2784606458, 'author': 'gopidesupavan', 'body': 'Nice, will create some subtasks. :)', 'created_at': datetime.datetime(2025, 1, 13, 20, 53, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2588255683, 'issue_id': 2784606458, 'author': 'gopidesupavan', 'body': 'I have created sandbox environment and added some of our docs to s3 and created static website. \n\nhttp://docs.dummyairflow.org.s3-website-us-east-1.amazonaws.com/\n\nOne thing here its not ssl verified to get verified , we may need to create domain and register that in aws and install ssl that way it wont flag :) \n\nBTW thats a sandbox environment i created to experiment, after 1hr will loose that completely, sorry if youre unable to access it later :)', 'created_at': datetime.datetime(2025, 1, 13, 21, 29, 6, tzinfo=datetime.timezone.utc)}]","potiuk (Issue Creator) on (2025-01-13 16:51:24 UTC): cc: @gopidesupavan @ashb

gopidesupavan (Assginee) on (2025-01-13 20:53:34 UTC): Nice, will create some subtasks. :)

gopidesupavan (Assginee) on (2025-01-13 21:29:06 UTC): I have created sandbox environment and added some of our docs to s3 and created static website. 

http://docs.dummyairflow.org.s3-website-us-east-1.amazonaws.com/

One thing here its not ssl verified to get verified , we may need to create domain and register that in aws and install ssl that way it wont flag :) 

BTW thats a sandbox environment i created to experiment, after 1hr will loose that completely, sorry if youre unable to access it later :)

"
2784579818,issue,open,,Modernize build environment for Airlfow-site,"Currently we use github action and ""site"" script to build airflow-site docs. But those tools there like npm and hugo are old and outdated and they do not run on modern OS-ses (MacOS ubuntu) - not out-of-the-box at leas.

We need to modernize the tooling and possibly have a dockerized environment to run them on on all kind of os's variants

Possibly we could also use [go-hugo](https://github.com/gohugoio) or [Pelican](https://docs.getpelican.com/en/latest/)
",potiuk,2025-01-13 16:42:36+00:00,[],2025-01-13 16:53:56+00:00,,https://github.com/apache/airflow/issues/45620,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:dev-tools', '')]",[],
2784487475,issue,closed,completed,Add extra link to `SageMakerTransformOperator`,"### Body

The operator invokes a Sagemaker transform job but it doesn't offer extra link to the invoked job.

The task: add the needed link.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2025-01-13 16:13:04+00:00,['Anurag-Kumar-01'],2025-01-15 17:47:42+00:00,2025-01-15 17:47:42+00:00,https://github.com/apache/airflow/issues/45618,"[('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('kind:feature', 'Feature Requests'), ('good first issue', '')]","[{'comment_id': 2589135209, 'issue_id': 2784487475, 'author': 'Anurag-Kumar-01', 'body': '@eladkal sir i want to take up this issue.', 'created_at': datetime.datetime(2025, 1, 14, 6, 30, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2589205865, 'issue_id': 2784487475, 'author': 'eladkal', 'body': '@Anurag-Kumar-01 assigned', 'created_at': datetime.datetime(2025, 1, 14, 7, 28, 25, tzinfo=datetime.timezone.utc)}]","Anurag-Kumar-01 (Assginee) on (2025-01-14 06:30:29 UTC): @eladkal sir i want to take up this issue.

eladkal (Issue Creator) on (2025-01-14 07:28:25 UTC): @Anurag-Kumar-01 assigned

"
2783928887,issue,closed,completed,AIP 38 : Add extra links to task instance page,"### Description

I didn't see extra links design in the task instance designs. Extra links are useful in the old UI where tasks that are deferred whose status can be tracked through extra link. If there is a design for this finalized with screenshots I can pick this up since API is available.

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",tirkarthi,2025-01-13 13:13:02+00:00,[],2025-02-05 13:29:59+00:00,2025-02-05 13:29:59+00:00,https://github.com/apache/airflow/issues/45614,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]",[],
2783728135,issue,open,,Make sphinx-airflow-theme part of the main airflow workspace,"During the recent ""quest"" with upgrading Sphinx for airflow docs generation we found that the current approach of sharing sphinx-airflow-theme and doc updates in airflow is a very difficult one to make and test changes.

The process - especially when involving changes that require modifications in both - theme and doc generation process in Airflow is very complex today:

1) someone has to figure out changes in both ""theme"" and doc building 
2) there should be a PR merged in ""airflow-site"" sphinx-airflow-theme
3) the sphinx-airflow-theme has to be build (which is a challenge on its own on a modern OS - we have to use some old versions of OS as a base and it does not work on MacOS) 
4) the sphinx-airlfow-theme has to be published in PyPI (only a handful of people can do it)
5) there should be a PR in Airlfow docs building to switch to using this new theme and only then you can really test if this worked


You can somehow manually attempt and builld those theme projects between repos and use them without publishing to PyPI - and try to locally build image, but - for example until the `sphinx-airlfow-theme` package is published to PyPI, we cannot reall run PR in Airlfow docs through CI. 

This is very complex and difficult and we should simplify it.

The proposal to simplify it: 

- [ ] modernize ""sphinx-airflow-theme"" to use standard pyproject.toml and build backends to build the theme with `uv build` or `hatch build` frontend - including node/npm setup and conifiguration that does not require old OS
- [ ] move the `sphinx-airlfow-theme` to **airflow** repo as sub-project, all separated from airflow code (in docs dir) and connected via `uv workspace` with the main airflow repo. This will allow also to remove `doc` extra from `airflow` package (dependencies will move to the `sphinx-airlfow-theme` project and will be used from there. The `sphinx-airflow-theme` package will be used by doc building process from the locally installed package, which means that you will be able to test and modify the process as a single `PR` - all runnable and testable in CI and you will be able to use regular `breeze build-docs` to build and test the documentation
- [ ] once we move the standard pyproject.toml exposed in Airflow repo, we could modify airlfow-site to NOT use local version or PyPI - but install the `sphinx-airlfow-theme` following the github URL installation pattern `pip install ""sphinx-airflow-theme @ git+https://github.com/apache/airflow/docs/sphinx-airflow-theme/@BRANCH` - which is one of the standard ways of instaling modern python packages https://pip.pypa.io/en/stable/topics/vcs-support/ - that would completely remove the need of publishing the `sphinx-airflow-theme` to PyPI

 



",potiuk,2025-01-13 11:42:05+00:00,[],2025-01-14 16:07:06+00:00,,https://github.com/apache/airflow/issues/45613,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:dev-tools', '')]","[{'comment_id': 2588147199, 'issue_id': 2783728135, 'author': 'gopidesupavan', 'body': 'AH Nice proposal, that simplifies nicely :). Think the major advantage is to get out from this build and test simply.', 'created_at': datetime.datetime(2025, 1, 13, 20, 38, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2588303082, 'issue_id': 2783728135, 'author': 'jscheffl', 'body': 'I thought there is a reason behind this split but after readin this I understand now... it is just ""legacy"" :-D', 'created_at': datetime.datetime(2025, 1, 13, 21, 56, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2590367518, 'issue_id': 2783728135, 'author': 'potiuk', 'body': 'Indeed and additionally eventually we might also trigger publishing from a single Github Actions workflow rather than having to do it locally with all the copying involved. cc: @eladkal @ephraimbuddy @utkarsharma2. I created a follow up [#45641](https://github.com/apache/airflow/issues/45641) for that one.', 'created_at': datetime.datetime(2025, 1, 14, 16, 7, 5, tzinfo=datetime.timezone.utc)}]","gopidesupavan on (2025-01-13 20:38:35 UTC): AH Nice proposal, that simplifies nicely :). Think the major advantage is to get out from this build and test simply.

jscheffl on (2025-01-13 21:56:10 UTC): I thought there is a reason behind this split but after readin this I understand now... it is just ""legacy"" :-D

potiuk (Issue Creator) on (2025-01-14 16:07:05 UTC): Indeed and additionally eventually we might also trigger publishing from a single Github Actions workflow rather than having to do it locally with all the copying involved. cc: @eladkal @ephraimbuddy @utkarsharma2. I created a follow up [#45641](https://github.com/apache/airflow/issues/45641) for that one.

"
2783160513,issue,closed,completed,Dask executor doesn't get correct response if worker restarted during task running,"### Apache Airflow Provider(s)

standard

### Versions of Apache Airflow Providers

apache-airflow-providers-daskexecutor==1.1.1
apache-airflow-providers-fab==1.4.1
apache-airflow-providers-ftp==3.11.1
apache-airflow-providers-http==4.13.1
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-smtp==1.8.0
apache-airflow-providers-sqlite==3.9.0
apache-airflow-providers-common-compat==1.2.1
apache-airflow-providers-common-io==1.4.2
apache-airflow-providers-common-sql==1.18.0

### Apache Airflow version

2.10.2

### Operating System

Debian GNU/Linux 12 (bookworm)

### Deployment

Other Docker-based deployment

### Deployment details

Kubernetes: v1.28
GoVersion:""go1.15.9""

### What happened

```
./dask_executor.log-2025-01-12 08:30:47,368 - distributed.worker - INFO - Closing worker gracefully: tcp://{IP:port}. Reason: worker-lifetime-reached
./dask_executor.log-[2025-01-12T15:30:47.438+0000][271075][MainProcess] {{task_command.py:467}} INFO - Running <TaskInstance: {dag_id}.{task_id}.import manual__2025-01-12T15:28:42+00:00 [queued]> on host
./dask_executor.log-2025-01-12 08:30:47,497 - distributed.worker - INFO - Stopping worker at tcp://{IP:port}. Reason: worker-lifetime-reached
./dask_executor.log:2025-01-12 08:30:47,498 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name='execute(""TaskInstanceKey(dag_id=\'...\', task_id=\'...\', run_id=\'manual__2025-01-12T15:28:42+00:00\', try_number=1, map_index=-1)"")' coro=<Worker.execute() done, defined at /opt/nucolumnaradmin/.venv/lib/python3.11/site-packages/distributed/worker_state_machine.py:3610>> ended with CancelledError
./dask_executor.log-2025-01-12 08:30:47,499 - distributed.core - INFO - Connection to ... has been closed.
./dask_executor.log-2025-01-12 08:30:55,887 - distributed.nanny - INFO - Worker closed
```
Show as above logs, the dask worker triggered gracefully restart when a airflow task still running.
After that, the task's status keep in 'pending' and will no longer changed.
The running task's context is a shell cmd started with [airflow run ...], it will keep running and mark itself as success in airflow's metadata DB.
```
...
[2025-01-12T15:30:54.978+0000][271103][MainProcess] {{taskinstance.py:352}} INFO - Marking task as SUCCESS. dag_id=..., task_id=..., run_id=manual__2025-01-12T15:28:42+00:00, execution_date=20250112T152842, start_date=20250112T153048, end_date=20250112T153054
[2025-01-12T15:30:55.048+0000][271075][MainProcess] {{local_task_job_runner.py:266}} INFO - Task exited with return code 0
```
Their date are actually a same value with different time zone.

After that, this airflow task's status in airflow DB was success and will no longer be scheduled, at the mean time, its status in Dask executor(in airflow scheduler process) will keep in 'pending' until airflow scheduler restarted and will affect value of `executor_slots_available`, when the `executor_slots_available` reach zero, the airflow scheduler will stop working.

The debug dump from DaskExecutor, actually no task is running during that time:
```
SIGUSR2 received, printing debug
--------------------------------------------------------------------------------
[2025-01-13T03:18:54.192+0000][683642][MainProcess] {{scheduler_job_runner.py:278}} INFO - Debug dump for the executor DaskExecutor(parallelism=0)
[2025-01-13T03:18:54.192+0000][683642][MainProcess] {{base_executor.py:627}} INFO - executor.queued (0)

[2025-01-13T03:18:54.193+0000][683642][MainProcess] {{base_executor.py:632}} INFO - executor.running (176)
```

### What you think should happen instead

Obviously the above status is incorrect and looks like a memory leak Bug.


### How to reproduce

Maybe airflow should sync task status in Dask executor with metadata DB and cleanup succeed tasks.
Or just make sure Dask executor can receive the same status as the actually running cmd.

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",danxian-baiheng,2025-01-13 06:50:04+00:00,[],2025-01-13 11:01:45+00:00,2025-01-13 11:01:45+00:00,https://github.com/apache/airflow/issues/45606,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('area:Executors-core', 'LocalExecutor & SequentialExecutor')]","[{'comment_id': 2586325826, 'issue_id': 2783160513, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 13, 6, 50, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2586327638, 'issue_id': 2783160513, 'author': 'danxian-baiheng', 'body': ""I haven't found the DaskExecutor tag in airflow-provider's list.\r\nDoes it be removed from airflow in the latest version?"", 'created_at': datetime.datetime(2025, 1, 13, 6, 51, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2586796978, 'issue_id': 2783160513, 'author': 'potiuk', 'body': ""> I haven't found the DaskExecutor tag in airflow-provider's list. Does it be removed from airflow in the latest version?\r\n\r\nYes. We no longer publish nor fix DasK Executor, it has been removed from maintance as result of this LAZY CONSENSUS https://lists.apache.org/thread/fxv44cqqljrrhll3fdpdgc9h9fz5ghcy and you will find the detailed discussion on why we decided to do it. But if someone would like to for the code of last dask executor, look at any problems there and release a new version - likely with a 3rd-party managed dask provider, they are entirely free to do it."", 'created_at': datetime.datetime(2025, 1, 13, 11, 1, 37, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-13 06:50:07 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

danxian-baiheng (Issue Creator) on (2025-01-13 06:51:27 UTC): I haven't found the DaskExecutor tag in airflow-provider's list.
Does it be removed from airflow in the latest version?

potiuk on (2025-01-13 11:01:37 UTC): Yes. We no longer publish nor fix DasK Executor, it has been removed from maintance as result of this LAZY CONSENSUS https://lists.apache.org/thread/fxv44cqqljrrhll3fdpdgc9h9fz5ghcy and you will find the detailed discussion on why we decided to do it. But if someone would like to for the code of last dask executor, look at any problems there and release a new version - likely with a 3rd-party managed dask provider, they are entirely free to do it.

"
2782650260,issue,closed,completed,Adopt Unified Bulk Operation Approach in Rest API (FastAPI),"### Description

Adopt the new unified approach for bulk operations in Rest API. We should bring new approaches to the already existing bulk operations.
* Connections - assignee:@bugraoz93 
* Pools - assignee: @shubhamraj-git 

**Dependencies:**
* This can be started after this is merged: #45577 

### Use case/motivation

Unifying approach for bulk operations in Rest API. 
https://apache-airflow.slack.com/archives/C0809U4S1Q9/p1736418768381509

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2025-01-12 19:17:21+00:00,"['bugraoz93', 'shubhamraj-git']",2025-01-25 21:47:39+00:00,2025-01-23 20:54:44+00:00,https://github.com/apache/airflow/issues/45601,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API"")]","[{'comment_id': 2585885385, 'issue_id': 2782650260, 'author': 'bugraoz93', 'body': '`Pools` still has no assignee and wants somebody to work on it, please write a comment if you want to work on this issue. Thanks!', 'created_at': datetime.datetime(2025, 1, 12, 19, 18, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585889462, 'issue_id': 2782650260, 'author': 'shubhamraj-git', 'body': '@bugraoz93 I can take it once bulk variables is merged.', 'created_at': datetime.datetime(2025, 1, 12, 19, 32, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585890414, 'issue_id': 2782650260, 'author': 'bugraoz93', 'body': '> @bugraoz93 I can take it once bulk variables is merged.\n\nAmazing, thanks! Assigned', 'created_at': datetime.datetime(2025, 1, 12, 19, 35, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2610997559, 'issue_id': 2782650260, 'author': 'bugraoz93', 'body': 'I think we can close this one, thanks a lot @shubhamraj-git!', 'created_at': datetime.datetime(2025, 1, 23, 20, 54, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614109944, 'issue_id': 2782650260, 'author': 'potiuk', 'body': 'Yep.', 'created_at': datetime.datetime(2025, 1, 25, 21, 47, 38, tzinfo=datetime.timezone.utc)}]","bugraoz93 (Issue Creator) on (2025-01-12 19:18:02 UTC): `Pools` still has no assignee and wants somebody to work on it, please write a comment if you want to work on this issue. Thanks!

shubhamraj-git (Assginee) on (2025-01-12 19:32:04 UTC): @bugraoz93 I can take it once bulk variables is merged.

bugraoz93 (Issue Creator) on (2025-01-12 19:35:33 UTC): Amazing, thanks! Assigned

bugraoz93 (Issue Creator) on (2025-01-23 20:54:45 UTC): I think we can close this one, thanks a lot @shubhamraj-git!

potiuk on (2025-01-25 21:47:38 UTC): Yep.

"
2782580593,issue,closed,completed,Transport endpoint is not connected,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

We use the official Helm Chart Airflow with configured CeleryKubernetes Executor and csi-s3. Our scheduler restarts with different periodicity in time, and sometimes it does not start at all. According to the logs, the problem is with the csi-s3 driver, unmount the directory with the logs and until you manually restart the scheduler service, it will hang with an error, but we only have this problem with the scheduler, there are no problems with other services.

```
--- Logging error ---
Traceback (most recent call last):
  File ""/usr/local/lib/python3.12/logging/handlers.py"", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/logging/handlers.py"", line 193, in shouldRollover
    pos = self.stream.tell()
          ^^^^^^^^^^^^^^^^^^
OSError: [Errno 107] Transport endpoint is not connected
Call stack:
  File ""/home/airflow/.local/bin/airflow"", line 8, in <module>
    sys.exit(main())
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/__main__.py"", line 62, in main
    args.func(args)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command
    return func(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper
    return f(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/providers_configuration_loader.py"", line 55, in wrapped_function
    return func(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/scheduler_command.py"", line 59, in scheduler
    run_command_with_daemon_option(
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/daemon_utils.py"", line 86, in run_command_with_daemon_option
    callback()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/scheduler_command.py"", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/scheduler_command.py"", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/job.py"", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/job.py"", line 450, in execute_job
    ret = execute_callable()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/scheduler_job_runner.py"", line 983, in _execute
    self.processor_agent.start()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/manager.py"", line 172, in start
    process.start()
  File ""/usr/local/lib/python3.12/multiprocessing/process.py"", line 121, in start
    self._popen = self._Popen(self)
  File ""/usr/local/lib/python3.12/multiprocessing/context.py"", line 282, in _Popen
    return Popen(process_obj)
  File ""/usr/local/lib/python3.12/multiprocessing/popen_fork.py"", line 19, in __init__
    self._launch(process_obj)
  File ""/usr/local/lib/python3.12/multiprocessing/popen_fork.py"", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File ""/usr/local/lib/python3.12/multiprocessing/process.py"", line 314, in _bootstrap
    self.run()
  File ""/usr/local/lib/python3.12/multiprocessing/process.py"", line 108, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/manager.py"", line 247, in _run_processor_manager
    processor_manager.start()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/manager.py"", line 489, in start
    return self._run_parsing_loop()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/manager.py"", line 667, in _run_parsing_loop
    self.collect_results()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/manager.py"", line 1196, in collect_results
    self._collect_results_from_processor(processor)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/manager.py"", line 1146, in _collect_results_from_processor
    self.log.error(
Message: 'Processor for %s exited with return code %s.'
Arguments: ('/opt/airflow/dags/repo/prod/ozon/ozon_dag.py', 1)
--- Logging error ---
Traceback (most recent call last):
  File ""/usr/local/lib/python3.12/logging/handlers.py"", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/logging/handlers.py"", line 193, in shouldRollover
    pos = self.stream.tell()
          ^^^^^^^^^^^^^^^^^^
OSError: [Errno 107] Transport endpoint is not connected
Call stack:
  File ""/home/airflow/.local/bin/airflow"", line 8, in <module>
    sys.exit(main())
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/__main__.py"", line 62, in main
    args.func(args)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command
    return func(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper
    return f(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/providers_configuration_loader.py"", line 55, in wrapped_function
    return func(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/scheduler_command.py"", line 59, in scheduler
    run_command_with_daemon_option(
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/daemon_utils.py"", line 86, in run_command_with_daemon_option
    callback()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/scheduler_command.py"", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/scheduler_command.py"", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/job.py"", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/job.py"", line 450, in execute_job
    ret = execute_callable()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/scheduler_job_runner.py"", line 983, in _execute
    self.processor_agent.start()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/manager.py"", line 172, in start
    process.start()
  File ""/usr/local/lib/python3.12/multiprocessing/process.py"", line 121, in start
    self._popen = self._Popen(self)
  File ""/usr/local/lib/python3.12/multiprocessing/context.py"", line 282, in _Popen
    return Popen(process_obj)
  File ""/usr/local/lib/python3.12/multiprocessing/popen_fork.py"", line 19, in __init__
    self._launch(process_obj)
  File ""/usr/local/lib/python3.12/multiprocessing/popen_fork.py"", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File ""/usr/local/lib/python3.12/multiprocessing/process.py"", line 314, in _bootstrap
    self.run()
  File ""/usr/local/lib/python3.12/multiprocessing/process.py"", line 108, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/manager.py"", line 247, in _run_processor_manager
    processor_manager.start()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/manager.py"", line 489, in start
    return self._run_parsing_loop()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/manager.py"", line 667, in _run_parsing_loop
    self.collect_results()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/manager.py"", line 1196, in collect_results
    self._collect_results_from_processor(processor)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/manager.py"", line 1146, in _collect_results_from_processor
    self.log.error(
Message: 'Processor for %s exited with return code %s.'
Arguments: ('/opt/airflow/dags/repo/prod/utils/dag_status.py', 1)
[2025-01-11T22:44:51.127+0000] {process_utils.py:132} INFO - Sending 15 to group 55. PIDs of all processes in the group: [55]
[2025-01-11T22:44:51.127+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 55
--- Logging error ---
Traceback (most recent call last):
  File ""/usr/local/lib/python3.12/logging/handlers.py"", line 73, in emit
    if self.shouldRollover(record):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/logging/handlers.py"", line 193, in shouldRollover
    pos = self.stream.tell()
          ^^^^^^^^^^^^^^^^^^
OSError: [Errno 107] Transport endpoint is not connected
Call stack:
  File ""/home/airflow/.local/bin/airflow"", line 8, in <module>
    sys.exit(main())
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/__main__.py"", line 62, in main
    args.func(args)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command
    return func(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper
    return f(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/providers_configuration_loader.py"", line 55, in wrapped_function
    return func(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/scheduler_command.py"", line 59, in scheduler
    run_command_with_daemon_option(
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/daemon_utils.py"", line 86, in run_command_with_daemon_option
    callback()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/scheduler_command.py"", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/scheduler_command.py"", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/job.py"", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/job.py"", line 450, in execute_job
    ret = execute_callable()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/scheduler_job_runner.py"", line 983, in _execute
    self.processor_agent.start()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/manager.py"", line 172, in start
    process.start()
  File ""/usr/local/lib/python3.12/multiprocessing/process.py"", line 121, in start
    self._popen = self._Popen(self)
  File ""/usr/local/lib/python3.12/multiprocessing/context.py"", line 282, in _Popen
    return Popen(process_obj)
  File ""/usr/local/lib/python3.12/multiprocessing/popen_fork.py"", line 19, in __init__
    self._launch(process_obj)
  File ""/usr/local/lib/python3.12/multiprocessing/popen_fork.py"", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File ""/usr/local/lib/python3.12/multiprocessing/process.py"", line 314, in _bootstrap
    self.run()
  File ""/usr/local/lib/python3.12/multiprocessing/process.py"", line 108, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/manager.py"", line 247, in _run_processor_manager
    processor_manager.start()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/manager.py"", line 489, in start
    return self._run_parsing_loop()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/manager.py"", line 573, in _run_parsing_loop
    ready = multiprocessing.connection.wait(self.waitables.keys(), timeout=poll_time)
  File ""/usr/local/lib/python3.12/multiprocessing/connection.py"", line 1136, in wait
    ready = selector.select(timeout)
  File ""/usr/local/lib/python3.12/selectors.py"", line 415, in select
    fd_event_list = self._selector.poll(timeout)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/manager.py"", line 465, in _exit_gracefully
    self.log.info(""Exiting gracefully upon receiving signal %s"", signum)
Message: 'Exiting gracefully upon receiving signal %s'
Arguments: (15,)
[2025-01-11T22:44:51.500+0000] {process_utils.py:80} INFO - Process psutil.Process(pid=55, status='terminated', exitcode=0, started='2025-01-09 12:30:47') (55) terminated with exit code 0
[2025-01-11T22:44:51.501+0000] {kubernetes_executor.py:760} INFO - Shutting down Kubernetes executor
[2025-01-11T22:44:51.501+0000] {scheduler_job_runner.py:1011} ERROR - Exception when executing Executor.end on CeleryKubernetesExecutor(parallelism=32)
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/scheduler_job_runner.py"", line 987, in _execute
    self._run_scheduler_loop()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1176, in _run_scheduler_loop
    time.sleep(min(self._scheduler_idle_sleep_time, next_event or 0))
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/scheduler_job_runner.py"", line 263, in _exit_gracefully
    sys.exit(os.EX_OK)
SystemExit: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1009, in _execute
    executor.end()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/celery/executors/celery_kubernetes_executor.py"", line 254, in end
    self.kubernetes_executor.end()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py"", line 763, in end
    self._flush_task_queue()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py"", line 719, in _flush_task_queue
    self.log.debug(""Executor shutting down, task_queue approximate size=%d"", self.task_queue.qsize())
                                                                             ^^^^^^^^^^^^^^^^^^^^^^^
  File ""<string>"", line 2, in qsize
  File ""/usr/local/lib/python3.12/multiprocessing/managers.py"", line 820, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File ""/usr/local/lib/python3.12/multiprocessing/connection.py"", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File ""/usr/local/lib/python3.12/multiprocessing/connection.py"", line 427, in _send_bytes
    self._send(header + buf)
  File ""/usr/local/lib/python3.12/multiprocessing/connection.py"", line 384, in _send
    n = write(self._handle, buf)
        ^^^^^^^^^^^^^^^^^^^^^^^^
BrokenPipeError: [Errno 32] Broken pipe
[2025-01-11T22:44:51.521+0000] {process_utils.py:132} INFO - Sending 15 to group 55. PIDs of all processes in the group: []
[2025-01-11T22:44:51.521+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 55
[2025-01-11T22:44:51.521+0000] {process_utils.py:101} INFO - Sending the signal 15 to process 55 as process group is missing.
[2025-01-11T22:44:51.521+0000] {scheduler_job_runner.py:1017} INFO - Exited execute loop
INFO: detected pid 1, running init handler
```

### What you think should happen instead?

_No response_

### How to reproduce

We launch airflow, the tasks are executed and after some time the scheduler service is restarted. It can restart and after the restart it continues to work, or it does not restart and the service stops launching new tasks.

### Operating System

Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==9.0.0
apache-airflow-providers-celery==3.8.3
apache-airflow-providers-cncf-kubernetes==9.0.1
apache-airflow-providers-common-compat==1.2.1
apache-airflow-providers-common-io==1.4.2
apache-airflow-providers-common-sql==1.19.0
apache-airflow-providers-docker==3.14.0
apache-airflow-providers-elasticsearch==5.5.2
apache-airflow-providers-fab==1.5.0
apache-airflow-providers-ftp==3.11.1
apache-airflow-providers-google==10.25.0
apache-airflow-providers-grpc==3.6.0
apache-airflow-providers-hashicorp==3.8.0
apache-airflow-providers-http==4.13.2
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-microsoft-azure==11.0.0
apache-airflow-providers-mysql==5.7.3
apache-airflow-providers-odbc==4.8.0
apache-airflow-providers-openlineage==1.13.0
apache-airflow-providers-postgres==5.13.1
apache-airflow-providers-redis==3.8.0
apache-airflow-providers-sendgrid==3.6.0
apache-airflow-providers-sftp==4.11.1
apache-airflow-providers-slack==8.9.1
apache-airflow-providers-smtp==1.8.0
apache-airflow-providers-snowflake==5.8.0
apache-airflow-providers-sqlite==3.9.0
apache-airflow-providers-ssh==3.14.0

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

k8s

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",asemelianov,2025-01-12 16:47:56+00:00,[],2025-01-12 17:30:02+00:00,2025-01-12 17:30:02+00:00,https://github.com/apache/airflow/issues/45597,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('area:logging', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2782525230,issue,closed,completed,airflow v3 : conn_id `xxx` isn't defined,"### Apache Airflow version

main (development)

### What happened?

Using airflow main branch , with breeze  `breeze start-airflow`

I create a K8S connection with the `normal` UI, the connection is present in the UI , I can see it with the swagger but I can run a task using it 


![Screenshot from 2025-01-12 15-45-28](https://github.com/user-attachments/assets/b074fa95-53a6-4637-a89c-cf8e244628ba)
![Screenshot from 2025-01-12 15-45-21](https://github.com/user-attachments/assets/441158b1-966e-4990-94a1-d0238dd6feb0)

```python
from airflow import DAG
from pendulum import today
from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator

with DAG(
    dag_id=""kubernetes_dag"",
    schedule=None,
    start_date=today(""UTC"").add(days=-1)
):
    KubernetesPodOperator(
        task_id=""task-one"",
        namespace=""default"",
        kubernetes_conn_id=""toto"",
        image=""alpine:3.16.2"",
        cmds=[""sh"", ""-c"", ""echo hello""],
    )

```

fail with

```log
{""logger"":""airflow.models.connection"",""timestamp"":""2025-01-12T15:02:27.744286"",""error_detail"":[{""exc_type"":""RuntimeError"",""exc_value"":""UNEXPECTED COMMIT - THIS WILL BREAK HA LOCKS!"",""syntax_error"":null,""is_cause"":false,""frames"":[{""filename"":""/opt/airflow/airflow/models/connection.py"",""lineno"":463,""name"":""get_connection_from_secrets""},{""filename"":""/opt/airflow/airflow/utils/session.py"",""lineno"":101,""name"":""wrapper""},{""filename"":""/usr/local/lib/python3.9/contextlib.py"",""lineno"":126,""name"":""__exit__""},{""filename"":""/opt/airflow/airflow/utils/session.py"",""lineno"":43,""name"":""create_session""},{""filename"":""/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py"",""lineno"":1454,""name"":""commit""},{""filename"":""/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py"",""lineno"":832,""name"":""commit""},{""filename"":""/usr/local/lib/python3.9/site-packages/sqlalchemy/orm/session.py"",""lineno"":800,""name"":""_prepare_impl""},{""filename"":""/usr/local/lib/python3.9/site-packages/sqlalchemy/event/attr.py"",""lineno"":346,""name"":""__call__""},{""filename"":""/opt/airflow/airflow/utils/sqlalchemy.py"",""lineno"":382,""name"":""_validate_commit""}]}],""event"":""Unable to retrieve connection from secrets backend (MetastoreBackend). Checking subsequent secrets backend."",""level"":""error""}
{""timestamp"":""2025-01-12T15:02:27.744664"",""logger"":""task"",""error_detail"":[{""exc_type"":""AirflowNotFoundException"",""exc_value"":""The conn_id `toto` isn't defined"",""syntax_error"":null,""is_cause"":false,""frames"":[{""filename"":""/opt/airflow/task_sdk/src/airflow/sdk/execution_time/task_runner.py"",""lineno"":448,""name"":""run""},{""filename"":""/opt/airflow/task_sdk/src/airflow/sdk/execution_time/task_runner.py"",""lineno"":547,""name"":""_execute_task""},{""filename"":""/opt/airflow/airflow/models/baseoperator.py"",""lineno"":376,""name"":""wrapper""},{""filename"":""/opt/airflow/providers/src/airflow/providers/cncf/kubernetes/operators/pod.py"",""lineno"":583,""name"":""execute""},{""filename"":""/opt/airflow/providers/src/airflow/providers/cncf/kubernetes/operators/pod.py"",""lineno"":591,""name"":""execute_sync""},{""filename"":""/opt/airflow/providers/src/airflow/providers/cncf/kubernetes/operators/pod.py"",""lineno"":1157,""name"":""build_pod_request_obj""},{""filename"":""/opt/airflow/providers/src/airflow/providers/cncf/kubernetes/hooks/kubernetes.py"",""lineno"":292,""name"":""is_in_cluster""},{""filename"":""/usr/local/lib/python3.9/functools.py"",""lineno"":993,""name"":""__get__""},{""filename"":""/opt/airflow/providers/src/airflow/providers/cncf/kubernetes/hooks/kubernetes.py"",""lineno"":300,""name"":""api_client""},{""filename"":""/opt/airflow/providers/src/airflow/providers/cncf/kubernetes/hooks/kubernetes.py"",""lineno"":212,""name"":""get_conn""},{""filename"":""/opt/airflow/providers/src/airflow/providers/cncf/kubernetes/hooks/kubernetes.py"",""lineno"":205,""name"":""_get_field""},{""filename"":""/usr/local/lib/python3.9/functools.py"",""lineno"":993,""name"":""__get__""},{""filename"":""/opt/airflow/providers/src/airflow/providers/cncf/kubernetes/hooks/kubernetes.py"",""lineno"":186,""name"":""conn_extras""},{""filename"":""/opt/airflow/providers/src/airflow/providers/cncf/kubernetes/hooks/kubernetes.py"",""lineno"":176,""name"":""get_connection""},{""filename"":""/opt/airflow/airflow/hooks/base.py"",""lineno"":65,""name"":""get_connection""},{""filename"":""/opt/airflow/airflow/models/connection.py"",""lineno"":474,""name"":""get_connection_from_secrets""}]}],""event"":""Task failed with exception"",""level"":""error""}

```


### Operating System

ubuntu 24.04

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",raphaelauv,2025-01-12 14:52:58+00:00,[],2025-01-12 15:07:23+00:00,2025-01-12 15:07:23+00:00,https://github.com/apache/airflow/issues/45595,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2585770386, 'issue_id': 2782525230, 'author': 'raphaelauv', 'body': 'from official airflow slack is found that you can set \r\n\r\nAIRFLOW__CORE__EXECUTOR=SequentialExecutor\r\n\r\nuntil LocalExecutor is switched to new AIP-72 implementation', 'created_at': datetime.datetime(2025, 1, 12, 15, 7, 23, tzinfo=datetime.timezone.utc)}]","raphaelauv (Issue Creator) on (2025-01-12 15:07:23 UTC): from official airflow slack is found that you can set 

AIRFLOW__CORE__EXECUTOR=SequentialExecutor

until LocalExecutor is switched to new AIP-72 implementation

"
2782026000,issue,open,,Refactor `BaseSensorOperator` to support Reschedule mode with Task SDK,"`BaseSensorOperator` has some logic to use `max_tries` from Context as well as access DB directly. This won't work with Task SDK and as such DAGs like [`example_sensor_decorator`](https://github.com/apache/airflow/blob/168f76562e617cd8e8a41ec684bfda09db6b5705/airflow/example_dags/example_sensor_decorator.py) will fail.

https://github.com/apache/airflow/blob/6844cce95e1d02b06a2de7159db9eb2ea48ac74b/airflow/sensors/base.py#L219-L243

<img width=""1247"" alt=""Image"" src=""https://github.com/user-attachments/assets/bea150bd-e08e-4b9b-9eca-e0ac09dc8871"" />

<img width=""1233"" alt=""Image"" src=""https://github.com/user-attachments/assets/4c7bdec0-9222-4ad3-8701-478402b8c972"" />

The `max_tries` is easier since we can pass it we can in the following code and re-generate the API client.

https://github.com/apache/airflow/blob/168f76562e617cd8e8a41ec684bfda09db6b5705/airflow/executors/workloads.py#L42-L52

Replacing direct DB access in that `BaseSensorOperator`, on the other hand, needs to be figured out!",kaxil,2025-01-11 17:12:20+00:00,[],2025-01-17 15:24:23+00:00,,https://github.com/apache/airflow/issues/45580,"[('area:core-operators', 'Operators, Sensors and hooks within Core Airflow'), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK'), ('area:task-sdk', None)]",[],
2781733342,issue,open,,Add an option to mount `airflow-site/sphinx_airflow_theme` when running CI locally,"### Description

As of today, making changes in Airflow's site theme requires a lot of effort, as `sphinx_airflow_theme` is installed from PyPi: https://github.com/apache/airflow/blob/14f6622827c216a1091b24778e868598422ad9f1/hatch_build.py#L158

It doesn't happen often, but when we need to modify it for any reason (for example, as part of upgrading Sphinx version and related components - see this [PR](https://github.com/apache/airflow-site/pull/1103/files)), we need to make a lot of workarounds to make it work.
Having the option to mount local `airflow-site` directory will make it way easier to test.


### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",shahar1,2025-01-11 09:40:56+00:00,[],2025-01-11 10:36:03+00:00,,https://github.com/apache/airflow/issues/45576,"[('kind:feature', 'Feature Requests'), ('area:CI', ""Airflow's tests and continious integration""), ('area:dev-env-Breeze2', 'Breeze 2 issues')]","[{'comment_id': 2585198103, 'issue_id': 2781733342, 'author': 'potiuk', 'body': 'As discussed on slack. One of the solutions is to add it our `uv workspace` in airflow and build it from there. I might want to take a stab with that one soon-ish.', 'created_at': datetime.datetime(2025, 1, 11, 10, 36, 2, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-11 10:36:02 UTC): As discussed on slack. One of the solutions is to add it our `uv workspace` in airflow and build it from there. I might want to take a stab with that one soon-ish.

"
2780357871,issue,open,,Web UI task logging with CloudWatch has a 60 second time delay,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.1

### What happened?

When using CloudWatch logging there seems to be a 60 second time delay between the logging output updating. Please see the video below and observe:
1. Initially Airflow can't find the remote logs (as there are none).
2. Airflow detects local logs.
3. Nothing happens in the UI for 60 seconds.
4. After 60 seconds logging appears, and the top of the printout states it is from CloudWatch logs.
5. It then periodically updates the logs every 60 seconds after until the task is completed.

_Please skip ahead in the video as most of it is static!_

https://github.com/user-attachments/assets/d3abd82b-793a-475a-9488-746d640573c7

As a second minor point, you can also see that grouping now longer works with the logs read from CloudWatch. However, this doesn't concern me as much.

### What you think should happen instead?

I'm not sure if I have configured something incorrectly, but I expected the same behaviour as local logging, i.e. tailing of the log file.

I struggled to find the default behaviour documented, but what I expect to happen was that Airflow would use the local logs if they were available and only use the remote logs if no local logs were found.
I found this logic in previous [documentation (<2.0)](https://airflow.apache.org/docs/apache-airflow/1.10.8/howto/write-logs.html), although this may now be outdated:
> In the Airflow Web UI, remote logs take precedence over local logs when remote logging is enabled. If remote logs can not be found or accessed, local logs will be displayed. Note that logs are only sent to remote storage once a task is complete (including failure); In other words, remote logs for running tasks are unavailable (but local logs are available).

Can you confirm that this is the expected behaviour and what is in the video above is a bug?

Alternatively, I can see in the browser that a request is made every second to update the logs, and I can confirm that the logs are only being written to CloudWatch every 60 seconds or when the task is complete. 
Is this expected behaviour? or should logs be written to cloudwatch at a higher rate?

If the behaviour in the video is actually what is expected, I would like to suggest one of the following options as we need a <60 second refresh window in our logging setup:
1. A configuration variable to use local logging first if available (e.g. `local_logging_prefer = True`).
2. A configuration variable for the update frequency of the logging when using remote logging (e.g. `remote_logging_refresh_period = 60`).

Let me know your thoughts.

### How to reproduce

The remote logging config was copied from [here](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/logging/cloud-watch-task-handlers.html):
```
[logging]
# Airflow can store logs remotely in AWS Cloudwatch. Users must supply a log group
# ARN (starting with 'cloudwatch://...') and an Airflow connection
# id that provides write and read access to the log location.
remote_logging = True
remote_base_log_folder = cloudwatch://arn:aws:logs:<region name>:<account id>:log-group:<group name>
remote_log_conn_id = MyCloudwatchConn
```

The demo DAG used in the video above prints to logging every 10 seconds and is as follows:
```python
import logging
from datetime import datetime
from time import sleep

from airflow.models import DAG
from airflow.operators.python import task

with DAG(
    dag_id=""dev__cloudwatch_logging_testing"",
    start_date=datetime(2024, 1, 1),
    schedule=None,
):

    @task
    def task1():
        sleeptime = 10
        for i in range(0, 300, sleeptime):
            logging.info(i)
            sleep(sleeptime)

    task1()
```

### Operating System

NAME=""Ubuntu"" VERSION=""20.04.6 LTS (Focal Fossa)"" ID=ubuntu ID_LIKE=debian PRETTY_NAME=""Ubuntu 20.04.6 LTS"" VERSION_ID=""20.04"" HOME_URL=""https://www.ubuntu.com/"" SUPPORT_URL=""https://help.ubuntu.com/"" BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/"" PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"" VERSION_CODENAME=focal UBUNTU_CODENAME=focal

### Versions of Apache Airflow Providers

```
apache-airflow==2.10.1
apache-airflow-providers-amazon==8.28.0
apache-airflow-providers-celery==3.8.1
apache-airflow-providers-cncf-kubernetes==8.4.1
apache-airflow-providers-common-compat==1.2.0
apache-airflow-providers-common-io==1.4.0
apache-airflow-providers-common-sql==1.16.0
apache-airflow-providers-docker==3.13.0
apache-airflow-providers-elasticsearch==5.5.0
apache-airflow-providers-fab==1.3.0
apache-airflow-providers-ftp==3.11.0
apache-airflow-providers-google==10.22.0
apache-airflow-providers-grpc==3.6.0
apache-airflow-providers-hashicorp==3.8.0
apache-airflow-providers-http==4.13.0
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-microsoft-azure==10.4.0
apache-airflow-providers-mysql==5.7.0
apache-airflow-providers-odbc==4.7.0
apache-airflow-providers-openlineage==1.11.0
apache-airflow-providers-postgres==5.12.0
apache-airflow-providers-redis==3.8.0
apache-airflow-providers-sendgrid==3.6.0
apache-airflow-providers-sftp==4.11.0
apache-airflow-providers-slack==8.9.0
apache-airflow-providers-smtp==1.8.0
apache-airflow-providers-snowflake==5.7.0
apache-airflow-providers-sqlite==3.9.0
apache-airflow-providers-ssh==3.13.1
```

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",walter9388,2025-01-10 14:46:54+00:00,['o-nikolas'],2025-01-27 20:18:28+00:00,,https://github.com/apache/airflow/issues/45554,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:logging', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2582874707, 'issue_id': 2780357871, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 10, 14, 46, 58, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-10 14:46:58 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2780134931,issue,closed,completed,SageMakerBaseOperator character limit bug,"### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

8.27.0

### Apache Airflow version

2.7.2

### Operating System

Amazon Linux 

### Deployment

Amazon (AWS) MWAA

### Deployment details

Standard MWAA deployment - V2.7.2

### What happened

SageMakerProcessingJobs have a hard limit of 64 characters for the ProcessingJobName.
In the SageMakerBaseOperator there is a check for uniqueness for the name. 
In the case that a name is not unique it adds a timestamp to prevent a potential collision, however there is no check to prevent the updated <jobname>-<timestamp> from exceeding 64 characters. This causes the creation of the SageMakerProcessingJob to fail. 


### What you think should happen instead


In the SageMaker Pipelines SDK they truncate the base name before adding the timestamp, therefor we recommend taking  a similar approach for consistency purposes. 

### How to reproduce

Create a SageMaker ProcessingJob using the SageMakerProcessingOperator with a name of longer than 50 characters, and trigger it more than once. On the second time it is triggered the time stamp will be added and in the airflow logs it will show the error stating the sagemaker processing job failed to create due to the name exceeding the character limit 

### Anything else

Every time that a scheduled run occurs with the same name (after the first run) 

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",dirkrkotzeml,2025-01-10 12:59:09+00:00,[],2025-01-15 17:47:28+00:00,2025-01-15 17:47:28+00:00,https://github.com/apache/airflow/issues/45550,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('good first issue', '')]","[{'comment_id': 2582655685, 'issue_id': 2780134931, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 10, 12, 59, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2591165050, 'issue_id': 2780134931, 'author': 'willdanckwerts', 'body': 'Hi There, I have replicated this issue and have the associated error log to hand.\n\nWould I be able to take this on as my first issue to work on?', 'created_at': datetime.datetime(2025, 1, 14, 21, 42, 35, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-10 12:59:12 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

willdanckwerts on (2025-01-14 21:42:35 UTC): Hi There, I have replicated this issue and have the associated error log to hand.

Would I be able to take this on as my first issue to work on?

"
2780002302,issue,open,,SPIKE: Get `dag.test` working with Task SDK,"`dag.test` isn't implemented with Task SDK, let's figure out how the developer experience/ DAG developing story would look like with Task SDK",kaxil,2025-01-10 11:51:54+00:00,[],2025-01-17 15:24:22+00:00,,https://github.com/apache/airflow/issues/45549,"[('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK'), ('area:task-sdk', None)]",[],
2779203839,issue,closed,completed,Occasional Airflow tasks: Upstream tasks succeed and downstream tasks are not scheduled,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

2.8.2

### What happened?

![EB2103E2-832C-487f-A2AD-39298FC716B9](https://github.com/user-attachments/assets/c1051dc6-a0a1-4553-8a36-111501250824)
![0B813AE6-4AFB-4072-BDB4-27FEB0ECCE06](https://github.com/user-attachments/assets/aca6a086-a6dc-4dc0-99ea-6579830282d9)
As shown in the figure above, after the stat_27 is successfully executed, the scheduler should schedule the downstream tasks, but in fact there is no scheduling, this problem is occasional, but recently our production frequency is relatively high, there are workers to help as soon as possible, thank you very much

### What you think should happen instead?

Looking at the source code, it seems that the logic of the scheduling check block has just been modified on October 23rd.,There's a labor gang to take a look.

### How to reproduce

I can't reproduce it for the time being, this problem is occasional, it should be a problem in the code logic

### Operating System

Ubuntu

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Airflow 2.10.4 deployed by K8S + Helm

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",lzf12,2025-01-10 03:37:54+00:00,[],2025-01-10 08:40:17+00:00,2025-01-10 08:40:17+00:00,https://github.com/apache/airflow/issues/45536,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2581677257, 'issue_id': 2779203839, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 10, 3, 37, 57, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-10 03:37:57 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2778798102,issue,closed,completed,Allow `FileTaskHandler` get running tasks logs not only from default executor,"### Description

`FileTaskHandler` should have the ability to get logs not only from the default executor, but from one configured on the task instance

### Use case/motivation

With the introduction of multiple executors flow of the `FileTaskHandler` log reading of the running task was broken if the task was run by a non-default executor.


For example, usage of ""LocalExecutor,KubernetesExecutor"" leads to an inability of online logs reading in web interface. 

I was able to overcome it with a custom executor workaround, but I belive that this would be a reasonable feature to expect with multiple executors. 

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",insomnes,2025-01-09 21:39:52+00:00,[],2025-01-25 07:50:22+00:00,2025-01-24 02:58:46+00:00,https://github.com/apache/airflow/issues/45529,"[('area:logging', ''), ('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2581303899, 'issue_id': 2778798102, 'author': 'insomnes', 'body': 'I am talking about this [line](https://github.com/apache/airflow/blob/main/airflow/utils/log/file_task_handler.py#L362):\r\n```python\r\n        if ti.state == TaskInstanceState.RUNNING:\r\n            response = self._executor_get_task_log(ti, try_number)\r\n\r\n...\r\n\r\n    @cached_property\r\n    def _executor_get_task_log(self) -> Callable[[TaskInstance, int], tuple[list[str], list[str]]]:\r\n        """"""This cached property avoids loading executor repeatedly.""""""\r\n        executor = ExecutorLoader.get_default_executor()\r\n        return executor.get_task_log\r\n```\r\n\r\nI\'m not very familiar with `ExecutorLoader` and maybe the real culprit is `@cached_property` usage, but this needs some rework for multiple executors support', 'created_at': datetime.datetime(2025, 1, 9, 21, 45, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2582868483, 'issue_id': 2778798102, 'author': 'jason810496', 'body': ""I've been working on refactoring the log reading process recently, also focusing on `FileTaskHandler`.\r\nI believe this can be achieved by reading directly from `ti.executor` instead of using `ExecutorLoader` to determine the correct executor method."", 'created_at': datetime.datetime(2025, 1, 10, 14, 44, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585594282, 'issue_id': 2778798102, 'author': 'eladkal', 'body': 'cc @o-nikolas wdyt?', 'created_at': datetime.datetime(2025, 1, 12, 5, 29, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2588254868, 'issue_id': 2778798102, 'author': 'o-nikolas', 'body': ""Hey @jason810496 thanks for the bug report! This does indeed look like it needs fixing. And you're correct that we should be looking at the current ti's executor, if it has one, otherwise the default.\n\nYou mentioned you had a workaround for this, is that something you can share or would like to contribute? Otherwise I can get started on a fix for this shortly."", 'created_at': datetime.datetime(2025, 1, 13, 21, 28, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2588561291, 'issue_id': 2778798102, 'author': 'jason810496', 'body': '> You mentioned you had a workaround for this, is that something you can share or would like to contribute? Otherwise I can get started on a fix for this shortly.\n\n\nHi @o-nikolas, I can fix this issue!  \n\nBy the way, the refactoring I mentioned previously is not related to this issue. It focuses on solving OOM when reading large task logs (#45129).', 'created_at': datetime.datetime(2025, 1, 14, 1, 25, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2590097952, 'issue_id': 2778798102, 'author': 'nradz', 'body': 'Hello! I has also experimented this issue and I has fixed it with the following code, if it could help in the PR. I try to maintain the ""cached_property"", assuming that load the executors is costly. Also, I don\'t know if _init_executors_ is the most ""elegant"" way to retrieve the executors, but it is the only function that I have found to do it.\n\n```python\n        def _executor_get_task_log(self, ti: TaskInstance, try_number: int) -> tuple[list[str], list[str]]:\n            """"""\n            Get logs from executor.\n\n            :param ti: task instance object\n            :param try_number: task instance try_number to read logs from\n            :return: a tuple of messages and logs\n            """"""\n            executor_name = ti.executor\n            if executor_name is None:\n                executor = list(self._available_executors.values())[0]\n            elif executor_name in self._available_executors:\n                executor = self._available_executors[executor_name]\n            else:\n                raise AirflowException(f""Executor {executor_name} not found for task {ti}"")\n\n            return executor.get_task_log(ti, try_number)\n\n        @cached_property\n        def _available_executors(self) -> Dict[str, BaseExecutor]:\n            """"""This cached property avoids loading executors repeatedly.""""""\n            return {\n                ex.__class__.__name__: ex\n                for ex in ExecutorLoader.init_executors()\n            }\n```', 'created_at': datetime.datetime(2025, 1, 14, 14, 38, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2591091467, 'issue_id': 2778798102, 'author': 'o-nikolas', 'body': ""> Hi @o-nikolas, I can fix this issue!\n\n> By the way, the refactoring I mentioned previously is not related to this issue. It focuses on solving OOM when reading large task logs (https://github.com/apache/airflow/pull/45129).\n\nRight, perhaps we can divide and conquer then if you're working on the OOM issue then either myself or @nradz can solve this one.\n\n> Hello! I has also experimented this issue and I has fixed it with the following code, if it could help in the PR.\n\nThanks for sharing this snippet! I'll work on a PR and perhaps use some of this snippet!"", 'created_at': datetime.datetime(2025, 1, 14, 20, 59, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2591588678, 'issue_id': 2778798102, 'author': 'jason810496', 'body': '> Also, I don\'t know if init_executors is the most ""elegant"" way to retrieve the executors, but it is the only function that I have found to do it.\n\nThanks @nradz, I think your solution is more cleaner and also avoid loading executor repeatedly, just adapt that for fixing this issue !', 'created_at': datetime.datetime(2025, 1, 15, 3, 52, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2613830130, 'issue_id': 2778798102, 'author': 'insomnes', 'body': 'Hey @jason810496 Thanks a lot for implementing this. It would be nice to drop the pile of workaround code I wrote to address this problem.', 'created_at': datetime.datetime(2025, 1, 25, 7, 50, 21, tzinfo=datetime.timezone.utc)}]","insomnes (Issue Creator) on (2025-01-09 21:45:24 UTC): I am talking about this [line](https://github.com/apache/airflow/blob/main/airflow/utils/log/file_task_handler.py#L362):
```python
        if ti.state == TaskInstanceState.RUNNING:
            response = self._executor_get_task_log(ti, try_number)

...

    @cached_property
    def _executor_get_task_log(self) -> Callable[[TaskInstance, int], tuple[list[str], list[str]]]:
        """"""This cached property avoids loading executor repeatedly.""""""
        executor = ExecutorLoader.get_default_executor()
        return executor.get_task_log
```

I'm not very familiar with `ExecutorLoader` and maybe the real culprit is `@cached_property` usage, but this needs some rework for multiple executors support

jason810496 on (2025-01-10 14:44:16 UTC): I've been working on refactoring the log reading process recently, also focusing on `FileTaskHandler`.
I believe this can be achieved by reading directly from `ti.executor` instead of using `ExecutorLoader` to determine the correct executor method.

eladkal on (2025-01-12 05:29:54 UTC): cc @o-nikolas wdyt?

o-nikolas on (2025-01-13 21:28:38 UTC): Hey @jason810496 thanks for the bug report! This does indeed look like it needs fixing. And you're correct that we should be looking at the current ti's executor, if it has one, otherwise the default.

You mentioned you had a workaround for this, is that something you can share or would like to contribute? Otherwise I can get started on a fix for this shortly.

jason810496 on (2025-01-14 01:25:42 UTC): Hi @o-nikolas, I can fix this issue!  

By the way, the refactoring I mentioned previously is not related to this issue. It focuses on solving OOM when reading large task logs (#45129).

nradz on (2025-01-14 14:38:01 UTC): Hello! I has also experimented this issue and I has fixed it with the following code, if it could help in the PR. I try to maintain the ""cached_property"", assuming that load the executors is costly. Also, I don't know if _init_executors_ is the most ""elegant"" way to retrieve the executors, but it is the only function that I have found to do it.

```python
        def _executor_get_task_log(self, ti: TaskInstance, try_number: int) -> tuple[list[str], list[str]]:
            """"""
            Get logs from executor.

            :param ti: task instance object
            :param try_number: task instance try_number to read logs from
            :return: a tuple of messages and logs
            """"""
            executor_name = ti.executor
            if executor_name is None:
                executor = list(self._available_executors.values())[0]
            elif executor_name in self._available_executors:
                executor = self._available_executors[executor_name]
            else:
                raise AirflowException(f""Executor {executor_name} not found for task {ti}"")

            return executor.get_task_log(ti, try_number)

        @cached_property
        def _available_executors(self) -> Dict[str, BaseExecutor]:
            """"""This cached property avoids loading executors repeatedly.""""""
            return {
                ex.__class__.__name__: ex
                for ex in ExecutorLoader.init_executors()
            }
```

o-nikolas on (2025-01-14 20:59:20 UTC): Right, perhaps we can divide and conquer then if you're working on the OOM issue then either myself or @nradz can solve this one.


Thanks for sharing this snippet! I'll work on a PR and perhaps use some of this snippet!

jason810496 on (2025-01-15 03:52:45 UTC): Thanks @nradz, I think your solution is more cleaner and also avoid loading executor repeatedly, just adapt that for fixing this issue !

insomnes (Issue Creator) on (2025-01-25 07:50:21 UTC): Hey @jason810496 Thanks a lot for implementing this. It would be nice to drop the pile of workaround code I wrote to address this problem.

"
2778595124,issue,open,,Update DagVersion instead of stamping new one if there are no DAG runs using that version,"### Body

We should only keep around DAG versions that have a corresponding run. I believe the easy answer is to allow DAGs to update in place, without stamping a new version, if there are no runs on that version.

 We should be careful of race conditions while we are at it.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2025-01-09 19:21:26+00:00,[],2025-02-08 05:44:59+00:00,,https://github.com/apache/airflow/issues/45524,"[('area:MetaDB', 'Meta Database related issues.'), ('kind:meta', 'High-level information important to the community'), ('AIP-65: DAG history in UI', '')]","[{'comment_id': 2644521661, 'issue_id': 2778595124, 'author': 'vatsrahul1001', 'body': 'closed issue https://github.com/apache/airflow/issues/44473 in favour of this issue. As this will handle better.', 'created_at': datetime.datetime(2025, 2, 8, 5, 44, 48, tzinfo=datetime.timezone.utc)}]","vatsrahul1001 on (2025-02-08 05:44:48 UTC): closed issue https://github.com/apache/airflow/issues/44473 in favour of this issue. As this will handle better.

"
2778271070,issue,closed,completed,Could not read served logs: Invalid URL 'http://:8793....': Not host supplied,"### Official Helm Chart version

1.15.0 (latest released)

### Apache Airflow version

2.9.3

### Kubernetes Version

Client Version: v1.30.2 Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3 Server Version: v1.30.6-eks-7f9249a

### Helm Chart configuration

I deployed Airflow using the Helm chart on the AWS Kubernetes cluster, made some changes like using an existing Redis and synchronizing with GitHub. However, my DAG always gives this log error. I saw that it could be the base_url; mine is set to http://airflow-webserver.airflow.svc.cluster.local:8080/. I tried updating my Airflow version to a more recent one in the values.yaml file, but my webserver pod throws an error when I do that.
![Captura de tela 2025-01-06 143256](https://github.com/user-attachments/assets/5c280180-d7b7-4a03-9148-02044df321da)



### Docker Image customizations

_No response_

### What happened

_No response_

### What you think should happen instead

_No response_

### How to reproduce

Using the helm chart at the link: https://airflow.apache.org/docs/helm-chart/stable/index.html

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",stefanikjabuti,2025-01-09 16:30:10+00:00,[],2025-01-09 19:42:10+00:00,2025-01-09 19:42:10+00:00,https://github.com/apache/airflow/issues/45519,"[('kind:bug', 'This is a clearly a bug'), ('area:helm-chart', 'Airflow Helm Chart'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2580744265, 'issue_id': 2778271070, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 9, 16, 30, 14, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-09 16:30:14 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2778128646,issue,closed,completed,KubernetesExecutor shadowing timeout_seconds and _request_seconds,"### Apache Airflow Provider(s)

cncf-kubernetes

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes==10.0.1

### Apache Airflow version

2.10.4

### Operating System

Debian Bookworm

### Deployment

Other

### Deployment details

It's deployment independent and related to code directly

### What happened

kube client query options of `timeout_seconds` and `_request_seconds` provided via `kube_client_request_args` config option are re-written via hardcoded values in executor code of `KubernetesJobWatcher`. 

### What you think should happen instead

Values set via `AIRFLOW__KUBERNETES_EXECUTOR__KUBE_CLIENT_REQUEST_ARGS`  for `timeout_seconds` and `_request_seconds` should be respected, hardcoded values should be provided only in case of missing keys in the provided JSON.

Simple if for checking if key is already present would be enough

### How to reproduce

1. Set the [kube_client_request_args](https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/stable/configurations-ref.html#kube-client-request-args) option with JSON: `{""timeout_seconds"":  1800, ""_request_timeout"": 60}`
2. Run airflow scheduler with kubernetes executor configured
3. In logs you can see that without pod events the `KubernetesJobWatcher` will still die every 30 seconds instead of 60.

### Anything else

The problem was introduced with:
https://github.com/apache/airflow/commit/610747d25a6153574c07624afaadcbf575aa2960#diff-d884d637ab746b1301ce80b30ba2c1908299ba6f13edcad6535c837fb4d30938R151

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",insomnes,2025-01-09 15:26:53+00:00,['insomnes'],2025-01-13 21:31:33+00:00,2025-01-13 16:56:36+00:00,https://github.com/apache/airflow/issues/45517,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]","[{'comment_id': 2580586649, 'issue_id': 2778128646, 'author': 'insomnes', 'body': ""I am happy to provide the PR fixing it. \r\n\r\nI see this behavior as a 100% bug. But I want to be sure it's not intended in any way."", 'created_at': datetime.datetime(2025, 1, 9, 15, 35, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2580704350, 'issue_id': 2778128646, 'author': 'RNHTTR', 'body': 'Is the idea to update these lines to default to 30/3600 but allow them to be overwritten via kube_client_request_args? If so, I think that makes sense.', 'created_at': datetime.datetime(2025, 1, 9, 16, 17, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2580722237, 'issue_id': 2778128646, 'author': 'insomnes', 'body': 'Yes. I was thinking about checking the presence of these keys:\r\n\r\n```python\r\n        if ""_request_timeout"" not in kwargs:\r\n            client_timeout = 30\r\n            kwargs[""_request_timeout""] = client_timeout\r\n```', 'created_at': datetime.datetime(2025, 1, 9, 16, 23, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2580798313, 'issue_id': 2778128646, 'author': 'RNHTTR', 'body': 'Cool -- assigned you.', 'created_at': datetime.datetime(2025, 1, 9, 16, 48, 10, tzinfo=datetime.timezone.utc)}]","insomnes (Issue Creator) on (2025-01-09 15:35:57 UTC): I am happy to provide the PR fixing it. 

I see this behavior as a 100% bug. But I want to be sure it's not intended in any way.

RNHTTR on (2025-01-09 16:17:36 UTC): Is the idea to update these lines to default to 30/3600 but allow them to be overwritten via kube_client_request_args? If so, I think that makes sense.

insomnes (Issue Creator) on (2025-01-09 16:23:16 UTC): Yes. I was thinking about checking the presence of these keys:

```python
        if ""_request_timeout"" not in kwargs:
            client_timeout = 30
            kwargs[""_request_timeout""] = client_timeout
```

RNHTTR on (2025-01-09 16:48:10 UTC): Cool -- assigned you.

"
2778082570,issue,closed,completed,Unable to see logs in the web UI when the job is running,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.3

### What happened?

Since our migration from Airflow 2.4.3 to 2.9.3 and then to 2.10.3, we have noticed that it has become impossible to access logs via the web UI or the Rest API for a running Task instance.

We run our Airflow instance within the in-house k8s infrastructure, using S3 as our remote logging end.

When the Task instance completes its run, the remote log is visible through the web UI. In v2.4.3 for the same params we never encountered similar issues. Here are our logging config section:

```ini
[logging]
base_log_folder = /opt/airflow/logs
remote_logging = True
remote_log_conn_id = s3_airflow_logs
delete_local_logs = False
google_key_path = 
remote_base_log_folder = s3://the-bucket
remote_task_handler_kwargs = 
encrypt_s3_logs = False
logging_level = INFO
celery_logging_level = 
fab_logging_level = WARNING
logging_config_class = 
colored_console_log = False
colored_log_format = [%%(blue)s%%(asctime)s%%(reset)s] {%%(blue)s%%(filename)s:%%(reset)s%%(lineno)d} %%(log_color)s%%(levelname)s%%(reset)s - %%(log_color)s%%(message)s%%(reset)s
colored_formatter_class = airflow.utils.log.colored_log.CustomTTYColoredFormatter
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s
dag_processor_log_target = file
dag_processor_log_format = [%%(asctime)s] [SOURCE:DAG_PROCESSOR] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
log_formatter_class = airflow.utils.log.timezone_aware.TimezoneAware
secret_mask_adapter = 
task_log_prefix_template = 
log_filename_template = dag_id={{ ti.dag_id }}/run_id={{ ti.run_id }}/task_id={{ ti.task_id }}/{%% if ti.map_index >= 0 %%}map_index={{ ti.map_index }}/{%% endif %%}attempt={{ try_number }}.log
log_processor_filename_template = {{ filename }}.log
dag_processor_manager_log_location = /opt/airflow/logs/dag_processor_manager/dag_processor_manager.log
dag_processor_manager_log_stdout = False
task_log_reader = task
extra_logger_names = 
worker_log_server_port = 8793
trigger_log_server_port = 8794
# interleave_timestamp_parser = 
file_task_handler_new_folder_permissions = 0o775
file_task_handler_new_file_permissions = 0o664
celery_stdout_stderr_separation = False
enable_task_context_logger = True
color_log_error_keywords = error,exception
color_log_warning_keywords = warn
```

When we try to access the logs for the running task, we see the following text with no content:
![image](https://github.com/user-attachments/assets/791f3c4b-76e1-454f-89ff-e5dad6b665d6)

Same result for already finalized task attempts:
![image](https://github.com/user-attachments/assets/7bad069e-9536-4a6e-988d-4c4079a14b62)

When we try to get the logs via the REST API (`/api/v1/dags/MY-DAG1/dagRuns/manual__DATE/taskInstances/MY-TASK/logs/8?full_content=false`) after long waiting, we get a time-out exception and following page:
![image](https://github.com/user-attachments/assets/055c9ee1-f485-43e2-ae30-1fec74d661b0)



### What you think should happen instead?

If we check the webserver logs we notice the following exceptions:
```
Traceback (most recent call last):
  File ""/my-path/python3.10/site-packages/flask/app.py"", line 2529, in wsgi_app
    response = self.full_dispatch_request()
  File ""/my-path/python3.10/site-packages/flask/app.py"", line 1825, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/my-path/python3.10/site-packages/flask/app.py"", line 1823, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/my-path/python3.10/site-packages/flask/app.py"", line 1799, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
  File ""/my-path/python3.10/site-packages/connexion/decorators/decorator.py"", line 68, in wrapper
    response = function(request)
  File ""/my-path/python3.10/site-packages/connexion/decorators/uri_parsing.py"", line 149, in wrapper
    response = function(request)
  File ""/my-path/python3.10/site-packages/connexion/decorators/validation.py"", line 399, in wrapper
    return function(request)
  File ""/my-path/python3.10/site-packages/connexion/decorators/response.py"", line 113, in wrapper
    return _wrapper(request, response)
  File ""/my-path/python3.10/site-packages/connexion/decorators/response.py"", line 90, in _wrapper
    self.operation.api.get_connexion_response(response, self.mimetype)
  File ""/my-path/python3.10/site-packages/connexion/apis/abstract.py"", line 366, in get_connexion_response
    return cls._framework_to_connexion_response(response=response, mimetype=mimetype)
  File ""/my-path/python3.10/site-packages/connexion/apis/flask_api.py"", line 165, in _framework_to_connexion_response
    body=response.get_data() if not response.direct_passthrough else None,
  File ""/my-path/python3.10/site-packages/werkzeug/wrappers/response.py"", line 314, in get_data
    self._ensure_sequence()
  File ""/my-path/python3.10/site-packages/werkzeug/wrappers/response.py"", line 376, in _ensure_sequence
    self.make_sequence()
  File ""/my-path/python3.10/site-packages/werkzeug/wrappers/response.py"", line 391, in make_sequence
    self.response = list(self.iter_encoded())
  File ""/my-path/python3.10/site-packages/werkzeug/wrappers/response.py"", line 50, in _iter_encoded
    for item in iterable:
  File ""/my-path/python3.10/site-packages/airflow/utils/log/log_reader.py"", line 94, in read_log_stream
    logs, metadata = self.read_log_chunks(ti, current_try_number, metadata)
  File ""/my-path/python3.10/site-packages/airflow/utils/log/log_reader.py"", line 66, in read_log_chunks
    logs, metadatas = self.log_handler.read(ti, try_number, metadata=metadata)
  File ""/my-path/python3.10/site-packages/airflow/utils/log/file_task_handler.py"", line 491, in read
    log, out_metadata = self._read(task_instance, try_number_element, metadata)
  File ""/my-path/python3.10/site-packages/airflow/utils/log/file_task_handler.py"", line 389, in _read
    response = self._executor_get_task_log(ti, try_number)
  File ""/my-path/python3.10/functools.py"", line 981, in __get__
    val = self.func(instance)
  File ""/my-path/python3.10/site-packages/airflow/utils/log/file_task_handler.py"", line 346, in _executor_get_task_log
    executor = ExecutorLoader.get_default_executor()
  File ""/my-path/python3.10/site-packages/airflow/executors/executor_loader.py"", line 165, in get_default_executor
    default_executor = cls.load_executor(cls.get_default_executor_name())
  File ""/my-path/python3.10/site-packages/airflow/executors/executor_loader.py"", line 246, in load_executor
    executor = executor_cls()
  File ""/my-path/python3.10/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py"", line 135, in __init__
    self.task_queue: Queue[KubernetesJobType] = self._manager.Queue()
  File ""/my-path/python3.10/multiprocessing/managers.py"", line 723, in temp
    token, exp = self._create(typeid, *args, **kwds)
  File ""/my-path/python3.10/multiprocessing/managers.py"", line 606, in _create
    conn = self._Client(self._address, authkey=self._authkey)
  File ""/my-path/python3.10/multiprocessing/connection.py"", line 508, in Client
    answer_challenge(c, authkey)
  File ""/my-path/python3.10/multiprocessing/connection.py"", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File ""/my-path/python3.10/multiprocessing/connection.py"", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File ""/my-path/python3.10/multiprocessing/connection.py"", line 414, in _recv_bytes
    buf = self._recv(4)
  File ""/my-path/python3.10/multiprocessing/connection.py"", line 379, in _recv
    chunk = read(handle, remaining)
BlockingIOError: [Errno 11] Resource temporarily unavailable
*.*.*.* - - [???? +0000] ""GET /api/v1/dags/MY-DAG1/dagRuns/manual__DATE/taskInstances/MY-TASK/logs/8?full_content=false HTTP/1.1"" 500 1589 ""-"" ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:133.0) Gecko/20100101 Firefox/133.0""
```
What we notice is that the `s3_task_handler` does its part of the job correctly, for a running task it gets the s3 content and if there is no content it clearly says `No logs found on s3 for ti=<TaskInstance: ...` The problem starts when we try to get stdout for the running k8s pod, as shown above it ends with `BlockingIOError - Resource temporarily unavailable`. It all fails in `file_task_handler` within `_read` method:

```
log, out_metadata = self._read(task_instance, try_number_element, metadata)
```

It looks like this problem has been around for several minor releases.

### How to reproduce

You need to deploy an instance of airflow within a k8s cluster with remote logs activated, it should be enough. For solving another issue related to the remote logging, we set up following env vars(not sure if it's relevant):
```
_AIRFLOW_PATCH_GEVENT=1
AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES___AIRFLOW_PATCH_GEVENT=1
```

### Operating System

Debian GNU/Linux trixie/sid

### Versions of Apache Airflow Providers

```
apache-airflow-providers-amazon==9.0.0
apache-airflow-providers-apache-cassandra==3.6.0
apache-airflow-providers-apache-druid==3.12.0
apache-airflow-providers-apache-hdfs==4.6.0
apache-airflow-providers-apache-hive==8.2.0
apache-airflow-providers-apache-kylin==3.7.0
apache-airflow-providers-apache-livy==3.9.2
apache-airflow-providers-apache-spark==5.0.0
apache-airflow-providers-celery==3.8.3
apache-airflow-providers-cncf-kubernetes==9.0.1
apache-airflow-providers-common-compat==1.2.1
apache-airflow-providers-common-io==1.4.2
apache-airflow-providers-common-sql==1.19.0
apache-airflow-providers-fab==1.5.0
apache-airflow-providers-ftp==3.11.1
apache-airflow-providers-google==10.25.0
apache-airflow-providers-grpc==3.6.0
apache-airflow-providers-hashicorp==3.8.0
apache-airflow-providers-http==4.13.2
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-microsoft-azure==11.0.0
apache-airflow-providers-microsoft-mssql==3.9.1
apache-airflow-providers-microsoft-winrm==3.6.0
apache-airflow-providers-mongo==4.2.2
apache-airflow-providers-papermill==3.8.1
apache-airflow-providers-postgres==5.13.1
apache-airflow-providers-sftp==4.11.1
apache-airflow-providers-slack==8.9.1
apache-airflow-providers-smtp==1.8.0
apache-airflow-providers-sqlite==3.9.0
apache-airflow-providers-ssh==3.14.0

```

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Kube version: `v1.30.4`
Helm: `version.BuildInfo{Version:""v3.15.2"", GitCommit:""1a500d5625419a524fdae4b33de351cc4f58ec35"", GitTreeState:""clean"", GoVersion:""go1.22.4""}`

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vba,2025-01-09 15:07:01+00:00,[],2025-01-13 18:58:14+00:00,2025-01-13 18:58:14+00:00,https://github.com/apache/airflow/issues/45516,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:webserver', 'Webserver related Issues'), ('area:logging', ''), ('pending-response', ''), ('area:core', '')]","[{'comment_id': 2580513979, 'issue_id': 2778082570, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 9, 15, 7, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2582131026, 'issue_id': 2778082570, 'author': 'potiuk', 'body': 'Can you please explain what executir and what log volume configuration you have? \r\n\r\nI believe it might have something to do with the the volume you are using to store the logs. This looks very much like the volume does not allow to concurrently write and read files from. I think it would be great if you could check that and see what type of volume you have there.\r\n\r\nOr @dstandish -> do you think that might be related to the logging change you implemented? Maybe that rings a bell too?', 'created_at': datetime.datetime(2025, 1, 10, 9, 11, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2582873585, 'issue_id': 2778082570, 'author': 'jason810496', 'body': 'Could this issue be related to #45529? The error log traceback also references `executor_loader`.  \r\nIf the TaskInstance is still in `RUNNING` stage, the file task handler will read from `_executor_get_task_log` instead of reading from `_read_from_logs_server`.', 'created_at': datetime.datetime(2025, 1, 10, 14, 46, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2587306687, 'issue_id': 2778082570, 'author': 'vba', 'body': 'Hi @potiuk, \n\n> Can you please explain what executir and what log volume configuration you have?\n\nAs you can see in the stacktrace in the middle of my issue, the executor is `kubernetes_executor` and as for the volume used for logging, here\'s the configuration (as shown in the issue):\n\n```ini\nbase_log_folder = /opt/airflow/logs\nremote_logging = True\nremote_log_conn_id = s3_airflow_logs\ndelete_local_logs = False\ngoogle_key_path = \nremote_base_log_folder = s3://the-bucket\n```\n\nhere is how it\'s run by the k8s cluster:\n\n```yml\n# ...\nspec:\n  containers:\n  - volumeMounts:\n    - mountPath: /opt/airflow/logs\n      name: logs\n# ...\n  volumes\n  - emptyDir:\n      sizeLimit: 10Gi\n    name: logs\n```\n\n> I believe it might have something to do with the the volume you are using to store the logs. This looks very much like the volume does not allow to concurrently write and read files from. I think it would be great if you could check that and see what type of volume you have there.\n\nI don\'t think my problem is purely a configuration issue. If I downgrade my airflow instance to version `2.4.3`, everything works fine for the same k8s infrastructure. A silly test reveals no difficulty:\n```\necho ""my test"" > /opt/airflow/logs/log.txt && cat /opt/airflow/logs/log.txt\n...\nmy test\n```', 'created_at': datetime.datetime(2025, 1, 13, 14, 48, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2587322575, 'issue_id': 2778082570, 'author': 'vba', 'body': 'Hi @jason810496 \n\n> Could this issue be related to [#45529](https://github.com/apache/airflow/issues/45529)? The error log traceback also references `executor_loader`. If the TaskInstance is still in `RUNNING` stage, the file task handler will read from `_executor_get_task_log` instead of reading from `_read_from_logs_server`.\n\nSorry, I was not clear in my previous message, the problem only occurs when a job and its tasks of interest are to be run, which assumes that its state is `RUNNING`, once the job is finished all logs for the last active task instance regain their visibility. I think this issue was introduced with the version `2.6` or `2.7`.', 'created_at': datetime.datetime(2025, 1, 13, 14, 53, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2587461784, 'issue_id': 2778082570, 'author': 'potiuk', 'body': '> As you can see in the stacktrace in the middle of my issue, the executor is kubernetes_executor \n\nThanks. that saves a bit of searching through a stack trace. In the future might be better to specify it explicitly rather than leave a chnce that somoene will find it. It allows for people who look at it and try to help to quickly assess whether they can help or whether the case ""rings a bell"" without actually spending time and looking at such details. It simply optimizes for time of those who try to help you to solve your problem\n\n>  and as for the volume used for logging, here\'s the configuration (as shown in the issue):\n\nI was more thinking - what are properties of the volume you have. Something that you can look at your K8S way of handling volumes of the specific kind you use. The error indicates, that somewhere during receiving logs you get ""resource unavailable"" error. After looking at this - it seems that somewhere the k8s reads logs from remote pod and something does not let it read it.\n\nAnd  I think in this case it\'s something in your K8S configuration., There is a similar issue https://github.com/kubernetes/kubernetes/issues/105928  which indicates that somewhere logs are filing space - for example containerd version used has a problem.\n\nAnd yes - I think the way how logs are read has changed between versions of k8s provider - you can take a look at the changelog - so maybe you had uncovered a configuration or another issue in your K8S. Maybe you can try to see your k8s logs correlating with the events and see if you have some other errors in other components of K8S that indicate what is a root cause.\n\nUnfortunately k8s has 100s of moving parts and sometimes you need to dig deeper to find out the root causes (for example often problems - very strange) might occur when your DNS does not have enough resources to respond on time, and the only way to see what\'s going on is to generally look at what happens in your K8S and see potential issues that are correlated with the event.\n\nBut I am mostly guessing here - I wanted to help and direct the discussion but I have no deep knowledge on this particular part.', 'created_at': datetime.datetime(2025, 1, 13, 15, 39, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2587728230, 'issue_id': 2778082570, 'author': 'vba', 'body': 'Hi @potiuk,\n\nThanks for your reply.\n\n>  It simply optimizes for time of those who try to help you to solve your problem.\n\nNo pb, as the subject is very complex, I didn\'t know how to present the key elements.\n\n> it seems that somewhere the k8s reads logs from remote pod and something does not let it read it.\n\nI\'ve actually researched this problem quite a bit. Firstly, if I roll back to Airflow 2.4.3, the problem disappears. Another thing is that I\'ve patched the Airflow code with `icecream`, trying to understand the problem step by step. In the log below, I call the API for the task instance that is running, you will notice that the remote log was fetched, and the problem starts afterwards :\n\n```\n10.*.*.* - - [09/Jan/2025:10:23:02 +0000] ""GET /api/v1/dags/code-python-airflow-sample/dagRuns/manual__2024-12-27T20:29:34.637180+00:00/taskInstances/python_task/logs/8?full_content=false HTTP/1.1"" 500 1589 ""https://airflow1-sandbox-dv.numberly.dev/dags/code-python-airflow-sample/grid?dag_run_id=manual__2024-12-27T20%3A29%3A34.637180%2B00%3A00&tab=logs&task_id=python_task"" ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:133.0) Gecko/20100101 Firefox/133.0""\n[2025-01-09 10:23:02 +0000] [70] [INFO] Parent changed, shutting down: <Worker 61>\n[2025-01-09 10:23:02 +0000] [70] [INFO] Worker exiting (pid: 61)\n10.*.*.* - - [09/Jan/2025:10:23:03 +0000] ""GET /health HTTP/1.1"" 200 283 ""-"" ""kube-probe/1.30""\n10.*.*.* - - [09/Jan/2025:10:23:03 +0000] ""GET /health HTTP/1.1"" 200 283 ""-"" ""kube-probe/1.30""\nLOG ISSUE DEBUG -> log_endpoint.py:60 in get_log()\n                   ""Starting #get_log"": \'Starting #get_log\'\nLOG ISSUE DEBUG -> log_endpoint.py:62 in get_log()\n                   key: \'****************\'\nLOG ISSUE DEBUG -> log_endpoint.py:63 in get_log()- token: None\nLOG ISSUE DEBUG -> log_endpoint.py:71 in get_log()- metadata: {}\nLOG ISSUE DEBUG -> log_endpoint.py:80 in get_log()\n                   metadata: {\'download_logs\': False}\nLOG ISSUE DEBUG -> log_endpoint.py:82 in get_log()\n                   task_log_reader: <airflow.utils.log.log_reader.TaskLogReader object at 0x7f539c1ed510>\nLOG ISSUE DEBUG -> log_reader.py:119 in log_handler()\n                   task_log_reader: \'task\'\nLOG ISSUE DEBUG -> log_reader.py:120 in log_handler()\n                   logging.getLogger(""airflow.task"").handlers: [<S3TaskHandler (NOTSET)>]\nLOG ISSUE DEBUG -> log_reader.py:121 in log_handler()\n                   logging.getLogger().handlers: [<RedirectStdHandler <stdout> (NOTSET)>]\nLOG ISSUE DEBUG -> log_endpoint.py:96 in get_log()\n                   ti: <TaskInstance: code-python-airflow-sample.python_task manual__2024-12-27T20:29:34.637180+00:00 [running]>\nLOG ISSUE DEBUG -> log_endpoint.py:103 in get_log()\n                   dag: <DAG: code-python-airflow-sample>\nLOG ISSUE DEBUG -> log_endpoint.py:107 in get_log()\n                   ti.task: <Task(PythonOperator): python_task>\nLOG ISSUE DEBUG -> log_endpoint.py:112 in get_log()\n                   return_type: \'text/plain\'\nLOG ISSUE DEBUG -> log_endpoint.py:128 in get_log()\n                   logs: <generator object TaskLogReader.read_log_stream at 0x7f5396d165e0>\nLOG ISSUE DEBUG -> log_endpoint.py:130 in get_log()- \'Ending\'\nLOG ISSUE DEBUG -> log_reader.py:78 in read_log_stream()\n                   ""Starting #read_log_stream"": \'Starting #read_log_stream\'\nLOG ISSUE DEBUG -> log_reader.py:84 in read_log_stream()\n                   try_numbers: [8]\nLOG ISSUE DEBUG -> log_reader.py:85 in read_log_stream()\n                   ti: <TaskInstance: code-python-airflow-sample.python_task manual__2024-12-27T20:29:34.637180+00:00 [running]>\nLOG ISSUE DEBUG -> log_reader.py:86 in read_log_stream()\n                   metadata: {\'download_logs\': False}\nLOG ISSUE DEBUG -> log_reader.py:92 in read_log_stream()\n                   metadata: {\'download_logs\': False}\nLOG ISSUE DEBUG -> log_reader.py:65 in read_log_chunks()\n                   self.log_handler: <S3TaskHandler (NOTSET)>\nLOG ISSUE DEBUG -> s3_task_handler.py:122 in _read_remote_logs()\n                   ti: <TaskInstance: code-python-airflow-sample.python_task manual__2024-12-27T20:29:34.637180+00:00 [running]>\nLOG ISSUE DEBUG -> s3_task_handler.py:123 in _read_remote_logs()\n                   metadata: {\'download_logs\': False}\nLOG ISSUE DEBUG -> s3_task_handler.py:125 in _read_remote_logs()\n                   worker_log_rel_path: \'dag_id=code-python-airflow-sample/run_id=manual__2024-12-27T20:29:34.637180+00:00/task_id=python_task/attempt=8.log\'\nLOG ISSUE DEBUG -> s3_task_handler.py:130 in _read_remote_logs()\n                   bucket: \'my-bucket-logs\'\nLOG ISSUE DEBUG -> s3_task_handler.py:131 in _read_remote_logs()\n                   prefix: \'airflow1-sandbox-dv/dag_id=code-python-airflow-sample/run_id=manual__2024-12-27T20:29:34.637180+00:00/task_id=python_task/attempt=8.log\'\nLOG ISSUE DEBUG -> s3_task_handler.py:133 in _read_remote_logs()\n                   keys: [\'airflow1-sandbox-dv/dag_id=code-python-airflow-sample/run_id=manual__2024-12-27T20:29:34.637180+00:00/task_id=python_task/attempt=8.log\']\nLOG ISSUE DEBUG -> s3_task_handler.py:142 in _read_remote_logs()\n                   messages: [\'Found logs in s3:\',\n                              \'  * \'\n                              \'s3://my-bucket-logs/airflow1-sandbox-dv/dag_id=code-python-airflow-sample/run_id=manual__2024-12-27T20:29:34.637180+00:00/task_id=python_task/attempt=8.log\']\nLOG ISSUE DEBUG -> s3_task_handler.py:143 in _read_remote_logs()\n                   logs: [\'[2025-01-08T19:29:02.537+0000] {local_task_job_runner.py:123} INFO - \'\n                          \'::group::Pre task execution logs\n                         \'\n                          \'[2025-01-08T19:29:02.589+0000] {taskinstance.py:2613} INFO - Dependencies \'\n                          \'all met for dep_context=non-requeueable deps ti=<TaskInstance: \'\n                          \'code-python-airflow-sample.python_task \'\n                          \'manual__2024-12-27T20:29:34.637180+00:00 [queued]>\n                         \'\n                          \'[2025-01-08T19:29:02.602+0000] {taskinstance.py:2613} INFO - Dependencies \'\n                          \'all met for dep_context=requeueable deps ti=<TaskInstance: \'\n                          \'code-python-airflow-sample.python_task \'\n                          \'manual__2024-12-27T20:29:34.637180+00:00 [queued]>\n                         \'\n                          \'[2025-01-08T19:29:02.602+0000] {taskinstance.py:2866} INFO - Starting \'\n                          \'attempt 8 of 9\n                         \'\n                          \'[2025-01-08T19:29:02.627+0000] {taskinstance.py:2889} INFO - Executing \'\n                          \'<Task(PythonOperator): python_task> on 2024-12-27 20:29:34.637180+00:00\n                         \'\n                          \'[2025-01-08T19:29:02.633+0000] {standard_task_runner.py:72} INFO - Started \'\n                          \'process 9 to run task\n                         \'\n                          \'[2025-01-08T19:29:02.641+0000] {standard_task_runner.py:104} INFO - Running: \'\n                          ""[\'airflow\', \'tasks\', \'run\', \'code-python-airflow-sample\', \'python_task\', ""\n                          ""\'manual__2024-12-27T20:29:34.637180+00:00\', \'--job-id\', \'133\', \'--raw\', ""\n                          ""\'--subdir\', \'DAGS_FOLDER/code_python_airflow_sample/airflow_dag.py\', ""\n                          ""\'--cfg-path\', \'/tmp/tmp38klyd4c\']\n                         ""\n                          \'[2025-01-08T19:29:02.646+0000] {standard_task_runner.py:105} INFO - Job 133: \'\n                          \'Subtask python_task\n                          \'[2025-01-08T19:59:03.705+0000] {taskinstance.py:352} INFO - Marking task as \'\n                          \'SUCCESS. dag_id=code-python-airflow-sample, task_id=python_task, \'\n                          \'run_id=manual__2024-12-27T20:29:34.637180+00:00, \'\n                          \'execution_date=20241227T202934, start_date=20250108T192902, \'\n                          \'end_date=20250108T195903\n                         \'\n                          \'[2025-01-08T19:59:03.763+0000] {local_task_job_runner.py:266} INFO - Task \'\n                          \'exited with return code 0\n                         \'\n                          \'[2025-01-08T19:59:03.857+0000] {taskinstance.py:3895} INFO - 0 downstream \'\n                          \'tasks scheduled from follow-on schedule check\n                         \'\n                          \'[2025-01-08T19:59:03.860+0000] {local_task_job_runner.py:245} INFO - \'\n                          \'::endgroup::\n                         \']\n[2025-01-09T10:23:03.662+0000] {app.py:1744} ERROR - Exception on /api/v1/dags/code-python-airflow-sample/dagRuns/manual__2024-12-27T20:29:34.637180+00:00/taskInstances/python_task/logs/8 [GET]\nTraceback (most recent call last):\n  File ""/my-loc/lib/python3.10/site-packages/flask/app.py"", line 2529, in wsgi_app\n    response = self.full_dispatch_request()\n  File ""/my-loc/lib/python3.10/site-packages/flask/app.py"", line 1825, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File ""/my-loc/lib/python3.10/site-packages/flask/app.py"", line 1823, in full_dispatch_request\n    rv = self.dispatch_request()\n  File ""/my-loc/lib/python3.10/site-packages/flask/app.py"", line 1799, in dispatch_request\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n  File ""/my-loc/lib/python3.10/site-packages/connexion/decorators/decorator.py"", line 68, in wrapper\n    response = function(request)\n  File ""/my-loc/lib/python3.10/site-packages/connexion/decorators/uri_parsing.py"", line 149, in wrapper\n    response = function(request)\n  File ""/my-loc/lib/python3.10/site-packages/connexion/decorators/validation.py"", line 399, in wrapper\n    return function(request)\n  File ""/my-loc/lib/python3.10/site-packages/connexion/decorators/response.py"", line 113, in wrapper\n    return _wrapper(request, response)\n  File ""/my-loc/lib/python3.10/site-packages/connexion/decorators/response.py"", line 90, in _wrapper\n    self.operation.api.get_connexion_response(response, self.mimetype)\n  File ""/my-loc/lib/python3.10/site-packages/connexion/apis/abstract.py"", line 366, in get_connexion_response\n    return cls._framework_to_connexion_response(response=response, mimetype=mimetype)\n  File ""/my-loc/lib/python3.10/site-packages/connexion/apis/flask_api.py"", line 165, in _framework_to_connexion_response\n    body=response.get_data() if not response.direct_passthrough else None,\n  File ""/my-loc/lib/python3.10/site-packages/werkzeug/wrappers/response.py"", line 314, in get_data\n    self._ensure_sequence()\n  File ""/my-loc/lib/python3.10/site-packages/werkzeug/wrappers/response.py"", line 376, in _ensure_sequence\n    self.make_sequence()\n  File ""/my-loc/lib/python3.10/site-packages/werkzeug/wrappers/response.py"", line 391, in make_sequence\n    self.response = list(self.iter_encoded())\n  File ""/my-loc/lib/python3.10/site-packages/werkzeug/wrappers/response.py"", line 50, in _iter_encoded\n    for item in iterable:\n  File ""/my-loc/lib/python3.10/site-packages/airflow/utils/log/log_reader.py"", line 94, in read_log_stream\n    logs, metadata = self.read_log_chunks(ti, current_try_number, metadata)\n  File ""/my-loc/lib/python3.10/site-packages/airflow/utils/log/log_reader.py"", line 66, in read_log_chunks\n    logs, metadatas = self.log_handler.read(ti, try_number, metadata=metadata)\n  File ""/my-loc/lib/python3.10/site-packages/airflow/utils/log/file_task_handler.py"", line 491, in read\n    log, out_metadata = self._read(task_instance, try_number_element, metadata)\n  File ""/my-loc/lib/python3.10/site-packages/airflow/utils/log/file_task_handler.py"", line 389, in _read\n    response = self._executor_get_task_log(ti, try_number)\n  File ""/my-loc/lib/python3.10/functools.py"", line 981, in __get__\n    val = self.func(instance)\n  File ""/my-loc/lib/python3.10/site-packages/airflow/utils/log/file_task_handler.py"", line 346, in _executor_get_task_log\n    executor = ExecutorLoader.get_default_executor()\n  File ""/my-loc/lib/python3.10/site-packages/airflow/executors/executor_loader.py"", line 165, in get_default_executor\n    default_executor = cls.load_executor(cls.get_default_executor_name())\n  File ""/my-loc/lib/python3.10/site-packages/airflow/executors/executor_loader.py"", line 246, in load_executor\n    executor = executor_cls()\n  File ""/my-loc/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py"", line 135, in __init__\n    self.task_queue: Queue[KubernetesJobType] = self._manager.Queue()\n  File ""/my-loc/lib/python3.10/multiprocessing/managers.py"", line 723, in temp\n    token, exp = self._create(typeid, *args, **kwds)\n  File ""/my-loc/lib/python3.10/multiprocessing/managers.py"", line 606, in _create\n    conn = self._Client(self._address, authkey=self._authkey)\n  File ""/my-loc/lib/python3.10/multiprocessing/connection.py"", line 508, in Client\n    answer_challenge(c, authkey)\n  File ""/my-loc/lib/python3.10/multiprocessing/connection.py"", line 752, in answer_challenge\n    message = connection.recv_bytes(256)         # reject large message\n  File ""/my-loc/lib/python3.10/multiprocessing/connection.py"", line 216, in recv_bytes\n    buf = self._recv_bytes(maxlength)\n  File ""/my-loc/lib/python3.10/multiprocessing/connection.py"", line 414, in _recv_bytes\n    buf = self._recv(4)\n  File ""/my-loc/lib/python3.10/multiprocessing/connection.py"", line 379, in _recv\n    chunk = read(handle, remaining)\nBlockingIOError: [Errno 11] Resource temporarily unavailable\n```\n\nAs you can see, all lines of code are slightly offset.', 'created_at': datetime.datetime(2025, 1, 13, 17, 22, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2587953576, 'issue_id': 2778082570, 'author': 'potiuk', 'body': 'Yeah, I think I knw what it is.  it looks like your processes have too low ulimit set. The error usually happens when there are not enough sockets. This is an ""os"" level error and often caused by `ulimit` constraints on the kernel level. Simply it looks like whatever underlying kernel configuration you have, it opened too many sockets already. This might also happen when you have many, many many processes/pods sharing the same kernel on the machine you run it - each of them will open sockets for all kind of communication and at some point of time you simply run out of those.\n\nSo you have to look at the configuration of your Kubernetes and Kernel to see how you can increase the numbers. \n\nConverting it into a discussion in case more is needeed.', 'created_at': datetime.datetime(2025, 1, 13, 18, 58, 2, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-09 15:07:04 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2025-01-10 09:11:16 UTC): Can you please explain what executir and what log volume configuration you have? 

I believe it might have something to do with the the volume you are using to store the logs. This looks very much like the volume does not allow to concurrently write and read files from. I think it would be great if you could check that and see what type of volume you have there.

Or @dstandish -> do you think that might be related to the logging change you implemented? Maybe that rings a bell too?

jason810496 on (2025-01-10 14:46:39 UTC): Could this issue be related to #45529? The error log traceback also references `executor_loader`.  
If the TaskInstance is still in `RUNNING` stage, the file task handler will read from `_executor_get_task_log` instead of reading from `_read_from_logs_server`.

vba (Issue Creator) on (2025-01-13 14:48:15 UTC): Hi @potiuk, 


As you can see in the stacktrace in the middle of my issue, the executor is `kubernetes_executor` and as for the volume used for logging, here's the configuration (as shown in the issue):

```ini
base_log_folder = /opt/airflow/logs
remote_logging = True
remote_log_conn_id = s3_airflow_logs
delete_local_logs = False
google_key_path = 
remote_base_log_folder = s3://the-bucket
```

here is how it's run by the k8s cluster:

```yml
# ...
spec:
  containers:
  - volumeMounts:
    - mountPath: /opt/airflow/logs
      name: logs
# ...
  volumes
  - emptyDir:
      sizeLimit: 10Gi
    name: logs
```


I don't think my problem is purely a configuration issue. If I downgrade my airflow instance to version `2.4.3`, everything works fine for the same k8s infrastructure. A silly test reveals no difficulty:
```
echo ""my test"" > /opt/airflow/logs/log.txt && cat /opt/airflow/logs/log.txt
...
my test
```

vba (Issue Creator) on (2025-01-13 14:53:58 UTC): Hi @jason810496 


Sorry, I was not clear in my previous message, the problem only occurs when a job and its tasks of interest are to be run, which assumes that its state is `RUNNING`, once the job is finished all logs for the last active task instance regain their visibility. I think this issue was introduced with the version `2.6` or `2.7`.

potiuk on (2025-01-13 15:39:07 UTC): Thanks. that saves a bit of searching through a stack trace. In the future might be better to specify it explicitly rather than leave a chnce that somoene will find it. It allows for people who look at it and try to help to quickly assess whether they can help or whether the case ""rings a bell"" without actually spending time and looking at such details. It simply optimizes for time of those who try to help you to solve your problem


I was more thinking - what are properties of the volume you have. Something that you can look at your K8S way of handling volumes of the specific kind you use. The error indicates, that somewhere during receiving logs you get ""resource unavailable"" error. After looking at this - it seems that somewhere the k8s reads logs from remote pod and something does not let it read it.

And  I think in this case it's something in your K8S configuration., There is a similar issue https://github.com/kubernetes/kubernetes/issues/105928  which indicates that somewhere logs are filing space - for example containerd version used has a problem.

And yes - I think the way how logs are read has changed between versions of k8s provider - you can take a look at the changelog - so maybe you had uncovered a configuration or another issue in your K8S. Maybe you can try to see your k8s logs correlating with the events and see if you have some other errors in other components of K8S that indicate what is a root cause.

Unfortunately k8s has 100s of moving parts and sometimes you need to dig deeper to find out the root causes (for example often problems - very strange) might occur when your DNS does not have enough resources to respond on time, and the only way to see what's going on is to generally look at what happens in your K8S and see potential issues that are correlated with the event.

But I am mostly guessing here - I wanted to help and direct the discussion but I have no deep knowledge on this particular part.

vba (Issue Creator) on (2025-01-13 17:22:46 UTC): Hi @potiuk,

Thanks for your reply.


No pb, as the subject is very complex, I didn't know how to present the key elements.


I've actually researched this problem quite a bit. Firstly, if I roll back to Airflow 2.4.3, the problem disappears. Another thing is that I've patched the Airflow code with `icecream`, trying to understand the problem step by step. In the log below, I call the API for the task instance that is running, you will notice that the remote log was fetched, and the problem starts afterwards :

```
10.*.*.* - - [09/Jan/2025:10:23:02 +0000] ""GET /api/v1/dags/code-python-airflow-sample/dagRuns/manual__2024-12-27T20:29:34.637180+00:00/taskInstances/python_task/logs/8?full_content=false HTTP/1.1"" 500 1589 ""https://airflow1-sandbox-dv.numberly.dev/dags/code-python-airflow-sample/grid?dag_run_id=manual__2024-12-27T20%3A29%3A34.637180%2B00%3A00&tab=logs&task_id=python_task"" ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:133.0) Gecko/20100101 Firefox/133.0""
[2025-01-09 10:23:02 +0000] [70] [INFO] Parent changed, shutting down: <Worker 61>
[2025-01-09 10:23:02 +0000] [70] [INFO] Worker exiting (pid: 61)
10.*.*.* - - [09/Jan/2025:10:23:03 +0000] ""GET /health HTTP/1.1"" 200 283 ""-"" ""kube-probe/1.30""
10.*.*.* - - [09/Jan/2025:10:23:03 +0000] ""GET /health HTTP/1.1"" 200 283 ""-"" ""kube-probe/1.30""
LOG ISSUE DEBUG -> log_endpoint.py:60 in get_log()
                   ""Starting #get_log"": 'Starting #get_log'
LOG ISSUE DEBUG -> log_endpoint.py:62 in get_log()
                   key: '****************'
LOG ISSUE DEBUG -> log_endpoint.py:63 in get_log()- token: None
LOG ISSUE DEBUG -> log_endpoint.py:71 in get_log()- metadata: {}
LOG ISSUE DEBUG -> log_endpoint.py:80 in get_log()
                   metadata: {'download_logs': False}
LOG ISSUE DEBUG -> log_endpoint.py:82 in get_log()
                   task_log_reader: <airflow.utils.log.log_reader.TaskLogReader object at 0x7f539c1ed510>
LOG ISSUE DEBUG -> log_reader.py:119 in log_handler()
                   task_log_reader: 'task'
LOG ISSUE DEBUG -> log_reader.py:120 in log_handler()
                   logging.getLogger(""airflow.task"").handlers: [<S3TaskHandler (NOTSET)>]
LOG ISSUE DEBUG -> log_reader.py:121 in log_handler()
                   logging.getLogger().handlers: [<RedirectStdHandler <stdout> (NOTSET)>]
LOG ISSUE DEBUG -> log_endpoint.py:96 in get_log()
                   ti: <TaskInstance: code-python-airflow-sample.python_task manual__2024-12-27T20:29:34.637180+00:00 [running]>
LOG ISSUE DEBUG -> log_endpoint.py:103 in get_log()
                   dag: <DAG: code-python-airflow-sample>
LOG ISSUE DEBUG -> log_endpoint.py:107 in get_log()
                   ti.task: <Task(PythonOperator): python_task>
LOG ISSUE DEBUG -> log_endpoint.py:112 in get_log()
                   return_type: 'text/plain'
LOG ISSUE DEBUG -> log_endpoint.py:128 in get_log()
                   logs: <generator object TaskLogReader.read_log_stream at 0x7f5396d165e0>
LOG ISSUE DEBUG -> log_endpoint.py:130 in get_log()- 'Ending'
LOG ISSUE DEBUG -> log_reader.py:78 in read_log_stream()
                   ""Starting #read_log_stream"": 'Starting #read_log_stream'
LOG ISSUE DEBUG -> log_reader.py:84 in read_log_stream()
                   try_numbers: [8]
LOG ISSUE DEBUG -> log_reader.py:85 in read_log_stream()
                   ti: <TaskInstance: code-python-airflow-sample.python_task manual__2024-12-27T20:29:34.637180+00:00 [running]>
LOG ISSUE DEBUG -> log_reader.py:86 in read_log_stream()
                   metadata: {'download_logs': False}
LOG ISSUE DEBUG -> log_reader.py:92 in read_log_stream()
                   metadata: {'download_logs': False}
LOG ISSUE DEBUG -> log_reader.py:65 in read_log_chunks()
                   self.log_handler: <S3TaskHandler (NOTSET)>
LOG ISSUE DEBUG -> s3_task_handler.py:122 in _read_remote_logs()
                   ti: <TaskInstance: code-python-airflow-sample.python_task manual__2024-12-27T20:29:34.637180+00:00 [running]>
LOG ISSUE DEBUG -> s3_task_handler.py:123 in _read_remote_logs()
                   metadata: {'download_logs': False}
LOG ISSUE DEBUG -> s3_task_handler.py:125 in _read_remote_logs()
                   worker_log_rel_path: 'dag_id=code-python-airflow-sample/run_id=manual__2024-12-27T20:29:34.637180+00:00/task_id=python_task/attempt=8.log'
LOG ISSUE DEBUG -> s3_task_handler.py:130 in _read_remote_logs()
                   bucket: 'my-bucket-logs'
LOG ISSUE DEBUG -> s3_task_handler.py:131 in _read_remote_logs()
                   prefix: 'airflow1-sandbox-dv/dag_id=code-python-airflow-sample/run_id=manual__2024-12-27T20:29:34.637180+00:00/task_id=python_task/attempt=8.log'
LOG ISSUE DEBUG -> s3_task_handler.py:133 in _read_remote_logs()
                   keys: ['airflow1-sandbox-dv/dag_id=code-python-airflow-sample/run_id=manual__2024-12-27T20:29:34.637180+00:00/task_id=python_task/attempt=8.log']
LOG ISSUE DEBUG -> s3_task_handler.py:142 in _read_remote_logs()
                   messages: ['Found logs in s3:',
                              '  * '
                              's3://my-bucket-logs/airflow1-sandbox-dv/dag_id=code-python-airflow-sample/run_id=manual__2024-12-27T20:29:34.637180+00:00/task_id=python_task/attempt=8.log']
LOG ISSUE DEBUG -> s3_task_handler.py:143 in _read_remote_logs()
                   logs: ['[2025-01-08T19:29:02.537+0000] {local_task_job_runner.py:123} INFO - '
                          '::group::Pre task execution logs
                         '
                          '[2025-01-08T19:29:02.589+0000] {taskinstance.py:2613} INFO - Dependencies '
                          'all met for dep_context=non-requeueable deps ti=<TaskInstance: '
                          'code-python-airflow-sample.python_task '
                          'manual__2024-12-27T20:29:34.637180+00:00 [queued]>
                         '
                          '[2025-01-08T19:29:02.602+0000] {taskinstance.py:2613} INFO - Dependencies '
                          'all met for dep_context=requeueable deps ti=<TaskInstance: '
                          'code-python-airflow-sample.python_task '
                          'manual__2024-12-27T20:29:34.637180+00:00 [queued]>
                         '
                          '[2025-01-08T19:29:02.602+0000] {taskinstance.py:2866} INFO - Starting '
                          'attempt 8 of 9
                         '
                          '[2025-01-08T19:29:02.627+0000] {taskinstance.py:2889} INFO - Executing '
                          '<Task(PythonOperator): python_task> on 2024-12-27 20:29:34.637180+00:00
                         '
                          '[2025-01-08T19:29:02.633+0000] {standard_task_runner.py:72} INFO - Started '
                          'process 9 to run task
                         '
                          '[2025-01-08T19:29:02.641+0000] {standard_task_runner.py:104} INFO - Running: '
                          ""['airflow', 'tasks', 'run', 'code-python-airflow-sample', 'python_task', ""
                          ""'manual__2024-12-27T20:29:34.637180+00:00', '--job-id', '133', '--raw', ""
                          ""'--subdir', 'DAGS_FOLDER/code_python_airflow_sample/airflow_dag.py', ""
                          ""'--cfg-path', '/tmp/tmp38klyd4c']
                         ""
                          '[2025-01-08T19:29:02.646+0000] {standard_task_runner.py:105} INFO - Job 133: '
                          'Subtask python_task
                          '[2025-01-08T19:59:03.705+0000] {taskinstance.py:352} INFO - Marking task as '
                          'SUCCESS. dag_id=code-python-airflow-sample, task_id=python_task, '
                          'run_id=manual__2024-12-27T20:29:34.637180+00:00, '
                          'execution_date=20241227T202934, start_date=20250108T192902, '
                          'end_date=20250108T195903
                         '
                          '[2025-01-08T19:59:03.763+0000] {local_task_job_runner.py:266} INFO - Task '
                          'exited with return code 0
                         '
                          '[2025-01-08T19:59:03.857+0000] {taskinstance.py:3895} INFO - 0 downstream '
                          'tasks scheduled from follow-on schedule check
                         '
                          '[2025-01-08T19:59:03.860+0000] {local_task_job_runner.py:245} INFO - '
                          '::endgroup::
                         ']
[2025-01-09T10:23:03.662+0000] {app.py:1744} ERROR - Exception on /api/v1/dags/code-python-airflow-sample/dagRuns/manual__2024-12-27T20:29:34.637180+00:00/taskInstances/python_task/logs/8 [GET]
Traceback (most recent call last):
  File ""/my-loc/lib/python3.10/site-packages/flask/app.py"", line 2529, in wsgi_app
    response = self.full_dispatch_request()
  File ""/my-loc/lib/python3.10/site-packages/flask/app.py"", line 1825, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/my-loc/lib/python3.10/site-packages/flask/app.py"", line 1823, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/my-loc/lib/python3.10/site-packages/flask/app.py"", line 1799, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
  File ""/my-loc/lib/python3.10/site-packages/connexion/decorators/decorator.py"", line 68, in wrapper
    response = function(request)
  File ""/my-loc/lib/python3.10/site-packages/connexion/decorators/uri_parsing.py"", line 149, in wrapper
    response = function(request)
  File ""/my-loc/lib/python3.10/site-packages/connexion/decorators/validation.py"", line 399, in wrapper
    return function(request)
  File ""/my-loc/lib/python3.10/site-packages/connexion/decorators/response.py"", line 113, in wrapper
    return _wrapper(request, response)
  File ""/my-loc/lib/python3.10/site-packages/connexion/decorators/response.py"", line 90, in _wrapper
    self.operation.api.get_connexion_response(response, self.mimetype)
  File ""/my-loc/lib/python3.10/site-packages/connexion/apis/abstract.py"", line 366, in get_connexion_response
    return cls._framework_to_connexion_response(response=response, mimetype=mimetype)
  File ""/my-loc/lib/python3.10/site-packages/connexion/apis/flask_api.py"", line 165, in _framework_to_connexion_response
    body=response.get_data() if not response.direct_passthrough else None,
  File ""/my-loc/lib/python3.10/site-packages/werkzeug/wrappers/response.py"", line 314, in get_data
    self._ensure_sequence()
  File ""/my-loc/lib/python3.10/site-packages/werkzeug/wrappers/response.py"", line 376, in _ensure_sequence
    self.make_sequence()
  File ""/my-loc/lib/python3.10/site-packages/werkzeug/wrappers/response.py"", line 391, in make_sequence
    self.response = list(self.iter_encoded())
  File ""/my-loc/lib/python3.10/site-packages/werkzeug/wrappers/response.py"", line 50, in _iter_encoded
    for item in iterable:
  File ""/my-loc/lib/python3.10/site-packages/airflow/utils/log/log_reader.py"", line 94, in read_log_stream
    logs, metadata = self.read_log_chunks(ti, current_try_number, metadata)
  File ""/my-loc/lib/python3.10/site-packages/airflow/utils/log/log_reader.py"", line 66, in read_log_chunks
    logs, metadatas = self.log_handler.read(ti, try_number, metadata=metadata)
  File ""/my-loc/lib/python3.10/site-packages/airflow/utils/log/file_task_handler.py"", line 491, in read
    log, out_metadata = self._read(task_instance, try_number_element, metadata)
  File ""/my-loc/lib/python3.10/site-packages/airflow/utils/log/file_task_handler.py"", line 389, in _read
    response = self._executor_get_task_log(ti, try_number)
  File ""/my-loc/lib/python3.10/functools.py"", line 981, in __get__
    val = self.func(instance)
  File ""/my-loc/lib/python3.10/site-packages/airflow/utils/log/file_task_handler.py"", line 346, in _executor_get_task_log
    executor = ExecutorLoader.get_default_executor()
  File ""/my-loc/lib/python3.10/site-packages/airflow/executors/executor_loader.py"", line 165, in get_default_executor
    default_executor = cls.load_executor(cls.get_default_executor_name())
  File ""/my-loc/lib/python3.10/site-packages/airflow/executors/executor_loader.py"", line 246, in load_executor
    executor = executor_cls()
  File ""/my-loc/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py"", line 135, in __init__
    self.task_queue: Queue[KubernetesJobType] = self._manager.Queue()
  File ""/my-loc/lib/python3.10/multiprocessing/managers.py"", line 723, in temp
    token, exp = self._create(typeid, *args, **kwds)
  File ""/my-loc/lib/python3.10/multiprocessing/managers.py"", line 606, in _create
    conn = self._Client(self._address, authkey=self._authkey)
  File ""/my-loc/lib/python3.10/multiprocessing/connection.py"", line 508, in Client
    answer_challenge(c, authkey)
  File ""/my-loc/lib/python3.10/multiprocessing/connection.py"", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File ""/my-loc/lib/python3.10/multiprocessing/connection.py"", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File ""/my-loc/lib/python3.10/multiprocessing/connection.py"", line 414, in _recv_bytes
    buf = self._recv(4)
  File ""/my-loc/lib/python3.10/multiprocessing/connection.py"", line 379, in _recv
    chunk = read(handle, remaining)
BlockingIOError: [Errno 11] Resource temporarily unavailable
```

As you can see, all lines of code are slightly offset.

potiuk on (2025-01-13 18:58:02 UTC): Yeah, I think I knw what it is.  it looks like your processes have too low ulimit set. The error usually happens when there are not enough sockets. This is an ""os"" level error and often caused by `ulimit` constraints on the kernel level. Simply it looks like whatever underlying kernel configuration you have, it opened too many sockets already. This might also happen when you have many, many many processes/pods sharing the same kernel on the machine you run it - each of them will open sockets for all kind of communication and at some point of time you simply run out of those.

So you have to look at the configuration of your Kubernetes and Kernel to see how you can increase the numbers. 

Converting it into a discussion in case more is needeed.

"
2777955873,issue,open,,`BigQueryCreateExternalTableOperator` doesn't trigger schema discovery,"### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

_No response_

### Apache Airflow version

2.7.3

### Operating System

Darwin Kernel Version 23.6.0

### Deployment

Google Cloud Composer

### Deployment details

_No response_

### What happened

Using `BigQueryCreateExternalTableOperator` with `autodetect=True` doesn't trigger schema to be created in BQ:
<img width=""744"" alt=""image"" src=""https://github.com/user-attachments/assets/a8c5bb21-a5ba-41d1-a007-85b45868a017"" />
<img width=""668"" alt=""Screenshot 2025-01-09 at 17 10 16"" src=""https://github.com/user-attachments/assets/9e2fa0e2-b507-4e0e-b58f-d3c32416d544"" />
<img width=""721"" alt=""Screenshot 2025-01-09 at 17 14 58"" src=""https://github.com/user-attachments/assets/e68f3658-9056-47a8-9f5f-5d9d7abb5dd0"" />



### What you think should happen instead

Running the similar command from CLI leads to correct schema discovery processing:
```
bq mk --table \
  --external_table_definition=@CSV=gs://.../test.csv \
 <dataset_name>.airflow_test_2
```
<img width=""827"" alt=""image"" src=""https://github.com/user-attachments/assets/b7dc61b6-0aed-4ad7-ad11-6866edfd5cd1"" />


### How to reproduce

Upload CSV file to GCS:
```
name,age
Bill,42
Sally,15
```

Run `BigQueryCreateExternalTableOperator`:
```python
BigQueryCreateExternalTableOperator(
        task_id=""bq_op"",
        bucket=""***"",
        source_objects=[""test.csv""],
        destination_project_dataset_table=""***"",
        source_format=""CSV"",
        autodetect=True,
        skip_leading_rows=1,
        location=""us-east4"",
    )
```

### Anything else

Most probably switching from deprecated API `bq_hook.create_external_table` to `create_empty_table ` led to that behavior
https://github.com/apache/airflow/pull/24363

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",EugeneYushin,2025-01-09 14:14:13+00:00,['EugeneYushin'],2025-02-01 12:57:16+00:00,,https://github.com/apache/airflow/issues/45512,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('area:providers', '')]","[{'comment_id': 2580321839, 'issue_id': 2777955873, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 9, 14, 14, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2581896186, 'issue_id': 2777955873, 'author': 'EugeneYushin', 'body': 'I was able to trace it to that line:\r\nhttps://github.com/apache/airflow/blob/4c5d85aeec5382d4da6505f33581076b526c37de/providers/src/airflow/providers/google/cloud/operators/bigquery.py#L1720\r\n\r\nProviding empty `schema_fields` explicitly intervenes with BQ logic for schema auto-detection. Nominal check to pass it to BQ client if only this field is set should fix the issue.', 'created_at': datetime.datetime(2025, 1, 10, 6, 38, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2581988950, 'issue_id': 2777955873, 'author': 'EugeneYushin', 'body': ""Although, it looks like the related part of the API is deprecated. Hence, it might not make much sense to fix it while it's up to be deleted soon. It will be a sole responsibility of the caller to provide valid table definition and the API will just pass it through to GCP:\r\nhttps://github.com/apache/airflow/blob/4c5d85aeec5382d4da6505f33581076b526c37de/providers/src/airflow/providers/google/cloud/operators/bigquery.py#L1603-L1608"", 'created_at': datetime.datetime(2025, 1, 10, 7, 52, 11, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-09 14:14:36 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

EugeneYushin (Issue Creator) on (2025-01-10 06:38:43 UTC): I was able to trace it to that line:
https://github.com/apache/airflow/blob/4c5d85aeec5382d4da6505f33581076b526c37de/providers/src/airflow/providers/google/cloud/operators/bigquery.py#L1720

Providing empty `schema_fields` explicitly intervenes with BQ logic for schema auto-detection. Nominal check to pass it to BQ client if only this field is set should fix the issue.

EugeneYushin (Issue Creator) on (2025-01-10 07:52:11 UTC): Although, it looks like the related part of the API is deprecated. Hence, it might not make much sense to fix it while it's up to be deleted soon. It will be a sole responsibility of the caller to provide valid table definition and the API will just pass it through to GCP:
https://github.com/apache/airflow/blob/4c5d85aeec5382d4da6505f33581076b526c37de/providers/src/airflow/providers/google/cloud/operators/bigquery.py#L1603-L1608

"
2777750092,issue,open,,Add/port metrics to the new Execution interface,"Find a way to emit metrics from the new supervisor process.

Some examples of what needs porting over:

https://github.com/apache/airflow/blob/84907f16af99e455951ac95d36fba5a966ccf763/airflow/jobs/local_task_job_runner.py#L224

https://github.com/apache/airflow/blob/84907f16af99e455951ac95d36fba5a966ccf763/airflow/jobs/local_task_job_runner.py#L347-L361",kaxil,2025-01-09 12:49:03+00:00,[],2025-01-17 15:24:22+00:00,,https://github.com/apache/airflow/issues/45511,"[('area:metrics', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK'), ('area:task-sdk', None)]",[],
2776355737,issue,closed,completed,"Misleading Warning: ""Custom Operator Inheriting from SageMakerTrainingOperator Incorrectly Triggers execute cannot be called outside TaskInstance""","### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

8.28.0

### Apache Airflow version

2.10.2

### Operating System

ubuntu

### Deployment

Docker-Compose

### Deployment details

_No response_

### What happened

We have created a custom operator, `CustomSageMakerTrainingOperator`, that inherits from `SageMakerTrainingOperator`. Although the operator is executed within a proper TaskInstance context, we receive a persistent and seemingly incorrect warning in the logs:

[2025-01-03, 22:57:16 UTC] {baseoperator.py:405} WARNING - CustomSageMakerTrainingOperator.execute cannot be called outside TaskInstance!

This warning appears even though our operator’s `execute` method is only called as part of a TaskInstance execution. The operator code, which extends the functionality of `SageMakerTrainingOperator`, confirms that no misuse exists. This suggests that the warning is a false positive, potentially due to an issue within Airflow’s internals handling of such inherited operators.


### What you think should happen instead

The custom operator should run the SageMaker training job without generating the misleading warning, since the `execute` is indeed called in a valid `TaskInstance` context.


### How to reproduce

The custom operator should run the SageMaker training job without generating the misleading warning. The warning should not appear when the `execute` method is correctly invoked within a TaskInstance context. Ideally, Airflow should correctly assess the context for inherited operators like `CustomSageMakerTrainingOperator` and refrain from showing this warning.


### Anything else


Below is the core of the custom operator code (anonymized). Full code snippet is in the original text:

```python
import logging
from typing import Optional, Dict, Any

from airflow.providers.amazon.aws.operators.sagemaker import SageMakerTrainingOperator
from airflow.exceptions import AirflowSkipException

class CustomSageMakerTrainingOperator(SageMakerTrainingOperator):
    ...

```
I've tried to confirm that the execute method is only invoked via a TaskInstance, so I'm not sure why the warning triggers. Possibly there's a check in baseoperator.py or within SageMakerTrainingOperator that incorrectly flags this scenario.


Potential Evaluated Solutions:

Investigate if the super().__init__ call with config=self.config is triggering unexpected logic in SageMakerTrainingOperator.
Check if there's an internal method or property in SageMakerTrainingOperator that must be overridden or called differently.
Possibly a bug in the logic that triggers the “outside TaskInstance” warning for operators inheriting from SageMakerTrainingOperator.


### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",luancaarvalho,2025-01-08 21:10:16+00:00,[],2025-01-10 15:01:11+00:00,2025-01-10 15:01:11+00:00,https://github.com/apache/airflow/issues/45498,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2581147260, 'issue_id': 2776355737, 'author': 'eladkal', 'body': 'cc @vincbeck @ferruzzi', 'created_at': datetime.datetime(2025, 1, 9, 20, 1, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2581411429, 'issue_id': 2776355737, 'author': 'insomnes', 'body': ""I've met this issue with my extended operators too. I believe any child class with `super().execute` call would lead to this warning.\r\n\r\nIn #41470 [comment](https://github.com/apache/airflow/issues/41470#issuecomment-2459272506) says:\r\n> The fix is now available in Airflow 2.10.3"", 'created_at': datetime.datetime(2025, 1, 9, 23, 3, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2582878173, 'issue_id': 2776355737, 'author': 'luancaarvalho', 'body': '@insomnes Thank you for the heads-up! Just out of curiosity, do you know if there’s a better way to refer to the execute function in the custom operator instead of using super().execute?', 'created_at': datetime.datetime(2025, 1, 10, 14, 47, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2582905545, 'issue_id': 2776355737, 'author': 'potiuk', 'body': '> @insomnes Thank you for the heads-up! Just out of curiosity, do you know if there’s a better way to refer to the execute function in the custom operator instead of using super().execute?\r\n\r\nNope. Best way is to upgrade to 2.10.4 (which you should do regarrdless - as you miss all the fixes done in 2.10.3 and 2.10.4 (see the changelog)', 'created_at': datetime.datetime(2025, 1, 10, 15, 0, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2582906082, 'issue_id': 2776355737, 'author': 'potiuk', 'body': 'Closing as duplicate (Fixed in 2.10.3)', 'created_at': datetime.datetime(2025, 1, 10, 15, 1, 11, tzinfo=datetime.timezone.utc)}]","eladkal on (2025-01-09 20:01:28 UTC): cc @vincbeck @ferruzzi

insomnes on (2025-01-09 23:03:05 UTC): I've met this issue with my extended operators too. I believe any child class with `super().execute` call would lead to this warning.

In #41470 [comment](https://github.com/apache/airflow/issues/41470#issuecomment-2459272506) says:

luancaarvalho (Issue Creator) on (2025-01-10 14:47:38 UTC): @insomnes Thank you for the heads-up! Just out of curiosity, do you know if there’s a better way to refer to the execute function in the custom operator instead of using super().execute?

potiuk on (2025-01-10 15:00:55 UTC): Nope. Best way is to upgrade to 2.10.4 (which you should do regarrdless - as you miss all the fixes done in 2.10.3 and 2.10.4 (see the changelog)

potiuk on (2025-01-10 15:01:11 UTC): Closing as duplicate (Fixed in 2.10.3)

"
2775867347,issue,closed,completed,AIP-66: Make callbacks bundle aware,,ephraimbuddy,2025-01-08 16:52:14+00:00,['ephraimbuddy'],2025-02-04 01:44:08+00:00,2025-02-04 01:44:08+00:00,https://github.com/apache/airflow/issues/45496,[],"[{'comment_id': 2580911236, 'issue_id': 2775867347, 'author': 'okirialbert', 'body': ""Hi @ephraimbuddy, I'd like to work on this."", 'created_at': datetime.datetime(2025, 1, 9, 17, 45, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2582242060, 'issue_id': 2775867347, 'author': 'ephraimbuddy', 'body': ""> Hi @ephraimbuddy, I'd like to work on this.\r\n\r\nHi @okirialbert , this should have been created as a task, but I mistakenly created it as an issue. It's part of the AIP-66, which we are working on and already looking into."", 'created_at': datetime.datetime(2025, 1, 10, 9, 55, 28, tzinfo=datetime.timezone.utc)}]","okirialbert on (2025-01-09 17:45:14 UTC): Hi @ephraimbuddy, I'd like to work on this.

ephraimbuddy (Issue Creator) on (2025-01-10 09:55:28 UTC): Hi @okirialbert , this should have been created as a task, but I mistakenly created it as an issue. It's part of the AIP-66, which we are working on and already looking into.

"
2775800049,issue,open,,"Add support for Lifecycle (`on_starting`, `before_stopping`) Listeners with Task SDK",,kaxil,2025-01-08 16:20:37+00:00,[],2025-01-08 16:22:48+00:00,,https://github.com/apache/airflow/issues/45495,"[('area:Listeners', '')]",[],
2775799922,issue,open,,Add support for `on_*_import_error` Listeners with Task SDK,"Part of https://github.com/apache/airflow/issues/45491

Port https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/listeners.html#dag-import-error-events to Task SDK. 

https://github.com/apache/airflow/blob/3fc73229da651420cd2b974e1fff9d9786d09ec1/airflow/listeners/spec/importerrors.py#L25-L32",kaxil,2025-01-08 16:20:34+00:00,[],2025-01-08 16:22:47+00:00,,https://github.com/apache/airflow/issues/45494,"[('area:Listeners', '')]",[],
2775799815,issue,open,,Add support for `on_asset_*` Listeners with Task SDK,"Part of https://github.com/apache/airflow/issues/45491

Port https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/listeners.html#dataset-events to Task SDK. 

https://github.com/apache/airflow/blob/3fc73229da651420cd2b974e1fff9d9786d09ec1/airflow/listeners/spec/asset.py#L30-L42",kaxil,2025-01-08 16:20:31+00:00,[],2025-01-08 16:23:34+00:00,,https://github.com/apache/airflow/issues/45493,"[('area:Listeners', '')]",[],
2775799701,issue,closed,completed,Add support for `on_dag_run_*` Listeners with Task SDK,"Part of https://github.com/apache/airflow/issues/45491

Port Listeners https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/listeners.html#listeners to Task SDK. 

https://github.com/apache/airflow/blob/3fc73229da651420cd2b974e1fff9d9786d09ec1/airflow/listeners/spec/dagrun.py#L30-L42",kaxil,2025-01-08 16:20:27+00:00,[],2025-01-08 20:21:31+00:00,2025-01-08 20:21:30+00:00,https://github.com/apache/airflow/issues/45492,"[('area:Listeners', '')]","[{'comment_id': 2578329511, 'issue_id': 2775799701, 'author': 'mobuchowski', 'body': '@kaxil those would continue to work (primarily) on scheduler, as scheduler is the one thing that changes the state of DR. The edge case is ""mark run as failed"" in the UI, but it\'s also not Task SDK related: https://github.com/apache/airflow/issues/40735', 'created_at': datetime.datetime(2025, 1, 8, 18, 18, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2578581271, 'issue_id': 2775799701, 'author': 'kaxil', 'body': 'You are right!', 'created_at': datetime.datetime(2025, 1, 8, 20, 21, 30, tzinfo=datetime.timezone.utc)}]","mobuchowski on (2025-01-08 18:18:04 UTC): @kaxil those would continue to work (primarily) on scheduler, as scheduler is the one thing that changes the state of DR. The edge case is ""mark run as failed"" in the UI, but it's also not Task SDK related: https://github.com/apache/airflow/issues/40735

kaxil (Issue Creator) on (2025-01-08 20:21:30 UTC): You are right!

"
2775798811,issue,open,,Ensure Listeners work with Task SDK,"Port Listeners https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/listeners.html#listeners to Task SDK. 

- https://github.com/apache/airflow/issues/45423
- [x] #45492
- [ ] #45493
- [ ] #45494
- [ ] #45495",kaxil,2025-01-08 16:20:00+00:00,['mobuchowski'],2025-01-08 20:21:31+00:00,,https://github.com/apache/airflow/issues/45491,"[('area:Listeners', '')]",[],
2775552745,issue,closed,not_planned,TLS Handshake Error with Vault after Airflow and provider updates,"### Apache Airflow Provider(s)

hashicorp

### Versions of Apache Airflow Providers

```
apache-airflow-providers-amazon==8.19.0
apache-airflow-providers-common-compat==1.2.1
apache-airflow-providers-common-io==1.4.2
apache-airflow-providers-fab==1.5.0
apache-airflow-providers-ftp==3.11.1
apache-airflow-providers-hashicorp=4.0.0
apache-airflow-providers-http==4.13.3
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-microsoft-azure>=9.0.1
apache-airflow-providers-microsoft-mssql==3.9.2
apache-airflow-providers-postgres>=5.10.2
apache-airflow-providers-smtp==1.8.0
apache-airflow-providers-sqlite==3.9.0
```

### Apache Airflow version

2.10.3

### Operating System

apache/airflow:2.10.3-python3.11

### Deployment

Other Docker-based deployment

### Deployment details

Hashicorp Vault version: 1.16.6+ent

### What happened

After updating Airflow (from 2.9.0 to 2.10.3) as well as Hashicorp provider (from 3.6.4 to 4.0.0), secrets and connections available in our running Vault instance are not found anymore during DAG runs / compilation. 

It seems that this is related to a TLS handshake error to our running Vault instance first observed after the update:
```
Jan 08 14:04:10 CWCOL0VXDTIMG02 vault[817]: 2025-01-08T14:04:10.018+0100 [INFO]  http: TLS handshake error from 10.74.73.21:56698: EOF
Jan 08 14:04:10 CWCOL0VXDTIMG02 vault[817]: 2025-01-08T14:04:10.193+0100 [INFO]  http: TLS handshake error from 10.74.73.21:56748: EOF
Jan 08 14:04:10 CWCOL0VXDTIMG02 vault[817]: 2025-01-08T14:04:10.306+0100 [INFO]  http: TLS handshake error from 10.74.73.21:56788: EOF
```

### What you think should happen instead

When downgrading to Airflow 2.9.0 as well as Hashicorp provider 3.6.4, this error doesn't occur, and all Vault secrets and connections can be accessed again from within the DAG run.

### How to reproduce

1. Set up an Apache Airflow environment with version 2.10.3 using the apache/airflow:2.10.3-python3.11 Docker image.
2. Configure the HashiCorp Vault connection in Airflow using the apache-airflow-providers-hashicorp==4.0.0 provider. Ensure the Vault instance is accessible.
3. Verify that the Vault instance is running with version 1.16.6+ent and is properly configured to store secrets/connections.
4. Attempt to retrieve a secret from the Vault using an Airflow task or within the DAG.
5. Observe the logs for TLS handshake errors, as described in the ""What happened"" section.

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",khe-cw-de,2025-01-08 14:33:56+00:00,[],2025-02-06 00:15:19+00:00,2025-02-06 00:15:19+00:00,https://github.com/apache/airflow/issues/45487,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('area:providers', ''), ('pending-response', ''), ('provider:hashicorp', 'Hashicorp provider related issues')]","[{'comment_id': 2577822046, 'issue_id': 2775552745, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 8, 14, 33, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2578813566, 'issue_id': 2775552745, 'author': 'potiuk', 'body': ""I think it might be related to a kubernetes bug and it is triggered via different things being upgraded as well.\r\n\r\nI guess together with the upgrade you  upgraded to a newer version of K8S and you hit this issue https://github.com/kubernetes/kubernetes/issues/109022  - or maybe hvac version incompatibility (`hvac` is the hashicorp library used to communicate with hashicorp). You might want to narrow it down by limiting things to upgrade and diagnose what is the faulty component:\r\n\r\n1) downgrade proivder separately on Airflow 2.10.0 - without downloading hvac\r\n2) downgrade hvac separately to the version that was present in constraints of Airlfow 2.9.0 \r\n3) downgrade k8s version (or any other thing you upgraded that might contain the faulty go version).\r\n\r\nYou can then see which one works. My guess it's the hvac upgrade that caused it, then you can report the issue to `hvac` repository.\r\n\r\nPlease let us know what your investigation brings."", 'created_at': datetime.datetime(2025, 1, 8, 22, 39, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2582920519, 'issue_id': 2775552745, 'author': 'khe-cw-de', 'body': '@potiuk Many thanks for your reply!\r\nI did some testing of different dependency configs between Airflow, Hashicorp provider and HVAC to narrow the issue down:\r\n\r\n### Former stable configuration: ✅ \r\n```\r\napache-airflow==2.9.0\r\napache-airflow-providers-hashicorp==3.6.4\r\nhvac==2.1.0\r\n```\r\n\r\n### Updating Airflow, provider and HVAC: ❌ \r\n```\r\napache-airflow==2.10.0\r\napache-airflow-providers-hashicorp==3.8.0\r\nhvac==2.3.0\r\n```\r\n\r\n### Former provider version and HVAC: ✅ \r\n```\r\napache-airflow==2.10.0\r\napache-airflow-providers-hashicorp==3.6.4\r\nhvac==2.2.0\r\n```\r\n\r\n### After updating provider again: ❌\r\n```\r\napache-airflow==2.10.0\r\napache-airflow-providers-hashicorp==3.8.0\r\nhvac==2.2.0\r\n```\r\n\r\n\r\nIt seems that the issue is somehow introduced with the provider update to version 3.8.0.', 'created_at': datetime.datetime(2025, 1, 10, 15, 8, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2617240397, 'issue_id': 2775552745, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2025, 1, 28, 0, 15, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2638320832, 'issue_id': 2775552745, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2025, 2, 6, 0, 15, 18, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-08 14:33:59 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2025-01-08 22:39:03 UTC): I think it might be related to a kubernetes bug and it is triggered via different things being upgraded as well.

I guess together with the upgrade you  upgraded to a newer version of K8S and you hit this issue https://github.com/kubernetes/kubernetes/issues/109022  - or maybe hvac version incompatibility (`hvac` is the hashicorp library used to communicate with hashicorp). You might want to narrow it down by limiting things to upgrade and diagnose what is the faulty component:

1) downgrade proivder separately on Airflow 2.10.0 - without downloading hvac
2) downgrade hvac separately to the version that was present in constraints of Airlfow 2.9.0 
3) downgrade k8s version (or any other thing you upgraded that might contain the faulty go version).

You can then see which one works. My guess it's the hvac upgrade that caused it, then you can report the issue to `hvac` repository.

Please let us know what your investigation brings.

khe-cw-de (Issue Creator) on (2025-01-10 15:08:21 UTC): @potiuk Many thanks for your reply!
I did some testing of different dependency configs between Airflow, Hashicorp provider and HVAC to narrow the issue down:

### Former stable configuration: ✅ 
```
apache-airflow==2.9.0
apache-airflow-providers-hashicorp==3.6.4
hvac==2.1.0
```

### Updating Airflow, provider and HVAC: ❌ 
```
apache-airflow==2.10.0
apache-airflow-providers-hashicorp==3.8.0
hvac==2.3.0
```

### Former provider version and HVAC: ✅ 
```
apache-airflow==2.10.0
apache-airflow-providers-hashicorp==3.6.4
hvac==2.2.0
```

### After updating provider again: ❌
```
apache-airflow==2.10.0
apache-airflow-providers-hashicorp==3.8.0
hvac==2.2.0
```


It seems that the issue is somehow introduced with the provider update to version 3.8.0.

github-actions[bot] on (2025-01-28 00:15:10 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2025-02-06 00:15:18 UTC): This issue has been closed because it has not received response from the issue author.

"
2775151880,issue,open,,Handle Custom XCom Backend on Task SDK,"Follow-up of https://github.com/apache/airflow/issues/45231 to move XCom Backend on Task SDK. 

As mentioned on https://github.com/apache/airflow/issues/45231, it will be the client's responsibility to serialize and deserialize before sending it to the API server to support multiple language. 

For XCom Backends, where you want to stop sending MBs or GBs of data to the DB (via the API-server), we will handle this on the client side so the data is sent from the Worker (and client) itself.

https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/xcoms.html#custom-xcom-backends",kaxil,2025-01-08 11:27:57+00:00,['kaxil'],2025-01-08 12:41:42+00:00,,https://github.com/apache/airflow/issues/45481,"[('area:core', '')]",[],
2775095531,issue,open,,"Add migration test in CI that initializes Airflow 2.10, upgrades and runs 3.0 migration tests","### Description

Since we pruned the migration files, we no longer have those other migration files from 2.7.0 downwards, and this means that for tests in CI/dev, we have to reset the DB from ORM. The effect of this is that if you are doing a development and adding a field or a new table, you have to initialize the DB without your changes before Alembic can autogenerate the needed migrations. Else, it sees no changes. 

Using breeze to create a DB from airflow 2.10 and then upgrading and running all the migration tests would help in detecting missing fields/tables in the migration files.

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",ephraimbuddy,2025-01-08 11:00:33+00:00,[],2025-01-11 05:54:10+00:00,,https://github.com/apache/airflow/issues/45479,"[('kind:feature', 'Feature Requests'), ('area:CI', ""Airflow's tests and continious integration""), ('area:db-migrations', 'PRs with DB migration')]",[],
2774511009,issue,open,,Using celery + gevent cause exception to be thrown from ExecutorSafeguard,"### Apache Airflow Provider(s)

celery

### Versions of Apache Airflow Providers

apache-airflow==2.10.4
apache-airflow-providers-amazon==9.1.0
apache-airflow-providers-celery==3.8.5
apache-airflow-providers-cncf-kubernetes==10.0.1
apache-airflow-providers-common-compat==1.2.2
apache-airflow-providers-common-io==1.4.2
apache-airflow-providers-common-sql==1.20.0
apache-airflow-providers-docker==3.14.1
apache-airflow-providers-elasticsearch==5.5.3
apache-airflow-providers-fab==1.5.1
apache-airflow-providers-ftp==3.11.1
apache-airflow-providers-google==11.0.0
apache-airflow-providers-grpc==3.6.0
apache-airflow-providers-hashicorp==3.8.0
apache-airflow-providers-http==4.13.3
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-microsoft-azure==11.1.0
apache-airflow-providers-mysql==5.7.4
apache-airflow-providers-odbc==4.8.1
apache-airflow-providers-openlineage==1.14.0
apache-airflow-providers-postgres==5.14.0
apache-airflow-providers-redis==3.8.0
apache-airflow-providers-sendgrid==3.6.0
apache-airflow-providers-sftp==4.11.1
apache-airflow-providers-slack==8.9.2
apache-airflow-providers-smtp==1.8.1
apache-airflow-providers-snowflake==5.8.1
apache-airflow-providers-sqlite==3.9.1
apache-airflow-providers-ssh==3.14.0
google-cloud-orchestration-airflow==1.15.1

### Apache Airflow version

2.10.4

### Operating System

Debian GNU/Linux 12 (bookworm)

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

environment variables:
* AIRFLOW__CELERY__POOL=gevent
* _AIRFLOW_PATCH_GEVENT=1

### What happened

When using gevent pool on celery and enabling gevent monkey patch using envvar `_AIRFLOW_PATCH_GEVENT=1` cause task to failed, with the following traceback.

```
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 413, in wrapper
    cls._sentinel.callers[sentinel_key] = sentinel
    ^^^^^^^^^^^^^^^^^^^^^
  File ""src/gevent/local.py"", line 410, in gevent._gevent_clocal.local.__getattribute__
AttributeError: 'gevent._gevent_clocal.local' object has no attribute 'callers'
```

### What you think should happen instead

_No response_

### How to reproduce

1. Use celery worker with following envvar `AIRFLOW__CELERY__POOL=gevent` and `_AIRFLOW_PATCH_GEVENT=1`.
2. Create new DAG with simple bash operator that sleep for 5 seconds.
3. Unpause the new DAG.

### Anything else

If `_AIRFLOW_PATCH_GEVENT` is not set, the task finish successfully but the subsequent task run will hang in queued state and when checking worker log show `RecursionError: maximum recursion depth exceeded`.
```
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/amazon/aws/log/s3_task_handler.py"", line 167, in s3_write
    if append and self.s3_log_exists(remote_log_location):
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/amazon/aws/log/s3_task_handler.py"", line 134, in s3_log_exists
    return self.hook.check_for_key(remote_log_location)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/amazon/aws/hooks/s3.py"", line 152, in wrapper
    return func(*bound_args.args, **bound_args.kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/amazon/aws/hooks/s3.py"", line 125, in wrapper
    return func(*bound_args.args, **bound_args.kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/amazon/aws/hooks/s3.py"", line 958, in check_for_key
    obj = self.head_object(key, bucket_name)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/amazon/aws/hooks/s3.py"", line 152, in wrapper
    return func(*bound_args.args, **bound_args.kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/amazon/aws/hooks/s3.py"", line 125, in wrapper
    return func(*bound_args.args, **bound_args.kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/amazon/aws/hooks/s3.py"", line 938, in head_object
    return self.get_conn().head_object(Bucket=bucket_name, Key=key)
           ^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/amazon/aws/hooks/base_aws.py"", line 783, in get_conn
    return self.conn
           ^^^^^^^^^
  File ""/usr/local/lib/python3.12/functools.py"", line 995, in __get__
    val = self.func(instance)
          ^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/amazon/aws/hooks/base_aws.py"", line 740, in conn
    return self.get_client_type(region_name=self.region_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/amazon/aws/hooks/base_aws.py"", line 710, in get_client_type
    return session.client(
           ^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/boto3/session.py"", line 299, in client
    return self._session.create_client(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/botocore/session.py"", line 951, in create_client
    credentials = self.get_credentials()
                  ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/botocore/session.py"", line 507, in get_credentials
    self._credentials = self._components.get_component(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/botocore/session.py"", line 1108, in get_component
    self._components[name] = factory()
                             ^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/botocore/session.py"", line 186, in _create_credential_resolver
    return botocore.credentials.create_credential_resolver(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/botocore/credentials.py"", line 92, in create_credential_resolver
    container_provider = ContainerProvider()
                         ^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/botocore/credentials.py"", line 1893, in __init__
    fetcher = ContainerMetadataFetcher()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/botocore/utils.py"", line 2910, in __init__
    session = botocore.httpsession.URLLib3Session(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/botocore/httpsession.py"", line 323, in __init__
    self._manager = PoolManager(**self._get_pool_manager_kwargs())
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/botocore/httpsession.py"", line 341, in _get_pool_manager_kwargs
    'ssl_context': self._get_ssl_context(),
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/botocore/httpsession.py"", line 350, in _get_ssl_context
    return create_urllib3_context()
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/botocore/httpsession.py"", line 139, in create_urllib3_context
    context.options |= options
    ^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/ssl.py"", line 561, in options
    super(SSLContext, SSLContext).options.__set__(self, value)
  File ""/usr/local/lib/python3.12/ssl.py"", line 561, in options
    super(SSLContext, SSLContext).options.__set__(self, value)
  File ""/usr/local/lib/python3.12/ssl.py"", line 561, in options
    super(SSLContext, SSLContext).options.__set__(self, value)
  [Previous line repeated 929 more times]
RecursionError: maximum recursion depth exceeded
```
I think this happen because the monkey patch in celery_command.py does not happen early enough as is clearly shown by warning of the 2nd line of this logs.
```
/home/airflow/.local/lib/python3.12/site-packages/airflow/metrics/statsd_logger.py:184 RemovedInAirflow3Warning: The basic metric validator will be deprecated in the future in favor of pattern-matching.  You can try this now by setting config option metrics_use_pattern_match to True.
/home/airflow/.local/lib/python3.12/site-packages/celery/__init__.py:113 MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.util.ssl_ (/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/ssl_.py)', 'aiohttp.client_exceptions (/home/airflow/.local/lib/python3.12/site-packages/aiohttp/client_exceptions.py)', 'urllib3.contrib.pyopenssl (/home/airflow/.local/lib/python3.12/site-packages/urllib3/contrib/pyopenssl.py)', 'aiohttp.connector (/home/airflow/.local/lib/python3.12/site-packages/aiohttp/connector.py)', 'aiohttp.client_reqrep (/home/airflow/.local/lib/python3.12/site-packages/aiohttp/client_reqrep.py)', 'urllib3.util (/home/airflow/.local/lib/python3.12/site-packages/urllib3/util/__init__.py)', 'jwt.jwks_client (/home/airflow/.local/lib/python3.12/site-packages/jwt/jwks_client.py)', 'botocore.httpsession (/home/airflow/.local/lib/python3.12/site-packages/botocore/httpsession.py)'].
[2025-01-07 11:15:35 +0000] [15] [INFO] Starting gunicorn 23.0.0
[2025-01-07 11:15:35 +0000] [15] [INFO] Listening at: http://[::]:8793 (15)
[2025-01-07 11:15:35 +0000] [15] [INFO] Using worker: sync
[2025-01-07 11:15:35 +0000] [16] [INFO] Booting worker with pid: 16
[2025-01-07 11:15:35 +0000] [17] [INFO] Booting worker with pid: 17

 -------------- celery@airflow-ptu-worker-alt-pool-5b78c98f5f-6zxg2 v5.4.0 (opalescent)
--- ***** -----
-- ******* ---- Linux-6.1.112-x86_64-with-glibc2.36 2025-01-07 11:15:36
- *** --- * ---
- ** ---------- [config]
- ** ---------- .> app:         airflow.providers.celery.executors.celery_executor:0x7f8457cd8b90
- ** ---------- .> transport:   ***
- ** ---------- .> results:     ***
- *** --- * --- .> concurrency: 24 (gevent)
-- ******* ---- .> task events: OFF (enable -E to monitor tasks in this worker)
--- ***** -----
 -------------- [queues]
                .> alt_pool         exchange=alt_pool(direct) key=alt_pool


[tasks]
  . airflow.providers.celery.executors.celery_executor_utils.execute_command
```

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",collinpowerkariman,2025-01-08 07:28:38+00:00,['Anurag-Kumar-01'],2025-01-26 23:22:09+00:00,,https://github.com/apache/airflow/issues/45475,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('provider:celery', '')]","[{'comment_id': 2576927044, 'issue_id': 2774511009, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 8, 7, 28, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2580908721, 'issue_id': 2774511009, 'author': 'collinpowerkariman', 'body': 'I think this happen because of the `callers` attribute was set outside of the gevent\'s loop, i think changing the way `callers` is set to this should fix it, i haven\'t try this but once i have the time i will and report back.\r\n```\r\nclass ExecutorSafeguard:\r\n    """"""\r\n    The ExecutorSafeguard decorator.\r\n\r\n    Checks if the execute method of an operator isn\'t manually called outside\r\n    the TaskInstance as we want to avoid bad mixing between decorated and\r\n    classic operators.\r\n    """"""\r\n\r\n    test_mode = conf.getboolean(""core"", ""unit_test_mode"")\r\n    _sentinel = local()\r\n\r\n    @classmethod\r\n    def decorator(cls, func):\r\n        if not hasattr(cls._sentinel, ""callers""):\r\n            cls._sentinel.callers = {}\r\n```', 'created_at': datetime.datetime(2025, 1, 9, 17, 43, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585848469, 'issue_id': 2774511009, 'author': 'AlekseiMalkov', 'body': 'Same situation with 2.10.3', 'created_at': datetime.datetime(2025, 1, 12, 17, 47, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2586948118, 'issue_id': 2774511009, 'author': 'collinpowerkariman', 'body': 'I did a quick monkey patch by changing the `/home/airflow/.local/bin/airflow` file to this:\n\n```\n#!/home/airflow/.local/bin/python\n# -*- coding: utf-8 -*-\nimport re\nimport sys\nfrom functools import wraps\n\nfrom airflow.__main__ import main\nfrom airflow.exceptions import AirflowException\nfrom airflow.models.base import _sentinel\nfrom airflow.models.baseoperator import ExecutorSafeguard\n\n\ndef decorator(cls, func):\n    if not hasattr(cls._sentinel, ""callers""):\n        cls._sentinel.callers = {}\n\n    @wraps(func)\n    def wrapper(self, *args, **kwargs):\n        from airflow.decorators.base import DecoratedOperator\n\n        sentinel_key = f""{self.__class__.__name__}__sentinel""\n        sentinel = kwargs.pop(sentinel_key, None)\n\n        if sentinel:\n            if not getattr(cls._sentinel, ""callers"", None):\n                cls._sentinel.callers = {}\n            cls._sentinel.callers[sentinel_key] = sentinel\n        else:\n            sentinel = cls._sentinel.callers.pop(f""{func.__qualname__.split(\'.\')[0]}__sentinel"", None)\n\n        if not cls.test_mode and not sentinel == _sentinel and not isinstance(self, DecoratedOperator):\n            message = f""{self.__class__.__name__}.{func.__name__} cannot be called outside TaskInstance!""\n            if not self.allow_nested_operators:\n                raise AirflowException(message)\n            self.log.warning(message)\n        return func(self, *args, **kwargs)\n\n    return wrapper\n\n\nprint(""patching ExecutorSafeguard"")\nExecutorSafeguard.decorator = decorator\n\nif __name__ == \'__main__\':\n    sys.argv[0] = re.sub(r\'(-script\\.pyw|\\.exe)?$\', \'\', sys.argv[0])\n    sys.exit(main())\n```\n\nAfter the monkey patch the task run successfully, but now even though the task is fine the worker keep throwing this exception:\n```\n[2025-01-13 11:45:17,369: WARNING/MainProcess] Traceback (most recent call last):\n[2025-01-13 11:45:17,370: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/redis/connection.py"", line 357, in connect\n    sock = self.retry.call_with_retry(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-01-13 11:45:17,370: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/redis/retry.py"", line 62, in call_with_retry\n    return do()\n           ^^^^\n[2025-01-13 11:45:17,370: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/redis/connection.py"", line 358, in <lambda>\n    lambda: self._connect(), lambda error: self.disconnect(error)\n            ^^^^^^^^^^^^^^^\n[2025-01-13 11:45:17,370: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/redis/connection.py"", line 1021, in _connect\n    return self._wrap_socket_with_ssl(sock)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-01-13 11:45:17,370: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/redis/connection.py"", line 1066, in _wrap_socket_with_ssl\n    sslsock = context.wrap_socket(sock, server_hostname=self.host)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-01-13 11:45:17,370: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/gevent/ssl.py"", line 122, in wrap_socket\n    return self.sslsocket_class(\n           ^^^^^^^^^^^^^^^^^^^^^\n[2025-01-13 11:45:17,370: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/gevent/ssl.py"", line 351, in __init__\n    self.do_handshake()\n[2025-01-13 11:45:17,370: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/gevent/ssl.py"", line 740, in do_handshake\n    self._sslobj.do_handshake()\n[2025-01-13 11:45:17,370: WARNING/MainProcess] ConnectionResetError: [Errno 104] Connection reset by peer\n[2025-01-13 11:45:17,371: WARNING/MainProcess]\nDuring handling of the above exception, another exception occurred:\n[2025-01-13 11:45:17,371: WARNING/MainProcess] Traceback (most recent call last):\n[2025-01-13 11:45:17,371: WARNING/MainProcess]   File ""src/gevent/_waiter.py"", line 122, in gevent._gevent_c_waiter.Waiter.switch\n[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/celery/worker/pidbox.py"", line 112, in loop\n    self._do_reset(c, connection)\n[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/celery/worker/pidbox.py"", line 102, in _do_reset\n    self.consumer = self.node.listen(callback=self.on_message)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/pidbox.py"", line 89, in listen\n    consumer = self.Consumer(channel=channel,\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/pidbox.py"", line 75, in Consumer\n    return Consumer(\n           ^^^^^^^^^\n[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/messaging.py"", line 402, in __init__\n    self.revive(self.channel)\n[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/messaging.py"", line 424, in revive\n    self.declare()\n[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/messaging.py"", line 438, in declare\n    queue.declare()\n[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/entity.py"", line 617, in declare\n    self._create_queue(nowait=nowait, channel=channel)\n[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/entity.py"", line 626, in _create_queue\n    self.queue_declare(nowait=nowait, passive=False, channel=channel)\n[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/entity.py"", line 655, in queue_declare\n    ret = channel.queue_declare(\n          ^^^^^^^^^^^^^^^^^^^^^^\n[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/transport/virtual/base.py"", line 538, in queue_declare\n    return queue_declare_ok_t(queue, self._size(queue), 0)\n                                     ^^^^^^^^^^^^^^^^^\n[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/transport/redis.py"", line 1005, in _size\n    sizes = pipe.execute()\n            ^^^^^^^^^^^^^^\n[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/redis/client.py"", line 1524, in execute\n    conn = self.connection_pool.get_connection(""MULTI"", self.shard_hint)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[2025-01-13 11:45:17,373: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/redis/connection.py"", line 1422, in get_connection\n    connection.connect()\n[2025-01-13 11:45:17,373: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/redis/connection.py"", line 363, in connect\n    raise ConnectionError(self._error_message(e))\n[2025-01-13 11:45:17,373: WARNING/MainProcess] redis.exceptions.ConnectionError: Error 104 connecting to ****:****. Connection reset by peer.\n[2025-01-13 11:45:17,373: WARNING/MainProcess] 2025-01-13T11:45:17Z\n[2025-01-13 11:45:17,373: WARNING/MainProcess]\n[2025-01-13 11:45:17,373: WARNING/MainProcess] <built-in method switch of gevent._gevent_c_greenlet_primitives.TrackedRawGreenlet object at 0x7f610735be40> failed with ConnectionError\n```', 'created_at': datetime.datetime(2025, 1, 13, 12, 14, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2587305779, 'issue_id': 2774511009, 'author': 'Anurag-Kumar-01', 'body': 'I want to take up this issue', 'created_at': datetime.datetime(2025, 1, 13, 14, 47, 53, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-08 07:28:41 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

collinpowerkariman (Issue Creator) on (2025-01-09 17:43:48 UTC): I think this happen because of the `callers` attribute was set outside of the gevent's loop, i think changing the way `callers` is set to this should fix it, i haven't try this but once i have the time i will and report back.
```
class ExecutorSafeguard:
    """"""
    The ExecutorSafeguard decorator.

    Checks if the execute method of an operator isn't manually called outside
    the TaskInstance as we want to avoid bad mixing between decorated and
    classic operators.
    """"""

    test_mode = conf.getboolean(""core"", ""unit_test_mode"")
    _sentinel = local()

    @classmethod
    def decorator(cls, func):
        if not hasattr(cls._sentinel, ""callers""):
            cls._sentinel.callers = {}
```

AlekseiMalkov on (2025-01-12 17:47:47 UTC): Same situation with 2.10.3

collinpowerkariman (Issue Creator) on (2025-01-13 12:14:50 UTC): I did a quick monkey patch by changing the `/home/airflow/.local/bin/airflow` file to this:

```
#!/home/airflow/.local/bin/python
# -*- coding: utf-8 -*-
import re
import sys
from functools import wraps

from airflow.__main__ import main
from airflow.exceptions import AirflowException
from airflow.models.base import _sentinel
from airflow.models.baseoperator import ExecutorSafeguard


def decorator(cls, func):
    if not hasattr(cls._sentinel, ""callers""):
        cls._sentinel.callers = {}

    @wraps(func)
    def wrapper(self, *args, **kwargs):
        from airflow.decorators.base import DecoratedOperator

        sentinel_key = f""{self.__class__.__name__}__sentinel""
        sentinel = kwargs.pop(sentinel_key, None)

        if sentinel:
            if not getattr(cls._sentinel, ""callers"", None):
                cls._sentinel.callers = {}
            cls._sentinel.callers[sentinel_key] = sentinel
        else:
            sentinel = cls._sentinel.callers.pop(f""{func.__qualname__.split('.')[0]}__sentinel"", None)

        if not cls.test_mode and not sentinel == _sentinel and not isinstance(self, DecoratedOperator):
            message = f""{self.__class__.__name__}.{func.__name__} cannot be called outside TaskInstance!""
            if not self.allow_nested_operators:
                raise AirflowException(message)
            self.log.warning(message)
        return func(self, *args, **kwargs)

    return wrapper


print(""patching ExecutorSafeguard"")
ExecutorSafeguard.decorator = decorator

if __name__ == '__main__':
    sys.argv[0] = re.sub(r'(-script\.pyw|\.exe)?$', '', sys.argv[0])
    sys.exit(main())
```

After the monkey patch the task run successfully, but now even though the task is fine the worker keep throwing this exception:
```
[2025-01-13 11:45:17,369: WARNING/MainProcess] Traceback (most recent call last):
[2025-01-13 11:45:17,370: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/redis/connection.py"", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2025-01-13 11:45:17,370: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/redis/retry.py"", line 62, in call_with_retry
    return do()
           ^^^^
[2025-01-13 11:45:17,370: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/redis/connection.py"", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
[2025-01-13 11:45:17,370: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/redis/connection.py"", line 1021, in _connect
    return self._wrap_socket_with_ssl(sock)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2025-01-13 11:45:17,370: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/redis/connection.py"", line 1066, in _wrap_socket_with_ssl
    sslsock = context.wrap_socket(sock, server_hostname=self.host)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2025-01-13 11:45:17,370: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/gevent/ssl.py"", line 122, in wrap_socket
    return self.sslsocket_class(
           ^^^^^^^^^^^^^^^^^^^^^
[2025-01-13 11:45:17,370: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/gevent/ssl.py"", line 351, in __init__
    self.do_handshake()
[2025-01-13 11:45:17,370: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/gevent/ssl.py"", line 740, in do_handshake
    self._sslobj.do_handshake()
[2025-01-13 11:45:17,370: WARNING/MainProcess] ConnectionResetError: [Errno 104] Connection reset by peer
[2025-01-13 11:45:17,371: WARNING/MainProcess]
During handling of the above exception, another exception occurred:
[2025-01-13 11:45:17,371: WARNING/MainProcess] Traceback (most recent call last):
[2025-01-13 11:45:17,371: WARNING/MainProcess]   File ""src/gevent/_waiter.py"", line 122, in gevent._gevent_c_waiter.Waiter.switch
[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/celery/worker/pidbox.py"", line 112, in loop
    self._do_reset(c, connection)
[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/celery/worker/pidbox.py"", line 102, in _do_reset
    self.consumer = self.node.listen(callback=self.on_message)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/pidbox.py"", line 89, in listen
    consumer = self.Consumer(channel=channel,
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/pidbox.py"", line 75, in Consumer
    return Consumer(
           ^^^^^^^^^
[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/messaging.py"", line 402, in __init__
    self.revive(self.channel)
[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/messaging.py"", line 424, in revive
    self.declare()
[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/messaging.py"", line 438, in declare
    queue.declare()
[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/entity.py"", line 617, in declare
    self._create_queue(nowait=nowait, channel=channel)
[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/entity.py"", line 626, in _create_queue
    self.queue_declare(nowait=nowait, passive=False, channel=channel)
[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/entity.py"", line 655, in queue_declare
    ret = channel.queue_declare(
          ^^^^^^^^^^^^^^^^^^^^^^
[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/transport/virtual/base.py"", line 538, in queue_declare
    return queue_declare_ok_t(queue, self._size(queue), 0)
                                     ^^^^^^^^^^^^^^^^^
[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/transport/redis.py"", line 1005, in _size
    sizes = pipe.execute()
            ^^^^^^^^^^^^^^
[2025-01-13 11:45:17,372: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/redis/client.py"", line 1524, in execute
    conn = self.connection_pool.get_connection(""MULTI"", self.shard_hint)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2025-01-13 11:45:17,373: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/redis/connection.py"", line 1422, in get_connection
    connection.connect()
[2025-01-13 11:45:17,373: WARNING/MainProcess]   File ""/home/airflow/.local/lib/python3.12/site-packages/redis/connection.py"", line 363, in connect
    raise ConnectionError(self._error_message(e))
[2025-01-13 11:45:17,373: WARNING/MainProcess] redis.exceptions.ConnectionError: Error 104 connecting to ****:****. Connection reset by peer.
[2025-01-13 11:45:17,373: WARNING/MainProcess] 2025-01-13T11:45:17Z
[2025-01-13 11:45:17,373: WARNING/MainProcess]
[2025-01-13 11:45:17,373: WARNING/MainProcess] <built-in method switch of gevent._gevent_c_greenlet_primitives.TrackedRawGreenlet object at 0x7f610735be40> failed with ConnectionError
```

Anurag-Kumar-01 (Assginee) on (2025-01-13 14:47:53 UTC): I want to take up this issue

"
2774432934,issue,closed,completed,Add bundle_name to ParseImportError,,ephraimbuddy,2025-01-08 06:38:36+00:00,['ephraimbuddy'],2025-01-15 23:09:45+00:00,2025-01-15 23:09:45+00:00,https://github.com/apache/airflow/issues/45474,[],[],
2772491964,issue,open,,Templates not resolved in DatabricksNotebookOperator then in DatabricksWorkflowTaskGroup used,"### Apache Airflow Provider(s)

databricks

### Versions of Apache Airflow Providers

apache-airflow-providers-databrick 6.9.0

### Apache Airflow version

2.9.2

### Operating System

Amazon Linux 2023

### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### What happened

Templates/macros defined in DatabricksNotebookOperator inside DatabricksWorkflowTaskGroup are passed to Notebook as template text, not as an expected value.
![image](https://github.com/user-attachments/assets/75f81721-a0db-4101-8fbb-ac055913d402)



### What you think should happen instead

Resolution happens on task execution. But Databricks job is already created at that moment

### How to reproduce

If you define notebook_params in DatabricksWorkflowTaskGroup templates will be resolved, if in DatabricksNotebookOperator inside  DatabricksWorkflowTaskGroup - not resolved.
In provided part of DAG (unrelated removed) param1 correctly passed to notebook, but param2 passed as `{{ ds }}`
```python
    with DatabricksWorkflowTaskGroup(
...
        notebook_params={""param1"": ""{{ ds }}""},
    ) as tg:
        task1 = DatabricksNotebookOperator(
...
            notebook_params={""param2"": ""{{ ds }}""},
        )
```

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",a-baturin,2025-01-07 10:37:17+00:00,[],2025-01-16 11:34:46+00:00,,https://github.com/apache/airflow/issues/45462,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('provider:databricks', '')]","[{'comment_id': 2574950292, 'issue_id': 2772491964, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 7, 10, 37, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2575257041, 'issue_id': 2772491964, 'author': 'potiuk', 'body': 'Feel free to implement it if you would like to', 'created_at': datetime.datetime(2025, 1, 7, 13, 7, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2590618206, 'issue_id': 2772491964, 'author': 'ajitg25', 'body': ""Hey @potiuk , I was looking into this issue and considering a fix, but I'm unable to figure out how to test it with Databricks. Do we need an account to test there and setup the connection with our PAT in Databricks, or is there a way to test it using the Airflow Breeze local setup?"", 'created_at': datetime.datetime(2025, 1, 14, 17, 19, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2590748644, 'issue_id': 2772491964, 'author': 'potiuk', 'body': 'You need databricks account (your own). We have no ""shared"" account to use. If you do not have it, better to look at other issues.', 'created_at': datetime.datetime(2025, 1, 14, 18, 5, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2595284148, 'issue_id': 2772491964, 'author': 'SantanuBasuMNS', 'body': 'Would be great to get a fix .\n\nOn first run : \n    with DatabricksWorkflowTaskGroup(\n...\n        notebook_params={""param1"": ""{{ ds }}""},\n    ) as tg:\n        task1 = DatabricksNotebookOperator(\n...\n            notebook_params={""param2"": ""{{ ds }}""},\n        )\n\nThe above code snippet works perfectly\n\nBut on rerun it goes back to the same issue .\nAlso dag_run attributes are not working as expected , literal values gets passed to the DatabricksNotebookOperator unless they are defined in the DatabricksWorkflowTaskGroup.\nBut on rerun again the literal values are passed.', 'created_at': datetime.datetime(2025, 1, 16, 11, 34, 44, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-07 10:37:20 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2025-01-07 13:07:25 UTC): Feel free to implement it if you would like to

ajitg25 on (2025-01-14 17:19:06 UTC): Hey @potiuk , I was looking into this issue and considering a fix, but I'm unable to figure out how to test it with Databricks. Do we need an account to test there and setup the connection with our PAT in Databricks, or is there a way to test it using the Airflow Breeze local setup?

potiuk on (2025-01-14 18:05:05 UTC): You need databricks account (your own). We have no ""shared"" account to use. If you do not have it, better to look at other issues.

SantanuBasuMNS on (2025-01-16 11:34:44 UTC): Would be great to get a fix .

On first run : 
    with DatabricksWorkflowTaskGroup(
...
        notebook_params={""param1"": ""{{ ds }}""},
    ) as tg:
        task1 = DatabricksNotebookOperator(
...
            notebook_params={""param2"": ""{{ ds }}""},
        )

The above code snippet works perfectly

But on rerun it goes back to the same issue .
Also dag_run attributes are not working as expected , literal values gets passed to the DatabricksNotebookOperator unless they are defined in the DatabricksWorkflowTaskGroup.
But on rerun again the literal values are passed.

"
2772485413,issue,open,,Add pre-commit to prevent usage of session.query in Airflow Core,"### Description

We should add a pre-commit that prevents the use of `session.query` in airflow core. We should limit it to the source and not the tests/ package for now. The core airflow was almost rid of `session.query`, but the usage is gradually increasing. Adding this would require fixing all the `session.query` usages in the core code



### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",ephraimbuddy,2025-01-07 10:34:31+00:00,['Prab-27'],2025-01-13 14:42:42+00:00,,https://github.com/apache/airflow/issues/45461,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('kind:feature', 'Feature Requests'), ('good first issue', '')]","[{'comment_id': 2575281883, 'issue_id': 2772485413, 'author': 'Prab-27', 'body': ""@potiuk ,I'd like to work on this"", 'created_at': datetime.datetime(2025, 1, 7, 13, 20, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2575290709, 'issue_id': 2772485413, 'author': 'ephraimbuddy', 'body': ""> @potiuk ,I'd like to work on this\r\n\r\nAssigned to you @Prab-27"", 'created_at': datetime.datetime(2025, 1, 7, 13, 24, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2587290720, 'issue_id': 2772485413, 'author': 'Anurag-Kumar-01', 'body': 'i also want to take up this issue', 'created_at': datetime.datetime(2025, 1, 13, 14, 42, 41, tzinfo=datetime.timezone.utc)}]","Prab-27 (Assginee) on (2025-01-07 13:20:10 UTC): @potiuk ,I'd like to work on this

ephraimbuddy (Issue Creator) on (2025-01-07 13:24:29 UTC): Assigned to you @Prab-27

Anurag-Kumar-01 on (2025-01-13 14:42:41 UTC): i also want to take up this issue

"
2772257595,issue,closed,completed,Cleanup Context type hinting for Task SDK,"I have change typehints from `Context` to `Mapping[str, Any]` in https://github.com/apache/airflow/pull/45444. Let's make it consistent -- the `Context` class currently manages deprecations too which we haven't ported to Task SDK yet.",kaxil,2025-01-07 08:54:43+00:00,['kaxil'],2025-01-15 05:36:01+00:00,2025-01-15 05:36:01+00:00,https://github.com/apache/airflow/issues/45454,"[('area:dev-tools', '')]",[],
2772207481,issue,closed,completed,"Kubernetes XCOM 10.1.0 regresssion, sidecar isn't being shutdown after job is complete","### Apache Airflow Provider(s)

cncf-kubernetes

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes==10.1.0


### Apache Airflow version

2.10.4

### Operating System

alpine

### Deployment

Other Docker-based deployment

### Deployment details

self hosted kubernetes on AWS

### What happened

When the KubernetesPodOperator runs with xcom_push

it ends up stalling in the end because of 

```
Running command... if [ -s /airflow/xcom/return.json ]; then cat /airflow/xcom/return.json; else echo __airflow_xcom_result_empty__; fi
Running command... kill -2 $(pgrep -u $(whoami) -f trap)
stderr from command: whoami: unknown uid 1000
pgrep: unknown user -f
```

### What you think should happen instead

A recent change caused this to change because it looks like the username changed, (now root). And we don't want to shut down all processed only the sleep that is causing the sidecar to wait. However, the new setup doesn't pick up those processed, and just returns an empty result.

### How to reproduce

Can be tested manually either with the kubernetespodoperator, or via. debian/ubuntu by itself

```
 docker run -it alpine sh
 sh -c trap ""exit 0"" INT; while true; do sleep 1; done;
 
 # in another terminal or docker desktop
 $(pgrep -u $(whoami) -f trap)
 # it returns nothing because of -f trap. If it is removed we get three process ids and it works
```

### Anything else

All the time.

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",kjuulh,2025-01-07 08:31:17+00:00,['kjuulh'],2025-01-23 14:27:44+00:00,2025-01-23 14:27:44+00:00,https://github.com/apache/airflow/issues/45452,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2574676797, 'issue_id': 2772207481, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 7, 8, 31, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2575244864, 'issue_id': 2772207481, 'author': 'potiuk', 'body': 'Assigned you', 'created_at': datetime.datetime(2025, 1, 7, 13, 1, 2, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-07 08:31:20 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2025-01-07 13:01:02 UTC): Assigned you

"
2772083158,issue,open,,AIP-72: Add support to get Variables in task sdk outside of context,"### Body

To write dags using the task sdk, we need to add support to get variables using client side definitions and the task sdk machinery.

Move Secrets Backend on the client side!

Currently, Secrets Backends are only supported (and configured) on the API Server. The Python Task SDK client does not do any lookup locally and relies on Server. 

This is for supporting the following use-case:

>Deploying tasks in transient or ephemeral environments (e.g., GPU cloud services) where secrets need to be retrieved dynamically via the API

This allows for Secrets to be defined once and allows central management with fewer configurations on the worker.

However, there is a complementary use case as below

>An organization processes sensitive customer data for financial transactions. Regulatory and security policies mandate that certain credentials (e.g., database credentials, API keys, encryption keys) cannot leave a specific network zone. This includes ensuring that the Airflow scheduler, API server, or other components outside this zone cannot access these secrets.

This use-case means we need a way where secrets are already provisioned locally on the worker. 

Secrets Backend will be available to be configured on the Client by default. Packaging-wise: The External Secrets Backend will be part of Providers, which depend on Task SDK. 

For the API-Server, by default it will only look at Env Variable and then Database by default.
1. Optionally, if users want the API server to fetch secrets from the External Secrets backend, they will have to install the Task SDK (and relevant providers like Hashicorp) with the scheduler/Airflow core code.
2. Longer term: we might be able to remove requirements on Task SDK by implementing some protocol but this will be discussed again post AF 3.0.",amoghrajesh,2025-01-07 07:22:39+00:00,['amoghrajesh'],2025-01-21 22:26:05+00:00,,https://github.com/apache/airflow/issues/45449,"[('kind:feature', 'Feature Requests'), ('area:core', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK'), ('area:task-sdk', None)]",[],
2771605105,issue,closed,completed,Add Support for `Connection.extra_dejson` in Task SDK,"Airflow Templates support `extra_dejson` that should contain a Python Dict by deserializing `Connection.extra` string.

https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html#airflow-connections-in-templates",kaxil,2025-01-06 23:18:42+00:00,['amoghrajesh'],2025-01-07 16:59:37+00:00,2025-01-07 16:59:37+00:00,https://github.com/apache/airflow/issues/45443,"[('kind:feature', 'Feature Requests'), ('area:core', '')]",[],
2771260356,issue,open,,SPIKE: Move Macros to Task SDK,"Make a plan for porting https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html#macros to Task SDK.

Macros are used in Jinja Templates and some of them are provided built-in and users can register some via Airflow Plugins: https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/plugins.html

",kaxil,2025-01-06 19:01:48+00:00,[],2025-01-28 08:15:20+00:00,,https://github.com/apache/airflow/issues/45440,"[('area:plugins', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]","[{'comment_id': 2574859341, 'issue_id': 2771260356, 'author': 'kaxil', 'body': 'Some of the macros that did not have new dependencies have been ported in https://github.com/apache/airflow/pull/45444', 'created_at': datetime.datetime(2025, 1, 7, 9, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2618187542, 'issue_id': 2771260356, 'author': 'vatsrahul1001', 'body': ""@kaxil @amoghrajesh While testing our regression DAG's with airflow main we see our DAG's are failing as macros are not implemented. Do we plan to get this done before airflow alpha's?\n\nMore details here: https://astronomer.slack.com/archives/C01UJJEN0P3/p1738051755616179?thread_ts=1738051741.614759&cid=C01UJJEN0P3"", 'created_at': datetime.datetime(2025, 1, 28, 8, 15, 19, tzinfo=datetime.timezone.utc)}]","kaxil (Issue Creator) on (2025-01-07 09:57:00 UTC): Some of the macros that did not have new dependencies have been ported in https://github.com/apache/airflow/pull/45444

vatsrahul1001 on (2025-01-28 08:15:19 UTC): @kaxil @amoghrajesh While testing our regression DAG's with airflow main we see our DAG's are failing as macros are not implemented. Do we plan to get this done before airflow alpha's?

More details here: https://astronomer.slack.com/archives/C01UJJEN0P3/p1738051755616179?thread_ts=1738051741.614759&cid=C01UJJEN0P3

"
2770825638,issue,closed,completed,Port `SecretsMasker` log filter to Task SDK,https://github.com/apache/airflow/blob/9ba279d15f26088400b15281b3cc346e2d7a0e30/airflow/utils/log/secrets_masker.py#L147,kaxil,2025-01-06 14:51:02+00:00,"['ashb', 'kaxil', 'amoghrajesh']",2025-02-05 07:34:50+00:00,2025-02-05 07:34:50+00:00,https://github.com/apache/airflow/issues/45438,"[('area:logging', ''), ('area:secrets', '')]","[{'comment_id': 2575368511, 'issue_id': 2770825638, 'author': 'Prab-27', 'body': ""@kaxil I'd like to work on this issue. As I understand it, I need to implement the `redact_jwt` method from \r\n[airflow/sdk/log.py](https://github.com/apache/airflow/blob/main/task_sdk/src/airflow/sdk/log.py)\r\ninstead of current methods used in this class to redact secrets from logs \r\nPlease let me know if this interpretation is correct or if there are any additional requirements ."", 'created_at': datetime.datetime(2025, 1, 7, 14, 2, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2575383637, 'issue_id': 2770825638, 'author': 'kaxil', 'body': 'Hi, @Prab-27. My bad, I should not have kept this issue unassigned. This issue involves porting [secret_masker functionality](https://github.com/apache/airflow/blob/9ba279d15f26088400b15281b3cc346e2d7a0e30/airflow/utils/log/secrets_masker.py#L147) to the Task SDK, which is part of [AIP-72](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-72+Task+Execution+Interface+aka+Task+SDK) and would involve making some design choices.\r\n\r\nSince we are at a time crunch for Airflow 3.0, I am assigning it to Amogh, Ash & I since it would involve a lot of discussion. Thank you though for your willingness to pick it up.', 'created_at': datetime.datetime(2025, 1, 7, 14, 9, 56, tzinfo=datetime.timezone.utc)}]","Prab-27 on (2025-01-07 14:02:48 UTC): @kaxil I'd like to work on this issue. As I understand it, I need to implement the `redact_jwt` method from 
[airflow/sdk/log.py](https://github.com/apache/airflow/blob/main/task_sdk/src/airflow/sdk/log.py)
instead of current methods used in this class to redact secrets from logs 
Please let me know if this interpretation is correct or if there are any additional requirements .

kaxil (Issue Creator) on (2025-01-07 14:09:56 UTC): Hi, @Prab-27. My bad, I should not have kept this issue unassigned. This issue involves porting [secret_masker functionality](https://github.com/apache/airflow/blob/9ba279d15f26088400b15281b3cc346e2d7a0e30/airflow/utils/log/secrets_masker.py#L147) to the Task SDK, which is part of [AIP-72](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-72+Task+Execution+Interface+aka+Task+SDK) and would involve making some design choices.

Since we are at a time crunch for Airflow 3.0, I am assigning it to Amogh, Ash & I since it would involve a lot of discussion. Thank you though for your willingness to pick it up.

"
2770668904,issue,open,,Port Taskflow decorators over to Task SDK,,kaxil,2025-01-06 13:30:44+00:00,"['ashb', 'kaxil', 'amoghrajesh']",2025-01-06 13:30:44+00:00,,https://github.com/apache/airflow/issues/45436,[],[],
2770665765,issue,open,,Move Secrets Backend for Variables & Connections support to Task SDK,"Currently, the Task Execution API server handles secrets backend logic, which is ideal for scenarios such as:

>Deploying tasks in transient or ephemeral environments (e.g., GPU cloud services) where secrets need to be retrieved dynamically via the API.

However, there is a complementary use case where secrets or configurations are already provisioned locally on the worker. 

>An organization processes sensitive customer data for financial transactions. Regulatory and security policies mandate that certain credentials (e.g., database credentials, API keys, encryption keys) cannot leave a specific network zone. This includes ensuring that the Airflow scheduler, API server, or other components outside this zone cannot access these secrets.

This use-case means we need a way where secrets are already provisioned locally on the worker. 

Secrets Backend will be available to be configured on the Client by default.

Longer term the resolution order will be configurable but for now it will be the same as now:
    1. External Secret Backend on worker
    2. Environment Variable on worker
    3. Reach out to API Server

Packaging-wise: The External Secrets Backend will be part of Providers, which depend on Task SDK. The resolution code would be in Task SDK

For the API-Server, by default it will only look at Env Variable and then Database by default.
    1. Optionally, if users want the API server to fetch secrets from the External Secrets backend, they will have to install the Task SDK (and relevant providers like Hashicorp) with the scheduler/Airflow core code.
    2. Longer term: we might be able to remove requirements on Task SDK by implementing some protocol but this will be discussed again post AF 3.0.
",kaxil,2025-01-06 13:29:04+00:00,[],2025-01-28 17:08:47+00:00,,https://github.com/apache/airflow/issues/45435,"[('area:secrets', ''), ('area:task-sdk', None)]","[{'comment_id': 2592629440, 'issue_id': 2770665765, 'author': 'ashb', 'body': ""Do we need to limit worker/client to only env var, or could we allow it to use any secret store if it's configured?\n\ni.e. the full resolution order could be \n- worker env vars\n- worker secret backend\n- API server env vars\n- API server secret backend\n- API server database lookup"", 'created_at': datetime.datetime(2025, 1, 15, 12, 14, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2619591881, 'issue_id': 2770665765, 'author': 'ashb', 'body': 'Look at `get_connection_from_secrets` in airflow.models.connection for where this is used (and you can follow the chain back up to settings and BaseSecretBackend etc.', 'created_at': datetime.datetime(2025, 1, 28, 17, 8, 46, tzinfo=datetime.timezone.utc)}]","ashb on (2025-01-15 12:14:02 UTC): Do we need to limit worker/client to only env var, or could we allow it to use any secret store if it's configured?

i.e. the full resolution order could be 
- worker env vars
- worker secret backend
- API server env vars
- API server secret backend
- API server database lookup

ashb on (2025-01-28 17:08:46 UTC): Look at `get_connection_from_secrets` in airflow.models.connection for where this is used (and you can follow the chain back up to settings and BaseSecretBackend etc.

"
2770610880,issue,open,,Add color support for XCom,"### Description

When XCcom values are JSON it will be good to have them highlighted and formatted for better readability. Once there is a port of ReactJSON or other approach to copy JSON values the same could be applied here.

Ref : https://github.com/apache/airflow/pull/44869#issuecomment-2538718473

### Use case/motivation

_No response_

### Related issues

Ref : https://github.com/apache/airflow/pull/44869#issuecomment-2538718473

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",tirkarthi,2025-01-06 12:59:07+00:00,[],2025-01-11 06:45:56+00:00,,https://github.com/apache/airflow/issues/45432,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]","[{'comment_id': 2573651383, 'issue_id': 2770610880, 'author': 'eladkal', 'body': 'Probably should consider this also for Airflow Variables with json?', 'created_at': datetime.datetime(2025, 1, 6, 18, 13, 26, tzinfo=datetime.timezone.utc)}]","eladkal on (2025-01-06 18:13:26 UTC): Probably should consider this also for Airflow Variables with json?

"
2770485153,issue,open,,SPIKE: Figure out change needed in Providers to use Task SDK,"This is a SPIKE issue to identify things needed in Providers to depend on Task SDK and remove `apache-airflow` as dependency. Things like `BaseOperator`, usages of `utils` module, DB calls etc will need to be changed. Identify what more and create a plan. Once this issue is done, the effort can be parallelized



",kaxil,2025-01-06 11:47:06+00:00,[],2025-01-06 11:49:24+00:00,,https://github.com/apache/airflow/issues/45430,"[('area:providers', ''), ('kind:meta', 'High-level information important to the community')]",[],
2770480997,issue,open,,Handle Triggerer / Deferrable Operators with Task SDK machinery,Figure out a strategy to handle Triggers and Triggerer with Task SDK machinery,kaxil,2025-01-06 11:44:36+00:00,"['ashb', 'kaxil']",2025-01-06 11:46:48+00:00,,https://github.com/apache/airflow/issues/45429,"[('area:async-operators', 'AIP-40: Deferrable (""Async"") Operators'), ('area:Triggerer', '')]",[],
2770467702,issue,open,,Make Serialization & De-serialization stricter with versioning with Task SDK,"Currently the serialization and de-serialization logic lives in [`airflow/serialization`](https://github.com/apache/airflow/tree/f03b1d4d996cf2b3f62c21f8c6a38aa53ba1e3be/airflow/serialization) in the Core. With Airflow 3 and the separation of Task SDK, we will need to make serialization and its versioning much stricter.

We should bump the current DAG serialization version to 2.

The serialization code should live closer to language-specific Task SDK as it knows best how to serialize objects in a language to a JSON-formatted string.

The Core/scheduler will contain the de-serialization code -- and it does need to be language specific as it contains only the info needed by the scheduler.

The contract between those two is the [`schema.json`](https://github.com/apache/airflow/blob/main/airflow/serialization/schema.json) file that contains the serialization. Both the client and server could support multiple versions at a time. ",kaxil,2025-01-06 11:36:35+00:00,"['ashb', 'kaxil']",2025-01-06 11:38:46+00:00,,https://github.com/apache/airflow/issues/45428,"[('area:serialization', '')]",[],
2770454004,issue,open,,Convert the `KubernetesExecutor` to run tasks using new Task SDK supervisor code,Same as https://github.com/apache/airflow/pull/44427 for `KubernetesExecutor`,kaxil,2025-01-06 11:28:21+00:00,[],2025-01-17 15:24:21+00:00,,https://github.com/apache/airflow/issues/45427,"[('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2770451517,issue,closed,completed,"Change Celery executor to accept new ""Activity"" payload format and update it to run tasks via task SDK supervisor instead of LocalTaskJob",Similar to https://github.com/apache/airflow/pull/44427 for CeleryExecutor,kaxil,2025-01-06 11:26:50+00:00,['ashb'],2025-01-31 15:24:06+00:00,2025-01-31 15:24:06+00:00,https://github.com/apache/airflow/issues/45426,"[('area:core', '')]",[],
2770444656,issue,open,,Move ObjectStoragePath to Task SDK,https://airflow.apache.org/docs/apache-airflow/stable/tutorial/objectstorage.html#object-storage,kaxil,2025-01-06 11:22:51+00:00,[],2025-01-06 11:23:36+00:00,,https://github.com/apache/airflow/issues/45425,[],[],
2770438659,issue,open,,Ensure Notifiers work with Task SDK,"https://airflow.apache.org/docs/apache-airflow/stable/howto/notifications.html#using-a-notifier

https://github.com/apache/airflow/blob/f03b1d4d996cf2b3f62c21f8c6a38aa53ba1e3be/airflow/notifications/basenotifier.py#L34",kaxil,2025-01-06 11:19:16+00:00,[],2025-01-06 11:19:16+00:00,,https://github.com/apache/airflow/issues/45424,[],[],
2770436582,issue,open,,Ensure Task Instance Listeners work with Task SDK,"Part of https://github.com/apache/airflow/issues/45491

Port Listeners https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/listeners.html#listeners to Task SDK. 

https://github.com/apache/airflow/blob/3fc73229da651420cd2b974e1fff9d9786d09ec1/airflow/listeners/spec/taskinstance.py#L33-L54",kaxil,2025-01-06 11:18:02+00:00,['mobuchowski'],2025-01-17 15:24:20+00:00,,https://github.com/apache/airflow/issues/45423,"[('area:Listeners', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2770392331,issue,closed,completed,Allow retrieving Variable from Task Context,"Similar to https://github.com/apache/airflow/pull/45043, we need a similar thing for Task SDK to fetch Variable from Task Context.

Requirement:
- A minimal `Variable` user-facing object in Task SDK definition for use in the DAG file
- Logic to get `Variable` in the context. 

https://github.com/apache/airflow/blob/f03b1d4d996cf2b3f62c21f8c6a38aa53ba1e3be/airflow/models/taskinstance.py#L1043-L1046

https://github.com/apache/airflow/blob/f03b1d4d996cf2b3f62c21f8c6a38aa53ba1e3be/airflow/utils/context.py#L117-L138


Now, we will need following 3 related objects:
- `VariableResponse` is auto-generated and tightly coupled with the API schema.
- `VariableResult` is runtime-specific and meant for internal communication between Supervisor & Task Runner.
- `Variable` class here is where the public-facing, user-relevant aspects are exposed, hiding internal details.
",kaxil,2025-01-06 10:52:32+00:00,['amoghrajesh'],2025-01-07 06:05:44+00:00,2025-01-07 06:05:44+00:00,https://github.com/apache/airflow/issues/45421,"[('area:core', ''), ('area:core-operators', 'Operators, Sensors and hooks within Core Airflow'), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2770330284,issue,closed,completed,Handle clearing XCom in `run` Execution endpoint,"We should clear XCom value when task starts execution in the Execution API server. This is how it is done now:

https://github.com/apache/airflow/blob/f03b1d4d996cf2b3f62c21f8c6a38aa53ba1e3be/airflow/models/taskinstance.py#L2846-L2848

Ideally, this can be done in the [`run` endpoint](https://github.com/apache/airflow/blob/f03b1d4d996cf2b3f62c21f8c6a38aa53ba1e3be/airflow/api_fastapi/execution_api/routes/task_instances.py#L65). We need to ensure we don't clear it for Deferrable tasks.

Port the necessary tests from current execution model to the API server",kaxil,2025-01-06 10:18:34+00:00,['amoghrajesh'],2025-01-09 11:08:04+00:00,2025-01-09 11:08:04+00:00,https://github.com/apache/airflow/issues/45419,"[('area:API', ""Airflow's REST/HTTP API"")]",[],
2769875238,issue,closed,completed,Add namespace variable to hashicorp vault client kwargs,"### Description

Currently the hashicorp vault client cannot set the ""namespace"" variable (it's defaulted to None).

Believe its a case of adding ""namespace"" to the back end kwargs in the airflow.cfg file and then a slight tweak to this [file](https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/hashicorp/_internal_client/vault_client.py).

and adding the following

```python
namespace = self.kwargs[""namespace""] if 'namespace' in self.kwargs else None

_client = hvac.Client(url=self.url, namespace=namespace,**self.kwargs)
```


### Use case/motivation

We use the namespace to logically separate environments and so need the ability to pass the ""namespace"" to the backend kwargs so we can authenticate with vault.

### Related issues

N/A

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",davidsharp7,2025-01-06 05:46:12+00:00,['davidsharp7'],2025-01-07 19:46:02+00:00,2025-01-07 19:45:26+00:00,https://github.com/apache/airflow/issues/45413,"[('area:providers', ''), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:hashicorp', 'Hashicorp provider related issues')]","[{'comment_id': 2572325045, 'issue_id': 2769875238, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 6, 5, 46, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2572355346, 'issue_id': 2769875238, 'author': 'potiuk', 'body': 'Sure - go ahead', 'created_at': datetime.datetime(2025, 1, 6, 6, 13, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2576098171, 'issue_id': 2769875238, 'author': 'davidsharp7', 'body': 'Can pass namespace via backend kwargs which will map automatically to the namespace variable.', 'created_at': datetime.datetime(2025, 1, 7, 19, 46, 1, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-06 05:46:14 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2025-01-06 06:13:17 UTC): Sure - go ahead

davidsharp7 (Issue Creator) on (2025-01-07 19:46:01 UTC): Can pass namespace via backend kwargs which will map automatically to the namespace variable.

"
2769215549,issue,closed,completed,Airflow logs are not getting generated in AKS PVC/PV (azureblob-fuse-premium). All pods are failing.,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.3

### What happened?

I am deploying airflow 2.9.3 in AKS with Helm Chart 1.15.0. I am using **CeleryExecutor**. For logs I have created a PV and PVC with Storageclass **azureblob-fuse-premium**.  But I found that after deploy all pods are getting failed when trying to create files in PV. 

Note :: Inside from pods I have tried to generate a files (touch test.txt) from the path **/opt/airflow/logs** and it's generated in PV [Azure storage account]

### What you think should happen instead?

I have tried using the PVC with Azure **file-share** driver, the logs are getting generated but airflow pods are unable to fetch the log from the PV. Because Azure file-share does not allow os.chmod operation. Then I have tried with azureblob-fuse-premium pvc after following this document [Azure Airflow Document](https://learn.microsoft.com/en-us/azure/aks/airflow-deploy). **The logs should be generated in the PV and airflow pods should be able to read those logs.**

**Logs**

Usage: python -m celery [OPTIONS] COMMAND [ARGS]...
Try 'python -m celery --help' for help.

Error: Invalid value for '-A' / '--app':
Unable to load celery application.
While trying to load the module airflow.providers.celery.executors.celery_executor.app the following error occurred:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/pathlib.py"", line 1288, in mkdir
    self._accessor.mkdir(self, mode)
FileNotFoundError: [Errno 2] No such file or directory: '/opt/airflow/logs/scheduler/2025-01-05'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/pathlib.py"", line 1288, in mkdir
    self._accessor.mkdir(self, mode)
FileNotFoundError: [Errno 2] No such file or directory: '/opt/airflow/logs/scheduler'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/logging/config.py"", line 563, in configure
    handler = self.configure_handler(handlers[name])
  File ""/usr/local/lib/python3.8/logging/config.py"", line 744, in configure_handler
    result = factory(**kwargs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py"", line 53, in __init__
    Path(self._get_log_directory()).mkdir(parents=True, exist_ok=True)
  File ""/usr/local/lib/python3.8/pathlib.py"", line 1292, in mkdir
    self.parent.mkdir(parents=True, exist_ok=True)
  File ""/usr/local/lib/python3.8/pathlib.py"", line 1293, in mkdir
    self.mkdir(mode, parents=False, exist_ok=exist_ok)
  File ""/usr/local/lib/python3.8/pathlib.py"", line 1288, in mkdir
    self._accessor.mkdir(self, mode)
FileNotFoundError: [Errno 2] No such file or directory: '/opt/airflow/logs/scheduler'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.8/site-packages/kombu/utils/imports.py"", line 59, in symbol_by_name
    module = imp(module_name, package=package, **kwargs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/celery/utils/imports.py"", line 109, in import_from_cwd
    return imp(module, package=package)
  File ""/usr/local/lib/python3.8/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 843, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/__init__.py"", line 74, in <module>
    settings.initialize()
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py"", line 531, in initialize
    LOGGING_CLASS_PATH = configure_logging()
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/logging_config.py"", line 74, in configure_logging
    raise e
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/logging_config.py"", line 69, in configure_logging
    dictConfig(logging_config)
  File ""/usr/local/lib/python3.8/logging/config.py"", line 808, in dictConfig
    dictConfigClass(config).configure()
  File ""/usr/local/lib/python3.8/logging/config.py"", line 570, in configure
    raise ValueError('Unable to configure handler '
ValueError: Unable to configure handler 'processor'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.8/site-packages/celery/bin/celery.py"", line 58, in convert
    return find_app(value)
  File ""/home/airflow/.local/lib/python3.8/site-packages/celery/app/utils.py"", line 383, in find_app
    sym = symbol_by_name(app, imp=imp)
  File ""/home/airflow/.local/lib/python3.8/site-packages/kombu/utils/imports.py"", line 61, in symbol_by_name
    reraise(ValueError,
  File ""/home/airflow/.local/lib/python3.8/site-packages/kombu/exceptions.py"", line 34, in reraise
    raise value.with_traceback(tb)
  File ""/home/airflow/.local/lib/python3.8/site-packages/kombu/utils/imports.py"", line 59, in symbol_by_name
    module = imp(module_name, package=package, **kwargs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/celery/utils/imports.py"", line 109, in import_from_cwd
    return imp(module, package=package)
  File ""/usr/local/lib/python3.8/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 843, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/__init__.py"", line 74, in <module>
    settings.initialize()
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py"", line 531, in initialize
    LOGGING_CLASS_PATH = configure_logging()
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/logging_config.py"", line 74, in configure_logging
    raise e
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/logging_config.py"", line 69, in configure_logging
    dictConfig(logging_config)
  File ""/usr/local/lib/python3.8/logging/config.py"", line 808, in dictConfig
    dictConfigClass(config).configure()
  File ""/usr/local/lib/python3.8/logging/config.py"", line 570, in configure
    raise ValueError('Unable to configure handler '
ValueError: Couldn't import 'airflow.providers.celery.executors.celery_executor.app': Unable to configure handler 'processor'
PS C:\AirflowSetup\Prod> kubectl logs airflow-worker-6f9f456bbd-bdfnk -n airflow293
Defaulted container ""worker"" out of: worker, wait-for-airflow-migrations (init)
....................
ERROR! Maximum number of retries (20) reached.

Last check result:
$ airflow db check
Unable to load the config, contains a configuration error.
Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/pathlib.py"", line 1288, in mkdir
    self._accessor.mkdir(self, mode)
FileNotFoundError: [Errno 2] No such file or directory: '/opt/airflow/logs/scheduler/2025-01-05'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/pathlib.py"", line 1288, in mkdir
    self._accessor.mkdir(self, mode)
FileNotFoundError: [Errno 2] No such file or directory: '/opt/airflow/logs/scheduler'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/logging/config.py"", line 563, in configure
    handler = self.configure_handler(handlers[name])
  File ""/usr/local/lib/python3.8/logging/config.py"", line 744, in configure_handler
    result = factory(**kwargs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/log/file_processor_handler.py"", line 53, in __init__
    Path(self._get_log_directory()).mkdir(parents=True, exist_ok=True)
  File ""/usr/local/lib/python3.8/pathlib.py"", line 1292, in mkdir
    self.parent.mkdir(parents=True, exist_ok=True)
  File ""/usr/local/lib/python3.8/pathlib.py"", line 1293, in mkdir
    self.mkdir(mode, parents=False, exist_ok=exist_ok)
  File ""/usr/local/lib/python3.8/pathlib.py"", line 1288, in mkdir
    self._accessor.mkdir(self, mode)
FileNotFoundError: [Errno 2] No such file or directory: '/opt/airflow/logs/scheduler'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/airflow/.local/bin/airflow"", line 5, in <module>
    from airflow.__main__ import main
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/__init__.py"", line 74, in <module>
    settings.initialize()
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/settings.py"", line 531, in initialize
    LOGGING_CLASS_PATH = configure_logging()
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/logging_config.py"", line 74, in configure_logging
    raise e
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/logging_config.py"", line 69, in configure_logging
    dictConfig(logging_config)
  File ""/usr/local/lib/python3.8/logging/config.py"", line 808, in dictConfig
    dictConfigClass(config).configure()
  File ""/usr/local/lib/python3.8/logging/config.py"", line 570, in configure
    raise ValueError('Unable to configure handler '
ValueError: Unable to configure handler 'processor'

### How to reproduce

As I am doing setup of airgap airflow deployment.
1. I have created a docker image in ACR.
2. Created PV and PVC.
3. Used Helm chart to deploy.

```
logs:
  # Configuration for empty dir volume (if logs.persistence.enabled == false)
  # emptyDirConfig:
  #   sizeLimit: 1Gi
  #   medium: Memory
  persistence:
    # Enable persistent volume for storing logs
    enabled: true
    # Volume size for logs
    size: 50Gi
    # Annotations for the logs PVC
    annotations: {}
    # If using a custom storageClass, pass name here
    storageClassName: azureblob-fuse-premium
    ## the name of an existing PVC to use
    existingClaim: pvc-airflow-logs-blobfuse
```

### Operating System

Linux-Ubantu

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.25.0
apache-airflow-providers-celery==3.7.2
apache-airflow-providers-cncf-kubernetes==8.4.2
apache-airflow-providers-common-io==1.3.2
apache-airflow-providers-common-sql==1.14.2
apache-airflow-providers-docker==3.12.2
apache-airflow-providers-elasticsearch==5.4.1
apache-airflow-providers-fab==1.2.2
apache-airflow-providers-ftp==3.10.0
apache-airflow-providers-google==10.21.0
apache-airflow-providers-grpc==3.5.2
apache-airflow-providers-hashicorp==3.7.1
apache-airflow-providers-http==4.12.0
apache-airflow-providers-imap==3.6.1
apache-airflow-providers-microsoft-azure==10.0.0
apache-airflow-providers-microsoft-winrm==3.4.0
apache-airflow-providers-mysql==5.6.2
apache-airflow-providers-odbc==4.6.2
apache-airflow-providers-openlineage==1.9.1
apache-airflow-providers-postgres==5.11.2
apache-airflow-providers-redis==3.7.1
apache-airflow-providers-sendgrid==3.5.1
apache-airflow-providers-sftp==4.10.2
apache-airflow-providers-slack==8.7.1
apache-airflow-providers-smtp==1.7.1
apache-airflow-providers-snowflake==4.1.0
apache-airflow-providers-sqlite==3.8.1
apache-airflow-providers-ssh==3.11.2

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Airflow - 2.9.3
Python - 3.8
Helm - 1.15.0
Kubernetes - 1.29.9

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",abhijit-sarkar-infocepts,2025-01-05 10:46:39+00:00,[],2025-01-05 12:19:46+00:00,2025-01-05 12:19:46+00:00,https://github.com/apache/airflow/issues/45405,"[('kind:bug', 'This is a clearly a bug'), ('area:logging', ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2571582291, 'issue_id': 2769215549, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 5, 10, 46, 42, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-05 10:46:42 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2768820876,issue,open,,Task Instance returning wrong url through log_url,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Hi,

Looks like the URL returned through the property log_url is not right:

![image](https://github.com/user-attachments/assets/6c0b1e4c-c570-4fe6-85d5-40cb10a76ced)

This URL is not opening correctly the task logs (nothing selected) and seems to be the base_date the issue:
![image](https://github.com/user-attachments/assets/1ca2e436-3e93-4dac-961c-297718f80817)

If we remove the base_date from the URL now it is open correctly:

![image](https://github.com/user-attachments/assets/1928fb2c-e10c-4ef7-a813-1b6a6aceb15c)

Thank you!


### What you think should happen instead?

_No response_

### How to reproduce

Just set up the Airflow SMTP and create and run a DAG raising an error.

### Operating System

Ubuntu 20.04.5

### Versions of Apache Airflow Providers

apache-airflow-providers-microsoft-mssql==3.9.1
apache-airflow-providers-snowflake==5.8.0
apache-airflow-providers-microsoft-azure==11.0.0
apache-airflow-providers-http==4.13.2
apache-airflow-providers-cncf-kubernetes==9.0.1
apache-airflow-providers-common-sql==1.19.0
apache-airflow-providers-google==10.25.0

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",fbmatilla,2025-01-04 12:40:54+00:00,[],2025-02-05 04:06:42+00:00,,https://github.com/apache/airflow/issues/45395,"[('kind:bug', 'This is a clearly a bug'), (""Can't Reproduce"", 'The problem cannot be reproduced'), ('area:logging', ''), ('pending-response', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2571410522, 'issue_id': 2768820876, 'author': 'gopidesupavan', 'body': 'Cant reproduce this issue, please provide the dag your using.\r\n\r\nLooking at your first screenshot your it seems your on XCom tab?. try refresh and check', 'created_at': datetime.datetime(2025, 1, 4, 21, 1, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2571716628, 'issue_id': 2768820876, 'author': 'fbmatilla', 'body': 'Hi @gopidesupavan, \r\n\r\nthank you for your reply.\r\n\r\nIt is really weird because the selected menu is ""Logs"" but in the body you see ""No XCom"".\r\n\r\n\r\nIn order to reproduce it you will need to:\r\n\r\n1.- Set up the smtp_default connection to a right SMTP server:\r\n![image](https://github.com/user-attachments/assets/e249e6c5-215d-45f3-b51a-7088e273e774)\r\n\r\n2.- Put the following DAG in you DAG folder, and please change ""your@email.com"" by your email:\r\n```\r\n\r\nfrom airflow import DAG\r\nfrom airflow.utils.dates import days_ago\r\nfrom airflow.operators.python import PythonOperator\r\nfrom typing import Any\r\n\r\n\r\ndefault_args = {\r\n    \'owner\': \'Axesor\',\r\n    \'depends_on_past\': False,\r\n    \'start_date\': days_ago(1),\r\n    \'email\': ""your@email.com"",\r\n    \'email_on_failure\': True\r\n}\r\n\r\n\r\n\r\ndef task_1(ds: Any, **kwargs: Any) -> None:\r\n    raise Exception(""Error"")\r\n\r\n\r\nwith DAG(\r\n    \'example\',\r\n    schedule_interval=None,\r\n    default_args=default_args\r\n) as dag:\r\n\r\n    task1 = PythonOperator (\r\n        task_id=""task_1"",\r\n        python_callable=task_1\r\n    )\r\n\r\n    task1\r\n```\r\n\r\n3.- Run the DAG and you will receive an email like this:\r\n![image](https://github.com/user-attachments/assets/4a709273-b353-4e49-8ffc-c42dacea2756)\r\n\r\n4.- If you click on the Log link you will see the ""No XCom"" in the main screen and if you remove the base_date from the URL then you will see the logs now correctly.\r\n\r\nLet me know please.', 'created_at': datetime.datetime(2025, 1, 5, 18, 45, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2600967036, 'issue_id': 2768820876, 'author': 'kolfild26', 'body': 'Hi all,\nJust would like to ""vote"" for this as I\'m facing exactly the same behaviour.\n- log_url leads to nothing\n- XCom and Log are mixed up\n- removing the base_date fixes the issue.\n\nStill struggling in reproducing as when I am trying to cut the dag to provide an MVP example the issue disappears.\nIt\'s defenetely an intermittent error and it\'s hard to reproduce but it does exist so don\'t close this please.', 'created_at': datetime.datetime(2025, 1, 19, 18, 13, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2600968368, 'issue_id': 2768820876, 'author': 'gopidesupavan', 'body': 'Will look into this week. @fbmatilla thanks for the steps.', 'created_at': datetime.datetime(2025, 1, 19, 18, 17, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2600993527, 'issue_id': 2768820876, 'author': 'fbmatilla', 'body': 'Thank you @gopidesupavan!', 'created_at': datetime.datetime(2025, 1, 19, 19, 36, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2607565850, 'issue_id': 2768820876, 'author': 'mtsadler-branch', 'body': 'Here\'s a helper function for anyone running into this issue while calling `TaskInstance.log_url` directly:\n\n```python\ndef get_log_url(context):\n    """"""\n    Get the log URL for the task instance.\n\n    Args:\n        context: Airflow context object\n\n    Returns:\n        str: The log URL for the task instance\n    """"""\n    from urllib import parse\n    task_instance = context[""ti""]\n    parsed_url = parse.urlparse(task_instance.log_url)\n    base_url = f""{parsed_url.scheme}://{parsed_url.netloc}""\n    attrs = {\n        ""dag_id"": task_instance.dag_id,\n        ""task_id"": task_instance.task_id,\n        ""execution_date"": task_instance.execution_date,\n    }\n    formatted_attrs = parse.urlencode(attrs)\n    formatted_attrs += f""&map_index={task_instance.map_index}"" if task_instance.map_index >= 0 else """"\n    log_url = f""{base_url}/log?{formatted_attrs}""\n    return log_url\n```\n\nIe.\n`log_url = context.get(""task_instance"").log_url`\nwould become\n`log_url = get_log_url(context)`', 'created_at': datetime.datetime(2025, 1, 22, 15, 33, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2635656147, 'issue_id': 2768820876, 'author': 'tanweipeng', 'body': 'We can use regex to solve it actually\n\n```python\nlog_url = get_log_url(log_url=task_instance.log_url)\n\ndef get_log_url(log_url: str):\n    """"""\n    Remove the base_date parameter from the log URL.\n    Args:\n        log_url (str): The log URL.\n    Returns:\n        str: The log URL without the base_date parameter.\n    """"""\n    # Regex pattern to match \'base_date\' and its value\n    updated_url = re.sub(r\'(&?base_date=[^&]*)\', \'\', log_url)\n\n    # Ensure no trailing \'?\' or \'&\' if the last param was removed\n    updated_url = re.sub(r\'[?&]$\', \'\', updated_url)\n\n    return updated_url\n```', 'created_at': datetime.datetime(2025, 2, 5, 4, 6, 41, tzinfo=datetime.timezone.utc)}]","gopidesupavan on (2025-01-04 21:01:40 UTC): Cant reproduce this issue, please provide the dag your using.

Looking at your first screenshot your it seems your on XCom tab?. try refresh and check

fbmatilla (Issue Creator) on (2025-01-05 18:45:03 UTC): Hi @gopidesupavan, 

thank you for your reply.

It is really weird because the selected menu is ""Logs"" but in the body you see ""No XCom"".


In order to reproduce it you will need to:

1.- Set up the smtp_default connection to a right SMTP server:
![image](https://github.com/user-attachments/assets/e249e6c5-215d-45f3-b51a-7088e273e774)

2.- Put the following DAG in you DAG folder, and please change ""your@email.com"" by your email:
```

from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.operators.python import PythonOperator
from typing import Any


default_args = {
    'owner': 'Axesor',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email': ""your@email.com"",
    'email_on_failure': True
}



def task_1(ds: Any, **kwargs: Any) -> None:
    raise Exception(""Error"")


with DAG(
    'example',
    schedule_interval=None,
    default_args=default_args
) as dag:

    task1 = PythonOperator (
        task_id=""task_1"",
        python_callable=task_1
    )

    task1
```

3.- Run the DAG and you will receive an email like this:
![image](https://github.com/user-attachments/assets/4a709273-b353-4e49-8ffc-c42dacea2756)

4.- If you click on the Log link you will see the ""No XCom"" in the main screen and if you remove the base_date from the URL then you will see the logs now correctly.

Let me know please.

kolfild26 on (2025-01-19 18:13:51 UTC): Hi all,
Just would like to ""vote"" for this as I'm facing exactly the same behaviour.
- log_url leads to nothing
- XCom and Log are mixed up
- removing the base_date fixes the issue.

Still struggling in reproducing as when I am trying to cut the dag to provide an MVP example the issue disappears.
It's defenetely an intermittent error and it's hard to reproduce but it does exist so don't close this please.

gopidesupavan on (2025-01-19 18:17:33 UTC): Will look into this week. @fbmatilla thanks for the steps.

fbmatilla (Issue Creator) on (2025-01-19 19:36:19 UTC): Thank you @gopidesupavan!

mtsadler-branch on (2025-01-22 15:33:04 UTC): Here's a helper function for anyone running into this issue while calling `TaskInstance.log_url` directly:

```python
def get_log_url(context):
    """"""
    Get the log URL for the task instance.

    Args:
        context: Airflow context object

    Returns:
        str: The log URL for the task instance
    """"""
    from urllib import parse
    task_instance = context[""ti""]
    parsed_url = parse.urlparse(task_instance.log_url)
    base_url = f""{parsed_url.scheme}://{parsed_url.netloc}""
    attrs = {
        ""dag_id"": task_instance.dag_id,
        ""task_id"": task_instance.task_id,
        ""execution_date"": task_instance.execution_date,
    }
    formatted_attrs = parse.urlencode(attrs)
    formatted_attrs += f""&map_index={task_instance.map_index}"" if task_instance.map_index >= 0 else """"
    log_url = f""{base_url}/log?{formatted_attrs}""
    return log_url
```

Ie.
`log_url = context.get(""task_instance"").log_url`
would become
`log_url = get_log_url(context)`

tanweipeng on (2025-02-05 04:06:41 UTC): We can use regex to solve it actually

```python
log_url = get_log_url(log_url=task_instance.log_url)

def get_log_url(log_url: str):
    """"""
    Remove the base_date parameter from the log URL.
    Args:
        log_url (str): The log URL.
    Returns:
        str: The log URL without the base_date parameter.
    """"""
    # Regex pattern to match 'base_date' and its value
    updated_url = re.sub(r'(&?base_date=[^&]*)', '', log_url)

    # Ensure no trailing '?' or '&' if the last param was removed
    updated_url = re.sub(r'[?&]$', '', updated_url)

    return updated_url
```

"
2768297416,issue,open,,HA Scheduler does not respect max_active_runs in edge case ,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Two queued dagruns of a DAG with max_active_runs of 1 started within 0.2 seconds of each other.

The deployment has two schedulers, A and B. I suspect scheduler A started one dagrun and scheduler B started the other dagrun. Because a scheduler queries the active dagrun information every scheduling loop via [_start_queued_dagruns]( https://github.com/apache/airflow/blob/2.10.4/airflow/jobs/scheduler_job_runner.py#L1537), it is possible for the limit to be exceeded as the information is not shared between schedulers. Both schedulers thought there were no active dagruns and started their respective queued dagrun.

The question is how does one dagrun end up in one query and not the other. One explanation could be scheduler A goes out and locks only 1 dagrun row because the other dagrun row is out of the [max_dagruns_per_loop_to_schedule](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#max-dagruns-per-loop-to-schedule) range. The other scheduler then picked up the dagrun that did not get queried. Even though it is very unlikely, I suspect both scheduling loops ran the query very closely in time.

### What you think should happen instead?

_No response_

### How to reproduce

This scenario requires extreme luck (or lack thereof) so I have not been able to reproduce this behaviour.

Perhaps the key to reproduce this is with max_dagruns_per_loop_to_schedule set to 1.

### Operating System

n/a

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",wolfier,2025-01-03 23:44:10+00:00,[],2025-01-06 04:43:03+00:00,,https://github.com/apache/airflow/issues/45388,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('area:core', ''), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2571281856, 'issue_id': 2768297416, 'author': 'potiuk', 'body': 'Yes. That is quite possible edge case though rather infrequent. I am not sure if there is a way we could protect it - we would likely have to add lock on dag not only on DagRun, but it would likely heavily decrease some parallelism scenarios for scheduling. @ashb  wdyt ?', 'created_at': datetime.datetime(2025, 1, 4, 13, 1, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2571407550, 'issue_id': 2768297416, 'author': 'jedcunningham', 'body': ""As I was chatting with Alan about this yesterday, another thought I had was to do optimistic locking. e.g. set the state to running only if the count of running for that dag matches what we expect (and used to determine it's okay to start another!)."", 'created_at': datetime.datetime(2025, 1, 4, 20, 46, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2571410913, 'issue_id': 2768297416, 'author': 'potiuk', 'body': 'Can we do it ? How?\r\n\r\nI think this is the ""logical"" problem to solve.\r\n\r\nWhatever optimisitc query you can run is ""read"" and in order to make it really protected, you need to make it part of a transaction - you need to query and update the state in the same transaction essentially. \r\n\r\nOr you have to read a ""state"" of the system and use it in the ""write"" statement to detect that the state has changed.  You cannot really separate ""read"" and ""update"" action - they should be connected by either transaction or state retrieved from the ""read"" action has to be used as a ""potentially failing"" check in the update state.', 'created_at': datetime.datetime(2025, 1, 4, 21, 3, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2571411707, 'issue_id': 2768297416, 'author': 'potiuk', 'body': 'Generally:\r\n\r\na) pessimistic locking\r\n\r\n* lock update\r\n* run query and update state\r\n* save the state change and unlocki\r\n\r\n\r\nb) optimistic locking\r\n\r\n * read a state\r\n* run query and only update if the state has not changed in a single transaction and commit in the same operation (and fail otherwise)\r\n\r\nI am not sure what ""state"" we can read here and how we can verify the state has not changed since we read it.', 'created_at': datetime.datetime(2025, 1, 4, 21, 7, 32, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-04 13:01:36 UTC): Yes. That is quite possible edge case though rather infrequent. I am not sure if there is a way we could protect it - we would likely have to add lock on dag not only on DagRun, but it would likely heavily decrease some parallelism scenarios for scheduling. @ashb  wdyt ?

jedcunningham on (2025-01-04 20:46:32 UTC): As I was chatting with Alan about this yesterday, another thought I had was to do optimistic locking. e.g. set the state to running only if the count of running for that dag matches what we expect (and used to determine it's okay to start another!).

potiuk on (2025-01-04 21:03:29 UTC): Can we do it ? How?

I think this is the ""logical"" problem to solve.

Whatever optimisitc query you can run is ""read"" and in order to make it really protected, you need to make it part of a transaction - you need to query and update the state in the same transaction essentially. 

Or you have to read a ""state"" of the system and use it in the ""write"" statement to detect that the state has changed.  You cannot really separate ""read"" and ""update"" action - they should be connected by either transaction or state retrieved from the ""read"" action has to be used as a ""potentially failing"" check in the update state.

potiuk on (2025-01-04 21:07:32 UTC): Generally:

a) pessimistic locking

* lock update
* run query and update state
* save the state change and unlocki


b) optimistic locking

 * read a state
* run query and only update if the state has not changed in a single transaction and commit in the same operation (and fail otherwise)

I am not sure what ""state"" we can read here and how we can verify the state has not changed since we read it.

"
2767490794,issue,open,,No logs after task completion,"### Official Helm Chart version

1.15.0 (latest released)

### Apache Airflow version

2.10.4

### Kubernetes Version

1.28

### Helm Chart configuration

_No response_

### Docker Image customizations

_No response_

### What happened

We use CeleryKubernetes Executor. Not in all cases, but sometimes after the task is completed, the output with logs disappears. In order for the output with logs to appear in the UI, you have to refresh the page.
<img width=""1884"" alt=""Снимок экрана 2025-01-03 в 15 16 52"" src=""https://github.com/user-attachments/assets/286bdaf2-188d-44da-b8f8-895956967176"" />


### What you think should happen instead

_No response_

### How to reproduce

We launch the pipeline, select any task and wait for its completion. After the task is completed, the output disappears. We refresh the page, the output with logs is in place. Sometimes refreshing the page does not immediately display the logs, but you need to wait about a minute.

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",asemelianov,2025-01-03 12:23:31+00:00,[],2025-01-14 12:01:25+00:00,,https://github.com/apache/airflow/issues/45376,"[('kind:bug', 'This is a clearly a bug'), ('area:logging', ''), ('area:helm-chart', 'Airflow Helm Chart'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2569144800, 'issue_id': 2767490794, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 3, 12, 23, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569764317, 'issue_id': 2767490794, 'author': 'javier-salazar-zefr', 'body': 'We experienced a similar issue with version `2.10.3`. In our case what was happening was that it would fail to fetch remote logs. From looking at the requests it seemed it only tried to get attempt=1 which did not exist in the bucket then after refreshing it would get attempt=2.log and that would successfully show the logs in the UI, this behavior was consistent with every task on the DAG.\r\n\r\nAfter trying different things we ended up downgrading to 2.9.2 and the issue went away. In version 2.9.2 it seems that it gets all attempts on the first time loading the screen so it showed 1 and 2, if we looked at 1 we could see the same error we saw on that first load on 2.10.3 but clicking on the `2` tab showed the full logs from the bucket.\r\n\r\nUnsure if this would be the same issue but they sound similar.', 'created_at': datetime.datetime(2025, 1, 3, 20, 16, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2572739549, 'issue_id': 2767490794, 'author': 'potiuk', 'body': '> Unsure if this would be the same issue but they sound similar.\r\n\r\nThat sounds like related to num_try change @dstandish (https://github.com/apache/airflow/pull/39336)', 'created_at': datetime.datetime(2025, 1, 6, 9, 43, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2575090682, 'issue_id': 2767490794, 'author': 'asemelianov', 'body': '@javier-salazar-zefr @potiuk thanks!', 'created_at': datetime.datetime(2025, 1, 7, 11, 49, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2575757285, 'issue_id': 2767490794, 'author': 'dstandish', 'body': ""This issue (i.e. the one reported by @asemelianov ) does not seem try number related.  The reported issue is after task completion, the logs are temporarily absent.  This is not terribly surprising because while the task is running, airlfow will read logs from the worker.  Then, when task done, the logs are uploaded to blob storage (most commonly) and then the webserver will have to read from there instead.  There were other recent meddlings with this area of code, i.e. dealing with scenarios when the logs are not found on worker, and that also could change this behavior.\r\n\r\nI'm not quite clear on the scenario from @javier-salazar-zefr but it may warrant a separate issue with more detailed explanation."", 'created_at': datetime.datetime(2025, 1, 7, 16, 39, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2589727427, 'issue_id': 2767490794, 'author': 'Jorricks', 'body': ""I think this might be unrelated to the issue posted here, but decided to reply here as https://github.com/apache/airflow/pull/39336 was mentioned as a potential issue here. I think the try_number is actually correct except in a very specific case;\n\nWe just did the Airflow upgrade of 2.3.4 to 2.10.4. Usually, we are very strict on turning of all components during any sort of maintenance, this time we were a bit more Yolo (as it was our beta environment). The only preparation was setting some pools to 0, leading to quite some tasks being queued in the pools. \n\nWe found out that the tasks that were queued before the upgrade were uploading logs to `try_number=0`. This checks out with the change regarding the `try_number`; https://github.com/apache/airflow/pull/39336. This is because previously with Airflow 2.3.4 we'd increment the version in the workers, now with 2.10.4 we increment it in the scheduler. For the tasks that were queued with 2.3.4, but executed on celery workers of 2.10.4, they never got an incremented try_number.\n\nWe were actually very impressed with the upgrade as we left some 2.3.4 workers on (on k8s in terminating state) that were still finishing some tasks, and even during the DB upgrade & after the DB upgrade, the jobs they were executing finished fine and were marked in the DB as a success. \n\nTLDR; we only had issues with the ones that are scheduled with <2.10 and executed with celery workers >=2.10, as we could not fetch their logs."", 'created_at': datetime.datetime(2025, 1, 14, 11, 58, 47, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-03 12:23:33 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

javier-salazar-zefr on (2025-01-03 20:16:45 UTC): We experienced a similar issue with version `2.10.3`. In our case what was happening was that it would fail to fetch remote logs. From looking at the requests it seemed it only tried to get attempt=1 which did not exist in the bucket then after refreshing it would get attempt=2.log and that would successfully show the logs in the UI, this behavior was consistent with every task on the DAG.

After trying different things we ended up downgrading to 2.9.2 and the issue went away. In version 2.9.2 it seems that it gets all attempts on the first time loading the screen so it showed 1 and 2, if we looked at 1 we could see the same error we saw on that first load on 2.10.3 but clicking on the `2` tab showed the full logs from the bucket.

Unsure if this would be the same issue but they sound similar.

potiuk on (2025-01-06 09:43:33 UTC): That sounds like related to num_try change @dstandish (https://github.com/apache/airflow/pull/39336)

asemelianov (Issue Creator) on (2025-01-07 11:49:18 UTC): @javier-salazar-zefr @potiuk thanks!

dstandish on (2025-01-07 16:39:44 UTC): This issue (i.e. the one reported by @asemelianov ) does not seem try number related.  The reported issue is after task completion, the logs are temporarily absent.  This is not terribly surprising because while the task is running, airlfow will read logs from the worker.  Then, when task done, the logs are uploaded to blob storage (most commonly) and then the webserver will have to read from there instead.  There were other recent meddlings with this area of code, i.e. dealing with scenarios when the logs are not found on worker, and that also could change this behavior.

I'm not quite clear on the scenario from @javier-salazar-zefr but it may warrant a separate issue with more detailed explanation.

Jorricks on (2025-01-14 11:58:47 UTC): I think this might be unrelated to the issue posted here, but decided to reply here as https://github.com/apache/airflow/pull/39336 was mentioned as a potential issue here. I think the try_number is actually correct except in a very specific case;

We just did the Airflow upgrade of 2.3.4 to 2.10.4. Usually, we are very strict on turning of all components during any sort of maintenance, this time we were a bit more Yolo (as it was our beta environment). The only preparation was setting some pools to 0, leading to quite some tasks being queued in the pools. 

We found out that the tasks that were queued before the upgrade were uploading logs to `try_number=0`. This checks out with the change regarding the `try_number`; https://github.com/apache/airflow/pull/39336. This is because previously with Airflow 2.3.4 we'd increment the version in the workers, now with 2.10.4 we increment it in the scheduler. For the tasks that were queued with 2.3.4, but executed on celery workers of 2.10.4, they never got an incremented try_number.

We were actually very impressed with the upgrade as we left some 2.3.4 workers on (on k8s in terminating state) that were still finishing some tasks, and even during the DB upgrade & after the DB upgrade, the jobs they were executing finished fine and were marked in the DB as a success. 

TLDR; we only had issues with the ones that are scheduled with <2.10 and executed with celery workers >=2.10, as we could not fetch their logs.

"
2767051464,issue,closed,completed,Random missing DAG from DagBag,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.0

### What happened?

There is a DAG which we wrote. It randomly pops up in UI sometimes and goes missing. When we hit that URL it shows DAG missing from DAGBAG. and even if nobody changes anything, It just pops up from somewhere. On logs we have checked that even when the DAG is missing from DagBag it still runs and does it's tasks (we verified this via db and pod). Can we please get some light on why this randomness ?

### What you think should happen instead?

Dag should either be visible all the time or stay missing all the time.

### How to reproduce

No idea

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Harshil-Jani,2025-01-03 06:31:55+00:00,[],2025-01-03 09:18:21+00:00,2025-01-03 09:18:21+00:00,https://github.com/apache/airflow/issues/45372,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('area:DAG-processing', '')]","[{'comment_id': 2568750608, 'issue_id': 2767051464, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 3, 6, 31, 58, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-03 06:31:58 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2766945898,issue,closed,completed,There is a bug in connecting to EKS using the airflow.providers.amazon.aws.operators.eks library in China.,"### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

9.2.0

Even though I'm using the latest version, I think this bug exists in all historical versions.

### Apache Airflow version

2.10.1

### Operating System

Amazon Linux 2023

### Deployment

Amazon (AWS) MWAA

### Deployment details

You can reproduce this bug stably without any customization.

### What happened

Their dag script to connect to EKS cluster using [from airflow.providers.amazon.aws.operators.eks import EksPodOperator] returned 401 .

script：
```
from airflow import DAG
from airflow.operators.dummy import DummyOperator
from airflow.providers.amazon.aws.operators.eks import EksPodOperator
from kubernetes.client import models as k8s

from datetime import datetime

DEFAULT_ARGS = {
    'owner': 'XXX',
}


with DAG(
        'test2_eks_pod_operator_poc',
        default_args=DEFAULT_ARGS,
        schedule_interval=None,  # trigger manually for now
        start_date=datetime(2024, 4, 28),
        catchup=False,
        tags=['examples']
) as dag:

    start = DummyOperator(task_id='start', retries=2)
    end = DummyOperator(task_id='end', retries=2)

    test2_eks_pod_operator = EksPodOperator(
        task_id='test2_eks_pod_operator',
        region='cn-north-1',
        cluster_name='eks-cluster',
        namespace='mwaa',
        service_account_name='default',
        pod_name='eks_pod_operator_poc',
        image='amazon/aws-cli:latest',
        image_pull_policy='IfNotPresent',
        node_selector={
            'type': 'app'
        },
        tolerations=[
            k8s.V1Toleration(
                effect='NoSchedule',
                key='type',
                operator='Equal',
                value='app'
            )
        ],
        cmds=['/bin/bash', '-c'],
        arguments=['echo ""hello world""'],
        is_delete_operator_pod=True,
    )

    start >> test2_eks_pod_operator >> end
```

error log:
```
*** Reading remote log from Cloudwatch log_group: airflow-mwaa-Task log_stream: dag_id=test2_eks_pod_operator_poc/run_id=manual__2024-12-31T08_47_12.225701+00_00/task_id=test2_eks_pod_operator/attempt=1.log.
[2024-12-31, 08:47:15 UTC] {local_task_job_runner.py:123} ▶ Pre task execution logs
[2024-12-31, 08:47:16 UTC] {base.py:84} INFO - Retrieving connection 'aws_default'
[2024-12-31, 08:47:16 UTC] {baseoperator.py:405} WARNING - EksPodOperator.execute cannot be called outside TaskInstance!
[2024-12-31, 08:47:16 UTC] {pod.py:1133} INFO - Building pod eks-pod-operator-poc-ylyc6uh3 with labels: {'dag_id': 'test2_eks_pod_operator_poc', 'task_id': 'test2_eks_pod_operator', 'run_id': 'manual__2024-12-31T084712.2257010000-a8af56277', 'kubernetes_pod_operator': 'True', 'try_number': '1'}
[2024-12-31, 08:47:16 UTC] {base.py:84} INFO - Retrieving connection 'kubernetes_default'
[2024-12-31, 08:47:19 UTC] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/utils/operator_helpers.py"", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py"", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/amazon/aws/operators/eks.py"", line 1103, in execute
    return super().execute(context)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py"", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py"", line 593, in execute
    return self.execute_sync(context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py"", line 603, in execute_sync
    self.pod = self.get_or_create_pod(  # must set `self.pod` for `on_kill`
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py"", line 561, in get_or_create_pod
    pod = self.find_pod(self.namespace or pod_request_obj.metadata.namespace, context=context)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py"", line 534, in find_pod
    pod_list = self.client.list_namespaced_pod(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/kubernetes/client/api/core_v1_api.py"", line 15823, in list_namespaced_pod
    return self.list_namespaced_pod_with_http_info(namespace, **kwargs)  # noqa: E501
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/kubernetes/client/api/core_v1_api.py"", line 15942, in list_namespaced_pod_with_http_info
    return self.api_client.call_api(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/kubernetes/client/api_client.py"", line 348, in call_api
    return self.__call_api(resource_path, method,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/kubernetes/client/api_client.py"", line 180, in __call_api
    response_data = self.request(
                    ^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/kubernetes/client/api_client.py"", line 373, in request
    return self.rest_client.GET(url,
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/kubernetes/client/rest.py"", line 244, in GET
    return self.request(""GET"", url,
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/kubernetes/client/rest.py"", line 238, in request
    raise ApiException(http_resp=r)
kubernetes.client.exceptions.ApiException: (401)
Reason: Unauthorized
HTTP response headers: HTTPHeaderDict({'Audit-Id': '6bab071a-ed5b-41b9-9df7-d76d7247ebcd', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'Date': 'Tue, 31 Dec 2024 08:47:19 GMT', 'Content-Length': '129'})
HTTP response body: {""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""Unauthorized"",""reason"":""Unauthorized"",""code"":401}


[2024-12-31, 08:47:19 UTC] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=test2_eks_pod_operator_poc, task_id=test2_eks_pod_operator, run_id=manual__2024-12-31T08:47:12.225701+00:00, execution_date=20241231T084712, start_date=20241231T084715, end_date=20241231T084719
[2024-12-31, 08:47:19 UTC] {taskinstance.py:340} ▶ Post task execution logs
```

### What you think should happen instead

### Here's the investigation I've done:

#### Examining the source code for EksPodOperator shows that this class automatically generates a kube_config file if no external kube_config file is specified during initialization:
```
https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/_modules/airflow/providers/amazon/aws/operators/eks.html
        # There is no need to manage the kube_config file, as it will be generated automatically.
        # All Kubernetes parameters (except config_file) are also valid for the EksPodOperator.
```
#### There seems to be a problem with this auto-generated kube_config file, so I printed the contents of the file in debug and examined the source code associated with it generating the contents of the file:
```
[docs]    def execute(self, context: Context):
        eks_hook = EksHook(
            aws_conn_id=self.aws_conn_id,
            region_name=self.region,
        )
        with eks_hook.generate_config_file(
            eks_cluster_name=self.cluster_name, pod_namespace=self.namespace
        ) as self.config_file:
            return super().execute(context)
```
#### https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/_modules/airflow/providers/amazon/aws/hooks/eks.html
```
        cluster_config = {
            ""apiVersion"": ""v1"",
            ""kind"": ""Config"",
            ""clusters"": [
                {
                    ""cluster"": {""server"": cluster_ep, ""certificate-authority-data"": cluster_cert},
                    ""name"": eks_cluster_name,
                }
            ],
            ""contexts"": [
                {
                    ""context"": {
                        ""cluster"": eks_cluster_name,
                        ""namespace"": pod_namespace,
                        ""user"": _POD_USERNAME,
                    },
                    ""name"": _CONTEXT_NAME,
                }
            ],
            ""current-context"": _CONTEXT_NAME,
            ""preferences"": {},
            ""users"": [
                {
                    ""name"": _POD_USERNAME,
                    ""user"": {
                        ""exec"": {
                            ""apiVersion"": AUTHENTICATION_API_VERSION,
                            ""command"": ""sh"",
                            ""args"": [
                                ""-c"",
                                COMMAND.format(
                                    python_executable=python_executable,
                                    eks_cluster_name=eks_cluster_name,
                                    args=args,
                                ),
                            ],
                            ""interactiveMode"": ""Never"",
                        }
                    },
                }
            ],
        }
```
#### Here it is executing a bash command, searching the COMMAND variable you can see the exact command executed as follows, you can see it is getting the eks token.
```
COMMAND = """"""
            output=$({python_executable} -m airflow.providers.amazon.aws.utils.eks_get_token \
                --cluster-name {eks_cluster_name} {args} 2>&1)

            if [ $? -ne 0 ]; then
                echo ""Error running the script""
                exit 1
            fi

            expiration_timestamp=$(echo ""$output"" | grep -oP 'expirationTimestamp: \\K[^,]+')
            token=$(echo ""$output"" | grep -oP 'token: \\K[^,]+')

            json_string=$(printf '{{""kind"": ""ExecCredential"",""apiVersion"": \
                ""client.authentication.k8s.io/v1alpha1"",""spec"": {{}},""status"": \
                {{""expirationTimestamp"": ""%s"",""token"": ""%s""}}}}' ""$expiration_timestamp"" ""$token"")
            echo $json_string
            """"""
```
#### https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/_modules/airflow/providers/amazon/aws/utils/eks_get_token.html
```
[docs]def main():
    parser = get_parser()
    args = parser.parse_args()
    eks_hook = EksHook(aws_conn_id=args.aws_conn_id, region_name=args.region_name)
    access_token = eks_hook.fetch_access_token_for_cluster(args.cluster_name)
    access_token_expiration = get_expiration_time()
    print(f""expirationTimestamp: {access_token_expiration}, token: {access_token}"")
```
#### access_token from eks_hook.fetch_access_token_for_cluster(args.cluster_name)  Check out the implementation of the eks_hook.fetch_access_token_for_cluster method: https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/_modules/airflow/providers/amazon/aws/hooks/eks.html
```
def fetch_access_token_for_cluster(self, eks_cluster_name: str) -> str:
        session = self.get_session()
        service_id = self.conn.meta.service_model.service_id
        sts_url = (
            f""https://sts.{session.region_name}.amazonaws.com/?Action=GetCallerIdentity&Version=2011-06-15""
        )
```
### The address to access STS here points to the global address, not the China STS service address. So the eks token obtained cannot be used in China. The sts_url that should be used in China is f “https://sts.{session.region_name}.amazonaws.com.cn/?Action=GetCallerIdentity&Version=2011-06-15”

### How to reproduce

You can easily reproduce this using the dag script above, provided you use the identity credentials of your China AWS account.

### Anything else

no

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",QiaoLiar,2025-01-03 04:10:38+00:00,[],2025-01-16 22:46:39+00:00,2025-01-16 22:46:39+00:00,https://github.com/apache/airflow/issues/45368,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('good first issue', '')]","[{'comment_id': 2568670706, 'issue_id': 2766945898, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 3, 4, 10, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2574998409, 'issue_id': 2766945898, 'author': 'willdanckwerts', 'body': 'Hi there, is there any easy way to replicate this issue without holding an AWS China account?', 'created_at': datetime.datetime(2025, 1, 7, 11, 1, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2575004573, 'issue_id': 2766945898, 'author': 'QiaoLiar', 'body': '> Hi there, is there any easy way to replicate this issue without holding an AWS China account?\r\n\r\nNo, this bug is only triggered when using AWS credentials based in China.', 'created_at': datetime.datetime(2025, 1, 7, 11, 4, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2581196535, 'issue_id': 2766945898, 'author': 'vincbeck', 'body': 'Re-opening it because we are reverting the fix (because it introduced another bug, more harmful). Revert: #45526', 'created_at': datetime.datetime(2025, 1, 9, 20, 33, 48, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-03 04:10:41 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

willdanckwerts on (2025-01-07 11:01:17 UTC): Hi there, is there any easy way to replicate this issue without holding an AWS China account?

QiaoLiar (Issue Creator) on (2025-01-07 11:04:25 UTC): No, this bug is only triggered when using AWS credentials based in China.

vincbeck on (2025-01-09 20:33:48 UTC): Re-opening it because we are reverting the fix (because it introduced another bug, more harmful). Revert: #45526

"
2766534070,issue,open,,`try_number` is incremented for Sensor Tasks in Reschedule Mode when using mysql database,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.2

### What happened?

Since 2.10, when running a sensor task in `mode=reschedule`, the `try_number` is incremented with each poke. This has a lot of task execution behavior implications such as 

- early task failures as `try_number` is incrememnted with poke and will exceed max number of retries more quickly
- each poke is broken out into a new log file instead of aggregating many pokes into a single file

From my testing this only occurs with `mysql` database. I have been unable to repro with sqlite in multiple environments.

### What you think should happen instead?

Sensors with `mode=reschedule` should only increment `try_number` after `timeout` has been reached or there is an error on execution. 

### How to reproduce

Execute the following DAG in an airflow environment using `mysql` as the metadb. It will repeatedly run a sensor with `mode=reschedule`.  On the `Logs` tab for the sensor task, you can verify that each poke is being broken out into its own log file and that `try_number` is being incremented

```
from airflow.decorators import dag, task
from pendulum import datetime
import requests
from airflow.sensors.python import PythonSensor


def check_dog_availability_func(**context):
    return False


@dag(
    start_date=datetime(2022, 12, 1),
    schedule=""@once"",
    catchup=False,
    tags=[""sensor""],
)
def pythonsensor_example():
    # turn any Python function into a sensor
    check_dog_availability = PythonSensor(
        task_id=""check_dog_availability"",
        poke_interval=10,
        timeout=3600,
        mode=""reschedule"",
        python_callable=check_dog_availability_func,
    )

    # click the link in the logs for a cute picture :)
    @task
    def print_dog_picture_url(url):
        print(url)

    print_dog_picture_url(check_dog_availability.output)


pythonsensor_example()
```



### Operating System

Debian Slim-Buster

### Versions of Apache Airflow Providers

```
apache-airflow = {version = ""2.10.2""}
apache-airflow-providers-cncf-kubernetes = ""8.4.1""
apache-airflow-providers-celery = ""3.8.1""
apache-airflow-providers-redis = ""3.8.0""
apache-airflow-providers-mysql = ""5.7.0""
apache-airflow-providers-amazon = ""8.28.0""
apache-airflow-providers-apache-pinot = ""4.5.0""
apache-airflow-providers-apache-hive = ""8.1.1"" # conflicts with sqlpen pandas <2.0 requirement
apache-airflow-providers-arangodb = ""2.6.0""
apache-airflow-providers-google = ""10.19.0""
apache-airflow-providers-imap = ""3.7.0""
apache-airflow-providers-snowflake = ""5.5.1""
apache-airflow-providers-presto = ""5.5.1""
apache-airflow-providers-tableau = ""4.6.0""
mysqlclient = ""2.2.4""
```

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### Anything else?

My best guess at this time is that the following `sqlalchemy` logic for checking if the state of the task is `UP_FOR_RESCHEDULE` is not working as expected for `mysql` databases. We are hoping to find time to verify this is where the bug is occurring in the coming week.

https://github.com/apache/airflow/blob/2.10.2/airflow/models/dagrun.py#L1612-L1623

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",andscoop,2025-01-02 19:29:02+00:00,[],2025-01-02 19:31:15+00:00,,https://github.com/apache/airflow/issues/45366,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:mysql', '')]",[],
2766517535,issue,open,,RabbitMQ connection unexpectedly closing,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Using Airflow with RabbitMQ produces these RabbitMQ logs that seem to mean it's not properly closing connections interally.
```
rabbitmq-1   | 2025-01-02 19:12:24.816895+00:00 [info] <0.1548.0> accepting AMQP connection <0.1548.0> (172.19.0.8:34678 -> 172.19.0.2:5672)
rabbitmq-1   | 2025-01-02 19:12:24.818316+00:00 [info] <0.1548.0> connection <0.1548.0> (172.19.0.8:34678 -> 172.19.0.2:5672): user 'airflow' authenticated and granted access to vhost 'airflow'
rabbitmq-1   | 2025-01-02 19:12:24.822822+00:00 [info] <0.1565.0> accepting AMQP connection <0.1565.0> (172.19.0.8:34694 -> 172.19.0.2:5672)
rabbitmq-1   | 2025-01-02 19:12:24.823657+00:00 [info] <0.1565.0> connection <0.1565.0> (172.19.0.8:34694 -> 172.19.0.2:5672): user 'airflow' authenticated and granted access to vhost 'airflow'
rabbitmq-1   | 2025-01-02 19:12:24.898763+00:00 [warning] <0.1565.0> closing AMQP connection <0.1565.0> (172.19.0.8:34694 -> 172.19.0.2:5672, vhost: 'airflow', user: 'airflow'):
rabbitmq-1   | 2025-01-02 19:12:24.898763+00:00 [warning] <0.1565.0> client unexpectedly closed TCP connection
rabbitmq-1   | 2025-01-02 19:12:24.898856+00:00 [warning] <0.1548.0> closing AMQP connection <0.1548.0> (172.19.0.8:34678 -> 172.19.0.2:5672, vhost: 'airflow', user: 'airflow'):
rabbitmq-1   | 2025-01-02 19:12:24.898856+00:00 [warning] <0.1548.0> client unexpectedly closed TCP connection
```

### What you think should happen instead?

No warning messages about unexpected connection closures.

### How to reproduce

Start Airflow with RabbitMQ as the broker.

### Operating System

OSX, docker

### Versions of Apache Airflow Providers

apache-airflow-providers-apache-hdfs==4.7.0
apache-airflow-providers-apache-kafka==1.7.0
apache-airflow-providers-apache-spark==5.0.0
apache-airflow-providers-atlassian-jira==3.0.0
apache-airflow-providers-celery==3.8.5
apache-airflow-providers-common-compat==1.3.0
apache-airflow-providers-common-io==1.5.0
apache-airflow-providers-common-sql==1.21.0
apache-airflow-providers-docker==4.0.0
apache-airflow-providers-elasticsearch==6.0.0
apache-airflow-providers-fab==1.5.1
apache-airflow-providers-ftp==3.12.0
apache-airflow-providers-http==5.0.0
apache-airflow-providers-imap==3.8.0
apache-airflow-providers-postgres==6.0.0
apache-airflow-providers-redis==4.0.0
apache-airflow-providers-smtp==1.9.0
apache-airflow-providers-sqlite==4.0.0

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",kurtrwall,2025-01-02 19:15:46+00:00,['Nikunj-Aggarwal'],2025-01-03 22:06:34+00:00,,https://github.com/apache/airflow/issues/45365,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:core', '')]","[{'comment_id': 2569684075, 'issue_id': 2766517535, 'author': 'Nikunj-Aggarwal', 'body': 'Could you please assign this bug to me?', 'created_at': datetime.datetime(2025, 1, 3, 19, 1, 15, tzinfo=datetime.timezone.utc)}]","Nikunj-Aggarwal (Assginee) on (2025-01-03 19:01:15 UTC): Could you please assign this bug to me?

"
2766478290,issue,closed,completed,Airflow Celery CLI broken,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

2.10.3, 2.10.2

### What happened?

I'm not entirely sure of the situation because it's a little weird. When I try to install `apache-airflow`, it will always gives the version `2.10.3` for versions `2.10.2-4`. Additionally, it seems like there was a bug that's fixed on the `main` branch that breaks the `airflow celery` CLI like so:
```
worker-1  |
worker-1  | Traceback (most recent call last):
worker-1  |   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/celery/cli/celery_command.py"", line 45, in _run_command_with_daemon_option
worker-1  |     from airflow.cli.commands.local_commands.daemon_utils import run_command_with_daemon_option
worker-1  | ModuleNotFoundError: No module named 'airflow.cli.commands.local_commands'
worker-1  |
worker-1  | During handling of the above exception, another exception occurred:
worker-1  |
worker-1  | Traceback (most recent call last):
worker-1  |   File ""/home/airflow/.local/bin/airflow"", line 8, in <module>
worker-1  |     sys.exit(main())
worker-1  |              ^^^^^^
worker-1  |   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/__main__.py"", line 62, in main
worker-1  |     args.func(args)
worker-1  |   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command
worker-1  |     return func(*args, **kwargs)
worker-1  |            ^^^^^^^^^^^^^^^^^^^^^
worker-1  |   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper
worker-1  |     return f(*args, **kwargs)
worker-1  |            ^^^^^^^^^^^^^^^^^^
worker-1  |   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/celery/cli/celery_command.py"", line 61, in wrapper
worker-1  |     providers_configuration_loaded(func)(*args, **kwargs)
worker-1  |   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/providers_configuration_loader.py"", line 55, in wrapped_function
worker-1  |     return func(*args, **kwargs)
worker-1  |            ^^^^^^^^^^^^^^^^^^^^^
worker-1  |   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/celery/cli/celery_command.py"", line 234, in worker
worker-1  |     _run_command_with_daemon_option(
worker-1  |   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/celery/cli/celery_command.py"", line 51, in _run_command_with_daemon_option
worker-1  |     raise AirflowOptionalProviderFeatureException(
worker-1  | airflow.exceptions.AirflowOptionalProviderFeatureException: Failed to import run_command_with_daemon_option. This feature is only available in Airflow versions >= 2.8.0
```

The offending code seems to be repaired with [this PR](https://github.com/apache/airflow/pull/45255), but, and here's another weird part, if you browse the codebase at the aforementioned versions (tags), the offending code doesn't exist. To me, it seems like somehow ""bad"" versions of the code were packaged and uploaded to PyPI.

### What you think should happen instead?

- installing the `apache-airflow` at versions `2.10.2` or `2.10.4` should install that version, not `2.10.3`
- the code at the version should be the same as what's on GitHub at that version's tag
- running `airflow celery worker` CLI command should work

### How to reproduce

1. `pip install apache-airflow==2.10.4`
2. `airflow celery worker`

### Operating System

OSX

### Versions of Apache Airflow Providers

apache-airflow-providers-celery==3.9

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",kurtrwall,2025-01-02 18:43:05+00:00,[],2025-01-03 07:17:44+00:00,2025-01-03 07:17:43+00:00,https://github.com/apache/airflow/issues/45364,"[('kind:bug', 'This is a clearly a bug'), ('area:CLI', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:celery', '')]","[{'comment_id': 2568209809, 'issue_id': 2766478290, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 2, 18, 43, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568247121, 'issue_id': 2766478290, 'author': 'kurtrwall', 'body': ""Sorry if the description is confusing and ill-informed. I'm still not exactly sure what's happening or how the providers are built, but at least the celery CLI is fixed with `apache-airflow-providers-celery<3.9`."", 'created_at': datetime.datetime(2025, 1, 2, 19, 12, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568535614, 'issue_id': 2766478290, 'author': 'ncrocfer', 'body': 'We are also experiencing this issue with the CeleryExecutor, which prevents any tasks from being executed. To resolve the problem, we rolled back to `apache-airflow-providers-celery==3.8.5`: https://github.com/opencve/opencve/pull/493\r\n\r\n```\r\nPython: 3.11.11\r\nAirflow: 2.10.4\r\nCelery Provider: 3.9.0\r\n```', 'created_at': datetime.datetime(2025, 1, 2, 23, 58, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568784717, 'issue_id': 2766478290, 'author': 'potiuk', 'body': 'Yes. This PR has been merged 4 days ago but fixed celery provider has not yet been relased - see the changelog:\r\n https://airflow.apache.org/docs/apache-airflow-providers-celery/stable/changelog.html \r\n\r\nLook at the devlist announcements about the next release when we announce it - there will be - as usual ""status of testing"" issue created in here and you will be asked to test if the new RC candidate solves the issue.\r\n\r\nLooking forward for your testing and confirming it @kurtrwall @ncrocfer. Marking it as closed and linking to the PR.', 'created_at': datetime.datetime(2025, 1, 3, 7, 17, 43, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-02 18:43:08 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

kurtrwall (Issue Creator) on (2025-01-02 19:12:05 UTC): Sorry if the description is confusing and ill-informed. I'm still not exactly sure what's happening or how the providers are built, but at least the celery CLI is fixed with `apache-airflow-providers-celery<3.9`.

ncrocfer on (2025-01-02 23:58:44 UTC): We are also experiencing this issue with the CeleryExecutor, which prevents any tasks from being executed. To resolve the problem, we rolled back to `apache-airflow-providers-celery==3.8.5`: https://github.com/opencve/opencve/pull/493

```
Python: 3.11.11
Airflow: 2.10.4
Celery Provider: 3.9.0
```

potiuk on (2025-01-03 07:17:43 UTC): Yes. This PR has been merged 4 days ago but fixed celery provider has not yet been relased - see the changelog:
 https://airflow.apache.org/docs/apache-airflow-providers-celery/stable/changelog.html 

Look at the devlist announcements about the next release when we announce it - there will be - as usual ""status of testing"" issue created in here and you will be asked to test if the new RC candidate solves the issue.

Looking forward for your testing and confirming it @kurtrwall @ncrocfer. Marking it as closed and linking to the PR.

"
2766432099,issue,open,,Reevaluate if we should continue reserializing DAGs during db migration,"### Body

Once the dust settles a bit, we should reevaluate if we should start reserializing DAGs during migration. In theory, we shouldn't need to any longer. But let's check how well that pans out in practice once we can go through the scenarios.

This was dropped in #45362 as part of adding parsing support for bundles.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2025-01-02 18:05:54+00:00,[],2025-01-02 18:08:08+00:00,,https://github.com/apache/airflow/issues/45361,"[('kind:meta', 'High-level information important to the community'), ('area:db-migrations', 'PRs with DB migration')]",[],
2766292735,issue,open,reopened,The new release of Pytest Asyncio (0.25.1 and 0.25.2) makes a number of asyncio tests fail,"After Pytest-asyncio 0.25.1 our canary tests started to fail with ""RuntimeError: There is no current event loop in thread 'MainThread'."" This happens randomly - a lot of times in Python 3.12 non-db tests, but sometimes they succeed there, and fail in other Python versions. It happens with xdist with xdist n=1 and without xdist alike. 

Example failures:

* https://github.com/apache/airflow/actions/runs/12583221724/job/35070671929#step:6:904
* https://github.com/apache/airflow/actions/runs/12579028168/job/35066133378#step:6:904

Stacktrace: 

```python
_ ERROR at setup of TestRunPipelineJobTrigger.test_run_yields_success_event_on_successful_pipeline_state[4] _
[gw2] linux -- Python 3.11.11 /usr/local/bin/python
src/python/grpcio/grpc/_cython/_cygrpc/aio/common.pyx.pxi:184: in grpc._cython.cygrpc.get_working_loop
    ???
E   RuntimeError: no running event loop

During handling of the above exception, another exception occurred:
providers/tests/google/cloud/triggers/test_vertex_ai.py:133: in pipeline_service_async_client
    return PipelineServiceAsyncClient(
/usr/local/lib/python3.11/site-packages/google/cloud/aiplatform_v1/services/pipeline_service/async_client.py:304: in __init__
    self._client = PipelineServiceClient(
/usr/local/lib/python3.11/site-packages/google/cloud/aiplatform_v1/services/pipeline_service/client.py:[889](https://github.com/apache/airflow/actions/runs/12583221724/job/35070671929#step:6:906): in __init__
    self._transport = transport_init(
/usr/local/lib/python3.11/site-packages/google/cloud/aiplatform_v1/services/pipeline_service/transports/grpc_asyncio.py:310: in __init__
    self._grpc_channel = channel_init(
/usr/local/lib/python3.11/site-packages/google/cloud/aiplatform_v1/services/pipeline_service/transports/grpc_asyncio.py:177: in create_channel
    return grpc_helpers_async.create_channel(
/usr/local/lib/python3.11/site-packages/google/api_core/grpc_helpers_async.py:290: in create_channel
    return aio.secure_channel(
/usr/local/lib/python3.11/site-packages/grpc/aio/_channel.py:621: in secure_channel
    return Channel(
/usr/local/lib/python3.11/site-packages/grpc/aio/_channel.py:367: in __init__
    self._loop = cygrpc.get_working_loop()
src/python/grpcio/grpc/_cython/_cygrpc/aio/common.pyx.pxi:186: in grpc._cython.cygrpc.get_working_loop
    ???
src/python/grpcio/grpc/_cython/_cygrpc/aio/common.pyx.pxi:190: in grpc._cython.cygrpc.get_working_loop
    ???
/usr/local/lib/python3.11/asyncio/events.py:681: in get_event_loop
    raise RuntimeError('There is no current event loop in thread %r.'
E   RuntimeError: There is no current event loop in thread 'MainThread'.
_ ERROR at setup of TestRunPipelineJobTrigger.test_run_yields_success_event_on_successful_pipeline_state[8] _
[gw2] linux -- Python 3.11.11 /usr/local/bin/python
src/python/grpcio/grpc/_cython/_cygrpc/aio/common.pyx.pxi:184: in grpc._cython.cygrpc.get_working_loop
    ???
E   RuntimeError: no running event loop
```

Failing tests:

```
ERROR providers/tests/google/cloud/triggers/test_vertex_ai.py::TestRunPipelineJobTrigger::test_run_yields_success_event_on_successful_pipeline_state[4] - RuntimeError: There is no current event loop in thread 'MainThread'.
ERROR providers/tests/google/cloud/triggers/test_vertex_ai.py::TestRunPipelineJobTrigger::test_run_yields_success_event_on_successful_pipeline_state[8] - RuntimeError: There is no current event loop in thread 'MainThread'.
ERROR providers/tests/google/cloud/triggers/test_vertex_ai.py::TestRunPipelineJobTrigger::test_run_yields_error_event_on_failed_pipeline_state[5] - RuntimeError: There is no current event loop in thread 'MainThread'.
ERROR providers/tests/google/cloud/triggers/test_vertex_ai.py::TestRunPipelineJobTrigger::test_run_yields_error_event_on_failed_pipeline_state[7] - RuntimeError: There is no current event loop in thread 'MainThread'.
ERROR providers/tests/google/cloud/triggers/test_vertex_ai.py::TestCustomTrainingJobTrigger::test_run_yields_success_event_on_successful_pipeline_state[4] - RuntimeError: There is no current event loop in thread 'MainThread'.
ERROR providers/tests/google/cloud/triggers/test_vertex_ai.py::TestCustomTrainingJobTrigger::test_run_yields_success_event_on_successful_pipeline_state[8] - RuntimeError: There is no current event loop in thread 'MainThread'.
ERROR providers/tests/google/cloud/triggers/test_vertex_ai.py::TestCustomTrainingJobTrigger::test_run_yields_error_event_on_failed_pipeline_state[5] - RuntimeError: There is no current event loop in thread 'MainThread'.
ERROR providers/tests/google/cloud/triggers/test_vertex_ai.py::TestCustomTrainingJobTrigger::test_run_yields_error_event_on_failed_pipeline_state[7] - RuntimeError: There is no current event loop in thread 'MainThread'.
ERROR providers/tests/google/cloud/triggers/test_vertex_ai.py::TestCustomContainerTrainingJobTrigger::test_run_yields_success_event_on_successful_pipeline_state[4] - RuntimeError: There is no current event loop in thread 'MainThread'.
ERROR providers/tests/google/cloud/triggers/test_vertex_ai.py::TestCustomContainerTrainingJobTrigger::test_run_yields_success_event_on_successful_pipeline_state[8] - RuntimeError: There is no current event loop in thread 'MainThread'.
ERROR providers/tests/google/cloud/triggers/test_vertex_ai.py::TestCustomContainerTrainingJobTrigger::test_run_yields_error_event_on_failed_pipeline_state[5] - RuntimeError: There is no current event loop in thread 'MainThread'.
ERROR providers/tests/google/cloud/triggers/test_vertex_ai.py::TestCustomContainerTrainingJobTrigger::test_run_yields_error_event_on_failed_pipeline_state[7] - RuntimeError: There is no current event loop in thread 'MainThread'.
ERROR providers/tests/google/cloud/triggers/test_vertex_ai.py::TestCustomPythonPackageTrainingJobTrigger::test_run_yields_success_event_on_successful_pipeline_state[4] - RuntimeError: There is no current event loop in thread 'MainThread'.
ERROR providers/tests/google/cloud/triggers/test_vertex_ai.py::TestCustomPythonPackageTrainingJobTrigger::test_run_yields_success_event_on_successful_pipeline_state[8] - RuntimeError: There is no current event loop in thread 'MainThread'.
ERROR providers/tests/google/cloud/triggers/test_vertex_ai.py::TestCustomPythonPackageTrainingJobTrigger::test_run_yields_error_event_on_failed_pipeline_state[5] - RuntimeError: There is no current event loop in thread 'MainThread'.
ERROR providers/tests/google/cloud/triggers/test_vertex_ai.py::TestCustomPythonPackageTrainingJobTrigger::test_run_yields_error_event_on_failed_pipeline_state[7] - RuntimeError: There is no current event loop in thread 'MainThread'.
```


Also https://github.com/apache/airflow/actions/runs/12666006582/job/35297011421#step:6:4025

```
==================================== ERRORS ====================================
__ ERROR at teardown of TestAirbyteSyncTrigger.test_airbyte_run_sync_trigger ___
/usr/local/lib/python3.11/site-packages/pytest_asyncio/plugin.py:1151: in event_loop
    with _temporary_event_loop_policy(new_loop_policy), _provide_event_loop() as loop:
/usr/local/lib/python3.11/contextlib.py:144: in __exit__
    next(self.gen)
/usr/local/lib/python3.11/site-packages/pytest_asyncio/plugin.py:1168: in _provide_event_loop
    loop.run_until_complete(loop.shutdown_asyncgens())
/usr/local/lib/python3.11/asyncio/base_events.py:652: in run_until_complete
    raise RuntimeError('Event loop stopped before Future completed.')
E   RuntimeError: Event loop stopped before Future completed.
----------------------------- Captured stdout call -----------------------------
[2025-01-08T07:44:52.728+0000] {base.py:66} INFO - Retrieving connection 'airbyte_default'
------------------------------ Captured log call -------------------------------
INFO     airflow.hooks.base:base.py:66 Retrieving connection 'airbyte_default'
____________ ERROR at teardown of TestTrigger.test_trigger_run_bad _____________
/usr/local/lib/python3.11/site-packages/pytest_asyncio/plugin.py:1151: in event_loop
    with _temporary_event_loop_policy(new_loop_policy), _provide_event_loop() as loop:
/usr/local/lib/python3.11/contextlib.py:144: in __exit__
    next(self.gen)
/usr/local/lib/python3.11/site-packages/pytest_asyncio/plugin.py:1168: in _provide_event_loop
    loop.run_until_complete(loop.shutdown_asyncgens())
/usr/local/lib/python3.11/asyncio/base_events.py:652: in run_until_complete
    raise RuntimeError('Event loop stopped before Future completed.')
E   RuntimeError: Event loop stopped before Future completed.
______ ERROR at teardown of TestWasbBlobSensorTrigger.test_running[True] _______
/usr/local/lib/python3.11/site-packages/pytest_asyncio/plugin.py:1151: in event_loop
    with _temporary_event_loop_policy(new_loop_policy), _provide_event_loop() as loop:
/usr/local/lib/python3.11/contextlib.py:144: in __exit__
    next(self.gen)
/usr/local/lib/python3.11/site-packages/pytest_asyncio/plugin.py:1168: in _provide_event_loop
    loop.run_until_complete(loop.shutdown_asyncgens())
/usr/local/lib/python3.11/asyncio/base_events.py:652: in run_until_complete
    raise RuntimeError('Event loop stopped before Future completed.')
E   RuntimeError: Event loop stopped before Future completed.
______ ERROR at teardown of TestWasbBlobSensorTrigger.test_running[False] ______
```",potiuk,2025-01-02 16:15:57+00:00,[],2025-01-08 09:29:59+00:00,,https://github.com/apache/airflow/issues/45355,"[('provider:google', 'Google (including GCP) related issues'), ('area:CI', ""Airflow's tests and continious integration""), ('Quarantine', 'Issues that are occasionally failing and are quarantined')]",[],
2765681589,issue,open,,CSS bug on Dag Run Details view,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.2

### What happened?

I've a DAG with some parameters.
In the DAG run Notes, the params are listed as JSON in the `Run config` line at the end of the table, but the footer is hover the params list, as you can see.
![Capture d’écran du 2025-01-02 10-07-34](https://github.com/user-attachments/assets/8d272212-f54b-4019-a78e-f137f96d5919)


### What you think should happen instead?

_No response_

### How to reproduce

Create a DAG with some params.

### Operating System

Debian

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rmaziere,2025-01-02 09:14:57+00:00,['rawwar'],2025-01-16 17:54:51+00:00,,https://github.com/apache/airflow/issues/45342,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2567468774, 'issue_id': 2765681589, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2025, 1, 2, 9, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585119165, 'issue_id': 2765681589, 'author': 'rawwar', 'body': 'I checked this on 2.x dev branch and can\'t replicate it. I created 100 params and I can scroll and see all of them. \r\n\r\n<img width=""1534"" alt=""image"" src=""https://github.com/user-attachments/assets/eab81cf6-1081-4783-bb06-04174cc29efc"" />', 'created_at': datetime.datetime(2025, 1, 11, 6, 35, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585121226, 'issue_id': 2765681589, 'author': 'rawwar', 'body': ""Checked this on 2.10.2 as well and can't replicate it. Can you share the exact DAG which you used? @rmaziere"", 'created_at': datetime.datetime(2025, 1, 11, 6, 43, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2592920173, 'issue_id': 2765681589, 'author': 'rmaziere', 'body': 'The problem occurs on DAGs of all sizes with few parameters such as a large number.\nTo reproduce: \nfrom the general interface /home,\nselect the DAG by clicking on its name,\ndisplay of the details tab and the parameters are truncated.\n\nBut, if I refresh the page or if I switch to another ""tab"" (graph, gantt, code...), the display is complete.\n\nThere is a screen record, hope it helps.\n\nhttps://github.com/user-attachments/assets/2a3c25f2-372e-4d4e-944a-69545db886bb', 'created_at': datetime.datetime(2025, 1, 15, 13, 54, 51, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2025-01-02 09:15:00 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

rawwar (Assginee) on (2025-01-11 06:35:33 UTC): I checked this on 2.x dev branch and can't replicate it. I created 100 params and I can scroll and see all of them. 

<img width=""1534"" alt=""image"" src=""https://github.com/user-attachments/assets/eab81cf6-1081-4783-bb06-04174cc29efc"" />

rawwar (Assginee) on (2025-01-11 06:43:41 UTC): Checked this on 2.10.2 as well and can't replicate it. Can you share the exact DAG which you used? @rmaziere

rmaziere (Issue Creator) on (2025-01-15 13:54:51 UTC): The problem occurs on DAGs of all sizes with few parameters such as a large number.
To reproduce: 
from the general interface /home,
select the DAG by clicking on its name,
display of the details tab and the parameters are truncated.

But, if I refresh the page or if I switch to another ""tab"" (graph, gantt, code...), the display is complete.

There is a screen record, hope it helps.

https://github.com/user-attachments/assets/2a3c25f2-372e-4d4e-944a-69545db886bb

"
2765604218,issue,closed,completed,"AIP-72: Gracefully handle ""not-found"" XCOMs in task sdk API client","### Body

Example DAG:
```
from airflow import DAG
from airflow.providers.standard.operators.python import PythonOperator

def push_to_xcom(**kwargs):
    value = ""Hello, XCom!""
    return value


def pull_from_xcom(**kwargs):
    ti = kwargs['ti']
    xcom_value = ti.xcom_pull(task_ids='invalid_id')
    print(f""Retrieved XCom Value: {xcom_value}"")


with DAG(
    'xcom_example',
    schedule=None,
    catchup=False,
) as dag:

    push_xcom_task = PythonOperator(
        task_id='push_xcom_task',
        python_callable=push_to_xcom,
    )

    pull_xcom_task = PythonOperator(
        task_id='pull_xcom_task',
        python_callable=pull_from_xcom,
    )

    push_xcom_task >> pull_xcom_task
```

Here the `invalid_id` task id doesn't exist. So the XCOM pull should fail gracefully.
Instead, the executor just crashed:
```
[2024-12-31T06:53:15.741+0000] {_client.py:1026} INFO - HTTP Request: GET http://localhost:9091/execution/xcoms/xcom_example/manual__2024-12-31T06:53:14.523233+00:00/invalid_id/return_value?map_index=-1 ""HTTP/1.1 404 Not Found""
2024-12-31 06:53:15 [warning  ] Server error                   [airflow.sdk.api.client] detail={'detail': {'reason': 'not_found', 'message': ""XCom with key 'return_value' not found for task 'invalid_id' in DAG 'xcom_example'""}}
[2024-12-31T06:53:15.742+0000] {local_executor.py:96} ERROR - uhoh
Traceback (most recent call last):
  File ""/opt/airflow/airflow/executors/local_executor.py"", line 92, in _run_worker
    _execute_work(log, workload)
  File ""/opt/airflow/airflow/executors/local_executor.py"", line 113, in _execute_work
    supervise(
  File ""/opt/airflow/task_sdk/src/airflow/sdk/execution_time/supervisor.py"", line 898, in supervise
    exit_code = process.wait()
  File ""/opt/airflow/task_sdk/src/airflow/sdk/execution_time/supervisor.py"", line 512, in wait
    self._monitor_subprocess()
  File ""/opt/airflow/task_sdk/src/airflow/sdk/execution_time/supervisor.py"", line 554, in _monitor_subprocess
    alive = self._service_subprocess(max_wait_time=max_wait_time) is None
  File ""/opt/airflow/task_sdk/src/airflow/sdk/execution_time/supervisor.py"", line 598, in _service_subprocess
    need_more = socket_handler(key.fileobj)
  File ""/opt/airflow/task_sdk/src/airflow/sdk/execution_time/supervisor.py"", line 785, in cb
    gen.send(line)
  File ""/opt/airflow/task_sdk/src/airflow/sdk/execution_time/supervisor.py"", line 708, in handle_requests
    self._handle_request(msg, log)
  File ""/opt/airflow/task_sdk/src/airflow/sdk/execution_time/supervisor.py"", line 728, in _handle_request
    xcom = self.client.xcoms.get(msg.dag_id, msg.run_id, msg.task_id, msg.key, msg.map_index)
  File ""/opt/airflow/task_sdk/src/airflow/sdk/api/client.py"", line 222, in get
    resp = self.client.get(f""xcoms/{dag_id}/{run_id}/{task_id}/{key}"", params=params)
  File ""/usr/local/lib/python3.9/site-packages/httpx/_client.py"", line 1054, in get
    return self.request(
  File ""/usr/local/lib/python3.9/site-packages/tenacity/__init__.py"", line 336, in wrapped_f
    return copy(f, *args, **kw)
  File ""/usr/local/lib/python3.9/site-packages/tenacity/__init__.py"", line 475, in __call__
    do = self.iter(retry_state=retry_state)
  File ""/usr/local/lib/python3.9/site-packages/tenacity/__init__.py"", line 376, in iter
    result = action(retry_state)
  File ""/usr/local/lib/python3.9/site-packages/tenacity/__init__.py"", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
  File ""/usr/local/lib/python3.9/concurrent/futures/_base.py"", line 439, in result
    return self.__get_result()
  File ""/usr/local/lib/python3.9/concurrent/futures/_base.py"", line 391, in __get_result
    raise self._exception
  File ""/usr/local/lib/python3.9/site-packages/tenacity/__init__.py"", line 478, in __call__
    result = fn(*args, **kwargs)
  File ""/opt/airflow/task_sdk/src/airflow/sdk/api/client.py"", line 317, in request
    return super().request(*args, **kwargs)
  File ""/usr/local/lib/python3.9/site-packages/httpx/_client.py"", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
  File ""/usr/local/lib/python3.9/site-packages/httpx/_client.py"", line 914, in send
    response = self._send_handling_auth(
  File ""/usr/local/lib/python3.9/site-packages/httpx/_client.py"", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File ""/usr/local/lib/python3.9/site-packages/httpx/_client.py"", line 999, in _send_handling_redirects
    raise exc
  File ""/usr/local/lib/python3.9/site-packages/httpx/_client.py"", line 982, in _send_handling_redirects
    hook(response)
  File ""/opt/airflow/task_sdk/src/airflow/sdk/api/client.py"", line 93, in raise_on_4xx_5xx
    return get_json_error(response) or response.raise_for_status()
  File ""/opt/airflow/task_sdk/src/airflow/sdk/api/client.py"", line 89, in get_json_error
    raise err
airflow.sdk.api.client.ServerResponseError: Server returned error
```

Legacy Airflow just ignores such cases and moves on with a return.
![image (16)](https://github.com/user-attachments/assets/879de435-a27b-4bd3-82e3-2f315bc15166)


### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",amoghrajesh,2025-01-02 08:07:02+00:00,['amoghrajesh'],2025-01-03 09:11:45+00:00,2025-01-03 09:11:45+00:00,https://github.com/apache/airflow/issues/45341,"[('kind:bug', 'This is a clearly a bug'), ('area:API', ""Airflow's REST/HTTP API""), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2765427585,issue,closed,completed,Make `dags reserialize` bundle aware,"### Body

Today, the `dags reserialize` command can just create a new dagbag against the dags folder and sync it. Once we land support for bundles, however, it's not longer that simple.

This command was temporarily disabled in order to makes reviews easier in #45337, so once #42289 is done, we can rebuild this command.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2025-01-02 04:24:51+00:00,[],2025-01-14 12:28:19+00:00,2025-01-14 12:28:19+00:00,https://github.com/apache/airflow/issues/45336,"[('area:serialization', ''), ('kind:meta', 'High-level information important to the community')]",[],
2765149768,issue,open,,Test TestLocalExecutor.test_execution(unlimited) sometimes fail,"The test sometimes fails:

```python
=================================== FAILURES ===================================
_________________ TestLocalExecutor.test_execution[unlimited] __________________
tests/executors/test_local_executor.py:114: in test_execution
    self._test_execute(parallelism=parallelism)
/usr/local/lib/python3.11/unittest/mock.py:1378: in patched
    return func(*newargs, **newkeywargs)
tests/executors/test_local_executor.py:91: in _test_execute
    executor.end()
airflow/executors/local_executor.py:230: in end
    proc.join()
/usr/local/lib/python3.11/multiprocessing/process.py:149: in join
    res = self._popen.wait(timeout)
/usr/local/lib/python3.11/multiprocessing/popen_fork.py:43: in wait
    return self.poll(os.WNOHANG if timeout == 0.0 else 0)
/usr/local/lib/python3.11/multiprocessing/popen_fork.py:27: in poll
    pid, sts = os.waitpid(self.pid, flag)
E   Failed: Timeout >60.0s
----------------------------- Captured stdout call -----------------------------
[2025-01-01T13:45:44.503+0000] {local_executor.py:60} INFO - Worker starting up pid=288
[2025-01-01T13:45:44.519+0000] {local_executor.py:60} INFO - Worker starting up pid=290
[2025-01-01T13:45:44.523+0000] {local_executor.py:215} INFO - Shutting down LocalExecutor; waiting for running tasks to finish.  Signal again if you don't want to wait.
[2025-01-01T13:45:44.525+0000] {local_executor.py:96} ERROR - uhoh
Traceback (most recent call last):
  File ""/opt/airflow/airflow/executors/local_executor.py"", line 92, in _run_worker
    _execute_work(log, workload)
  File ""/opt/airflow/airflow/executors/local_executor.py"", line 113, in _execute_work
    supervise(
  File ""/usr/local/lib/python3.11/unittest/mock.py"", line 1124, in __call__
    return self._mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/unittest/mock.py"", line 1128, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/unittest/mock.py"", line 1189, in _execute_mock_call
    result = effect(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/tests/executors/test_local_executor.py"", line 71, in fake_supervise
    raise RuntimeError(""fake failure"")
RuntimeError: fake failure
[2025-01-01T13:45:44.527+0000] {local_executor.py:60} INFO - Worker starting up pid=305
[2025-01-01T13:45:44.564+0000] {local_executor.py:60} INFO - Worker starting up pid=318
[2025-01-01T13:45:44.580+0000] {local_executor.py:60} INFO - Worker starting up pid=297
------------------------------ Captured log call -------------------------------
INFO     airflow.executors.local_executor.LocalExecutor:local_executor.py:215 Shutting down LocalExecutor; waiting for running tasks to finish.  Signal again if you don't want to wait.
----------- generated xml file: /files/test_result-core-postgres.xml -----------
```

For example https://github.com/apache/airflow/actions/runs/12571140824/job/35041860269#step:6:2934",potiuk,2025-01-01 16:19:00+00:00,[],2025-01-01 16:21:16+00:00,,https://github.com/apache/airflow/issues/45329,"[('Quarantine', 'Issues that are occasionally failing and are quarantined'), ('area:Executors-core', 'LocalExecutor & SequentialExecutor')]",[],
2764671013,issue,closed,duplicate,Add support for `BaseOperator.execution_timeout` in Task SDK,,kaxil,2024-12-31 20:57:45+00:00,['amoghrajesh'],2024-12-31 20:58:05+00:00,2024-12-31 20:58:05+00:00,https://github.com/apache/airflow/issues/45317,[],"[{'comment_id': 2566707866, 'issue_id': 2764671013, 'author': 'kaxil', 'body': 'Closing in favor of https://github.com/apache/airflow/issues/45307', 'created_at': datetime.datetime(2024, 12, 31, 20, 58, 5, tzinfo=datetime.timezone.utc)}]","kaxil (Issue Creator) on (2024-12-31 20:58:05 UTC): Closing in favor of https://github.com/apache/airflow/issues/45307

"
2764014285,issue,closed,completed,Port task timeout over to task sdk,"### Body

Handle task timeouts similar to what we do in legacy Airflow. Ref: https://github.com/apache/airflow/blob/main/airflow/models/taskinstance.py#L726-L744",amoghrajesh,2024-12-31 06:35:17+00:00,['amoghrajesh'],2025-01-01 05:09:06+00:00,2025-01-01 05:09:06+00:00,https://github.com/apache/airflow/issues/45307,"[('area:core', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK'), ('area:task-sdk', None)]","[{'comment_id': 2566347447, 'issue_id': 2764014285, 'author': 'raphaelauv', 'body': 'maybe the occasion to fix the timeout on dynamic task mapping -> https://github.com/apache/airflow/issues/37332 \r\n\r\nwdyt ?', 'created_at': datetime.datetime(2024, 12, 31, 10, 59, 23, tzinfo=datetime.timezone.utc)}]","raphaelauv on (2024-12-31 10:59:23 UTC): maybe the occasion to fix the timeout on dynamic task mapping -> https://github.com/apache/airflow/issues/37332 

wdyt ?

"
2763702421,issue,open,,AIP-81 Implement Log Insert Endpoint to REST API (FastAPI),"### Description

We should be able to insert Log (`from airflow.models.log import Log`) similar to here,
https://github.com/apache/airflow/blob/52ed7d72e9669a7bdfaf11caeea6daa29911bc8f/airflow/utils/cli_action_loggers.py#L106
This will enable us to use the logging feature without relying on the session.

### Use case/motivation

To provide the same functionality for the CLI with API integration.
[AIP-81](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-81+Enhanced+Security+in+CLI+via+Integration+of+API)

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-12-30 21:12:31+00:00,[],2025-01-11 16:43:35+00:00,,https://github.com/apache/airflow/issues/45304,"[('area:logging', ''), ('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]","[{'comment_id': 2569181088, 'issue_id': 2763702421, 'author': 'jason810496', 'body': 'Hi @bugraoz93,  \r\n\r\nIf I may suggest, this could also be achieved by using a command-line-specific `audience` in the JWT. Once the new `action_logging` decorator is applied to FastAPI, the audit log logic could be handled in a single API call.  \r\nThis approach could also reduce the chances of missing audit logs. For instance, if the CLI is forcibly terminated right after calling the core API and before the audit log API is invoked, the audit log might not be recorded.  \r\n\r\nWhat do you think?\r\ncc @pierrejeambrun  @vincbeck', 'created_at': datetime.datetime(2025, 1, 3, 12, 55, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569721034, 'issue_id': 2763702421, 'author': 'perry2of5', 'body': ""What would this log from the CLI code that shouldn't already be logged by the REST API itself?"", 'created_at': datetime.datetime(2025, 1, 3, 19, 35, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569791080, 'issue_id': 2763702421, 'author': 'bugraoz93', 'body': '> Hi @bugraoz93,\r\n> \r\n> If I may suggest, this could also be achieved by using a command-line-specific `audience` in the JWT. Once the new `action_logging` decorator is applied to FastAPI, the audit log logic could be handled in a single API call. This approach could also reduce the chances of missing audit logs. For instance, if the CLI is forcibly terminated right after calling the core API and before the audit log API is invoked, the audit log might not be recorded.\r\n> \r\n> What do you think? cc @pierrejeambrun @vincbeck\r\n\r\n@jason810496 It makes sense. There is one small thing here, we should identify when the event originates from cli_ and log the full_command. One way to achieve this is by using headers in the API, though this would result in a custom implementation specific to the CLI on the API side.\r\nIf we intend to include this functionality in the REST API for the CLI use case, I would be happy to close this in favour of API auditing the CLI actions. Otherwise, I would suggest moving forward with this approach.', 'created_at': datetime.datetime(2025, 1, 3, 20, 42, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569795695, 'issue_id': 2763702421, 'author': 'bugraoz93', 'body': '> What would this log from the CLI code that shouldn\'t already be logged by the REST API itself?\r\n\r\nWe are passing the event as a `cli_` with `extra` as `full_command` (`json.dumps({""host_name"": host_name, ""full_command"": full_command})`). This is a specific event sent by the CLI that wraps all the CLI action commands. Below, `cli_action_logger` will stay for `local commands`, but it should be replaced for `remote commands` because this uses the `session`. There are two things to consider here. The first is API call Audit Logs, which will again come from `signed in` users, which is the regular API auditing. The second one is we should also log this event coming from CLI means `event: cli_<>` and `extra: full_command`, while understanding that the requests are coming from the CLI via `headers` or similar. If we only use the API auditing, there will be nothing to differentiate if the action is taken from the CLI or not. This again requires bespoke implementation on the API end to make it possible. This brings about maintaining code pieces in the API for the CLI, which can make the two things more tightly coupled. That\'s why I thought it would be easier just to implement an endpoint for it. I think understanding if a call is coming from the CLI would make things a lot easier to debug in case of security emergencies or for collecting evidence for breaches.\r\n \r\nhttps://github.com/apache/airflow/blob/a2c5a48434a95432ef369a1ffcd69a1dac435ed7/airflow/utils/cli.py#L117\r\n\r\nhttps://github.com/apache/airflow/blob/52ed7d72e9669a7bdfaf11caeea6daa29911bc8f/airflow/utils/cli_action_loggers.py#L118-L160', 'created_at': datetime.datetime(2025, 1, 3, 20, 47, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569886390, 'issue_id': 2763702421, 'author': 'perry2of5', 'body': ""Thanks.\n\nPerhaps this is a solved problem, but I can't think of a way to guarantee we know if a client is CLI or something else if a malicious actor is involved. I would think they could do the OAuth dance in the CLI way or the WebUI way and get a different audience unless a particular account was locked to only one type or the other. Still, it would be a known, though likely compromised, account that accessed the API..... In this case knowing the IP address the request reached the outermost load balancer is probably as much as we can rely on. Possibly my knowledge is out of date here.\n\nFor a non-malicious actor, knowing the exact CLI command might help to research why something was changed a particular way. I suppose the question is: is there is enough value to knowing the exact CLI command in addition to the REST API request parameters and body to be worth adding the special case? Beyond that, knowing the audience (and knowing not to depend on it) might be as much as is worth implementing. I'm wishy-washy on the value of this. I hope my ramblings are useful :)"", 'created_at': datetime.datetime(2025, 1, 3, 22, 20, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569905240, 'issue_id': 2763702421, 'author': 'bugraoz93', 'body': 'I was giving an example of why we should maintain the behaviour and explaining why this feature is needed in case the current behaviours persist. Keeping an additional audit log is always valuable when it comes to security. \r\n\r\nI think every idea is useful here, as it brings another perspective to the table. Thanks! :)', 'created_at': datetime.datetime(2025, 1, 3, 22, 46, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2575295585, 'issue_id': 2763702421, 'author': 'pierrejeambrun', 'body': 'All CLI actions will eventually go through the API and the API is responsible for login any user action already with the `action_logging` or something similar on the new FastAPI API.\r\n\r\nI assume that API authentication when calling from the CLI has specific permissions etc... and that it is authenticated through a dedicated role / has a specific token / something identifiable from the API to know that this is a call that is coming from the CLI. Because for instance some endpoints are only accessible from the CLI (specific permissions etc...). Thus I think the API should be able to log properly the actions and the `owner` should be `cli`. Command can be pushed into extra ?\r\n\r\nI think this comes down to:\r\n- How is the API authentication working when calling from the CLI ? How is the token retrieved, what user and role, etc.', 'created_at': datetime.datetime(2025, 1, 7, 13, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585329944, 'issue_id': 2763702421, 'author': 'bugraoz93', 'body': ""> All CLI actions will eventually go through the API and the API is responsible for login any user action already with the `action_logging` or something similar on the new FastAPI API.\r\n> \r\n> I assume that API authentication when calling from the CLI has specific permissions etc... and that it is authenticated through a dedicated role / has a specific token / something identifiable from the API to know that this is a call that is coming from the CLI. Because for instance some endpoints are only accessible from the CLI (specific permissions etc...). Thus I think the API should be able to log properly the actions and the `owner` should be `cli`. Command can be pushed into extra ?\r\n> \r\n> I think this comes down to:\r\n> \r\n> * How is the API authentication working when calling from the CLI ? How is the token retrieved, what user and role, etc.\r\n\r\nMany thanks for your perspective on this! I have very high-level answers at the moment which may not be enough to make decisions. The answers to these questions will enlighten multiple parts of this project. I will answer them in a detailed way very soon to be brought up into discussion. So that we won't do any duplicate or overlapping work"", 'created_at': datetime.datetime(2025, 1, 11, 16, 43, 35, tzinfo=datetime.timezone.utc)}]","jason810496 on (2025-01-03 12:55:25 UTC): Hi @bugraoz93,  

If I may suggest, this could also be achieved by using a command-line-specific `audience` in the JWT. Once the new `action_logging` decorator is applied to FastAPI, the audit log logic could be handled in a single API call.  
This approach could also reduce the chances of missing audit logs. For instance, if the CLI is forcibly terminated right after calling the core API and before the audit log API is invoked, the audit log might not be recorded.  

What do you think?
cc @pierrejeambrun  @vincbeck

perry2of5 on (2025-01-03 19:35:21 UTC): What would this log from the CLI code that shouldn't already be logged by the REST API itself?

bugraoz93 (Issue Creator) on (2025-01-03 20:42:39 UTC): @jason810496 It makes sense. There is one small thing here, we should identify when the event originates from cli_ and log the full_command. One way to achieve this is by using headers in the API, though this would result in a custom implementation specific to the CLI on the API side.
If we intend to include this functionality in the REST API for the CLI use case, I would be happy to close this in favour of API auditing the CLI actions. Otherwise, I would suggest moving forward with this approach.

bugraoz93 (Issue Creator) on (2025-01-03 20:47:12 UTC): We are passing the event as a `cli_` with `extra` as `full_command` (`json.dumps({""host_name"": host_name, ""full_command"": full_command})`). This is a specific event sent by the CLI that wraps all the CLI action commands. Below, `cli_action_logger` will stay for `local commands`, but it should be replaced for `remote commands` because this uses the `session`. There are two things to consider here. The first is API call Audit Logs, which will again come from `signed in` users, which is the regular API auditing. The second one is we should also log this event coming from CLI means `event: cli_<>` and `extra: full_command`, while understanding that the requests are coming from the CLI via `headers` or similar. If we only use the API auditing, there will be nothing to differentiate if the action is taken from the CLI or not. This again requires bespoke implementation on the API end to make it possible. This brings about maintaining code pieces in the API for the CLI, which can make the two things more tightly coupled. That's why I thought it would be easier just to implement an endpoint for it. I think understanding if a call is coming from the CLI would make things a lot easier to debug in case of security emergencies or for collecting evidence for breaches.
 
https://github.com/apache/airflow/blob/a2c5a48434a95432ef369a1ffcd69a1dac435ed7/airflow/utils/cli.py#L117

https://github.com/apache/airflow/blob/52ed7d72e9669a7bdfaf11caeea6daa29911bc8f/airflow/utils/cli_action_loggers.py#L118-L160

perry2of5 on (2025-01-03 22:20:21 UTC): Thanks.

Perhaps this is a solved problem, but I can't think of a way to guarantee we know if a client is CLI or something else if a malicious actor is involved. I would think they could do the OAuth dance in the CLI way or the WebUI way and get a different audience unless a particular account was locked to only one type or the other. Still, it would be a known, though likely compromised, account that accessed the API..... In this case knowing the IP address the request reached the outermost load balancer is probably as much as we can rely on. Possibly my knowledge is out of date here.

For a non-malicious actor, knowing the exact CLI command might help to research why something was changed a particular way. I suppose the question is: is there is enough value to knowing the exact CLI command in addition to the REST API request parameters and body to be worth adding the special case? Beyond that, knowing the audience (and knowing not to depend on it) might be as much as is worth implementing. I'm wishy-washy on the value of this. I hope my ramblings are useful :)

bugraoz93 (Issue Creator) on (2025-01-03 22:46:41 UTC): I was giving an example of why we should maintain the behaviour and explaining why this feature is needed in case the current behaviours persist. Keeping an additional audit log is always valuable when it comes to security. 

I think every idea is useful here, as it brings another perspective to the table. Thanks! :)

pierrejeambrun on (2025-01-07 13:27:00 UTC): All CLI actions will eventually go through the API and the API is responsible for login any user action already with the `action_logging` or something similar on the new FastAPI API.

I assume that API authentication when calling from the CLI has specific permissions etc... and that it is authenticated through a dedicated role / has a specific token / something identifiable from the API to know that this is a call that is coming from the CLI. Because for instance some endpoints are only accessible from the CLI (specific permissions etc...). Thus I think the API should be able to log properly the actions and the `owner` should be `cli`. Command can be pushed into extra ?

I think this comes down to:
- How is the API authentication working when calling from the CLI ? How is the token retrieved, what user and role, etc.

bugraoz93 (Issue Creator) on (2025-01-11 16:43:35 UTC): Many thanks for your perspective on this! I have very high-level answers at the moment which may not be enough to make decisions. The answers to these questions will enlighten multiple parts of this project. I will answer them in a detailed way very soon to be brought up into discussion. So that we won't do any duplicate or overlapping work

"
2763697552,issue,closed,completed,AIP-81 Include Overwrite Functionality to Bulk Insert API Endpoints for Connection&Pool,"### Description

When inserting bulk objects in REST API (FastAPI), a flag should decide whether passed objects should be updated in case of existence (--overwrite).
- `/connections/bulk` 
- `/pools/bulk` 

### Use case/motivation

To provide the same functionality for the CLI Connection command with API integration.
[AIP-81](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-81+Enhanced+Security+in+CLI+via+Integration+of+API)

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-12-30 21:08:28+00:00,['jason810496'],2025-01-05 08:16:51+00:00,2025-01-05 08:16:51+00:00,https://github.com/apache/airflow/issues/45303,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]","[{'comment_id': 2569156450, 'issue_id': 2763697552, 'author': 'jason810496', 'body': 'Hi @bugraoz93, I can work on this issue, could you assign to me ? Thanks !\r\nI also have a small question: Should we replace the HTTP method from `POST` to `PUT`? For this use case, the endpoint is no longer solely for **create** operations.', 'created_at': datetime.datetime(2025, 1, 3, 12, 33, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569192483, 'issue_id': 2763697552, 'author': 'bugraoz93', 'body': ""Hi @jason810496 , great point! I think we can. Indeed, it won't only accept inserts in this case and fits into PUT more as a context and definition. \nAssigned, thanks!"", 'created_at': datetime.datetime(2025, 1, 3, 13, 3, 46, tzinfo=datetime.timezone.utc)}]","jason810496 (Assginee) on (2025-01-03 12:33:50 UTC): Hi @bugraoz93, I can work on this issue, could you assign to me ? Thanks !
I also have a small question: Should we replace the HTTP method from `POST` to `PUT`? For this use case, the endpoint is no longer solely for **create** operations.

bugraoz93 (Issue Creator) on (2025-01-03 13:03:46 UTC): Hi @jason810496 , great point! I think we can. Indeed, it won't only accept inserts in this case and fits into PUT more as a context and definition. 
Assigned, thanks!

"
2763689050,issue,closed,completed,AIP-81 Implement Create Default Connections in REST API (FastAPI),"### Description

Implement an endpoint that creates default connections `from airflow.utils.db import create_default_connections as db_create_default_connections`.

### Use case/motivation

To provide the same functionality for the CLI Connection command with API integration.
[AIP-81](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-81+Enhanced+Security+in+CLI+via+Integration+of+API)

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-12-30 20:57:31+00:00,['bugraoz93'],2025-01-08 10:48:29+00:00,2025-01-08 10:48:29+00:00,https://github.com/apache/airflow/issues/45302,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]",[],
2763685335,issue,closed,completed,AIP-81 Implement DAG Report Endpoint in REST API (FastAPI),"### Description

Create an endpoint in FastAPI that returns the DAG report (`dagbag.dagbag_stats`).

### Related issues

[AIP-81](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-81+Enhanced+Security+in+CLI+via+Integration+of+API)

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-12-30 20:52:39+00:00,['jason810496'],2025-01-21 16:06:39+00:00,2025-01-21 16:06:39+00:00,https://github.com/apache/airflow/issues/45301,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]","[{'comment_id': 2569139532, 'issue_id': 2763685335, 'author': 'jason810496', 'body': 'Hi @bugraoz93, I can work on this issue, could you assign to me ? Thanks !', 'created_at': datetime.datetime(2025, 1, 3, 12, 19, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569187035, 'issue_id': 2763685335, 'author': 'bugraoz93', 'body': 'Hi @jason810496 , for sure, assigned, thanks!', 'created_at': datetime.datetime(2025, 1, 3, 12, 59, 20, tzinfo=datetime.timezone.utc)}]","jason810496 (Assginee) on (2025-01-03 12:19:01 UTC): Hi @bugraoz93, I can work on this issue, could you assign to me ? Thanks !

bugraoz93 (Issue Creator) on (2025-01-03 12:59:20 UTC): Hi @jason810496 , for sure, assigned, thanks!

"
2763571417,issue,closed,completed,"Status of testing Providers that were prepared on December 30, 2024","### Body

I have a kind request for all the contributors to the latest provider packages release.
Could you please help us to test the RC versions of the providers?

The guidelines on how to test providers can be found in

[Verify providers by contributors](https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDER_PACKAGES.md#verify-the-release-candidate-by-contributors)

Let us know in the comment, whether the issue is addressed.

Those are providers that require testing as there were some substantial changes introduced:


## Provider [fab: 1.5.2rc1](https://pypi.org/project/apache-airflow-providers-fab/1.5.2rc1)
   - [x] [Correctly import isabs from os.path (#45178)](https://github.com/apache/airflow/pull/45178): @potiuk
     Linked issues:
       - [x] [Linked Issue #45139](https://github.com/apache/airflow/pull/45139): @shubhamraj-git
   - [x] [Invalidate user session on password reset (#45139)](https://github.com/apache/airflow/pull/45139): @shubhamraj-git

<!--

NOTE TO RELEASE MANAGER:

You can move here the providers that have doc-only changes or for which changes are trivial, and
you could assess that they are OK.

-->
All users involved in the PRs:
@potiuk @shubhamraj-git


### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2024-12-30 18:43:54+00:00,[],2025-01-03 14:42:32+00:00,2025-01-03 14:42:31+00:00,https://github.com/apache/airflow/issues/45298,"[('area:providers', ''), ('kind:meta', 'High-level information important to the community'), ('testing status', 'Status of testing releases')]","[{'comment_id': 2565915866, 'issue_id': 2763571417, 'author': 'potiuk', 'body': 'Installed the new FAB provider with airflow 2.10.4. All works fine, including the scenario of CLI password reset.', 'created_at': datetime.datetime(2024, 12, 30, 21, 7, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2566127866, 'issue_id': 2763571417, 'author': 'amoghrajesh', 'body': 'Tested out the scenario of password reset from the CLI and it successfully invalidates the session, after installing the new FAB provider with Airflow 2.10.4 version using breeze. Looks good!', 'created_at': datetime.datetime(2024, 12, 31, 4, 43, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2566309445, 'issue_id': 2763571417, 'author': 'shubhamraj-git', 'body': 'Installed the airflow 2.10.4, and installed the fab provider 1.5.2rc1\r\nTested out the CLI password reset scenario, it successfully invalidates.\r\nIt works fine.\r\n\r\nThanks.\r\n\r\nOn Tue, Dec 31, 2024 at 10:13\u202fAM Amogh Desai ***@***.***>\r\nwrote:\r\n\r\n> Tested out the scenario of password reset from the CLI and it successfully\r\n> invalidates the session, after installing the new FAB provider with Airflow\r\n> 2.10.4 version using breeze. Looks good!\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/apache/airflow/issues/45298#issuecomment-2566127866>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/ALPQ3RRSZKYNTO2RTGTSO3T2IIOIFAVCNFSM6AAAAABUMNXDX2VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDKNRWGEZDOOBWGY>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>', 'created_at': datetime.datetime(2024, 12, 31, 10, 7, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569336516, 'issue_id': 2763571417, 'author': 'eladkal', 'body': 'Thank you everyone. Providers are released.\r\n\r\nI invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).', 'created_at': datetime.datetime(2025, 1, 3, 14, 42, 32, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-12-30 21:07:54 UTC): Installed the new FAB provider with airflow 2.10.4. All works fine, including the scenario of CLI password reset.

amoghrajesh on (2024-12-31 04:43:25 UTC): Tested out the scenario of password reset from the CLI and it successfully invalidates the session, after installing the new FAB provider with Airflow 2.10.4 version using breeze. Looks good!

shubhamraj-git on (2024-12-31 10:07:17 UTC): Installed the airflow 2.10.4, and installed the fab provider 1.5.2rc1
Tested out the CLI password reset scenario, it successfully invalidates.
It works fine.

Thanks.

On Tue, Dec 31, 2024 at 10:13 AM Amogh Desai ***@***.***>
wrote:

eladkal (Issue Creator) on (2025-01-03 14:42:32 UTC): Thank you everyone. Providers are released.

I invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).

"
2763509259,issue,closed,completed,Can't add Connections through Helm Official chart,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.3

### What happened?

I was following the [example of the docs on how to add a connection ](https://airflow.apache.org/docs/helm-chart/stable/adding-connections-and-variables.html), 

I'm trying to add an example conn by uri:
`HTTP_EXAMPLE=http://abc:123@foo`

and so adapted my values.yaml:
```
secret:
  - envName: ""AIRFLOW_CONN_HTTP_EXAMPLE""
    secretName: ""my-airflow-connections""
    secretKey: ""AIRFLOW_CONN_HTTP_EXAMPLE""

extraSecrets:
  my-airflow-connections:
    data: |
      AIRFLOW_CONN_HTTP_EXAMPLE: 'aHR0cDovL2FiYzoxMjNAZm9v'
```

Then checked the envvars of scheduler to see if the conn variable was set and I could see my env there:
`kubectl -n airflow exec -it airflow-scheduler-78fd579f5d-bv895 -- printenv` 
the result was:
```
AIRFLOW_CONN_HTTP_EXAMPLE=http://abc:123@foo
AIRFLOW__KUBERNETES_SECRETS__AIRFLOW_CONN_HTTP_EXAMPLE=my-airflow-connections=AIRFLOW_CONN_HTTP_EXAMPLE
```


But then when I get check the connections throught Web UI or CLI  (`airflow connections list`), it shows `No data found`.
```
$ airflow connections list
/home/airflow/.local/lib/python3.12/site-packages/airflow/metrics/statsd_logger.py:184 RemovedInAirflow3Warning: The basic metric validator will be deprecated in the future in favor of pattern-matching.  You can try this now by setting config option metrics_use_pattern_match to True.
No data found
```

I checked the logs of scheduler but found no clue, 
I'm wondering where to watch for logs of conns or help on how to fix this.


### What you think should happen instead?

A connection be added to the airflow

### How to reproduce

Officla helm chart version: `airflow-1.15.0`
App version: `2.9.3 `
Chart: `https://artifacthub.io/packages/helm/apache-airflow/airflow`

### Operating System

helm airflow-1.15.0

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bmt8,2024-12-30 17:38:45+00:00,[],2024-12-31 12:32:27+00:00,2024-12-30 20:41:08+00:00,https://github.com/apache/airflow/issues/45295,"[('kind:bug', 'This is a clearly a bug'), ('area:helm-chart', 'Airflow Helm Chart'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2565752574, 'issue_id': 2763509259, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 30, 17, 38, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2565897810, 'issue_id': 2763509259, 'author': 'potiuk', 'body': 'This is expected behaviour. As explained in the connection page - connections added through env variable are not shown in UI/ API.', 'created_at': datetime.datetime(2024, 12, 30, 20, 41, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2566195070, 'issue_id': 2763509259, 'author': 'chandsharma', 'body': '@potiuk \r\n> This is expected behaviour. As explained in the connection page - connections added through env variable are not shown in UI/ API.\r\n\r\nhow do I show these connections in the UI (to make sure they are there)?', 'created_at': datetime.datetime(2024, 12, 31, 7, 15, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2566411946, 'issue_id': 2763509259, 'author': 'potiuk', 'body': "">  how do I show these connections in the UI (to make sure they are there)?\r\n\r\nYou can't.  You can run Python code to read connections (see connection documentation) and check that they are properly retrieved."", 'created_at': datetime.datetime(2024, 12, 31, 12, 32, 26, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-30 17:38:47 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-12-30 20:41:08 UTC): This is expected behaviour. As explained in the connection page - connections added through env variable are not shown in UI/ API.

chandsharma on (2024-12-31 07:15:32 UTC): @potiuk 

how do I show these connections in the UI (to make sure they are there)?

potiuk on (2024-12-31 12:32:26 UTC): You can't.  You can run Python code to read connections (see connection documentation) and check that they are properly retrieved.

"
2763369312,issue,closed,completed,Manual success of a mid-pipeline task triggers its downstream even though an upstream sensor is still deferred (question about none_failed behavior),"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.1

### What happened?

I have a DAG with three tasks in sequence:

wait_for_3am (a DateTimeSensorAsync), which waits until 3:00 AM and thus remains in deferred state until the target time.
empty_task_1 (an EmptyOperator), which depends on wait_for_3am.
empty_task_2 (another EmptyOperator), which depends on empty_task_1.
All tasks inherit the default trigger rule of none_failed. According to the documentation for none_failed, all upstream tasks must have “not failed or upstream_failed” (i.e. they should have succeeded or been skipped) in order to trigger the downstream tasks.

However, if I manually mark empty_task_1 as success while wait_for_3am is still in the deferred state (i.e., it has neither failed nor succeeded yet), then empty_task_2 immediately runs. From a scheduler standpoint, empty_task_2 sees empty_task_1 = success and therefore proceeds under none_failed, even though wait_for_3am has not actually completed.

### What you think should happen instead?

I expected that if an upstream sensor (wait_for_3am) is still in a non-terminal (deferred) state, marking empty_task_1 as success would not trigger empty_task_2 immediately .

### How to reproduce

Try this DAG, mark empty_task_1 as success .

```
from datetime import datetime, timedelta
from airflow import DAG
from airflow.utils.trigger_rule import TriggerRule
from airflow.sensors.date_time import DateTimeSensorAsync
from airflow.operators.empty import EmptyOperator

default_args = {
    'owner': 'airflow',
    ""trigger_rule"": TriggerRule.NONE_FAILED,
}

with DAG(
    'simple_dag_with_datetime_sensor',
    default_args=default_args,
    description='A simple DAG with DateTimeSensorAsync and two EmptyOperators',
    schedule_interval=timedelta(days=1),
    start_date=datetime(2024, 12, 30),
    catchup=False,
    max_active_runs=1,
) as dag:

    wait_for_3am = DateTimeSensorAsync(
        task_id='wait_for_3am',
        target_time=datetime.combine(datetime.today(), datetime.min.time()) + timedelta(hours=3),
    )

    empty_task_1 = EmptyOperator(
        task_id='empty_task_1',
    )

    empty_task_2 = EmptyOperator(
        task_id='empty_task_2',
    )

    wait_for_3am >> empty_task_1 >> empty_task_2
```

### Operating System

RHEL8

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",darenpang,2024-12-30 15:31:59+00:00,['darenpang'],2025-01-05 15:54:01+00:00,2025-01-05 15:54:00+00:00,https://github.com/apache/airflow/issues/45292,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('area:core', '')]","[{'comment_id': 2567071546, 'issue_id': 2763369312, 'author': 'RNHTTR', 'body': 'Assigned you @darenpang', 'created_at': datetime.datetime(2025, 1, 1, 16, 33, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567245286, 'issue_id': 2763369312, 'author': 'darenpang', 'body': '@RNHTTR \r\nWould you please confirm the intended behavior?', 'created_at': datetime.datetime(2025, 1, 2, 3, 2, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2571669741, 'issue_id': 2763369312, 'author': 'darenpang', 'body': '@RNHTTR \r\nI’ve found this in the document.\r\n\r\n> By default, Airflow will wait for all upstream (direct parents) tasks for a task to be [successful](https://airflow.apache.org/docs/apache-airflow/2.10.1/core-concepts/tasks.html#concepts-task-states) before it runs that task.\r\n\r\nSo Airflow does not recursively check grandparent tasks. It only looks to see if all direct parents are not failed. So this behavior is expected and consistent with the documentation.\r\nIf you confirm this , maybe this issue should be closed . Thank you .', 'created_at': datetime.datetime(2025, 1, 5, 15, 51, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2571670644, 'issue_id': 2763369312, 'author': 'potiuk', 'body': ""Correct. That's expected."", 'created_at': datetime.datetime(2025, 1, 5, 15, 54, tzinfo=datetime.timezone.utc)}]","RNHTTR on (2025-01-01 16:33:34 UTC): Assigned you @darenpang

darenpang (Issue Creator) on (2025-01-02 03:02:09 UTC): @RNHTTR 
Would you please confirm the intended behavior?

darenpang (Issue Creator) on (2025-01-05 15:51:01 UTC): @RNHTTR 
I’ve found this in the document.


So Airflow does not recursively check grandparent tasks. It only looks to see if all direct parents are not failed. So this behavior is expected and consistent with the documentation.
If you confirm this , maybe this issue should be closed . Thank you .

potiuk on (2025-01-05 15:54:00 UTC): Correct. That's expected.

"
2763120871,issue,open,,Incorrect field rendering in DAG Params,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I was using 2.7.1 before and didn't have this problem but after upgrading to 2.10.4 I am facing this problem in my existing DAG.
I'm adding parameters to the DAG using Params. When I create a field with the **array** type and specify the element type **number**, the field is rendered as a list on the UI.
<img width=""704"" alt=""image"" src=""https://github.com/user-attachments/assets/a5025127-1c98-44d9-bc6a-50d581b5025b"" />

But when I create the same field with the **array** type but specify the element type **string**, the list is not rendered on the UI and each element must be passed on a new line. I think this is not very convenient and not intuitive.
<img width=""621"" alt=""image"" src=""https://github.com/user-attachments/assets/7dfc5965-62fb-4a7a-971a-b0c3466bc98d"" />


### What you think should happen instead?

I think it would be more convenient if the data type did not affect the way the field is rendered on the UI, and if the array type is specified, then it would be displayed on the UI as a familiar list.

### How to reproduce

Code for array with numbers:
`params={
        ""sf_additional_schemas"": Param(
            title=""Additional Snowflake schemas"",
            default=[1, 2, 3],
            type=""array"",
            items={
                ""type"": ""number""
            }
        ),`
        
Code for array with strings:
`params={
        ""sf_additional_schemas"": Param(
            title=""Additional Snowflake schemas"",
            default=[""schema1""],
            type=""array"",
            items={
                ""type"": ""string""
            }
        ),`

### Operating System

MacOS 15.0

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",andrewD46,2024-12-30 12:12:57+00:00,[],2025-01-11 10:32:20+00:00,,https://github.com/apache/airflow/issues/45290,"[('type:new-feature', 'Changelog: New Features'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2565404371, 'issue_id': 2763120871, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 30, 12, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585129892, 'issue_id': 2763120871, 'author': 'rawwar', 'body': 'So, after going through the code, strings items are specifically excluded with the condition\r\n`and (""type"" not in form_details.schema[""items""] or ""string"" not in form_details.schema[""items""][""type""]) ` here: https://github.com/apache/airflow/blob/830192ab88f2b8089793209fa480c248784b8fb7/airflow/www/templates/airflow/trigger.html#L89\r\n\r\nI remove the condition and it renders like below :\r\n<img width=""627"" alt=""image"" src=""https://github.com/user-attachments/assets/4ad52e3a-a1fb-4570-8f89-41c5fbfa91a5"" />', 'created_at': datetime.datetime(2025, 1, 11, 7, 20, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585130132, 'issue_id': 2763120871, 'author': 'rawwar', 'body': ""This was committed in PR https://github.com/apache/airflow/pull/39993 . @jscheffl , is it ok if we remove the condition? I'm not completely sure if there are other consequences. I'm still trying to understand the code here.\r\n\r\nEdit: After reading through the issue, it looks like an intentional change."", 'created_at': datetime.datetime(2025, 1, 11, 7, 21, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585195095, 'issue_id': 2763120871, 'author': 'jscheffl', 'body': 'An array of strings was the initial implementation and the ""non string any other type"" was added as requested in https://github.com/apache/airflow/issues/39972\r\n\r\nAny further complexity I would not like to add in 2.10 line and also reverting the support for non-string I would keep as people are using it.\r\n\r\nStill if you find it in-conventient to copy/paste a string array as multiline text you can copy your JSIN style string array to the JSON parameter dict in Advanced Options.\r\n\r\nIf you want to improve/contribute then you need to wait a moment until the new UI in Airflow 3 is implemented and then it can be enhanced there. I assume the UI has always end-less demand for features and options, especially in UI entry options. It can slowly grow in 3.x line.\r\n\r\nBy the way, I think for 90% of users who want to have a list of strings being added it is much easier in a simple multiline-text field than if you need to format your strings into a JSON style.', 'created_at': datetime.datetime(2025, 1, 11, 10, 23, 23, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-30 12:13:00 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

rawwar on (2025-01-11 07:20:17 UTC): So, after going through the code, strings items are specifically excluded with the condition
`and (""type"" not in form_details.schema[""items""] or ""string"" not in form_details.schema[""items""][""type""]) ` here: https://github.com/apache/airflow/blob/830192ab88f2b8089793209fa480c248784b8fb7/airflow/www/templates/airflow/trigger.html#L89

I remove the condition and it renders like below :
<img width=""627"" alt=""image"" src=""https://github.com/user-attachments/assets/4ad52e3a-a1fb-4570-8f89-41c5fbfa91a5"" />

rawwar on (2025-01-11 07:21:23 UTC): This was committed in PR https://github.com/apache/airflow/pull/39993 . @jscheffl , is it ok if we remove the condition? I'm not completely sure if there are other consequences. I'm still trying to understand the code here.

Edit: After reading through the issue, it looks like an intentional change.

jscheffl on (2025-01-11 10:23:23 UTC): An array of strings was the initial implementation and the ""non string any other type"" was added as requested in https://github.com/apache/airflow/issues/39972

Any further complexity I would not like to add in 2.10 line and also reverting the support for non-string I would keep as people are using it.

Still if you find it in-conventient to copy/paste a string array as multiline text you can copy your JSIN style string array to the JSON parameter dict in Advanced Options.

If you want to improve/contribute then you need to wait a moment until the new UI in Airflow 3 is implemented and then it can be enhanced there. I assume the UI has always end-less demand for features and options, especially in UI entry options. It can slowly grow in 3.x line.

By the way, I think for 90% of users who want to have a list of strings being added it is much easier in a simple multiline-text field than if you need to format your strings into a JSON style.

"
2762980983,issue,open,,"Scheduled_duration metric is rarely sent, and with too many tags","### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Hello!
We are running Airflow 2.10.4 on EKS version 1.30.6.
I set up statsd metric collection as described [here](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/metrics.html).
Using datadog agent running as a daemonset on each node to collect the metrics.
I am able to receive and search all metrics successfully in datadog, but task.scheduled_duration is very rarely sent, and when it's sent the numbers and tagging don't make sense to me.
For example, over the last week I only have a single datapoint at 3.26k, and it seems like it's tagged with several task_ids and dag_ids.

### What you think should happen instead?

I expect to receive the metric every time a task is scheduled, and tagged correctly with only the relevant task_id and dag_id.

### How to reproduce

Deployment details listed below, please let me know if there are any other missing information that might be relevant.

### Operating System

Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

```
apache-airflow==2.10.4
apache-airflow-providers-amazon==9.1.0
apache-airflow-providers-celery==3.8.5
apache-airflow-providers-cncf-kubernetes==10.0.1
apache-airflow-providers-common-compat==1.2.2
apache-airflow-providers-common-io==1.4.2
apache-airflow-providers-common-sql==1.20.0
apache-airflow-providers-dbt-cloud==3.11.2
apache-airflow-providers-docker==3.14.1
apache-airflow-providers-elasticsearch==5.5.3
apache-airflow-providers-fab==1.5.1
apache-airflow-providers-ftp==3.11.1
apache-airflow-providers-google==11.0.0
apache-airflow-providers-grpc==3.6.0
apache-airflow-providers-hashicorp==3.8.0
apache-airflow-providers-http==4.13.3
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-microsoft-azure==11.1.0
apache-airflow-providers-mysql==5.7.4
apache-airflow-providers-odbc==4.8.1
apache-airflow-providers-openlineage==1.14.0
apache-airflow-providers-postgres==5.14.0
apache-airflow-providers-redis==3.8.0
apache-airflow-providers-sendgrid==3.6.0
apache-airflow-providers-sftp==4.11.1
apache-airflow-providers-slack==8.9.2
apache-airflow-providers-smtp==1.8.1
apache-airflow-providers-snowflake==5.8.1
apache-airflow-providers-sqlite==3.9.1
apache-airflow-providers-ssh==3.14.0
```

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Airflow 2.10.4 on EKS 1.30, using official helm chart version 1.13.1.

Datadog version - `datadog/agent:7.56.2`

Airflow metrics configuration:
```
[metrics]
metrics_use_pattern_match = False
metrics_allow_list =
metrics_block_list =
statsd_on = true
statsd_host = 10.128.31.107
statsd_port = 8125
statsd_prefix = airflow
stat_name_handler =
statsd_datadog_enabled = True
statsd_datadog_tags = project:dbt
statsd_datadog_metrics_tags = True
statsd_disabled_tags = job_id,run_id
statsd_influxdb_enabled = False
```

### Anything else?

Thank you, and please let me know if there are any additional details I can provide to help triage or reproduce this issue.


### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",noamst-monday,2024-12-30 10:19:39+00:00,[],2025-01-10 18:21:42+00:00,,https://github.com/apache/airflow/issues/45285,"[('kind:bug', 'This is a clearly a bug'), ('area:metrics', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2565285368, 'issue_id': 2762980983, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 30, 10, 19, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2581186669, 'issue_id': 2762980983, 'author': 'ferruzzi', 'body': ""I'm not really familiar with datadog, but what are you using to view the emitted metric?  Is it possible to filter the metric by those tags?  I suspect that the 3.26k you are seeing is the aggregate and you should be able to drill down into that using the tags?\r\n\r\nDo you see other tagged timers working the way you expect them to?"", 'created_at': datetime.datetime(2025, 1, 9, 20, 27, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2581892179, 'issue_id': 2762980983, 'author': 'noamst-monday', 'body': 'Hey, thanks for replying\r\nOther timers are working well, and I am able to filter them by tags.\r\nFor example, here is a graph showing scheduled_duration, and queued_duration.\r\nScheduled_duration is broken down by task_id, and queued_duration is without breakdown ( due to the large number of different tasks/dags)\r\n<img width=""1648"" alt=""image"" src=""https://github.com/user-attachments/assets/d4824e8d-6611-4894-918c-4373061fadad"" />\r\n\r\nYou can see there\'s only a single data point for scheduled_duration, while queued_duration is graphed continuously.\r\nAlso, the data is available for just a single task and dag.', 'created_at': datetime.datetime(2025, 1, 10, 6, 34, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2583478503, 'issue_id': 2762980983, 'author': 'ferruzzi', 'body': 'hm.     Both of those are defined and emitted together, I think ([here](https://github.com/apache/airflow/blob/f3fd262de274b4fd1e26d36f9cead0a56775a309/airflow/models/taskinstance.py#L2644)) so it is odd that one would work but not the other.', 'created_at': datetime.datetime(2025, 1, 10, 18, 21, 41, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-30 10:19:42 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

ferruzzi on (2025-01-09 20:27:17 UTC): I'm not really familiar with datadog, but what are you using to view the emitted metric?  Is it possible to filter the metric by those tags?  I suspect that the 3.26k you are seeing is the aggregate and you should be able to drill down into that using the tags?

Do you see other tagged timers working the way you expect them to?

noamst-monday (Issue Creator) on (2025-01-10 06:34:58 UTC): Hey, thanks for replying
Other timers are working well, and I am able to filter them by tags.
For example, here is a graph showing scheduled_duration, and queued_duration.
Scheduled_duration is broken down by task_id, and queued_duration is without breakdown ( due to the large number of different tasks/dags)
<img width=""1648"" alt=""image"" src=""https://github.com/user-attachments/assets/d4824e8d-6611-4894-918c-4373061fadad"" />

You can see there's only a single data point for scheduled_duration, while queued_duration is graphed continuously.
Also, the data is available for just a single task and dag.

ferruzzi on (2025-01-10 18:21:41 UTC): hm.     Both of those are defined and emitted together, I think ([here](https://github.com/apache/airflow/blob/f3fd262de274b4fd1e26d36f9cead0a56775a309/airflow/models/taskinstance.py#L2644)) so it is odd that one would work but not the other.

"
2762647011,issue,open,,Inconsistent DAG Import Errors Display in Airflow UI,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When refreshing the Airflow UI, the displayed count of DAG import errors is inconsistent. The following issues are observed:

1. The number of DAG import errors changes with each refresh. For example, errors may fluctuate between 13, 14, and 17.
2. Some old import errors disappear after a refresh but reappear during subsequent refreshes.

Impact:
This inconsistency makes it difficult to accurately identify and address DAG import issues, leading to potential confusion and inefficiencies in debugging workflows.

### What you think should happen instead?

<img width=""774"" alt=""Screenshot 2024-12-30 at 10 18 12 AM"" src=""https://github.com/user-attachments/assets/0320c487-17c5-4cca-8471-1ae1e2fce187"" />
<img width=""795"" alt=""Screenshot 2024-12-30 at 10 18 44 AM"" src=""https://github.com/user-attachments/assets/68deaf50-85a7-4b96-988a-e36622d7ffe9"" />

Expected Behavior:
The count and details of DAG import errors should remain consistent across UI refreshes until the underlying errors are resolved.



### How to reproduce

Steps to Reproduce:

1. Open the Airflow UI.
2. Note the number of DAG import errors displayed.
3. Refresh the UI multiple times and observe the changes in the count of errors.
4. Compare the errors to verify if previously resolved issues reappear.

### Operating System

NAME=""Red Hat Enterprise Linux"" VERSION=""9.4 (Plow)"" ID=""rhel"" ID_LIKE=""fedora"" VERSION_ID=""9.4"" PLATFORM_ID=""platform:el9"" PRETTY_NAME=""Red Hat Enterprise Linux 9.4 (Plow)"" ANSI_COLOR=""0;31"" LOGO=""fedora-logo-icon"" CPE_NAME=""cpe:/o:redhat:enterprise_linux:9::baseos"" HOME_URL=""https://www.redhat.com/"" DOCUMENTATION_URL=""https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9"" BUG_REPORT_URL=""https://issues.redhat.com/""  REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 9"" REDHAT_BUGZILLA_PRODUCT_VERSION=9.4 REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"" REDHAT_SUPPORT_PRODUCT_VERSION=""9.4""

### Versions of Apache Airflow Providers

2.9.2

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",swapnilpatil0905,2024-12-30 04:53:50+00:00,[],2024-12-30 07:45:13+00:00,,https://github.com/apache/airflow/issues/45276,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2565025086, 'issue_id': 2762647011, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 30, 4, 53, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2565117784, 'issue_id': 2762647011, 'author': 'ephraimbuddy', 'body': ""DAGs are constantly being parsed, and if some are due to import timeouts during parsing, then those DAGs could pass at times and fail at other times. I think that's what you are experiencing, and it's not a bug"", 'created_at': datetime.datetime(2024, 12, 30, 7, 28, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2565125753, 'issue_id': 2762647011, 'author': 'tirkarthi', 'body': 'Are the dag files in the dags folder python files or zip files?', 'created_at': datetime.datetime(2024, 12, 30, 7, 38, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2565128774, 'issue_id': 2762647011, 'author': 'swapnilpatil0905', 'body': '@tirkarthi Few are in dags folder whereas few are zipped. Note: We are running on multipod webserver mode.\r\n<img width=""1453"" alt=""Screenshot 2024-12-30 at 1 03 47\u202fPM"" src=""https://github.com/user-attachments/assets/975c3285-25ac-42d7-8638-6c510a94d3ca"" />\r\n<img width=""1463"" alt=""Screenshot 2024-12-30 at 1 05 26\u202fPM"" src=""https://github.com/user-attachments/assets/2a1eacb6-7290-44ce-aa53-08bd052fd4a4"" />\r\n<img width=""1458"" alt=""Screenshot 2024-12-30 at 1 04 39\u202fPM"" src=""https://github.com/user-attachments/assets/6f584472-6701-473b-9f29-4777c54f7b57"" />', 'created_at': datetime.datetime(2024, 12, 30, 7, 42, 46, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-30 04:53:53 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

ephraimbuddy on (2024-12-30 07:28:01 UTC): DAGs are constantly being parsed, and if some are due to import timeouts during parsing, then those DAGs could pass at times and fail at other times. I think that's what you are experiencing, and it's not a bug

tirkarthi on (2024-12-30 07:38:50 UTC): Are the dag files in the dags folder python files or zip files?

swapnilpatil0905 (Issue Creator) on (2024-12-30 07:42:46 UTC): @tirkarthi Few are in dags folder whereas few are zipped. Note: We are running on multipod webserver mode.
<img width=""1453"" alt=""Screenshot 2024-12-30 at 1 03 47 PM"" src=""https://github.com/user-attachments/assets/975c3285-25ac-42d7-8638-6c510a94d3ca"" />
<img width=""1463"" alt=""Screenshot 2024-12-30 at 1 05 26 PM"" src=""https://github.com/user-attachments/assets/2a1eacb6-7290-44ce-aa53-08bd052fd4a4"" />
<img width=""1458"" alt=""Screenshot 2024-12-30 at 1 04 39 PM"" src=""https://github.com/user-attachments/assets/6f584472-6701-473b-9f29-4777c54f7b57"" />

"
2762590169,issue,closed,not_planned,Catchup=FALSE but it still run for missing Task?,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.2

### What happened?

Hi All,

Why does my DAG still behave like catchup=True and run for the missing DAG run when I change the schedule interval? I already set it to False, but why is it still running for the missing DAG run?

DAG code:
```python
with DAG(
    dag_id=dag_id,
    schedule=schedule_value,
    start_date=datetime(2024, 12, 1),
    catchup=False,
    tags=[""xxx""],
    default_args=default_args,
) as dag:
    schemas = variable_mapping[""data""]

    with TaskGroup(
        group_id=""compare"",
        tooltip=""vs"",
    ) as column_compare_tg:
        for schema in list(schemas.keys()):
            create_task(
                env_vars, job_name=job_name, redshift_schema=schema, _type=""compare""
            )

    send_alert = create_task(env_vars, job_name=job_name, _type=""alert"")

    column_compare_tg >> send_alert
```

### What you think should happen instead?

When I changed the schedule interval, the DAG should skip the missing run and run only one task.

### How to reproduce

1. Create a DAG with `schedule_value` as a Airflow variable
2. First time: set the schedule as 2 time a day
3. Second time: set the schedule as 5 time a day

### Operating System

Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.28.0
apache-airflow-providers-celery==3.8.1
apache-airflow-providers-cncf-kubernetes==8.4.1
apache-airflow-providers-common-compat==1.2.0
apache-airflow-providers-common-io==1.4.0
apache-airflow-providers-common-sql==1.16.0
apache-airflow-providers-docker==3.13.0
apache-airflow-providers-elasticsearch==5.5.0
apache-airflow-providers-fab==1.3.0
apache-airflow-providers-ftp==3.11.0
apache-airflow-providers-google==10.22.0
apache-airflow-providers-grpc==3.6.0
apache-airflow-providers-hashicorp==3.8.0
apache-airflow-providers-http==4.13.0
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-microsoft-azure==10.4.0
apache-airflow-providers-mysql==5.7.0
apache-airflow-providers-odbc==4.7.0
apache-airflow-providers-openlineage==1.11.0
apache-airflow-providers-postgres==5.12.0
apache-airflow-providers-redis==3.8.0
apache-airflow-providers-sendgrid==3.6.0
apache-airflow-providers-sftp==4.11.0
apache-airflow-providers-slack==8.9.0
apache-airflow-providers-smtp==1.8.0
apache-airflow-providers-snowflake==5.7.0
apache-airflow-providers-sqlite==3.9.0
apache-airflow-providers-ssh==3.13.1

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

helm, on AWS EKS

### Anything else?

No.

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",amzar96,2024-12-30 03:14:32+00:00,[],2025-02-06 00:15:21+00:00,2025-02-06 00:15:20+00:00,https://github.com/apache/airflow/issues/45274,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('pending-response', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2564982817, 'issue_id': 2762590169, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 30, 3, 14, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2571635233, 'issue_id': 2762590169, 'author': 'eladkal', 'body': ""catchup is considered against latest dagrun. It does not affect existed runs.\r\nIt's not clear from this report what exactly is the bug. If you believe there is a bug please provide a more clear explanation of what happened and what you expect to happen including exact dates and screenshots to support what you describe."", 'created_at': datetime.datetime(2025, 1, 5, 13, 55, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2617240433, 'issue_id': 2762590169, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2025, 1, 28, 0, 15, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2638320866, 'issue_id': 2762590169, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2025, 2, 6, 0, 15, 19, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-30 03:14:36 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2025-01-05 13:55:30 UTC): catchup is considered against latest dagrun. It does not affect existed runs.
It's not clear from this report what exactly is the bug. If you believe there is a bug please provide a more clear explanation of what happened and what you expect to happen including exact dates and screenshots to support what you describe.

github-actions[bot] on (2025-01-28 00:15:12 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2025-02-06 00:15:19 UTC): This issue has been closed because it has not received response from the issue author.

"
2762369217,issue,closed,completed,Update Cohere API from v1 to v2,"### Description

We need to update the Cohere provider in Apache Airflow from using the v1 API to the recently released v2 API. This update will ensure compatibility with the latest features and improvements made by Cohere.

### Use case/motivation

- **Compatibility**: Ensure that Airflow users can leverage new features introduced in Cohere v2.
- **Performance**: Potentially benefit from performance enhancements in v2.
- **Security**: Upgrade to more secure API endpoints if any security issues were addressed in the new version.
- **Feature Usage**: Utilize any new functionalities like better document structuring or tool calls for more efficient workflow management.


### Related issues

#45267 focuses on updating the Cohere provider in Apache Airflow to use the Cohere API v2.

Related issues
#38465
#38349


### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",okirialbert,2024-12-29 18:59:46+00:00,[],2024-12-29 20:44:03+00:00,2024-12-29 20:44:03+00:00,https://github.com/apache/airflow/issues/45271,"[('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:cohere', '')]","[{'comment_id': 2564813340, 'issue_id': 2762369217, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 29, 18, 59, 50, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-29 18:59:50 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2762283039,issue,closed,completed,"Add ""load"" command flags to restore CI image from Job ID / PR ID","With https://github.com/apache/airflow/pull/45261 we switched to storing ""per-job"" image dumps as artifacts in the jobs from GitHub registry - this also means that we cannot use Commit SHA TAG to pull the image from registry when we want to reproduce a CI job environment.

For now we can do it manually - by downloading the artifact and using `breeze ci-image load --image-file <file>`  - but this can be all easily automated.

The `load` command could have optional `--from-job <JOB>` and `--from-pr PR` flags that could use GiHub API to find and download the right artifact before loading. For JOB - it would take it from the JOB, from PR it would load it from the latest JOB for the PR. 

We would also add diagnostic to error out if the image is not with the right platform (for now only `linux/amd` is supported) and in the future when we add ARM support via ARC (https://github.com/apache/airflow/issues/44512) - we should be able to also produce and store ARM image as artifact, and in this case we would just find and use the right one depending on the platform we are in.",potiuk,2024-12-29 14:52:02+00:00,['gopidesupavan'],2024-12-30 13:29:41+00:00,2024-12-30 13:29:41+00:00,https://github.com/apache/airflow/issues/45269,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:dev-tools', '')]",[],
2762280400,issue,open,,"Update ASF ""stash"" action to allow list of keys","In the new ""caching"" mechanism we are using ""stash"" GitHub action from @assignUser which only allows one key per action.

The action is in https://github.com/apache/infrastructure-actions/tree/main/stash

This is a a limiting factor when we want to download artifacts with a dynamic list of keys coming from a variable (say list of keys derived python versions - ""3.9 3.10 3.11 3.12"" -> ""key-3.9 key-3.10 key-3.11 key-3.12"".

Currently in https://github.com/apache/airflow/pull/45266  (https://gtithub.com/apache/airflow/pull/45266/files#diff-a2dbc854d7d9f769ffe6efe98247baa68750cba87d785fb70d4616331010c7d0) we are hard-coding the possible actions to run with parameters and we are skipping those that are not on the list, but this is more of a hack than target solution.

Ideally we should be able to pass `keys` parameter to the action instead of `key` and be able to produce ""hit-keys"" list as output  - then we could generate and pass the list of keys as input.",potiuk,2024-12-29 14:44:48+00:00,[],2024-12-30 08:11:14+00:00,,https://github.com/apache/airflow/issues/45268,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:dev-tools', '')]","[{'comment_id': 2565017876, 'issue_id': 2762280400, 'author': 'assignUser', 'body': ""So you would want the action to be able to download multiple 'stash' artifacts and unpack them into a single dir? I can see how this would be useful to simplify this: https://github.com/apache/airflow/pull/45266/files#diff-a2dbc854d7d9f769ffe6efe98247baa68750cba87d785fb70d4616331010c7d0R43\r\n\r\nThe main motivation for the action was caching compiler caches for which this doesn't really make sense but when used as a more general `actions/cache` replacement this makes sense."", 'created_at': datetime.datetime(2024, 12, 30, 4, 36, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2565151774, 'issue_id': 2762280400, 'author': 'potiuk', 'body': ""Yeah. That's the main motivation - to avoid hard-coding of the conditional cache retrieval :) .. We might have either me or someone from our team to contribute that."", 'created_at': datetime.datetime(2024, 12, 30, 8, 11, 14, tzinfo=datetime.timezone.utc)}]","assignUser on (2024-12-30 04:36:56 UTC): So you would want the action to be able to download multiple 'stash' artifacts and unpack them into a single dir? I can see how this would be useful to simplify this: https://github.com/apache/airflow/pull/45266/files#diff-a2dbc854d7d9f769ffe6efe98247baa68750cba87d785fb70d4616331010c7d0R43

The main motivation for the action was caching compiler caches for which this doesn't really make sense but when used as a more general `actions/cache` replacement this makes sense.

potiuk (Issue Creator) on (2024-12-30 08:11:14 UTC): Yeah. That's the main motivation - to avoid hard-coding of the conditional cache retrieval :) .. We might have either me or someone from our team to contribute that.

"
2760778664,issue,closed,completed,Support pulling multiple XCom values,"Currently, we only support pulling single XCom value from `ti.xcom_pull` in the Task SDK.

Also port tests from 

https://github.com/apache/airflow/blob/9178f8f0b1ffd80b8eacf4d02b732528fde218e5/tests/models/test_taskinstance.py#L1614-L1815",kaxil,2024-12-27 11:40:29+00:00,['amoghrajesh'],2025-01-10 12:57:36+00:00,2025-01-10 12:57:36+00:00,https://github.com/apache/airflow/issues/45243,"[('kind:feature', 'Feature Requests'), ('area:core', '')]","[{'comment_id': 2572794199, 'issue_id': 2760778664, 'author': 'kaxil', 'body': 'Or we could simplify this on the Server side and loop through on the client side.', 'created_at': datetime.datetime(2025, 1, 6, 10, 14, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2579829020, 'issue_id': 2760778664, 'author': 'kaxil', 'body': ""@amoghrajesh While we wait on the discussion regarding Variable, if you'd like you can pick this up too before you go on leave. \n\nIf you recall our last discussion, we can do this entirely on the client side by looping through multiple task_ids via the API call."", 'created_at': datetime.datetime(2025, 1, 9, 10, 53, 53, tzinfo=datetime.timezone.utc)}]","kaxil (Issue Creator) on (2025-01-06 10:14:50 UTC): Or we could simplify this on the Server side and loop through on the client side.

kaxil (Issue Creator) on (2025-01-09 10:53:53 UTC): @amoghrajesh While we wait on the discussion regarding Variable, if you'd like you can pick this up too before you go on leave. 

If you recall our last discussion, we can do this entirely on the client side by looping through multiple task_ids via the API call.

"
2760730413,issue,open,,Databricks Extra Links See Job Run does not work with custom S3 backend,"### Apache Airflow Provider(s)

databricks

### Versions of Apache Airflow Providers

6.0.0

### Apache Airflow version

2.8.1

### Operating System

Amazon

### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### What happened

When I click on the link to see job run details, it directs to https://(s3 path of my xcom json file), instead of directing to databricks workspace

### What you think should happen instead

I expect the link to work and directs to job run details in our team's databricks workspace

### How to reproduce

1. Create and use a custom XCOM backend that writes to S3
2. Run a databricks job
3. Click the link to go to the job run details
4. Airflow 404

### Anything else

Currently our custom XCOM backend, stores everything in S3, it does not store large values only as other XCOM, yet the behavior of the extra link should not depend on that:
I had a look at https://airflow.apache.org/docs/apache-airflow-providers-databricks/6.0.0/_modules/airflow/providers/databricks/operators/databricks.html#DatabricksJobRunLink.get_link and https://github.com/apache/airflow/blob/2.8.1/airflow/models/xcom.py#L873-L876 yet I doubt that TYPE_CHECKING is true during runtime, because if it is, then the provider won't use our custom class.
I also tried to override the orm_deserialize_value method hoping this will fix the issue, yet I ran into the issue I mentioned in this comment https://github.com/apache/airflow/discussions/44232#discussioncomment-11636619 (contributions to this discussion are very welcome as well)


### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",mamdouhtawfik,2024-12-27 10:51:50+00:00,['Prab-27'],2025-01-02 09:29:33+00:00,,https://github.com/apache/airflow/issues/45240,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('provider:databricks', '')]","[{'comment_id': 2563576160, 'issue_id': 2760730413, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 27, 10, 51, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563752115, 'issue_id': 2760730413, 'author': 'Prab-27', 'body': ""@potiuk ,I'd like to work on this -"", 'created_at': datetime.datetime(2024, 12, 27, 14, 26, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567056335, 'issue_id': 2760730413, 'author': 'mohamedmeqlad99', 'body': 'I have submitted a PR to address this issue: [PR ##45328](https://github.com/apache/airflow/pull/45328). \r\nThe PR resolves the problem where the ""See Job Run"" link does not work with a custom S3 XCom backend. Feedback is welcome!', 'created_at': datetime.datetime(2025, 1, 1, 15, 46, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567485697, 'issue_id': 2760730413, 'author': 'mamdouhtawfik', 'body': 'I had a deeper look into the issue and I think the issue is related to a ""bug"" in the xcom.py file, it was resolved in this PR https://github.com/apache/airflow/pull/37058/files#diff-3f95f161bd9ef4bb455611e0c58583899769360afc53f755cd1577cf194553c5R423, calling the XCom variable to deserialize the value instead of BaseXCom class. I am not sure if there is an easy way to address this within airflow 2.8.1\r\n\r\n@mohamedmeqlad99 thanks for spending time to try to support with this, yet I am not sure if your PR is addressing the issue here and generally I don\'t think we should go in an S3 specific path but rather use the deserialization of the customer xcom backend (which is what was ""fixed"" in the PR I shared)', 'created_at': datetime.datetime(2025, 1, 2, 9, 29, 32, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-27 10:51:52 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

Prab-27 (Assginee) on (2024-12-27 14:26:38 UTC): @potiuk ,I'd like to work on this -

mohamedmeqlad99 on (2025-01-01 15:46:26 UTC): I have submitted a PR to address this issue: [PR ##45328](https://github.com/apache/airflow/pull/45328). 
The PR resolves the problem where the ""See Job Run"" link does not work with a custom S3 XCom backend. Feedback is welcome!

mamdouhtawfik (Issue Creator) on (2025-01-02 09:29:32 UTC): I had a deeper look into the issue and I think the issue is related to a ""bug"" in the xcom.py file, it was resolved in this PR https://github.com/apache/airflow/pull/37058/files#diff-3f95f161bd9ef4bb455611e0c58583899769360afc53f755cd1577cf194553c5R423, calling the XCom variable to deserialize the value instead of BaseXCom class. I am not sure if there is an easy way to address this within airflow 2.8.1

@mohamedmeqlad99 thanks for spending time to try to support with this, yet I am not sure if your PR is addressing the issue here and generally I don't think we should go in an S3 specific path but rather use the deserialization of the customer xcom backend (which is what was ""fixed"" in the PR I shared)

"
2760667016,issue,closed,completed,What's the purpose of the response_check parameter in HttpOperator?,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I wanted to raise an AirflowSkipException if a HTTP 404 is returned by a REST endpoint if the requested resource isn't found

### What you think should happen instead?

I would have expected the custom response_check code being invoked, but it isn't, as the response status_code is already checked within the check_response method of the HttpHook.  There, when the status_code isn't within the 2x or 3x range, an AirflowException is being raised, thus making the possibility to check the response yourself through a custom response_check obsolete, as the code will never be invoked.  A possible solution would be to pass the custom response_check defined int the operator to the HttpHook, that way the same logic would be applied.

### How to reproduce

Just define a custom response_check which ignores a 404 and run the HttpOperator for a non existing resource which will return a 404, you'll see that the custom response_check will never be invoked and the task will fail.

### Operating System

RedHat

### Versions of Apache Airflow Providers

apache-airflow-providers-http 5.0.0

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",dabla,2024-12-27 09:56:47+00:00,[],2025-01-09 07:02:05+00:00,2025-01-09 07:02:05+00:00,https://github.com/apache/airflow/issues/45237,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:http', '')]","[{'comment_id': 2564413815, 'issue_id': 2760667016, 'author': 'raphaelauv', 'body': ""set \r\n\r\n`extra_options = {'check_response': False}`"", 'created_at': datetime.datetime(2024, 12, 28, 19, 32, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564451942, 'issue_id': 2760667016, 'author': 'dabla', 'body': ""> set\r\n> \r\n> `extra_options = {'check_response': False}`\r\n\r\nThanks didn’t know that.  Wouldn’t it be more logical that when you pass a  custom_response check the hook doesn’t chech the response? Now this is confusing imho"", 'created_at': datetime.datetime(2024, 12, 28, 20, 51, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564745039, 'issue_id': 2760667016, 'author': 'dabla', 'body': ""> {'check_response': False}\r\n\r\nJust tested it and it fails with following error as it considers the check_response passed in the extra_options as a HTTP header:\r\n\r\n`requests.exceptions.InvalidHeader: Header part (False) from ('check_response', False) must be of type str or bytes, not <class 'bool'>`\r\n\r\nWill do a PR to fix this."", 'created_at': datetime.datetime(2024, 12, 29, 14, 39, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564824430, 'issue_id': 2760667016, 'author': 'topherinternational', 'body': ""Just dropping some thoughts - \r\n\r\nI agree with the OP that this is unclear and confusing. \r\n\r\nIt looks like `response_check` is designed to be a callback that examines the text/bytes from a successful (2xx/3xx) response, and under normal use, the hook will raise an exception when a 4xx/5xx status is received from the server. This has an unfortunate name collision with `HttpHook`'s `check_response`, which passes through to the `Response` object's `raise_for_status` method and then massages a raised exception for 4xx/5xx responses (checking only status code and not the response content). \r\n\r\nThere's a lot of leaky API between the `HttpOperator` which wraps the `HttpHook` which wraps the `requests` library, and  the operator user/DAG writer has to be aware of all of them to really understand how the operator is going to behave. (And keep aware if those wrapped components change behavior.)\r\n\r\nOne simple way around this particular issue might be to have the HttpOperator take an optional argument like `raise_for_status` whose behavior is to pass `{'check_response': False}` to the `HttpHook`, and document the parameter and the `response_check` parameter better so it's clear how to use them if you want to do what the OP does - interrogate the response code yourself.\r\n\r\nA more fluent alternative might be to have the Operator take a `check_status` parameter that is a callback function to examine the response code, and if it is set, the operator code will (a) set the Hook to not do its own status check, and (b) get called after the hook is run and before any `response_check` function is called. That way `response_check` can preserve its semantics of only getting called for a successful response, and DAG writer can either use the hook's status check or substitute their own function (but never both)."", 'created_at': datetime.datetime(2024, 12, 29, 19, 53, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564829415, 'issue_id': 2760667016, 'author': 'dabla', 'body': ""> Just dropping some thoughts -\r\n> \r\n> I agree with the OP that this is unclear and confusing.\r\n> \r\n> It looks like `response_check` is designed to be a callback that examines the text/bytes from a successful (2xx/3xx) response, and under normal use, the hook will raise an exception when a 4xx/5xx status is received from the server. This has an unfortunate name collision with `HttpHook`'s `check_response`, which passes through to the `Response` object's `raise_for_status` method and then massages a raised exception for 4xx/5xx responses (checking only status code and not the response content).\r\n> \r\n> There's a lot of leaky API between the `HttpOperator` which wraps the `HttpHook` which wraps the `requests` library, and the operator user/DAG writer has to be aware of all of them to really understand how the operator is going to behave. (And keep aware if those wrapped components change behavior.)\r\n> \r\n> One simple way around this particular issue might be to have the HttpOperator take an optional argument like `raise_for_status` whose behavior is to pass `{'check_response': False}` to the `HttpHook`, and document the parameter and the `response_check` parameter better so it's clear how to use them if you want to do what the OP does - interrogate the response code yourself.\r\n> \r\n> A more fluent alternative might be to have the Operator take a `check_status` parameter that is a callback function to examine the response code, and if it is set, the operator code will (a) set the Hook to not do its own status check, and (b) get called after the hook is run and before any `response_check` function is called. That way `response_check` can preserve its semantics of only getting called for a successful response, and DAG writer can either use the hook's status check or substitute their own function (but never both).\r\n\r\nCompletely agree with this detailed elaboration.\r\n\r\nI would suggest that when a custom check_response callback is specified within the operator, it would then in turn be passed through the HttpHook run method so that it doesn’t use the default check_status.  Also specifying the check_response in the extra options didn’t work and only makes the useage overly complicated for nothing."", 'created_at': datetime.datetime(2024, 12, 29, 20, 17, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564834090, 'issue_id': 2760667016, 'author': 'topherinternational', 'body': ""> > {'check_response': False}\r\n> \r\n> Just tested it and it fails with following error as it considers the check_response passed in the extra_options as a HTTP header:\r\n> \r\n> `requests.exceptions.InvalidHeader: Header part (False) from ('check_response', False) must be of type str or bytes, not <class 'bool'>`\r\n\r\nWhat exactly was your code here? Did you put the `check_response` pair in the Operator constructor's `extra_options`? Or in the Connection object?\r\n\r\nEdit: I ask bc I played with the tests and the only way I could get that InvalidHeader error was to pass the `check_response` param in the operator's headers arg instead of the extra_options, or to put check_response=false in the Connection's extra table."", 'created_at': datetime.datetime(2024, 12, 29, 20, 42, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2572444069, 'issue_id': 2760667016, 'author': 'dabla', 'body': ""> > > {'check_response': False}\r\n> > \r\n> > \r\n> > Just tested it and it fails with following error as it considers the check_response passed in the extra_options as a HTTP header:\r\n> > `requests.exceptions.InvalidHeader: Header part (False) from ('check_response', False) must be of type str or bytes, not <class 'bool'>`\r\n> \r\n> What exactly was your code here? Did you put the `check_response` pair in the Operator constructor's `extra_options`? Or in the Connection object?\r\n> \r\n> Edit: I ask bc I played with the tests and the only way I could get that InvalidHeader error was to pass the `check_response` param in the operator's headers arg instead of the extra_options, or to put check_response=false in the Connection's extra table.\r\n\r\nI've defined those in the extra_options of the connection, tested on the latest Airflow version."", 'created_at': datetime.datetime(2025, 1, 6, 7, 16, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2572505568, 'issue_id': 2760667016, 'author': 'dabla', 'body': 'The issue is in this method, it pops all non related extra_options parameters and keeps the rest to pass it as headers, an extra pop should be needed on the check_response key to avoid the issue, I\'ll open a PR to fix that:\r\n\r\n```\r\n    def _configure_session_from_extra(\r\n            self, session: requests.Session, connection: Connection\r\n        ) -> requests.Session:\r\n            extra = connection.extra_dejson\r\n            extra.pop(""timeout"", None)\r\n            extra.pop(""allow_redirects"", None)\r\n            extra.pop(""check_response"", None)  # this should be added\r\n            session.proxies = extra.pop(""proxies"", extra.pop(""proxy"", {}))\r\n            session.stream = extra.pop(""stream"", False)\r\n            session.verify = extra.pop(""verify"", extra.pop(""verify_ssl"", True))\r\n            session.cert = extra.pop(""cert"", None)\r\n            session.max_redirects = extra.pop(""max_redirects"", DEFAULT_REDIRECT_LIMIT)\r\n            session.trust_env = extra.pop(""trust_env"", True)\r\n            try:\r\n                session.headers.update(extra)\r\n            except TypeError:\r\n                self.log.warning(""Connection to %s has invalid extra field."", connection.host)\r\n            return session\r\n```', 'created_at': datetime.datetime(2025, 1, 6, 7, 59, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2573021373, 'issue_id': 2760667016, 'author': 'topherinternational', 'body': ""> > > > {'check_response': False}\n> \n> > > \n> \n> > > \n> \n> > > Just tested it and it fails with following error as it considers the check_response passed in the extra_options as a HTTP header:\n> \n> > > `requests.exceptions.InvalidHeader: Header part (False) from ('check_response', False) must be of type str or bytes, not <class 'bool'>`\n> \n> > \n> \n> > What exactly was your code here? Did you put the `check_response` pair in the Operator constructor's `extra_options`? Or in the Connection object?\n> \n> > \n> \n> > Edit: I ask bc I played with the tests and the only way I could get that InvalidHeader error was to pass the `check_response` param in the operator's headers arg instead of the extra_options, or to put check_response=false in the Connection's extra table.\n> \n> \n> \n> I've defined those in the extra_options of the connection, tested on the latest Airflow version.\n\nAh, I think what the other commentor meant was to pass `extra_options` to the `HttpOperator`, not the Connection. When I pass it to the operator the setting is passed to the hook as expected and the `response_check` function is called.\n\nThis should solve your immediate problem, checking status and response content with your callback instead of the hook raising a preemptive exception."", 'created_at': datetime.datetime(2025, 1, 6, 12, 33, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2573054118, 'issue_id': 2760667016, 'author': 'topherinternational', 'body': '> The issue is in this method, it pops all non related extra_options parameters and keeps the rest to pass it as headers, an extra pop should be needed on the check_response key to avoid the issue, I\'ll open a PR to fix that:\n> \n> \n> \n> ```\n> \n>     def _configure_session_from_extra(\n> \n>             self, session: requests.Session, connection: Connection\n> \n>         ) -> requests.Session:\n> \n>             extra = connection.extra_dejson\n> \n>             extra.pop(""timeout"", None)\n> \n>             extra.pop(""allow_redirects"", None)\n> \n>             extra.pop(""check_response"", None)  # this should be added\n> \n>             session.proxies = extra.pop(""proxies"", extra.pop(""proxy"", {}))\n> \n>             session.stream = extra.pop(""stream"", False)\n> \n>             session.verify = extra.pop(""verify"", extra.pop(""verify_ssl"", True))\n> \n>             session.cert = extra.pop(""cert"", None)\n> \n>             session.max_redirects = extra.pop(""max_redirects"", DEFAULT_REDIRECT_LIMIT)\n> \n>             session.trust_env = extra.pop(""trust_env"", True)\n> \n>             try:\n> \n>                 session.headers.update(extra)\n> \n>             except TypeError:\n> \n>                 self.log.warning(""Connection to %s has invalid extra field."", connection.host)\n> \n>             return session\n> \n> ```\n\nThis is the call of someone in the core contributor club, so I\'m just giving my opinion - \n\nThis does look like a bug in the HTTP hook, but it seems like a separate issue to this issue. I see the two breaking down like this:\n- When I use the `HttpOperator`, I want to tell it to skip any status checking and let me check status through the existing `response_check` callback argument\n- When I declare a connection to use with the HttpHook, I want to specify that the hook should not do a status check on any call made with this connection\n\nUsing the hook-connection contract to configure the control flow of the operator perpetuates the API leaking problem that spawned this issue in the first place. \n\nIt\'s a separate issue/question whether the hook should check it status based on an option in the connection object, or if that behavior should always be configured by the caller (args to the constructor or run()). I lean towards yes but that can be discussed in a new issue filing.', 'created_at': datetime.datetime(2025, 1, 6, 12, 54, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2574923281, 'issue_id': 2760667016, 'author': 'dabla', 'body': ""I've created a [PR ](https://github.com/apache/airflow/pull/45451) which solves the issue of the check_response being interpreted as a HTTP header when defined under the extras of the HTTP connection."", 'created_at': datetime.datetime(2025, 1, 7, 10, 24, 13, tzinfo=datetime.timezone.utc)}]","raphaelauv on (2024-12-28 19:32:06 UTC): set 

`extra_options = {'check_response': False}`

dabla (Issue Creator) on (2024-12-28 20:51:48 UTC): Thanks didn’t know that.  Wouldn’t it be more logical that when you pass a  custom_response check the hook doesn’t chech the response? Now this is confusing imho

dabla (Issue Creator) on (2024-12-29 14:39:28 UTC): Just tested it and it fails with following error as it considers the check_response passed in the extra_options as a HTTP header:

`requests.exceptions.InvalidHeader: Header part (False) from ('check_response', False) must be of type str or bytes, not <class 'bool'>`

Will do a PR to fix this.

topherinternational on (2024-12-29 19:53:58 UTC): Just dropping some thoughts - 

I agree with the OP that this is unclear and confusing. 

It looks like `response_check` is designed to be a callback that examines the text/bytes from a successful (2xx/3xx) response, and under normal use, the hook will raise an exception when a 4xx/5xx status is received from the server. This has an unfortunate name collision with `HttpHook`'s `check_response`, which passes through to the `Response` object's `raise_for_status` method and then massages a raised exception for 4xx/5xx responses (checking only status code and not the response content). 

There's a lot of leaky API between the `HttpOperator` which wraps the `HttpHook` which wraps the `requests` library, and  the operator user/DAG writer has to be aware of all of them to really understand how the operator is going to behave. (And keep aware if those wrapped components change behavior.)

One simple way around this particular issue might be to have the HttpOperator take an optional argument like `raise_for_status` whose behavior is to pass `{'check_response': False}` to the `HttpHook`, and document the parameter and the `response_check` parameter better so it's clear how to use them if you want to do what the OP does - interrogate the response code yourself.

A more fluent alternative might be to have the Operator take a `check_status` parameter that is a callback function to examine the response code, and if it is set, the operator code will (a) set the Hook to not do its own status check, and (b) get called after the hook is run and before any `response_check` function is called. That way `response_check` can preserve its semantics of only getting called for a successful response, and DAG writer can either use the hook's status check or substitute their own function (but never both).

dabla (Issue Creator) on (2024-12-29 20:17:56 UTC): Completely agree with this detailed elaboration.

I would suggest that when a custom check_response callback is specified within the operator, it would then in turn be passed through the HttpHook run method so that it doesn’t use the default check_status.  Also specifying the check_response in the extra options didn’t work and only makes the useage overly complicated for nothing.

topherinternational on (2024-12-29 20:42:38 UTC): What exactly was your code here? Did you put the `check_response` pair in the Operator constructor's `extra_options`? Or in the Connection object?

Edit: I ask bc I played with the tests and the only way I could get that InvalidHeader error was to pass the `check_response` param in the operator's headers arg instead of the extra_options, or to put check_response=false in the Connection's extra table.

dabla (Issue Creator) on (2025-01-06 07:16:25 UTC): I've defined those in the extra_options of the connection, tested on the latest Airflow version.

dabla (Issue Creator) on (2025-01-06 07:59:56 UTC): The issue is in this method, it pops all non related extra_options parameters and keeps the rest to pass it as headers, an extra pop should be needed on the check_response key to avoid the issue, I'll open a PR to fix that:

```
    def _configure_session_from_extra(
            self, session: requests.Session, connection: Connection
        ) -> requests.Session:
            extra = connection.extra_dejson
            extra.pop(""timeout"", None)
            extra.pop(""allow_redirects"", None)
            extra.pop(""check_response"", None)  # this should be added
            session.proxies = extra.pop(""proxies"", extra.pop(""proxy"", {}))
            session.stream = extra.pop(""stream"", False)
            session.verify = extra.pop(""verify"", extra.pop(""verify_ssl"", True))
            session.cert = extra.pop(""cert"", None)
            session.max_redirects = extra.pop(""max_redirects"", DEFAULT_REDIRECT_LIMIT)
            session.trust_env = extra.pop(""trust_env"", True)
            try:
                session.headers.update(extra)
            except TypeError:
                self.log.warning(""Connection to %s has invalid extra field."", connection.host)
            return session
```

topherinternational on (2025-01-06 12:33:08 UTC): Ah, I think what the other commentor meant was to pass `extra_options` to the `HttpOperator`, not the Connection. When I pass it to the operator the setting is passed to the hook as expected and the `response_check` function is called.

This should solve your immediate problem, checking status and response content with your callback instead of the hook raising a preemptive exception.

topherinternational on (2025-01-06 12:54:02 UTC): This is the call of someone in the core contributor club, so I'm just giving my opinion - 

This does look like a bug in the HTTP hook, but it seems like a separate issue to this issue. I see the two breaking down like this:
- When I use the `HttpOperator`, I want to tell it to skip any status checking and let me check status through the existing `response_check` callback argument
- When I declare a connection to use with the HttpHook, I want to specify that the hook should not do a status check on any call made with this connection

Using the hook-connection contract to configure the control flow of the operator perpetuates the API leaking problem that spawned this issue in the first place. 

It's a separate issue/question whether the hook should check it status based on an option in the connection object, or if that behavior should always be configured by the caller (args to the constructor or run()). I lean towards yes but that can be discussed in a new issue filing.

dabla (Issue Creator) on (2025-01-07 10:24:13 UTC): I've created a [PR ](https://github.com/apache/airflow/pull/45451) which solves the issue of the check_response being interpreted as a HTTP header when defined under the extras of the HTTP connection.

"
2760608539,issue,open,,Airflow Scheduler Error with LocalExecutor: 'QueuedLocalWorker' object has no attribute 'wrapper',"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Description:
While using Airflow with LocalExecutor, the scheduler fails to run and raises the following error:

Error Traceback:

scheduler  | AttributeError: 'QueuedLocalWorker' object has no attribute 'wrapper'
scheduler  | Traceback (most recent call last):
scheduler  | File ""<string>"", line 1, in <module>
scheduler  | File ""/Users/xggz/anaconda3/envs/airflow-project/lib/python3.9/multiprocessing/spawn.py"", line 116, in spawn_main
scheduler  | exitcode = _main(fd, parent_sentinel)
scheduler  | File ""/Users/xggz/anaconda3/envs/airflow-project/lib/python3.9/multiprocessing/spawn.py"", line 126, in _main

### What you think should happen instead?

_No response_

### How to reproduce

Set up Airflow with LocalExecutor in the configuration.
Start the scheduler using the airflow scheduler command.
Observe the scheduler failing to run and the above error being thrown.

### Operating System

macos intel

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",xggz,2024-12-27 09:00:27+00:00,[],2024-12-27 09:02:47+00:00,,https://github.com/apache/airflow/issues/45235,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('area:Executors-core', 'LocalExecutor & SequentialExecutor')]","[{'comment_id': 2563480924, 'issue_id': 2760608539, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 27, 9, 0, 31, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-27 09:00:31 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2760560876,issue,closed,completed,Add support for `get_current_context` in Task SDK,"The following task should work. This task is part of [`tutorial_taskflow_templates`](https://github.com/apache/airflow/blob/c300e0e7c6b9b74b1171cce9dbc6f683f721c0cc/airflow/example_dags/tutorial_taskflow_templates.py) DAG
```py
    from airflow.providers.standard.operators.python import get_current_context

    @task(
        # Causes variables that end with `.sql` to be read and templates
        # within to be rendered.
        templates_exts=["".sql""],
    )
    def template_test(sql, test_var, data_interval_end):
        context = get_current_context()

        # Will print...
        # select * from test_data
        # where 1=1
        #     and run_id = 'scheduled__2024-10-09T00:00:00+00:00'
        #     and something_else = 'param_from_task'
        print(f""sql: {sql}"")

        # Will print `scheduled__2024-10-09T00:00:00+00:00`
        print(f""test_var: {test_var}"")

        # Will print `2024-10-10 00:00:00+00:00`.
        # Note how we didn't pass this value when calling the task. Instead
        # it was passed by the decorator from the context
        print(f""data_interval_end: {data_interval_end}"")

        # Will print...
        # run_id: scheduled__2024-10-09T00:00:00+00:00; params.other_param: from_dag
        template_str = ""run_id: {{ run_id }}; params.other_param: {{ params.other_param }}""
        rendered_template = context[""task""].render_template(
            template_str,
            context,
        )
        print(f""rendered template: {rendered_template}"")

        # Will print the full context dict
        print(f""context: {context}"")

```",kaxil,2024-12-27 08:12:12+00:00,['kaxil'],2025-01-09 07:15:55+00:00,2025-01-09 07:15:55+00:00,https://github.com/apache/airflow/issues/45234,"[('kind:feature', 'Feature Requests'), ('area:core', '')]","[{'comment_id': 2564960213, 'issue_id': 2760560876, 'author': 'okirialbert', 'body': ""Hi @kaxil, I'd like to work on this."", 'created_at': datetime.datetime(2024, 12, 30, 2, 23, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2577438621, 'issue_id': 2760560876, 'author': 'kaxil', 'body': 'Hey @okirialbert , thanks for your interest in getting this one but since we are in time crunch for Airflow 3.0 and since I am already on it, I will pick this up for now', 'created_at': datetime.datetime(2025, 1, 8, 11, 26, 11, tzinfo=datetime.timezone.utc)}]","okirialbert on (2024-12-30 02:23:17 UTC): Hi @kaxil, I'd like to work on this.

kaxil (Issue Creator) on (2025-01-08 11:26:11 UTC): Hey @okirialbert , thanks for your interest in getting this one but since we are in time crunch for Airflow 3.0 and since I am already on it, I will pick this up for now

"
2760560834,issue,open,,Add support for getting connections via BaseHook (AIP-72),,kaxil,2024-12-27 08:12:09+00:00,[],2024-12-27 08:14:22+00:00,,https://github.com/apache/airflow/issues/45233,"[('area:core', '')]",[],
2760560787,issue,closed,completed,Add support for Taskflow API in Task SDK & template rendering,,kaxil,2024-12-27 08:12:07+00:00,['kaxil'],2025-01-07 09:38:20+00:00,2025-01-07 09:38:20+00:00,https://github.com/apache/airflow/issues/45232,"[('area:API', ""Airflow's REST/HTTP API"")]",[],
2760560737,issue,open,,Move XCom serialization and deserialization methods to the Task SDK,"Currently, as part of AIP-72, we implemented XCom API endpoints in the Task Execution API Server that expect a JSON-serialized string

https://github.com/apache/airflow/blob/9be59712a8934c9c7eb90a478964a2e0f8260ecf/airflow/api_fastapi/execution_api/routes/xcoms.py#L114-L138

This was done so that we can have the same interface for multi-language support (Python, Go, Java etc) for Task SDK. The contract is simple: API Server always deals with JSON-formatted string --- and the responsibility of serialization and de-serialization of Native objects to string and back lies to the language-specific clients.

In order to do that we should move the current Serialization and de-serialization logic to clients. This will also allow storing and using XCom backends correctly, which can also handle different languages.

As part of this task, we should also figure out how we can avoid serialization and deserialization from SQLAlchemy as we use JSON type for `value` column --- which leads to double / triple serialization

1. Serialization from Task SDK and sent to API Server
2. API Server calls XCom.serialize
3. SQLAlchemy `JSON` type serializes it further

Result of Triple serialization:

<img width=""819"" alt=""image"" src=""https://github.com/user-attachments/assets/4f94ebbe-369f-4382-933e-a780d4f90579"" />


",kaxil,2024-12-27 08:12:04+00:00,['kaxil'],2025-01-02 08:07:17+00:00,,https://github.com/apache/airflow/issues/45231,"[('area:core', '')]","[{'comment_id': 2567402702, 'issue_id': 2760560737, 'author': 'amoghrajesh', 'body': 'I think this might be the right issue to introduce a ""XComAccessor"" kind of interface. I am adding support for handling error cases in https://github.com/apache/airflow/issues/45341 to match legacy', 'created_at': datetime.datetime(2025, 1, 2, 8, 7, 16, tzinfo=datetime.timezone.utc)}]","amoghrajesh on (2025-01-02 08:07:16 UTC): I think this might be the right issue to introduce a ""XComAccessor"" kind of interface. I am adding support for handling error cases in https://github.com/apache/airflow/issues/45341 to match legacy

"
2760560653,issue,closed,completed,Push XCom if `BaseOperator.do_xcom_push` is True and if task returns a value,"Add support for pushing XCom values by just returning from a function.

```python
def push_by_returning(**kwargs):
    """"""Pushes an XCom without a specific target, just by returning it""""""
    return ""hi from push_by_returning""

push2 = PythonOperator(
    task_id=""push_by_returning"",
    dag=dag,
    python_callable=push_by_returning,
)
```

Currently, we support explicit `ti.xcom_push` and pull with Task SDK",kaxil,2024-12-27 08:11:59+00:00,['kaxil'],2024-12-27 17:57:53+00:00,2024-12-27 17:57:53+00:00,https://github.com/apache/airflow/issues/45230,"[('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2760553953,issue,closed,completed,Rename `fail_stop` DAG property to `fail_fast`,"### Body

`fail_stop` isn't a good name at all. The property isn't indicative of the purpose. The original PR https://github.com/apache/airflow/pull/29406 intended to call it `fail_fast` which is much better. ",amoghrajesh,2024-12-27 08:05:06+00:00,['hprassad'],2025-01-21 02:04:43+00:00,2025-01-21 02:04:42+00:00,https://github.com/apache/airflow/issues/45229,"[('kind:feature', 'Feature Requests'), ('good first issue', ''), ('area:core', '')]","[{'comment_id': 2563466531, 'issue_id': 2760553953, 'author': 'hprassad', 'body': 'Hi @amoghrajesh , I would like to work on this. Can you please assign this to me ?', 'created_at': datetime.datetime(2024, 12, 27, 8, 43, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563591697, 'issue_id': 2760553953, 'author': 'amoghrajesh', 'body': 'Feel free @hprassad.\r\n\r\nWe will need a deprecation for 2.11 and then we will have to start using the new property name for 3.x onwards (main branch). Example for deprecations https://github.com/apache/airflow/pull/44791', 'created_at': datetime.datetime(2024, 12, 27, 11, 11, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2565730648, 'issue_id': 2760553953, 'author': 'amoghrajesh', 'body': '@hprassad are you still working on this issue? Do you need any assistance?', 'created_at': datetime.datetime(2024, 12, 30, 17, 13, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2566164538, 'issue_id': 2760553953, 'author': 'hprassad', 'body': ""@amoghrajesh, Yes, i'm working on this. Any assistance would be helpful"", 'created_at': datetime.datetime(2024, 12, 31, 6, 14, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2566988371, 'issue_id': 2760553953, 'author': 'amoghrajesh', 'body': 'As discussed offline, please proceed with a pull request, and we can discuss further there.', 'created_at': datetime.datetime(2025, 1, 1, 12, 28, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567024839, 'issue_id': 2760553953, 'author': 'hprassad', 'body': 'Hi @amoghrajesh , PR is raised', 'created_at': datetime.datetime(2025, 1, 1, 14, 10, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2577614913, 'issue_id': 2760553953, 'author': 'richochetclementine1315', 'body': 'want to take up this  issue', 'created_at': datetime.datetime(2025, 1, 8, 12, 59, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2577615376, 'issue_id': 2760553953, 'author': 'richochetclementine1315', 'body': '.take issue', 'created_at': datetime.datetime(2025, 1, 8, 12, 59, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2577616977, 'issue_id': 2760553953, 'author': 'amoghrajesh', 'body': '@richochetclementine1315 this issue is already assigned and a PR is up for it.', 'created_at': datetime.datetime(2025, 1, 8, 13, 0, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2587268890, 'issue_id': 2760553953, 'author': 'Anurag-Kumar-01', 'body': 'I also want to take this issue', 'created_at': datetime.datetime(2025, 1, 13, 14, 34, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2588211049, 'issue_id': 2760553953, 'author': 'amoghrajesh', 'body': 'Hello @Anurag-Kumar-01, its already assigned and is being worked on. So, unfortunately cannot be reassigned.', 'created_at': datetime.datetime(2025, 1, 13, 21, 9, 23, tzinfo=datetime.timezone.utc)}]","hprassad (Assginee) on (2024-12-27 08:43:12 UTC): Hi @amoghrajesh , I would like to work on this. Can you please assign this to me ?

amoghrajesh (Issue Creator) on (2024-12-27 11:11:53 UTC): Feel free @hprassad.

We will need a deprecation for 2.11 and then we will have to start using the new property name for 3.x onwards (main branch). Example for deprecations https://github.com/apache/airflow/pull/44791

amoghrajesh (Issue Creator) on (2024-12-30 17:13:58 UTC): @hprassad are you still working on this issue? Do you need any assistance?

hprassad (Assginee) on (2024-12-31 06:14:09 UTC): @amoghrajesh, Yes, i'm working on this. Any assistance would be helpful

amoghrajesh (Issue Creator) on (2025-01-01 12:28:08 UTC): As discussed offline, please proceed with a pull request, and we can discuss further there.

hprassad (Assginee) on (2025-01-01 14:10:08 UTC): Hi @amoghrajesh , PR is raised

richochetclementine1315 on (2025-01-08 12:59:10 UTC): want to take up this  issue

richochetclementine1315 on (2025-01-08 12:59:24 UTC): .take issue

amoghrajesh (Issue Creator) on (2025-01-08 13:00:16 UTC): @richochetclementine1315 this issue is already assigned and a PR is up for it.

Anurag-Kumar-01 on (2025-01-13 14:34:20 UTC): I also want to take this issue

amoghrajesh (Issue Creator) on (2025-01-13 21:09:23 UTC): Hello @Anurag-Kumar-01, its already assigned and is being worked on. So, unfortunately cannot be reassigned.

"
2760541176,issue,open,,DAG Import Errors message is dangling in web-interface,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.2

### What happened?

<img width=""420"" alt=""dag_report_error"" src=""https://github.com/user-attachments/assets/fc99412d-7ca8-4308-88b1-cd568594846e"" />

We have this message in the Airflow's web-interface no matter what. This DAG can exist or not - the error still shows.

We have found many different topics across the Internet for various Airflow versions. Some of them from 2018. So users constantly witness these dangling messages, but there are no clarification for this case.

Also we don't have an understanding what is a source for this notification message. It sits in the Airflow DB and webserver is selecting it from there or other core components are generating it for webserver somehow.

### What you think should happen instead?

Even though the error message itself is far from perfect, we expect that if DAG doesn't exist anymore, the error message should disappear.

### How to reproduce

We use gitsync option for storing DAGs:
````yaml
# DAGs Config
dags:
  gitSync:
    enabled: true
    repo: ""https://my.repo/path/name.git""
    branch: ""master""
    rev: ""HEAD""
    subPath: ""airflowk8s/dags""
    credentialsSecret: my-secret
    period: 5s
    maxFailures: 10
````
And our DAG is just script.py like that, nothing complicated:
`print(""D0ne!"")`

So I guess steps should be:
1) Create a broken DAG in remote repository;
2) Get the error message in the Airflow's web-interface;
3) Delete a broken DAG in remote repository;
4) Wait for your gitsync;
5) The error message in the Airflow's web-interface will dangle.

But this bug appears randomly, so we don't know exact reason.

### Operating System

Kubernetes  v1.31.0-eks

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",eanikindfi,2024-12-27 07:51:09+00:00,[],2025-01-17 14:28:59+00:00,,https://github.com/apache/airflow/issues/45227,"[('kind:bug', 'This is a clearly a bug'), ('area:MetaDB', 'Meta Database related issues.'), ('good first issue', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2596981750, 'issue_id': 2760541176, 'author': 'potiuk', 'body': ""Yeah. I think you'd need to fix the dag first and then delete it, which is annoying - as a workaround you can delete the import_error entry related to the DAG - marked it as a good first issue for someone to tackle (if they can reproduce it of course)"", 'created_at': datetime.datetime(2025, 1, 16, 21, 54, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2597406008, 'issue_id': 2760541176, 'author': 'eanikindfi', 'body': '@potiuk Thanks! A workaround helped.\nIn the future we will try to work like that ""you\'d need to fix the dag first and then delete it"".\n\nBut I guess it would be helpful to validate entries in this table somehow? At least, if the DAG is no longer exist, then Airflow should delete the related entry.', 'created_at': datetime.datetime(2025, 1, 17, 4, 21, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2598491054, 'issue_id': 2760541176, 'author': 'potiuk', 'body': '> But I guess it would be helpful to validate entries in this table somehow? At least, if the DAG is no longer exist, then Airflow should delete the related entry.\n\nSure. If you wish to contribute it - feel free.', 'created_at': datetime.datetime(2025, 1, 17, 14, 28, 58, tzinfo=datetime.timezone.utc)}]","potiuk on (2025-01-16 21:54:10 UTC): Yeah. I think you'd need to fix the dag first and then delete it, which is annoying - as a workaround you can delete the import_error entry related to the DAG - marked it as a good first issue for someone to tackle (if they can reproduce it of course)

eanikindfi (Issue Creator) on (2025-01-17 04:21:25 UTC): @potiuk Thanks! A workaround helped.
In the future we will try to work like that ""you'd need to fix the dag first and then delete it"".

But I guess it would be helpful to validate entries in this table somehow? At least, if the DAG is no longer exist, then Airflow should delete the related entry.

potiuk on (2025-01-17 14:28:58 UTC): Sure. If you wish to contribute it - feel free.

"
2760509460,issue,closed,completed,Python Operator reports an error when connecting to Clickhouse using Clickhouse connect,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

```
def show_tables():
    client = clickhouse_connect.create_client(
        interface=""https"",
        host='localhost',
        port=9090,
        user='default',
        password='123456',
        ca_cert='/Users/cc.cai/develop/clickhouse/testing/testing-ca.crt'
    )
    result = client.command(""select 1"")
    print(result)
```

The above python can be executed locally, but when I use pythonoperator, an error is reported

```
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from clickhouse_connect import get_client
from dill import settings
from sqlalchemy_utils.types.enriched_datetime.pendulum_datetime import pendulum
import datetime
import clickhouse_connect

def show_tables():
    client = clickhouse_connect.create_client(
        interface=""https"",
        host='localhost',
        port=9090,
        user='default',
        password='123456',
        ca_cert='/Users/cc.cai/develop/clickhouse/testing/testing-ca.crt'
    )
    result = client.command(""select 1"")
    print(result)

with DAG(
    dag_id=""example_python_operator"",
    schedule=""0 0 * * *"",
    start_date=pendulum.datetime(2021, 1, 1, tz=""UTC""),
    catchup=False,
    dagrun_timeout=datetime.timedelta(minutes=60),
    tags=[""example"", ""example2""],
    params={""example_key"": ""example_value""},
) as dag:
    show_tables_task = PythonOperator(
        task_id='show_tables_task',
        python_callable=show_tables,
        dag=dag
    )

```


Traceback (most recent call last):
```
  File ""/Users/cc.cai/airflow_venv/lib/python3.10/site-packages/airflow/models/dagbag.py"", line 383, in parse
    loader.exec_module(new_module)
  File ""<frozen importlib._bootstrap_external>"", line 883, in exec_module
  File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed
  File ""/Users/cc.cai/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_python_operator.py"", line 57, in <module>
    show_tables()
  File ""/Users/cc.cai/airflow_venv/lib/python3.10/site-packages/airflow/example_dags/example_python_operator.py"", line 31, in show_tables
    client = clickhouse_connect.create_client(
  File ""/Users/cc.cai/airflow_venv/lib/python3.10/site-packages/clickhouse_connect/driver/__init__.py"", line 115, in create_client
    return HttpClient(interface, host, port, username, password, database, settings=settings, **kwargs)
  File ""/Users/cc.cai/airflow_venv/lib/python3.10/site-packages/clickhouse_connect/driver/httpclient.py"", line 157, in __init__
    super().__init__(database=database,
  File ""/Users/cc.cai/airflow_venv/lib/python3.10/site-packages/clickhouse_connect/driver/client.py"", line 69, in __init__
    self._init_common_settings(apply_server_timezone)
  File ""/Users/cc.cai/airflow_venv/lib/python3.10/site-packages/clickhouse_connect/driver/client.py"", line 74, in _init_common_settings
    tuple(self.command('SELECT version(), timezone()', use_database=False))
  File ""/Users/cc.cai/airflow_venv/lib/python3.10/site-packages/clickhouse_connect/driver/httpclient.py"", line 351, in command
    response = self._raw_request(payload, params, headers, method, fields=fields, server_wait=False)
  File ""/Users/cc.cai/airflow_venv/lib/python3.10/site-packages/clickhouse_connect/driver/httpclient.py"", line 449, in _raw_request
    raise OperationalError(f'Error {ex} executing HTTP request attempt {attempts}{err_url}') from ex
clickhouse_connect.driver.exceptions.OperationalError: Error HTTPSConnectionPool(host='clickhouse-testing.automizely.me', port=9090): Max retries exceeded with url: /? (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed:
```

### What you think should happen instead?

_No response_

### How to reproduce

```
def show_tables():
    client = clickhouse_connect.create_client(
        interface=""https"",
        host='localhost',
        port=9090,
        user='default',
        password='123456',
        ca_cert='/Users/cc.cai/develop/clickhouse/testing/testing-ca.crt'
    )
    result = client.command(""select 1"")
    print(result)
```

The above python can be executed locally, but when I use pythonoperator, an error is reported

```
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from clickhouse_connect import get_client
from dill import settings
from sqlalchemy_utils.types.enriched_datetime.pendulum_datetime import pendulum
import datetime
import clickhouse_connect

def show_tables():
    client = clickhouse_connect.create_client(
        interface=""https"",
        host='localhost',
        port=9090,
        user='default',
        password='123456',
        ca_cert='/Users/cc.cai/develop/clickhouse/testing/testing-ca.crt'
    )
    result = client.command(""select 1"")
    print(result)

with DAG(
    dag_id=""example_python_operator"",
    schedule=""0 0 * * *"",
    start_date=pendulum.datetime(2021, 1, 1, tz=""UTC""),
    catchup=False,
    dagrun_timeout=datetime.timedelta(minutes=60),
    tags=[""example"", ""example2""],
    params={""example_key"": ""example_value""},
) as dag:
    show_tables_task = PythonOperator(
        task_id='show_tables_task',
        python_callable=show_tables,
        dag=dag
    )
```

Max retries exceeded with url: /? (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed

### Operating System

pythonoperator

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",caicancai,2024-12-27 07:18:25+00:00,[],2024-12-27 21:31:57+00:00,2024-12-27 21:31:57+00:00,https://github.com/apache/airflow/issues/45226,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2563403452, 'issue_id': 2760509460, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 27, 7, 18, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563883569, 'issue_id': 2760509460, 'author': 'caicancai', 'body': 'I tried SimplehttpOperator and had the same problem', 'created_at': datetime.datetime(2024, 12, 27, 17, 12, 19, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-27 07:18:27 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

caicancai (Issue Creator) on (2024-12-27 17:12:19 UTC): I tried SimplehttpOperator and had the same problem

"
2759905831,issue,closed,completed,Support OAuth2 directly with Authlib without relying on Flask AppBuilder,"### Description

Make it possible to have OAuth2 without depending on Flask AppBuilder, e.g. for Okta.

### Use case/motivation

Examples for Okta rely on Flask AppBuilder for OAuth2 support. With Airflow 3 not using Flask, it'd be beneficial to support OAuth2 without relying on Flash libs. In particular, Flask AppBuilder uses Authlib https://flask-appbuilder.readthedocs.io/en/latest/security.html#authentication-oauth, so we might be able to use it directly.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",andrii-korotkov-verkada,2024-12-26 15:48:06+00:00,[],2024-12-26 17:31:18+00:00,2024-12-26 17:31:18+00:00,https://github.com/apache/airflow/issues/45219,"[('kind:feature', 'Feature Requests'), ('area:auth', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2562974099, 'issue_id': 2759905831, 'author': 'potiuk', 'body': 'We are not going to build in all the kind of authentication mechanims in Airflow 3 - we are moving away from those mechanisms being effectively ""embedded"" to be ""externalized"". We already separated AuthManager interface in Airflow 2 and it\'s further re-inforced In Airflow 3 where we have a plan to support KeyCloak as external authentication mechanims and building KeyCloak Authentication mechanism.\r\n\r\nIn fact we are looking for someone volunteering and contributing back KeyCloak Auth Manager to implement it (with a fallback we are going to keep to be FAB). Airflow 3 is deliberatly **not** going to depend on Flask nor Flask App Builder - instead FAB provider is going to be the only ""dependency"" to those and it will be swappable with another implementation of Auth Manager.\r\n\r\nIf you would like to implement such KeyCloak AuthManager, you are absolutely free to do so. See https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-56+Extensible+user+management and ""keycloak auth manager"" section.\r\n\r\nConverting it into discussion if more needed.', 'created_at': datetime.datetime(2024, 12, 26, 17, 31, 6, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-12-26 17:31:06 UTC): We are not going to build in all the kind of authentication mechanims in Airflow 3 - we are moving away from those mechanisms being effectively ""embedded"" to be ""externalized"". We already separated AuthManager interface in Airflow 2 and it's further re-inforced In Airflow 3 where we have a plan to support KeyCloak as external authentication mechanims and building KeyCloak Authentication mechanism.

In fact we are looking for someone volunteering and contributing back KeyCloak Auth Manager to implement it (with a fallback we are going to keep to be FAB). Airflow 3 is deliberatly **not** going to depend on Flask nor Flask App Builder - instead FAB provider is going to be the only ""dependency"" to those and it will be swappable with another implementation of Auth Manager.

If you would like to implement such KeyCloak AuthManager, you are absolutely free to do so. See https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-56+Extensible+user+management and ""keycloak auth manager"" section.

Converting it into discussion if more needed.

"
2759444036,issue,closed,completed,Add support for markdown rendering of dagrun and task instance note in header section.,"### Description

Currently, the task instance and dagrun note are simple text components in the respective page's header. The notes support markdown rendering in legacy view. The same could be added here. The notes could be displayed in a modal like task/dag docs on clicking the button.

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",tirkarthi,2024-12-26 08:10:10+00:00,['aditya0yadav'],2025-01-22 04:21:45+00:00,2025-01-22 04:21:44+00:00,https://github.com/apache/airflow/issues/45216,"[('kind:feature', 'Feature Requests'), ('good first issue', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]","[{'comment_id': 2564699202, 'issue_id': 2759444036, 'author': 'rawwar', 'body': 'I will be working on this one.', 'created_at': datetime.datetime(2024, 12, 29, 11, 50, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585642403, 'issue_id': 2759444036, 'author': 'aditya0yadav', 'body': 'if it is available, can i work on this', 'created_at': datetime.datetime(2025, 1, 12, 8, 36, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585702398, 'issue_id': 2759444036, 'author': 'rawwar', 'body': 'I am actively working on this.', 'created_at': datetime.datetime(2025, 1, 12, 11, 52, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585840445, 'issue_id': 2759444036, 'author': 'aditya0yadav', 'body': 'I want to gain experience in open source.  \nCould we collaborate on this project, if possible?', 'created_at': datetime.datetime(2025, 1, 12, 17, 20, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585840547, 'issue_id': 2759444036, 'author': 'aditya0yadav', 'body': 'Sorry I mean issue', 'created_at': datetime.datetime(2025, 1, 12, 17, 21, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2585934644, 'issue_id': 2759444036, 'author': 'rawwar', 'body': '@aditya0yadav , assigned it to you. All the best! \n\nI picked this as i was learning front end. Looking forward to your PR. Also, community is active on slack. If you have questions specific to this Issue, please comment on the issue itself', 'created_at': datetime.datetime(2025, 1, 12, 22, 5, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2586345415, 'issue_id': 2759444036, 'author': 'aditya0yadav', 'body': 'thank you, rawwar', 'created_at': datetime.datetime(2025, 1, 13, 7, 5, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2606262298, 'issue_id': 2759444036, 'author': 'tirkarthi', 'body': 'resolved by https://github.com/apache/airflow/pull/45829', 'created_at': datetime.datetime(2025, 1, 22, 4, 21, 44, tzinfo=datetime.timezone.utc)}]","rawwar on (2024-12-29 11:50:13 UTC): I will be working on this one.

aditya0yadav (Assginee) on (2025-01-12 08:36:30 UTC): if it is available, can i work on this

rawwar on (2025-01-12 11:52:48 UTC): I am actively working on this.

aditya0yadav (Assginee) on (2025-01-12 17:20:41 UTC): I want to gain experience in open source.  
Could we collaborate on this project, if possible?

aditya0yadav (Assginee) on (2025-01-12 17:21:07 UTC): Sorry I mean issue

rawwar on (2025-01-12 22:05:36 UTC): @aditya0yadav , assigned it to you. All the best! 

I picked this as i was learning front end. Looking forward to your PR. Also, community is active on slack. If you have questions specific to this Issue, please comment on the issue itself

aditya0yadav (Assginee) on (2025-01-13 07:05:35 UTC): thank you, rawwar

tirkarthi (Issue Creator) on (2025-01-22 04:21:44 UTC): resolved by https://github.com/apache/airflow/pull/45829

"
2759352536,issue,closed,completed, Airflow 2 to 3 auto migration rules - airflow config lint (44080 ~ 45017),"### Description

Parent issue: https://github.com/apache/airflow/issues/41641

## airflow config

### Removal
* `core.task_runner`
* `core.enable_xcom_pickling`

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-12-26 06:38:11+00:00,['Lee-W'],2024-12-31 12:28:47+00:00,2024-12-31 12:28:47+00:00,https://github.com/apache/airflow/issues/45213,"[('area:dev-tools', ''), ('kind:feature', 'Feature Requests'), ('area:upgrade', 'Facilitating migration to a newer version of Airflow')]",[],
2759351593,issue,open,,Airflow 2 to 3 auto migration rules - ruff (44080 ~ 45017),"### Description
Parent issue: https://github.com/apache/airflow/issues/41641

## Ruff
### AIR302

#### name
* [x] `TriggerRule.NONE_FAILED_OR_SKIPPED` (from #44475)
* [x] argument `appbuilder` in BaseAuthManager and its subclasses (from #45009)

#### context key
* [ ] `conf` (from #44820)

### AIR303
* [x] `airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG` → `airflow.providers.celery.executors.default_celery.DEFAULT_CELERY_CONFIG` (from #45017) (3.3.0)

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-12-26 06:37:06+00:00,['Lee-W'],2025-01-02 03:07:11+00:00,,https://github.com/apache/airflow/issues/45212,"[('area:dev-tools', ''), ('kind:feature', 'Feature Requests'), ('area:upgrade', 'Facilitating migration to a newer version of Airflow')]","[{'comment_id': 2562294029, 'issue_id': 2759351593, 'author': 'Lee-W', 'body': 'the appbuilder rule has been added to https://github.com/astral-sh/ruff/pull/15083', 'created_at': datetime.datetime(2024, 12, 26, 8, 11, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562319070, 'issue_id': 2759351593, 'author': 'Lee-W', 'body': 'https://github.com/astral-sh/ruff/pull/15145 created for the 303 one (merged)', 'created_at': datetime.datetime(2024, 12, 26, 8, 39, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567247070, 'issue_id': 2759351593, 'author': 'Lee-W', 'body': 'the remaining rule has been addressed by https://github.com/astral-sh/ruff/pull/15144', 'created_at': datetime.datetime(2025, 1, 2, 3, 7, 10, tzinfo=datetime.timezone.utc)}]","Lee-W (Issue Creator) on (2024-12-26 08:11:56 UTC): the appbuilder rule has been added to https://github.com/astral-sh/ruff/pull/15083

Lee-W (Issue Creator) on (2024-12-26 08:39:28 UTC): https://github.com/astral-sh/ruff/pull/15145 created for the 303 one (merged)

Lee-W (Issue Creator) on (2025-01-02 03:07:10 UTC): the remaining rule has been addressed by https://github.com/astral-sh/ruff/pull/15144

"
2758879377,issue,open,,Add support for favorite/pin dags to dashboard.,"### Description

There is a widget to show recent dags viewed. I was prototyping this by using `useEffect` on dags page on load to add the dag id to a local storage key and display it. The key could store 5 items. I guess it would be useful to allow users to favorite or pin the dags. Simplified approach would be local storage but it won't persist across browsers. There is no model to store this in the server side.

Sample screenshot : 

![image](https://github.com/user-attachments/assets/65d6d876-c82c-4669-aa2a-8c8b05a170be)


### Use case/motivation

Allow users to quickly visit their favorite dags from dashboard.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",tirkarthi,2024-12-25 16:10:27+00:00,[],2025-01-23 22:51:50+00:00,,https://github.com/apache/airflow/issues/45207,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]","[{'comment_id': 2561940700, 'issue_id': 2758879377, 'author': 'tirkarthi', 'body': 'Related https://github.com/apache/airflow/issues/21720', 'created_at': datetime.datetime(2024, 12, 25, 16, 11, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561951001, 'issue_id': 2758879377, 'author': 'tirkarthi', 'body': 'cc: @pierrejeambrun @bbovenzi', 'created_at': datetime.datetime(2024, 12, 25, 16, 54, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2575958701, 'issue_id': 2758879377, 'author': 'bbovenzi', 'body': 'Yes, I imagined we would use localStorage for this.', 'created_at': datetime.datetime(2025, 1, 7, 18, 23, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2577601768, 'issue_id': 2758879377, 'author': 'pierrejeambrun', 'body': ""> Yes, I imagined we would use localStorage for this.\r\n\r\nThat's definitely the easiest approach but feels like a workaround to avoid having to update the database and do the appropriate backend-work. Is losing favorites because we swap browser / computer, or use a private navigation a big deal ?\r\n\r\nOn the other hand doing it in the backend represent a much bigger effort.\r\n\r\nWe can always start small, with the local storage, so the front-end development can progress. And eventually if the user request it and if the front-end only solution is not enough, we can always do the backend part later."", 'created_at': datetime.datetime(2025, 1, 8, 12, 52, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2577811847, 'issue_id': 2758879377, 'author': 'tirkarthi', 'body': ""Does react provide a way to run a function on every dag page and it's sub pages like dagruns, taskinstances etc ? Something that runs whenever dag/* path matches."", 'created_at': datetime.datetime(2025, 1, 8, 14, 29, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2577969165, 'issue_id': 2758879377, 'author': 'pierrejeambrun', 'body': ""> Does react provide a way to run a function on every dag page and it's sub pages like dagruns, taskinstances etc ? Something that runs whenever dag/* path matches.\r\n\r\nNot that I am aware of. What are you trying to achieve ?"", 'created_at': datetime.datetime(2025, 1, 8, 15, 32, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2577998376, 'issue_id': 2758879377, 'author': 'tirkarthi', 'body': 'Sorry, the comment was slightly off topic and the query was more around use case to build recently viewed dags which was the initial dashboard design. I created this while working on that since I felt favourite dags are more useful than recently viewed dags.', 'created_at': datetime.datetime(2025, 1, 8, 15, 44, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2578029021, 'issue_id': 2758879377, 'author': 'pierrejeambrun', 'body': ""Basically to get started we need 2 mutations (Create and Delete Favories) and 1 fetch (retrieve favorites).\r\n\r\nYou can wrap those read-write from the Local Storage into a `rect-query`. This way it's agnostic for our application code if the query is fetching data from the API or from the local storage benefiting from the powerful react-query interface (isLoading, isSuccess, Caching, retries, data sharing and invalidation etc.)"", 'created_at': datetime.datetime(2025, 1, 8, 15, 56, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2578063171, 'issue_id': 2758879377, 'author': 'bbovenzi', 'body': 'We can check url changes via useLocation and useEffect in a wrapper component:\r\n\r\n```\r\nconst location = useLocation();\r\nuseEffect(() => {\r\n   console.log(""if contains a dag_id, update view count"")\r\n}, [location]);\r\n```\r\n\r\nOther DB-supported methods to filter the dashboard could include `owner` or `tags`. The problem is that everyone organizes their dags very differently', 'created_at': datetime.datetime(2025, 1, 8, 16, 11, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2578178153, 'issue_id': 2758879377, 'author': 'tirkarthi', 'body': 'There seems to be a built in approach from react-query for interface to localstorage and other persistent storage.\r\n\r\nhttps://tanstack.com/query/latest/docs/framework/react/plugins/persistQueryClient', 'created_at': datetime.datetime(2025, 1, 8, 17, 2, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2610153962, 'issue_id': 2758879377, 'author': 'rawwar', 'body': 'should we limit favourites/pins to DAGs or also allow tasks?', 'created_at': datetime.datetime(2025, 1, 23, 15, 42, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2611177699, 'issue_id': 2758879377, 'author': 'bbovenzi', 'body': ""> should we limit favourites/pins to DAGs or also allow tasks?\n\nI don't see why not?"", 'created_at': datetime.datetime(2025, 1, 23, 22, 51, 49, tzinfo=datetime.timezone.utc)}]","tirkarthi (Issue Creator) on (2024-12-25 16:11:07 UTC): Related https://github.com/apache/airflow/issues/21720

tirkarthi (Issue Creator) on (2024-12-25 16:54:20 UTC): cc: @pierrejeambrun @bbovenzi

bbovenzi on (2025-01-07 18:23:14 UTC): Yes, I imagined we would use localStorage for this.

pierrejeambrun on (2025-01-08 12:52:35 UTC): That's definitely the easiest approach but feels like a workaround to avoid having to update the database and do the appropriate backend-work. Is losing favorites because we swap browser / computer, or use a private navigation a big deal ?

On the other hand doing it in the backend represent a much bigger effort.

We can always start small, with the local storage, so the front-end development can progress. And eventually if the user request it and if the front-end only solution is not enough, we can always do the backend part later.

tirkarthi (Issue Creator) on (2025-01-08 14:29:38 UTC): Does react provide a way to run a function on every dag page and it's sub pages like dagruns, taskinstances etc ? Something that runs whenever dag/* path matches.

pierrejeambrun on (2025-01-08 15:32:49 UTC): Not that I am aware of. What are you trying to achieve ?

tirkarthi (Issue Creator) on (2025-01-08 15:44:32 UTC): Sorry, the comment was slightly off topic and the query was more around use case to build recently viewed dags which was the initial dashboard design. I created this while working on that since I felt favourite dags are more useful than recently viewed dags.

pierrejeambrun on (2025-01-08 15:56:49 UTC): Basically to get started we need 2 mutations (Create and Delete Favories) and 1 fetch (retrieve favorites).

You can wrap those read-write from the Local Storage into a `rect-query`. This way it's agnostic for our application code if the query is fetching data from the API or from the local storage benefiting from the powerful react-query interface (isLoading, isSuccess, Caching, retries, data sharing and invalidation etc.)

bbovenzi on (2025-01-08 16:11:03 UTC): We can check url changes via useLocation and useEffect in a wrapper component:

```
const location = useLocation();
useEffect(() => {
   console.log(""if contains a dag_id, update view count"")
}, [location]);
```

Other DB-supported methods to filter the dashboard could include `owner` or `tags`. The problem is that everyone organizes their dags very differently

tirkarthi (Issue Creator) on (2025-01-08 17:02:49 UTC): There seems to be a built in approach from react-query for interface to localstorage and other persistent storage.

https://tanstack.com/query/latest/docs/framework/react/plugins/persistQueryClient

rawwar on (2025-01-23 15:42:26 UTC): should we limit favourites/pins to DAGs or also allow tasks?

bbovenzi on (2025-01-23 22:51:49 UTC): I don't see why not?

"
2758812300,issue,closed,completed,ansible operator support ,"### Description

Airflow running ansible playbook like AWX, I will create PR, but I need help to make it more airflow pluginable.

If you think it is a good job, I can provide more example dags and documents

```
@task()
def prepare_data():
    return ""hello.yaml""
 
@task() 
def prepare_inventory():
    return {} # ansible inventory
 
@task.ansible()
def hello(playbook,inventory): # pylint: disable=unused-argument
    """"""hello""""""
    context = get_current_context()
    task_log.debug(context[""ansible_return""])
    return context[""ansible_return""]
 
@dag(
...
)
def main():
    """"""main""""""
    playbook_data = prepare_data()
    inventory = prepare_inventory()
    hello(playbook=playbook_data,inventory=inventory)
```

### Use case/motivation

run ansible playbook

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",liuzheng,2024-12-25 13:34:51+00:00,[],2025-01-26 03:29:53+00:00,2024-12-30 01:36:25+00:00,https://github.com/apache/airflow/issues/45203,"[('area:plugins', ''), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2561894049, 'issue_id': 2758812300, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 25, 13, 34, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564939345, 'issue_id': 2758812300, 'author': 'liuzheng', 'body': 'I will try the third-party-airflow-plugins-and-providers.', 'created_at': datetime.datetime(2024, 12, 30, 1, 36, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614194231, 'issue_id': 2758812300, 'author': 'liuzheng', 'body': 'https://github.com/liuzheng/airflow-ansible-provider', 'created_at': datetime.datetime(2025, 1, 26, 3, 29, 52, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-25 13:34:54 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

liuzheng (Issue Creator) on (2024-12-30 01:36:25 UTC): I will try the third-party-airflow-plugins-and-providers.

liuzheng (Issue Creator) on (2025-01-26 03:29:52 UTC): https://github.com/liuzheng/airflow-ansible-provider

"
2758339611,issue,closed,completed,"AIP-72  | ""unsafe""  level","### Description

I would like to still be able to write dag with a low level access to airflow database for operational purposes in airflow v3

it's convenient to be able to programmatically access the database session in a pythonoperator for specific cleaning / operational purposes

example 

```python
    def clear_dag_runs(dag_id, status_to_clear):
        context = get_current_context()
        session = settings.Session()

        query = session.query(DagRun).filter(
            DagRun.state == status_to_clear, DagRun.dag_id == dag_id)
        rst = query.all()

        dag_bag = DagBag(dag_folder=path.join(SRC_FOLDER, 'dags'), include_examples=False)
        dag: DAG = dag_bag.get_dag(dag_id, session)

        for dag_run in rst:
            dag.clear(
                start_date=dag_run.logical_date,
                end_date=dag_run.logical_date,
                task_ids=None,
                include_subdags=True,
                include_parentdag=True,
                only_failed=False,
            )
        session.close()

    def delete_dag(dag_id):
        context = get_current_context()
        session = settings.Session()
        keep_records_in_log = False

        for model in get_sqla_model_classes():
            if hasattr(model, ""dag_id"") and (not keep_records_in_log or model.__name__ != ""Log""):
                session.execute(
                    delete(model)
                    .where(model.dag_id.__eq__(dag_id))
                    .execution_options(synchronize_session=""fetch"")
                )

    PythonOperator(
        task_id=""delete"",
        python_callable=delete_dag,
        op_kwargs={""dag_id"": ""{{params.dag_name}}""}
    )


```


wdyt ? 

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",raphaelauv,2024-12-24 22:49:33+00:00,[],2024-12-26 14:10:17+00:00,2024-12-26 13:31:46+00:00,https://github.com/apache/airflow/issues/45200,"[('kind:feature', 'Feature Requests'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2562480526, 'issue_id': 2758339611, 'author': 'potiuk', 'body': ""Not going to happen IMHO. I would definitely be voting NO if it came to a vote. First of all airflow DB is an internal detail. Secondly you have REST APi and Python client and we are going to have built in authentication to work automatically from worker so that you can use Python. Client'Pythonic APi to interact with Airflow DB over REST API and this is the way it is going to be the 'official'way of interacting with Airflow DB. \n\nIf you think you miss some ways of interacting with Airflow DB (do r example bulk updates) - the right approach is to add appropriate API endpoints to do it NOT use the SQL queries on the Database that might and will change any time without any warning.\n\nWhat you are asking for is a step back in security and architecture and I highly doubt any maintainer would support it .\n\nBTW. In many cases using. python Client will produce more readable and simpler code than using sqlqchemy"", 'created_at': datetime.datetime(2024, 12, 26, 11, 15, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562511167, 'issue_id': 2758339611, 'author': 'raphaelauv', 'body': 'Thank you for the answer 👍\n\nWithout security compromise, would it be possible to create a pluggable SDK extension system ?\n\nSo at run time an airflow ""custom_sdk_extension"" or  ""sdk_custom_backend"" that would let the user enrich the API of the SDK (so no need to maintain a custom fork airflow project for the user ) ?\n\nThanks', 'created_at': datetime.datetime(2024, 12, 26, 11, 30, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562551337, 'issue_id': 2758339611, 'author': 'potiuk', 'body': 'Why Python client is not enough ?', 'created_at': datetime.datetime(2024, 12, 26, 11, 51, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562571443, 'issue_id': 2758339611, 'author': 'potiuk', 'body': ""What precisely features you miss and why they cannot be added to standard API as pull request ? The 'extendable sdk' is anti-thesis of having an API. The REST API is designed to make a maintainable, public way of interacting with airflow. What your are asking for is maintainable API to be able to extend rhe API which is 'meta API API'\r\n\r\nThe thing is that you are NOT supposed to be interacting with the DB of Airflow because it might and will change. So if you do it (which you still can do if you really want by configuring your own way of accessing the same DB and writing your own code to do so.  Then you are doing it on absolutely your own risk and cost. The whole thing about exposing the API by maintainers and not giving ANY official way to interacting with the DB that aims for exactly this - to make you solely and ultimately responsible if you access the things we made 'internal' deliberately .\r\n\r\nSo what you are asking for REALLY is for maintainers and community to take responsibility and maintenance cost and burden for accessing airflow db in the ways we are not able to foresee.\r\n\r\nIMHO - this is very clear ask from maintainers:\r\n\r\na) if you see some access patterns that can be reused by others - contribute it back as REST API \r\n\r\nb) if you see something useful only for you - do what you need In your but without impacting community in ways that it will make it more difficult to maintain and make changes. You need to take full responsibility for adapting the changes to future airflow version and it should be clear to you that you are commiting to it\r\n\r\nSimply 'with great powers come great responsibilities' and if you want more power you should take full responsibility for what it means"", 'created_at': datetime.datetime(2024, 12, 26, 12, 1, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562617167, 'issue_id': 2758339611, 'author': 'raphaelauv', 'body': 'Yes you are right, the main pattern that I have in mind is the bulk maintenance of the airflow database ( there are few users, doing >100k task_run by day ).\n\nThanks again for your detailed answer, it will help many users realize that building  directly on top of Airflow a very custom platform ( I seen many cases of users using XCOM as almost a product database 😱, and also querying the airflow database for very very custom orchestration decisioning ) without separation of concern is not a good idea.\n\nI guess that users will surely have intriguing and subtle questions when airflow V3 will be release 😀\n\nThanks again', 'created_at': datetime.datetime(2024, 12, 26, 12, 25, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562706114, 'issue_id': 2758339611, 'author': 'potiuk', 'body': ""I think it's not a problem to build custom platform - the REST API of ours is precisely what's making it possible and it was the main motivator for the API implementation.\r\n\r\nAnd again - if some features are missing there, adding an API endpoint via PR is precisely how you can achieve what you need. \r\n\r\nYou can also export the data somewhere (even using Airflow DAG !) using the API and do whatever you need outside of airflow DB. This is a typical pattern that internal DB needs to be performant and maintainable for the Application using it and if you need to do any other mechanism to access the data and aggregate or process it in other ways you export the data and so it 'outside'.\r\n\r\nAnd yes - you can query and retrieve data using XCOM REST API. And again - interacting with them over the XCOM part of the Python client is even easier than using sqlalchemy or SQL.\r\n\r\nAnd the fact that you should not be using db access had not only been mentioned every time someone brought the subject in slack and GitHub but also very clearly explained in https://airflow.apache.org/docs/apache-airflow/stable/public-airflow-interface.html#what-is-not-part-of-the-public-interface-of-apache-airflow\r\n\r\nWhen we implemented REST API 4 years ago for Airflow 2 we made it very clear that it is the only maintained and supported interface and we gave our users 4 years to adapt.\r\n\r\nSo I think it is not unexpected, it's a very deliberate, very well announced move and we went a great length to add tools and explanations how to use it.\r\n\r\nIf anyone is still not ready for that, well, there is still time to prepare."", 'created_at': datetime.datetime(2024, 12, 26, 13, 17, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562714398, 'issue_id': 2758339611, 'author': 'potiuk', 'body': '> I guess that users will surely have intriguing and subtle questions when airflow V3 will be release 😀\r\n\r\nI hope so. And I hope that it will lead that very users to contribute back whatever they need and miss in Airflow 3 API in a reusable way for others \r\n\r\nThat is one of the intentions we had when we added the API', 'created_at': datetime.datetime(2024, 12, 26, 13, 22, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562721006, 'issue_id': 2758339611, 'author': 'raphaelauv', 'body': 'With airflow 2.11 we will be able to use the sdk , right ?', 'created_at': datetime.datetime(2024, 12, 26, 13, 26, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562792406, 'issue_id': 2758339611, 'author': 'potiuk', 'body': ""> With airflow 2.11 we will be able to use the sdk , right ?\n\nI think it is planned as one of the rules . Certainly we discussed it as an option and we already did something similar for db isolation AIP-44 work - so we already have the right code implemented in v2-10-test branch, we just need to turn the AIP-44 error into deprecation warning \n\nIf it is not already there https://github.com/apache/airflow/issues/41641 - it's.a.good idea to add it there"", 'created_at': datetime.datetime(2024, 12, 26, 14, 10, 16, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-12-26 11:15:54 UTC): Not going to happen IMHO. I would definitely be voting NO if it came to a vote. First of all airflow DB is an internal detail. Secondly you have REST APi and Python client and we are going to have built in authentication to work automatically from worker so that you can use Python. Client'Pythonic APi to interact with Airflow DB over REST API and this is the way it is going to be the 'official'way of interacting with Airflow DB. 

If you think you miss some ways of interacting with Airflow DB (do r example bulk updates) - the right approach is to add appropriate API endpoints to do it NOT use the SQL queries on the Database that might and will change any time without any warning.

What you are asking for is a step back in security and architecture and I highly doubt any maintainer would support it .

BTW. In many cases using. python Client will produce more readable and simpler code than using sqlqchemy

raphaelauv (Issue Creator) on (2024-12-26 11:30:41 UTC): Thank you for the answer 👍

Without security compromise, would it be possible to create a pluggable SDK extension system ?

So at run time an airflow ""custom_sdk_extension"" or  ""sdk_custom_backend"" that would let the user enrich the API of the SDK (so no need to maintain a custom fork airflow project for the user ) ?

Thanks

potiuk on (2024-12-26 11:51:02 UTC): Why Python client is not enough ?

potiuk on (2024-12-26 12:01:44 UTC): What precisely features you miss and why they cannot be added to standard API as pull request ? The 'extendable sdk' is anti-thesis of having an API. The REST API is designed to make a maintainable, public way of interacting with airflow. What your are asking for is maintainable API to be able to extend rhe API which is 'meta API API'

The thing is that you are NOT supposed to be interacting with the DB of Airflow because it might and will change. So if you do it (which you still can do if you really want by configuring your own way of accessing the same DB and writing your own code to do so.  Then you are doing it on absolutely your own risk and cost. The whole thing about exposing the API by maintainers and not giving ANY official way to interacting with the DB that aims for exactly this - to make you solely and ultimately responsible if you access the things we made 'internal' deliberately .

So what you are asking for REALLY is for maintainers and community to take responsibility and maintenance cost and burden for accessing airflow db in the ways we are not able to foresee.

IMHO - this is very clear ask from maintainers:

a) if you see some access patterns that can be reused by others - contribute it back as REST API 

b) if you see something useful only for you - do what you need In your but without impacting community in ways that it will make it more difficult to maintain and make changes. You need to take full responsibility for adapting the changes to future airflow version and it should be clear to you that you are commiting to it

Simply 'with great powers come great responsibilities' and if you want more power you should take full responsibility for what it means

raphaelauv (Issue Creator) on (2024-12-26 12:25:36 UTC): Yes you are right, the main pattern that I have in mind is the bulk maintenance of the airflow database ( there are few users, doing >100k task_run by day ).

Thanks again for your detailed answer, it will help many users realize that building  directly on top of Airflow a very custom platform ( I seen many cases of users using XCOM as almost a product database 😱, and also querying the airflow database for very very custom orchestration decisioning ) without separation of concern is not a good idea.

I guess that users will surely have intriguing and subtle questions when airflow V3 will be release 😀

Thanks again

potiuk on (2024-12-26 13:17:55 UTC): I think it's not a problem to build custom platform - the REST API of ours is precisely what's making it possible and it was the main motivator for the API implementation.

And again - if some features are missing there, adding an API endpoint via PR is precisely how you can achieve what you need. 

You can also export the data somewhere (even using Airflow DAG !) using the API and do whatever you need outside of airflow DB. This is a typical pattern that internal DB needs to be performant and maintainable for the Application using it and if you need to do any other mechanism to access the data and aggregate or process it in other ways you export the data and so it 'outside'.

And yes - you can query and retrieve data using XCOM REST API. And again - interacting with them over the XCOM part of the Python client is even easier than using sqlalchemy or SQL.

And the fact that you should not be using db access had not only been mentioned every time someone brought the subject in slack and GitHub but also very clearly explained in https://airflow.apache.org/docs/apache-airflow/stable/public-airflow-interface.html#what-is-not-part-of-the-public-interface-of-apache-airflow

When we implemented REST API 4 years ago for Airflow 2 we made it very clear that it is the only maintained and supported interface and we gave our users 4 years to adapt.

So I think it is not unexpected, it's a very deliberate, very well announced move and we went a great length to add tools and explanations how to use it.

If anyone is still not ready for that, well, there is still time to prepare.

potiuk on (2024-12-26 13:22:38 UTC): I hope so. And I hope that it will lead that very users to contribute back whatever they need and miss in Airflow 3 API in a reusable way for others 

That is one of the intentions we had when we added the API

raphaelauv (Issue Creator) on (2024-12-26 13:26:44 UTC): With airflow 2.11 we will be able to use the sdk , right ?

potiuk on (2024-12-26 14:10:16 UTC): I think it is planned as one of the rules . Certainly we discussed it as an option and we already did something similar for db isolation AIP-44 work - so we already have the right code implemented in v2-10-test branch, we just need to turn the AIP-44 error into deprecation warning 

If it is not already there https://github.com/apache/airflow/issues/41641 - it's.a.good idea to add it there

"
2757829353,issue,closed,not_planned,"Support AES256-encrypted values for sensitive parameters in configs, env variables etc.","### Description

Support config values like `fernet_key_encrypted`, env variables like `AIRFLOW__CORE__FERNET_KEY_ENCRYPTED` etc. AES256 is chosen since it's [supported by Helm templates](https://helm.sh/docs/chart_template_guide/function_list/#encryptaes) and is quite generic.

### Use case/motivation

The goal is to better support deploying manifests with ArgoCD or similar, where the manifests are stored in Git and thus can't contain secret values due to security reasons. Encrypted values can be stored though, so only the encryption key itself would have to be fetched from a secret, significantly reducing the operational overhead.

### Related issues

https://github.com/apache/airflow/issues/45171

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",andrii-korotkov-verkada,2024-12-24 13:49:18+00:00,[],2024-12-26 18:53:37+00:00,2024-12-26 18:53:36+00:00,https://github.com/apache/airflow/issues/45194,"[('area:secrets', ''), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2563025613, 'issue_id': 2757829353, 'author': 'andrii-korotkov-verkada', 'body': 'After more discussion in https://github.com/apache/airflow/pull/45195, this seems not worth it.', 'created_at': datetime.datetime(2024, 12, 26, 18, 53, 37, tzinfo=datetime.timezone.utc)}]","andrii-korotkov-verkada (Issue Creator) on (2024-12-26 18:53:37 UTC): After more discussion in https://github.com/apache/airflow/pull/45195, this seems not worth it.

"
2757402702,issue,open,,Add option to limit tests and checks to only selected provider (and depending providers),"While preparing `fab-providers/v1-5` branch and making the branch `green` - we had to backport a number of commits from `main` in order to make that older branch of `fab` provider green: See https://github.com/apache/airflow/pull/45185

This is a bit annoying and time-consuming - and also the problem is that just ""running"" the whole suite of docs, tests, checks etc. takes a lot of time in CI.

We could add an option to selective checks to only limit tests to selected provider and it's depending providers in such branch. That should be relatively simple task - where we could add a flag `--limit-to-providers fab` for example and whenever we determine which providers are affected, we shuld just limit it to those that are limited and providers that depend on them.

Then the process of preparing branch for past version of provider would look like this:

1) create a branch
2) add LIMIT_TO_PROVIDERS=`<PROVIDER>` variable in ""ci.yml` where selective checks are run

This could save quite some time on making the branch `green`.

Note! It's likely it will not cover `all` cases - there are certain cases where just test collection might stop working because newer version of dependencies retrieved might cause collection to fail and we will not have a chance to ""skip"" those tests if they fail during collection, but it should severly limit the needs of back porting some main changes for:

a) runnning the tests 
b) runninng the compatibility tests 

It should also significantly speed up docs building time for PRS for such ""old provider versions"" branches.",potiuk,2024-12-24 08:24:32+00:00,[],2024-12-24 10:07:00+00:00,,https://github.com/apache/airflow/issues/45192,"[('kind:feature', 'Feature Requests'), ('area:CI', ""Airflow's tests and continious integration"")]","[{'comment_id': 2560926436, 'issue_id': 2757402702, 'author': 'eladkal', 'body': ""What direction of depended are we talking about?\r\nFor example: If I make changes to presto provider. It has dependency on common.sql so I guess we expect tests for presto and common.sql to run and we have the list of dependencies in the `provider.yaml` of presto, but what about the different direction? Say we change something in common.sql. Souldn't we run also presto tests? To make sure we didn't break anything in the common.sql interface? In this case we don't have the information of presto dependent on common.sql because we scan only the provider.yaml of common.sql."", 'created_at': datetime.datetime(2024, 12, 24, 9, 57, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2560933763, 'issue_id': 2757402702, 'author': 'potiuk', 'body': 'It\'s both directions. This is the same code that we run currently in selective checks in ""regular"" PRs - to be on a safe side we select both sides - the stuff we depend on and the stuff that depends on us (and it\'s recursive).\r\n\r\nhttps://github.com/apache/airflow/blob/main/dev/breeze/src/airflow_breeze/utils/selective_checks.py#L1443\r\n\r\nWe also have a few other rules there that trigger different providers when we know there are implicit dependencies.', 'created_at': datetime.datetime(2024, 12, 24, 10, 4, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2560935356, 'issue_id': 2757402702, 'author': 'potiuk', 'body': 'This is the actual code: https://github.com/apache/airflow/blob/f56038e06af324127e9a9781bcf5e0edb3187071/dev/breeze/src/airflow_breeze/utils/provider_dependencies.py#L31 -> both upstream and downstream cross-provider dependencies are calculated here.', 'created_at': datetime.datetime(2024, 12, 24, 10, 5, 35, tzinfo=datetime.timezone.utc)}]","eladkal on (2024-12-24 09:57:38 UTC): What direction of depended are we talking about?
For example: If I make changes to presto provider. It has dependency on common.sql so I guess we expect tests for presto and common.sql to run and we have the list of dependencies in the `provider.yaml` of presto, but what about the different direction? Say we change something in common.sql. Souldn't we run also presto tests? To make sure we didn't break anything in the common.sql interface? In this case we don't have the information of presto dependent on common.sql because we scan only the provider.yaml of common.sql.

potiuk (Issue Creator) on (2024-12-24 10:04:08 UTC): It's both directions. This is the same code that we run currently in selective checks in ""regular"" PRs - to be on a safe side we select both sides - the stuff we depend on and the stuff that depends on us (and it's recursive).

https://github.com/apache/airflow/blob/main/dev/breeze/src/airflow_breeze/utils/selective_checks.py#L1443

We also have a few other rules there that trigger different providers when we know there are implicit dependencies.

potiuk (Issue Creator) on (2024-12-24 10:05:35 UTC): This is the actual code: https://github.com/apache/airflow/blob/f56038e06af324127e9a9781bcf5e0edb3187071/dev/breeze/src/airflow_breeze/utils/provider_dependencies.py#L31 -> both upstream and downstream cross-provider dependencies are calculated here.

"
2756718227,issue,open,,Better integration between datasets and data intervals,"### Description

Currently, one is able to trigger a DAG based on a dataset or a time schedule or a DatasetOrTimeSchedule, but it would be good if the dataset itself (or dataset event) could be associated with a schedule or logical_date. E.g. a monthly dataset, where an event is emitted by a DAG at most once for a given month, and such that the `catchup` argument of a downstream DAG is respected.

For example a DAG with two dataset dependencies, if dataset 1 has been produced for month1 and dataset2 gets produced for month2, the DAG will be triggered even though the two dataset events relate to separate intervals. I'd like to trigger the DAG only if the datset events were emitted for the same interval.

I'm fairly new to using datasets so apologies if my issue already has a solution or workaround. 

### Use case/motivation

I have a few issues with datasets that I'm having trouble solving:
- A dataset producer DAG gets re-run, but we don't want downstream DAGs to be re-triggered for the same data interval. 
- Out-of-sync issues where a DAG is triggered based on a stale event in cases where multiple dataset triggers are defined: https://github.com/apache/airflow/discussions/36618
- If a producer dag gets run with catchup=True and we don't want consumer DAGs to be backfilled, can we restrict backfill on consumer DAGs.

Technically this could be accomplished with TriggerDagRunOperator/ExternalTaskSensor, but these have other issues that datasets solve quite nicely. The benefit of decoupling DAGs using datasets is huge. However by using datasets, some of the benefits of time schedules are lost.

### Related issues

https://github.com/apache/airflow/discussions/36618

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",casperhart,2024-12-23 21:08:49+00:00,[],2025-02-06 22:23:20+00:00,,https://github.com/apache/airflow/issues/45187,"[('kind:feature', 'Feature Requests'), ('pending-response', ''), ('area:datasets', 'Issues related to the datasets feature')]","[{'comment_id': 2560299324, 'issue_id': 2756718227, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 23, 21, 8, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2560555092, 'issue_id': 2756718227, 'author': 'tirkarthi', 'body': 'cc: @Lee-W @uranusjr', 'created_at': datetime.datetime(2024, 12, 24, 2, 32, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2560776737, 'issue_id': 2756718227, 'author': 'potiuk', 'body': 'cc: @dstandish', 'created_at': datetime.datetime(2024, 12, 24, 7, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2560779839, 'issue_id': 2756718227, 'author': 'potiuk', 'body': 'This is another case where I think ""data interval"" is so established term that we should embrace it, not move away from it (re:  https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-83+amendment+to+support+classic+Airflow+authoring+style) .\r\n\r\n\r\ncc: @casperhart -> I think it would be great if you also incorporate your points in the discussion in that AIP-83 amendment, I think it\'s pretty relevant, and I think it would be valuable to hear from others as well about the cases they think about.', 'created_at': datetime.datetime(2024, 12, 24, 7, 37, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563136469, 'issue_id': 2756718227, 'author': 'casperhart', 'body': ""After skimming through the doc I would say that I agree that logical_date can be a confusing term, especially because the meaning of the logical_date is different depending on if a DAG is scheduled or manually triggered. With a manual trigger it defaults to the trigger date, but for scheduled DAGs it's the same as data_interval_start and so is redundant. But independent of the logical_date, data intervals are incredibly useful and IMHO airflow's most powerful feature. I'll add comments to the doc a bit later.\r\n\r\nBut for this issue specifically, if datasets were better integrated with data intervals and backfill features, that would be very helpful."", 'created_at': datetime.datetime(2024, 12, 26, 22, 21, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563158889, 'issue_id': 2756718227, 'author': 'dstandish', 'body': ""There's a section in that doc that lists some of the issues with data intervals.  They are, as you say, mostly redundant since they are generally derived from logical date.  They are also static i.e. there's no way for the task to record what it actually did.  Also they are at the dag run scope so, it's assumed all tasks are processing the same data.  And there's no way e.g. to backfill a wide range, but rather we are stuck in a partition-driven paradigm where we must create runs for every interval / partition.\r\n\r\nThere's no one-size-fits-all rule that would govern how to map data intervals from a triggering dag to triggered dag with dataset triggers, so when this was being implemented I did not think we should do it.  But we did.  We take min and max.  I suspect most of the time this is not meaningful, and probably not used.\r\n\r\nI think you could make a stronger argument for this kind of thing if instead of listening for a dataset update (which does not have a data interval associated with it per se) if we actually could listen for a dag run event and schedule on _that_, then it could make more sense.\r\n\r\nI think that you probably want to look to the the work that @uranusjr is planning to do with assets to try to implement some of the functionality you are seeking.  I also am a little unclear on the use cases you are trying to explain and I think it would be helpful in aiding others understanding if you could go into more detail for each one cus it's a bit unclear."", 'created_at': datetime.datetime(2024, 12, 26, 23, 8, 51, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-23 21:08:52 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

tirkarthi on (2024-12-24 02:32:24 UTC): cc: @Lee-W @uranusjr

potiuk on (2024-12-24 07:34:00 UTC): cc: @dstandish

potiuk on (2024-12-24 07:37:09 UTC): This is another case where I think ""data interval"" is so established term that we should embrace it, not move away from it (re:  https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-83+amendment+to+support+classic+Airflow+authoring+style) .


cc: @casperhart -> I think it would be great if you also incorporate your points in the discussion in that AIP-83 amendment, I think it's pretty relevant, and I think it would be valuable to hear from others as well about the cases they think about.

casperhart (Issue Creator) on (2024-12-26 22:21:08 UTC): After skimming through the doc I would say that I agree that logical_date can be a confusing term, especially because the meaning of the logical_date is different depending on if a DAG is scheduled or manually triggered. With a manual trigger it defaults to the trigger date, but for scheduled DAGs it's the same as data_interval_start and so is redundant. But independent of the logical_date, data intervals are incredibly useful and IMHO airflow's most powerful feature. I'll add comments to the doc a bit later.

But for this issue specifically, if datasets were better integrated with data intervals and backfill features, that would be very helpful.

dstandish on (2024-12-26 23:08:51 UTC): There's a section in that doc that lists some of the issues with data intervals.  They are, as you say, mostly redundant since they are generally derived from logical date.  They are also static i.e. there's no way for the task to record what it actually did.  Also they are at the dag run scope so, it's assumed all tasks are processing the same data.  And there's no way e.g. to backfill a wide range, but rather we are stuck in a partition-driven paradigm where we must create runs for every interval / partition.

There's no one-size-fits-all rule that would govern how to map data intervals from a triggering dag to triggered dag with dataset triggers, so when this was being implemented I did not think we should do it.  But we did.  We take min and max.  I suspect most of the time this is not meaningful, and probably not used.

I think you could make a stronger argument for this kind of thing if instead of listening for a dataset update (which does not have a data interval associated with it per se) if we actually could listen for a dag run event and schedule on _that_, then it could make more sense.

I think that you probably want to look to the the work that @uranusjr is planning to do with assets to try to implement some of the functionality you are seeking.  I also am a little unclear on the use cases you are trying to explain and I think it would be helpful in aiding others understanding if you could go into more detail for each one cus it's a bit unclear.

"
2756607548,issue,open,,Task Runner Dails to Update rendered_task_instance_fields,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.2

### What happened?

Airflow tasks fail with the following error. It always failed on one specific dag with specific task_id. The error repeats even after every retry. 

It is fixed after I manually clean the rendered_task_instance_fields table using a query like `DELETE FROM rendered_task_instance_fields WHERE dag_id = 'X' AND task_id = 'Y' `

Will upgrade to 2.10.4  fix the problem? 

[2024-12-23, 19:11:10 UTC] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 3122, in _execute_task_with_callbacks
    _update_rtif(ti=self, rendered_fields=rendered_fields)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py"", line 139, in wrapper
    return func(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 1642, in _update_rtif
    RenderedTaskInstanceFields.delete_old_records(ti.task_id, ti.dag_id, session=session)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py"", line 94, in wrapper
    return func(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/renderedtifields.py"", line 271, in delete_old_records
    session.flush()
  File ""/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 3449, in flush
    self._flush(objects)
  File ""/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 3588, in _flush
    with util.safe_reraise():
  File ""/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 211, in raise_
    raise exception
  File ""/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 3549, in _flush
    flush_context.execute()
  File ""/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py"", line 456, in execute
    rec.execute(self)
  File ""/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py"", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File ""/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py"", line 237, in save_obj
    _emit_update_statements(
  File ""/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py"", line 1035, in _emit_update_statements
    raise orm_exc.StaleDataError(
sqlalchemy.orm.exc.StaleDataError: UPDATE statement on table 'rendered_task_instance_fields' expected to update 1 row(s); 0 were matched.

### What you think should happen instead?

_No response_

### How to reproduce

I can only reproduce it my Airflow instance, in this specific dag. 

### Operating System

Azure Kubernetes Service 

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.28.0
apache-airflow-providers-celery==3.8.1
apache-airflow-providers-cncf-kubernetes==8.3.0
apache-airflow-providers-common-compat==1.2.0
apache-airflow-providers-common-io==1.4.0
apache-airflow-providers-common-sql==1.16.0
apache-airflow-providers-docker==3.13.0
apache-airflow-providers-elasticsearch==5.5.0
apache-airflow-providers-fab==1.3.0
apache-airflow-providers-ftp==3.11.0
apache-airflow-providers-google==10.22.0
apache-airflow-providers-grpc==3.6.0
apache-airflow-providers-hashicorp==3.8.0
apache-airflow-providers-http==4.13.0
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-microsoft-azure==10.4.0
apache-airflow-providers-mysql==5.7.0
apache-airflow-providers-odbc==4.7.0
apache-airflow-providers-openlineage==1.11.0
apache-airflow-providers-postgres==5.12.0
apache-airflow-providers-redis==3.8.0
apache-airflow-providers-sendgrid==3.6.0
apache-airflow-providers-sftp==4.11.0
apache-airflow-providers-slack==8.9.0
apache-airflow-providers-smtp==1.8.0
apache-airflow-providers-snowflake==5.7.0
apache-airflow-providers-sqlite==3.9.0
apache-airflow-providers-ssh==3.13.1

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",hbc-acai,2024-12-23 19:26:03+00:00,[],2024-12-23 19:43:56+00:00,,https://github.com/apache/airflow/issues/45186,"[('kind:bug', 'This is a clearly a bug'), ('area:MetaDB', 'Meta Database related issues.'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2755413860,issue,open,,Different behavior for DagBag.dagbag_stats.file on Linux vs Windows,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

On Windows, `dagbag.dagbag_stats.file` contains an absolute path, whereas on Linux, this field contains a path relative to `dagbag.dag_folder`.

### What you think should happen instead?

The behavior should be the same regardless of the OS.

### How to reproduce

Run the below on Windows and Linux and compare the results:
```python
from os import environ
from pathlib import Path

from airflow.models import DagBag

repo_root: Path = next(p for p in Path(__file__).parents if p.name == ""my_proj"")
home: Path = repo_root / ""airflow""
environ[""AIRFLOW_HOME""] = home.as_posix()
dagbag = DagBag(include_examples=False)
```

### Operating System

Win10 + RL9.3

### Versions of Apache Airflow Providers

Irrelevant

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

I believe the reason for this difference is that

```python
file=filepath.replace(settings.DAGS_FOLDER, """"),
```

Doesn't normalize the folder separators, so in the case of 
```python
# These are values copied from the debugger on Win
settings.DAGS_FOLDER == 'C:\\repositories\\my_proj\\airflow/dags'
filepath == ""C:\\repositories\\my_proj\\airflow\\dags\\my_dag.py""
```
nothing gets replaced. A possible solution for this is applying `Path(raw_path).as_posix()` to all paths involved.

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Dev-iL,2024-12-23 07:31:01+00:00,[],2024-12-23 07:31:01+00:00,,https://github.com/apache/airflow/issues/45172,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2755064693,issue,closed,completed,"Support not generating secrets at all, giving an option to rely on other means to get secrets ","### Description

Allow to skip secrets creation completely, providing a way to use other means to get the secret data (e.g. AWS secrets manager with init in `airflowLocalSettings`).

### Use case/motivation

When using ArgoCD to manage manifests, committing secrets to the repo is undesirable due to security reasons. Also, if using AWS secrets manager to store secrets, the Kubernetes secrets won't be necessary.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",andrii-korotkov-verkada,2024-12-23 02:21:39+00:00,[],2025-01-12 08:19:29+00:00,2025-01-12 08:19:29+00:00,https://github.com/apache/airflow/issues/45171,"[('area:secrets', ''), ('kind:feature', 'Feature Requests')]","[{'comment_id': 2560438590, 'issue_id': 2755064693, 'author': 'andrii-korotkov-verkada', 'body': 'A related discussion about storing encrypted secret values https://github.com/apache/airflow/discussions/45190.', 'created_at': datetime.datetime(2024, 12, 23, 23, 46, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563880177, 'issue_id': 2755064693, 'author': 'andrii-korotkov-verkada', 'body': ""This can be mostly done by setting some placeholder values for secret names, as well as not setting env variables on pod templates. A couple of things that can't be configured this way are pgbouncer mounts and pgbouncer stats env variable for connection. I'll add a PR to address that."", 'created_at': datetime.datetime(2024, 12, 27, 17, 7, 31, tzinfo=datetime.timezone.utc)}]","andrii-korotkov-verkada (Issue Creator) on (2024-12-23 23:46:08 UTC): A related discussion about storing encrypted secret values https://github.com/apache/airflow/discussions/45190.

andrii-korotkov-verkada (Issue Creator) on (2024-12-27 17:07:31 UTC): This can be mostly done by setting some placeholder values for secret names, as well as not setting env variables on pod templates. A couple of things that can't be configured this way are pgbouncer mounts and pgbouncer stats env variable for connection. I'll add a PR to address that.

"
2754621710,issue,closed,completed,"Status of testing Providers that were prepared on December 22, 2024","### Body


I have a kind request for all the contributors to the latest provider packages release.
Could you please help us to test the RC versions of the providers?

The guidelines on how to test providers can be found in

[Verify providers by contributors](https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDER_PACKAGES.md#verify-the-release-candidate-by-contributors)

Let us know in the comment, whether the issue is addressed.

Those are providers that require testing as there were some substantial changes introduced:


## Provider [airbyte: 5.0.0rc2](https://pypi.org/project/apache-airflow-providers-airbyte/5.0.0rc2)
   - [ ] [Remove deprecated code from Airbyte provider (#44577)](https://github.com/apache/airflow/pull/44577): @ajitg25
## Provider [alibaba: 3.0.0rc2](https://pypi.org/project/apache-airflow-providers-alibaba/3.0.0rc2)
   - [ ] [Removed deprecated code from Alibaba provider package (#44576)](https://github.com/apache/airflow/pull/44576): @Tushar4432
## Provider [amazon: 9.2.0rc2](https://pypi.org/project/apache-airflow-providers-amazon/9.2.0rc2)
   - [ ] [Add DMS Serverless Operators (#43988)](https://github.com/apache/airflow/pull/43988): @ellisms
   - [ ] [Add fail_on_file_not_exist option to SFTPToS3Operator (#44320)](https://github.com/apache/airflow/pull/44320): @Guaqamole
   - [ ] [Add wait_policy option to EmrCreateJobFlowOperator. (#44055)](https://github.com/apache/airflow/pull/44055): @adrian-adikteev
   - [ ] [Use `S3CopyObjectOperator` in `example_comprehend_document_classifier` (#44160)](https://github.com/apache/airflow/pull/44160): @vincbeck
   - [x] [Remove unnecessary compatibility code in S3 asset import (#44714)](https://github.com/apache/airflow/pull/44714): @potiuk
   - [x] [Remove AIP-44 from taskinstance (#44540)](https://github.com/apache/airflow/pull/44540): @potiuk
   - [x] [Add do_xcom_push documentation in EcsRunTaskOperator (#44440)](https://github.com/apache/airflow/pull/44440): @leonidasefrem
   - [ ] [Move Asset user facing components to task_sdk (#43773)](https://github.com/apache/airflow/pull/43773): @Lee-W
   - [ ] [Set up JWT token authentication in Fast APIs (#42634)](https://github.com/apache/airflow/pull/42634): @vincbeck
   - [x] [Bump to mypy-boto3-appflow and pass without `# type: ignore[arg-type]` (#44115)](https://github.com/apache/airflow/pull/44115): @jx2lee
     Linked issues:
       - [x] [Linked Issue #44111](https://github.com/apache/airflow/issues/44111): @eladkal
## Provider [apache.beam: 6.0.0rc2](https://pypi.org/project/apache-airflow-providers-apache-beam/6.0.0rc2)
   - [ ] [Removed deprecated code from provider Apach.beam (#44700)](https://github.com/apache/airflow/pull/44700): @ajitg25
     Linked issues:
       - [ ] [Linked Issue #44559](https://github.com/apache/airflow/issues/44559): @eladkal
   - [ ] [Fix deferrable mode for BeamRunPythonPipelineOperator (#44386)](https://github.com/apache/airflow/pull/44386): @MaksYermak
## Provider [apache.drill: 3.0.0rc2](https://pypi.org/project/apache-airflow-providers-apache-drill/3.0.0rc2)
   - [ ] [Remove provider deprecations in Apache Drill (#44575)](https://github.com/apache/airflow/pull/44575): @kunaljubce
     Linked issues:
       - [ ] [Linked Issue #44559](https://github.com/apache/airflow/issues/44559): @eladkal
## Provider [apache.druid: 4.0.0rc2](https://pypi.org/project/apache-airflow-providers-apache-druid/4.0.0rc2)
   - [ ] [Remove provider deprecations in Apache Druid (#44765)](https://github.com/apache/airflow/pull/44765): @kunaljubce
     Linked issues:
       - [ ] [Linked Issue #44559](https://github.com/apache/airflow/issues/44559): @eladkal
   - [ ] [Rename execution_date to logical_date across codebase (#43902)](https://github.com/apache/airflow/pull/43902): @sunank200
## Provider [apache.hdfs: 4.7.0rc2](https://pypi.org/project/apache-airflow-providers-apache-hdfs/4.7.0rc2)
   - [x] [Add mTLS support to WebHDFSHook (#44561)](https://github.com/apache/airflow/pull/44561): @markhatch
## Provider [apache.hive: 9.0.0rc2](https://pypi.org/project/apache-airflow-providers-apache-hive/9.0.0rc2)
   - [ ] [Remove deprecations from Apache hive Provider (#44715)](https://github.com/apache/airflow/pull/44715): @vatsrahul1001
   - [ ] [Rename execution_date to logical_date across codebase (#43902)](https://github.com/apache/airflow/pull/43902): @sunank200
## Provider [apache.kafka: 1.7.0rc2](https://pypi.org/project/apache-airflow-providers-apache-kafka/1.7.0rc2)
   - [ ] [FIX add error_cb to `confluent.Consumer` config in `ConsumerFromTopic` (#44307)](https://github.com/apache/airflow/pull/44307): @SuccessMoses
## Provider [apache.livy: 4.0.0rc2](https://pypi.org/project/apache-airflow-providers-apache-livy/4.0.0rc2)
   - [ ] [Remove Provider Deprecations in Apache Livy (#44631)](https://github.com/apache/airflow/pull/44631): @jason810496
     Linked issues:
       - [ ] [Linked Issue #44559](https://github.com/apache/airflow/issues/44559): @eladkal
## Provider [apache.spark: 5.0.0rc2](https://pypi.org/project/apache-airflow-providers-apache-spark/5.0.0rc2)
   - [ ] [Remove deprecated code from apache spark provider (#44567)](https://github.com/apache/airflow/pull/44567): @kunaljubce
     Linked issues:
       - [ ] [Linked Issue #44559](https://github.com/apache/airflow/issues/44559): @eladkal
   - [ ] [Fix failing mypy check on `main` (#44191)](https://github.com/apache/airflow/pull/44191): @kaxil
   - [ ] [spark-submit: replace `principle` by `principal` (#44150)](https://github.com/apache/airflow/pull/44150): @brouberol
     Linked issues:
       - [ ] [Linked Issue #43679](https://github.com/apache/airflow/pull/43679): @brouberol
## Provider [apprise: 2.0.0rc2](https://pypi.org/project/apache-airflow-providers-apprise/2.0.0rc2)
   - [x] [Remove Provider Deprecations in Apprise (#44764)](https://github.com/apache/airflow/pull/44764): @jscheffl
## Provider [arangodb: 2.7.0rc2](https://pypi.org/project/apache-airflow-providers-arangodb/2.7.0rc2)
   - [ ] [Added the ArangoDBCollectionOperator that executes collection operations in a ArangoDB database (#44676)](https://github.com/apache/airflow/pull/44676): @harjeevanmaan
## Provider [atlassian.jira: 3.0.0rc2](https://pypi.org/project/apache-airflow-providers-atlassian-jira/3.0.0rc2)
   - [ ] [Remove Provider Deprecations in Atlassian Jira (#44644)](https://github.com/apache/airflow/pull/44644): @jason810496
     Linked issues:
       - [ ] [Linked Issue #44559](https://github.com/apache/airflow/issues/44559): @eladkal
## Provider [celery: 3.9.0rc2](https://pypi.org/project/apache-airflow-providers-celery/3.9.0rc2)
   - [x] [AIP-81 Move CLI Commands to directories according to Hybrid, Local and Remote (#44538)](https://github.com/apache/airflow/pull/44538): @bugraoz93
     Linked issues:
       - [x] [Linked Issue #44204](https://github.com/apache/airflow/issues/44204): @bugraoz93
   - [x] [Remove AIP-44 configuration from the code (#44454)](https://github.com/apache/airflow/pull/44454): @potiuk
     Linked issues:
       - [x] [Linked Issue #44441](https://github.com/apache/airflow/pull/44441): @potiuk
       - [x] [Linked Issue #44436](https://github.com/apache/airflow/issues/44436): @potiuk
## Provider [cncf.kubernetes: 10.1.0rc2](https://pypi.org/project/apache-airflow-providers-cncf-kubernetes/10.1.0rc2)
   - [ ] [Add logging support for init containers in KubernetesPodOperator (#42498) (#43853)](https://github.com/apache/airflow/pull/43853): @mrk-andreev
   - [ ] [New Kubernetes Kueue Operators (#44568)](https://github.com/apache/airflow/pull/44568): @moiseenkov
   - [ ] [support grouping of log lines for KubernetesPodOperator (#44428)](https://github.com/apache/airflow/pull/44428): @karunpoudel
   - [ ] [Compare k8s executor against alias, not full ExecutorName repr (#44967)](https://github.com/apache/airflow/pull/44967): @o-nikolas
     Linked issues:
       - [ ] [Linked Issue #44931](https://github.com/apache/airflow/pull/44931): @amoghrajesh
   - [x] [Fix failing KubeExecutor tests due to #44710 (#44931)](https://github.com/apache/airflow/pull/44931): @amoghrajesh
## Provider [common.compat: 1.3.0rc2](https://pypi.org/project/apache-airflow-providers-common-compat/1.3.0rc2)
   - [ ] [fix(providers/common/compat): add back add_input_dataset and add_output_dataset to NoOpCollector (#44681)](https://github.com/apache/airflow/pull/44681): @Lee-W
   - [ ] [Fix name of private function in compat provider (#44680)](https://github.com/apache/airflow/pull/44680): @jedcunningham
   - [x] [Remove unnecessary compatibility code in S3 asset import (#44714)](https://github.com/apache/airflow/pull/44714): @potiuk
   - [ ] [Move Asset user facing components to task_sdk (#43773)](https://github.com/apache/airflow/pull/43773): @Lee-W
   - [ ] [Make AssetAliasEvent a class context.py (#44709)](https://github.com/apache/airflow/pull/44709): @uranusjr
   - [x] [Move triggers to standard provider (#43608)](https://github.com/apache/airflow/pull/43608): @gopidesupavan
     Linked issues:
       - [x] [Linked Issue #43641](https://github.com/apache/airflow/issues/43641): @gopidesupavan
## Provider [common.io: 1.5.0rc2](https://pypi.org/project/apache-airflow-providers-common-io/1.5.0rc2)
   - [x] [feat: add OpenLineage support for transfer operators between gcs and local (#44417)](https://github.com/apache/airflow/pull/44417): @kacpermuda
     Linked issues:
       - [x] [Linked Issue #44410](https://github.com/apache/airflow/pull/44410): @kacpermuda
   - [ ] [Move Asset user facing components to task_sdk (#43773)](https://github.com/apache/airflow/pull/43773): @Lee-W
   - [ ] [Migrate pickled data & change XCom value type to JSON (#44166)](https://github.com/apache/airflow/pull/44166): @kaxil
## Provider [common.sql: 1.21.0rc2](https://pypi.org/project/apache-airflow-providers-common-sql/1.21.0rc2)
   - [ ] [Added output_processor parameter to SQLQueryOperator and fixed bug with return_single_query_results handler when None is passed as split_statements (#44781)](https://github.com/apache/airflow/
   - [ ] [Moved common SQL handler methods of common-sql-provider into dedicated module (#43747)](https://github.com/apache/airflow/pull/43747): @dabla
   - [x] [Fix static checks in common SQL hooks (#44930)](https://github.com/apache/airflow/pull/44930): @shahar1
     Linked issues:
       - [x] [Linked Issue #43747](https://github.com/apache/airflow/pull/43747): @dabla
   - [ ] [Remove Provider Deprecations in Common SQL (#44645)](https://github.com/apache/airflow/pull/44645): @jason810496
     Linked issues:
       - [ ] [Linked Issue #44559](https://github.com/apache/airflow/issues/44559): @eladkal
## Provider [databricks: 7.0.0rc2](https://pypi.org/project/apache-airflow-providers-databricks/7.0.0rc2)
   - [ ] [Remove deprecations from Databricks Provider (#44566)](https://github.com/apache/airflow/pull/44566): @vatsrahul1001
   - [ ] [[FIX] Fixed databricks repair run deferrable (#44213)](https://github.com/apache/airflow/pull/44213): @raghvendra-singh1
   - [ ] [fix(providers/databricks): remove additional argument passed to repair_run (#44140)](https://github.com/apache/airflow/pull/44140): @Lee-W
   - [ ] [Added job_clusters as a templated parameter to CreateDatabricksWorkflowOperator (#45022)](https://github.com/apache/airflow/pull/45022): @kunaljubce
     Linked issues:
       - [ ] [Linked Issue #42438](https://github.com/apache/airflow/issues/42438): @sushil-louisa
   - [ ] [Upgrade databricks provider dependency (#43272)](https://github.com/apache/airflow/pull/43272): @dcmshi
   - [ ] [Rename execution_date to logical_date across codebase (#43902)](https://github.com/apache/airflow/pull/43902): @sunank200
## Provider [dbt.cloud: 4.0.0rc2](https://pypi.org/project/apache-airflow-providers-dbt-cloud/4.0.0rc2)
   - [ ] [Remove Provider Deprecations in DBT (#44638)](https://github.com/apache/airflow/pull/44638): @jason810496
     Linked issues:
       - [ ] [Linked Issue #44559](https://github.com/apache/airflow/issues/44559): @eladkal
   - [ ] [utilize map_index for deterministic generation of OpenLineage's run_id (#43936)](https://github.com/apache/airflow/pull/43936): @mobuchowski
   - [ ] [Remove commented breakpoint in dbt provider (#44163)](https://github.com/apache/airflow/pull/44163): @dstandish
   - [ ] [Rename execution_date to logical_date across codebase (#43902)](https://github.com/apache/airflow/pull/43902): @sunank200
## Provider [docker: 4.0.0rc2](https://pypi.org/project/apache-airflow-providers-docker/4.0.0rc2)
   - [x] [Remove Provider Deprecations in Docker (#44583)](https://github.com/apache/airflow/pull/44583): @jscheffl
   - [x] [Fix docker documentation auth url (#44112)](https://github.com/apache/airflow/pull/44112): @gopidesupavan
## Provider [elasticsearch: 6.0.0rc2](https://pypi.org/project/apache-airflow-providers-elasticsearch/6.0.0rc2)
   - [ ] [Remove Provider Deprecations in Elasticsearch (#44629)](https://github.com/apache/airflow/pull/44629): @jason810496
     Linked issues:
       - [ ] [Linked Issue #44559](https://github.com/apache/airflow/issues/44559): @eladkal
   - [ ] [Rename execution_date to logical_date across codebase (#43902)](https://github.com/apache/airflow/pull/43902): @sunank200
## Provider [google: 12.0.0rc2](https://pypi.org/project/apache-airflow-providers-google/12.0.0rc2)
   - [x] [Google provider delete deprecated reaching removal date (December 2024) (#45084)](https://github.com/apache/airflow/pull/45084): @olegkachur-e
   - [ ] [Add Google Vertex AI Feature Store - Feature View Sync Operators, Sensor (#44891)](https://github.com/apache/airflow/pull/44891): @CYarros10
   - [x] [Introduce GCP translation (V3), translate document providers (#44971)](https://github.com/apache/airflow/pull/44971): @olegkachur-e
   - [x] [Introduce gcp advance (V3) API translate native models operators (#44627)](https://github.com/apache/airflow/pull/44627): @olegkachur-e
   - [ ] [Support multiple SQL queries in Dataproc SQL job (#44890)](https://github.com/apache/airflow/pull/44890): @amirmor1
   - [x] [feat: add OpenLineage support for BigQuery Create Table operators (#44783)](https://github.com/apache/airflow/pull/44783): @kacpermuda
   - [x] [feat: add OpenLineage support for S3ToGCSOperator (#44426)](https://github.com/apache/airflow/pull/44426): @kacpermuda
   - [x] [feat: automatically inject OL info into spark job in DataprocSubmitJobOperator (#44477)](https://github.com/apache/airflow/pull/44477): @kacpermuda
   - [x] [Implement AlloyDB operators: create/update/delete clusters (#45027)](https://github.com/apache/airflow/pull/45027): @moiseenkov
   - [ ] [Fix MetastoreHivePartitionSensor failing due to duplicate aliases (#45001)](https://github.com/apache/airflow/pull/45001): @CYarros10
   - [ ] [Fix failing OpenLineage emition for InsertBigQueryOperator  (#44650)](https://github.com/apache/airflow/pull/44650): @spapi17
   - [x] [BigQueryInsertJobOperator: log transient error and check job state before marking task as success (#44279)](https://github.com/apache/airflow/pull/44279): @pankajastro
   - [ ] [FIX make `CloudBatchSubmitJobOperator` fail when job fails (#44425)](https://github.com/apache/airflow/pull/44425): @SuccessMoses
     Linked issues:
       - [ ] [Linked Issue #43744](https://github.com/apache/airflow/issues/43744): @adamszustak
   - [x] [feat: add OpenLineage support for transfer operators between gcs and local (#44417)](https://github.com/apache/airflow/pull/44417): @kacpermuda
     Linked issues:
       - [x] [Linked Issue #44410](https://github.com/apache/airflow/pull/44410): @kacpermuda
   - [x] [chore: remove deprecated BigQuery facets from OpenLineage utils (#44838)](https://github.com/apache/airflow/pull/44838): @kacpermuda
   - [x] [New Kubernetes Kueue Operators (#44568)](https://github.com/apache/airflow/pull/44568): @moiseenkov
   - [ ] [Fix system test for Dataform operators (#44729)](https://github.com/apache/airflow/pull/44729): @MaksYermak
   - [ ] [Deprecate VertexAI PaLM text generative model (#44719)](https://github.com/apache/airflow/pull/44719): @MaksYermak
   - [ ] [Show prominent warning for deprecations in docs (#44479)](https://github.com/apache/airflow/pull/44479): @omkar-foss
   - [x] [chore: unify handling of gcs paths in OpenLineage processes (#44410)](https://github.com/apache/airflow/pull/44410): @kacpermuda
## Provider [hashicorp: 4.0.0rc2](https://pypi.org/project/apache-airflow-providers-hashicorp/4.0.0rc2)
   - [ ] [Removed deprecated code from hashicorp provider (#44598)](https://github.com/apache/airflow/pull/44598): @Prab-27
     Linked issues:
       - [ ] [Linked Issue #44559](https://github.com/apache/airflow/issues/44559): @eladkal
## Provider [http: 5.0.0rc2](https://pypi.org/project/apache-airflow-providers-http/5.0.0rc2)
   - [x] [Remove Provider Deprecations in HTTP (#44542)](https://github.com/apache/airflow/pull/44542): @jscheffl
   - [ ] [[Providers/HTTP] Add adapter parameter to HttpHook to allow custom requests adapters (#44302)](https://github.com/apache/airflow/pull/44302): @jieyao-MilestoneHub
     Linked issues:
       - [ ] [Linked Issue #44285](https://github.com/apache/airflow/issues/44285): @kiaradlf
   - [ ] [Rename execution_date to logical_date across codebase (#43902)](https://github.com/apache/airflow/pull/43902): @sunank200
   - [ ] [Bump `aiohttp` to `3.11.1` (#44036)](https://github.com/apache/airflow/pull/44036): @kaxil
## Provider [jdbc: 5.0.0rc2](https://pypi.org/project/apache-airflow-providers-jdbc/5.0.0rc2)
   - [x] [Remove Provider Deprecations in JDBC (#44662)](https://github.com/apache/airflow/pull/44662): @jscheffl
   - [ ] [Made get_conn in JdbcHook threadsafe to avoid OSError: JVM is already started (#44718)](https://github.com/apache/airflow/pull/44718): @dabla
   - [ ] [Suppress JException when get_autocommit and set_autocommit methods aren't supported on JDBC driver (#43786)](https://github.com/apache/airflow/pull/43786): @dabla
## Provider [jenkins: 4.0.0rc2](https://pypi.org/project/apache-airflow-providers-jenkins/4.0.0rc2)
   - [ ] [Remove Provider Deprecations in Jenkins (#44630)](https://github.com/apache/airflow/pull/44630): @jason810496
     Linked issues:
       - [ ] [Linked Issue #44559](https://github.com/apache/airflow/issues/44559): @eladkal
## Provider [microsoft.azure: 12.0.0rc2](https://pypi.org/project/apache-airflow-providers-microsoft-azure/12.0.0rc2)
   - [ ] [Remove Provider Deprecations in Microsoft-AZURE (#44763)](https://github.com/apache/airflow/pull/44763): @vatsrahul1001
   - [ ] [Added test-case for callable values in path and query parameters of MSGraphAsyncOperator (#43799)](https://github.com/apache/airflow/pull/43799): @dabla
   - [x] [PowerBIDatasetRefreshOperator should fail when refresh fails (#44696)](https://github.com/apache/airflow/pull/44696): @Ohashiro
   - [x] [Move triggers to standard provider (#43608)](https://github.com/apache/airflow/pull/43608): @gopidesupavan
     Linked issues:
       - [x] [Linked Issue #43641](https://github.com/apache/airflow/issues/43641): @gopidesupavan
   - [ ] [Remove XCom pickling (#43905)](https://github.com/apache/airflow/pull/43905): @kaxil
   - [ ] [Update path of example dags in docs (#45069)](https://github.com/apache/airflow/pull/45069): @eladkal
   - [ ] [Avoid 1.1.8 version of msgraph-core (#45044)](https://github.com/apache/airflow/pull/45044): @potiuk
   - [ ] [Added MS Graph connection type (#45006)](https://github.com/apache/airflow/pull/45006): @dabla
## Provider [microsoft.mssql: 4.0.0rc2](https://pypi.org/project/apache-airflow-providers-microsoft-mssql/4.0.0rc2)
   - [x] [Remove Provider Deprecations in Microsoft-MSSQL (#44762)](https://github.com/apache/airflow/pull/44762): @jscheffl
   - [x] [Support connection extra parameters in MsSqlHook (#44310)](https://github.com/apache/airflow/pull/44310): @jx2lee
     Linked issues:
       - [x] [Linked Issue #43798](https://github.com/apache/airflow/issues/43798): @ilarionkuleshov
## Provider [microsoft.psrp: 3.0.0rc2](https://pypi.org/project/apache-airflow-providers-microsoft-psrp/3.0.0rc2)
   - [x] [Remove Provider Deprecations in Microsoft-PSRP (#44761)](https://github.com/apache/airflow/pull/44761): @jscheffl
## Provider [microsoft.winrm: 3.7.0rc2](https://pypi.org/project/apache-airflow-providers-microsoft-winrm/3.7.0rc2)
   - [ ] [Remove XCom pickling (#43905)](https://github.com/apache/airflow/pull/43905): @kaxil
## Provider [mongo: 5.0.0rc2](https://pypi.org/project/apache-airflow-providers-mongo/5.0.0rc2)
   - [ ] [Remove Provider Deprecations in Mongo (#44632)](https://github.com/apache/airflow/pull/44632): @jason810496
     Linked issues:
       - [ ] [Linked Issue #44559](https://github.com/apache/airflow/issues/44559): @eladkal
## Provider [mysql: 6.0.0rc2](https://pypi.org/project/apache-airflow-providers-mysql/6.0.0rc2)
   - [x] [Remove Provider Deprecations in MySQL (#44665)](https://github.com/apache/airflow/pull/44665): @jscheffl
   - [ ] [Fix error file not found. tmp file is deleted before inserting rows to DB in VerticaToMySQLOperator bulk  (#44028)](https://github.com/apache/airflow/pull/44028): @bareketamir
   - [ ] [Add basic asyncio support (#43944)](https://github.com/apache/airflow/pull/43944): @dstandish
## Provider [openlineage: 2.0.0rc2](https://pypi.org/project/apache-airflow-providers-openlineage/2.0.0rc2)
   - [ ] [Remove Provider Deprecations in OpenLineage (#44636)](https://github.com/apache/airflow/pull/44636): @jason810496
     Linked issues:
       - [ ] [Linked Issue #44559](https://github.com/apache/airflow/issues/44559): @eladkal
   - [ ] [add clear_number to OpenLineage's dagrun-level event run id generation (#44617)](https://github.com/apache/airflow/pull/44617): @mobuchowski
   - [ ] [utilize map_index for deterministic generation of OpenLineage's run_id (#43936)](https://github.com/apache/airflow/pull/43936): @mobuchowski
   - [x] [feat: automatically inject OL info into spark job in DataprocSubmitJobOperator (#44477)](https://github.com/apache/airflow/pull/44477): @kacpermuda
   - [ ] [add basic system tests for OpenLineage (#43643)](https://github.com/apache/airflow/pull/43643): @mobuchowski
   - [ ] [Move Asset user facing components to task_sdk (#43773)](https://github.com/apache/airflow/pull/43773): @Lee-W
   - [ ] [Rename execution_date to logical_date across codebase (#43902)](https://github.com/apache/airflow/pull/43902): @sunank200
## Provider [opensearch: 1.6.0rc2](https://pypi.org/project/apache-airflow-providers-opensearch/1.6.0rc2)
   - [ ] [Rename execution_date to logical_date across codebase (#43902)](https://github.com/apache/airflow/pull/43902): @sunank200
## Provider [oracle: 4.0.0rc2](https://pypi.org/project/apache-airflow-providers-oracle/4.0.0rc2)
   - [x] [Remove Provider Deprecations in Oracle (#44704)](https://github.com/apache/airflow/pull/44704): @jscheffl
## Provider [pagerduty: 4.0.0rc2](https://pypi.org/project/apache-airflow-providers-pagerduty/4.0.0rc2)
   - [ ] [Remove deprecated code from Pagerduty provider  (#44653)](https://github.com/apache/airflow/pull/44653): @Prab-27
     Linked issues:
       - [ ] [Linked Issue #44559](https://github.com/apache/airflow/issues/44559): @eladkal
## Provider [postgres: 6.0.0rc2](https://pypi.org/project/apache-airflow-providers-postgres/6.0.0rc2)
   - [x] [Remove Provider Deprecations in Postgres (#44705)](https://github.com/apache/airflow/pull/44705): @jscheffl
   - [ ] [Add basic asyncio support (#43944)](https://github.com/apache/airflow/pull/43944): @dstandish
## Provider [presto: 5.8.0rc2](https://pypi.org/project/apache-airflow-providers-presto/5.8.0rc2)
   - [ ] [Rename execution_date to logical_date across codebase (#43902)](https://github.com/apache/airflow/pull/43902): @sunank200
## Provider [redis: 4.0.0rc2](https://pypi.org/project/apache-airflow-providers-redis/4.0.0rc2)
   - [ ] [Remove Provider Deprecations in Redis (#44633)](https://github.com/apache/airflow/pull/44633): @jason810496
     Linked issues:
       - [ ] [Linked Issue #44559](https://github.com/apache/airflow/issues/44559): @eladkal
## Provider [sendgrid: 4.0.0rc2](https://pypi.org/project/apache-airflow-providers-sendgrid/4.0.0rc2)
   - [ ] [Remove Provider Deprecations in SendGrid (#44637)](https://github.com/apache/airflow/pull/44637): @jason810496
     Linked issues:
       - [ ] [Linked Issue #44559](https://github.com/apache/airflow/issues/44559): @eladkal
## Provider [sftp: 5.0.0rc2](https://pypi.org/project/apache-airflow-providers-sftp/5.0.0rc2)
   - [ ] [Remove deprecations from SFTP Provider (#44740)](https://github.com/apache/airflow/pull/44740): @vatsrahul1001
   - [x] [feat: retrieve sftp file attrs onces instead multiple time (#44625)](https://github.com/apache/airflow/pull/44625): @dondaum
   - [ ] [Add host_proxy_cmd parameter to SSHHook and SFTPHook (#44565)](https://github.com/apache/airflow/pull/44565): @ajitg25
## Provider [slack: 9.0.0rc2](https://pypi.org/project/apache-airflow-providers-slack/9.0.0rc2)
   - [ ] [Remove deprecations from Slack Provider (#44693)](https://github.com/apache/airflow/pull/44693): @vatsrahul1001
## Provider [snowflake: 6.0.0rc2](https://pypi.org/project/apache-airflow-providers-snowflake/6.0.0rc2)
   - [ ] [Remove deprecations from Snowflake Provider (#44756)](https://github.com/apache/airflow/pull/44756): @vatsrahul1001
   - [x] [[Snowflake] enable client_store_temporary_credential for snowflake provider (#44431)](https://github.com/apache/airflow/pull/44431): @yzliao
   - [ ] [Allow `json_result_force_utf8_encoding` specification in `providers.snowflake.hooks.SnowflakeHook` extra dict (#44264)](https://github.com/apache/airflow/pull/44264): @ttzhou
   - [ ] [Make host/port configurable for Snowflake connections (#44079)](https://github.com/apache/airflow/pull/44079): @whummer
## Provider [sqlite: 4.0.0rc2](https://pypi.org/project/apache-airflow-providers-sqlite/4.0.0rc2)
   - [x] [Remove Provider Deprecations in Sqlite (#44707)](https://github.com/apache/airflow/pull/44707): @jscheffl
   - [ ] [Add basic asyncio support (#43944)](https://github.com/apache/airflow/pull/43944): @dstandish
## Provider [ssh: 4.0.0rc2](https://pypi.org/project/apache-airflow-providers-ssh/4.0.0rc2)
   - [x] [Remove Provider Deprecations in SSH (#44544)](https://github.com/apache/airflow/pull/44544): @jscheffl
   - [ ] [Add host_proxy_cmd parameter to SSHHook and SFTPHook (#44565)](https://github.com/apache/airflow/pull/44565): @ajitg25
   - [ ] [Remove XCom pickling (#43905)](https://github.com/apache/airflow/pull/43905): @kaxil
## Provider [standard: 0.0.3rc2](https://pypi.org/project/apache-airflow-providers-standard/0.0.3rc2)
   - [x] [Remove Provider Deprecations in Standard (#44541)](https://github.com/apache/airflow/pull/44541): @jscheffl
   - [ ] [Add backward compatibility check for StartTriggerArgs import in filesystem sensor (#44458)](https://github.com/apache/airflow/pull/44458): @kunaljubce
     Linked issues:
       - [ ] [Linked Issue #44324](https://github.com/apache/airflow/issues/44324): @eladkal
   - [x] [Remove Pydanitc models introduced for AIP-44 (#44552)](https://github.com/apache/airflow/pull/44552): @potiuk
     Linked issues:
       - [x] [Linked Issue #44436](https://github.com/apache/airflow/issues/44436): @potiuk
   - [ ] [Deferrable sensors can implement sensor timeout (#33718)](https://github.com/apache/airflow/pull/33718): @dstandish
   - [x] [Remove AIP-44 code from renderedtifields.py (#44546)](https://github.com/apache/airflow/pull/44546): @potiuk
     Linked issues:
       - [x] [Linked Issue #44436](https://github.com/apache/airflow/issues/44436): @potiuk
   - [x] [Remove AIP-44 from taskinstance (#44540)](https://github.com/apache/airflow/pull/44540): @potiuk
   - [x] [Move `LatestOnlyOperator` operator to standard provider. (#44309)](https://github.com/apache/airflow/pull/44309): @hardeybisey
     Linked issues:
       - [x] [Linked Issue #43641](https://github.com/apache/airflow/issues/43641): @gopidesupavan
   - [x] [Remove AIP-44 configuration from the code (#44454)](https://github.com/apache/airflow/pull/44454): @potiuk
     Linked issues:
       - [x] [Linked Issue #44441](https://github.com/apache/airflow/pull/44441): @potiuk
       - [x] [Linked Issue #44436](https://github.com/apache/airflow/issues/44436): @potiuk
   - [ ] [Move external task sensor to standard provider (#44288)](https://github.com/apache/airflow/pull/44288): @kunaljubce
     Linked issues:
       - [ ] [Linked Issue #43641](https://github.com/apache/airflow/issues/43641): @gopidesupavan
   - [x] [Move triggers to standard provider (#43608)](https://github.com/apache/airflow/pull/43608): @gopidesupavan
     Linked issues:
       - [x] [Linked Issue #43641](https://github.com/apache/airflow/issues/43641): @gopidesupavan
## Provider [tableau: 5.0.0rc2](https://pypi.org/project/apache-airflow-providers-tableau/5.0.0rc2)
   - [ ] [Remove deprecations from Tableau Provider  (#44757)](https://github.com/apache/airflow/pull/44757): @vatsrahul1001
## Provider [telegram: 4.7.0rc2](https://pypi.org/project/apache-airflow-providers-telegram/4.7.0rc2)
   - [x] [Feat: telegram send file Operator and Hook (#44040)](https://github.com/apache/airflow/pull/44040): @HaKkaz
## Provider [teradata: 3.0.0rc2](https://pypi.org/project/apache-airflow-providers-teradata/3.0.0rc2)
   - [ ] [Remove deprecations from Teradata Provider (#44746)](https://github.com/apache/airflow/pull/44746): @vatsrahul1001
## Provider [trino: 6.0.0rc2](https://pypi.org/project/apache-airflow-providers-trino/6.0.0rc2)
   - [ ] [Remove Provider Deprecations in Trino  (#44717)](https://github.com/apache/airflow/pull/44717): @Prab-27
     Linked issues:
       - [ ] [Linked Issue #44559](https://github.com/apache/airflow/issues/44559): @eladkal
   - [ ] [Rename execution_date to logical_date across codebase (#43902)](https://github.com/apache/airflow/pull/43902): @sunank200
## Provider [vertica: 4.0.0rc2](https://pypi.org/project/apache-airflow-providers-vertica/4.0.0rc2)
   - [x] [Remove Provider Deprecations in Vertica (#44748)](https://github.com/apache/airflow/pull/44748): @jscheffl
## Provider [weaviate: 3.0.0rc2](https://pypi.org/project/apache-airflow-providers-weaviate/3.0.0rc2)
   - [ ] [Remove deprecations from Weaviate Provider (#44745)](https://github.com/apache/airflow/pull/44745): @vatsrahul1001
## Provider [yandex: 4.0.0rc2](https://pypi.org/project/apache-airflow-providers-yandex/4.0.0rc2)
   - [ ] [Remove Provider Deprecations in Yandex provider (#44754)](https://github.com/apache/airflow/pull/44754): @rawwar

<!--

NOTE TO RELEASE MANAGER:

You can move here the providers that have doc-only changes or for which changes are trivial, and
you could assess that they are OK.

-->
All users involved in the PRs:
@kaxil @raghvendra-singh1 @uranusjr @Lee-W @mobuchowski @leonidasefrem @adrian-adikteev @dstandish @omkar-foss @HaKkaz @jedcunningham @shahar1 @potiuk @mrk-andreev @hardeybisey @o-nikolas @Tushar4432 @sunank200



### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2024-12-22 11:55:55+00:00,[],2024-12-28 21:47:21+00:00,2024-12-26 23:04:28+00:00,https://github.com/apache/airflow/issues/45148,"[('area:providers', ''), ('kind:meta', 'High-level information important to the community'), ('testing status', 'Status of testing releases')]","[{'comment_id': 2558582176, 'issue_id': 2754621710, 'author': 'jscheffl', 'body': 'Checked the deprecations which were my main contributions and all seem to be OK.', 'created_at': datetime.datetime(2024, 12, 22, 20, 10, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558620566, 'issue_id': 2754621710, 'author': 'potiuk', 'body': 'All my removals checked. All looks good !', 'created_at': datetime.datetime(2024, 12, 22, 22, 9, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2559050096, 'issue_id': 2754621710, 'author': 'markhatch', 'body': '- apache.hdfs: 4.7.0rc2\r\n\r\nTested WebHDFS hook with mTLS - looks good.', 'created_at': datetime.datetime(2024, 12, 23, 7, 14, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2559051716, 'issue_id': 2754621710, 'author': 'amoghrajesh', 'body': 'https://github.com/apache/airflow/pull/44931 works as expected, niko reworked it and even that fix works as expected.', 'created_at': datetime.datetime(2024, 12, 23, 7, 15, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2559254064, 'issue_id': 2754621710, 'author': 'moiseenkov', 'body': 'Hi,\r\n#44568 and #45027 work as expected', 'created_at': datetime.datetime(2024, 12, 23, 9, 5, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2559262799, 'issue_id': 2754621710, 'author': 'Ohashiro', 'body': 'Hello\r\nI think https://github.com/apache/airflow/pull/44696 works well', 'created_at': datetime.datetime(2024, 12, 23, 9, 10, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2559434935, 'issue_id': 2754621710, 'author': 'hardeybisey', 'body': 'Hi, #44309 works as expected.', 'created_at': datetime.datetime(2024, 12, 23, 10, 51, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2559467724, 'issue_id': 2754621710, 'author': 'kacpermuda', 'body': 'All my changes look good and produce expected results with some sample DAGs.', 'created_at': datetime.datetime(2024, 12, 23, 11, 7, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2559637430, 'issue_id': 2754621710, 'author': 'dondaum', 'body': ""Tested #44625. All good.\r\n\r\nOne thing. It seems that some contributors, including myself, are somehow not tagged correctly. That's why I didn't get a notification."", 'created_at': datetime.datetime(2024, 12, 23, 12, 41, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2559665928, 'issue_id': 2754621710, 'author': 'jx2lee', 'body': '#44115 \r\n#44310\r\nworks fine!', 'created_at': datetime.datetime(2024, 12, 23, 12, 59, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2559672121, 'issue_id': 2754621710, 'author': 'potiuk', 'body': ""> One thing. It seems that some contributors, including myself, are somehow not tagged correctly. That's why I didn't get a notification.\r\n\r\nInteresting seems it's only @dondaum @yzliao @HaKkaz @hardeybisey - interesting why, as it's not something that we have control over."", 'created_at': datetime.datetime(2024, 12, 23, 13, 3, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2559674237, 'issue_id': 2754621710, 'author': 'potiuk', 'body': 'Likely some intermittent GitHub Issue - now all the users are tagged properly when mentioned....', 'created_at': datetime.datetime(2024, 12, 23, 13, 4, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2560166992, 'issue_id': 2754621710, 'author': 'gopidesupavan', 'body': 'Verified my changes all looks good.', 'created_at': datetime.datetime(2024, 12, 23, 18, 57, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2560375032, 'issue_id': 2754621710, 'author': 'yzliao', 'body': 'Verified Snowflake provider and all LGTM. My change https://github.com/apache/airflow/pull/44431 is good to go.', 'created_at': datetime.datetime(2024, 12, 23, 22, 20, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561226267, 'issue_id': 2754621710, 'author': 'pankajastro', 'body': 'https://github.com/apache/airflow/pull/44279 looks good.', 'created_at': datetime.datetime(2024, 12, 24, 15, 19, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561274512, 'issue_id': 2754621710, 'author': 'olegkachur-e', 'body': 'The #44971 <https://github.com/apache/airflow/pull/44971>, #44627\n<https://github.com/apache/airflow/pull/44627>,  #45084\n<https://github.com/apache/airflow/pull/45084> looks good.\nThank you!', 'created_at': datetime.datetime(2024, 12, 24, 16, 29, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562942645, 'issue_id': 2754621710, 'author': 'HaKkaz', 'body': '#44040 looks good.', 'created_at': datetime.datetime(2024, 12, 26, 16, 43, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563109196, 'issue_id': 2754621710, 'author': 'bugraoz93', 'body': 'I verified my changes (in #44204) and they look good. Thanks!', 'created_at': datetime.datetime(2024, 12, 26, 21, 24, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563156500, 'issue_id': 2754621710, 'author': 'eladkal', 'body': 'Thank you everyone. Providers are released.\r\n\r\nI invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).', 'created_at': datetime.datetime(2024, 12, 26, 23, 4, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563457938, 'issue_id': 2754621710, 'author': 'dabla', 'body': ""Following have al been tested and are working as expected.\r\n \r\n Moved common SQL handler methods of common-sql-provider into dedicated module (#[43747](https://github.com/apache/airflow/pull/43747)): @dabla\r\n Made get_conn in JdbcHook threadsafe to avoid OSError: JVM is already started (#[44718](https://github.com/apache/airflow/pull/44718)): @dabla\r\n Suppress JException when get_autocommit and set_autocommit methods aren't supported on JDBC driver (#[43786](https://github.com/apache/airflow/pull/43786)): @dabla\r\n Added test-case for callable values in path and query parameters of MSGraphAsyncOperator (#[43799](https://github.com/apache/airflow/pull/43799)): @dabla\r\n Added MS Graph connection type (#[45006](https://github.com/apache/airflow/pull/45006)): @dabla"", 'created_at': datetime.datetime(2024, 12, 27, 8, 34, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564489109, 'issue_id': 2754621710, 'author': 'mrk-andreev', 'body': 'Add logging support for init containers in KubernetesPodOperator (#42498) (#43853): @mrk-andreev  verified\r\n\r\n[Apache Airflow _ Add logging support for init containers in KubernetesPodOperator.pdf](https://github.com/user-attachments/files/18267798/Apache.Airflow._.Add.logging.support.for.init.containers.in.KubernetesPodOperator.pdf)', 'created_at': datetime.datetime(2024, 12, 28, 21, 47, 20, tzinfo=datetime.timezone.utc)}]","jscheffl on (2024-12-22 20:10:57 UTC): Checked the deprecations which were my main contributions and all seem to be OK.

potiuk on (2024-12-22 22:09:09 UTC): All my removals checked. All looks good !

markhatch on (2024-12-23 07:14:12 UTC): - apache.hdfs: 4.7.0rc2

Tested WebHDFS hook with mTLS - looks good.

amoghrajesh on (2024-12-23 07:15:34 UTC): https://github.com/apache/airflow/pull/44931 works as expected, niko reworked it and even that fix works as expected.

moiseenkov on (2024-12-23 09:05:42 UTC): Hi,
#44568 and #45027 work as expected

Ohashiro on (2024-12-23 09:10:47 UTC): Hello
I think https://github.com/apache/airflow/pull/44696 works well

hardeybisey on (2024-12-23 10:51:53 UTC): Hi, #44309 works as expected.

kacpermuda on (2024-12-23 11:07:50 UTC): All my changes look good and produce expected results with some sample DAGs.

dondaum on (2024-12-23 12:41:15 UTC): Tested #44625. All good.

One thing. It seems that some contributors, including myself, are somehow not tagged correctly. That's why I didn't get a notification.

jx2lee on (2024-12-23 12:59:40 UTC): #44115 
#44310
works fine!

potiuk on (2024-12-23 13:03:36 UTC): Interesting seems it's only @dondaum @yzliao @HaKkaz @hardeybisey - interesting why, as it's not something that we have control over.

potiuk on (2024-12-23 13:04:54 UTC): Likely some intermittent GitHub Issue - now all the users are tagged properly when mentioned....

gopidesupavan on (2024-12-23 18:57:49 UTC): Verified my changes all looks good.

yzliao on (2024-12-23 22:20:22 UTC): Verified Snowflake provider and all LGTM. My change https://github.com/apache/airflow/pull/44431 is good to go.

pankajastro on (2024-12-24 15:19:28 UTC): https://github.com/apache/airflow/pull/44279 looks good.

olegkachur-e on (2024-12-24 16:29:34 UTC): The #44971 <https://github.com/apache/airflow/pull/44971>, #44627
<https://github.com/apache/airflow/pull/44627>,  #45084
<https://github.com/apache/airflow/pull/45084> looks good.
Thank you!

HaKkaz on (2024-12-26 16:43:59 UTC): #44040 looks good.

bugraoz93 on (2024-12-26 21:24:41 UTC): I verified my changes (in #44204) and they look good. Thanks!

eladkal (Issue Creator) on (2024-12-26 23:04:28 UTC): Thank you everyone. Providers are released.

I invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).

dabla on (2024-12-27 08:34:23 UTC): Following have al been tested and are working as expected.
 
 Moved common SQL handler methods of common-sql-provider into dedicated module (#[43747](https://github.com/apache/airflow/pull/43747)): @dabla
 Made get_conn in JdbcHook threadsafe to avoid OSError: JVM is already started (#[44718](https://github.com/apache/airflow/pull/44718)): @dabla
 Suppress JException when get_autocommit and set_autocommit methods aren't supported on JDBC driver (#[43786](https://github.com/apache/airflow/pull/43786)): @dabla
 Added test-case for callable values in path and query parameters of MSGraphAsyncOperator (#[43799](https://github.com/apache/airflow/pull/43799)): @dabla
 Added MS Graph connection type (#[45006](https://github.com/apache/airflow/pull/45006)): @dabla

mrk-andreev on (2024-12-28 21:47:20 UTC): Add logging support for init containers in KubernetesPodOperator (#42498) (#43853): @mrk-andreev  verified

[Apache Airflow _ Add logging support for init containers in KubernetesPodOperator.pdf](https://github.com/user-attachments/files/18267798/Apache.Airflow._.Add.logging.support.for.init.containers.in.KubernetesPodOperator.pdf)

"
2754580789,issue,closed,not_planned,Add `allowed_run_types` parameter to Dag class,"### Body

Some workflows are designed to be purely automatic. We should allow the capability for DAG author to declare DAG as scheduled run only. This means:
1. Create manual run button from the UI will generate error / be disabled.
2. Create manual run from API will generate error.
3. Scheduler will not run tasks of manual DagRuns if the parameter is set.

To allow flexibility I'm proposing the `allowed_run_types`  should be enum `['Scheduled', 'Backfill', 'Manual']` where user choose what to set. Thus we need to define the set of allowed rules per type.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2024-12-22 10:00:53+00:00,[],2025-01-06 18:08:17+00:00,2025-01-06 18:08:17+00:00,https://github.com/apache/airflow/issues/45146,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2558518554, 'issue_id': 2754580789, 'author': 'jason810496', 'body': 'Hi @eladkal, I can work on this issue, could you assign to me ? Thanks !\r\nI also have a question: Does this mean that `allowed_run_types` should accept `DagRunType` instead of creating a new enum class in the `models.dag` module?', 'created_at': datetime.datetime(2024, 12, 22, 16, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558519312, 'issue_id': 2754580789, 'author': 'potiuk', 'body': ""> To allow flexibility I'm proposing the `allowed_run_types` should be enum `['Scheduled', 'Backfill', 'Manual']` where user choose what to set. Thus we need to define the set of allowed rules per type.\r\n\r\nI have a feeling that this one might be somewhat impacted by decision on what we do here https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-83+amendment+to+support+classic+Airflow+authoring+style - that seems very much related."", 'created_at': datetime.datetime(2024, 12, 22, 17, 0, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558519462, 'issue_id': 2754580789, 'author': 'potiuk', 'body': 'So probably we should wait until decision for this one happens', 'created_at': datetime.datetime(2024, 12, 22, 17, 1, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2573642887, 'issue_id': 2754580789, 'author': 'eladkal', 'body': 'duplicate of https://github.com/apache/airflow/issues/40990', 'created_at': datetime.datetime(2025, 1, 6, 18, 8, 17, tzinfo=datetime.timezone.utc)}]","jason810496 on (2024-12-22 16:58:00 UTC): Hi @eladkal, I can work on this issue, could you assign to me ? Thanks !
I also have a question: Does this mean that `allowed_run_types` should accept `DagRunType` instead of creating a new enum class in the `models.dag` module?

potiuk on (2024-12-22 17:00:38 UTC): I have a feeling that this one might be somewhat impacted by decision on what we do here https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-83+amendment+to+support+classic+Airflow+authoring+style - that seems very much related.

potiuk on (2024-12-22 17:01:03 UTC): So probably we should wait until decision for this one happens

eladkal (Issue Creator) on (2025-01-06 18:08:17 UTC): duplicate of https://github.com/apache/airflow/issues/40990

"
2754577909,issue,closed,completed,K8S tests are flaky with test_integration_run_dag_with_scheduler_failurE tests,"THE test_integration_run_dag_with_scheduler_failure  is flaky and fails sometimes.

For example https://github.com/apache/airflow/actions/runs/12452312988/job/34761457306

```python
kubernetes_tests/test_other_executors.py .F                              [100%]
  
  =================================== FAILURES ===================================
  __ TestCeleryAndLocalExecutor.test_integration_run_dag_with_scheduler_failure __
  
  self = <kubernetes_tests.test_other_executors.TestCeleryAndLocalExecutor object at 0x7f7c8d269f70>
  
      @pytest.mark.xfail(
          EXECUTOR == ""LocalExecutor"",
          reason=""https://github.com/apache/airflow/issues/44481 needs to be implemented"",
      )
      def test_integration_run_dag_with_scheduler_failure(self):
          dag_id = ""example_xcom""
      
          dag_run_id, logical_date = self.start_job_in_kubernetes(dag_id, self.host)
      
          self._delete_airflow_pod(""scheduler"")
      
          time.sleep(10)  # give time for pod to restart
      
          # Wait some time for the operator to complete
  >       self.monitor_task(
              host=self.host,
              dag_run_id=dag_run_id,
              dag_id=dag_id,
              task_id=""push"",
              expected_final_state=""success"",
              timeout=40,  # This should fail fast if failing
          )
  
  kubernetes_tests/test_other_executors.py:71: 
  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
  
  self = <kubernetes_tests.test_other_executors.TestCeleryAndLocalExecutor object at 0x7f7c8d269f70>
  host = 'localhost:41460'
  dag_run_id = 'manual__2024-12-22T08:49:39.[1302](https://github.com/apache/airflow/actions/runs/12452312988/job/34761457306#step:10:1304)07+00:00', dag_id = 'example_xcom'
  task_id = 'push', expected_final_state = 'success', timeout = 40
  
      def monitor_task(self, host, dag_run_id, dag_id, task_id, expected_final_state, timeout):
          tries = 0
          state = """"
          max_tries = max(int(timeout / 5), 1)
          # Wait some time for the operator to complete
          while tries < max_tries:
              time.sleep(5)
              # Check task state
              try:
                  get_string = (
                      f""http://{host}/api/v1/dags/{dag_id}/dagRuns/{dag_run_id}/taskInstances/{task_id}""
                  )
                  print(f""Calling [monitor_task]#1 {get_string}"")
                  result = self.session.get(get_string)
                  if result.status_code == 404:
                      check_call([""echo"", ""api returned 404.""])
                      tries += 1
                      continue
                  assert result.status_code == 200, ""Could not get the status""
                  result_json = result.json()
                  print(f""Received [monitor_task]#2: {result_json}"")
                  state = result_json[""state""]
                  print(f""Attempt {tries}: Current state of operator is {state}"")
      
                  if state == expected_final_state:
                      break
                  if state in {""failed"", ""upstream_failed"", ""removed""}:
                      # If the TI is in failed state (and that's not the state we want) there's no point
                      # continuing to poll, it won't change
                      break
                  self._describe_resources(namespace=""airflow"")
                  self._describe_resources(namespace=""default"")
                  tries += 1
              except requests.exceptions.ConnectionError as e:
                  check_call([""echo"", f""api call failed. trying again. error {e}""])
          if state != expected_final_state:
              print(f""The expected state is wrong {state} != {expected_final_state} (expected)!"")
  >       assert state == expected_final_state
  E       AssertionError: assert equals failed
  E         None       'success'
  
  kubernetes_tests/test_base.py:197: AssertionError
  ---------------------------- Captured stdout setup -----------------------------
```",potiuk,2024-12-22 09:53:13+00:00,['jason810496'],2025-02-06 08:06:14+00:00,2025-02-06 08:06:14+00:00,https://github.com/apache/airflow/issues/45145,"[('area:CI', ""Airflow's tests and continious integration""), ('Quarantine', 'Issues that are occasionally failing and are quarantined'), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]","[{'comment_id': 2564736123, 'issue_id': 2754577909, 'author': 'jason810496', 'body': 'Hi @potiuk I can investigate this issue, could you assign to me? Thanks!', 'created_at': datetime.datetime(2024, 12, 29, 14, 6, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564736673, 'issue_id': 2754577909, 'author': 'potiuk', 'body': 'Assigned! Cool !', 'created_at': datetime.datetime(2024, 12, 29, 14, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2565052272, 'issue_id': 2754577909, 'author': 'amoghrajesh', 'body': '@jason810496 some ideas for you to start with:\r\n1. One of the issues could be that the scheduler is getting killed before the tasks enter the executor, leading to tasks remaining in `queued` state. \r\n2. Some ideas that might be useful:\r\n- Tweak around with the timeouts here so that we can have enough time after the scheduler restart.\r\n- Wait for few seconds for the tasks to enter the executor, and then kill the scheduler.\r\n- Add api calls that can check for status of the task in the `monitor_task` loop. So that we can wait appropriately.', 'created_at': datetime.datetime(2024, 12, 30, 5, 52, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2565717201, 'issue_id': 2754577909, 'author': 'jason810496', 'body': ""Thanks, @amoghrajesh!  I just reproduced the issue using Breeze, and it turns out that the scheduler is stuck in a `CrashLoopBackOff` state. I'll take a closer look and try out the approach you suggested."", 'created_at': datetime.datetime(2024, 12, 30, 16, 59, 44, tzinfo=datetime.timezone.utc)}]","jason810496 (Assginee) on (2024-12-29 14:06:09 UTC): Hi @potiuk I can investigate this issue, could you assign to me? Thanks!

potiuk (Issue Creator) on (2024-12-29 14:08:00 UTC): Assigned! Cool !

amoghrajesh on (2024-12-30 05:52:27 UTC): @jason810496 some ideas for you to start with:
1. One of the issues could be that the scheduler is getting killed before the tasks enter the executor, leading to tasks remaining in `queued` state. 
2. Some ideas that might be useful:
- Tweak around with the timeouts here so that we can have enough time after the scheduler restart.
- Wait for few seconds for the tasks to enter the executor, and then kill the scheduler.
- Add api calls that can check for status of the task in the `monitor_task` loop. So that we can wait appropriately.

jason810496 (Assginee) on (2024-12-30 16:59:44 UTC): Thanks, @amoghrajesh!  I just reproduced the issue using Breeze, and it turns out that the scheduler is stuck in a `CrashLoopBackOff` state. I'll take a closer look and try out the approach you suggested.

"
2754354258,issue,closed,not_planned,Support generating secrets without a data section in Helm charts,"### Description

Currently the secrets are either not included or included with a non-empty data section with some secret values. It'd be great to have an option to generate secrets but without any secret data included (e.g. to be deployed with ArgoCD).

### Use case/motivation

The motivation is security - secrets shouldn't be committed to the repository which is used to manage ArgoCD manifests.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",andrii-korotkov-verkada,2024-12-21 20:38:23+00:00,[],2024-12-23 02:21:52+00:00,2024-12-23 02:21:52+00:00,https://github.com/apache/airflow/issues/45140,"[('area:secrets', ''), ('kind:feature', 'Feature Requests'), ('area:helm-chart', 'Airflow Helm Chart'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2558770931, 'issue_id': 2754354258, 'author': 'andrii-korotkov-verkada', 'body': ""Now I'm not sure this would be useful. Some problems in my case arise from trying to use ArgoCD instead of Helm to install. A better approach is to either generate secrets and apply them separately without committing to the repo or use AWS secrets manager and init secrets as a part of `airflowLocalSettings` value. Then for ArgoCD we should just not generate secrets at all."", 'created_at': datetime.datetime(2024, 12, 23, 2, 18, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558773748, 'issue_id': 2754354258, 'author': 'andrii-korotkov-verkada', 'body': ""I've filed https://github.com/apache/airflow/issues/45171 instead."", 'created_at': datetime.datetime(2024, 12, 23, 2, 21, 52, tzinfo=datetime.timezone.utc)}]","andrii-korotkov-verkada (Issue Creator) on (2024-12-23 02:18:27 UTC): Now I'm not sure this would be useful. Some problems in my case arise from trying to use ArgoCD instead of Helm to install. A better approach is to either generate secrets and apply them separately without committing to the repo or use AWS secrets manager and init secrets as a part of `airflowLocalSettings` value. Then for ArgoCD we should just not generate secrets at all.

andrii-korotkov-verkada (Issue Creator) on (2024-12-23 02:21:52 UTC): I've filed https://github.com/apache/airflow/issues/45171 instead.

"
2754345210,issue,closed,completed,Add support for serverless cluster in Databricks operator in databricks provider,"### Description

The current Databricks operator to run jobs (DatabricksNotebookOperator, DatabricksTaskOperator and DatabricksWorkflowTaskGroupOperator) doesn't support serverless. All the above operators expect either an existing_cluster_id or job_cluster_key, whereas neither should be passed for serverless workflow. The work required is :  
 - add a named parameter to support environments which are used for serverless
 - add a named_parameter called is_serverless as boolean. This flag will make existing_cluster_id and job_cluster_key optional and submit a serverless job
 - Update doc to reflect the new addition

### Use case/motivation

This change will allow users to submit a Databricks serverless job for DatabricksNotebookOperator, DatabricksTaskOperator and DatabricksWorkflowTaskGroupOperator which is currently supported for only DatabricksSubmitRunOperator

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",HariGS-DB,2024-12-21 20:13:17+00:00,['HariGS-DB'],2025-01-28 12:06:47+00:00,2025-01-28 12:06:47+00:00,https://github.com/apache/airflow/issues/45138,"[('area:providers', ''), ('kind:feature', 'Feature Requests'), ('provider:databricks', '')]","[{'comment_id': 2558227477, 'issue_id': 2754345210, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 21, 20, 13, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558228543, 'issue_id': 2754345210, 'author': 'potiuk', 'body': 'feel free', 'created_at': datetime.datetime(2024, 12, 21, 20, 17, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567784888, 'issue_id': 2754345210, 'author': 'big-c-note', 'body': ""I use DatabricksSubmitRun for what it's worth. Because it's serverless, the start up time is super quick, and you get all the lineage in Databricks because it's running natively on Workflows. That's a huge plus to doing it that way. You can write a little script with Databricks sdk to automate task creation. We literally use one task per workflow so we don't have to deal with databricks workflow dependency stuff. And with serverless it works pretty good."", 'created_at': datetime.datetime(2025, 1, 2, 13, 34, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567787232, 'issue_id': 2754345210, 'author': 'big-c-note', 'body': ""That said I would actually prefer the method you describe (if it weren't for missing out on the lineage easily). Just wanted to offer a work around"", 'created_at': datetime.datetime(2025, 1, 2, 13, 36, 46, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-21 20:13:20 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-12-21 20:17:01 UTC): feel free

big-c-note on (2025-01-02 13:34:55 UTC): I use DatabricksSubmitRun for what it's worth. Because it's serverless, the start up time is super quick, and you get all the lineage in Databricks because it's running natively on Workflows. That's a huge plus to doing it that way. You can write a little script with Databricks sdk to automate task creation. We literally use one task per workflow so we don't have to deal with databricks workflow dependency stuff. And with serverless it works pretty good.

big-c-note on (2025-01-02 13:36:46 UTC): That said I would actually prefer the method you describe (if it weren't for missing out on the lineage easily). Just wanted to offer a work around

"
2754213107,issue,closed,not_planned,SPAM Message,SPAM,jt4568536,2024-12-21 16:29:17+00:00,[],2025-01-05 17:33:54+00:00,2024-12-21 16:47:55+00:00,https://github.com/apache/airflow/issues/45135,[],"[{'comment_id': 2558167047, 'issue_id': 2754213107, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 21, 16, 29, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558171504, 'issue_id': 2754213107, 'author': 'shahar1', 'body': 'This is a SPAM message, do not click on any links.\r\n\r\nEdit: Reported to GitHub and removed the suspicious links.', 'created_at': datetime.datetime(2024, 12, 21, 16, 47, 55, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-21 16:29:20 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

shahar1 on (2024-12-21 16:47:55 UTC): This is a SPAM message, do not click on any links.

Edit: Reported to GitHub and removed the suspicious links.

"
2752933475,issue,closed,completed,Logging out from Web UI still raises Airflow 405 error,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I still have the same error as described in #40470 even though i'm running 2.10.4 with apache-airflow-providers-fab             1.4.0. It's probably linked to some package - but i did ran
`pip install ""apache-airflow==2.10.4"" --constraint ""https://raw.githubusercontent.com/apache/airflow/constraints-2.10.4/constraints-3.8.txt""`


### What you think should happen instead?

logout process needs to complete correctly

### How to reproduce

connected as admin and try to logout

### Operating System

Fedora 8.3

### Versions of Apache Airflow Providers

apache-airflow-providers-fab==1.4.0


### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",punx120,2024-12-20 15:13:59+00:00,['Spaarsh'],2025-01-02 17:12:00+00:00,2025-01-02 17:12:00+00:00,https://github.com/apache/airflow/issues/45116,"[('kind:bug', 'This is a clearly a bug'), ('pending-response', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2561884875, 'issue_id': 2752933475, 'author': 'Spaarsh', 'body': 'Hey! I would like to work on this issue!', 'created_at': datetime.datetime(2024, 12, 25, 13, 8, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562961137, 'issue_id': 2752933475, 'author': 'Spaarsh', 'body': '(I have already posted a comment under the referenced issue and am posting the same comment here as well.)\r\n\r\nIt took me a while to triage this issue. I went through the main-branch code as well as the last released source code of airflow (2.10.4). I think I may have a way of solving this. Please correct me if I am wrong!\r\n\r\n### Triaging\r\nI thoroughly went through the entirety of the auth-related code (to the best of my ability). There is no wrong method being called. But since the FAB requires us to not allow any GET requests at the /logout API endpoint, the HTML can\'t be rendered when a user manually enters the URL in the browser point.\r\n\r\nBefore moving further though, I wanted to ensure that the /logout endpoint does work for POST requests. There is one instance of a JS code sending a POST request to the /logout endpoint via the ```no_roles_permissions.html```, where there is a logout button. In order to test this, I created a user with no roles using the command:\r\n```\r\nairflow users create \\\r\n    --username no_roles_user \\\r\n    --firstname No \\\r\n    --lastname Roles \\\r\n    --email no_roles_user@example.com \\\r\n    --password your_password \\\r\n    --role Public\r\n```\r\n\r\nWhen I then went to the /home page, the expected page showed up:\r\n![image](https://github.com/user-attachments/assets/3593d7d9-3c63-4475-a674-afad514607c4)\r\n\r\nWhen I clicked on the ""logout"" button, I was successfully able to log out, indicating no fault at the endpoint itself.\r\n\r\n### Solution\r\nHence my solution is as follows:\r\nWe can create a new endpoint such as ```/logout_page``` which renders an HTML which has a logout button. The logout button has a JS event handler that sends the POST request to the /logout endpoint, resulting in successful logout using GUI.\r\n\r\nThis way, the GET request doesn\'t happen on our /logout endpoint itself (thus not violating any FAB requirements) while also enabling a GUI-based logout action.\r\n\r\nIf this is the correct approach, I am willing to open a PR.', 'created_at': datetime.datetime(2024, 12, 26, 17, 10, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562988725, 'issue_id': 2752933475, 'author': 'shahar1', 'body': '@Spaarsh well analyzed, solution seems solid.\r\nFeel free to tag me in your PR for review', 'created_at': datetime.datetime(2024, 12, 26, 17, 53, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563009187, 'issue_id': 2752933475, 'author': 'Spaarsh', 'body': '@shahar1 Sure! I had a question though. Should I add the logout button on some existing html page? Or provide a hyperlink on some existing page?', 'created_at': datetime.datetime(2024, 12, 26, 18, 26, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563017393, 'issue_id': 2752933475, 'author': 'shahar1', 'body': '> @shahar1 Sure! I had a question though. Should I add the logout button on some existing html page? Or provide a hyperlink on some existing page?\n\nJust try to make it as much as trivial to the user, as logging out should be a simple action. If you encounter any limitations, we could discuss them in the PR.', 'created_at': datetime.datetime(2024, 12, 26, 18, 39, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564492146, 'issue_id': 2752933475, 'author': 'Spaarsh', 'body': '@punx120 could you please tell us the steps to recreate this error? That will greatly help in fixing it. Thanks!', 'created_at': datetime.datetime(2024, 12, 28, 22, 5, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567909314, 'issue_id': 2752933475, 'author': 'punx120', 'body': '> @punx120 could you please tell us the steps to recreate this error? That will greatly help in fixing it. Thanks!\r\n![Screenshot From 2025-01-02 10-01-50](https://github.com/user-attachments/assets/26814a85-6c61-48ca-902e-2808e973ca21)\r\n\r\nI simply click on the logout link.', 'created_at': datetime.datetime(2025, 1, 2, 15, 2, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568045490, 'issue_id': 2752933475, 'author': 'punx120', 'body': 'On a fresh install, it works - so it must be a config and/or package conflict - will try to dig into it', 'created_at': datetime.datetime(2025, 1, 2, 16, 31, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568063132, 'issue_id': 2752933475, 'author': 'Spaarsh', 'body': 'Next time the error occurs, inspect the logout button and check the corresponding action that is being taken on clicking it. From what I have discerned, the error should only surface if a GET request is sent at the ```/logout/``` endpoint. Check in the webserver logs as well and check the request being sent. If it is GET, then we may have a lead.', 'created_at': datetime.datetime(2025, 1, 2, 16, 44, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568099827, 'issue_id': 2752933475, 'author': 'potiuk', 'body': 'And take a look also at the providers reported by webserver (in the webserver menu). It could be that your webserver had - for whatever reason - different provider installed - or maybe even different airflow+provider installed when you run it. And it this case the error is on the side of provider installed in the webserver.', 'created_at': datetime.datetime(2025, 1, 2, 17, 10, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568101048, 'issue_id': 2752933475, 'author': 'potiuk', 'body': ""I convert it to a discussion as well - as it's something differnt - the issue seems to be solved but we might still discuss what's the problem you encountered with packages."", 'created_at': datetime.datetime(2025, 1, 2, 17, 11, 52, tzinfo=datetime.timezone.utc)}]","Spaarsh (Assginee) on (2024-12-25 13:08:53 UTC): Hey! I would like to work on this issue!

Spaarsh (Assginee) on (2024-12-26 17:10:57 UTC): (I have already posted a comment under the referenced issue and am posting the same comment here as well.)

It took me a while to triage this issue. I went through the main-branch code as well as the last released source code of airflow (2.10.4). I think I may have a way of solving this. Please correct me if I am wrong!

### Triaging
I thoroughly went through the entirety of the auth-related code (to the best of my ability). There is no wrong method being called. But since the FAB requires us to not allow any GET requests at the /logout API endpoint, the HTML can't be rendered when a user manually enters the URL in the browser point.

Before moving further though, I wanted to ensure that the /logout endpoint does work for POST requests. There is one instance of a JS code sending a POST request to the /logout endpoint via the ```no_roles_permissions.html```, where there is a logout button. In order to test this, I created a user with no roles using the command:
```
airflow users create \
    --username no_roles_user \
    --firstname No \
    --lastname Roles \
    --email no_roles_user@example.com \
    --password your_password \
    --role Public
```

When I then went to the /home page, the expected page showed up:
![image](https://github.com/user-attachments/assets/3593d7d9-3c63-4475-a674-afad514607c4)

When I clicked on the ""logout"" button, I was successfully able to log out, indicating no fault at the endpoint itself.

### Solution
Hence my solution is as follows:
We can create a new endpoint such as ```/logout_page``` which renders an HTML which has a logout button. The logout button has a JS event handler that sends the POST request to the /logout endpoint, resulting in successful logout using GUI.

This way, the GET request doesn't happen on our /logout endpoint itself (thus not violating any FAB requirements) while also enabling a GUI-based logout action.

If this is the correct approach, I am willing to open a PR.

shahar1 on (2024-12-26 17:53:55 UTC): @Spaarsh well analyzed, solution seems solid.
Feel free to tag me in your PR for review

Spaarsh (Assginee) on (2024-12-26 18:26:43 UTC): @shahar1 Sure! I had a question though. Should I add the logout button on some existing html page? Or provide a hyperlink on some existing page?

shahar1 on (2024-12-26 18:39:46 UTC): Just try to make it as much as trivial to the user, as logging out should be a simple action. If you encounter any limitations, we could discuss them in the PR.

Spaarsh (Assginee) on (2024-12-28 22:05:15 UTC): @punx120 could you please tell us the steps to recreate this error? That will greatly help in fixing it. Thanks!

punx120 (Issue Creator) on (2025-01-02 15:02:28 UTC): ![Screenshot From 2025-01-02 10-01-50](https://github.com/user-attachments/assets/26814a85-6c61-48ca-902e-2808e973ca21)

I simply click on the logout link.

punx120 (Issue Creator) on (2025-01-02 16:31:22 UTC): On a fresh install, it works - so it must be a config and/or package conflict - will try to dig into it

Spaarsh (Assginee) on (2025-01-02 16:44:15 UTC): Next time the error occurs, inspect the logout button and check the corresponding action that is being taken on clicking it. From what I have discerned, the error should only surface if a GET request is sent at the ```/logout/``` endpoint. Check in the webserver logs as well and check the request being sent. If it is GET, then we may have a lead.

potiuk on (2025-01-02 17:10:53 UTC): And take a look also at the providers reported by webserver (in the webserver menu). It could be that your webserver had - for whatever reason - different provider installed - or maybe even different airflow+provider installed when you run it. And it this case the error is on the side of provider installed in the webserver.

potiuk on (2025-01-02 17:11:52 UTC): I convert it to a discussion as well - as it's something differnt - the issue seems to be solved but we might still discuss what's the problem you encountered with packages.

"
2752754030,issue,closed,completed,Providing SMTP user and password from airflow.cfg or environment variables,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Currently airflow smtp email.py utils is not taking  ```user, password``` values from airflow.cfg/env variables, rather it gets ```user, password``` values from smtp connection https://github.com/apache/airflow/blob/main/airflow/utils/email.py#L245-L246

```
AIRFLOW__EMAIL__EMAIL_BACKEND: airflow.utils.email.send_email_smtp
AIRFLOW__SMTP__SMTP_HOST: host
AIRFLOW__SMTP__SMTP_STARTTLS: true
AIRFLOW__SMTP__SMTP_SSL: False
AIRFLOW__SMTP__SMTP_USER: ""user@domain""
AIRFLOW__SMTP__SMTP_PASSWORD: 'pwd'
AIRFLOW__SMTP__SMTP_PORT: 25
AIRFLOW__SMTP__SMTP_MAIL_FROM: from@email.com 
```

### What you think should happen instead?

In production its not realistic to manually supply smtp user password in connection, thus it would be ideal to consider to get ```user and password ``` from airflow.cfg in case they are supplied ```AIRFLOW__SMTP__SMTP_USER: ""user@domain""  AIRFLOW__SMTP__SMTP_PASSWORD: 'pwd' ```.


### How to reproduce

na

### Operating System

na

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

Please suggest if there is a work around to provide smtp password securely.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",BalaMuralibi,2024-12-20 13:32:53+00:00,[],2024-12-27 09:19:57+00:00,2024-12-20 20:23:12+00:00,https://github.com/apache/airflow/issues/45114,"[('kind:bug', 'This is a clearly a bug'), ('invalid', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2557027610, 'issue_id': 2752754030, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 20, 13, 32, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2557670351, 'issue_id': 2752754030, 'author': 'potiuk', 'body': 'You can provide them as connection variables: https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html#storing-connections-in-environment-variables', 'created_at': datetime.datetime(2024, 12, 20, 20, 23, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2557671530, 'issue_id': 2752754030, 'author': 'potiuk', 'body': 'For the future - please use discussions if you have a question or doubt of that sort. Issues are for reporting problems that are airflow problems and when you want to do something and do not know how, open discussion.', 'created_at': datetime.datetime(2024, 12, 20, 20, 24, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563464764, 'issue_id': 2752754030, 'author': 'BalaMuralibi', 'body': ""Thanks for your reply. I had tried with connection variable  ```AIRFLOW_CONN_SMTP_DEFAULT: smtp://$(smtpuser):$(smtppwd)@host:25?disable_ssl=true ``` prior posting here. It didn't help."", 'created_at': datetime.datetime(2024, 12, 27, 8, 40, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563495320, 'issue_id': 2752754030, 'author': 'potiuk', 'body': ""I have no idea if it's a good format or not or whether there are some typos or lack of variables. But you should  make sure to create your env variable using the tool we provided (just scroll down the page I posted). URLs need a number of escaping - especially if your password contains special characters (this is how URL works) and the tooling we create. In the document I linked above there are both - tools that allow you to generate proper URL as well as an explanation how you can use JSON stored in an env variable to avoid escaping. I recommend you to read and follow those."", 'created_at': datetime.datetime(2024, 12, 27, 9, 17, 36, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-20 13:32:57 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-12-20 20:23:12 UTC): You can provide them as connection variables: https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html#storing-connections-in-environment-variables

potiuk on (2024-12-20 20:24:16 UTC): For the future - please use discussions if you have a question or doubt of that sort. Issues are for reporting problems that are airflow problems and when you want to do something and do not know how, open discussion.

BalaMuralibi (Issue Creator) on (2024-12-27 08:40:52 UTC): Thanks for your reply. I had tried with connection variable  ```AIRFLOW_CONN_SMTP_DEFAULT: smtp://$(smtpuser):$(smtppwd)@host:25?disable_ssl=true ``` prior posting here. It didn't help.

potiuk on (2024-12-27 09:17:36 UTC): I have no idea if it's a good format or not or whether there are some typos or lack of variables. But you should  make sure to create your env variable using the tool we provided (just scroll down the page I posted). URLs need a number of escaping - especially if your password contains special characters (this is how URL works) and the tooling we create. In the document I linked above there are both - tools that allow you to generate proper URL as well as an explanation how you can use JSON stored in an env variable to avoid escaping. I recommend you to read and follow those.

"
2752721027,issue,open,,AIP-38 Update TaskInstance and DagRun clearing process accordingly to AIP-66,"### Body

This issue aims to update in the front-end how we do the clearing of task instances and dag runs. With AIP-66, clearing a task in the past doesn't really make sense. This is closer to a ""rerun"" feature and should create a new run for the same period.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-12-20 13:14:31+00:00,"['bbovenzi', 'pierrejeambrun']",2024-12-20 13:19:08+00:00,,https://github.com/apache/airflow/issues/45113,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]",[],
2752439054,issue,open,,Create JWT token issuer infrastructure for strong Task Identity,,kaxil,2024-12-20 10:34:11+00:00,[],2025-01-17 15:24:20+00:00,,https://github.com/apache/airflow/issues/45107,"[('area:auth', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]","[{'comment_id': 2566635214, 'issue_id': 2752439054, 'author': 'Spaarsh', 'body': 'Hello @kaxil!\r\nI would be glad to work on this issue. I have read the existing JWT token issuer infrastructure for airflow already.\r\n\r\nThanks!', 'created_at': datetime.datetime(2024, 12, 31, 18, 10, 38, tzinfo=datetime.timezone.utc)}]","Spaarsh on (2024-12-31 18:10:38 UTC): Hello @kaxil!
I would be glad to work on this issue. I have read the existing JWT token issuer infrastructure for airflow already.

Thanks!

"
2751692140,issue,closed,completed,Allow Dynamic Tasks to be Searchable Using `map_index_template`,"### Description

It looks like this data is already stored in the [database](https://github.com/apache/airflow/blob/main/airflow/models/taskinstance.py#L1709) and it would be helpful if these were what is rendered/searched upon in the browse -> task instances UI given they are provided.  Happy to hack on this -- if someone can point me in the direction of where this is set up in the UI in the code that would be extremely helpful!

The existing data for dynamic task mapping in the browse -> task instances UI only allows one to search on the index value, which seems correct but provides little to no value especially in the case a user has specified a template to use instead of the default integer.

### Use case/motivation

I think this is valuable because I am never going to search for index 0 on a dynamic task, and am much more likely to use the ""id"" of the task which I would consider to be the values of the `map_index_template` variable that I am setting. 

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jayceslesar,2024-12-20 01:01:53+00:00,[],2024-12-20 20:30:18+00:00,2024-12-20 20:20:16+00:00,https://github.com/apache/airflow/issues/45100,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('area:dynamic-task-mapping', 'AIP-42')]","[{'comment_id': 2556282623, 'issue_id': 2751692140, 'author': 'tirkarthi', 'body': 'Given that FAB views and Airflow 2 UI will not receive active development this should be done in the new UI for Airflow 3 once browse pages are up with filter support.', 'created_at': datetime.datetime(2024, 12, 20, 4, 51, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556862173, 'issue_id': 2751692140, 'author': 'jscheffl', 'body': 'Agree... until I realized that it is only 1 line of change needed...', 'created_at': datetime.datetime(2024, 12, 20, 11, 58, 39, tzinfo=datetime.timezone.utc)}]","tirkarthi on (2024-12-20 04:51:37 UTC): Given that FAB views and Airflow 2 UI will not receive active development this should be done in the new UI for Airflow 3 once browse pages are up with filter support.

jscheffl on (2024-12-20 11:58:39 UTC): Agree... until I realized that it is only 1 line of change needed...

"
2750839308,issue,open,,Continuous timetable causes false dag triggering if dag has already run and start_date is in future,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

hello,
here is a sample dag
```python
import pendulum 
import airflow
import logging
from airflow import DAG
from datetime import timedelta, datetime
from airflow.utils.dates import days_ago
from airflow.operators.python import PythonOperator
from airflow.decorators import task

local_tz = pendulum.timezone(""Europe/Paris"")

default_args = {
	'owner': 'interfaces',
	'depends_on_past': False,
	'retries': 0,
	'retry_delay': timedelta(minutes=1),
}
with DAG(
	dag_id='test_continous',
	default_args=default_args,
	schedule='@continuous',
	start_date=datetime(2024,12,19, 16, 40, tzinfo=local_tz),
    end_date= datetime(2024,12,19, 16, 46, tzinfo=local_tz),
	max_active_runs=1,
	catchup=False,
) as dag:
    
    @task()
    def the_task():
        import time
        time.sleep(25)

    the_task()
```

the first time you activate it, it will correctly wait for start_date to trigger dag run but if it has already run, every seconds it will trigger a ""void"" dag run (no task started just a row inserted in the dag_run table)

### What you think should happen instead?

Dag shouldn't be triggered until the new start_date is passed

### How to reproduce

use the given dag, change start_date and end_date in order to let dag runs for a few minutes. when end_date is passed change start_date and end_date in order to make dag runs in future.

### Operating System

debian 11

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",darkag,2024-12-19 16:19:31+00:00,['jx2lee'],2024-12-20 20:42:36+00:00,,https://github.com/apache/airflow/issues/45081,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('good first issue', ''), ('area:core', '')]","[{'comment_id': 2557180151, 'issue_id': 2750839308, 'author': 'jx2lee', 'body': 'I would like to work on this. Could you assign it to me? Thanks. ☺️', 'created_at': datetime.datetime(2024, 12, 20, 15, 4, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2557691551, 'issue_id': 2750839308, 'author': 'potiuk', 'body': '> I would like to work on this. Could you assign it to me? Thanks. ☺️\r\n\r\ndid', 'created_at': datetime.datetime(2024, 12, 20, 20, 42, 35, tzinfo=datetime.timezone.utc)}]","jx2lee (Assginee) on (2024-12-20 15:04:23 UTC): I would like to work on this. Could you assign it to me? Thanks. ☺️

potiuk on (2024-12-20 20:42:35 UTC): did

"
2750437326,issue,open,,Resolve OOM when reading large logs in webserver,"### Description

Related context: https://github.com/apache/airflow/issues/44753#issuecomment-2526209568

TL;DR

After conducting some research and implementing a POC, I would like to propose a potential solution. However, this solution requires changes to the `airflow.utils.log.file_task_handler.FileTaskHandler`. If the solution is accepted, it will necessitate modifications to 10 providers that extend the `FileTaskHandler` class. 

## Main Concept for Refactoring

The proposed solution focuses on:
1. Returning a generator instead of loading the entire file content at once.
2. Leveraging a heap to merge logs incrementally, rather than sorting entire chunks.

The POC for this refactoring shows a **90% reduction in memory usage** with **similar processing times**!


## Experiment Details


- **830 MB**
- Approximately **8,670,000 lines**

## Main Root Causes of OOM

1. `_interleave_logs` Function in `airflow.utils.log.file_task_handler`
- Extends all log strings into the `records` list.
- Sorts the entire `records` list.
- Yields lines with deduplication.
2. `_read` Method in `airflow.utils.log.file_task_handler.FileTaskHandler`
- Joins all aggregated logs into a single string using:
  ```python
  ""\n"".join(_interleave_logs(all_log_sources))
  ```
3. Methods That Use `_read`:
These methods read the entire log content and return it as a string instead of a generator:
    - `_read_from_local`
    - `_read_from_logs_server`
    - `_read_remote_logs`  (Implemented by providers)

## Proposed Refactoring Solution

The main concept includes:

- Return a **generator** for reading log sources (local or external) instead of whole file content as string. 
- Merge logs using **K-Way Merge instead of Sorting**
   - Since each source of logs is already sorted, merge them incrementally using `heapq` with streams of logs.
   - Return a stream of the merged result.

### Breaking Changes in This Solution

1. **Interface of the `read` Method** in `FileTaskHandler`:
   - Will now return a generator instead of a string.

2. **Interfaces of `read_log_chunks` and `read_log_stream`** in `TaskLogReader`:
   - Adjustments to support the generator-based approach.
3. Methods That Use `_read`
    - `_read_from_local`
    - `_read_from_logs_server`
    - `_read_remote_logs`  ( there are 10 providers implement this method )



## Experimental Environment:

- **Setup**: Docker Compose without memory limits.
- **Memory Profiling**: [memray](https://github.com/bloomberg/memray)
- **Log Size**: `830 MB`, about `8670000` lines

## Benchmark Metrics

- **Original Implementation**:
  - **Memory Usage**: Average 3GB, peaks at 4GB when returning the final stream.
    - <img width=""1198"" alt=""Original-CPU-Memory"" src=""https://github.com/user-attachments/assets/bb15261c-9bf2-44c2-af8e-0d05b2c51260"" />
  - **Processing Time**: ~60 seconds.
  - Memory Flame Graph
      - https://www.zhu424.dev/Airflow-Webserver-Resolving-OOM-for-Large-Log-Reads/memray-flamegraph-memray_logs.py.html

- **POC (Refactored Implementation)**:
  - **Memory Usage**: Average 300MB.
    - <img width=""1201"" alt=""POC-CPU-Memory"" src=""https://github.com/user-attachments/assets/8e450c12-898b-4218-82f4-6107c432963b"" />
  - **Processing Time**: ~60 seconds.
  - Memory Flame Graph
      - https://www.zhu424.dev/Airflow-Webserver-Resolving-OOM-for-Large-Log-Reads/memray-flamegraph-read_large_logs-k-way-merge-heap-optimize.py.html

## Summary

Feel free to share any feedback! I believe we should have more discussions before adopting this solution, as it involves breaking changes to the `FileTaskHandler` interface and requires refactoring in 10 providers as well.

### Related issues

#44753

## TODO Tasks

- [ ] Refactor to Use K-Way Merge for Log Streams Instead of Sorting Entire Log Records #45129 
- Refactor providers that implemented `FileTaskHandler`
    - `_read_remote_logs` method
        - [ ] Amazon AWS S3
        - [ ] Google Cloud GCS
        - [ ] Microsoft Azure WASB
        - [ ] Apache HDFS
    - `_read` method
        - [ ] Alibaba Cloud 
        - [ ] Amazon AWS Cloud Watch
        - [ ] Elasticsearch
        - [ ] OpenSearch
        - [ ] Redis
- Refactor executors that implemented `get_task_log`
    - [ ] `CeleryKubernetesExecutor`
    - [ ] `KubernetesExecutor`
    - [ ] `LocalKubernetesExecutor`
- [ ] Remove compatible utility
- [ ] Add pagination interface for APIs

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jason810496,2024-12-19 14:03:48+00:00,['jason810496'],2024-12-23 13:27:42+00:00,,https://github.com/apache/airflow/issues/45079,"[('area:providers', ''), ('area:logging', ''), ('kind:feature', 'Feature Requests')]","[{'comment_id': 2554170838, 'issue_id': 2750437326, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 19, 14, 3, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2554244232, 'issue_id': 2750437326, 'author': 'potiuk', 'body': ""Yes. That's exactly how I envisioned solving this problem. @dstandish  ?"", 'created_at': datetime.datetime(2024, 12, 19, 14, 17, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2554251353, 'issue_id': 2750437326, 'author': 'potiuk', 'body': 'FYI. Breaking changes to FileTaskHandler is not a problem - we can work out back-compatibility or simply break it for Airflow 3 - this is not  a big deal, since this is only a deployment configuration and does not require DAG adaptations.', 'created_at': datetime.datetime(2024, 12, 19, 14, 18, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2554270738, 'issue_id': 2750437326, 'author': 'jason810496', 'body': 'Hi @potiuk,  \r\n\r\nWould it be okay if I treat this issue as an umbrella issue to track other TODO tasks while refactoring each provider? Or would it be more preferable to refactor `FileTaskHandler` and all providers in a single PR in this case?  Thanks !', 'created_at': datetime.datetime(2024, 12, 19, 14, 22, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2554436249, 'issue_id': 2750437326, 'author': 'potiuk', 'body': 'Sure. It can be separate set of PRs and that issue can remain ""umbrella"" - you do not need to have more issues. PRs are enough', 'created_at': datetime.datetime(2024, 12, 19, 14, 53, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2555840386, 'issue_id': 2750437326, 'author': 'dstandish', 'body': ""> Yes. That's exactly how I envisioned solving this problem. @dstandish ?\r\n\r\nIIRC this should be fine when task done but may present challenges when task is in flight because at any moment the location of the logs may shift eg from worker to remote storage etc"", 'created_at': datetime.datetime(2024, 12, 19, 21, 55, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2555846127, 'issue_id': 2750437326, 'author': 'potiuk', 'body': '> IIRC this should be fine when task done but may present challenges when task is in flight because at any moment the location of the logs may shift eg from worker to remote storage etc\r\n\r\nIs it not the same case now?', 'created_at': datetime.datetime(2024, 12, 19, 21, 59, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556208606, 'issue_id': 2750437326, 'author': 'tirkarthi', 'body': 'Related issue https://github.com/apache/airflow/issues/31105', 'created_at': datetime.datetime(2024, 12, 20, 3, 20, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556220144, 'issue_id': 2750437326, 'author': 'jason810496', 'body': ""> > Yes. That's exactly how I envisioned solving this problem. @dstandish ?\r\n> \r\n> IIRC this should be fine when task done but may present challenges when task is in flight because at any moment the location of the logs may shift eg from worker to remote storage etc\r\n\r\nTaking `S3TaskHandler` as an example, it requires additional refactoring and might need a `read_stream` method added to `S3Hook` that returns a generator-based result:  \r\nhttps://github.com/apache/airflow/blob/main/providers/src/airflow/providers/amazon/aws/log/s3_task_handler.py#L136-L192\r\n\r\nFrom my perspective, for the `s3_write` case, I would download the old log as temporary file and append the new log stream into a temporary file, and use the [`upload_file`](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html) method to upload the file to prevent memory starvation and remain the same result."", 'created_at': datetime.datetime(2024, 12, 20, 3, 34, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556539424, 'issue_id': 2750437326, 'author': 'potiuk', 'body': '> Taking S3TaskHandler as an example, it requires additional refactoring and might need a read_stream method added to S3Hook that returns a generator-based result:\r\nhttps://github.com/apache/airflow/blob/main/providers/src/airflow/providers/amazon/aws/log/s3_task_handler.py#L136-L192\r\n\r\n> From my perspective, for the s3_write case, I would download the old log as temporary file and append the new log stream into a temporary file, and use the [upload_file](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html) method to upload the file to prevent memory starvation and remain the same result.\r\n\r\nYep. There will be dga cases like that.  And yes the proposed method is good.', 'created_at': datetime.datetime(2024, 12, 20, 8, 45, 50, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-19 14:03:52 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-12-19 14:17:17 UTC): Yes. That's exactly how I envisioned solving this problem. @dstandish  ?

potiuk on (2024-12-19 14:18:44 UTC): FYI. Breaking changes to FileTaskHandler is not a problem - we can work out back-compatibility or simply break it for Airflow 3 - this is not  a big deal, since this is only a deployment configuration and does not require DAG adaptations.

jason810496 (Issue Creator) on (2024-12-19 14:22:37 UTC): Hi @potiuk,  

Would it be okay if I treat this issue as an umbrella issue to track other TODO tasks while refactoring each provider? Or would it be more preferable to refactor `FileTaskHandler` and all providers in a single PR in this case?  Thanks !

potiuk on (2024-12-19 14:53:44 UTC): Sure. It can be separate set of PRs and that issue can remain ""umbrella"" - you do not need to have more issues. PRs are enough

dstandish on (2024-12-19 21:55:23 UTC): IIRC this should be fine when task done but may present challenges when task is in flight because at any moment the location of the logs may shift eg from worker to remote storage etc

potiuk on (2024-12-19 21:59:54 UTC): Is it not the same case now?

tirkarthi on (2024-12-20 03:20:16 UTC): Related issue https://github.com/apache/airflow/issues/31105

jason810496 (Issue Creator) on (2024-12-20 03:34:32 UTC): Taking `S3TaskHandler` as an example, it requires additional refactoring and might need a `read_stream` method added to `S3Hook` that returns a generator-based result:  
https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/amazon/aws/log/s3_task_handler.py#L136-L192

From my perspective, for the `s3_write` case, I would download the old log as temporary file and append the new log stream into a temporary file, and use the [`upload_file`](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html) method to upload the file to prevent memory starvation and remain the same result.

potiuk on (2024-12-20 08:45:50 UTC): https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/amazon/aws/log/s3_task_handler.py#L136-L192


Yep. There will be dga cases like that.  And yes the proposed method is good.

"
2750418594,issue,closed,not_planned,Tasks fails without logs under heavy load,"### Apache Airflow version

2.10.4

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I have multiple dag_run of a dag, running parallel on a kubernetes cluster with a single worker pod.
I use 16 as parallelism and a retry_count of 4.
This dags is composed of mapped_tasks. The bigger one spawns 36 mapped task.
Every day 100 dag_run will be spawned toghter and the dag_run with most task will fail with 3/4 mapped tasks failed. 
Those tasks fails after 4 retry, but most of the times i see only 1 or 2 logs of execution.
Most of the time the log is :
```
[2024-12-19T11:06:21.433+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-12-19T11:06:21.792+0000] {taskinstance.py:2603} INFO - Dependencies not met for <TaskInstance: ExportSii.XMLGeneration manual__2024-12-19T11:05:23.599025+00:00 map_index=9 [up_for_retry]>, dependency 'Not In Retry Period' FAILED: Task is not ready for retry yet but will be retried automatically. Current date is 2024-12-19T11:06:21.791874+00:00 and task will be retried at 2024-12-19T11:06:44.472566+00:00.
[2024-12-19T11:06:21.805+0000] {local_task_job_runner.py:166} INFO - Task is not able to be run
```

This for example is `attempt=2.log` and i dont have 1,3 or 4. Neither in logs or in the UI.

Then when I clear the state of failed tasks they will run correctly without errors.




### What you think should happen instead?

I would like to see all the attempt, and a more clear trace of what happened so i can debug the problem.

### How to reproduce

It's mostly dependent on the workload. On another istance with the same code, but less stress it doesn't happen.

### Operating System

helm-chart on kubernetes

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Kubernetes on GKE

### Anything else?

I dont have any error at kubernetes level or on the worker log

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",team-hawking-stam,2024-12-19 13:57:06+00:00,[],2025-01-10 09:16:25+00:00,2025-01-10 09:16:20+00:00,https://github.com/apache/airflow/issues/45078,"[('kind:bug', 'This is a clearly a bug'), (""Can't Reproduce"", 'The problem cannot be reproduced'), ('area:logging', ''), ('duplicate', 'Issue that is duplicated'), ('pending-response', ''), ('area:core', ''), ('area:dynamic-task-mapping', 'AIP-42'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2554133547, 'issue_id': 2750418594, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 19, 13, 57, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2582043484, 'issue_id': 2750418594, 'author': 'shahar1', 'body': 'It is not as clear from the description how to reproduce the issue so others could resolve it - if you could provide a minimal example for reproducing it, it would be helpful.', 'created_at': datetime.datetime(2025, 1, 10, 8, 26, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2582141602, 'issue_id': 2750418594, 'author': 'shahar1', 'body': ""It seems like a duplicate of https://github.com/apache/airflow/issues/42107, I'm closing this one and we could continue the discussion there."", 'created_at': datetime.datetime(2025, 1, 10, 9, 16, 20, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-19 13:57:09 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

shahar1 on (2025-01-10 08:26:45 UTC): It is not as clear from the description how to reproduce the issue so others could resolve it - if you could provide a minimal example for reproducing it, it would be helpful.

shahar1 on (2025-01-10 09:16:20 UTC): It seems like a duplicate of https://github.com/apache/airflow/issues/42107, I'm closing this one and we could continue the discussion there.

"
2750138759,issue,open,,Tidy up DagFileProcessor logging (remove reload and double loading etc) by porting it to structlog,Include newsfragment about changed logging as Jed requested here https://github.com/apache/airflow/pull/44972#discussion_r1889280660,ashb,2024-12-19 12:17:02+00:00,['ashb'],2024-12-19 12:30:15+00:00,,https://github.com/apache/airflow/issues/45072,"[('area:logging', '')]",[],
2748870673,issue,open,,Support concurrent logs collection for containers in KubernetesPodOperator,"### Description

During the discussion in [PR #43853](https://github.com/apache/airflow/pull/43853) with [@dstandish](https://github.com/dstandish), we noticed that there are two separate implementation flows for collecting logs from init_containers and containers that do similar things.

The current implementation processes logs sequentially, which is correct for init_containers because they are executed one after another—each starting only after the previous one has successfully completed.

However, for regular containers (not init_containers), this logic isn’t fully accurate since they can run in parallel.

Related code are https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/cncf/kubernetes/utils/pod_manager.py#L617

I suggest  to implement concurrent logs collection for containers. 

### Use case/motivation

This might cause issues during long-running executions, as we won’t see logs from other containers until one of them has completed.

### Related issues

https://github.com/apache/airflow/pull/43853

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",mrk-andreev,2024-12-18 22:31:50+00:00,[],2024-12-20 11:45:09+00:00,,https://github.com/apache/airflow/issues/45061,"[('kind:feature', 'Feature Requests'), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2552394582, 'issue_id': 2748870673, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 18, 22, 31, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2552396703, 'issue_id': 2748870673, 'author': 'mrk-andreev', 'body': '@potiuk , @eladkal , @dstandish \r\n\r\nWhat do you think about that?', 'created_at': datetime.datetime(2024, 12, 18, 22, 33, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556774687, 'issue_id': 2748870673, 'author': 'potiuk', 'body': 'I think it would be a great idea to review and make it ""common"" and reused accross the components. Also we have another, related discussion about logging in https://github.com/apache/airflow/issues/45079 where a solutions is proposed to handle both - streaming logs and serving logs after task is completed - with regards to OOM issues. \r\n\r\nCurrently we merge and store logs in memory which - in some situations might in some situations lead to OOMs. And some refactoring and restructuring of the logging mechanism might be necessary to handle things better. \r\n\r\nI think both of you @mrk-andreev and @jason810496  are very well skilled and prepared to handle that - under @dstandish  @eladkal (and a little of mine) guidande and mentoring, so if you both work on this together to get both sides of logging improved.\r\n\r\nI know it\'s not directly the same issue, but having a small team of skilled people to focus on both and collaborate, might be a great idea.\r\n\r\n@dstandish - WDYT? Having the three of maintainers here to overlook it while Mark and LIU ZHE YOU work on those, might be a good idea to distract from other Airlfow 3 streams?', 'created_at': datetime.datetime(2024, 12, 20, 11, 0, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556832230, 'issue_id': 2748870673, 'author': 'jason810496', 'body': 'Hi @potiuk,  I’m happy to get involved in the refactoring of the stream logs issue!  \r\n\r\nI might start by breaking down #45079 into some TODO tasks and proposing PRs. Do we need to collaborate on Slack, or is discussing on GitHub sufficient?  \r\n\r\n> I think it would be a great idea to review and make it ""common"" and reused across the components.  \r\n\r\nFrom my perspective, it’s quite challenging to define a common interface for both use cases at this stage. \r\nFor #45079 , there are some additional logic to deal with metadata for logs.\r\nPerhaps Mark and I could work in parallel for now, and later consolidate the streaming logs part into common modules, which might make it easier to identify shared utilities.', 'created_at': datetime.datetime(2024, 12, 20, 11, 38, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556841998, 'issue_id': 2748870673, 'author': 'potiuk', 'body': ""> I might start by breaking down https://github.com/apache/airflow/issues/45079 into some TODO tasks and proposing PRs. Do we need to collaborate on Slack, or is discussing on GitHub sufficient?\r\n\r\nGitHub is enough.\r\n\r\n> From my perspective, it’s quite challenging to define a common interface for both use cases at this stage.\r\nFor https://github.com/apache/airflow/issues/45079 , there are some additional logic to deal with metadata for logs.\r\nPerhaps Mark and I could work in parallel for now, and later consolidate the streaming logs part into common modules, which might make it easier to identify shared utilities.\r\n\r\nYeah. I was more thinking about both of you starting to work in parallel and review and comment on each-other's PRs / issues with us overlooking it - since the subjects are very close (even if not the same) - having extra pairs of eyes on the work done while it is being done is always helpful."", 'created_at': datetime.datetime(2024, 12, 20, 11, 45, 7, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-18 22:31:53 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

mrk-andreev (Issue Creator) on (2024-12-18 22:33:30 UTC): @potiuk , @eladkal , @dstandish 

What do you think about that?

potiuk on (2024-12-20 11:00:52 UTC): I think it would be a great idea to review and make it ""common"" and reused accross the components. Also we have another, related discussion about logging in https://github.com/apache/airflow/issues/45079 where a solutions is proposed to handle both - streaming logs and serving logs after task is completed - with regards to OOM issues. 

Currently we merge and store logs in memory which - in some situations might in some situations lead to OOMs. And some refactoring and restructuring of the logging mechanism might be necessary to handle things better. 

I think both of you @mrk-andreev and @jason810496  are very well skilled and prepared to handle that - under @dstandish  @eladkal (and a little of mine) guidande and mentoring, so if you both work on this together to get both sides of logging improved.

I know it's not directly the same issue, but having a small team of skilled people to focus on both and collaborate, might be a great idea.

@dstandish - WDYT? Having the three of maintainers here to overlook it while Mark and LIU ZHE YOU work on those, might be a good idea to distract from other Airlfow 3 streams?

jason810496 on (2024-12-20 11:38:39 UTC): Hi @potiuk,  I’m happy to get involved in the refactoring of the stream logs issue!  

I might start by breaking down #45079 into some TODO tasks and proposing PRs. Do we need to collaborate on Slack, or is discussing on GitHub sufficient?  


From my perspective, it’s quite challenging to define a common interface for both use cases at this stage. 
For #45079 , there are some additional logic to deal with metadata for logs.
Perhaps Mark and I could work in parallel for now, and later consolidate the streaming logs part into common modules, which might make it easier to identify shared utilities.

potiuk on (2024-12-20 11:45:07 UTC): GitHub is enough.

For https://github.com/apache/airflow/issues/45079 , there are some additional logic to deal with metadata for logs.
Perhaps Mark and I could work in parallel for now, and later consolidate the streaming logs part into common modules, which might make it easier to identify shared utilities.

Yeah. I was more thinking about both of you starting to work in parallel and review and comment on each-other's PRs / issues with us overlooking it - since the subjects are very close (even if not the same) - having extra pairs of eyes on the work done while it is being done is always helpful.

"
2748463299,issue,closed,completed,Received SIGTERM. Terminating subprocesses.,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.1

### What happened?

Sometimes some dags are failing with:
```
[2024-12-16, 07:33:00 UTC] {base.py:290} INFO - OpenMetadata: Processed 0 records, updated 0 records, filtered 0 records, found 0 errors
[2024-12-16, 07:33:35 UTC] {local_task_job_runner.py:288} WARNING - Recorded pid 99982 does not match the current pid 96297
[2024-12-16, 07:33:35 UTC] {local_task_job_runner.py:222} ▲▲▲ Log group end
[2024-12-16, 07:33:35 UTC] {process_utils.py:132} INFO - Sending Signals.SIGTERM to group 96297. PIDs of all processes in the group: [96297]
[2024-12-16, 07:33:35 UTC] {process_utils.py:87} INFO - Sending the signal Signals.SIGTERM to group 96297
[2024-12-16, 07:33:35 UTC] {taskinstance.py:2611} ERROR - Received SIGTERM. Terminating subprocesses.
[2024-12-16, 07:33:35 UTC] {common.py:304} INFO - Sending failed status from callback...
[2024-12-16, 07:33:35 UTC] {server_mixin.py:74} INFO - OpenMetadata client running with Server version [1.5.12] and Client version [1.5.12.0]
[2024-12-16, 07:33:35 UTC] {common.py:310} INFO - Sending status to Ingestion Pipeline tessera-sql4.online_view.dbo.FL_Polk_Circuit_View.testSuite.tessera-sql4_online_view_dbo_FL_Polk_Circuit_View_Pipeline
[2024-12-16, 07:33:36 UTC] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py"", line 400, in wrapper
    return func(self, *args, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py"", line 235, in execute
    return_value = self.execute_callable()
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/python.py"", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/openmetadata_managed_apis/workflows/ingestion/test_suite.py"", line 48, in test_suite_workflow
    workflow.execute()
  File ""/home/airflow/.local/lib/python3.10/site-packages/metadata/workflow/base.py"", line 187, in execute
    self.execute_internal()
  File ""/home/airflow/.local/lib/python3.10/site-packages/metadata/workflow/ingestion.py"", line 131, in execute_internal
    processed_record = step.run(processed_record)
  File ""/home/airflow/.local/lib/python3.10/site-packages/metadata/ingestion/api/step.py"", line 109, in run
    result: Either = self._run(record)
  File ""/home/airflow/.local/lib/python3.10/site-packages/metadata/data_quality/processor/test_case_runner.py"", line 111, in _run
    test_results = [
  File ""/home/airflow/.local/lib/python3.10/site-packages/metadata/data_quality/processor/test_case_runner.py"", line 114, in <listcomp>
    if (test_case_result := self._run_test_case(test_case, test_suite_runner))
  File ""/home/airflow/.local/lib/python3.10/site-packages/metadata/data_quality/processor/test_case_runner.py"", line 295, in _run_test_case
    test_result = test_suite_runner.run_and_handle(test_case)
  File ""/home/airflow/.local/lib/python3.10/site-packages/metadata/data_quality/runner/core.py"", line 37, in run_and_handle
    test_result = self.test_runner_interface.run_test_case(
  File ""/home/airflow/.local/lib/python3.10/site-packages/metadata/data_quality/interface/test_suite_interface.py"", line 124, in run_test_case
    return validator.run_validation()
  File ""/home/airflow/.local/lib/python3.10/site-packages/metadata/data_quality/validations/table/sqlalchemy/TableColumnCompletenessToMeetThreshold.py"", line 224, in run_validation
    null_missing_result = self.run_query_results(
  File ""/home/airflow/.local/lib/python3.10/site-packages/metadata/data_quality/validations/mixins/sqa_validator_mixin.py"", line 75, in run_query_results
    value = dict(runner.dispatch_query_select_first(metric_fn))  # type: ignore
  File ""/home/airflow/.local/lib/python3.10/site-packages/metadata/utils/timeout.py"", line 59, in inner
    result = fn(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/metadata/profiler/processor/runner.py"", line 147, in dispatch_query_select_first
    return self.select_first_from_table(*entities, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/metadata/utils/timeout.py"", line 59, in inner
    result = fn(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/metadata/profiler/processor/handle_partition.py"", line 126, in handle_and_execute
    return func(_self, *args, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/metadata/profiler/processor/runner.py"", line 102, in select_first_from_table
    return query.first()
  File ""/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/query.py"", line 2824, in first
    return self.limit(1)._iter().first()
  File ""/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/query.py"", line 2916, in _iter
    result = self.session.execute(
  File ""/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py"", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File ""/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File ""/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/sql/elements.py"", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File ""/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File ""/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File ""/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 2138, in _handle_dbapi_exception
    util.raise_(exc_info[1], with_traceback=exc_info[2])
  File ""/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py"", line 211, in raise_
    raise exception
  File ""/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context
    self.dialect.do_execute(
  File ""/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py"", line 736, in do_execute
    cursor.execute(statement, parameters)
  File ""/home/airflow/.local/lib/python3.10/site-packages/pytds/cursor.py"", line 322, in execute
    self._session.execute(operation, params)
  File ""/home/airflow/.local/lib/python3.10/site-packages/pytds/tds_session.py"", line 916, in execute
    self.begin_response()
  File ""/home/airflow/.local/lib/python3.10/site-packages/pytds/tds_session.py"", line 1261, in begin_response
    return self._reader.begin_response()
  File ""/home/airflow/.local/lib/python3.10/site-packages/pytds/tds_reader.py"", line 201, in begin_response
    self._read_packet()
  File ""/home/airflow/.local/lib/python3.10/site-packages/pytds/tds_reader.py"", line 214, in _read_packet
    received = self._transport.recv_into(
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 2613, in signal_handler
    raise AirflowTaskTerminated(""Task received SIGTERM signal"")
airflow.exceptions.AirflowTaskTerminated: Task received SIGTERM signal
[2024-12-16, 07:33:36 UTC] {common.py:304} INFO - Sending failed status from callback...
```
Then task restarts with
```
*** Found local files:
***   * /opt/airflow/logs/dag_id=tessera-sql4_online_view_dbo_FL_Polk_Circuit_View_Pipeline/run_id=manual__2024-12-16T07:30:55+00:00/task_id=test_suite_task/attempt=2.log
[2024-12-16, 07:33:30 UTC] {local_task_job_runner.py:120} ▶ Pre task execution logs
[2024-12-16, 07:33:31 UTC] {server_mixin.py:74} INFO - OpenMetadata client running with Server version [1.5.12] and Client version [1.5.12.0]
[2024-12-16, 07:33:33 UTC] {core.py:33} INFO - Executing test case TableColumnCompletenessToMeetThresholdTest for entity tessera-sql4.online_view.dbo.FL_Polk_Circuit_View
[2024-12-16, 07:33:33 UTC] {server_mixin.py:74} INFO - OpenMetadata client running with Server version [1.5.12] and Client version [1.5.12.0]
[2024-12-16, 07:33:33 UTC] {TableColumnCompletenessToMeetThreshold.py:204} INFO - TestParameters(min_threshold=15, max_threshold=15, columns_to_display=10)
[2024-12-16, 07:33:33 UTC] {TableColumnCompletenessToMeetThreshold.py:211} INFO - Columns to check:213:[Column('Source_ID', VARCHAR(), table=<FL_Polk_Circuit_View>, key='source_id', primary_key=True, nullable=False), Column('Offender_ID', VARCHAR(), table=<FL_Polk_Circuit_View>, key='offender_id'), Column('File_Data_Date', VARCHAR(), table=<FL_Polk_Circuit_View>, key='file_data_date'), Column('DC_Number', VARCHAR(), table=<FL_Polk_Circuit_View>, key='dc_number'), Column('Inmate_Number', VARCHAR(), table=<FL_Polk_Circuit_View>, key='inmate_number'), Column('FBI_Number', VARCHAR(), table=<FL_Polk_Circuit_View>, key='fbi_number'), Column('State_ID_Number', VARCHAR(), table=<FL_Polk_Circuit_View>, key='state_id_number'), Column('State_Code', VARCHAR(), table=<FL_Polk_Circuit_View>, key='state_code'), Column('Booking_no', VARCHAR(), table=<FL_Polk_Circuit_View>, key='booking_no'), Column('Full_Name', VARCHAR(), table=<FL_Polk_Circuit_View>, key='full_name'), Column('Last_Name', VARCHAR(), table=<FL_Polk_Circuit_View>, key='last_name'), Column('First_Name', VARCHAR(), table=<FL_Polk_Circuit_View>, key='first_name'), Column('Middle_Name', VARCHAR(), table=<FL_Polk_Circuit_View>, key='middle_name'), Column('Suffix', VARCHAR(), table=<FL_Polk_Circuit_View>, key='suffix'), Column('DOB', VARCHAR(), table=<FL_Polk_Circuit_View>, key='dob'), Column('Age', VARCHAR(), table=<FL_Polk_Circuit_View>, key='age'), Column('Birth_County', VARCHAR(), table=<FL_Polk_Circuit_View>, key='birth_county'), Column('Birth_State', VARCHAR(), table=<FL_Polk_Circuit_View>, key='birth_state'), Column('Birth_Country', VARCHAR(), table=<FL_Polk_Circuit_View>, key='birth_country'), Column('Gender_Code', VARCHAR(), table=<FL_Polk_Circuit_View>, key='gender_code'), Column('Gender', VARCHAR(), table=<FL_Polk_Circuit_View>, key='gender'), Column('Race_Code', VARCHAR(), table=<FL_Polk_Circuit_View>, key='race_code'), Column('Race', VARCHAR(), table=<FL_Polk_Circuit_View>, key='race'), Column('Ethnicity_Code', VARCHAR(), table=<FL_Polk_Circuit_View>, key='ethnicity_code'), Column('Ethnicity', VARCHAR(), table=<FL_Polk_Circuit_View>, key='ethnicity'), Column('Eye_Color', VARCHAR(), table=<FL_Polk_Circuit_View>, key='eye_color'), Column('Hair_Color', VARCHAR(), table=<FL_Polk_Circuit_View>, key='hair_color'), Column('Skin_Color', VARCHAR(), table=<FL_Polk_Circuit_View>, key='skin_color'), Column('Height', VARCHAR(), table=<FL_Polk_Circuit_View>, key='height'), Column('Weight', VARCHAR(), table=<FL_Polk_Circuit_View>, key='weight'), Column('Physical_Build', VARCHAR(), table=<FL_Polk_Circuit_View>, key='physical_build'), Column('SSN', VARCHAR(), table=<FL_Polk_Circuit_View>, key='ssn'), Column('Driver_License_Number', VARCHAR(), table=<FL_Polk_Circuit_View>, key='driver_license_number'), Column('Driver_License_State_Issue', VARCHAR(), table=<FL_Polk_Circuit_View>, key='driver_license_state_issue'), Column('Other_DOB_Used', VARCHAR(), table=<FL_Polk_Circuit_View>, key='other_dob_used'), Column('Citizenship', VARCHAR(), table=<FL_Polk_Circuit_View>, key='citizenship'), Column('Military_Service', VARCHAR(), table=<FL_Polk_Circuit_View>, key='military_service'), Column('Military_Branch', VARCHAR(), table=<FL_Polk_Circuit_View>, key='military_branch'), Column('Military_Discharge', VARCHAR(), table=<FL_Polk_Circuit_View>, key='military_discharge'), Column('Military_Discharge_Date', VARCHAR(), table=<FL_Polk_Circuit_View>, key='military_discharge_date'), Column('ScareMarkTatoo', VARCHAR(), table=<FL_Polk_Circuit_View>, key='scaremarktatoo'), Column('Photo', VARCHAR(), table=<FL_Polk_Circuit_View>, key='photo'), Column('Vehicle_year', VARCHAR(), table=<FL_Polk_Circuit_View>, key='vehicle_year'), Column('Vehicle_color', VARCHAR(), table=<FL_Polk_Circuit_View>, key='vehicle_color'), Column('Vehicle_mark', VARCHAR(), table=<FL_Polk_Circuit_View>, key='vehicle_mark'), Column('Vehicle_model', VARCHAR(), table=<FL_Polk_Circuit_View>, key='vehicle_model'), Column('Alias_full_Name', VARCHAR(), table=<FL_Polk_Circuit_View>, key='alias_full_name'), Column('Alias_First_Name', VARCHAR(), table=<FL_Polk_Circuit_View>, key='alias_first_name'), Column('Alias_Middle_Name', VARCHAR(), table=<FL_Polk_Circuit_View>, key='alias_middle_name'), Column('Alias_Last_Name', VARCHAR(), table=<FL_Polk_Circuit_View>, key='alias_last_name'), Column('Alias_Suffix', VARCHAR(), table=<FL_Polk_Circuit_View>, key='alias_suffix'), Column('Judge', VARCHAR(), table=<FL_Polk_Circuit_View>, key='judge'), Column('Attorney', VARCHAR(), table=<FL_Polk_Circuit_View>, key='attorney'), Column('Party_code', VARCHAR(), table=<FL_Polk_Circuit_View>, key='party_code'), Column('Party_Relationship', VARCHAR(), table=<FL_Polk_Circuit_View>, key='party_relationship'), Column('Party_full_name', VARCHAR(), table=<FL_Polk_Circuit_View>, key='party_full_name'), Column('Party_Last_name', VARCHAR(), table=<FL_Polk_Circuit_View>, key='party_last_name'), Column('Party_First_name', VARCHAR(), table=<FL_Polk_Circuit_View>, key='party_first_name'), Column('Party_Middle_name', VARCHAR(), table=<FL_Polk_Circuit_View>, key='party_middle_name'), Column('Party_Suffix_name', VARCHAR(), table=<FL_Polk_Circuit_View>, key='party_suffix_name'), Column('Party_DOB', VARCHAR(), table=<FL_Polk_Circuit_View>, key='party_dob'), Column('Party_Age', VARCHAR(), table=<FL_Polk_Circuit_View>, key='party_age'), Column('Party_Sex', VARCHAR(), table=<FL_Polk_Circuit_View>, key='party_sex'), Column('Party_Address', VARCHAR(), table=<FL_Polk_Circuit_View>, key='party_address'), Column('Party_City', VARCHAR(), table=<FL_Polk_Circuit_View>, key='party_city'), Column('Party_State', VARCHAR(), table=<FL_Polk_Circuit_View>, key='party_state'), Column('Party_Zip', VARCHAR(), table=<FL_Polk_Circuit_View>, key='party_zip'), Column('Party_Phone', VARCHAR(), table=<FL_Polk_Circuit_View>, key='party_phone'), Column('Full_address', VARCHAR(), table=<FL_Polk_Circuit_View>, key='full_address'), Column('Address_1', VARCHAR(), table=<FL_Polk_Circuit_View>, key='address_1'), Column('Address_2', VARCHAR(), table=<FL_Polk_Circuit_View>, key='address_2'), Column('Address_3', VARCHAR(), table=<FL_Polk_Circuit_View>, key='address_3'), Column('City', VARCHAR(), table=<FL_Polk_Circuit_View>, key='city'), Column('County', VARCHAR(), table=<FL_Polk_Circuit_View>, key='county'), Column('State', VARCHAR(), table=<FL_Polk_Circuit_View>, key='state'), Column('Zip', VARCHAR(), table=<FL_Polk_Circuit_View>, key='zip'), Column('Phone', VARCHAR(), table=<FL_Polk_Circuit_View>, key='phone'), Column('Last_Institution_Code', VARCHAR(), table=<FL_Polk_Circuit_View>, key='last_institution_code'), Column('Last_Institution', VARCHAR(), table=<FL_Polk_Circuit_View>, key='last_institution'), Column('Last_Institution_Address', VARCHAR(), table=<FL_Polk_Circuit_View>, key='last_institution_address'), Column('Last_Institution_City', VARCHAR(), table=<FL_Polk_Circuit_View>, key='last_institution_city'), Column('Last_Institution_State', VARCHAR(), table=<FL_Polk_Circuit_View>, key='last_institution_state'), Column('Last_Institution_Zip', VARCHAR(), table=<FL_Polk_Circuit_View>, key='last_institution_zip'), Column('Institution_Code', VARCHAR(), table=<FL_Polk_Circuit_View>, key='institution_code'), Column('Institution_Name', VARCHAR(), table=<FL_Polk_Circuit_View>, key='institution_name'), Column('Institution_Address', VARCHAR(), table=<FL_Polk_Circuit_View>, key='institution_address'), Column('Institution_City', VARCHAR(), table=<FL_Polk_Circuit_View>, key='institution_city'), Column('Institution_State', VARCHAR(), table=<FL_Polk_Circuit_View>, key='institution_state'), Column('Institution_Zip', VARCHAR(), table=<FL_Polk_Circuit_View>, key='institution_zip'), Column('Institution_Phone', VARCHAR(), table=<FL_Polk_Circuit_View>, key='institution_phone'), Column('Institution_Details', VARCHAR(), table=<FL_Polk_Circuit_View>, key='institution_details'), Column('Released_to_Name', VARCHAR(), table=<FL_Polk_Circuit_View>, key='released_to_name'), Column('Released_to_Address', VARCHAR(), table=<FL_Polk_Circuit_View>, key='released_to_address'), Column('Released_to_City', VARCHAR(), table=<FL_Polk_Circuit_View>, key='released_to_city'), Column('Released_to_State', VARCHAR(), table=<FL_Polk_Circuit_View>, key='released_to_state'), Column('Released_to_Zip', VARCHAR(), table=<FL_Polk_Circuit_View>, key='released_to_zip'), Column('Release_Information', VARCHAR(), table=<FL_Polk_Circuit_View>, key='release_information'), Column('Court_Case_Number', VARCHAR(), table=<FL_Polk_Circuit_View>, key='court_case_number'), Column('Court_Code', VARCHAR(), table=<FL_Polk_Circuit_View>, key='court_code'), Column('Court_Name', VARCHAR(), table=<FL_Polk_Circuit_View>, key='court_name'), Column('Court_County', VARCHAR(), table=<FL_Polk_Circuit_View>, key='court_county'), Column('Court_type', VARCHAR(), table=<FL_Polk_Circuit_View>, key='court_type'), Column('Case_Category_Code', VARCHAR(), table=<FL_Polk_Circuit_View>, key='case_category_code'), Column('Case_Category', VARCHAR(), table=<FL_Polk_Circuit_View>, key='case_category'), Column('Case_Year', VARCHAR(), table=<FL_Polk_Circuit_View>, key='case_year'), Column('Case_Type_code', VARCHAR(), table=<FL_Polk_Circuit_View>, key='case_type_code'), Column('Case_Type', VARCHAR(), table=<FL_Polk_Circuit_View>, key='case_type'), Column('Arresting_Agency', VARCHAR(), table=<FL_Polk_Circuit_View>, key='arresting_agency'), Column('Filing_Agency', VARCHAR(), table=<FL_Polk_Circuit_View>, key='filing_agency'), Column('Date_Filed', VARCHAR(), table=<FL_Polk_Circuit_View>, key='date_filed'), Column('Arrest_Date', VARCHAR(), table=<FL_Polk_Circuit_View>, key='arrest_date'), Column('Arraignment_Date', VARCHAR(), table=<FL_Polk_Circuit_View>, key='arraignment_date'), Column('Court_Clerk_Phone', VARCHAR(), table=<FL_Polk_Circuit_View>, key='court_clerk_phone'), Column('Fine', VARCHAR(), table=<FL_Polk_Circuit_View>, key='fine'), Column('Court_cost', VARCHAR(), table=<FL_Polk_Circuit_View>, key='court_cost'), Column('Case_disposition_code', VARCHAR(), table=<FL_Polk_Circuit_View>, key='case_disposition_code'), Column('Case_disposition', VARCHAR(), table=<FL_Polk_Circuit_View>, key='case_disposition'), Column('Rebuttal', VARCHAR(), table=<FL_Polk_Circuit_View>, key='rebuttal'), Column('Bail_amount', VARCHAR(), table=<FL_Polk_Circuit_View>, key='bail_amount'), Column('Bond', VARCHAR(), table=<FL_Polk_Circuit_View>, key='bond'), Column('Offender_Status_Code', VARCHAR(), table=<FL_Polk_Circuit_View>, key='offender_status_code'), Column('Offender_Status', VARCHAR(), table=<FL_Polk_Circuit_View>, key='offender_status'), Column('Case_status', VARCHAR(), table=<FL_Polk_Circuit_View>, key='case_status'), Column('NCIC_Code', VARCHAR(), table=<FL_Polk_Circuit_View>, key='ncic_code'), Column('Offense_Code', VARCHAR(), table=<FL_Polk_Circuit_View>, key='offense_code'), Column('Offense_Prefix', VARCHAR(), table=<FL_Polk_Circuit_View>, key='offense_prefix'), Column('Offense', VARCHAR(), table=<FL_Polk_Circuit_View>, key='offense'), Column('Offense_Type', VARCHAR(), table=<FL_Polk_Circuit_View>, key='offense_type'), Column('Offense_Class', VARCHAR(), table=<FL_Polk_Circuit_View>, key='offense_class'), Column('Offense_Degree', VARCHAR(), table=<FL_Polk_Circuit_View>, key='offense_degree'), Column('Offense_Level', VARCHAR(), table=<FL_Polk_Circuit_View>, key='offense_level'), Column('Amended_Offense', VARCHAR(), table=<FL_Polk_Circuit_View>, key='amended_offense'), Column('Amended_Offense_Type', VARCHAR(), table=<FL_Polk_Circuit_View>, key='amended_offense_type'), Column('Amended_Offense_class', VARCHAR(), table=<FL_Polk_Circuit_View>, key='amended_offense_class'), Column('Amended_Offense_Degree', VARCHAR(), table=<FL_Polk_Circuit_View>, key='amended_offense_degree'), Column('Amended_Offense_Level', VARCHAR(), table=<FL_Polk_Circuit_View>, key='amended_offense_level'), Column('Sub_Offense_Code', VARCHAR(), table=<FL_Polk_Circuit_View>, key='sub_offense_code'), Column('Sub_Offense_Prefix', VARCHAR(), table=<FL_Polk_Circuit_View>, key='sub_offense_prefix'), Column('Sub_Offense', VARCHAR(), table=<FL_Polk_Circuit_View>, key='sub_offense'), Column('Sub_Offense_Type', VARCHAR(), table=<FL_Polk_Circuit_View>, key='sub_offense_type'), Column('Sub_Offense_Class', VARCHAR(), table=<FL_Polk_Circuit_View>, key='sub_offense_class'), Column('Charge_No', VARCHAR(), table=<FL_Polk_Circuit_View>, key='charge_no'), Column('Total_Charge_counts', VARCHAR(), table=<FL_Polk_Circuit_View>, key='total_charge_counts'), Column('Offense_Date', VARCHAR(), table=<FL_Polk_Circuit_View>, key='offense_date'), Column('Citation_Number', VARCHAR(), table=<FL_Polk_Circuit_View>, key='citation_number'), Column('Offense_City', VARCHAR(), table=<FL_Polk_Circuit_View>, key='offense_city'), Column('Offense_County', VARCHAR(), table=<FL_Polk_Circuit_View>, key='offense_county'), Column('Offense_State', VARCHAR(), table=<FL_Polk_Circuit_View>, key='offense_state'), Column('Original_Plea_code', VARCHAR(), table=<FL_Polk_Circuit_View>, key='original_plea_code'), Column('Original_Plea', VARCHAR(), table=<FL_Polk_Circuit_View>, key='original_plea'), Column('Original_Plea_Date', VARCHAR(), table=<FL_Polk_Circuit_View>, key='original_plea_date'), Column('Plea_Withdrawn_Date', VARCHAR(), table=<FL_Polk_Circuit_View>, key='plea_withdrawn_date'), Column('New_Plea_code', VARCHAR(), table=<FL_Polk_Circuit_View>, key='new_plea_code'), Column('New_Plea', VARCHAR(), table=<FL_Polk_Circuit_View>, key='new_plea'), Column('Verdict_Finding_code', VARCHAR(), table=<FL_Polk_Circuit_View>, key='verdict_finding_code'), Column('Verdict_Finding', VARCHAR(), table=<FL_Polk_Circuit_View>, key='verdict_finding'), Column('Verdict_Conviction_Date', VARCHAR(), table=<FL_Polk_Circuit_View>, key='verdict_conviction_date'), Column('County_Convicted', VARCHAR(), table=<FL_Polk_Circuit_View>, key='county_convicted'), Column('State_Convicted', VARCHAR(), table=<FL_Polk_Circuit_View>, key='state_convicted'), Column('Offense_Disposition_code', VARCHAR(), table=<FL_Polk_Circuit_View>, key='offense_disposition_code'), Column('Offense_Disposition', VARCHAR(), table=<FL_Polk_Circuit_View>, key='offense_disposition'), Column('Offense_Disposition_Date', VARCHAR(), table=<FL_Polk_Circuit_View>, key='offense_disposition_date'), Column('Warrant_Date', VARCHAR(), table=<FL_Polk_Circuit_View>, key='warrant_date'), Column('Sentence_Type', VARCHAR(), table=<FL_Polk_Circuit_View>, key='sentence_type'), Column('Sentence_Max_Years', VARCHAR(), table=<FL_Polk_Circuit_View>, key='sentence_max_years'), Column('Sentence_Max_Months', VARCHAR(), table=<FL_Polk_Circuit_View>, key='sentence_max_months'), Column('Sentence_Max_Days', VARCHAR(), table=<FL_Polk_Circuit_View>, key='sentence_max_days'), Column('Sentence_Min_Years', VARCHAR(), table=<FL_Polk_Circuit_View>, key='sentence_min_years'), Column('Sentence_Min_Months', VARCHAR(), table=<FL_Polk_Circuit_View>, key='sentence_min_months'), Column('Sentence_Min_Days', VARCHAR(), table=<FL_Polk_Circuit_View>, key='sentence_min_days'), Column('Sentence_length', VARCHAR(), table=<FL_Polk_Circuit_View>, key='sentence_length'), Column('Sentence_Date', VARCHAR(), table=<FL_Polk_Circuit_View>, key='sentence_date'), Column('Sentence_Begin_Date', VARCHAR(), table=<FL_Polk_Circuit_View>, key='sentence_begin_date'), Column('Sentence_Details', VARCHAR(), table=<FL_Polk_Circuit_View>, key='sentence_details'), Column('Parole_Start_Date', VARCHAR(), table=<FL_Polk_Circuit_View>, key='parole_start_date'), Column('Parole_End_Date', VARCHAR(), table=<FL_Polk_Circuit_View>, key='parole_end_date'), Column('Parole_Max_Years', VARCHAR(), table=<FL_Polk_Circuit_View>, key='parole_max_years'), Column('Parole_Max_Months', VARCHAR(), table=<FL_Polk_Circuit_View>, key='parole_max_months'), Column('Parole_Max_Days', VARCHAR(), table=<FL_Polk_Circuit_View>, key='parole_max_days'), Column('Parole_Min_Years', VARCHAR(), table=<FL_Polk_Circuit_View>, key='parole_min_years'), Column('Parole_Min_Months', VARCHAR(), table=<FL_Polk_Circuit_View>, key='parole_min_months'), Column('Parole_Min_Days', VARCHAR(), table=<FL_Polk_Circuit_View>, key='parole_min_days'), Column('Probation_Start_Date', VARCHAR(), table=<FL_Polk_Circuit_View>, key='probation_start_date'), Column('Probation_End_Date', VARCHAR(), table=<FL_Polk_Circuit_View>, key='probation_end_date'), Column('Probation_Max_Years', VARCHAR(), table=<FL_Polk_Circuit_View>, key='probation_max_years'), Column('Probation_Max_Months', VARCHAR(), table=<FL_Polk_Circuit_View>, key='probation_max_months'), Column('Probation_Max_Days', VARCHAR(), table=<FL_Polk_Circuit_View>, key='probation_max_days'), Column('Probation_Min_Years', VARCHAR(), table=<FL_Polk_Circuit_View>, key='probation_min_years'), Column('Probation_Min_Months', VARCHAR(), table=<FL_Polk_Circuit_View>, key='probation_min_months'), Column('Probation_Min_Days', VARCHAR(), table=<FL_Polk_Circuit_View>, key='probation_min_days'), Column('Probation_Agency', VARCHAR(), table=<FL_Polk_Circuit_View>, key='probation_agency'), Column('Probation_length', VARCHAR(), table=<FL_Polk_Circuit_View>, key='probation_length'), Column('Maximum_Release_Date', VARCHAR(), table=<FL_Polk_Circuit_View>, key='maximum_release_date'), Column('Scheduled_Release_Date', VARCHAR(), table=<FL_Polk_Circuit_View>, key='scheduled_release_date'), Column('Actual_Release_Date', VARCHAR(), table=<FL_Polk_Circuit_View>, key='actual_release_date'), Column('Release_Type', VARCHAR(), table=<FL_Polk_Circuit_View>, key='release_type'), Column('Time_Served_Years', VARCHAR(), table=<FL_Polk_Circuit_View>, key='time_served_years'), Column('Time_Served_Months', VARCHAR(), table=<FL_Polk_Circuit_View>, key='time_served_months'), Column('Time_Served_Days', VARCHAR(), table=<FL_Polk_Circuit_View>, key='time_served_days'), Column('Time_Served_length', VARCHAR(), table=<FL_Polk_Circuit_View>, key='time_served_length'), Column('Probation_Follows_Y_N', VARCHAR(), table=<FL_Polk_Circuit_View>, key='probation_follows_y_n'), Column('Supervision_County', VARCHAR(), table=<FL_Polk_Circuit_View>, key='supervision_county'), Column('Sentence_Status', VARCHAR(), table=<FL_Polk_Circuit_View>, key='sentence_status'), Column('Probation_Parole_Status', VARCHAR(), table=<FL_Polk_Circuit_View>, key='probation_parole_status'), Column('Sentence_Comments1', VARCHAR(), table=<FL_Polk_Circuit_View>, key='sentence_comments1'), Column('Sentence_Comments2', VARCHAR(), table=<FL_Polk_Circuit_View>, key='sentence_comments2'), Column('Suspended_Jail_Years', VARCHAR(), table=<FL_Polk_Circuit_View>, key='suspended_jail_years'), Column('Suspended_Jail_Months', VARCHAR(), table=<FL_Polk_Circuit_View>, key='suspended_jail_months'), Column('Suspended_Jail_Days', VARCHAR(), table=<FL_Polk_Circuit_View>, key='suspended_jail_days'), Column('Admission_Date', VARCHAR(), table=<FL_Polk_Circuit_View>, key='admission_date'), Column('Admission_Type', VARCHAR(), table=<FL_Polk_Circuit_View>, key='admission_type'), Column('EPR_File_Record_Date', DATETIME(), table=<FL_Polk_Circuit_View>, key='epr_file_record_date'), Column('address_verification_date', VARCHAR(), table=<FL_Polk_Circuit_View>)]
[2024-12-16, 07:33:33 UTC] {TableColumnCompletenessToMeetThreshold.py:55} INFO - Getting previous test results
[2024-12-16, 07:33:33 UTC] {TableColumnCompletenessToMeetThreshold.py:324} INFO - test_result.testCaseStatus: Success
[2024-12-16, 07:33:33 UTC] {TableColumnCompletenessToMeetThreshold.py:336} INFO - Found previous test results: {""Source_ID"": 100.0, ""Offender_ID"": 100.0, ""File_Data_Date"": 0.0, ""DC_Number"": 96.58, ""Inmate_Number"": 0.0, ""FBI_Number"": 0.0, ""State_ID_Number"": 96.58, ""State_Code"": 0.0, ""Booking_no"": 0.0, ""Full_Name"": 100.0, ""Last_Name"": 100.0, ""First_Name"": 100.0, ""Middle_Name"": 84.98, ""Suffix"": 0.0, ""DOB"": 99.84, ""Age"": 0.0, ""Birth_County"": 0.0, ""Birth_State"": 0.0, ""Birth_Country"": 0.0, ""Gender_Code"": 0.0, ""Gender"": 0.0, ""Race_Code"": 0.0, ""Race"": 0.0, ""Ethnicity_Code"": 0.0, ""Ethnicity"": 0.0, ""Eye_Color"": 0.0, ""Hair_Color"": 0.0, ""Skin_Color"": 0.0, ""Height"": 0.0, ""Weight"": 0.0, ""Physical_Build"": 0.0, ""SSN"": 0.0, ""Driver_License_Number"": 0.0, ""Driver_License_State_Issue"": 0.0, ""Other_DOB_Used"": 0.0, ""Citizenship"": 0.0, ""Military_Service"": 0.0, ""Military_Branch"": 0.0, ""Military_Discharge"": 0.0, ""Military_Discharge_Date"": 0.0, ""ScareMarkTatoo"": 0.0, ""Photo"": 0.0, ""Vehicle_year"": 0.0, ""Vehicle_color"": 0.0, ""Vehicle_mark"": 0.0, ""Vehicle_model"": 0.0, ""Alias_full_Name"": 0.0, ""Alias_First_Name"": 0.0, ""Alias_Middle_Name"": 0.0, ""Alias_Last_Name"": 0.0, ""Alias_Suffix"": 0.0, ""Judge"": 99.34, ""Attorney"": 0.0, ""Party_code"": 0.0, ""Party_Relationship"": 0.0, ""Party_full_name"": 0.0, ""Party_Last_name"": 0.0, ""Party_First_name"": 0.0, ""Party_Middle_name"": 0.0, ""Party_Suffix_name"": 0.0, ""Party_DOB"": 0.0, ""Party_Age"": 0.0, ""Party_Sex"": 0.0, ""Party_Address"": 0.0, ""Party_City"": 0.0, ""Party_State"": 0.0, ""Party_Zip"": 0.0, ""Party_Phone"": 0.0, ""Full_address"": 100.0, ""Address_1"": 99.55, ""Address_2"": 5.47, ""Address_3"": 0.0, ""City"": 99.41, ""County"": 0.0, ""State"": 99.4, ""Zip"": 98.85, ""Phone"": 0.0, ""Last_Institution_Code"": 0.0, ""Last_Institution"": 0.0, ""Last_Institution_Address"": 0.0, ""Last_Institution_City"": 0.0, ""Last_Institution_State"": 0.0, ""Last_Institution_Zip"": 0.0, ""Institution_Code"": 0.0, ""Institution_Name"": 0.0, ""Institution_Address"": 0.0, ""Institution_City"": 0.0, ""Institution_State"": 0.0, ""Institution_Zip"": 0.0, ""Institution_Phone"": 0.0, ""Institution_Details"": 0.0, ""Released_to_Name"": 0.0, ""Released_to_Address"": 0.0, ""Released_to_City"": 0.0, ""Released_to_State"": 0.0, ""Released_to_Zip"": 0.0, ""Release_Information"": 0.0, ""Court_Case_Number"": 100.0, ""Court_Code"": 0.0, ""Court_Name"": 0.0, ""Court_County"": 0.0, ""Court_type"": 0.0, ""Case_Category_Code"": 0.0, ""Case_Category"": 0.0, ""Case_Year"": 0.0, ""Case_Type_code"": 0.0, ""Case_Type"": 0.0, ""Arresting_Agency"": 99.28, ""Filing_Agency"": 0.0, ""Date_Filed"": 99.29, ""Arrest_Date"": 97.27, ""Arraignment_Date"": 0.0, ""Court_Clerk_Phone"": 0.0, ""Fine"": 0.0, ""Court_cost"": 0.0, ""Case_disposition_code"": 0.0, ""Case_disposition"": 0.0, ""Rebuttal"": 0.0, ""Bail_amount"": 0.0, ""Bond"": 0.0, ""Offender_Status_Code"": 0.0, ""Offender_Status"": 0.0, ""Case_status"": 100.0, ""NCIC_Code"": 0.0, ""Offense_Code"": 97.78, ""Offense_Prefix"": 0.0, ""Offense"": 99.34, ""Offense_Type"": 99.27, ""Offense_Class"": 0.0, ""Offense_Degree"": 99.27, ""Offense_Level"": 0.0, ""Amended_Offense"": 0.0, ""Amended_Offense_Type"": 0.0, ""Amended_Offense_class"": 0.0, ""Amended_Offense_Degree"": 0.0, ""Amended_Offense_Level"": 0.0, ""Sub_Offense_Code"": 0.0, ""Sub_Offense_Prefix"": 0.0, ""Sub_Offense"": 0.0, ""Sub_Offense_Type"": 0.0, ""Sub_Offense_Class"": 0.0, ""Charge_No"": 99.34, ""Total_Charge_counts"": 0.0, ""Offense_Date"": 99.33, ""Citation_Number"": 4.28, ""Offense_City"": 0.0, ""Offense_County"": 0.0, ""Offense_State"": 0.0, ""Original_Plea_code"": 0.0, ""Original_Plea"": 0.0, ""Original_Plea_Date"": 0.0, ""Plea_Withdrawn_Date"": 0.0, ""New_Plea_code"": 0.0, ""New_Plea"": 0.0, ""Verdict_Finding_code"": 0.0, ""Verdict_Finding"": 0.0, ""Verdict_Conviction_Date"": 0.0, ""County_Convicted"": 0.0, ""State_Convicted"": 0.0, ""Offense_Disposition_code"": 94.15, ""Offense_Disposition"": 94.14, ""Offense_Disposition_Date"": 94.15, ""Warrant_Date"": 0.0, ""Sentence_Type"": 33.95, ""Sentence_Max_Years"": 0.0, ""Sentence_Max_Months"": 0.0, ""Sentence_Max_Days"": 0.0, ""Sentence_Min_Years"": 0.0, ""Sentence_Min_Months"": 0.0, ""Sentence_Min_Days"": 0.0, ""Sentence_length"": 33.98, ""Sentence_Date"": 0.0, ""Sentence_Begin_Date"": 0.0, ""Sentence_Details"": 26.35, ""Parole_Start_Date"": 0.0, ""Parole_End_Date"": 0.0, ""Parole_Max_Years"": 0.0, ""Parole_Max_Months"": 0.0, ""Parole_Max_Days"": 0.0, ""Parole_Min_Years"": 0.0, ""Parole_Min_Months"": 0.0, ""Parole_Min_Days"": 0.0, ""Probation_Start_Date"": 0.0, ""Probation_End_Date"": 0.0, ""Probation_Max_Years"": 0.0, ""Probation_Max_Months"": 0.0, ""Probation_Max_Days"": 0.0, ""Probation_Min_Years"": 0.0, ""Probation_Min_Months"": 0.0, ""Probation_Min_Days"": 0.0, ""Probation_Agency"": 0.0, ""Probation_length"": 13.61, ""Maximum_Release_Date"": 0.0, ""Scheduled_Release_Date"": 0.0, ""Actual_Release_Date"": 0.0, ""Release_Type"": 0.0, ""Time_Served_Years"": 0.0, ""Time_Served_Months"": 0.0, ""Time_Served_Days"": 0.0, ""Time_Served_length"": 0.0, ""Probation_Follows_Y_N"": 0.0, ""Supervision_County"": 0.0, ""Sentence_Status"": 0.0, ""Probation_Parole_Status"": 0.0, ""Sentence_Comments1"": 13.61, ""Sentence_Comments2"": 0.0, ""Suspended_Jail_Years"": 0.0, ""Suspended_Jail_Months"": 0.0, ""Suspended_Jail_Days"": 0.0, ""Admission_Date"": 0.0, ""Admission_Type"": 0.0, ""EPR_File_Record_Date"": 100.0, ""address_verification_date"": 0.0}
[2024-12-16, 07:33:33 UTC] {TableColumnCompletenessToMeetThreshold.py:346} INFO - Parsed JSON previous results: {'Source_ID': 100.0, 'Offender_ID': 100.0, 'File_Data_Date': 0.0, 'DC_Number': 96.58, 'Inmate_Number': 0.0, 'FBI_Number': 0.0, 'State_ID_Number': 96.58, 'State_Code': 0.0, 'Booking_no': 0.0, 'Full_Name': 100.0, 'Last_Name': 100.0, 'First_Name': 100.0, 'Middle_Name': 84.98, 'Suffix': 0.0, 'DOB': 99.84, 'Age': 0.0, 'Birth_County': 0.0, 'Birth_State': 0.0, 'Birth_Country': 0.0, 'Gender_Code': 0.0, 'Gender': 0.0, 'Race_Code': 0.0, 'Race': 0.0, 'Ethnicity_Code': 0.0, 'Ethnicity': 0.0, 'Eye_Color': 0.0, 'Hair_Color': 0.0, 'Skin_Color': 0.0, 'Height': 0.0, 'Weight': 0.0, 'Physical_Build': 0.0, 'SSN': 0.0, 'Driver_License_Number': 0.0, 'Driver_License_State_Issue': 0.0, 'Other_DOB_Used': 0.0, 'Citizenship': 0.0, 'Military_Service': 0.0, 'Military_Branch': 0.0, 'Military_Discharge': 0.0, 'Military_Discharge_Date': 0.0, 'ScareMarkTatoo': 0.0, 'Photo': 0.0, 'Vehicle_year': 0.0, 'Vehicle_color': 0.0, 'Vehicle_mark': 0.0, 'Vehicle_model': 0.0, 'Alias_full_Name': 0.0, 'Alias_First_Name': 0.0, 'Alias_Middle_Name': 0.0, 'Alias_Last_Name': 0.0, 'Alias_Suffix': 0.0, 'Judge': 99.34, 'Attorney': 0.0, 'Party_code': 0.0, 'Party_Relationship': 0.0, 'Party_full_name': 0.0, 'Party_Last_name': 0.0, 'Party_First_name': 0.0, 'Party_Middle_name': 0.0, 'Party_Suffix_name': 0.0, 'Party_DOB': 0.0, 'Party_Age': 0.0, 'Party_Sex': 0.0, 'Party_Address': 0.0, 'Party_City': 0.0, 'Party_State': 0.0, 'Party_Zip': 0.0, 'Party_Phone': 0.0, 'Full_address': 100.0, 'Address_1': 99.55, 'Address_2': 5.47, 'Address_3': 0.0, 'City': 99.41, 'County': 0.0, 'State': 99.4, 'Zip': 98.85, 'Phone': 0.0, 'Last_Institution_Code': 0.0, 'Last_Institution': 0.0, 'Last_Institution_Address': 0.0, 'Last_Institution_City': 0.0, 'Last_Institution_State': 0.0, 'Last_Institution_Zip': 0.0, 'Institution_Code': 0.0, 'Institution_Name': 0.0, 'Institution_Address': 0.0, 'Institution_City': 0.0, 'Institution_State': 0.0, 'Institution_Zip': 0.0, 'Institution_Phone': 0.0, 'Institution_Details': 0.0, 'Released_to_Name': 0.0, 'Released_to_Address': 0.0, 'Released_to_City': 0.0, 'Released_to_State': 0.0, 'Released_to_Zip': 0.0, 'Release_Information': 0.0, 'Court_Case_Number': 100.0, 'Court_Code': 0.0, 'Court_Name': 0.0, 'Court_County': 0.0, 'Court_type': 0.0, 'Case_Category_Code': 0.0, 'Case_Category': 0.0, 'Case_Year': 0.0, 'Case_Type_code': 0.0, 'Case_Type': 0.0, 'Arresting_Agency': 99.28, 'Filing_Agency': 0.0, 'Date_Filed': 99.29, 'Arrest_Date': 97.27, 'Arraignment_Date': 0.0, 'Court_Clerk_Phone': 0.0, 'Fine': 0.0, 'Court_cost': 0.0, 'Case_disposition_code': 0.0, 'Case_disposition': 0.0, 'Rebuttal': 0.0, 'Bail_amount': 0.0, 'Bond': 0.0, 'Offender_Status_Code': 0.0, 'Offender_Status': 0.0, 'Case_status': 100.0, 'NCIC_Code': 0.0, 'Offense_Code': 97.78, 'Offense_Prefix': 0.0, 'Offense': 99.34, 'Offense_Type': 99.27, 'Offense_Class': 0.0, 'Offense_Degree': 99.27, 'Offense_Level': 0.0, 'Amended_Offense': 0.0, 'Amended_Offense_Type': 0.0, 'Amended_Offense_class': 0.0, 'Amended_Offense_Degree': 0.0, 'Amended_Offense_Level': 0.0, 'Sub_Offense_Code': 0.0, 'Sub_Offense_Prefix': 0.0, 'Sub_Offense': 0.0, 'Sub_Offense_Type': 0.0, 'Sub_Offense_Class': 0.0, 'Charge_No': 99.34, 'Total_Charge_counts': 0.0, 'Offense_Date': 99.33, 'Citation_Number': 4.28, 'Offense_City': 0.0, 'Offense_County': 0.0, 'Offense_State': 0.0, 'Original_Plea_code': 0.0, 'Original_Plea': 0.0, 'Original_Plea_Date': 0.0, 'Plea_Withdrawn_Date': 0.0, 'New_Plea_code': 0.0, 'New_Plea': 0.0, 'Verdict_Finding_code': 0.0, 'Verdict_Finding': 0.0, 'Verdict_Conviction_Date': 0.0, 'County_Convicted': 0.0, 'State_Convicted': 0.0, 'Offense_Disposition_code': 94.15, 'Offense_Disposition': 94.14, 'Offense_Disposition_Date': 94.15, 'Warrant_Date': 0.0, 'Sentence_Type': 33.95, 'Sentence_Max_Years': 0.0, 'Sentence_Max_Months': 0.0, 'Sentence_Max_Days': 0.0, 'Sentence_Min_Years': 0.0, 'Sentence_Min_Months': 0.0, 'Sentence_Min_Days': 0.0, 'Sentence_length': 33.98, 'Sentence_Date': 0.0, 'Sentence_Begin_Date': 0.0, 'Sentence_Details': 26.35, 'Parole_Start_Date': 0.0, 'Parole_End_Date': 0.0, 'Parole_Max_Years': 0.0, 'Parole_Max_Months': 0.0, 'Parole_Max_Days': 0.0, 'Parole_Min_Years': 0.0, 'Parole_Min_Months': 0.0, 'Parole_Min_Days': 0.0, 'Probation_Start_Date': 0.0, 'Probation_End_Date': 0.0, 'Probation_Max_Years': 0.0, 'Probation_Max_Months': 0.0, 'Probation_Max_Days': 0.0, 'Probation_Min_Years': 0.0, 'Probation_Min_Months': 0.0, 'Probation_Min_Days': 0.0, 'Probation_Agency': 0.0, 'Probation_length': 13.61, 'Maximum_Release_Date': 0.0, 'Scheduled_Release_Date': 0.0, 'Actual_Release_Date': 0.0, 'Release_Type': 0.0, 'Time_Served_Years': 0.0, 'Time_Served_Months': 0.0, 'Time_Served_Days': 0.0, 'Time_Served_length': 0.0, 'Probation_Follows_Y_N': 0.0, 'Supervision_County': 0.0, 'Sentence_Status': 0.0, 'Probation_Parole_Status': 0.0, 'Sentence_Comments1': 13.61, 'Sentence_Comments2': 0.0, 'Suspended_Jail_Years': 0.0, 'Suspended_Jail_Months': 0.0, 'Suspended_Jail_Days': 0.0, 'Admission_Date': 0.0, 'Admission_Type': 0.0, 'EPR_File_Record_Date': 100.0, 'address_verification_date': 0.0}
[2024-12-16, 07:33:36 UTC] {TableColumnCompletenessToMeetThreshold.py:219} INFO - Row count:596759
[2024-12-16, 07:33:46 UTC] {local_task_job_runner.py:310} WARNING - State of this instance has been externally set to failed. Terminating instance.
[2024-12-16, 07:33:46 UTC] {local_task_job_runner.py:222} ▲▲▲ Log group end
[2024-12-16, 07:33:46 UTC] {process_utils.py:132} INFO - Sending Signals.SIGTERM to group 99982. PIDs of all processes in the group: [99982]
[2024-12-16, 07:33:46 UTC] {process_utils.py:87} INFO - Sending the signal Signals.SIGTERM to group 99982
[2024-12-16, 07:33:46 UTC] {taskinstance.py:2611} ERROR - Received SIGTERM. Terminating subprocesses.
[2024-12-16, 07:33:46 UTC] {common.py:304} INFO - Sending failed status from callback...
[2024-12-16, 07:33:46 UTC] {server_mixin.py:74} INFO - OpenMetadata client running with Server version [1.5.12] and Client version [1.5.12.0]
[2024-12-16, 07:33:46 UTC] {common.py:310} INFO - Sending status to Ingestion Pipeline tessera-sql4.online_view.dbo.FL_Polk_Circuit_View.testSuite.tessera-sql4_online_view_dbo_FL_Polk_Circuit_View_Pipeline
[2024-12-16, 07:33:47 UTC] {process_utils.py:80} INFO - Process psutil.Process(pid=99982, status='terminated', exitcode=0, started='07:33:30') (99982) terminated with exit code 0
```



### What you think should happen instead?

DAG run is successful

### How to reproduce

Create a couple of thousands DAGs. It's not a performance issue, CPU and workers are okay.

### Operating System

linux (ec2, docker)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",serhiikovalenkoextcheckrcom,2024-12-18 18:18:02+00:00,[],2025-01-03 15:33:40+00:00,2025-01-03 15:33:40+00:00,https://github.com/apache/airflow/issues/45052,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2551985217, 'issue_id': 2748463299, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 18, 18, 18, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569409115, 'issue_id': 2748463299, 'author': 'nathadfield', 'body': ""@serhiikovalenkoextcheckrcom Unfortunately I think it's going to be almost impossible for anyone to give you any specific answers to the problem you are facing but the possible causes of a SIGTERM in this context could be one of the following.\r\n\r\n**Scheduler Timeout**\r\n* The Airflow scheduler may have enforced a task timeout (e.g., due to exceeding the execution_timeout parameter) and sent SIGTERM to stop the task.\r\n\r\n**Manual Termination**\r\n* A user or system admin may have manually terminated the task using airflow tasks kill or similar commands.\r\n\r\n**Orphaned Process**\r\n* The PID mismatch suggests that the task may have been restarted or orphaned, causing Airflow to send SIGTERM to clean up inconsistent state.\r\n\r\n**Resource Constraints**\r\n* The system or container running Airflow might have terminated the process due to resource exhaustion (e.g., memory, CPU limits).\r\n\t\r\n**Upstream Dependency or DAG Conflict**\r\n* If the task was part of a larger workflow, another task or operator failure could trigger an early termination."", 'created_at': datetime.datetime(2025, 1, 3, 15, 33, 21, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-18 18:18:05 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

nathadfield on (2025-01-03 15:33:21 UTC): @serhiikovalenkoextcheckrcom Unfortunately I think it's going to be almost impossible for anyone to give you any specific answers to the problem you are facing but the possible causes of a SIGTERM in this context could be one of the following.

**Scheduler Timeout**
* The Airflow scheduler may have enforced a task timeout (e.g., due to exceeding the execution_timeout parameter) and sent SIGTERM to stop the task.

**Manual Termination**
* A user or system admin may have manually terminated the task using airflow tasks kill or similar commands.

**Orphaned Process**
* The PID mismatch suggests that the task may have been restarted or orphaned, causing Airflow to send SIGTERM to clean up inconsistent state.

**Resource Constraints**
* The system or container running Airflow might have terminated the process due to resource exhaustion (e.g., memory, CPU limits).
	
**Upstream Dependency or DAG Conflict**
* If the task was part of a larger workflow, another task or operator failure could trigger an early termination.

"
2748445007,issue,closed,completed,Endless sensor rescheduling if the first sensor run failed to be saved in DB,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.3

### What happened?

Endless sensor rescheduling happening in `reschedule` mode if the first sensor run failed to be saved in DB.  

1. The sensor was configured in reschedule mode:

~~~
@task.sensor(
    task_id=""sensor-s3-version"",
    poke_interval=5 * 60,
    timeout=50 * 60,
    mode=""reschedule"",
    soft_fail=True,
)
def sensor_s3_version(connection_id: str, artefact: str) -> PokeReturnValue:
    ...

~~~

2. First sensor run failed because of Postgre session terminated and as a result no entry was added to the `task_reschedule` table:

~~~
...
[2024-12-18, 11:41:11 UTC] {taskinstance.py:340} ▼ Post task execution logs
[2024-12-18, 11:41:11 UTC] {standard_task_runner.py:124} ERROR - Failed to execute job 71468 for task sensor-s3-version ((psycopg2.OperationalError) server closed the connection unexpectedly
	This probably means the server terminated abnormally
	before or while processing the request.
...
~~~

3. Because of no record in the `task_reschedule` table for the `try_number == 1` condition the code [here](https://github.com/apache/airflow/blob/153200fa05229546cb91cc341a6088a7d74f88ac/airflow/sensors/base.py#L96) returns no data:

~~~
    ...
    return session.scalar(
        select(TaskReschedule)
        .where(
            TaskReschedule.dag_id == dag_id,
            TaskReschedule.task_id == task_id,
            TaskReschedule.run_id == run_id,
            TaskReschedule.map_index == map_index,
            TaskReschedule.try_number == try_number,
        )
        .order_by(TaskReschedule.id.asc())
        .with_only_columns(TaskReschedule.start_date)
        .limit(1)
    )

~~~

4. If no data received, the code [here](https://github.com/apache/airflow/blob/153200fa05229546cb91cc341a6088a7d74f88ac/airflow/sensors/base.py#L259) assigns the start_date to current system date and the cycle of rescheduling  never ends:

~~~
            ...
            if not start_date:
                start_date = timezone.utcnow()

~~~

5. As a result I have a task which lasts for 6 hours (with maximum 1 hour set) since the moment I've started debugging:

![image](https://github.com/user-attachments/assets/81437618-e65f-4414-b034-f718b9248086)


### What you think should happen instead?

In that case we should try to find the first available `task_reschedule` record after the initially needed try number, it can be done easily by modifying condition from:

~~~
TaskReschedule.try_number == first_try_number
~~~

to:

~~~
TaskReschedule.try_number >= first_try_number
~~~

Thanks to sorting `order_by(TaskReschedule.id.asc())` and limt `limit(1)` statements we would select the first record for a try next to initially needed.  

If there are no records at all, we would get `timezone.utcnow()` by already implemented logic

### How to reproduce

Fail the first try of a sensor and delete the records in the `task_reschedule` table for the first try group

### Operating System

Airflow official docker apache/airflow:2.9.3-python3.8: Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",morooshka,2024-12-18 18:06:45+00:00,[],2024-12-27 18:17:23+00:00,2024-12-27 18:17:23+00:00,https://github.com/apache/airflow/issues/45050,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2556874926, 'issue_id': 2748445007, 'author': 'jscheffl', 'body': 'I think your proposal is meaningful - whereas I am wondering what problems with Postgres are leading to this in-consistency. I think this should be addressed first. (1) You should check how to make your DB stable. If there are connection losses in other places I think a lot of more errors can happen. Airflow expects a stable DB. (2) I am wondering why in this cause you report an inconsistent result is left in the DB - I would have expected that all steps are executed in one transaction. Means either not any updates or a consistent update.', 'created_at': datetime.datetime(2024, 12, 20, 12, 6, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556916297, 'issue_id': 2748445007, 'author': 'morooshka', 'body': '@jscheffl Thank you for looking into it.\r\n1. I have stabilised the DB already, the reason was the lack of resources quoted to the Postgre pods. However the problem is still important because session can be closed on many reasons, it is the DB backend logics and we should not go into infinite loop in any case\r\n2. I do not see where I have said ""inconsistence"" but anyway - the counter of reschedules incremented, but the reschedule was not written in DB - here I see a doubtful (discussable) situation\r\n\r\nWhat would you suggest - can I implement the proposed logics as PR?\r\nThank you for your answer!', 'created_at': datetime.datetime(2024, 12, 20, 12, 30, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556922102, 'issue_id': 2748445007, 'author': 'potiuk', 'body': '> What would you suggest - can I implement the proposed logics as PR?\r\n\r\nAlways. There is absolutely nothing wrong with opening a PR, we might close and reject it after we look at the code - which is the best way to see if your proposal is good, far easier and faster for maintainers to understand the scope of the issue and solution', 'created_at': datetime.datetime(2024, 12, 20, 12, 34, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563131839, 'issue_id': 2748445007, 'author': 'morooshka', 'body': 'Hi @jscheffl ! Can you please have a look on my [PR](https://github.com/apache/airflow/pull/45224)?', 'created_at': datetime.datetime(2024, 12, 26, 22, 12, 13, tzinfo=datetime.timezone.utc)}]","jscheffl on (2024-12-20 12:06:48 UTC): I think your proposal is meaningful - whereas I am wondering what problems with Postgres are leading to this in-consistency. I think this should be addressed first. (1) You should check how to make your DB stable. If there are connection losses in other places I think a lot of more errors can happen. Airflow expects a stable DB. (2) I am wondering why in this cause you report an inconsistent result is left in the DB - I would have expected that all steps are executed in one transaction. Means either not any updates or a consistent update.

morooshka (Issue Creator) on (2024-12-20 12:30:47 UTC): @jscheffl Thank you for looking into it.
1. I have stabilised the DB already, the reason was the lack of resources quoted to the Postgre pods. However the problem is still important because session can be closed on many reasons, it is the DB backend logics and we should not go into infinite loop in any case
2. I do not see where I have said ""inconsistence"" but anyway - the counter of reschedules incremented, but the reschedule was not written in DB - here I see a doubtful (discussable) situation

What would you suggest - can I implement the proposed logics as PR?
Thank you for your answer!

potiuk on (2024-12-20 12:34:23 UTC): Always. There is absolutely nothing wrong with opening a PR, we might close and reject it after we look at the code - which is the best way to see if your proposal is good, far easier and faster for maintainers to understand the scope of the issue and solution

morooshka (Issue Creator) on (2024-12-26 22:12:13 UTC): Hi @jscheffl ! Can you please have a look on my [PR](https://github.com/apache/airflow/pull/45224)?

"
2748440213,issue,open,,Add support for extra JDBC parameters for Hive Client Wrapper in apache-hive provider,"### Description

The Apache Hive provider's Hive Client Wrapper connection is missing the possibility to pass extra JDBC parameters to build the JDBC connection string.

I would like to use it to connect to Hive in Cloudera CDP but unfortunately it is not possible in our environment since we cannot pass JDBC parameters like: transportMode, sslTrustStore, trustStorePassword.

Cloudera docs on JDBC connection string syntax can be found [here](https://docs.cloudera.com/cdp-private-cloud-base/7.1.9/integrating-hive-and-bi/topics/hive_connection_string_url_syntax.html)

### Use case/motivation

I want to be able to pass extra JDBC parameters to Hive ClientWrapper connection

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",tomwit-nx,2024-12-18 18:03:58+00:00,[],2024-12-23 09:33:06+00:00,,https://github.com/apache/airflow/issues/45049,"[('kind:feature', 'Feature Requests'), ('provider:apache-hive', '')]","[{'comment_id': 2551960428, 'issue_id': 2748440213, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 18, 18, 4, 2, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-18 18:04:02 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2748396880,issue,closed,completed,KubernetesPodOperator: xcom sidecar can't terminate when share process namespace is set,"### Apache Airflow Provider(s)

cncf-kubernetes

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes==8.3.4

### Apache Airflow version

v2.9.1

### Operating System

Debian GNU/Linux 11 (bullseye)

### Deployment

Google Cloud Composer

### Deployment details

_No response_

### What happened

xcom sidecar container fail to terminate because it tries to kill pid 1:

https://github.com/apache/airflow/blob/09d8a803ca3f76d8598e7eeeb86397cf1cd4076d/providers/src/airflow/providers/cncf/kubernetes/utils/pod_manager.py#L858

But fail: 
```
[2024-12-18, 16:26:28 UTC] {pod_manager.py:725} INFO - Checking if xcom sidecar container is started.
[2024-12-18, 16:26:28 UTC] {pod_manager.py:728} INFO - The xcom sidecar container is started.
[2024-12-18, 16:26:29 UTC] {pod_manager.py:805} INFO - Running command... if [ -s /***/xcom/return.json ]; then cat /***/xcom/return.json; else echo __***_xcom_result_empty__; fi
[2024-12-18, 16:26:30 UTC] {pod_manager.py:805} INFO - Running command... kill -2 1
[2024-12-18, 16:26:30 UTC] {pod_manager.py:815} INFO - stderr from command: sh: can't kill pid 1: Permission denied
[2024-12-18, 16:26:30 UTC] {pod.py:586} INFO - xcom result: 
{""hello"": ""world""}
```

This happen because when we use share process namespace process will not run with PID 1.

### What you think should happen instead

The xcom sidecar container should terminate gracefully, maybe we should switch the command to `kill -9 $(pgrep -f trap)` by default? 

### How to reproduce

Create a task using `GKEStartPodOperator` and set `full_pod_spec` on `V1PodSpec` set `share_process_namespace` to `True` 

```python
from kubernetes.client import models as k8s

GKEStartPodOperator(
        task_id=""gke-task"",
        do_xcom_push=True,
        full_pod_spec=k8s.V1Pod(
            spec=k8s.V1PodSpec(
                share_process_namespace=True,
                containers=[
                    k8s.V1Container(
                        name=""container1"",
                        image=""alpine"",
                        command=[""sh"", ""-c""],
                        args=[
                            """"""mkdir -p /airflow/xcom && echo '{""Hello"": ""World""}' > /airflow/xcom/return.json && echo 'Done'"""""",
                        ],
                    ),
                    k8s.V1Container(
                        name=""container2"",
                        image=""alpine"",
                        command=[""sh"", ""-c""],
                        args=[""sleep 15""],
                    ),
                ],
            ),
        ),
    )
```

### Anything else

Every time.

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Laerte,2024-12-18 17:38:04+00:00,[],2024-12-18 21:46:21+00:00,2024-12-18 21:46:21+00:00,https://github.com/apache/airflow/issues/45047,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2551914199, 'issue_id': 2748396880, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 18, 17, 38, 8, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-18 17:38:08 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2748363648,issue,closed,completed,REST API calls fail when run Airflow locally with docker,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.0.1.973

### What happened?

Web UI is working as expected on http://localhost:8080/home, but any REST API call, e.g.
curl --location 'http://localhost:8080/api/v1/dags' 
fails. I disabled authentication with auth_backends=airflow.api.auth.backend.default in
env-airflow-in-nsk.cfg

<!DOCTYPE html>
<html lang=""en"">
  <head>
    <link rel=""stylesheet"" href=""https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css"">
  </head>
  <body>
    <div class=""container"">
      <h1> Ooops! </h1>
      <div>
          <pre>
Something bad has happened.
Please consider letting us know by creating a <b><a href=""https://github.com/apache/airflow/issues/new/choose"">bug report using GitHub</a></b>.

Python version: 3.8.18
Airflow version: 2.0.1.973
Node: airflow-webserver
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File &#34;/usr/local/lib/python3.8/dist-packages/flask/app.py&#34;, line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File &#34;/usr/local/lib/python3.8/dist-packages/flask/app.py&#34;, line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File &#34;/usr/local/lib/python3.8/dist-packages/flask/app.py&#34;, line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File &#34;/usr/local/lib/python3.8/dist-packages/flask/_compat.py&#34;, line 39, in reraise
    raise value
  File &#34;/usr/local/lib/python3.8/dist-packages/flask/app.py&#34;, line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File &#34;/usr/local/lib/python3.8/dist-packages/flask/app.py&#34;, line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File &#34;/usr/local/lib/python3.8/dist-packages/connexion/decorators/decorator.py&#34;, line 68, in wrapper
    response = function(request)
  File &#34;/usr/local/lib/python3.8/dist-packages/connexion/decorators/uri_parsing.py&#34;, line 149, in wrapper
    response = function(request)
  File &#34;/usr/local/lib/python3.8/dist-packages/connexion/decorators/validation.py&#34;, line 399, in wrapper
    return function(request)
  File &#34;/usr/local/lib/python3.8/dist-packages/connexion/decorators/response.py&#34;, line 112, in wrapper
    response = function(request)
  File &#34;/usr/local/lib/python3.8/dist-packages/connexion/decorators/parameter.py&#34;, line 120, in wrapper
    return function(**kwargs)
  File &#34;/usr/local/lib/python3.8/dist-packages/airflow/api_connexion/security.py&#34;, line 45, in decorated
    check_authentication()
  File &#34;/usr/local/lib/python3.8/dist-packages/airflow/api_connexion/security.py&#34;, line 30, in check_authentication
    response = current_app.api_auth.requires_authentication(Response)()
  File &#34;/usr/local/lib/python3.8/dist-packages/nordstrom/api/auth/backend/custom_auth_api.py&#34;, line 47, in decorated
    f&#34;&#34;&#34;Request received with path: {request.path} &#34;&#34;&#34;
  File &#34;/usr/local/lib/python3.8/dist-packages/werkzeug/datastructures.py&#34;, line 1463, in __getitem__
    return _unicodify_header_value(self.environ[&#34;HTTP_&#34; + key])
KeyError: &#39;HTTP_X_AUTHPROXY_LANID&#39;
</pre>
      </div>
    </div>
  </body>
</html>


### What you think should happen instead?

_No response_

### How to reproduce

I am running Airflow locally using my company artifactory.
Please, let me know if error is not reproducible outside our settings.
 

### Operating System

Mac OS 14.7.1 (23H222)

### Versions of Apache Airflow Providers

N/A

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vdkrav93,2024-12-18 17:18:55+00:00,[],2024-12-18 17:40:10+00:00,2024-12-18 17:40:10+00:00,https://github.com/apache/airflow/issues/45046,"[('kind:bug', 'This is a clearly a bug'), ('area:API', ""Airflow's REST/HTTP API""), ('area:auth', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2551878219, 'issue_id': 2748363648, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 18, 17, 18, 59, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-18 17:18:59 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2748134848,issue,closed,completed,Support of custom celery configs,"### Description

Currently airflow supports few celery configs only like worker concurrency, worker prefetch multiplier etc. All others config are not supported. One of main out of them is worker max tasks per child which helps in recycling the celery worker processes. It helps in reclaiming the resources of the processes which got consumed due to high resource utilising task.

### Use case/motivation

We have a task which consumes very high memory. As task keeps on changing the processes so our memory keeps on increasing. To recycle the process after certain number of task is the celery support but currently not supported in airflow.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",arorasachin9,2024-12-18 15:34:23+00:00,[],2024-12-23 13:53:04+00:00,2024-12-23 13:53:04+00:00,https://github.com/apache/airflow/issues/45037,"[('area:providers', ''), ('kind:feature', 'Feature Requests'), ('provider:celery', '')]","[{'comment_id': 2551633479, 'issue_id': 2748134848, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 18, 15, 34, 26, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-18 15:34:26 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2748124220,issue,closed,completed,AWS Glue Job Operator Not Able to Read Jinja Format,"### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

2.10.1

### Apache Airflow version

9.1.0

### Operating System

N/A

### Deployment

Amazon (AWS) MWAA

### Deployment details

I tried to configure the Number Of Workers and worker type within the run_job_kwargs parameter with jinja format like the following 
run_job_kwargs={""NumberOfWorkers"": '{{ ti.xcom_pull(task_ids=""dummy"",key=""dummy"") }}', ""WorkerType"":""dummy_type""}  

### What happened

the value with the jinja format was not recognized during run time and it shows the error that the Number Of Workers should have input type int rather than a string '{{ ti.xcom_pull(task_ids=""dummy"",key=""dummy"") }}'

### What you think should happen instead

'{{ ti.xcom_pull(task_ids=""dummy"",key=""dummy"") }}' should pick up the real value during run time

### How to reproduce

this could be reproduced by creating a dummy function to push some value to the xcom first then using a glue job operator with specifying 
run_job_kwargs={""NumberOfWorkers"": '{{ ti.xcom_pull(task_ids=""dummy"",key=""dummy"") }}', ""WorkerType"":""dummy_type""} 

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",nicholascz666666,2024-12-18 15:29:46+00:00,"['nicholascz666666', 'basvi-chunara']",2025-01-23 16:36:52+00:00,2025-01-23 16:36:52+00:00,https://github.com/apache/airflow/issues/45036,"[('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('kind:feature', 'Feature Requests'), ('good first issue', '')]","[{'comment_id': 2551622322, 'issue_id': 2748124220, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 18, 15, 29, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2555894824, 'issue_id': 2748124220, 'author': 'jayceslesar', 'body': 'Does this persist even if you have `render_template_as_native_obj=True` to the DAG constructor?', 'created_at': datetime.datetime(2024, 12, 19, 22, 38, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558627498, 'issue_id': 2748124220, 'author': 'gopidesupavan', 'body': 'Yep, this field is not templated. You may raise changes for this, think this is similar to https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/amazon/aws/operators/glue.py#L89 `create_job_kwargs`', 'created_at': datetime.datetime(2024, 12, 22, 22, 30, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558920983, 'issue_id': 2748124220, 'author': 'basvi-chunara', 'body': 'I would like to work on this. Can this issue be assigned to me?', 'created_at': datetime.datetime(2024, 12, 23, 5, 10, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2559055779, 'issue_id': 2748124220, 'author': 'potiuk', 'body': 'Coordinate with @nicholascz666666  @basvi-chunara becasue he was assigned before - but I assigned both of you now.', 'created_at': datetime.datetime(2024, 12, 23, 7, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561213096, 'issue_id': 2748124220, 'author': 'basvi-chunara', 'body': 'okay, thank you for letting me know', 'created_at': datetime.datetime(2024, 12, 24, 15, 3, 8, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-18 15:29:49 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jayceslesar on (2024-12-19 22:38:23 UTC): Does this persist even if you have `render_template_as_native_obj=True` to the DAG constructor?

gopidesupavan on (2024-12-22 22:30:43 UTC): Yep, this field is not templated. You may raise changes for this, think this is similar to https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/amazon/aws/operators/glue.py#L89 `create_job_kwargs`

basvi-chunara (Assginee) on (2024-12-23 05:10:38 UTC): I would like to work on this. Can this issue be assigned to me?

potiuk on (2024-12-23 07:19:00 UTC): Coordinate with @nicholascz666666  @basvi-chunara becasue he was assigned before - but I assigned both of you now.

basvi-chunara (Assginee) on (2024-12-24 15:03:08 UTC): okay, thank you for letting me know

"
2747879684,issue,closed,completed,"Param does not work with type=['array', 'null'], items={'type': 'number'} and None default","### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.8.2

### What happened?

I am trying to pass a parameter to a DAG using `Param`. I need to pass an optional list of integers, hence, I tried using `type=['array', 'null'], items={'type': 'number'}` with None as default value. However, when I try to trigger the DAG and add a non-None array of numbers, the JSON config does not get updated and `null` is passed.

### What you think should happen instead?

The non-empty array of integers should get passed to the DAG.

### How to reproduce

Create a DAG with the mentioned Param, e.g.
```python
from airflow import DAG
from airflow.models.param import Param

default_args = {
    'owner': 'someone@email.com',
    'start_date': datetime(2023, 12, 13),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'example_dag',
    default_args=default_args,
    schedule='0 1 * * *',
    tags=['news-dags'],
    params={
        'ids': Param(None, type=['array', 'null'], items={'type': 'number'}),
    },
) as example_dag:
    ...
```
Now try to trigger it with config and add a list of integer IDs. I noted, that it works when I specify `type='array'` but the default should be `None`. The behaviour of my DAG differs when `ids` are `None` or an empty list, so passing `[]` as a default and removing `null` is not an option.

### Operating System

Debian bookworm

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.18.0
apache-airflow-providers-celery==3.6.0
apache-airflow-providers-common-io==1.3.0
apache-airflow-providers-common-sql==1.11.0
apache-airflow-providers-docker==3.9.1
apache-airflow-providers-ftp==3.7.0
apache-airflow-providers-http==4.9.1
apache-airflow-providers-imap==3.5.0
apache-airflow-providers-postgres==5.10.1
apache-airflow-providers-redis==3.6.0
apache-airflow-providers-sqlite==3.7.1

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### Anything else?

Always when trying to use Param with the mentioned combination.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",daniel-westerfeld,2024-12-18 13:52:39+00:00,[],2025-01-03 14:50:39+00:00,2024-12-31 19:56:10+00:00,https://github.com/apache/airflow/issues/45032,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2566511806, 'issue_id': 2747879684, 'author': 'jscheffl', 'body': 'Yes, you are right, I can re-produce this problem also with Airflow 2.10.4. It seems the ""onBlur()"" event in the UI does not trigger correctly if you leave the field. It _should_ update the JSON entry box on leaving the field focus with the newly added data.\r\nInterestingly the other CodeMirror views do not have this problem.', 'created_at': datetime.datetime(2024, 12, 31, 14, 53, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2566563023, 'issue_id': 2747879684, 'author': 'jscheffl', 'body': '@daniel-westerfeld fix is in PR #45313 - would be great if you can also test on your side. As you are on an older Airflow version, would either need to patch manually or take this as a step to upgrade.\r\nNote that the JavaScript is compiled at time of packaging so unfortunately it is not just a file replacement.', 'created_at': datetime.datetime(2024, 12, 31, 16, 9, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2569347584, 'issue_id': 2747879684, 'author': 'daniel-westerfeld', 'body': '@jscheffl I cannot update Airflow that easily but once I have, I will let you know! Thanks for solving this so quickly!', 'created_at': datetime.datetime(2025, 1, 3, 14, 50, 37, tzinfo=datetime.timezone.utc)}]","jscheffl on (2024-12-31 14:53:10 UTC): Yes, you are right, I can re-produce this problem also with Airflow 2.10.4. It seems the ""onBlur()"" event in the UI does not trigger correctly if you leave the field. It _should_ update the JSON entry box on leaving the field focus with the newly added data.
Interestingly the other CodeMirror views do not have this problem.

jscheffl on (2024-12-31 16:09:03 UTC): @daniel-westerfeld fix is in PR #45313 - would be great if you can also test on your side. As you are on an older Airflow version, would either need to patch manually or take this as a step to upgrade.
Note that the JavaScript is compiled at time of packaging so unfortunately it is not just a file replacement.

daniel-westerfeld (Issue Creator) on (2025-01-03 14:50:37 UTC): @jscheffl I cannot update Airflow that easily but once I have, I will let you know! Thanks for solving this so quickly!

"
2745257121,issue,open,,Replace pre-commit with prefligit,"As @ashb noticed there is a new drop-in replacement for pre-commit in town written in rust - with presumably less opinionated choices (and nicer features) than pre-commit. It's not ""ready yet"" - it does not have everything implemented yet, but looks promising.

https://github.com/j178/prefligit

We might want to take a look at figure out what is missing and replace `pre-commit` with it when we think it's ready
",potiuk,2024-12-17 15:37:26+00:00,[],2024-12-17 19:33:57+00:00,,https://github.com/apache/airflow/issues/44995,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code')]","[{'comment_id': 2548801695, 'issue_id': 2745257121, 'author': 'potiuk', 'body': ""Current status - it fails wit:\n\n```\n[jarek:~/code/airflow] main+ 101 ± RUST_BACKTRACE=1 prefligit run   \n⠙ Initializing hooks...                                                                                                                                                                                                                                                                                                                                                                       thread 'main' panicked at src/languages/mod.rs:47:18:\nnot yet implemented\nstack backtrace:\n   0: rust_begin_unwind\n   1: core::panicking::panic_fmt\n   2: core::panicking::panic\n   3: prefligit::hook::HookBuilder::combine\n   4: prefligit::hook::Project::init_hooks::{{closure}}\n   5: prefligit::cli::run::run::run::{{closure}}\n   6: prefligit::run::{{closure}}\n   7: tokio::runtime::scheduler::current_thread::CurrentThread::block_on\n   8: tokio::runtime::runtime::Runtime::block_on\n   9: prefligit::main\nnote: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.\n[jarek:~/code/airflow] main+ 101 ± \n```"", 'created_at': datetime.datetime(2024, 12, 17, 15, 42, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2548832348, 'issue_id': 2745257121, 'author': 'ashb', 'body': 'Shell completion!\r\n\r\nhttps://github.com/j178/prefligit/blob/921f86411cc2de04dbf94c696d572665bfca467f/src/main.rs#L235', 'created_at': datetime.datetime(2024, 12, 17, 15, 51, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2549437680, 'issue_id': 2745257121, 'author': 'potiuk', 'body': ""> Shell completion!\r\n> \r\n> https://github.com/j178/prefligit/blob/921f86411cc2de04dbf94c696d572665bfca467f/src/main.rs#L235\r\n\r\n:rocket: \r\n\r\nThat's what I was sincerely hoping for. This is main reason why we have `breeze static-checks`, another one are some predefined flags like `--last-comit` and `--only-my-changes` which I hope we might be able to contribute to prefligit once we get it working :)"", 'created_at': datetime.datetime(2024, 12, 17, 19, 33, 29, tzinfo=datetime.timezone.utc)}]","potiuk (Issue Creator) on (2024-12-17 15:42:52 UTC): Current status - it fails wit:

```
[jarek:~/code/airflow] main+ 101 ± RUST_BACKTRACE=1 prefligit run   
⠙ Initializing hooks...                                                                                                                                                                                                                                                                                                                                                                       thread 'main' panicked at src/languages/mod.rs:47:18:
not yet implemented
stack backtrace:
   0: rust_begin_unwind
   1: core::panicking::panic_fmt
   2: core::panicking::panic
   3: prefligit::hook::HookBuilder::combine
   4: prefligit::hook::Project::init_hooks::{{closure}}
   5: prefligit::cli::run::run::run::{{closure}}
   6: prefligit::run::{{closure}}
   7: tokio::runtime::scheduler::current_thread::CurrentThread::block_on
   8: tokio::runtime::runtime::Runtime::block_on
   9: prefligit::main
note: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.
[jarek:~/code/airflow] main+ 101 ± 
```

ashb on (2024-12-17 15:51:58 UTC): Shell completion!

https://github.com/j178/prefligit/blob/921f86411cc2de04dbf94c696d572665bfca467f/src/main.rs#L235

potiuk (Issue Creator) on (2024-12-17 19:33:29 UTC): :rocket: 

That's what I was sincerely hoping for. This is main reason why we have `breeze static-checks`, another one are some predefined flags like `--last-comit` and `--only-my-changes` which I hope we might be able to contribute to prefligit once we get it working :)

"
2745217432,issue,open,,KubernetesJobOperator fails if you launch more than one pod,"### Apache Airflow Provider(s)

cncf-kubernetes

### Versions of Apache Airflow Providers

`8.4.1`

### Apache Airflow version

2.10.2

### Operating System

Not sure - in GCP cloud composer

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### What happened

If you launch a kubernetes job operator, it tries to find the pod after execution. If the job has launched multiple pods, it then fails when trying to log since it can't find more than one pod. 

I can pinpoint the place in the source code if you give me a link, I just can't find where it is on github but have identified it locally.

It's when `raise AirflowException(f""More than one pod running with labels {label_selector}"")` gets called.

### What you think should happen instead

Should be able to have some flag in the job operator constructor that prevents this behaviour from happening - many k8s jobs will launch more than one pods; or the `find_pod` logic is smart enough to know you will have more than one pod when your job has a large parallellism count.

### How to reproduce

Launch a `KubernetesJobOperator` with parallelism > 1 and I hit it every time. This seems so basic though that I wonder if I am doing something wrong since I would have expected other people to run into it if that was the case.

I am running indexed jobs with completions equal to parallelism count. 

Full config looks like this:

```
my_task = KubernetesJobOperator(
    backoff_limit=18,
    wait_until_job_complete=True,
    completion_mode=""Indexed"",
    completions=PARALLELISM,
    parallelism=PARALLELISM,
    ttl_seconds_after_finished=60 * 30,
    reattach_on_restart=False,
    get_logs=False,
    labels={
        ""app.kubernetes.io/type"": ""<my pdb selector>"",
    },
    job_poll_interval=60,
    config_file=""/home/airflow/composer_kube_config"",
    task_id=""my_task"",
    namespace=""composer-user-workloads"",
    name=""my_task"",
    image=IMAGE_NAME,
    cmds=[""<my command>""],
    arguments=[""<my args""],
    container_resources=k8s_models.V1ResourceRequirements(
        requests={""cpu"": ""250m"", ""memory"": ""512Mi""},
        limits={""memory"": ""512Mi""},
    ),
    kubernetes_conn_id=""kubernetes_default"",
    retries=1,
)
```

### Anything else

I have fixed this issue by setting `reattach_on_restart` to `False`, which prevents the labels issue but has side effect of producing a stray pod. I then delete that with a python operator that makes use of the k8s hook:

```

  def delete_all_pods_from_job():
      k8s_hook = KubernetesHook(
          conn_id=""kubernetes_default"",
          config_file=""/home/airflow/composer_kube_config"",
      )
      pod_list: k8s_models.V1PodList = k8s_hook.core_v1_client.list_namespaced_pod(
          namespace=""composer-user-workloads"", label_selector=""mylabel=true""
      )
      print(f""found {len(pod_list.items)} pods to delete"")
      for pod in pod_list.items:
          print(f""Deleting pod with name {pod.metadata.name}"")
          k8s_hook.core_v1_client.delete_namespaced_pod(name=pod.metadata.name, namespace=pod.metadata.namespace)

  cleanup_pods = PythonOperator(
      task_id=""cleanup_pods"",
      python_callable=delete_all_pods_from_job,
  )
  ```

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",osintalex,2024-12-17 15:21:15+00:00,[],2024-12-17 15:33:23+00:00,,https://github.com/apache/airflow/issues/44994,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2548741262, 'issue_id': 2745217432, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 17, 15, 21, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2548770306, 'issue_id': 2745217432, 'author': 'osintalex', 'body': ""Ah found it in the source!\r\n\r\nhttps://github.com/apache/airflow/blob/ce4236f365c3e8e45262b04ef0ce596907043702/providers/src/airflow/providers/cncf/kubernetes/operators/job.py#L171 \r\n\r\ncalls into https://github.com/apache/airflow/blob/ce4236f365c3e8e45262b04ef0ce596907043702/providers/src/airflow/providers/cncf/kubernetes/operators/pod.py#L546\r\n\r\nwhich then calls in here https://github.com/apache/airflow/blob/ce4236f365c3e8e45262b04ef0ce596907043702/providers/src/airflow/providers/cncf/kubernetes/operators/pod.py#L973\r\n\r\nthis is going to match more than one pods always if you have a parallelism count > 1 from what I can tell\r\n\r\nwhich in turn triggers this error https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/cncf/kubernetes/operators/pod.py#L532\r\n\r\nUnless I'm missing something, I think this could be fixed by adding a condition here https://github.com/apache/airflow/blob/ce4236f365c3e8e45262b04ef0ce596907043702/providers/src/airflow/providers/cncf/kubernetes/operators/job.py#L170 where if parallelism is > 1 to skip this section and maybe log that this is happening because your job has more than one pod"", 'created_at': datetime.datetime(2024, 12, 17, 15, 32, 19, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-17 15:21:19 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

osintalex (Issue Creator) on (2024-12-17 15:32:19 UTC): Ah found it in the source!

https://github.com/apache/airflow/blob/ce4236f365c3e8e45262b04ef0ce596907043702/providers/src/airflow/providers/cncf/kubernetes/operators/job.py#L171 

calls into https://github.com/apache/airflow/blob/ce4236f365c3e8e45262b04ef0ce596907043702/providers/src/airflow/providers/cncf/kubernetes/operators/pod.py#L546

which then calls in here https://github.com/apache/airflow/blob/ce4236f365c3e8e45262b04ef0ce596907043702/providers/src/airflow/providers/cncf/kubernetes/operators/pod.py#L973

this is going to match more than one pods always if you have a parallelism count > 1 from what I can tell

which in turn triggers this error https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/cncf/kubernetes/operators/pod.py#L532

Unless I'm missing something, I think this could be fixed by adding a condition here https://github.com/apache/airflow/blob/ce4236f365c3e8e45262b04ef0ce596907043702/providers/src/airflow/providers/cncf/kubernetes/operators/job.py#L170 where if parallelism is > 1 to skip this section and maybe log that this is happening because your job has more than one pod

"
2745037770,issue,open,,deprecate and remove session access from TaskInstance state change listeners,"### Body

Currently, [TI state listeners](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/listeners.html#taskinstance-state-change-events) accept TI database model and session object.

In Airflow 3.0, the database access from worker is removed due to AIP-72. This necessitates removal of db access in TI state listener API - as it is also executed on worker. 

In Airflow 2.11, session argument should be deprecated, and removed (or always be None?) in 3.0. Additionally, in 3.0, it will be passed `RuntimeTaskInstance`.   

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",mobuchowski,2024-12-17 14:06:23+00:00,[],2024-12-17 14:56:24+00:00,,https://github.com/apache/airflow/issues/44990,"[('kind:meta', 'High-level information important to the community'), ('area:lineage', ''), ('area:Listeners', ''), ('airflow3.0:candidate', 'Potential candidates for Airflow 3.0'), ('airflow3.0:breaking', 'Candidates for Airflow 3.0 that contain breaking changes')]","[{'comment_id': 2548671383, 'issue_id': 2745037770, 'author': 'potiuk', 'body': 'Marked it as ""2.11.0"" milestone.', 'created_at': datetime.datetime(2024, 12, 17, 14, 56, 8, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-12-17 14:56:08 UTC): Marked it as ""2.11.0"" milestone.

"
2744710571,issue,open,,OpenLineage failed to send DAG start event,"### Apache Airflow Provider(s)

openlineage

### Versions of Apache Airflow Providers

1.14.0

Also seeing missing start DAG events for versions <= 1.12.0. However, those versions weren't logging the exception, making it difficult to determine if this is the same issue.

### Apache Airflow version

2.10.1

### Operating System

Amazon Linux

### Deployment

Amazon (AWS) MWAA

### Deployment details

MWAA with:
- requirements.txt with `apache-airflow-providers-openlineage==1.14.0`
- startup.sh with `OPENLINEAGE_URL` pointing to a webserver logging all received requests

### What happened

OpenLineage provider failed to send some DAG start events, with the following exception in the scheduler logs:

```
[2024-12-17T00:44:00.564+0000] {listener.py:528} WARNING - Failed to submit method to executor
concurrent.futures.process._RemoteTraceback:
""""""
Traceback (most recent call last):
  File ""/usr/local/lib/python3.11/multiprocessing/queues.py"", line 244, in _feed
    obj = _ForkingPickler.dumps(obj)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/multiprocessing/reduction.py"", line 51, in dumps
    cls(buf, protocol).dump(obj)
_pickle.PicklingError: Can't pickle <functools._lru_cache_wrapper object at 0x7fb8f1c02980>: 
it's not the same object as airflow.models.abstractoperator.AbstractOperator.get_parse_time_mapped_ti_count
""""""
```

### What you think should happen instead

_No response_

### How to reproduce

The failures to send events were non-deterministic and appear to be caused by a race condition. They seem to occur more frequently when multiple DAGs are being scheduled simultaneously.

 I used this code to reproduce the issue, and it failed to send at least one DAG start almost every minute.

```python
for i in range(4):
    with DAG(
        f'frequent_dag_{i}',
        schedule_interval=timedelta(minutes=1),
        start_date=days_ago(1),
        catchup=False,
    ) as dag:
        def task():
            print(""Task is running"")

        task = PythonOperator(
            task_id=f'print_task_{i}',
            python_callable=task,
            dag=dag,
        )
```

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",paul-laffon-dd,2024-12-17 11:43:58+00:00,[],2024-12-20 14:13:22+00:00,,https://github.com/apache/airflow/issues/44984,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:openlineage', 'AIP-53')]","[{'comment_id': 2548238474, 'issue_id': 2744710571, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 12, 17, 11, 44, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2548468158, 'issue_id': 2744710571, 'author': 'eladkal', 'body': ""cc @kacpermuda seems like https://github.com/apache/airflow/pull/42448 didn't fix this issue?"", 'created_at': datetime.datetime(2024, 12, 17, 13, 30, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2548588283, 'issue_id': 2744710571, 'author': 'kacpermuda', 'body': ""I think it's unrelated to #42448, but it should not happen anyway. Thanks @paul-laffon-dd for reporting that, I'll investigate it in my free time.\r\ncc @mobuchowski @JDarDagran"", 'created_at': datetime.datetime(2024, 12, 17, 14, 20, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556775523, 'issue_id': 2744710571, 'author': 'paul-laffon-dd', 'body': ""Thanks @kacpermuda \r\n\r\nFrom my understanding of the issue there is one argument of the [dag_started](https://github.com/apache/airflow/blob/providers-openlineage/1.14.0/providers/src/airflow/providers/openlineage/plugins/adapter.py#L326) that is trying to serialize an operator where the `get_parse_time_mapped_ti_count` has already been computed and cached. it's unclear to me which facet is holding this operator\r\n\r\nWhat do you think of switching to a `ThreadPoolExecutor` instead of a [ProcessPoolExecutor](https://github.com/apache/airflow/blob/providers-openlineage/1.14.0/providers/src/airflow/providers/openlineage/plugins/listener.py#L398) ? This would eliminate the need for serialization while still allowing asynchronous execution."", 'created_at': datetime.datetime(2024, 12, 20, 11, 1, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556786442, 'issue_id': 2744710571, 'author': 'potiuk', 'body': '> What do you think of switching to a ThreadPoolExecutor instead of a [ProcessPoolExecutor](https://github.com/apache/airflow/blob/providers-openlineage/1.14.0/providers/src/airflow/providers/openlineage/plugins/listener.py#L398) ? This would eliminate the need for serialization while still allowing asynchronous execution.\r\n\r\nFrom what I understand, there were MANY problems with previous implementation using ThreadPoolExecutor. The problem is that `Threads` are very flawed concept in Python due to GIL - and spawning new threads without full control over running any other threads and what they do (especially when you involve low-level C-code implemented in some libraries called from Python code) introduces a lot of contention, deadlock possibilities, various kinds of errors, especially if those libraries are not written in fully ""thread-safe"" way. I think @kacpermuda and @mobuchowski had a LOT of problems  - particularly with Snowlake integration - caused by this.', 'created_at': datetime.datetime(2024, 12, 20, 11, 7, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556807745, 'issue_id': 2744710571, 'author': 'mobuchowski', 'body': ""The solution should be to _not_ serialize any operators. Not exactly sure where the operator is coming from (I don't think it's this: https://github.com/apache/airflow/blob/490b5e816b804f338b0eb97f240ae874d4e15810/providers/src/airflow/providers/openlineage/utils/utils.py#L529). If we need something from operator, we should select the properties we need and pass them explicitely, rather than relying on pickle.\r\n\r\nRegarding ThreadPoolExecutor, we've switched from that solution since it caused even worse issues: https://github.com/apache/airflow/pull/39235\r\n\r\n@paul-laffon-dd do you know where it's coming from, or have a reproduction?"", 'created_at': datetime.datetime(2024, 12, 20, 11, 21, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2557095046, 'issue_id': 2744710571, 'author': 'paul-laffon-dd', 'body': ""I don't know from which facet this is coming from and I don't have a way to deterministically reproduce it. From my understanding, this only happens if the result of `get_parse_time_mapped_ti_count` is cached before the serialization occurs"", 'created_at': datetime.datetime(2024, 12, 20, 14, 13, 21, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-12-17 11:44:01 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2024-12-17 13:30:25 UTC): cc @kacpermuda seems like https://github.com/apache/airflow/pull/42448 didn't fix this issue?

kacpermuda on (2024-12-17 14:20:05 UTC): I think it's unrelated to #42448, but it should not happen anyway. Thanks @paul-laffon-dd for reporting that, I'll investigate it in my free time.
cc @mobuchowski @JDarDagran

paul-laffon-dd (Issue Creator) on (2024-12-20 11:01:16 UTC): Thanks @kacpermuda 

From my understanding of the issue there is one argument of the [dag_started](https://github.com/apache/airflow/blob/providers-openlineage/1.14.0/providers/src/airflow/providers/openlineage/plugins/adapter.py#L326) that is trying to serialize an operator where the `get_parse_time_mapped_ti_count` has already been computed and cached. it's unclear to me which facet is holding this operator

What do you think of switching to a `ThreadPoolExecutor` instead of a [ProcessPoolExecutor](https://github.com/apache/airflow/blob/providers-openlineage/1.14.0/providers/src/airflow/providers/openlineage/plugins/listener.py#L398) ? This would eliminate the need for serialization while still allowing asynchronous execution.

potiuk on (2024-12-20 11:07:58 UTC): From what I understand, there were MANY problems with previous implementation using ThreadPoolExecutor. The problem is that `Threads` are very flawed concept in Python due to GIL - and spawning new threads without full control over running any other threads and what they do (especially when you involve low-level C-code implemented in some libraries called from Python code) introduces a lot of contention, deadlock possibilities, various kinds of errors, especially if those libraries are not written in fully ""thread-safe"" way. I think @kacpermuda and @mobuchowski had a LOT of problems  - particularly with Snowlake integration - caused by this.

mobuchowski on (2024-12-20 11:21:43 UTC): The solution should be to _not_ serialize any operators. Not exactly sure where the operator is coming from (I don't think it's this: https://github.com/apache/airflow/blob/490b5e816b804f338b0eb97f240ae874d4e15810/providers/src/airflow/providers/openlineage/utils/utils.py#L529). If we need something from operator, we should select the properties we need and pass them explicitely, rather than relying on pickle.

Regarding ThreadPoolExecutor, we've switched from that solution since it caused even worse issues: https://github.com/apache/airflow/pull/39235

@paul-laffon-dd do you know where it's coming from, or have a reproduction?

paul-laffon-dd (Issue Creator) on (2024-12-20 14:13:21 UTC): I don't know from which facet this is coming from and I don't have a way to deterministically reproduce it. From my understanding, this only happens if the result of `get_parse_time_mapped_ti_count` is cached before the serialization occurs

"
2744655800,issue,closed,completed,Remove old lineage stuff,"### Body

We had some experiemental lineage classes in `airflow.lineage`. Those never caught on, and are not relevant anymore now that we have assets and trasparent OpenLineage support via [AIP-62](https://cwiki.apache.org/confluence/x/PoqSEQ). We should remove the old stuff in 3.0, and put up a deprecation warning in 2.11 if not already.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",uranusjr,2024-12-17 11:18:15+00:00,['jason810496'],2025-01-25 03:42:48+00:00,2025-01-25 03:42:48+00:00,https://github.com/apache/airflow/issues/44983,"[('good first issue', ''), ('area:lineage', ''), ('airflow3.0:candidate', 'Potential candidates for Airflow 3.0'), ('airflow3.0:breaking', 'Candidates for Airflow 3.0 that contain breaking changes')]","[{'comment_id': 2550739809, 'issue_id': 2744655800, 'author': 'jason810496', 'body': 'Hi @uranusjr, I can work on this issue, could you assign to me ? Thanks !', 'created_at': datetime.datetime(2024, 12, 18, 8, 59, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2550817929, 'issue_id': 2744655800, 'author': 'ashb', 'body': '@jason810496 All yours!', 'created_at': datetime.datetime(2024, 12, 18, 9, 28, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556731910, 'issue_id': 2744655800, 'author': 'jason810496', 'body': 'Hi,  I have a few questions while working on this issue:  \r\n\r\n- The compatibility module (`airflow.providers.common.compat.lineage`) still imports the `airflow.lineage` module. Does this mean I should move `airflow.lineage` to `airflow.providers.common.compat.lineage` instead of simply deleting it?  \r\n- For other `airflow` modules that import `airflow.lineage`, can all logic involving `lineage` be removed?  \r\n\r\nThanks !', 'created_at': datetime.datetime(2024, 12, 20, 10, 38, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561493132, 'issue_id': 2744655800, 'author': 'Lee-W', 'body': "">     * The compatibility module (`airflow.providers.common.compat.lineage`) still imports the `airflow.lineage` module. Does this mean I should move `airflow.lineage` to `airflow.providers.common.compat.lineage` instead of simply deleting it?\r\n\r\nI think so. As not every users upgrade to 3.0 right after release.\r\n\r\n \r\n>     * For other `airflow` modules that import `airflow.lineage`, can all logic involving `lineage` be removed?\r\n\r\nWe probably need to list what are `other airflow modules that import `airflow.lineage`. But if if the issue is already created here, I guess it's safe to remove them 🤔"", 'created_at': datetime.datetime(2024, 12, 24, 23, 48, 35, tzinfo=datetime.timezone.utc)}]","jason810496 (Assginee) on (2024-12-18 08:59:24 UTC): Hi @uranusjr, I can work on this issue, could you assign to me ? Thanks !

ashb on (2024-12-18 09:28:14 UTC): @jason810496 All yours!

jason810496 (Assginee) on (2024-12-20 10:38:13 UTC): Hi,  I have a few questions while working on this issue:  

- The compatibility module (`airflow.providers.common.compat.lineage`) still imports the `airflow.lineage` module. Does this mean I should move `airflow.lineage` to `airflow.providers.common.compat.lineage` instead of simply deleting it?  
- For other `airflow` modules that import `airflow.lineage`, can all logic involving `lineage` be removed?  

Thanks !

Lee-W on (2024-12-24 23:48:35 UTC): I think so. As not every users upgrade to 3.0 right after release.

 

We probably need to list what are `other airflow modules that import `airflow.lineage`. But if if the issue is already created here, I guess it's safe to remove them 🤔

"
2744195224,issue,closed,completed,Forbid extra fields in execution API request and response models,"### Body

Reference: https://github.com/apache/airflow/pull/44562/files#r1869482261

In lieueu of the above comment, we should update all the request and response models for execution API to forbid any extra fields. Although it makes more sense to do so for any of the PUT/POST APIs. Check under `airflow/api_fastapi/execution_api/datamodels/*.py` to find all the models. These models will be in sync with what is present in `task_sdk/src/airflow/sdk/api/datamodels/_generated.py`. Idea is to update `airflow/api_fastapi/execution_api/datamodels/*.py` first and make relevant changes to `task_sdk/src/airflow/sdk/api/datamodels/_generated.py` after.
",amoghrajesh,2024-12-17 08:02:05+00:00,['jx2lee'],2025-02-05 12:53:42+00:00,2025-02-05 12:53:42+00:00,https://github.com/apache/airflow/issues/44978,"[('area:API', ""Airflow's REST/HTTP API""), ('good first issue', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]","[{'comment_id': 2548166257, 'issue_id': 2744195224, 'author': 'jx2lee', 'body': ""Can I take over this ? If you assign it to me, I'll take a quick look at it.\r\n(Do I create and link a PR first?)"", 'created_at': datetime.datetime(2024, 12, 17, 11, 7, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2548253639, 'issue_id': 2744195224, 'author': 'amoghrajesh', 'body': 'Go for it. I will assign to you, you can start working on it', 'created_at': datetime.datetime(2024, 12, 17, 11, 51, 51, tzinfo=datetime.timezone.utc)}]","jx2lee (Assginee) on (2024-12-17 11:07:18 UTC): Can I take over this ? If you assign it to me, I'll take a quick look at it.
(Do I create and link a PR first?)

amoghrajesh (Issue Creator) on (2024-12-17 11:51:51 UTC): Go for it. I will assign to you, you can start working on it

"
2744037894,issue,open,,Change TI table to have unique UUID id/pk per attempt,"We want to update the task instance and ti history tables to have a unique id per attempt (this row id will be used in the strong identity token issued to reach TI attempt.)

We want to add a single column pk of a UUID, and should use UUID v7 (as it has better temporal sorting behaviours than the random v4). For the migration to update existing rows we can use v4 which most DBs have natively.

We should propose keep the ""denormalized"" columns of dag_id and run_id for easier searching/querying.",kaxil,2024-12-17 06:23:04+00:00,[],2024-12-17 06:25:20+00:00,,https://github.com/apache/airflow/issues/44975,"[('area:db-migrations', 'PRs with DB migration')]",[],
2743146556,issue,closed,completed,Revisit default bundle config,"Currently we have a `dags_folder` entry by default in `dag_bundles`.  I think it's a bit confusing.

When i see that, i assume it's the new location for the old `dags_folder` setting.  or that it's a setting that applies to all dag bundles.

I think we can do something to disabiguate this.

One option would be to do something like this
```
[dag_bundles]

bundle_configs = [
	{""name"": ""blah"", ""classpath"": ""blah""},
	{""name"": ""git"", ""classpath"": ""somethingsomething.Git"",""kwargs"": {}}
  ]
```",dstandish,2024-12-16 18:56:08+00:00,[],2025-01-28 04:29:31+00:00,2025-01-28 04:29:30+00:00,https://github.com/apache/airflow/issues/44965,[],"[{'comment_id': 2558350437, 'issue_id': 2743146556, 'author': 'Prab-27', 'body': ""@dstandish , I'd like to work on this issue. Could you please tell me the file path? I'm still new, and I can't find it"", 'created_at': datetime.datetime(2024, 12, 22, 6, 38, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2617819936, 'issue_id': 2743146556, 'author': 'jedcunningham', 'body': 'This has been refactored and I believe Daniel is happy(er) with it now :)', 'created_at': datetime.datetime(2025, 1, 28, 4, 29, 30, tzinfo=datetime.timezone.utc)}]","Prab-27 on (2024-12-22 06:38:31 UTC): @dstandish , I'd like to work on this issue. Could you please tell me the file path? I'm still new, and I can't find it

jedcunningham on (2025-01-28 04:29:30 UTC): This has been refactored and I believe Daniel is happy(er) with it now :)

"
2741941599,issue,closed,completed," Redirect URI Error, and I am using Airflow version 2.8.3 using helm chart version 1.13.1","### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.8.3

### What happened?

Redirect URI ERROR while integrating Airflow with SSO (Authentik)
Below are the configuration i have specified according to my requirements:
# webserver_config.py

from flask_appbuilder.security.manager import AUTH_OAUTH

AUTH_TYPE = AUTH_OAUTH

OAUTH_PROVIDERS = [
    {
        'name': 'authentik',
        'token_key': 'access_token',
        'icon': 'fa-address-card',
        'remote_app': {
            'client_id': '<>',
            'client_secret': '<>',
            'client_kwargs': {
                'scope': 'openid profile email',
            },
            'access_token_url': 'https://authentik.piyush.today/application/o/token/',
            'authorize_url': 'https://authentik.piyush.today/application/o/authorize/',
            'api_base_url': 'https://authentik.piyush.today/application/o/',
            'redirect_uri': 'https://authentik.piyush.today/oauth-authorized/authentik',
            'jwks_uri': 'https://authentik.piyush.today/application/o/customairflow/jwks/',
            ""server_metadata_url"": ""https://authentik.piyush.today/application/o/customairflow/.well-known/openid-configuration"",
            ""issuer"": ""https://authentik.piyush.today/application/o/customairflow/""
        }
    }
]

AUTH_USER_REGISTRATION = True
AUTH_USER_REGISTRATION_ROLE = ""Viewer""

#custom-values.yaml


# Define the Kubernetes Executor
executor: KubernetesExecutor

# -------------------------------
# PostgreSQL Configuration
# -------------------------------
postgresql:
  enabled: true
  auth: 
    postgresqlPassword: airflow
  postgresqlDatabase: airflow

# -------------------------------
# Redis Configuration (Optional for KubernetesExecutor)
# -------------------------------
redis:
  enabled: true


defaultAirflowTag: ""2.9.2""

airflowVersion: ""2.9.2""
# -------------------------------
# Airflow Configuration
# -------------------------------

migrateDatabaseJob:
  enabled: true
  ttlSecondsAfterFinished: 300
  args:
    - ""bash""
    - ""-c""
    - |
      set -e
      echo ""Running database migrations...""
      airflow db upgrade
      airflow db migrate

webserver:
  enabled: true
  webserverConfigConfigMapName: webserver-config  # Correct key name
  waitForMigrations:
    enabled: false  # Disable waitForMigrations for the webserver

scheduler:
  waitForMigrations:
    enabled: false  # Disable waitForMigrations for the schedulerl

workers:
  waitForMigrations:
    enabled: false  # Disable waitForMigrations for workers

triggerer:
  waitForMigrations:
    enabled: false  # Disable waitForMigrations for triggerer

dagProcessor:
  waitForMigrations:
    enabled: false  # Disable waitForMigrations for dagProcessor

# -------------------------------
# Ingress Configuration
# -------------------------------
ingress:
  web:
    enabled: true
    annotations:
      alb.ingress.kubernetes.io/scheme: ""internet-facing""
      alb.ingress.kubernetes.io/target-type: ""ip""
      alb.ingress.kubernetes.io/load-balancer-name: ""aws-alb-airflow""
      alb.ingress.kubernetes.io/listen-ports: '[{""HTTP"": 80}]'
      alb.ingress.kubernetes.io/healthcheck-path: /health
      alb.ingress.kubernetes.io/success-codes: ""200""
      alb.ingress.kubernetes.io/manage-backend-security-group-rules: ""true""
      alb.ingress.kubernetes.io/security-groups: ""sg-0e7244a820c5e03ce""
      alb.ingress.kubernetes.io/subnets: ""subnet-0e93acffe7efb7530,subnet-064a10482338a08a4,subnet-010368913ca69b726"" 
    path: /
    pathType: Prefix
    hosts:
      - name: lavish.piyush.today  
    ingressClassName: alb

when i am logging in to lavish.piyush.today the host of my airflow, over there after Clicking on Sign in with Authentik, it is showing ""Redirect URI Error"" but i have correctly specified all the uri's and then too, i am not getting why it is happening.




### What you think should happen instead?

It should take me to the the SSO and comeback to airflow dashboard after successful login in sso.

### How to reproduce

.

### Operating System

Windows

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Piyush-Fury,2024-12-16 10:17:27+00:00,[],2024-12-16 18:05:43+00:00,2024-12-16 18:05:43+00:00,https://github.com/apache/airflow/issues/44955,"[('kind:bug', 'This is a clearly a bug'), ('area:helm-chart', 'Airflow Helm Chart'), ('area:auth', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2741736925,issue,open,,Handle Task instance history from task SDK,"### Body

Usage: https://github.com/apache/airflow/blob/aaf29ee54bd8666a8dc3a129aed213f6b3b31bde/airflow/models/taskinstance.py#L3089-L3095",amoghrajesh,2024-12-16 08:49:47+00:00,['amoghrajesh'],2025-01-28 17:12:51+00:00,,https://github.com/apache/airflow/issues/44952,"[('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK'), ('area:task-sdk', None)]","[{'comment_id': 2619550833, 'issue_id': 2741736925, 'author': 'ashb', 'body': 'See also #43437 \n\nMy gut now is that we want to have have a TaskHistory row be created for each TI uuid, and at the same time as part of this PR that when a new TI try is being created (i.e. when we previoulsy changed try_number) we instead delete and create a new TI row.\n\nThis likely means that anything that is FKing to TI table today should instead FK to the TIHistory!  Ah, which I already created an issue for in #44975', 'created_at': datetime.datetime(2025, 1, 28, 16, 51, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2619600834, 'issue_id': 2741736925, 'author': 'ashb', 'body': 'Also related [#44147](https://github.com/apache/airflow/issues/44147)', 'created_at': datetime.datetime(2025, 1, 28, 17, 12, 50, tzinfo=datetime.timezone.utc)}]","ashb on (2025-01-28 16:51:50 UTC): See also #43437 

My gut now is that we want to have have a TaskHistory row be created for each TI uuid, and at the same time as part of this PR that when a new TI try is being created (i.e. when we previoulsy changed try_number) we instead delete and create a new TI row.

This likely means that anything that is FKing to TI table today should instead FK to the TIHistory!  Ah, which I already created an issue for in #44975

ashb on (2025-01-28 17:12:50 UTC): Also related [#44147](https://github.com/apache/airflow/issues/44147)

"
2741728602,issue,open,,Handle `fail_stop` / non teardown tasks in TASK SDK,"### Body

Handle the part where we stop all remaining tasks for while marking a task as failed.
Usage: https://github.com/apache/airflow/blob/aaf29ee54bd8666a8dc3a129aed213f6b3b31bde/airflow/models/taskinstance.py#L3086-L3087",amoghrajesh,2024-12-16 08:46:50+00:00,['amoghrajesh'],2025-01-21 02:05:17+00:00,,https://github.com/apache/airflow/issues/44951,"[('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK'), ('area:task-sdk', None)]","[{'comment_id': 2580076661, 'issue_id': 2741728602, 'author': 'kaxil', 'body': 'Ths is still pending, right @amoghrajesh ? Or is this one already complete?', 'created_at': datetime.datetime(2025, 1, 9, 12, 49, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2601408186, 'issue_id': 2741728602, 'author': 'amoghrajesh', 'body': 'Yeah this is still pending and we are waiting on https://github.com/apache/airflow/pull/45327 to get this one in.', 'created_at': datetime.datetime(2025, 1, 20, 5, 33, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2603479546, 'issue_id': 2741728602, 'author': 'Lee-W', 'body': 'just merged https://github.com/apache/airflow/pull/45327', 'created_at': datetime.datetime(2025, 1, 21, 2, 5, 16, tzinfo=datetime.timezone.utc)}]","kaxil on (2025-01-09 12:49:43 UTC): Ths is still pending, right @amoghrajesh ? Or is this one already complete?

amoghrajesh (Issue Creator) on (2025-01-20 05:33:40 UTC): Yeah this is still pending and we are waiting on https://github.com/apache/airflow/pull/45327 to get this one in.

Lee-W on (2025-01-21 02:05:16 UTC): just merged https://github.com/apache/airflow/pull/45327

"
