id,type,state,state_reason,title,body,author,created_at,assignees,updated_at,closed_at,url,labels,comments_list,comment_thread
2598311675,issue,open,,Make Airflow stack traces configurable and easy-to-understand,"### Description

As per users' feedback in the Airflow Debugging Survey 2024, 55.2% respondents find stack traces challenging.

### Use case/motivation

Goal for this issue:
- Configure depth of stack trace using a config property like `AIRFLOW_STACK_TRACK_DEPTH` which can then only show stack trace for that much depth e.g. if `AIRFLOW_STACK_TRACK_DEPTH=3` then we show only 3 levels deep stack trace as the stack unwinds.
- Either depth as above or can use number of lines as well (this will be more of an absolute config as in some case 1 level deep trace can take up all the lines!).
- Use color coding wherever necessary, and simpler language for the user to understand the stack trace - probably by eliminating any redundant parts of the stack trace wherever possible.

### Related issues

Parent Issue: https://github.com/apache/airflow/issues/40975

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-10-18 20:12:32+00:00,[],2024-10-29 20:11:48+00:00,,https://github.com/apache/airflow/issues/43177,"[('kind:feature', 'Feature Requests'), ('kind:meta', 'High-level information important to the community'), ('area:core', '')]","[{'comment_id': 2425656192, 'issue_id': 2598311675, 'author': 'hterik', 'body': ""Improving the stack traces sounds like an excellent idea. Similar effort was previously discussed in https://github.com/apache/airflow/discussions/20060 \r\nI still stand by that idea and think that splitting the stack trace per the different layers in the stack, rather than a number-based depth is preferable.\r\nIt's not possible to pick a depth that is good for all situations and it will constantly be a moving target, in case someone decides to just make one more function call. It's better to have known cutoff-points.\r\nThe `core.dagbag_import_error_traceback_depth` has that issue, picking a number too short will often leave out relevant parts.\r\n\r\nIt's better to declare everything up until some point as internals, that should not be presented. To give an example, tracebacks from a python script don't contain stacks from the CPython C-code for example."", 'created_at': datetime.datetime(2024, 10, 21, 5, 56, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2435728991, 'issue_id': 2598311675, 'author': 'potiuk', 'body': 'We could look at rich stacktraces. https://rich.readthedocs.io/en/stable/traceback.html - the are already way nicer than regular stack traces with colors, formatting, tables and even displaying local variables for each stack, and ability to suppress frames that we do not want to see. \r\n\r\nI already use them in `breeze`. \r\n\r\nThough there are a bit unfamiliar for Python people and require a bit gettigng used to, they are way more powerful. So likely some examples and discussions on the devlist should happen whether they will be acceptable for us and wheter we would like to have them exposed by default for end users (or maybe it should be somehow controlled)\r\n\r\nSee the blog post from Will - creator of rich with some examples https://www.willmcgugan.com/blog/tech/post/better-python-tracebacks-with-rich/\r\n\r\nBTW. The blog shows very old version of those - the actual traceback that can be produced now are even more powerful', 'created_at': datetime.datetime(2024, 10, 24, 16, 22, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443580191, 'issue_id': 2598311675, 'author': 'Dev-iL', 'body': '@potiuk You may remember [the discussion about supporting loguru](https://github.com/apache/airflow/discussions/36980). One of the reasons I like loguru is the context it provides for errors (through https://github.com/Qix-/better-exceptions, example below). Any logging solution that includes context would be greatly appreciated!\n\n----------\nExample traceback using `better-exceptions`:\n\n![](https://github.com/Qix-/better-exceptions/blob/master/screenshot.png?raw=true)\n\nExample traceback using `rich`:\n\n![](https://raw.githubusercontent.com/textualize/rich/master/imgs/traceback.png)', 'created_at': datetime.datetime(2024, 10, 29, 8, 40, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445229125, 'issue_id': 2598311675, 'author': 'potiuk', 'body': 'I honestly like `rich` traceback more. Somehow interleaving values and code from loguru is not as good - and nicely coloured snippets of code and separting locals to a separate ""window"" from rich is pretty cool.', 'created_at': datetime.datetime(2024, 10, 29, 20, 8, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445235032, 'issue_id': 2598311675, 'author': 'potiuk', 'body': 'And BTW. my comments about loguru was that it was a complete logging framework where you not only have different traceback but have a complete configuration of logging there, which a) we already have and b) it would likely not work out of the box with our ""task"" loggers, stdout/stderr redirection, remote logging (including not only writing but also reading logs by the webserver) :).', 'created_at': datetime.datetime(2024, 10, 29, 20, 11, 46, tzinfo=datetime.timezone.utc)}]","hterik on (2024-10-21 05:56:04 UTC): Improving the stack traces sounds like an excellent idea. Similar effort was previously discussed in https://github.com/apache/airflow/discussions/20060 
I still stand by that idea and think that splitting the stack trace per the different layers in the stack, rather than a number-based depth is preferable.
It's not possible to pick a depth that is good for all situations and it will constantly be a moving target, in case someone decides to just make one more function call. It's better to have known cutoff-points.
The `core.dagbag_import_error_traceback_depth` has that issue, picking a number too short will often leave out relevant parts.

It's better to declare everything up until some point as internals, that should not be presented. To give an example, tracebacks from a python script don't contain stacks from the CPython C-code for example.

potiuk on (2024-10-24 16:22:24 UTC): We could look at rich stacktraces. https://rich.readthedocs.io/en/stable/traceback.html - the are already way nicer than regular stack traces with colors, formatting, tables and even displaying local variables for each stack, and ability to suppress frames that we do not want to see. 

I already use them in `breeze`. 

Though there are a bit unfamiliar for Python people and require a bit gettigng used to, they are way more powerful. So likely some examples and discussions on the devlist should happen whether they will be acceptable for us and wheter we would like to have them exposed by default for end users (or maybe it should be somehow controlled)

See the blog post from Will - creator of rich with some examples https://www.willmcgugan.com/blog/tech/post/better-python-tracebacks-with-rich/

BTW. The blog shows very old version of those - the actual traceback that can be produced now are even more powerful

Dev-iL on (2024-10-29 08:40:33 UTC): @potiuk You may remember [the discussion about supporting loguru](https://github.com/apache/airflow/discussions/36980). One of the reasons I like loguru is the context it provides for errors (through https://github.com/Qix-/better-exceptions, example below). Any logging solution that includes context would be greatly appreciated!

----------
Example traceback using `better-exceptions`:

![](https://github.com/Qix-/better-exceptions/blob/master/screenshot.png?raw=true)

Example traceback using `rich`:

![](https://raw.githubusercontent.com/textualize/rich/master/imgs/traceback.png)

potiuk on (2024-10-29 20:08:57 UTC): I honestly like `rich` traceback more. Somehow interleaving values and code from loguru is not as good - and nicely coloured snippets of code and separting locals to a separate ""window"" from rich is pretty cool.

potiuk on (2024-10-29 20:11:46 UTC): And BTW. my comments about loguru was that it was a complete logging framework where you not only have different traceback but have a complete configuration of logging there, which a) we already have and b) it would likely not work out of the box with our ""task"" loggers, stdout/stderr redirection, remote logging (including not only writing but also reading logs by the webserver) :).

"
2598299648,issue,open,,Explore and add static checks for DAGs for early detection of common issues,"### Description

As per users' feedback in the Airflow Debugging Survey 2024, 48.3% of respondents chose early issue detection during execution as one of their top 2 choices.

### Use case/motivation

Goal of this issue:
- Enhance early detection of DAG issues to minimize dev time
- Some sort of static analysis similar to Ruff checks for DAGs
- Runtime analysis of DAGs if feasible can also be done to save dev time

### Related issues

Parent Issue: https://github.com/apache/airflow/issues/40975

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-10-18 20:03:00+00:00,[],2025-01-05 09:26:50+00:00,,https://github.com/apache/airflow/issues/43176,"[('kind:feature', 'Feature Requests'), ('area:DAG-processing', '')]","[{'comment_id': 2444077100, 'issue_id': 2598299648, 'author': 'omkar-foss', 'body': 'In the context of this issue, DAG linting as mentioned by this article (shared by @potiuk on slack) can also be explored:\r\n\r\nhttps://medium.com/@snir.isl/mastering-airflow-dag-standardization-with-pythons-ast-a-deep-dive-into-linting-at-scale-1396771a9b90\r\n\r\nInspired by above article, may be we can have commands like `airflow dag lint <dagname>.py` for checking common DAGs issues and `airflow dag grade <dagname>.py` for grading DAGs based on their code quality.', 'created_at': datetime.datetime(2024, 10, 29, 12, 29, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445202272, 'issue_id': 2598299648, 'author': 'potiuk', 'body': 'Love that idea. We could even had some way of checking for ""best practices"" - like not using DB while parsing etc. This might also be then used as part of the upgrade-check mechanism that we are planning for Airflow 2-> 3 migration - see https://github.com/apache/airflow/issues/41641 cc: @Lee-W', 'created_at': datetime.datetime(2024, 10, 29, 19, 54, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445607755, 'issue_id': 2598299648, 'author': 'Lee-W', 'body': 'I feel it\'s a bit different. 🤔 But for `not using DB while parsing`, that\'s something we should check in the upgrade check. But the ""best practices"" thing would probably be something else. Probably integrating with ruff or building our own linter would be better.', 'created_at': datetime.datetime(2024, 10, 30, 1, 11, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2571560579, 'issue_id': 2598299648, 'author': 'Dev-iL', 'body': 'Good to see new [airflow-specific Ruff rules](https://docs.astral.sh/ruff/rules/#airflow-air) being added (AIR3##). Nice work @Lee-W!', 'created_at': datetime.datetime(2025, 1, 5, 9, 26, 49, tzinfo=datetime.timezone.utc)}]","omkar-foss (Issue Creator) on (2024-10-29 12:29:23 UTC): In the context of this issue, DAG linting as mentioned by this article (shared by @potiuk on slack) can also be explored:

https://medium.com/@snir.isl/mastering-airflow-dag-standardization-with-pythons-ast-a-deep-dive-into-linting-at-scale-1396771a9b90

Inspired by above article, may be we can have commands like `airflow dag lint <dagname>.py` for checking common DAGs issues and `airflow dag grade <dagname>.py` for grading DAGs based on their code quality.

potiuk on (2024-10-29 19:54:07 UTC): Love that idea. We could even had some way of checking for ""best practices"" - like not using DB while parsing etc. This might also be then used as part of the upgrade-check mechanism that we are planning for Airflow 2-> 3 migration - see https://github.com/apache/airflow/issues/41641 cc: @Lee-W

Lee-W on (2024-10-30 01:11:02 UTC): I feel it's a bit different. 🤔 But for `not using DB while parsing`, that's something we should check in the upgrade check. But the ""best practices"" thing would probably be something else. Probably integrating with ruff or building our own linter would be better.

Dev-iL on (2025-01-05 09:26:49 UTC): Good to see new [airflow-specific Ruff rules](https://docs.astral.sh/ruff/rules/#airflow-air) being added (AIR3##). Nice work @Lee-W!

"
2598031545,issue,open,,"Make Airflow error messages more specific, clear and actionable","### Description

As per users' feedback in the Airflow Debugging Survey 2024, around 41.7% respondents don't consider error messages as actionable. Overall feedback also suggests that users find some error messages vague and confusing.

### Use case/motivation

**Goals for this issue are the following:**
- Identify and revise error messages that are vague, lack context, or do not provide clear guidance on resolving issues.
- Provide detailed information, context, and actionable steps within the error messages to help users troubleshoot.
- Transform error messages with meaningful linking wherever possible. e.g. an error like `Celery command failed on host` can be transformed or displayed with something like ""Please check your DAG processor timeout variable for this"". So the user has a starting point to start debugging.

### Related issues

Parent Issue: https://github.com/apache/airflow/issues/40975

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-10-18 17:22:17+00:00,[],2024-12-03 16:14:27+00:00,,https://github.com/apache/airflow/issues/43171,"[('kind:feature', 'Feature Requests'), ('kind:meta', 'High-level information important to the community'), ('area:core', '')]","[{'comment_id': 2425691353, 'issue_id': 2598031545, 'author': 'hterik', 'body': 'I can recommend this guide from Google about writing good error messages: https://developers.google.com/tech-writing/error-messages. The rest of the courses in that book are also really good btw.\r\n\r\n> an error like `Celery command failed on host` can be transformed or displayed with something like ""`Please check your DAG processor timeout variable for this`"". \r\n\r\nActionable errors are good, but has to be done very carefully, because if it gives misleading advice it will lead users down chasing the wrong rabbit hole. For example this log in `standard_task_runner.py` is most of the time not due to memory running out:  `""Job %s was killed before it finished (likely due to running out of memory)"",`. I\'ve seen our engineers chasing memory issues in vain countless of times because of that message. (yes we should have filed a PR :smile:)', 'created_at': datetime.datetime(2024, 10, 21, 6, 22, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2435764433, 'issue_id': 2598031545, 'author': 'potiuk', 'body': '> but has to be done very carefully, because if it gives misleading advice it will lead users down chasing the wrong rabbit hole. For example this log in standard_task_runner.py is most of the time not due to memory running out: ""Job %s was killed before it finished (likely due to running out of memory)"",. I\'ve seen our engineers chasing memory issues in vain countless of times because of that message. \r\n\r\nI am big fan of ""always tell the user what action from their side the error implies."". Agree things can be misleading and re the case you mentioned  - I cannot find it now (I think I discussed it in the past), but I think in case of such complicated and multi-possible-root-cause we should explain what\'s going on and link to a FAQ page on Airflow explaining possible reasons. This way when you have the error, and we find other reasons and more detailed explanations what could be wrong and how to remediate it - we can always update the docs and add more information that will be useful for many past versions of airflow that people will have.\r\n\r\n> (yes we should have filed a PR 😄)\r\n\r\nAbsolutely :)', 'created_at': datetime.datetime(2024, 10, 24, 16, 41, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2444113421, 'issue_id': 2598031545, 'author': 'omkar-foss', 'body': 'Have a suggestion for multi-possible-root-cause issues - we can print Airflow error code with the error message e.g. `AERR055: Job 10 was killed before it finished` and can have an error code mapping with possible root causes like (just examples, not real causes):\r\n\r\n| Error Code | Possible Commonly Observed Causes                       |\r\n|------------|---------------------------------------------------------|\r\n|  AERR055   | 1) Ran out of memory                                    |\r\n|            | 2) Job was stuck and killed after timeout               |\r\n|            | 3) Job being run on Spot Instance Node (K8S on EKS)     |\r\n\r\nSince error codes are shareable and easily searchable, it would be useful for team collaboration as well (e.g. instead of me saying ""I\'m looking into the error `Job 10 was killed before it finished`"", can probably just say ""I\'m looking into AERR055"". Much like how we use JIRA ticket numbers or GitHub issue/PR numbers.', 'created_at': datetime.datetime(2024, 10, 29, 12, 45, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445207696, 'issue_id': 2598031545, 'author': 'potiuk', 'body': ':heart:  this. This is what many other tools are doing already. And being able to classify and list all the different types of errors that the software can generate, together with explaining their cause and remediations  - even just list those - is a sign of high maturity of the software.', 'created_at': datetime.datetime(2024, 10, 29, 19, 57, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445213423, 'issue_id': 2598031545, 'author': 'potiuk', 'body': 'I really like it.\r\n\r\nWe could **finally** find a use for AirflowException - so far it was mainly about being a base class for a number of exceptions, but if we add mandatory ""error id"" to AirflowException and make Airflow Exception abstract, and add handling so that that Error ID is displayed in the logs and maybe also produced as metric (counting the errors) and produce an event in the OTEL trace when they happen, might be really great mechanism to have and to ""force"" classification of all the errors that we have in Airflow.', 'created_at': datetime.datetime(2024, 10, 29, 20, 0, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466733440, 'issue_id': 2598031545, 'author': 'kunaljubce', 'body': ""@potiuk @omkar-foss I really like how this discussion is shaping up. Have we established any guidelines or SOPs around how to designate the error codes? Or if there's a thread where this discussion is ongoing, would be happy to contribute (both via discussions and PR)."", 'created_at': datetime.datetime(2024, 11, 10, 13, 19, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2468648123, 'issue_id': 2598031545, 'author': 'omkar-foss', 'body': ""Nice to hear from you @kunaljubce.\r\n\r\nI'm working on a doc to describe a list of all Airflow-related exceptions - starting with the `AirflowException` (as @potiuk mentioned [above](https://github.com/apache/airflow/issues/43171#issuecomment-2445213423)) as `AERR001`, and subsequent error codes assigned incrementally in a bread-first order. Will share the doc in the next few days.\r\n\r\nWe can then update that list as required based on further discussion."", 'created_at': datetime.datetime(2024, 11, 11, 17, 3, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2505372476, 'issue_id': 2598031545, 'author': 'omkar-foss', 'body': "">I'm working on a doc to describe a list of all Airflow-related exceptions - starting with the AirflowException (as @potiuk mentioned https://github.com/apache/airflow/issues/43171#issuecomment-2445213423) as AERR001, and subsequent error codes assigned incrementally in a bread-first order. Will share the doc in the next few days.\r\n\r\nHi, I'm still working on this, got caught up with other things. Will share the list in the next couple of days or so."", 'created_at': datetime.datetime(2024, 11, 28, 6, 45, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2514907930, 'issue_id': 2598031545, 'author': 'omkar-foss', 'body': ""Hey all, apologies for the delay on this. I've created a very basic guide with the Airflow error mapping, which we all can start adding to and improving further. For further details, kindly refer to this Airflow community slack thread [here](https://apache-airflow.slack.com/archives/C07J87PK1BK/p1733239968796499).\r\n\r\nUpdate: You can also refer to https://github.com/apache/airflow/pull/44616"", 'created_at': datetime.datetime(2024, 12, 3, 15, 38, 12, tzinfo=datetime.timezone.utc)}]","hterik on (2024-10-21 06:22:22 UTC): I can recommend this guide from Google about writing good error messages: https://developers.google.com/tech-writing/error-messages. The rest of the courses in that book are also really good btw.


Actionable errors are good, but has to be done very carefully, because if it gives misleading advice it will lead users down chasing the wrong rabbit hole. For example this log in `standard_task_runner.py` is most of the time not due to memory running out:  `""Job %s was killed before it finished (likely due to running out of memory)"",`. I've seen our engineers chasing memory issues in vain countless of times because of that message. (yes we should have filed a PR :smile:)

potiuk on (2024-10-24 16:41:04 UTC): I am big fan of ""always tell the user what action from their side the error implies."". Agree things can be misleading and re the case you mentioned  - I cannot find it now (I think I discussed it in the past), but I think in case of such complicated and multi-possible-root-cause we should explain what's going on and link to a FAQ page on Airflow explaining possible reasons. This way when you have the error, and we find other reasons and more detailed explanations what could be wrong and how to remediate it - we can always update the docs and add more information that will be useful for many past versions of airflow that people will have.


Absolutely :)

omkar-foss (Issue Creator) on (2024-10-29 12:45:34 UTC): Have a suggestion for multi-possible-root-cause issues - we can print Airflow error code with the error message e.g. `AERR055: Job 10 was killed before it finished` and can have an error code mapping with possible root causes like (just examples, not real causes):

| Error Code | Possible Commonly Observed Causes                       |
|------------|---------------------------------------------------------|
|  AERR055   | 1) Ran out of memory                                    |
|            | 2) Job was stuck and killed after timeout               |
|            | 3) Job being run on Spot Instance Node (K8S on EKS)     |

Since error codes are shareable and easily searchable, it would be useful for team collaboration as well (e.g. instead of me saying ""I'm looking into the error `Job 10 was killed before it finished`"", can probably just say ""I'm looking into AERR055"". Much like how we use JIRA ticket numbers or GitHub issue/PR numbers.

potiuk on (2024-10-29 19:57:15 UTC): :heart:  this. This is what many other tools are doing already. And being able to classify and list all the different types of errors that the software can generate, together with explaining their cause and remediations  - even just list those - is a sign of high maturity of the software.

potiuk on (2024-10-29 20:00:31 UTC): I really like it.

We could **finally** find a use for AirflowException - so far it was mainly about being a base class for a number of exceptions, but if we add mandatory ""error id"" to AirflowException and make Airflow Exception abstract, and add handling so that that Error ID is displayed in the logs and maybe also produced as metric (counting the errors) and produce an event in the OTEL trace when they happen, might be really great mechanism to have and to ""force"" classification of all the errors that we have in Airflow.

kunaljubce on (2024-11-10 13:19:34 UTC): @potiuk @omkar-foss I really like how this discussion is shaping up. Have we established any guidelines or SOPs around how to designate the error codes? Or if there's a thread where this discussion is ongoing, would be happy to contribute (both via discussions and PR).

omkar-foss (Issue Creator) on (2024-11-11 17:03:55 UTC): Nice to hear from you @kunaljubce.

I'm working on a doc to describe a list of all Airflow-related exceptions - starting with the `AirflowException` (as @potiuk mentioned [above](https://github.com/apache/airflow/issues/43171#issuecomment-2445213423)) as `AERR001`, and subsequent error codes assigned incrementally in a bread-first order. Will share the doc in the next few days.

We can then update that list as required based on further discussion.

omkar-foss (Issue Creator) on (2024-11-28 06:45:38 UTC): Hi, I'm still working on this, got caught up with other things. Will share the list in the next couple of days or so.

omkar-foss (Issue Creator) on (2024-12-03 15:38:12 UTC): Hey all, apologies for the delay on this. I've created a very basic guide with the Airflow error mapping, which we all can start adding to and improving further. For further details, kindly refer to this Airflow community slack thread [here](https://apache-airflow.slack.com/archives/C07J87PK1BK/p1733239968796499).

Update: You can also refer to https://github.com/apache/airflow/pull/44616

"
2597932494,issue,closed,completed,Implement caching of NPM in CI  / local dev,"Currently when NPM packages are installed on CI they are installed from the scratch by ""compile www assets"" pre-commit. 

This causes fairly frequent, intermittent issues when there is a networking issue or NPM server issue - for example:

```
Compile www assets (manual)..............................................Failed
  - hook id: compile-www-assets
  - exit code: 1
  
  yarn install v1.22.21
  (node:490) [DEP0040] DeprecationWarning: The `punycode` module is deprecated. Please use a userland alternative instead.
  (Use `node --trace-deprecation ...` to show where the warning was created)
  [1/4] Resolving packages...
  [2/4] Fetching packages...
  [] 0/1573[] 5/1573[] 11/1573[] 18/1573[] 26/1573[] 34/1573[] 41/1573[] 45/1573[] 53/1573[] 61/1573[] 68/1573[] 75/1573[] 84/1573[] 90/1573[] 97/1573[] 104/1573[] 111/1573[] 116/1573[] 122/1573[] 128/1573[] 135/1573[] 140/1573[] 147/1573[] 153/1573[] 160/1573[] 167/1573[] 176/1573[] 185/1573[] 192/1573[] 197/1573[] 198/1573[] 202/1573[] 206/1573[] 211/1573[] 216/1573[] 223/1573[] 229/1573[] 232/1573[] 234/1573[] 238/1573[] 241/1573[] 243/1573[] 246/1573[] 248/1573[] 250/1573[] 255/1573[] 260/1573[] 265/1573[] 269/1573[] 276/1573[] 284/1573[] 286/1573[] 287/1573[] 290/1573[] 292/1573[] 295/1573[] 298/1573[] 305/1573[] 311/1573[] 316/1573[] 325/1573[] 331/1573[] 336/1573[] 342/1573[] 346/1573[] 349/1573[] 353/1573[] 359/1573[] 364/1573[] 368/1573[] 372/1573[] 378/1573[] 385/1573[] 391/1573[] 396/1573[] 402/1573[] 409/1573[] 414/1573[] 420/1573[] 427/1573[] 432/1573[] 437/1573[] 441/1573[] 443/1573[] 444/1573[] 445/1573[] 448/1573[] 451/1573[] 454/1573[] 457/1573[] 462/1573[] 467/1573[] 470/1573[] 473/1573[] 477/1573[] 481/1573[] 485/1573[] 490/1573[] 496/1573[] 503/1573[] 512/1573[] 519/1573[] 524/1573[] 526/1573[] 529/1573[] 531/1573[] 535/1573[] 539/1573[] 545/1573[] 548/1573[] 550/1573[] 553/1573[] 560/1573[] 567/1573[] 574/1573[] 580/1573[] 591/1573[] 599/1573[] 607/1573[] 616/1573[] 620/1573[] 624/1573[] 627/1573[] 630/1573[] 634/1573[] 641/1573[] 650/1573[] 658/1573[] 668/1573[] 676/1573[] 684/1573[] 692/1573error Error: https://registry.yarnpkg.com/@babel/plugin-syntax-logical-assignment-operators/-/plugin-syntax-logical-assignment-operators-7.10.4.tgz: Request failed ""500 Internal Server Error""
      at ResponseError.ExtendableBuiltin (/opt/airflow/files/home/.cache/pre-commit/repocv9ljx22/node_env-22.2.0/lib/node_modules/yarn/lib/cli.js:696:66)
      at new ResponseError (/opt/airflow/files/home/.cache/pre-commit/repocv9ljx22/node_env-22.2.0/lib/node_modules/yarn/lib/cli.js:802:124)
      at Request.<anonymous> (/opt/airflow/files/home/.cache/pre-commit/repocv9ljx22/node_env-22.2.0/lib/node_modules/yarn/lib/cli.js:66218:16)
      at Request.emit (node:events:520:28)
      at module.exports.Request.onRequestResponse (/opt/airflow/files/home/.cache/pre-commit/repocv9ljx22/node_env-22.2.0/lib/node_modules/yarn/lib/cli.js:141751:10)
      at ClientRequest.emit (node:events:520:28)
      at HTTPParser.parserOnIncomingClient (node:_http_client:700:27)
      at HTTPParser.parserOnHeadersComplete (node:_http_common:119:17)
      at TLSSocket.socketOnData (node:_http_client:542:22)
      at TLSSocket.emit (node:events:520:28)
  info Visit https://yarnpkg.com/en/docs/cli/install for documentation about this command.
  Traceback (most recent call last):
    File ""/opt/airflow/./scripts/ci/pre_commit/compile_www_assets.py"", line 71, in <module>
      subprocess.check_call([""yarn"", ""install"", ""--frozen-lockfile""], cwd=os.fspath(www_directory))
    File ""/usr/local/lib/python3.9/subprocess.py"", line 373, in check_call
      raise CalledProcessError(retcode, cmd)
  subprocess.CalledProcessError: Command '['yarn', 'install', '--frozen-lockfile']' returned non-zero exit status 1.
  
  Traceback (most recent call last):
    File ""/usr/local/lib/python3.9/runpy.py"", line 197, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/usr/local/lib/python3.9/runpy.py"", line 87, in _run_code
      exec(code, run_globals)
    File ""/opt/airflow/files/home/.local/share/hatch/env/virtual/apache-airflow-build/lib/python3.9/site-packages/hatchling/__main__.py"", line 6, in <module>
      sys.exit(hatchling())
    File ""/opt/airflow/files/home/.local/share/hatch/env/virtual/apache-airflow-build/lib/python3.9/site-packages/hatchling/cli/__init__.py"", line 26, in hatchling
      command(**kwargs)
    File ""/opt/airflow/files/home/.local/share/hatch/env/virtual/apache-airflow-build/lib/python3.9/site-packages/hatchling/cli/build/__init__.py"", line 82, in build_impl
      for artifact in builder.build(
    File ""/opt/airflow/files/home/.local/share/hatch/env/virtual/apache-airflow-build/lib/python3.9/site-packages/hatchling/builders/plugin/interface.py"", line 155, in build
      artifact = version_api[version](directory, **build_data)
    File ""/opt/airflow/hatch_build.py"", line 613, in build_standard
      run(cmd, cwd=work_dir.as_posix(), check=True, shell=True)
    File ""/usr/local/lib/python3.9/subprocess.py"", line 528, in run
      raise CalledProcessError(retcode, process.args,
  subprocess.CalledProcessError: Command '['pre-commit run --hook-stage manual compile-www-assets --all-files']' returned non-zero exit status 1.
  Error building Airflow packages
Error preparing Airflow package
```

We should - similarly to Python packages, implement caching strategy that should speed up the installation of npm packages on a clean CI runner, as well as local development instance - where previously installed packages could be stored in the cache and reused. This should increase both - speed of CI jobs an stability of them (and speed of local installation of npm packages).",potiuk,2024-10-18 16:20:55+00:00,['bugraoz93'],2024-12-01 12:37:06+00:00,2024-12-01 12:37:06+00:00,https://github.com/apache/airflow/issues/43167,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:CI', ""Airflow's tests and continious integration"")]","[{'comment_id': 2509669875, 'issue_id': 2597932494, 'author': 'bugraoz93', 'body': '@potiuk I closed the PR in favour of the discussion. :) Should we keep the issue or create it again when the issue arises again?', 'created_at': datetime.datetime(2024, 12, 1, 10, 9, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2509748559, 'issue_id': 2597932494, 'author': 'potiuk', 'body': 'Closing it for now as we do not see those issues happening often - we can always re-open if we see it happening again.', 'created_at': datetime.datetime(2024, 12, 1, 12, 37, 6, tzinfo=datetime.timezone.utc)}]","bugraoz93 (Assginee) on (2024-12-01 10:09:58 UTC): @potiuk I closed the PR in favour of the discussion. :) Should we keep the issue or create it again when the issue arises again?

potiuk (Issue Creator) on (2024-12-01 12:37:06 UTC): Closing it for now as we do not see those issues happening often - we can always re-open if we see it happening again.

"
2597767474,issue,closed,completed,AIP-84 | UI endpoint for config,"In the told UI, we passed a lot of variables via global variables in FAB templates. We should replace those with a `ui/config` endpoint, to exposed specifically what the UI needs to render such as:

Settings:
`state_color_mapping`

Config:
`audit_view_excluded_events`
`audit_view_included_events`
`warn_deployment_exposure`
`page_size`
`default_wrap`
`auto_refresh_interval`
`default_ui_timezone`
`hide_paused_dags_by_default`
`require_confirmation_dag_change`
`enable_swagger_ui`
`instance_name`
`instance_name_has_markup`
`show_trigger_form_if_no_params`
`test_connection`
`navbar_*` we may need to update this to support light/dark modes. but let's send it all for now
`IS_K8S` which collates a few values together:
  ```
  conf.get(""core"", ""EXECUTOR"") in {
      executor_constants.KUBERNETES_EXECUTOR,
      executor_constants.CELERY_KUBERNETES_EXECUTOR,
      executor_constants.LOCAL_KUBERNETES_EXECUTOR,
  }
  ```

Other config: 
`standalone_dag_processor` [This should be checked in the API, not the UI](https://github.com/apache/airflow/issues/44253)


We do need to migrate the config to the public rest api too in https://github.com/apache/airflow/issues/42745 but that can be toggled off. This UI endpoint always needs to return in order for the UI to render correctly.",bbovenzi,2024-10-18 14:57:14+00:00,['vatsrahul1001'],2024-11-25 15:58:37+00:00,2024-11-25 15:58:37+00:00,https://github.com/apache/airflow/issues/43166,"[('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2423777583, 'issue_id': 2597767474, 'author': 'tirkarthi', 'body': 'Adding a note to also update pagination to be of the same webserver page_size value in airflow.cfg which is hard-coded as 50 now.\r\n\r\nhttps://github.com/apache/airflow/blob/899dcbfcb2b0ca2a8eb50cfb807bf96c336a3552/airflow/ui/src/components/DataTable/useTableUrlState.ts#L25-L31', 'created_at': datetime.datetime(2024, 10, 19, 11, 40, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489039440, 'issue_id': 2597767474, 'author': 'bbovenzi', 'body': 'Actually we can generate `IS_K8S` via\n\n```\nIS_K8S_OR_K8SCELERY_EXECUTOR = conf.get(""core"", ""EXECUTOR"") in {\n    executor_constants.KUBERNETES_EXECUTOR,\n    executor_constants.CELERY_KUBERNETES_EXECUTOR,\n    executor_constants.LOCAL_KUBERNETES_EXECUTOR,\n}\n```\n\n\nI think we should deprecate `state_colors` and make that only user-configurable', 'created_at': datetime.datetime(2024, 11, 20, 16, 25, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2490308384, 'issue_id': 2597767474, 'author': 'vatsrahul1001', 'body': '@pierrejeambrun @bbovenzi \r\n\r\n1. Does the format below look okay to you for the `UI/config `endpoint?\r\n  \r\n ```\r\n{\r\n  ""settings"": [\r\n    {\r\n      ""state_color"": {\r\n        ""deferred"": ""mediumpurple"",\r\n        ""failed"": ""red"",\r\n        ""queued"": ""gray"",\r\n        ""removed"": ""lightgrey"",\r\n        ""restarting"": ""violet"",\r\n        ""running"": ""lime"",\r\n        ""scheduled"": ""tan"",\r\n        ""skipped"": ""hotpink"",\r\n        ""success"": ""green"",\r\n        ""up_for_reschedule"": ""turquoise"",\r\n        ""up_for_retry"": ""gold"",\r\n        ""upstream_failed"": ""orange""\r\n      }\r\n    },\r\n    {\r\n      ""is_k8s"": false\r\n    }\r\n  ],\r\n  ""configs"": {\r\n    ""navbar_color"": ""#fff"",\r\n    ""webserver_config"": {\r\n      ""access_denied_message"": ""Access is Denied"",\r\n      ""config_file"": ""/root/airflow/webserver_config.py"",\r\n      ""base_url"": ""http://localhost:8080"",\r\n      ""default_ui_timezone"": ""UTC"",\r\n      ""web_server_host"": ""0.0.0.0"",\r\n      ""web_server_port"": ""8080"",\r\n      ""web_server_ssl_cert"": """",\r\n      ""web_server_ssl_key"": """",\r\n      ""session_backend"": ""database"",\r\n      ""web_server_master_timeout"": ""120"",\r\n      ""web_server_worker_timeout"": ""120"",\r\n      ""worker_refresh_batch_size"": ""1"",\r\n      ""worker_refresh_interval"": ""6000"",\r\n      ""reload_on_plugin_change"": ""False"",\r\n      ""secret_key"": ""< hidden >"",\r\n      ""workers"": ""4"",\r\n      ""worker_class"": ""sync"",\r\n      ""access_logfile"": ""-"",\r\n      ""error_logfile"": ""-"",\r\n      ""access_logformat"": """",\r\n      ""expose_config"": ""True"",\r\n      ""expose_hostname"": ""False"",\r\n      ""expose_stacktrace"": ""False"",\r\n      ""dag_default_view"": ""grid"",\r\n      ""dag_orientation"": ""LR"",\r\n      ""grid_view_sorting_order"": ""topological"",\r\n      ""log_fetch_timeout_sec"": ""5"",\r\n      ""log_fetch_delay_sec"": ""2"",\r\n      ""log_auto_tailing_offset"": ""30"",\r\n      ""log_animation_speed"": ""1000"",\r\n      ""hide_paused_dags_by_default"": ""False"",\r\n      ""page_size"": ""100"",\r\n      ""navbar_color"": ""#fff"",\r\n      ""navbar_text_color"": ""#51504f"",\r\n      ""navbar_hover_color"": ""#eee"",\r\n      ""navbar_text_hover_color"": ""#51504f"",\r\n      ""navbar_logo_text_color"": ""#51504f"",\r\n      ""default_dag_run_display_number"": ""25"",\r\n      ""enable_proxy_fix"": ""False"",\r\n      ""proxy_fix_x_for"": ""1"",\r\n      ""proxy_fix_x_proto"": ""1"",\r\n      ""proxy_fix_x_host"": ""1"",\r\n      ""proxy_fix_x_port"": ""1"",\r\n      ""proxy_fix_x_prefix"": ""1"",\r\n      ""cookie_secure"": ""False"",\r\n      ""cookie_samesite"": ""Lax"",\r\n      ""default_wrap"": ""False"",\r\n      ""x_frame_enabled"": ""True"",\r\n      ""show_recent_stats_for_completed_runs"": ""True"",\r\n      ""session_lifetime_minutes"": ""43200"",\r\n      ""instance_name_has_markup"": ""False"",\r\n      ""auto_refresh_interval"": ""3"",\r\n      ""warn_deployment_exposure"": ""True"",\r\n      ""enable_swagger_ui"": ""True"",\r\n      ""run_internal_api"": ""False"",\r\n      ""caching_hash_method"": ""md5"",\r\n      ""show_trigger_form_if_no_params"": ""False"",\r\n      ""num_recent_configurations_for_trigger"": ""5"",\r\n      ""allowed_payload_size"": ""1.0"",\r\n      ""require_confirmation_dag_change"": ""False""\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n2. What should we display when `expose_config` is set to false? Should we return a 403, or is it acceptable to show the values in this case? As mentioned  above`This UI endpoint always needs to return in order for the UI to render correctly.` \r\n3. What webserver config values should we show in response?', 'created_at': datetime.datetime(2024, 11, 21, 7, 56, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2490591755, 'issue_id': 2597767474, 'author': 'pierrejeambrun', 'body': 'Those config are just for the UI to display properly. (For auth and maybe even non auth users).\r\n\r\nOnly very basic and display/UI specific things should be returned by this endpoint. We cannot return the entire configuration. just the attributes mentionned by Brent. (Those are safe to return).\r\n\r\n\r\nWhy is `navbar_color` outside of the `webserver_configuration` ?\r\n\r\nAlso I think we can get rid of the ""webserver_config"" key. In the context of the UI, there is no. ""sections"" (webserver, db,....) in the configuration. Just the config I would say.\r\n\r\nShould we split settings and config ?', 'created_at': datetime.datetime(2024, 11, 21, 9, 44, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2490610182, 'issue_id': 2597767474, 'author': 'vatsrahul1001', 'body': '@pierrejeambrun here @bbovenzi mentioned returning entire conf. webserver dict after checking which values we need to support. \r\n> probably the entire conf.webserver dict, which we should double check which values we still want to support.\r\n\r\nWe can do settings and config\r\n**settings**\r\n```\r\nstate_colors\r\nIS_K8S_OR_K8SCELERY_EXECUTOR\r\n```\r\n\r\n**config**\r\n```\r\npage_size\r\nauto_refresh_interval\r\ndefault_ui_timezone\r\nstandalone_dag_processor\r\nhide_paused_dags_by_default\r\nnavbar_color\r\ninstance_name\r\n```\r\nLet me know if we want them to be merged', 'created_at': datetime.datetime(2024, 11, 21, 9, 52, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2490904819, 'issue_id': 2597767474, 'author': 'vatsrahul1001', 'body': 'Below is the updated response. cc: @pierrejeambrun @bbovenzi \r\n```\r\n\r\n{\r\n  ""settings"": [\r\n    {\r\n      ""state_color"": {\r\n        ""deferred"": ""mediumpurple"",\r\n        ""failed"": ""red"",\r\n        ""queued"": ""gray"",\r\n        ""removed"": ""lightgrey"",\r\n        ""restarting"": ""violet"",\r\n        ""running"": ""lime"",\r\n        ""scheduled"": ""tan"",\r\n        ""skipped"": ""hotpink"",\r\n        ""success"": ""green"",\r\n        ""up_for_reschedule"": ""turquoise"",\r\n        ""up_for_retry"": ""gold"",\r\n        ""upstream_failed"": ""orange""\r\n      }\r\n    },\r\n    {\r\n      ""is_k8s"": false\r\n    }\r\n  ],\r\n  ""configs"": {\r\n    ""navbar_color"": ""#fff"",\r\n    ""page_size"": 100,\r\n    ""auto_refresh_interval"": 3,\r\n    ""default_ui_timezone"": ""UTC"",\r\n    ""hide_paused_dags_by_default"": false,\r\n    ""instance_name"": ""Airflow"",\r\n     ""standalone_dag_processor"": false\r\n  }\r\n}\r\n```', 'created_at': datetime.datetime(2024, 11, 21, 11, 46, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2491212505, 'issue_id': 2597767474, 'author': 'pierrejeambrun', 'body': ""> @pierrejeambrun here @bbovenzi mentioned returning entire conf. webserver dict after checking which values we need to support.\r\n\r\nI understand. That's true that most of the webserver config could be useful to the UI, it's just that I'm not sure if `everything` is safe to be exposed and should be exposed. Sticking to a 'on need' basis might be a safer approach at first.\r\n\r\nI have no strong opinion on that.\r\n\r\nThe last payload mentioned looks like something we can use. I'll let Brent confirm if this is enough for the front-end to start with."", 'created_at': datetime.datetime(2024, 11, 21, 13, 45, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2491487893, 'issue_id': 2597767474, 'author': 'bbovenzi', 'body': ""I just updated the main description changing some of the desired config values.\r\n\r\nAlso, let's make it all one flat dict and remove the distinction between `settings` and `config`"", 'created_at': datetime.datetime(2024, 11, 21, 15, 8, 40, tzinfo=datetime.timezone.utc)}]","tirkarthi on (2024-10-19 11:40:14 UTC): Adding a note to also update pagination to be of the same webserver page_size value in airflow.cfg which is hard-coded as 50 now.

https://github.com/apache/airflow/blob/899dcbfcb2b0ca2a8eb50cfb807bf96c336a3552/airflow/ui/src/components/DataTable/useTableUrlState.ts#L25-L31

bbovenzi (Issue Creator) on (2024-11-20 16:25:22 UTC): Actually we can generate `IS_K8S` via

```
IS_K8S_OR_K8SCELERY_EXECUTOR = conf.get(""core"", ""EXECUTOR"") in {
    executor_constants.KUBERNETES_EXECUTOR,
    executor_constants.CELERY_KUBERNETES_EXECUTOR,
    executor_constants.LOCAL_KUBERNETES_EXECUTOR,
}
```


I think we should deprecate `state_colors` and make that only user-configurable

vatsrahul1001 (Assginee) on (2024-11-21 07:56:05 UTC): @pierrejeambrun @bbovenzi 

1. Does the format below look okay to you for the `UI/config `endpoint?
  
 ```
{
  ""settings"": [
    {
      ""state_color"": {
        ""deferred"": ""mediumpurple"",
        ""failed"": ""red"",
        ""queued"": ""gray"",
        ""removed"": ""lightgrey"",
        ""restarting"": ""violet"",
        ""running"": ""lime"",
        ""scheduled"": ""tan"",
        ""skipped"": ""hotpink"",
        ""success"": ""green"",
        ""up_for_reschedule"": ""turquoise"",
        ""up_for_retry"": ""gold"",
        ""upstream_failed"": ""orange""
      }
    },
    {
      ""is_k8s"": false
    }
  ],
  ""configs"": {
    ""navbar_color"": ""#fff"",
    ""webserver_config"": {
      ""access_denied_message"": ""Access is Denied"",
      ""config_file"": ""/root/airflow/webserver_config.py"",
      ""base_url"": ""http://localhost:8080"",
      ""default_ui_timezone"": ""UTC"",
      ""web_server_host"": ""0.0.0.0"",
      ""web_server_port"": ""8080"",
      ""web_server_ssl_cert"": """",
      ""web_server_ssl_key"": """",
      ""session_backend"": ""database"",
      ""web_server_master_timeout"": ""120"",
      ""web_server_worker_timeout"": ""120"",
      ""worker_refresh_batch_size"": ""1"",
      ""worker_refresh_interval"": ""6000"",
      ""reload_on_plugin_change"": ""False"",
      ""secret_key"": ""< hidden >"",
      ""workers"": ""4"",
      ""worker_class"": ""sync"",
      ""access_logfile"": ""-"",
      ""error_logfile"": ""-"",
      ""access_logformat"": """",
      ""expose_config"": ""True"",
      ""expose_hostname"": ""False"",
      ""expose_stacktrace"": ""False"",
      ""dag_default_view"": ""grid"",
      ""dag_orientation"": ""LR"",
      ""grid_view_sorting_order"": ""topological"",
      ""log_fetch_timeout_sec"": ""5"",
      ""log_fetch_delay_sec"": ""2"",
      ""log_auto_tailing_offset"": ""30"",
      ""log_animation_speed"": ""1000"",
      ""hide_paused_dags_by_default"": ""False"",
      ""page_size"": ""100"",
      ""navbar_color"": ""#fff"",
      ""navbar_text_color"": ""#51504f"",
      ""navbar_hover_color"": ""#eee"",
      ""navbar_text_hover_color"": ""#51504f"",
      ""navbar_logo_text_color"": ""#51504f"",
      ""default_dag_run_display_number"": ""25"",
      ""enable_proxy_fix"": ""False"",
      ""proxy_fix_x_for"": ""1"",
      ""proxy_fix_x_proto"": ""1"",
      ""proxy_fix_x_host"": ""1"",
      ""proxy_fix_x_port"": ""1"",
      ""proxy_fix_x_prefix"": ""1"",
      ""cookie_secure"": ""False"",
      ""cookie_samesite"": ""Lax"",
      ""default_wrap"": ""False"",
      ""x_frame_enabled"": ""True"",
      ""show_recent_stats_for_completed_runs"": ""True"",
      ""session_lifetime_minutes"": ""43200"",
      ""instance_name_has_markup"": ""False"",
      ""auto_refresh_interval"": ""3"",
      ""warn_deployment_exposure"": ""True"",
      ""enable_swagger_ui"": ""True"",
      ""run_internal_api"": ""False"",
      ""caching_hash_method"": ""md5"",
      ""show_trigger_form_if_no_params"": ""False"",
      ""num_recent_configurations_for_trigger"": ""5"",
      ""allowed_payload_size"": ""1.0"",
      ""require_confirmation_dag_change"": ""False""
    }
  }
}
```

2. What should we display when `expose_config` is set to false? Should we return a 403, or is it acceptable to show the values in this case? As mentioned  above`This UI endpoint always needs to return in order for the UI to render correctly.` 
3. What webserver config values should we show in response?

pierrejeambrun on (2024-11-21 09:44:03 UTC): Those config are just for the UI to display properly. (For auth and maybe even non auth users).

Only very basic and display/UI specific things should be returned by this endpoint. We cannot return the entire configuration. just the attributes mentionned by Brent. (Those are safe to return).


Why is `navbar_color` outside of the `webserver_configuration` ?

Also I think we can get rid of the ""webserver_config"" key. In the context of the UI, there is no. ""sections"" (webserver, db,....) in the configuration. Just the config I would say.

Should we split settings and config ?

vatsrahul1001 (Assginee) on (2024-11-21 09:52:12 UTC): @pierrejeambrun here @bbovenzi mentioned returning entire conf. webserver dict after checking which values we need to support. 

We can do settings and config
**settings**
```
state_colors
IS_K8S_OR_K8SCELERY_EXECUTOR
```

**config**
```
page_size
auto_refresh_interval
default_ui_timezone
standalone_dag_processor
hide_paused_dags_by_default
navbar_color
instance_name
```
Let me know if we want them to be merged

vatsrahul1001 (Assginee) on (2024-11-21 11:46:49 UTC): Below is the updated response. cc: @pierrejeambrun @bbovenzi 
```

{
  ""settings"": [
    {
      ""state_color"": {
        ""deferred"": ""mediumpurple"",
        ""failed"": ""red"",
        ""queued"": ""gray"",
        ""removed"": ""lightgrey"",
        ""restarting"": ""violet"",
        ""running"": ""lime"",
        ""scheduled"": ""tan"",
        ""skipped"": ""hotpink"",
        ""success"": ""green"",
        ""up_for_reschedule"": ""turquoise"",
        ""up_for_retry"": ""gold"",
        ""upstream_failed"": ""orange""
      }
    },
    {
      ""is_k8s"": false
    }
  ],
  ""configs"": {
    ""navbar_color"": ""#fff"",
    ""page_size"": 100,
    ""auto_refresh_interval"": 3,
    ""default_ui_timezone"": ""UTC"",
    ""hide_paused_dags_by_default"": false,
    ""instance_name"": ""Airflow"",
     ""standalone_dag_processor"": false
  }
}
```

pierrejeambrun on (2024-11-21 13:45:46 UTC): I understand. That's true that most of the webserver config could be useful to the UI, it's just that I'm not sure if `everything` is safe to be exposed and should be exposed. Sticking to a 'on need' basis might be a safer approach at first.

I have no strong opinion on that.

The last payload mentioned looks like something we can use. I'll let Brent confirm if this is enough for the front-end to start with.

bbovenzi (Issue Creator) on (2024-11-21 15:08:40 UTC): I just updated the main description changing some of the desired config values.

Also, let's make it all one flat dict and remove the distinction between `settings` and `config`

"
2597556223,issue,closed,completed,Make Task Instance primary key be a UUID,"As part of [AIP-72](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-72+Task+Execution+Interface+aka+Task+SDK), we want to pass the Task Instance to the worker. Currently, the primary key of TI is a combination of `dag_id, task_id, run_id, map_index`.

https://github.com/apache/airflow/blob/b4269f33c7151e6d61e07333003ec1e219285b07/airflow/models/taskinstance.py#L1815-L1819

Instead of sending the entire key from the executor to worker via API-server, ideally the API server can just send over a TI UUID and the worker then uses it to fetch the correct TI to execute. 

We want to add a single column pk of a UUID, and should use UUID v7 (as it has better temporal sorting behaviours than the random v4). For the migration to update existing rows we can use v4 which most DBs have natively. 

The scope of this GitHub issue is to add UUIDs -- but not use it anywhere in the codebase yet until we need it on the Task Execution API server. We will keep the ""denormalized"" columns of dag_id and run_id for easier searching/querying.",kaxil,2024-10-18 13:41:49+00:00,['kaxil'],2024-10-28 12:40:37+00:00,2024-10-28 12:40:37+00:00,https://github.com/apache/airflow/issues/43161,"[('area:core', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2597435080,issue,closed,completed,TypeError: object of type 'AirflowSkipException' has no len(),"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I have 2 tasks that can possibily raise an AirflowSkipException, the first one is raised in a task directly within the DAG, the second task that possibly raises it is located within a TaskGroup.  If the first task raises the AirflowSkipException, everything goes smooth, but if the second task within the TaskGroup raises the AirflowSkipException, then I get an exception:

```
[2024-10-18, 12:32:35 UTC] {standard_task_runner.py:124} ERROR - Failed to execute job 7543209 for task ad_user_group_extraction.updated_group_ids (object of type 'AirflowSkipException' has no len(); 4538)
Traceback (most recent call last):
  File ""/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py"", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File ""/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py"", line 3149, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
  File ""/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py"", line 3173, in _execute_task
    return _execute_task(self, context, task_orig)
  File ""/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py"", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File ""/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py"", line 733, in _execute_callable
    return ExecutionCallableRunner(
  File ""/usr/local/lib/python3.9/site-packages/airflow/utils/operator_helpers.py"", line 252, in run
    return self.func(*args, **kwargs)
  File ""/usr/local/lib/python3.9/site-packages/airflow/models/baseoperator.py"", line 406, in wrapper
    return func(self, *args, **kwargs)
  File ""/usr/local/lib/python3.9/site-packages/airflow/providers/common/sql/operators/sql.py"", line 288, in execute
    output = hook.run(
  File ""/usr/local/lib/python3.9/site-packages/airflow/providers/common/sql/hooks/sql.py"", line 459, in run
    result = self._make_common_data_structure(handler(cur))
  File ""/usr/local/airflow/includes/projects/ad_user_extraction/functions.py"", line 85, in check_query_result
    raise AirflowSkipException(
airflow.exceptions.AirflowSkipException: No rows returned from the query, skipping the task.
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.9/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
  File ""/usr/local/lib/python3.9/site-packages/airflow/cli/cli_config.py"", line 49, in command
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.9/site-packages/airflow/utils/cli.py"", line 115, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File ""/usr/local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
  File ""/usr/local/lib/python3.9/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task
    return ti._run_raw_task(
  File ""/usr/local/lib/python3.9/site-packages/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File ""/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py"", line 2995, in _run_raw_task
    return _run_raw_task(
  File ""/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py"", line 301, in _run_raw_task
    ti.log.info(e)
  File ""/usr/local/lib/python3.9/logging/__init__.py"", line 1446, in info
    self._log(INFO, msg, args, **kwargs)
  File ""/usr/local/lib/python3.9/logging/__init__.py"", line 1589, in _log
    self.handle(record)
  File ""/usr/local/lib/python3.9/logging/__init__.py"", line 1598, in handle
    if (not self.disabled) and self.filter(record):
  File ""/usr/local/lib/python3.9/logging/__init__.py"", line 806, in filter
    result = f.filter(record)
  File ""/usr/local/airflow/config/log_config.py"", line 30, in filter
    if len(record.msg) > self.max_length:
TypeError: object of type 'AirflowSkipException' has no len()
```

### What you think should happen instead?

It seems the log_config module tries to calculate the length of a logging message but fails there as 'AirflowSkipException' has no len() attribute

- [ ] 

### How to reproduce

```
with DAG(
    ""user_relations_extraction"",
    default_args=DEFAULT_ARGS,
    schedule_interval=timedelta(minutes=5),
    dagrun_timeout=timedelta(hours=2),
    max_active_runs=1,
    max_active_tasks=3,
    catchup=False,
    tags=TAGS,
) as dag:

    updated_user_ids_task = SQLExecuteQueryOperator(
        task_id=""updated_user_ids"",
        conn_id=MSSQL_CONN_ID,
        sql=""""""
            SELECT TOP {{ var.value.get('users_extraction.chunk_size', 100) }} ID 
            FROM (
                SELECT DISTINCT ID, UPDATED_ON 
                FROM USERS 
                WHERE UPDATED_ON IS NOT NULL
            ) AS updated_users 
            ORDER BY UPDATED_ON ASC
            """""",
        handler=check_query_result,  # This method raises a AirflowSkipException when no records are returned
        show_return_value_in_logs=True,
        dag=dag,
    )

    user_registered_devices_task = MSGraphAsyncOperator.partial(
        task_id=""user_registered_devices"",
        conn_id=MSGRAPH_CONN_ID,
        url=""users/{userId}/registeredDevices"",
        result_processor=persist_response,
        event_handler=event_handler,
        do_xcom_push=False,
        retry_delay=60,
        trigger_rule=TriggerRule.ALL_SKIPPED,
        dag=dag,
    ).expand(path_parameters=updated_user_ids_task.output.map(lambda x: {""userId"": x[0]}))

    user_license_details_task = MSGraphAsyncOperator.partial(
        task_id=""user_license_details"",
        conn_id=MSGRAPH_CONN_ID,
        url=""users/{userId}/licenseDetails"",
        result_processor=persist_response,
        event_handler=event_handler,
        do_xcom_push=False,
        retry_delay=60,
        trigger_rule=TriggerRule.ALL_SKIPPED,
        dag=dag,
    ).expand(path_parameters=updated_user_ids_task.output.map(lambda x: {""userId"": x[0]}))

    user_group_members_task = MSGraphAsyncOperator.partial(
        task_id=""user_group_members"",
        conn_id=MSGRAPH_CONN_ID,
        url=""users/{userId}/getMemberGroups"",
        method=""POST"",
        data={""securityEnabledOnly"": True},
        result_processor=persist_response,
        event_handler=event_handler,
        do_xcom_push=False,
        retry_delay=60,
        trigger_rule=TriggerRule.ALL_SKIPPED,
        dag=dag,
    ).expand(path_parameters=updated_user_ids_task.output.map(lambda x: {""userId"": x[0]}))

    update_dirty_users_task = SQLExecuteQueryOperator(
        task_id=""update_dirty_users"",
        conn_id=MSSQL_CONN_ID,
        sql=""""""
            UPDATE USERS
            SET UPDATED_ON = NULL
            WHERE ID IN (
                {% set ids = ti.xcom_pull(task_ids='updated_user_ids') or ['0'] %}
                {% for id in ids %}
                    '{{ id[0] }}'{% if not loop.last %}, {% endif %}
                {% endfor %}
            )
            """""",
        show_return_value_in_logs=True,
        trigger_rule=TriggerRule.ALL_DONE,
        dag=dag,
    )

    with TaskGroup(""ad_user_group_extraction"") as groups_members_task:
        updated_group_ids_task = SQLExecuteQueryOperator(
            task_id=""updated_group_ids"",
            conn_id=""odbc_ms1744_sa_o365_dev"",
            sql=""""""
                SELECT TOP {{ var.value.get('users_extraction.chunk_size', 100) }} ID 
                FROM (
                    SELECT DISTINCT ID, UPDATED_ON 
                    FROM GROUPS 
                    WHERE UPDATED_ON IS NOT NULL
                ) AS updated_groups 
                ORDER BY UPDATED_ON ASC
                """""",
            handler=check_query_result,
            show_return_value_in_logs=True,
            trigger_rule=TriggerRule.ALL_DONE,
            dag=dag,
        )
```

### Operating System

RedHat

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other 3rd-party Helm chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",dabla,2024-10-18 12:47:58+00:00,[],2024-10-18 12:56:21+00:00,2024-10-18 12:56:21+00:00,https://github.com/apache/airflow/issues/43156,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:TaskGroup', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2596933782,issue,open,,Metastore Variables are not recognized by scheduler,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

We are using custom plugin with listener on_dag_run_running. That is being used within scheduler. It should fetch the variable defined by either Airflow UI or Airflow CLI, but fails to do it. 
`
KeyError: 'Variable monitoring_api_key does not exist'
`
I've been debugging this a bit and checked how airflow.models.variable.Variable picks it up.
So I've used ensure_secrets_loaded and iterated through secret backends, only without try/except block.
`
for secrets_backend in ensure_secrets_loaded():
    var_val = secrets_backend.get_variable(key=key)
    print(var_val)
`

Everything was fine until it reached Metastore and throw error
`
[2024-10-14T14:30:15.578+0200] {variable.py:357} ERROR - Unable to retrieve variable from secrets backend (MetastoreBackend). Checking subsequent secrets backend.
Traceback (most recent call last):
  File ""/data01/airflow/.pyenv/versions/3.12.6/lib/python3.12/site-packages/airflow/models/variable.py"", line 353, in get_variable_from_secrets
    var_val = secrets_backend.get_variable(key=key)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/data01/airflow/.pyenv/versions/3.12.6/lib/python3.12/site-packages/airflow/utils/session.py"", line 96, in wrapper
    with create_session() as session:
  File ""/data01/airflow/.pyenv/versions/3.12.6/lib/python3.12/contextlib.py"", line 144, in __exit__
    next(self.gen)
  File ""/data01/airflow/.pyenv/versions/3.12.6/lib/python3.12/site-packages/airflow/utils/session.py"", line 57, in create_session
    session.commit()
  File ""/data01/airflow/.pyenv/versions/3.12.6/lib/python3.12/site-packages/sqlalchemy/orm/session.py"", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File ""/data01/airflow/.pyenv/versions/3.12.6/lib/python3.12/site-packages/sqlalchemy/orm/session.py"", line 832, in commit
    self._prepare_impl()
  File ""/data01/airflow/.pyenv/versions/3.12.6/lib/python3.12/site-packages/sqlalchemy/orm/session.py"", line 800, in _prepare_impl
    self.session.dispatch.before_commit(self.session)
  File ""/data01/airflow/.pyenv/versions/3.12.6/lib/python3.12/site-packages/sqlalchemy/event/attr.py"", line 346, in __call__
    fn(*args, **kw)
  File ""/data01/airflow/.pyenv/versions/3.12.6/lib/python3.12/site-packages/airflow/utils/sqlalchemy.py"", line 424, in _validate_commit
    raise RuntimeError(""UNEXPECTED COMMIT - THIS WILL BREAK HA LOCKS!"")
RuntimeError: UNEXPECTED COMMIT - THIS WILL BREAK HA LOCKS!
`

If variable is used from envrionment or Hashicorp vault, then everything is fine. Only issue is if the variable exists only in Metastore.


### What you think should happen instead?

Scheduler should be able to pickup variables from Metastore as well. 

### How to reproduce

Set variable  with Airflow UI or Airflow CLI. Do not set variable into environment or secrets backend. 
Try to use variable within scheduler - eg. use event listener on_dag_run_running and use
`
import logging
from airflow.models import Variable
from datetime import datetime
from airflow.listeners import hookimpl
from airflow.plugins_manager import AirflowPlugin

class Plg(AirflowPlugin):
    class Listener:
        @hookimpl
        def on_dag_run_running(self, dag_run, msg: str):
            """"""
            This method is called when dag run state changes to RUNNING.
            """"""
            start_date = datetime.utcnow()
            state = dag_run.get_state()
            var_val = Variable.get(""your_variable"")
            logging.info(f""LSNR Dag running, status:{start_date} state:{state}, variable {var_val}"")
 
`

### Operating System

Red Hat Enterprise Linux 8.10 (Ootpa)

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.28.0
apache-airflow-providers-celery==3.8.1
apache-airflow-providers-common-compat==1.2.0
apache-airflow-providers-common-io==1.4.0
apache-airflow-providers-common-sql==1.16.0
apache-airflow-providers-dbt-cloud==3.10.0
apache-airflow-providers-fab==1.3.0
apache-airflow-providers-ftp==3.11.0
apache-airflow-providers-google==10.22.0
apache-airflow-providers-hashicorp==3.8.0
apache-airflow-providers-http==4.13.0
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-jdbc==4.5.0
apache-airflow-providers-microsoft-mssql==3.9.0
apache-airflow-providers-mysql==5.7.0
apache-airflow-providers-odbc==4.7.0
apache-airflow-providers-oracle==3.11.0
apache-airflow-providers-postgres==5.12.0
apache-airflow-providers-salesforce==5.8.0
apache-airflow-providers-sftp==4.11.0
apache-airflow-providers-slack==8.9.0
apache-airflow-providers-smtp==1.8.0
apache-airflow-providers-snowflake==5.7.0
apache-airflow-providers-sqlite==3.9.0
apache-airflow-providers-ssh==3.13.1


### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",BretaGlac,2024-10-18 09:08:55+00:00,[],2024-10-23 23:59:48+00:00,,https://github.com/apache/airflow/issues/43151,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('area:MetaDB', 'Meta Database related issues.'), ('area:core', '')]","[{'comment_id': 2433825491, 'issue_id': 2596933782, 'author': 'potiuk', 'body': ""Yes. It's because this listener is called inside scheduler commit. It's not very likely to be fixed soon in Airlfow 2 - in Airflow 3 we are rewriting how listeners work and it might get fixed ten."", 'created_at': datetime.datetime(2024, 10, 23, 23, 59, 47, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-10-23 23:59:47 UTC): Yes. It's because this listener is called inside scheduler commit. It's not very likely to be fixed soon in Airlfow 2 - in Airflow 3 we are rewriting how listeners work and it might get fixed ten.

"
2596917642,issue,closed,completed,Airflow worker start failed if password contains question mark (?) in AIRFLOW__DATABASE__SQL_ALCHEMY_CONN url ,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.3.4

### What happened?

On one of the customer deployment, Airflow worker started failing to start because password used to setup AIRFLOW__DATABASE__SQL_ALCHEMY_CONN has a question mark (eg - odLKJH!i8Udg?NL_ )

```
Traceback (most recent call last):[0m
  File ""/usr/local/lib/python3.10/dist-packages/celery/worker/worker.py"", line 203, in start[0m
    self.blueprint.start(self)[0m
  File ""/usr/local/lib/python3.10/dist-packages/celery/bootsteps.py"", line 112, in start[0m
    self.on_start()[0m
  File ""/usr/local/lib/python3.10/dist-packages/celery/apps/worker.py"", line 136, in on_start[0m
    self.emit_banner()[0m
  File ""/usr/local/lib/python3.10/dist-packages/celery/apps/worker.py"", line 170, in emit_banner[0m
    ' \n', self.startup_info(artlines=not use_image))),[0m
  File ""/usr/local/lib/python3.10/dist-packages/celery/apps/worker.py"", line 232, in startup_info[0m
    results=self.app.backend.as_uri(),[0m
  File ""/usr/local/lib/python3.10/dist-packages/celery/backends/base.py"", line 151, in as_uri[0m
    url = maybe_sanitize_url(self.url or '')[0m
  File ""/usr/local/lib/python3.10/dist-packages/kombu/utils/url.py"", line 112, in maybe_sanitize_url[0m
    return sanitize_url(url, mask)[0m
  File ""/usr/local/lib/python3.10/dist-packages/kombu/utils/url.py"", line 105, in sanitize_url[0m
    return as_url(*_parse_url(url), sanitize=True, mask=mask)[0m
  File ""/usr/local/lib/python3.10/dist-packages/kombu/utils/url.py"", line 70, in url_to_parts[0m
    parts.port,[0m
  File ""/usr/lib/python3.10/urllib/parse.py"", line 185, in port[0m
    raise ValueError(f""Port could not be cast to integer value as {port!r}"")[0m
ValueError: Port could not be cast to integer value as 'odLKJH!i8Udg'[0m
[2024-10-15 13:29:15 +0000] [56571] [INFO] Handling signal: term[0m
[2024-10-15 13:29:15 +0000] [56572] [INFO] Worker exiting (pid: 56572)[0m
[2024-10-15 13:29:15 +0000] [56573] [INFO] Worker exiting (pid: 56573)[0m
[2024-10-15 13:29:15 +0000] [56571] [INFO] Shutting down: Master[0m
```


### What you think should happen instead?

_No response_

### How to reproduce

Use password with question mark as special character in AIRFLOW__DATABASE__SQL_ALCHEMY_CONN db url 

### Operating System

Ubuntu 20.04.6 LTS

### Versions of Apache Airflow Providers

2.3.4

### Deployment

Other

### Deployment details

Kubernets

### Anything else?

NA

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",thedevd,2024-10-18 09:02:16+00:00,[],2024-10-18 13:08:23+00:00,2024-10-18 13:08:23+00:00,https://github.com/apache/airflow/issues/43150,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2421894906, 'issue_id': 2596917642, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 18, 9, 2, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2422440448, 'issue_id': 2596917642, 'author': 'thedevd', 'body': 'Sorry, closing this as this turned out to be because of SQL alchemy where to use such type of password we have to encode the password first \r\n\r\nhttps://docs.sqlalchemy.org/en/20/core/engines.html#escaping-special-characters-such-as-signs-in-passwords', 'created_at': datetime.datetime(2024, 10, 18, 13, 8, 23, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-18 09:02:19 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

thedevd (Issue Creator) on (2024-10-18 13:08:23 UTC): Sorry, closing this as this turned out to be because of SQL alchemy where to use such type of password we have to encode the password first 

https://docs.sqlalchemy.org/en/20/core/engines.html#escaping-special-characters-such-as-signs-in-passwords

"
2596818278,issue,closed,not_planned,Inconsistent display of timezone,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

<img width=""1028"" alt=""20241018160503"" src=""https://github.com/user-attachments/assets/915f3495-daa4-441d-bd64-4f61d6acc910"">


### What you think should happen instead?

The Timezone displayed in the details area on the webpage should be consistent with the<meta>dag_timezone in the<head>of the HTML source

### How to reproduce

Set the default_timezone in the airflows. cfg file to Asia/Shanghai, start airflow, open the homepage, and click on example_branch_operator

### Operating System

Ubuntu 22.04

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",hydeboy,2024-10-18 08:22:38+00:00,[],2024-11-24 00:17:26+00:00,2024-11-24 00:17:25+00:00,https://github.com/apache/airflow/issues/43149,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('pending-response', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2421808932, 'issue_id': 2596818278, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 18, 8, 22, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453113479, 'issue_id': 2596818278, 'author': 'jscheffl', 'body': 'See also https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/timezone.html#default-time-zone\r\n\r\nThis is the internal time zone for the scheduling and processing of jobs. The UI display is separate of this and in your case it seems you have selected UTC in the browser top-right. So I see this is rather a feature (DAG is configured to run in Timezone A and your Browser is in Timezone B).', 'created_at': datetime.datetime(2024, 11, 2, 20, 6, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2480862612, 'issue_id': 2596818278, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 11, 17, 0, 17, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2495713861, 'issue_id': 2596818278, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 11, 24, 0, 17, 25, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-18 08:22:43 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jscheffl on (2024-11-02 20:06:24 UTC): See also https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/timezone.html#default-time-zone

This is the internal time zone for the scheduling and processing of jobs. The UI display is separate of this and in your case it seems you have selected UTC in the browser top-right. So I see this is rather a feature (DAG is configured to run in Timezone A and your Browser is in Timezone B).

github-actions[bot] on (2024-11-17 00:17:04 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-11-24 00:17:25 UTC): This issue has been closed because it has not received response from the issue author.

"
2596770049,issue,closed,completed,Fix intermittent failure in React UI Time test ,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

None

### What happened?

Test fails intermittently when there's a second's difference between the variables getting initialized. GitHub run for reference: https://github.com/apache/airflow/actions/runs/11399466779/job/31718337427?pr=43084#step:9:48

**Test failure:**
```bash
❯ src/components/Time.test.tsx  (2 tests | 1 failed) 169ms
   × Test Time and TimezoneProvider > Displays a set timezone, includes UTC date in title
     → expected '2024-10-18, 07:34:18 UTC' to deeply equal '2024-10-18, 07:34:19 UTC'
 ✓ src/utils/RouterWrapper.test.tsx  (2 tests) 19ms
 ✓ src/utils/ChakraWrapper.test.tsx  (2 tests) 84ms

⎯⎯⎯⎯⎯⎯⎯ Failed Tests 1 ⎯⎯⎯⎯⎯⎯⎯

 FAIL  src/components/Time.test.tsx > Test Time and TimezoneProvider > Displays a set timezone, includes UTC date in title
AssertionError: expected '2024-10-18, 07:34:18 UTC' to deeply equal '2024-10-18, 07:34:19 UTC'

Expected: ""2024-10-18, 07:34:19 UTC""
Received: ""2024-10-18, 07:34:18 UTC""

 ❯ src/components/Time.test.tsx:60:29
     [58](https://github.com/apache/airflow/actions/runs/11399466779/job/31718337427?pr=43084#step:9:59)| 
     59|     expect(samoaTime).toBeDefined();
     60|     expect(samoaTime.title).toEqual(
       |                             ^
     [61](https://github.com/apache/airflow/actions/runs/11399466779/job/31718337427?pr=43084#step:9:62)|       dayjs().tz(""UTC"").format(defaultFormatWithTZ),
     [62](https://github.com/apache/airflow/actions/runs/11399466779/job/31718337427?pr=43084#step:9:63)|     );

⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯[1/1]⎯

 Test Files  1 failed | 7 passed (8)
      Tests  1 failed | 18 passed (19)
   Start at  07:34:15
   Duration  4.17s (transform 414ms, setup 8[71](https://github.com/apache/airflow/actions/runs/11399466779/job/31718337427?pr=43084#step:9:72)ms, collect 5.22s, tests 810ms, environment 2.58s, prepare 829ms)
```

### What you think should happen instead?

The test should pass.

### How to reproduce

Intermittent failure, so difficult to reproduce.

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-10-18 08:00:17+00:00,[],2024-10-18 12:38:33+00:00,2024-10-18 12:38:33+00:00,https://github.com/apache/airflow/issues/43146,"[('kind:bug', 'This is a clearly a bug'), ('area:CI', ""Airflow's tests and continious integration""), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2596453314,issue,closed,completed,DAG-level permissions for `/dags/{dag_id}/clearTaskInstances` is incorrect,"### What do you see as an issue?

The documentation on [DAG level permissions](https://airflow.apache.org/docs/apache-airflow-providers-fab/stable/auth-manager/access-control.html#dag-level-permissions) states that the permissions `DAGs.can_edit`, `DAG Runs.can_read`, `Task Instances.can_edit` are required for the endpoint `/dags/{dag_id}/clearTaskInstances`.



### Solving the problem

The permissions for the endpoint `/dags/{dag_id}/clearTaskInstances` are `DAGs.can_edit`, `DAG Runs.can_edit`, `Task Instances.can_edit`. The method is also ""PUT"" not ""POST"".

In Airflow 2.8.0, the [clear endpoint](https://github.com/apache/airflow/blob/2.9.3/airflow/api_connexion/endpoints/task_instance_endpoint.py#L444) updated (see [PR](https://github.com/apache/airflow/pull/34317)) to use the requires_access_dag function which leverages [is_authorized_dag](https://github.com/apache/airflow/blob/providers-fab/1.2.2/airflow/providers/fab/auth_manager/fab_auth_manager.py#L203). More importantly, the SAME resource method is used to check against each resource entity. The clear method passes the ""PUT"" resource method which [translates](https://github.com/apache/airflow/blob/2.9.3/airflow/auth/managers/utils/fab.py#L34-L40) to ""ACTION_CAN_EDIT"".
 
The logic translates to requiring the following permissions.
(permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG)
(permissions.ACTION_CAN_EDIT, permissions.RESOURCE_DAG_RUN)
(permissions.ACTION_CAN_EDIT, permissions.RESOURCE_TASK_INSTANCE)

### Anything else

I did not check the other DAG level permissions. I recommend verifying each entry.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",wolfier,2024-10-18 05:26:47+00:00,['SuccessMoses'],2024-10-31 08:47:20+00:00,2024-10-31 08:47:20+00:00,https://github.com/apache/airflow/issues/43140,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('kind:documentation', ''), ('area:auth', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2431304177, 'issue_id': 2596453314, 'author': 'potiuk', 'body': 'Yes. Would be great if someone takes it and reviews all the permissions and correct if there any problems. If you don\'t want to contribute it @wolfier, I marked it as a good first issue. It\'s not a high priority though as we are completely rewriting it for Airflow 3 and I believe the fastapi implementation there will have a consistenty between the docs and permissions pretty much guaranteed (@pierrejeambrun ?). \r\n\r\nOther than that, yes, this is not consistent with docs and unless someone would like to take it on, unlikely to be addressed in 2.*, we will leave it as ""good first issue"" to review and fix all inconsistencies.', 'created_at': datetime.datetime(2024, 10, 23, 8, 32, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2437978666, 'issue_id': 2596453314, 'author': 'pierrejeambrun', 'body': 'Documentation and code method will be automatically in sync. I will not be possible to have the documentation stating ""PUT"", and the code implementing a ""POST"" anymore.\r\n\r\nPermissions are for now just custom airflow code and FastAPI automatic documentation does not know how to handle that. If permissions need to be exposed in the swagger spec, we can write some code to automatically fill that in, but this is not supported out of the box. That can actually be a nice improvement.', 'created_at': datetime.datetime(2024, 10, 25, 14, 30, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2438665306, 'issue_id': 2596453314, 'author': 'potiuk', 'body': ""> Permissions are for now just custom airflow code and FastAPI automatic documentation does not know how to handle that. If permissions need to be exposed in the swagger spec, we can write some code to automatically fill that in, but this is not supported out of the box. That can actually be a nice improvement.\r\n\r\nLet's capture it as an issue then :) ?"", 'created_at': datetime.datetime(2024, 10, 25, 19, 42, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2440895697, 'issue_id': 2596453314, 'author': 'pierrejeambrun', 'body': 'Issue created here https://github.com/apache/airflow/issues/43430', 'created_at': datetime.datetime(2024, 10, 28, 8, 46, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443503661, 'issue_id': 2596453314, 'author': 'SuccessMoses', 'body': '@pierrejeambrun I want to work on #43430 as my first issue', 'created_at': datetime.datetime(2024, 10, 29, 8, 1, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443876248, 'issue_id': 2596453314, 'author': 'potiuk', 'body': 'Assigned you @SuccessMoses', 'created_at': datetime.datetime(2024, 10, 29, 10, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445254427, 'issue_id': 2596453314, 'author': 'SuccessMoses', 'body': 'what is the path to the source file for the documentation? I am not yet familiar with the code base', 'created_at': datetime.datetime(2024, 10, 29, 20, 21, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445344876, 'issue_id': 2596453314, 'author': 'potiuk', 'body': '> what is the path to the source file for the documentation? I am not yet familiar with the code base\r\n\r\nClick ""suggest a change on this page"" on the Page you want to edit and PR will be opened for you automatically in the right file.', 'created_at': datetime.datetime(2024, 10, 29, 21, 15, 20, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-10-23 08:32:57 UTC): Yes. Would be great if someone takes it and reviews all the permissions and correct if there any problems. If you don't want to contribute it @wolfier, I marked it as a good first issue. It's not a high priority though as we are completely rewriting it for Airflow 3 and I believe the fastapi implementation there will have a consistenty between the docs and permissions pretty much guaranteed (@pierrejeambrun ?). 

Other than that, yes, this is not consistent with docs and unless someone would like to take it on, unlikely to be addressed in 2.*, we will leave it as ""good first issue"" to review and fix all inconsistencies.

pierrejeambrun on (2024-10-25 14:30:35 UTC): Documentation and code method will be automatically in sync. I will not be possible to have the documentation stating ""PUT"", and the code implementing a ""POST"" anymore.

Permissions are for now just custom airflow code and FastAPI automatic documentation does not know how to handle that. If permissions need to be exposed in the swagger spec, we can write some code to automatically fill that in, but this is not supported out of the box. That can actually be a nice improvement.

potiuk on (2024-10-25 19:42:18 UTC): Let's capture it as an issue then :) ?

pierrejeambrun on (2024-10-28 08:46:30 UTC): Issue created here https://github.com/apache/airflow/issues/43430

SuccessMoses (Assginee) on (2024-10-29 08:01:39 UTC): @pierrejeambrun I want to work on #43430 as my first issue

potiuk on (2024-10-29 10:50:00 UTC): Assigned you @SuccessMoses

SuccessMoses (Assginee) on (2024-10-29 20:21:39 UTC): what is the path to the source file for the documentation? I am not yet familiar with the code base

potiuk on (2024-10-29 21:15:20 UTC): Click ""suggest a change on this page"" on the Page you want to edit and PR will be opened for you automatically in the right file.

"
2595527119,issue,closed,completed,Inconsistent usage of awslogs_stream_prefix,"### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.29.0

### Apache Airflow version

2.9.2

### Operating System

Amazon Linux 2023.5.20240805

### Deployment

Amazon (AWS) MWAA

### Deployment details

Using a Python 3.11 environment with botocore==1.34.106 and apache-airflow-providers-amazon==8.29.0.

### What happened

The code for EcsRunTaskOperator looks for log streams by using the pattern  `prefix-name/ecs-task-id`:
```python
    def _get_logs_stream_name(self) -> str:
        return f""{self.awslogs_stream_prefix}/{self._get_ecs_task_id(self.arn)}""
```

In the [AWS ECS docs for LogConfiguration](https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_LogConfiguration.html), log streams are created using the pattern `prefix-name/container-name/ecs-task-id`.

### What you think should happen instead

EcsRunTaskOperator should follow the same log stream naming convention as specified in the AWS ECS docs.

Because this isn't the case, you need to specify a stream prefix using a different pattern when registering your task vs. when running your task.

### How to reproduce

1. Register a task definition with a `containerDefinition` containing at least the following:
```json
  ""name"": ""my-container-name"",
  ""logConfiguration"": {
      ""logDriver"": ""awslogs"",
      ""options"": {
          ""awslogs-group"": ""my-log-group"",
          ""awslogs-region"": ""us-east-1"",
          ""awslogs-create-group"": ""true"",
          ""awslogs-stream-prefix"": ""my-container-prefix"",
      },
  },
```
The logs in your ECS task will be uploaded to CloudWatch under the log group `my-log-group` and the prefix `my-container-prefix/my-container-name/task-id`.

2. Run the task using the EcsRunTaskOperator with the same `awslogs-stream-prefix`.
3. View the logs in Airflow-- they will fail to fetch the task logs from CloudWatch because they are looking for the log group `my-log-group` and prefix `my-container-prefix/task-id`.

### Anything else

The only workaround for this issue is to specify the task definition's `awslogs-stream-prefix` with `my-prefix-name` and the EcsRunTaskOperator's `awslogs_stream_prefix` with `my-prefix-name/my-container-name`.

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",pyrr,2024-10-17 19:09:19+00:00,['pyrr'],2024-11-05 19:30:21+00:00,2024-11-05 19:30:21+00:00,https://github.com/apache/airflow/issues/43130,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('good first issue', '')]",[],
2595333157,issue,closed,completed,Add TimeoutError to be a retryable  error in databricks provider,"### Description

@lucafurrer reported that their Databricks jobs fail with `asyncio.exceptions.TimeoutError`. Below is their stack trace. I believe this should be considered a retryable error. However, I believe that letting users define their method which can confirm whether to retry or not will be a good and generic solution. I've created another issue for this https://github.com/apache/airflow/issues/43127


```
[2024-10-15, 20:07:05 CEST] {baseoperator.py:1598} ERROR - Trigger failed:
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/jobs/triggerer_job_runner.py"", line 529, in cleanup_finished_triggers
    result = details[""task""].result()
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/jobs/triggerer_job_runner.py"", line 601, in run_trigger
    async for event in trigger.run():
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/triggers/databricks.py"", line 86, in run
    run_state = await self.hook.a_get_run_state(self.run_id)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/hooks/databricks.py"", line 417, in a_get_run_state
    response = await self._a_do_api_call(GET_RUN_ENDPOINT, json)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/hooks/databricks_base.py"", line 651, in _a_do_api_call
    async for attempt in self._a_get_retry_object():
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/_asyncio.py"", line 71, in __anext__
    do = self.iter(retry_state=self._retry_state)
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 314, in iter
    return fut.result()
  File ""/usr/local/lib/python3.8/concurrent/futures/_base.py"", line 437, in result
    return self.__get_result()
  File ""/usr/local/lib/python3.8/concurrent/futures/_base.py"", line 389, in __get_result
    raise self._exception
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/hooks/databricks_base.py"", line 653, in _a_do_api_call
    async with request_func(
  File ""/home/airflow/.local/lib/python3.8/site-packages/aiohttp/client.py"", line 1194, in __aenter__
    self._resp = await self._coro
  File ""/home/airflow/.local/lib/python3.8/site-packages/aiohttp/client.py"", line 605, in _request
    await resp.start(conn)
  File ""/home/airflow/.local/lib/python3.8/site-packages/aiohttp/client_reqrep.py"", line 981, in start
    self._continue = None
  File ""/home/airflow/.local/lib/python3.8/site-packages/aiohttp/helpers.py"", line 735, in __exit__
    raise asyncio.TimeoutError from None
asyncio.exceptions.TimeoutError
[2024-10-15, 20:07:05 CEST] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py"", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py"", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py"", line 1599, in resume_execution
    raise TaskDeferralError(next_kwargs.get(""error"", ""Unknown""))
airflow.exceptions.TaskDeferralError: Trigger failure
```


### Use case/motivation

Original request from another related issue: https://github.com/apache/airflow/issues/43080#issuecomment-2418795335

### Related issues

https://github.com/apache/airflow/issues/43080

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rawwar,2024-10-17 17:32:49+00:00,['rawwar'],2024-10-18 07:20:06+00:00,2024-10-18 07:20:05+00:00,https://github.com/apache/airflow/issues/43128,"[('area:providers', ''), ('kind:feature', 'Feature Requests'), ('good first issue', ''), ('provider:databricks', '')]",[],
2595326900,issue,open,,Add support for user defined `_retryable_error` in databricks provider,"### Description

we can let users pass their own `_retryable_error` method, which will validate if retry is allowed or not.

As of now, databricks operators only accept `retry_limit` and `retry_delay`. Adding support for custom retry function and after func should be good enough to handle all custom requirements.

https://github.com/apache/airflow/blob/6b090b31038e6dd86f6efd49d81cf34b73d1e13e/providers/src/airflow/providers/databricks/hooks/databricks_base.py#L127

### Use case/motivation

It will allow user to set their own retry logics and also not require change to the provider everytime a user wants to retry for a particular type of exception.

### Related issues

https://github.com/apache/airflow/issues/43080 

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rawwar,2024-10-17 17:30:29+00:00,['rawwar'],2024-10-18 17:26:29+00:00,,https://github.com/apache/airflow/issues/43127,"[('area:providers', ''), ('kind:feature', 'Feature Requests'), ('provider:databricks', '')]","[{'comment_id': 2420119567, 'issue_id': 2595326900, 'author': 'rawwar', 'body': '@potiuk , @pankajkoti , requesting your comments on this feature.', 'created_at': datetime.datetime(2024, 10, 17, 17, 35, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2421316409, 'issue_id': 2595326900, 'author': 'pankajkoti', 'body': 'yeah, IMO another option is that we could accept a user supplied list of retryable errors & add those to our _retryable_error validation method. Probably we could also wait to work on this until there is a good demand for this? And users say how they would like it to be exposed as (how much control they would like)?', 'created_at': datetime.datetime(2024, 10, 18, 4, 33, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2421523565, 'issue_id': 2595326900, 'author': 'rawwar', 'body': ""> yeah, IMO another option is that we could accept a user supplied list of retryable errors & add those to our _retryable_error validation method. Probably we could also wait to work on this until there is a good demand for this? And users say how they would like it to be exposed as (how much control they would like)?\r\n\r\nI won't start working on this right away. But, if I see any users requesting this, I'll raise a PR"", 'created_at': datetime.datetime(2024, 10, 18, 6, 21, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2421531501, 'issue_id': 2595326900, 'author': 'lucafurrer', 'body': 'IMO I think it will be a simpler interface if we pass a list of errors to add instead of a function. In particular since rewriting your own function more or less implies to go check the current implementation to make that you have not forgot any special use case.', 'created_at': datetime.datetime(2024, 10, 18, 6, 28, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2422927719, 'issue_id': 2595326900, 'author': 'potiuk', 'body': 'Yes. list of errors look good and we implemented it elsewhere already (i am quite sure I saw it in other operators). Having a method to pass, but if you implement the operator in the way that it has a public ""should_retry(response)"" method, then you should be able to implement your custom operator deriving from the base one and rewrite only that method - which is the way the operators **should** be extended with code.', 'created_at': datetime.datetime(2024, 10, 18, 17, 26, 27, tzinfo=datetime.timezone.utc)}]","rawwar (Issue Creator) on (2024-10-17 17:35:48 UTC): @potiuk , @pankajkoti , requesting your comments on this feature.

pankajkoti on (2024-10-18 04:33:54 UTC): yeah, IMO another option is that we could accept a user supplied list of retryable errors & add those to our _retryable_error validation method. Probably we could also wait to work on this until there is a good demand for this? And users say how they would like it to be exposed as (how much control they would like)?

rawwar (Issue Creator) on (2024-10-18 06:21:54 UTC): I won't start working on this right away. But, if I see any users requesting this, I'll raise a PR

lucafurrer on (2024-10-18 06:28:10 UTC): IMO I think it will be a simpler interface if we pass a list of errors to add instead of a function. In particular since rewriting your own function more or less implies to go check the current implementation to make that you have not forgot any special use case.

potiuk on (2024-10-18 17:26:27 UTC): Yes. list of errors look good and we implemented it elsewhere already (i am quite sure I saw it in other operators). Having a method to pass, but if you implement the operator in the way that it has a public ""should_retry(response)"" method, then you should be able to implement your custom operator deriving from the base one and rewrite only that method - which is the way the operators **should** be extended with code.

"
2594915050,issue,closed,completed,[PostgreSQLConnection] `cursor` extra param missing in documentation,"### What do you see as an issue?

https://airflow.apache.org/docs/apache-airflow-providers-postgres/stable/connections/postgres.html#postgresql-connection

### Solving the problem

Add info about `cursor` (psycopg cursor_factory) param

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",taras0024,2024-10-17 14:29:04+00:00,['kaxil'],2024-10-18 00:18:51+00:00,2024-10-18 00:18:51+00:00,https://github.com/apache/airflow/issues/43120,"[('kind:bug', 'This is a clearly a bug'), ('kind:documentation', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:postgres', '')]","[{'comment_id': 2419713216, 'issue_id': 2594915050, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 17, 14, 29, 8, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-17 14:29:08 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2594733219,issue,closed,completed,"Airflow webUI ""This application is down for maintenance"" . for dags with huge number of tasks","### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.5.3

### What happened?

I have dag which dynamically generates close to 800 tasks. when i hit /tries?day=30 and /duration?days=30 webserver API. 

it just simply fails with error. 

I have tried the following things.
- Gunicorn Workers: Increasing the number of workers has not resolved the page loading issues, as the problem isn’t related to incoming request traffic.
- Timeout Settings: Adjusted both WORKER_TIMEOUT (3000) and master WORKER_TIMEOUT(3000) to higher values, but the page still fails to load.
- Worker Class: Switching to the gevent worker class did not improve performance, despite its ability to handle parallelism.
- CPU Limits: Increased the webserver’s CPU limits to 24 cores, yet one of the Gunicorn worker consistently maxes out CPU usage.


note: I have checked the webserver_error.log as well. no worker is getting killed abruptly.



<img width=""1328"" alt=""Screenshot 2024-10-16 at 9 21 40 PM"" src=""https://github.com/user-attachments/assets/128f627f-0f2a-473b-adda-2716bc5624aa"">


### What you think should happen instead?

_No response_

### How to reproduce


- Create a dag which generation dynamic tasks more than 800. and then hit /dag_id/tries?day=30 or /dag_id/duration?day=30 API. 





<img width=""1707"" alt=""Screenshot 2024-10-17 at 6 50 31 PM"" src=""https://github.com/user-attachments/assets/4316359c-bcb9-4227-933a-7bd1902559d9"">


### Operating System

Red Hat Enterprise Linux (9.4) - rhel

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other 3rd-party Helm chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",kaveti,2024-10-17 13:21:59+00:00,[],2024-10-17 22:09:50+00:00,2024-10-17 22:09:49+00:00,https://github.com/apache/airflow/issues/43117,"[('kind:bug', 'This is a clearly a bug'), ('area:performance', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2419536711, 'issue_id': 2594733219, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 17, 13, 22, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2420687528, 'issue_id': 2594733219, 'author': 'potiuk', 'body': 'There were lots of similar issues and optimisations implemented since 2.5.3. I recommend you to upgrade it to later version (2.5.3 is released 1.5 year ago and 2.10.2 has proably more than 1000 fixes since).\r\n\r\nPlease upgrade and try again.\r\n\r\nAlso if after upgrade you see the same issue - in order to see what happens, some anaysis of what happens in the proces would be useful. Running `pyspy` on the process of gunicorn could bring more issues. Until we get some of those details - converting it into a discussion, maybe someone with similar behaviour could chime in.', 'created_at': datetime.datetime(2024, 10, 17, 22, 9, 43, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-17 13:22:02 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-10-17 22:09:43 UTC): There were lots of similar issues and optimisations implemented since 2.5.3. I recommend you to upgrade it to later version (2.5.3 is released 1.5 year ago and 2.10.2 has proably more than 1000 fixes since).

Please upgrade and try again.

Also if after upgrade you see the same issue - in order to see what happens, some anaysis of what happens in the proces would be useful. Running `pyspy` on the process of gunicorn could bring more issues. Until we get some of those details - converting it into a discussion, maybe someone with similar behaviour could chime in.

"
2593354221,issue,open,,Docs: Add user facing docs for running a separate Task Execution API-server,"In https://github.com/apache/airflow/pull/43015, a new Task Execution API-server was added. This can be run with the Core Airflow APIs and can also be run separately.

To run all apis
```shell
airflow fastapi-api

# Alternate
airflow fastapi-api --apps all

# Alternate
airflow fastapi-api --apps core,execution
```

E.g to only run Task execution API
```shell
airflow fastapi-api --apps execution
```

The doc should contain details of our recommendation.",kaxil,2024-10-17 01:05:37+00:00,"['ashb', 'kaxil']",2024-11-11 13:03:18+00:00,,https://github.com/apache/airflow/issues/43103,"[('area:API', ""Airflow's REST/HTTP API""), ('kind:documentation', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]","[{'comment_id': 2422768470, 'issue_id': 2593354221, 'author': 'Brijeshthummar02', 'body': '@kaxil i can work on it, could you show me the path to files, and where to add it also assign me.', 'created_at': datetime.datetime(2024, 10, 18, 15, 48, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2422787775, 'issue_id': 2593354221, 'author': 'kaxil', 'body': 'Hey @Brijeshthummar02 , [AIP-72](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-72+Task+Execution+Interface+aka+Task+SDK?src=contextnavpagetreemode) is still under progress: https://github.com/orgs/apache/projects/405\r\n\r\nThis IMO would be need some background about [AIP-72](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-72+Task+Execution+Interface+aka+Task+SDK?src=contextnavpagetreemode), so unless you have read it might be difficult.\r\n\r\nKeeping it for myself or @ashb to work on it once we are done with that AIP.', 'created_at': datetime.datetime(2024, 10, 18, 15, 58, 22, tzinfo=datetime.timezone.utc)}]","Brijeshthummar02 on (2024-10-18 15:48:05 UTC): @kaxil i can work on it, could you show me the path to files, and where to add it also assign me.

kaxil (Issue Creator) on (2024-10-18 15:58:22 UTC): Hey @Brijeshthummar02 , [AIP-72](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-72+Task+Execution+Interface+aka+Task+SDK?src=contextnavpagetreemode) is still under progress: https://github.com/orgs/apache/projects/405

This IMO would be need some background about [AIP-72](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-72+Task+Execution+Interface+aka+Task+SDK?src=contextnavpagetreemode), so unless you have read it might be difficult.

Keeping it for myself or @ashb to work on it once we are done with that AIP.

"
2593021003,issue,closed,completed,The test_get_dags in fastapi API is not stable,"### Body

Sometimes (rarely) it fails because of sequence of retuned dags:

https://github.com/apache/airflow/actions/runs/11372256488/job/31636589074?pr=42754#step:7:4063

```python
FAILED tests/api_fastapi/core_api/routes/public/test_dags.py::TestGetDags::test_get_dags[query_params14-3-expected_ids14] - AssertionError: assert equals failed
  [                                [                               
    'test_dag3',                     'test_dag3',                  
                                     'test_dag1',                  
    'test_dag2',                     'test_dag2',                  
    'test_dag1',                                                   
  ]                                ]
======= 1 failed, 873 passed, 58 skipped, 1 warning in 166.23s (0:02:46) =======
```

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",potiuk,2024-10-16 20:45:16+00:00,['kaxil'],2024-10-16 21:48:36+00:00,2024-10-16 21:48:36+00:00,https://github.com/apache/airflow/issues/43099,"[('area:CI', ""Airflow's tests and continious integration""), ('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2417929663, 'issue_id': 2593021003, 'author': 'potiuk', 'body': 'FYI: @pierrejeambrun @kaxil  @omkar-foss', 'created_at': datetime.datetime(2024, 10, 16, 20, 46, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2417990764, 'issue_id': 2593021003, 'author': 'kaxil', 'body': 'PR: https://github.com/apache/airflow/pull/43100', 'created_at': datetime.datetime(2024, 10, 16, 21, 23, 10, tzinfo=datetime.timezone.utc)}]","potiuk (Issue Creator) on (2024-10-16 20:46:06 UTC): FYI: @pierrejeambrun @kaxil  @omkar-foss

kaxil (Assginee) on (2024-10-16 21:23:10 UTC): PR: https://github.com/apache/airflow/pull/43100

"
2592645876,issue,closed,completed,Python 3.8 with breeze causes import error,"When we are using Python 3.8 or less for breeze, it failes on functools.cache and the error is not telling too much.

![Image](https://github.com/user-attachments/assets/bf1af725-3e46-4d72-bc9f-a9045c3ddf14)

We should catch the import error and print meaningful message",potiuk,2024-10-16 17:58:00+00:00,['Bowrna'],2024-10-22 18:19:32+00:00,2024-10-22 18:19:32+00:00,https://github.com/apache/airflow/issues/43092,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('good first issue', '')]","[{'comment_id': 2417586223, 'issue_id': 2592645876, 'author': 'Bowrna', 'body': 'i will take a stab at this issue @potiuk', 'created_at': datetime.datetime(2024, 10, 16, 18, 20, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2417789773, 'issue_id': 2592645876, 'author': 'jscheffl', 'body': 'If a PR/fix is available we also need to add this to PR for version 2.10 in PR https://github.com/apache/airflow/pull/42788 (or on the same branch)', 'created_at': datetime.datetime(2024, 10, 16, 19, 43, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2423577288, 'issue_id': 2592645876, 'author': 'dirrao', 'body': 'Python 3.8 support was recently removed. Do we still need to address this issue, or is it now obsolete?', 'created_at': datetime.datetime(2024, 10, 19, 5, 18, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2423941231, 'issue_id': 2592645876, 'author': 'jscheffl', 'body': 'It is helping users who try to contribute but (by accident an old interpreter in the $PATH or just have not read the docs) that the root cause can be better identified. Same we had in the past when people tried to contribute tried (and failed) to start breeze in Windows.\r\nSo it might not only be specifically for Python 3.8 but for all versions <= 3.8', 'created_at': datetime.datetime(2024, 10, 19, 15, 5, 50, tzinfo=datetime.timezone.utc)}]","Bowrna (Assginee) on (2024-10-16 18:20:16 UTC): i will take a stab at this issue @potiuk

jscheffl on (2024-10-16 19:43:57 UTC): If a PR/fix is available we also need to add this to PR for version 2.10 in PR https://github.com/apache/airflow/pull/42788 (or on the same branch)

dirrao on (2024-10-19 05:18:46 UTC): Python 3.8 support was recently removed. Do we still need to address this issue, or is it now obsolete?

jscheffl on (2024-10-19 15:05:50 UTC): It is helping users who try to contribute but (by accident an old interpreter in the $PATH or just have not read the docs) that the root cause can be better identified. Same we had in the past when people tried to contribute tried (and failed) to start breeze in Windows.
So it might not only be specifically for Python 3.8 but for all versions <= 3.8

"
2592096778,issue,closed,completed,Add `aiohttp.client_exceptions.ClientConnectorError` to be a retryable error in databricks provider,"### Description

When there are SSL handshake issues(And usually intermittent), All deferrable Databricks operators fail in deferrable mode without retrying as `aiohttp.client_exceptions.ClientConnectorError` is not a retryable error. 

As of now, we only consider `aiohttp.ClientResponseError` to be retryable.  I would like to make `aiohttp.client_exceptions.ClientConnectorError` error to be retryable.

### Use case/motivation

When SSL handshake takes longer(usually 60 seconds by default), it fails with the below error:

```

  result = cls.__new__(cls)
[2024-10-16, 09:27:20 UTC] {taskinstance.py:1598} ERROR - Trigger failed:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/site-packages/aiohttp/connector.py"", line 1098, in _wrap_create_connection
    return await self._loop.create_connection(*args, **kwargs, sock=sock)
  File ""/usr/local/lib/python3.10/asyncio/base_events.py"", line 1103, in create_connection
    transport, protocol = await self._create_connection_transport(
  File ""/usr/local/lib/python3.10/asyncio/base_events.py"", line 1133, in _create_connection_transport
    await waiter
ConnectionAbortedError: SSL handshake is taking longer than 60.0 seconds: aborting the connection
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/site-packages/airflow/jobs/triggerer_job_runner.py"", line 529, in cleanup_finished_triggers
    result = details[""task""].result()
  File ""/usr/local/lib/python3.10/site-packages/airflow/jobs/triggerer_job_runner.py"", line 607, in run_trigger
    async for event in trigger.run():
  File ""/usr/local/lib/python3.10/site-packages/airflow/providers/databricks/triggers/databricks.py"", line 86, in run
    run_state = await self.hook.a_get_run_state(self.run_id)
  File ""/usr/local/lib/python3.10/site-packages/airflow/providers/databricks/hooks/databricks.py"", line 417, in a_get_run_state
    response = await self._a_do_api_call(GET_RUN_ENDPOINT, json)
  File ""/usr/local/lib/python3.10/site-packages/airflow/providers/databricks/hooks/databricks_base.py"", line 651, in _a_do_api_call
    async for attempt in self._a_get_retry_object():
  File ""/usr/local/lib/python3.10/site-packages/tenacity/_asyncio.py"", line 71, in __anext__
    do = self.iter(retry_state=self._retry_state)
  File ""/usr/local/lib/python3.10/site-packages/tenacity/__init__.py"", line 314, in iter
    return fut.result()
  File ""/usr/local/lib/python3.10/concurrent/futures/_base.py"", line 451, in result
    return self.__get_result()
  File ""/usr/local/lib/python3.10/concurrent/futures/_base.py"", line 403, in __get_result
    raise self._exception
  File ""/usr/local/lib/python3.10/site-packages/airflow/providers/databricks/hooks/databricks_base.py"", line 653, in _a_do_api_call
    async with request_func(
  File ""/usr/local/lib/python3.10/site-packages/aiohttp/client.py"", line 1359, in __aenter__
    self._resp: _RetType = await self._coro
  File ""/usr/local/lib/python3.10/site-packages/aiohttp/client.py"", line 663, in _request
    conn = await self._connector.connect(
  File ""/usr/local/lib/python3.10/site-packages/aiohttp/connector.py"", line 563, in connect
    proto = await self._create_connection(req, traces, timeout)
  File ""/usr/local/lib/python3.10/site-packages/aiohttp/connector.py"", line 1032, in _create_connection
    _, proto = await self._create_direct_connection(req, traces, timeout)
  File ""/usr/local/lib/python3.10/site-packages/aiohttp/connector.py"", line 1366, in _create_direct_connection
    raise last_exc
  File ""/usr/local/lib/python3.10/site-packages/aiohttp/connector.py"", line 1335, in _create_direct_connection
    transp, proto = await self._wrap_create_connection(
  File ""/usr/local/lib/python3.10/site-packages/aiohttp/connector.py"", line 1106, in _wrap_create_connection
    raise client_error(req.connection_key, exc) from exc
    aiohttp.client_exceptions.ClientConnectorError: Cannot connect to host adb-******.***REDACTED****.azuredatabricks.net:443 ssl:default [None]
```

And, that's intermittent. Making this retryable will help

### Related issues

NA

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rawwar,2024-10-16 14:37:50+00:00,['rawwar'],2024-10-17 11:20:54+00:00,2024-10-17 11:17:55+00:00,https://github.com/apache/airflow/issues/43080,"[('area:providers', ''), ('kind:feature', 'Feature Requests'), ('good first issue', ''), ('provider:databricks', '')]","[{'comment_id': 2417043103, 'issue_id': 2592096778, 'author': 'pankajkoti', 'body': '+1 to making `aiohttp.client_exceptions.ClientConnectorError` a retryable error', 'created_at': datetime.datetime(2024, 10, 16, 14, 40, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2418660298, 'issue_id': 2592096778, 'author': 'lucafurrer', 'body': '+1 from my side as well', 'created_at': datetime.datetime(2024, 10, 17, 6, 46, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2418683509, 'issue_id': 2592096778, 'author': 'lucafurrer', 'body': 'By the way we observe some problems with `asyncio.exceptions.TimeoutError`. In my opinion this should be retryable as well.', 'created_at': datetime.datetime(2024, 10, 17, 6, 53, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2418730818, 'issue_id': 2592096778, 'author': 'rawwar', 'body': ""> By the way we observe some problems with `asyncio.exceptions.TimeoutError`. In my opinion this should be retryable as well.\r\n\r\n~~Can you please share the exception message?~~ I think that message doesn't matter. But, can be added \r\n\r\nThinking about this, I think, we can let users pass their own `_retryable_error` method, which will validate if retry is allowed or not. \r\n\r\nAs of now, we only take `retry_limit` and `retry_delay` as arguments. Adding support for custom retry function and after func should be good enough to handle all custom requirements\r\n\r\nhttps://github.com/apache/airflow/blob/6b090b31038e6dd86f6efd49d81cf34b73d1e13e/providers/src/airflow/providers/databricks/hooks/databricks_base.py#L127"", 'created_at': datetime.datetime(2024, 10, 17, 7, 5, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2418757246, 'issue_id': 2592096778, 'author': 'lucafurrer', 'body': '> > By the way we observe some problems with `asyncio.exceptions.TimeoutError`. In my opinion this should be retryable as well.\r\n> \r\n> Can you please share the exception message?\r\n> \r\n> Thinking about this, I think, we can let users pass their own `_retryable_error` method, which will validate if retry is allowed or not.\r\n> \r\n> As of now, we only take `retry_limit` and `retry_delay` as arguments. Adding support for custom retry function and after func should be good enough to handle all custom requirements\r\n> \r\n> https://github.com/apache/airflow/blob/6b090b31038e6dd86f6efd49d81cf34b73d1e13e/providers/src/airflow/providers/databricks/hooks/databricks_base.py#L127\r\n\r\nHappy to share the message:\r\n\r\n```\r\n[2024-10-15, 20:07:05 CEST] {baseoperator.py:1598} ERROR - Trigger failed:\r\nTraceback (most recent call last):\r\n  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/jobs/triggerer_job_runner.py"", line 529, in cleanup_finished_triggers\r\n    result = details[""task""].result()\r\n  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/jobs/triggerer_job_runner.py"", line 601, in run_trigger\r\n    async for event in trigger.run():\r\n  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/triggers/databricks.py"", line 86, in run\r\n    run_state = await self.hook.a_get_run_state(self.run_id)\r\n  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/hooks/databricks.py"", line 417, in a_get_run_state\r\n    response = await self._a_do_api_call(GET_RUN_ENDPOINT, json)\r\n  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/hooks/databricks_base.py"", line 651, in _a_do_api_call\r\n    async for attempt in self._a_get_retry_object():\r\n  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/_asyncio.py"", line 71, in __anext__\r\n    do = self.iter(retry_state=self._retry_state)\r\n  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 314, in iter\r\n    return fut.result()\r\n  File ""/usr/local/lib/python3.8/concurrent/futures/_base.py"", line 437, in result\r\n    return self.__get_result()\r\n  File ""/usr/local/lib/python3.8/concurrent/futures/_base.py"", line 389, in __get_result\r\n    raise self._exception\r\n  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/hooks/databricks_base.py"", line 653, in _a_do_api_call\r\n    async with request_func(\r\n  File ""/home/airflow/.local/lib/python3.8/site-packages/aiohttp/client.py"", line 1194, in __aenter__\r\n    self._resp = await self._coro\r\n  File ""/home/airflow/.local/lib/python3.8/site-packages/aiohttp/client.py"", line 605, in _request\r\n    await resp.start(conn)\r\n  File ""/home/airflow/.local/lib/python3.8/site-packages/aiohttp/client_reqrep.py"", line 981, in start\r\n    self._continue = None\r\n  File ""/home/airflow/.local/lib/python3.8/site-packages/aiohttp/helpers.py"", line 735, in __exit__\r\n    raise asyncio.TimeoutError from None\r\nasyncio.exceptions.TimeoutError\r\n[2024-10-15, 20:07:05 CEST] {taskinstance.py:2731} ERROR - Task failed with exception\r\nTraceback (most recent call last):\r\n  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py"", line 444, in _execute_task\r\n    result = _execute_callable(context=context, **execute_callable_kwargs)\r\n  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py"", line 414, in _execute_callable\r\n    return execute_callable(context=context, **execute_callable_kwargs)\r\n  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py"", line 1599, in resume_execution\r\n    raise TaskDeferralError(next_kwargs.get(""error"", ""Unknown""))\r\nairflow.exceptions.TaskDeferralError: Trigger failure\r\n```\r\n\r\nIf there is a possibility to customize the retryable error this can be a solution for me. I could imagine that some of the exception types can be dependent on the cloud provider where Databricks is running on...', 'created_at': datetime.datetime(2024, 10, 17, 7, 19, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2418795335, 'issue_id': 2592096778, 'author': 'lucafurrer', 'body': 'However I think a timeout should be retryable per default as this is typically something which will be fixed a little bit later...', 'created_at': datetime.datetime(2024, 10, 17, 7, 39, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2419253548, 'issue_id': 2592096778, 'author': 'rawwar', 'body': '> However I think a timeout should be retryable per default as this is typically something which will be fixed a little bit later...\r\n\r\n@lucafurrer , I will create a separate issue and take it up later today. If you can create a new issue before that, please mention me', 'created_at': datetime.datetime(2024, 10, 17, 11, 20, 52, tzinfo=datetime.timezone.utc)}]","pankajkoti on (2024-10-16 14:40:45 UTC): +1 to making `aiohttp.client_exceptions.ClientConnectorError` a retryable error

lucafurrer on (2024-10-17 06:46:05 UTC): +1 from my side as well

lucafurrer on (2024-10-17 06:53:01 UTC): By the way we observe some problems with `asyncio.exceptions.TimeoutError`. In my opinion this should be retryable as well.

rawwar (Issue Creator) on (2024-10-17 07:05:44 UTC): ~~Can you please share the exception message?~~ I think that message doesn't matter. But, can be added 

Thinking about this, I think, we can let users pass their own `_retryable_error` method, which will validate if retry is allowed or not. 

As of now, we only take `retry_limit` and `retry_delay` as arguments. Adding support for custom retry function and after func should be good enough to handle all custom requirements

https://github.com/apache/airflow/blob/6b090b31038e6dd86f6efd49d81cf34b73d1e13e/providers/src/airflow/providers/databricks/hooks/databricks_base.py#L127

lucafurrer on (2024-10-17 07:19:49 UTC): Happy to share the message:

```
[2024-10-15, 20:07:05 CEST] {baseoperator.py:1598} ERROR - Trigger failed:
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/jobs/triggerer_job_runner.py"", line 529, in cleanup_finished_triggers
    result = details[""task""].result()
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/jobs/triggerer_job_runner.py"", line 601, in run_trigger
    async for event in trigger.run():
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/triggers/databricks.py"", line 86, in run
    run_state = await self.hook.a_get_run_state(self.run_id)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/hooks/databricks.py"", line 417, in a_get_run_state
    response = await self._a_do_api_call(GET_RUN_ENDPOINT, json)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/hooks/databricks_base.py"", line 651, in _a_do_api_call
    async for attempt in self._a_get_retry_object():
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/_asyncio.py"", line 71, in __anext__
    do = self.iter(retry_state=self._retry_state)
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 314, in iter
    return fut.result()
  File ""/usr/local/lib/python3.8/concurrent/futures/_base.py"", line 437, in result
    return self.__get_result()
  File ""/usr/local/lib/python3.8/concurrent/futures/_base.py"", line 389, in __get_result
    raise self._exception
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/databricks/hooks/databricks_base.py"", line 653, in _a_do_api_call
    async with request_func(
  File ""/home/airflow/.local/lib/python3.8/site-packages/aiohttp/client.py"", line 1194, in __aenter__
    self._resp = await self._coro
  File ""/home/airflow/.local/lib/python3.8/site-packages/aiohttp/client.py"", line 605, in _request
    await resp.start(conn)
  File ""/home/airflow/.local/lib/python3.8/site-packages/aiohttp/client_reqrep.py"", line 981, in start
    self._continue = None
  File ""/home/airflow/.local/lib/python3.8/site-packages/aiohttp/helpers.py"", line 735, in __exit__
    raise asyncio.TimeoutError from None
asyncio.exceptions.TimeoutError
[2024-10-15, 20:07:05 CEST] {taskinstance.py:2731} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py"", line 444, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py"", line 414, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py"", line 1599, in resume_execution
    raise TaskDeferralError(next_kwargs.get(""error"", ""Unknown""))
airflow.exceptions.TaskDeferralError: Trigger failure
```

If there is a possibility to customize the retryable error this can be a solution for me. I could imagine that some of the exception types can be dependent on the cloud provider where Databricks is running on...

lucafurrer on (2024-10-17 07:39:55 UTC): However I think a timeout should be retryable per default as this is typically something which will be fixed a little bit later...

rawwar (Issue Creator) on (2024-10-17 11:20:52 UTC): @lucafurrer , I will create a separate issue and take it up later today. If you can create a new issue before that, please mention me

"
2590560350,issue,open,,Clean up unneeded unique() calls on ORM,"### Body

Similar to #43064. Maybe there are more to be had? cc @dstandish 

I can find 6 with a naive text search in main, 4 in main and 2 in providers. We should review them individually.

https://github.com/apache/airflow/blob/e20146d44b340f719f7fb432f93741e011690558/airflow/assets/manager.py#L161-L167

https://github.com/apache/airflow/blob/e20146d44b340f719f7fb432f93741e011690558/airflow/dag_processing/collection.py#L64-L75

https://github.com/apache/airflow/blob/e20146d44b340f719f7fb432f93741e011690558/airflow/jobs/scheduler_job_runner.py#L1327-L1338

https://github.com/apache/airflow/blob/e20146d44b340f719f7fb432f93741e011690558/airflow/www/views.py#L1000-L1006

https://github.com/apache/airflow/blob/e20146d44b340f719f7fb432f93741e011690558/providers/src/airflow/providers/fab/auth_manager/api_endpoints/role_and_permission_endpoint.py#L88-L92

https://github.com/apache/airflow/blob/e20146d44b340f719f7fb432f93741e011690558/providers/src/airflow/providers/fab/auth_manager/security_manager/override.py#L2792-L2799

(There is a 7th `unique()` match in a Jinja2 template.)

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",uranusjr,2024-10-16 04:16:22+00:00,[],2024-10-16 04:18:38+00:00,,https://github.com/apache/airflow/issues/43068,"[('kind:meta', 'High-level information important to the community'), ('area:core', '')]",[],
2589807329,issue,closed,completed,AIP-38 | Add colorblind friendly color schemes,"We are starting to add status colors to the new UI. Before we build out too much we should make sure to think about a11y from the get go. A lot of status colors are hard for people (particularly red/green).

- Update our theme to always use semantic names instead of colors (ex: ""error"" instead of ""red"", ""success"" instead of ""green)

- Decide to use a [colorblind friendly color palette for everyone](https://personal.sron.nl/~pault/), or add a user setting to switch between common semantic colors and a friendlier palette",bbovenzi,2024-10-15 20:27:51+00:00,['bbovenzi'],2025-01-31 18:10:07+00:00,2025-01-31 18:10:07+00:00,https://github.com/apache/airflow/issues/43054,"[('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2415241613, 'issue_id': 2589807329, 'author': 'potiuk', 'body': '❤️', 'created_at': datetime.datetime(2024, 10, 15, 21, 59, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2420006689, 'issue_id': 2589807329, 'author': 'bbovenzi', 'body': 'We have 12 state colors right now, which is a lot, and leaves us with three options:\n\n1. Switch our state colors to an imperfect colorblind friendly palette for everyone. No need for settings but a lot of colors will really change: https://mk.bcgsc.ca/colorblind/palettes.mhtml#12-color-palette-for-colorbliness\n\nEx: ![Image](https://github.com/user-attachments/assets/ed37d55f-6cc8-4265-a6b4-fbd746c70cac)\n\n\n2. Add a color-blind friendly mode to allow users to opt-in to a more accessible mode\n\n3. Use a better color blind friendly palette, but with fewer colors and the need to consolidate task states.\nWith dag versioning, will we have Removed or just None or Skipped?\nOr can we move to a status/sub status model \n\n\n\nIn all scenarios, we should always try to depict state through other means than simply color.', 'created_at': datetime.datetime(2024, 10, 17, 16, 33, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2420246372, 'issue_id': 2589807329, 'author': 'bbovenzi', 'body': ""Possible groupings with the same color but some indication that they're separate:\n\nPre run: queued, schedule, deferred\nRe do: restarting, up_for_retry, up_for_reschedule\nNothing: no_status, skipped, removed, upstream_failed\nFailed\nSuccess\nRunning"", 'created_at': datetime.datetime(2024, 10, 17, 18, 27, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2420511431, 'issue_id': 2589807329, 'author': 'ashb', 'body': ""I think queued and scheduled are distinct enough to need different colors.\r\n\r\nAnd skipped /upstream_failedcould be the same colour, but I don't think no_status/null should be in the same grouping. One has never been, the other ran and failed. Removed I could se go in either side."", 'created_at': datetime.datetime(2024, 10, 17, 20, 37, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2420526770, 'issue_id': 2589807329, 'author': 'shahar1', 'body': ""> 3\\. Use a better color blind friendly palette, but with fewer colors and the need to consolidate task states.\r\n>     With dag versioning, will we have Removed or just None or Skipped?\r\n>     Or can we move to a status/sub status model\r\n\r\nIn general - great idea and kudos for making the UI more accessible for those in need!\r\n\r\nMy two cents regarding your suggestions:\r\n1. Not too bad, but some of the colors in the pallette are too similar:\r\n  a. restarting - skipping\r\n  b. deferred - removed\r\n2. In any case I'd make this transition optional for now (long term - maybe even user customized?).\r\n3. I think that I'd go with the first option rather than having less and too similar colors.\r\nIf we go in the direction of less colors, maybe we could utilize other features like opacity/fill/shape?\r\n\r\n\\+ Don't forget the dark mode :)"", 'created_at': datetime.datetime(2024, 10, 17, 20, 40, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2420549827, 'issue_id': 2589807329, 'author': 'bbovenzi', 'body': ""1. With some user testing, the 12 color palette I linked didn't help much.\r\n\r\n2. I am definitely moving forward with a version of this. Looking at a hue rotation that let's a user shift the whole UI as they see fit: https://stackoverflow.com/questions/46553023/use-slider-to-change-image-css-filter-with-javascript/46553057#46553057\r\n\r\n3. I need to figure out which other changes can work well with colors. I definitely like the idea of visually separating tasks that are in pending or final states. Pending can be a circle while final is more square shaped or we can animate a gradient bg for pending states."", 'created_at': datetime.datetime(2024, 10, 17, 20, 50, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2420738453, 'issue_id': 2589807329, 'author': 'potiuk', 'body': 'FYI Animation is also against guidelines for accessibility - we had issue about it https://github.com/apache/airflow/issues/34019', 'created_at': datetime.datetime(2024, 10, 17, 22, 37, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2420740790, 'issue_id': 2589807329, 'author': 'potiuk', 'body': 'We could also let the user pick their own colors for each state.', 'created_at': datetime.datetime(2024, 10, 17, 22, 39, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2430087816, 'issue_id': 2589807329, 'author': 'bbovenzi', 'body': 'Yes, we do allow `STATE_COLORS` to be edited via settings but on a deployment, not a user basis. I don\'t think that should exist in the browser\'s localstorage either. It would be great to create actual user settings we save on the db which can also include other requests like ""favorite_dags"".', 'created_at': datetime.datetime(2024, 10, 22, 19, 32, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2504583989, 'issue_id': 2589807329, 'author': 'vmatt', 'body': ""hey @bbovenzi and the rest of the community!\r\n\r\nWhat do you thing of introducing symbols instead of using colors? I've [created a userscript](https://greasyfork.org/en/scripts/518865-airflow-task-instance-status-enhancer) which does just that, but it's kinda hacky, and interferes with React's Virtual DOM. It would be way better to implement something similar natively.\r\n\r\nWith this change, the  default color scheme would remain, but users could switch to symbols / tweak it in their personal user settings. \r\n\r\n\r\n![image](https://github.com/user-attachments/assets/b9185743-6597-4319-b30f-9f2b728f6cb8)"", 'created_at': datetime.datetime(2024, 11, 27, 18, 50, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508902810, 'issue_id': 2589807329, 'author': 'jscheffl', 'body': ""> hey @bbovenzi and the rest of the community!\r\n> \r\n> What do you thing of introducing symbols instead of using colors? I've [created a userscript](https://greasyfork.org/en/scripts/518865-airflow-task-instance-status-enhancer) which does just that, but it's kinda hacky, and interferes with React's Virtual DOM. It would be way better to implement something similar natively.\r\n> \r\n> With this change, the default color scheme would remain, but users could switch to symbols / tweak it in their personal user settings.\r\n> \r\n\r\nI like the idea with icons - we just need to consider that while the enlarged mugshot look nice - especially with the details in the icons - on most screens (non retina) this will be just a few pixels. So symbols can help BUT we need to make them also visually distinguishable in todays 16 x 16 pixel raster (with the option to have more details in retina displays)\r\n\r\n![image](https://github.com/user-attachments/assets/86900c61-905d-47ba-9cb6-4173614d13e9)\r\n\r\nSo if we go for symbols they would need to be very reduced in detail and colors and need to be distinguishable being max 14 x 14 pixels (assuming 2 pixels border to the next icon)"", 'created_at': datetime.datetime(2024, 11, 30, 9, 39, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2509013814, 'issue_id': 2589807329, 'author': 'potiuk', 'body': 'Yep. I m also for symbols - but they should be done RIGHT.', 'created_at': datetime.datetime(2024, 11, 30, 15, 49, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2622020450, 'issue_id': 2589807329, 'author': 'bbovenzi', 'body': 'In case anyone missed it. We now have icons for each task state. This is implemented everywhere in the new UI except for the grid view for now.\n\n<img width=""1175"" alt=""Image"" src=""https://github.com/user-attachments/assets/3ca106d1-e0e1-4db5-9781-0476b8cb99ce"" />\n<img width=""1190"" alt=""Image"" src=""https://github.com/user-attachments/assets/c61ce9a7-d180-42e8-a928-77a9e9f9e9e8"" />\n<img width=""1152"" alt=""Image"" src=""https://github.com/user-attachments/assets/19e1d6df-44f0-43bc-9d3c-e6af487be0d7"" />', 'created_at': datetime.datetime(2025, 1, 29, 15, 48, 22, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-10-15 21:59:50 UTC): ❤️

bbovenzi (Issue Creator) on (2024-10-17 16:33:42 UTC): We have 12 state colors right now, which is a lot, and leaves us with three options:

1. Switch our state colors to an imperfect colorblind friendly palette for everyone. No need for settings but a lot of colors will really change: https://mk.bcgsc.ca/colorblind/palettes.mhtml#12-color-palette-for-colorbliness

Ex: ![Image](https://github.com/user-attachments/assets/ed37d55f-6cc8-4265-a6b4-fbd746c70cac)


2. Add a color-blind friendly mode to allow users to opt-in to a more accessible mode

3. Use a better color blind friendly palette, but with fewer colors and the need to consolidate task states.
With dag versioning, will we have Removed or just None or Skipped?
Or can we move to a status/sub status model 



In all scenarios, we should always try to depict state through other means than simply color.

bbovenzi (Issue Creator) on (2024-10-17 18:27:35 UTC): Possible groupings with the same color but some indication that they're separate:

Pre run: queued, schedule, deferred
Re do: restarting, up_for_retry, up_for_reschedule
Nothing: no_status, skipped, removed, upstream_failed
Failed
Success
Running

ashb on (2024-10-17 20:37:35 UTC): I think queued and scheduled are distinct enough to need different colors.

And skipped /upstream_failedcould be the same colour, but I don't think no_status/null should be in the same grouping. One has never been, the other ran and failed. Removed I could se go in either side.

shahar1 on (2024-10-17 20:40:43 UTC): In general - great idea and kudos for making the UI more accessible for those in need!

My two cents regarding your suggestions:
1. Not too bad, but some of the colors in the pallette are too similar:
  a. restarting - skipping
  b. deferred - removed
2. In any case I'd make this transition optional for now (long term - maybe even user customized?).
3. I think that I'd go with the first option rather than having less and too similar colors.
If we go in the direction of less colors, maybe we could utilize other features like opacity/fill/shape?

\+ Don't forget the dark mode :)

bbovenzi (Issue Creator) on (2024-10-17 20:50:46 UTC): 1. With some user testing, the 12 color palette I linked didn't help much.

2. I am definitely moving forward with a version of this. Looking at a hue rotation that let's a user shift the whole UI as they see fit: https://stackoverflow.com/questions/46553023/use-slider-to-change-image-css-filter-with-javascript/46553057#46553057

3. I need to figure out which other changes can work well with colors. I definitely like the idea of visually separating tasks that are in pending or final states. Pending can be a circle while final is more square shaped or we can animate a gradient bg for pending states.

potiuk on (2024-10-17 22:37:33 UTC): FYI Animation is also against guidelines for accessibility - we had issue about it https://github.com/apache/airflow/issues/34019

potiuk on (2024-10-17 22:39:47 UTC): We could also let the user pick their own colors for each state.

bbovenzi (Issue Creator) on (2024-10-22 19:32:31 UTC): Yes, we do allow `STATE_COLORS` to be edited via settings but on a deployment, not a user basis. I don't think that should exist in the browser's localstorage either. It would be great to create actual user settings we save on the db which can also include other requests like ""favorite_dags"".

vmatt on (2024-11-27 18:50:38 UTC): hey @bbovenzi and the rest of the community!

What do you thing of introducing symbols instead of using colors? I've [created a userscript](https://greasyfork.org/en/scripts/518865-airflow-task-instance-status-enhancer) which does just that, but it's kinda hacky, and interferes with React's Virtual DOM. It would be way better to implement something similar natively.

With this change, the  default color scheme would remain, but users could switch to symbols / tweak it in their personal user settings. 


![image](https://github.com/user-attachments/assets/b9185743-6597-4319-b30f-9f2b728f6cb8)

jscheffl on (2024-11-30 09:39:48 UTC): I like the idea with icons - we just need to consider that while the enlarged mugshot look nice - especially with the details in the icons - on most screens (non retina) this will be just a few pixels. So symbols can help BUT we need to make them also visually distinguishable in todays 16 x 16 pixel raster (with the option to have more details in retina displays)

![image](https://github.com/user-attachments/assets/86900c61-905d-47ba-9cb6-4173614d13e9)

So if we go for symbols they would need to be very reduced in detail and colors and need to be distinguishable being max 14 x 14 pixels (assuming 2 pixels border to the next icon)

potiuk on (2024-11-30 15:49:34 UTC): Yep. I m also for symbols - but they should be done RIGHT.

bbovenzi (Issue Creator) on (2025-01-29 15:48:22 UTC): In case anyone missed it. We now have icons for each task state. This is implemented everywhere in the new UI except for the grid view for now.

<img width=""1175"" alt=""Image"" src=""https://github.com/user-attachments/assets/3ca106d1-e0e1-4db5-9781-0476b8cb99ce"" />
<img width=""1190"" alt=""Image"" src=""https://github.com/user-attachments/assets/c61ce9a7-d180-42e8-a928-77a9e9f9e9e8"" />
<img width=""1152"" alt=""Image"" src=""https://github.com/user-attachments/assets/19e1d6df-44f0-43bc-9d3c-e6af487be0d7"" />

"
2589613464,issue,closed,completed,Break up airflow-providers-google-cloud into sub providers,"### Description

Currently there is one package to install if you want to manage connections to Google Cloud,  `apache-airflow-providers-google`, this package contains dependencies for Google services including:

> -  [Google Ads](https://ads.google.com/)
> -  [Google Cloud (GCP)](https://cloud.google.com/)
> -  [Google Firebase](https://firebase.google.com/)
> -  [Google LevelDB](https://github.com/google/leveldb/)
> -  [Google Marketing Platform](https://marketingplatform.google.com/)
> -  [Google Workspace](https://workspace.google.com/) (formerly Google Suite)

Breaking up this overarching package into its components could streamline installation for people who are just concerned with a component. I haven't looked into the structure/code for this component so I have no idea how complicated this is! 

### Use case/motivation

Currently we are pulling data from the Google Sheets API, and we want to be managing the necessary connection using a Google Connection. However, this does entail installing 48 google packages. We would like a leaner installation and also to reduce the complexity of our installations in case this leads to conflicts in installation.

### Related issues

Maybe this? https://github.com/apache/airflow/issues/31370

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",radumas,2024-10-15 18:56:07+00:00,[],2024-10-15 21:51:31+00:00,2024-10-15 21:51:31+00:00,https://github.com/apache/airflow/issues/43049,"[('provider:google', 'Google (including GCP) related issues'), ('area:providers', ''), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2414780653, 'issue_id': 2589613464, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 15, 18, 56, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415229818, 'issue_id': 2589613464, 'author': 'potiuk', 'body': ""Duplicate of https://github.com/apache/airflow/issues/15933 - if you would like to attempt to do it feel free - but it's 3 years and  it's not yet done, as it requires quite an effort. If you would like to do it - feel free @radumas - otherwise it will have to wait for someone who will take on the task.\r\n\r\nClosing as duplicate."", 'created_at': datetime.datetime(2024, 10, 15, 21, 51, 31, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-15 18:56:09 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-10-15 21:51:31 UTC): Duplicate of https://github.com/apache/airflow/issues/15933 - if you would like to attempt to do it feel free - but it's 3 years and  it's not yet done, as it requires quite an effort. If you would like to do it - feel free @radumas - otherwise it will have to wait for someone who will take on the task.

Closing as duplicate.

"
2589411794,issue,closed,completed,Unnecessary dependence between capacity_provider_strategy and volume_configurations,"### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.29.0

### Apache Airflow version

2.9.2

### Operating System

Amazon Linux 2023.5.20240805

### Deployment

Amazon (AWS) MWAA

### Deployment details

Using a Python 3.11 environment with `botocore==1.34.106` and `apache-airflow-providers-amazon==8.29.0`.

This allows us to use the `volume_configurations` feature so we can deploy DAGs that use the `ECSRunTaskOperator` with EBS volume attachments configured at launch.



### What happened

Omitting `capacity_provider_strategy` from `EcsRunTaskOperator` prevents `volume_configurations` from being passed to `self.client.run_task(**run_opts)` in `EcsRunTaskOperator._start_task()`.

So, if you aren't using `capacity_provider_strategy` (in this case we are using `launch_type=""FARGATE""`) this prevents an EBS volume configuration from being defined which throws the following error during DAG execution:

```python
Traceback (most recent call last):
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2479, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2676, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2701, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py"", line 401, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/amazon/aws/operators/ecs.py"", line 543, in execute
    self._start_task()
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/amazon/aws/operators/ecs.py"", line 643, in _start_task
    response = self.client.run_task(**run_opts)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/botocore/client.py"", line 565, in _api_call
    return self._make_api_call(operation_name, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/botocore/client.py"", line 1021, in _make_api_call
    raise error_class(parsed_response, operation_name)
botocore.errorfactory.InvalidParameterException: An error occurred (InvalidParameterException) when calling the RunTask operation: The volume name '<volume_name>' in your request does not have a matching volume in your Task Definition that is configurable at launch.
```

### What you think should happen instead

`EcsRunTaskOperator.volume_configurations` should be passed to `self.client.run_task(**run_opts)` regardless of the value of `capacity_provider_strategy`.

### How to reproduce

1. Create an ECS task definition that includes `volume` and `mountPoint` definitions for an EBS volume.
2. Run a task using `ECSRunTaskOperator` with `volume_configurations` containing a `managedEBSVolume` defined and `capacity_provider_strategy` undefined.
3. Try to run the corresponding DAG with an EBS volume attached.

### Anything else

- Directly calling [ECS.Client.run_task](https://boto3.amazonaws.com/v1/documentation/api/1.34.106/reference/services/ecs/client/run_task.html) with the boto3 API with the same configuration passed to `EcsRunTaskOperator` (with camelCase keywords) works.
- Replacing ecs.py in my deployment with the code in #43047 fixes the issue and allows us to run these tasks.

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",pyrr,2024-10-15 17:22:24+00:00,['pyrr'],2024-11-06 07:58:04+00:00,2024-11-06 07:58:04+00:00,https://github.com/apache/airflow/issues/43046,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('good first issue', '')]","[{'comment_id': 2414597373, 'issue_id': 2589411794, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 15, 17, 22, 27, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-15 17:22:27 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2589115733,issue,closed,completed,AIP-38 | Add more sort options to Dags List,"In our fastapi endpoint, we only sort the 

Right now, we only sort by `dag_id`. Let's swap that for display_name. Also, let's add `""next_dagrun"", ""last_run_state"", ""last_run_start_date""` as other sort options.

Since not all of these fields are currently columns in the dags list response, we probably need to always our `sort-by-select` instead of only when its a card list.

",bbovenzi,2024-10-15 15:20:40+00:00,['shubhamraj-git'],2024-11-13 18:06:08+00:00,2024-11-13 18:06:08+00:00,https://github.com/apache/airflow/issues/43043,"[('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2414892836, 'issue_id': 2589115733, 'author': 'shubhamraj-git', 'body': 'Could you please assign this to me.', 'created_at': datetime.datetime(2024, 10, 15, 19, 59, 32, tzinfo=datetime.timezone.utc)}]","shubhamraj-git (Assginee) on (2024-10-15 19:59:32 UTC): Could you please assign this to me.

"
2588823812,issue,open,,fix Microsoft provider to support `microsoft-kiota-abstractions>1.4.0`,"### Description

Newer version of microsoft-kiota-abstractions was released which has several breaking changes.

One of the breaking change.
following module missing: [1.4.0](https://github.com/microsoft/kiota-python/blob/v1.4.0/kiota_abstractions/default_query_parameters.py) . vs [1.3.3](https://github.com/microsoft/kiota-python/blob/v1.3.3/kiota_abstractions/default_query_parameters.py)

They released newer version at Mon, 14 Oct 2024 17:09:01 GMT . Source: https://pypi.org/rss/project/microsoft-kiota-abstractions/releases.xml

### Use case/motivation

To remove an upper bound pin on `microsoft-kiota-abstractions` in Microsoft provider.

### Related issues

related to https://github.com/apache/airflow/pull/43021

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rawwar,2024-10-15 13:42:01+00:00,['LefterisXefteris'],2024-10-17 21:55:29+00:00,,https://github.com/apache/airflow/issues/43036,"[('provider:microsoft-azure', 'Azure-related issues'), ('kind:feature', 'Feature Requests'), ('good first issue', ''), ('area:dependencies', 'Issues related to dependencies problems')]","[{'comment_id': 2416616001, 'issue_id': 2588823812, 'author': 'LefterisXefteris', 'body': ""Hello There i have been using airflow for my personal projects, and i would to work for this issue, if that's okay."", 'created_at': datetime.datetime(2024, 10, 16, 12, 1, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2416999273, 'issue_id': 2588823812, 'author': 'rawwar', 'body': ""> Hello There i have been using airflow for my personal projects, and i would to work for this issue, if that's okay.\r\n\r\n@ambika-garg mentioned she will be looking into this. @ambika-garg , can I assign this issue to you? If not, I can assign it to @LefterisXefteris"", 'created_at': datetime.datetime(2024, 10, 16, 14, 24, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2417119796, 'issue_id': 2588823812, 'author': 'ambika-garg', 'body': 'Thanks for considering me @rawwar! Due to my current workload, I think @LefterisXefteris might be able to handle this more quickly. Please feel free to assign it to him.', 'created_at': datetime.datetime(2024, 10, 16, 15, 9, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2418198138, 'issue_id': 2588823812, 'author': 'LefterisXefteris', 'body': 'Thank you for giving me the opportunity to contribute to such an exciting and impactful project as Apache Airflow. I’m looking forward to collaborating and making meaningful contributions to its development.', 'created_at': datetime.datetime(2024, 10, 17, 0, 17, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2420657882, 'issue_id': 2588823812, 'author': 'potiuk', 'body': '> Thank you for the opportunity to contribute to such an exciting and impactful project as Apache Airflow. I’m looking forward to collaborating and making meaningful contributions to its development.\r\n\r\nLooking forward to it !', 'created_at': datetime.datetime(2024, 10, 17, 21, 44, 53, tzinfo=datetime.timezone.utc)}]","LefterisXefteris (Assginee) on (2024-10-16 12:01:49 UTC): Hello There i have been using airflow for my personal projects, and i would to work for this issue, if that's okay.

rawwar (Issue Creator) on (2024-10-16 14:24:05 UTC): @ambika-garg mentioned she will be looking into this. @ambika-garg , can I assign this issue to you? If not, I can assign it to @LefterisXefteris

ambika-garg on (2024-10-16 15:09:52 UTC): Thanks for considering me @rawwar! Due to my current workload, I think @LefterisXefteris might be able to handle this more quickly. Please feel free to assign it to him.

LefterisXefteris (Assginee) on (2024-10-17 00:17:01 UTC): Thank you for giving me the opportunity to contribute to such an exciting and impactful project as Apache Airflow. I’m looking forward to collaborating and making meaningful contributions to its development.

potiuk on (2024-10-17 21:44:53 UTC): Looking forward to it !

"
2588791245,issue,closed,completed,Investigate if we can  replace `gunicornmontor` with `uvicorn.run()`,"It is most likely that we no longer need `gunicornmontor` or `UvicornMonitor` anymore.  @ashb 's suggestion is for Airflow `uvicorn.run()` should be enough.

Whoever takes this GitHub issue should verify the same and replace it if not needed.

The code:

- https://github.com/apache/airflow/blob/f38d56dbf4dc1639142fc5a494d5da24996a56cc/airflow/cli/commands/fastapi_api_command.py#L159-L190
- https://github.com/apache/airflow/blob/f38d56dbf4dc1639142fc5a494d5da24996a56cc/airflow/cli/commands/webserver_command.py#L49-L107",kaxil,2024-10-15 13:30:09+00:00,['vatsrahul1001'],2025-01-25 14:51:44+00:00,2025-01-25 14:51:43+00:00,https://github.com/apache/airflow/issues/43035,"[('area:CLI', '')]","[{'comment_id': 2547978428, 'issue_id': 2588791245, 'author': 'vatsrahul1001', 'body': ""The current GunicornMonitor provides the following capabilities:\r\n\r\n1. [Automatic worker restarts if workers crash or hang](https://github.com/apache/airflow/blob/main/airflow/cli/commands/local_commands/webserver_command.py#L257):  \r\n   Ensures that if a worker crashes or becomes unresponsive, it is automatically restarted.\r\n\r\n3. [Graceful worker scaling and reloads](https://github.com/apache/airflow/blob/main/airflow/cli/commands/local_commands/webserver_command.py#L162):\r\n  This allows for addition and removal of workers and reloads workers gracefully when needed.\r\n\r\n4. [Timeout management for unresponsive workers](https://github.com/apache/airflow/blob/main/airflow/cli/commands/local_commands/webserver_command.py#L154):\r\nGunicorn monitors workers for unresponsiveness and can terminate them if they exceed a set timeout, preventing hangs.\r\n\r\n\r\nIf we switch to `uvicorn.run()`, we would lose these features since `uvicorn.run() `lacks built-in process management. Specifically:\r\n\r\nIf a worker dies, there's no master process to restart it.\r\nThere will be no automatic scaling of workers, and no handling of worker timeouts or periodic restarts.\r\nTo replicate this functionality, we would need an external process manager like systemd or supervisord, which adds additional complexity and overhead.\r\n\r\ncc: @kaxil @ashb"", 'created_at': datetime.datetime(2024, 12, 17, 9, 49, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2548281676, 'issue_id': 2588791245, 'author': 'ashb', 'body': 'For 2: https://docs.gunicorn.org/en/stable/signals.html\r\n> TTIN: Increment the number of processes by one\r\n> TTOU: Decrement the number of processes by one\r\n\r\n\r\n> If a worker dies, there\'s no master process to restart it\r\n\r\nDoesn\'t Gunicorn do that itself? https://docs.gunicorn.org/en/stable/design.html#master\r\n\r\n> The master process is a simple loop that listens for various process signals and reacts accordingly. It manages the list of running workers by listening for signals like TTIN, TTOU, and CHLD. TTIN and TTOU tell the master to increase or decrease the number of running workers. CHLD indicates that a child process has terminated, in this case the master process automatically restarts the failed worker.\r\n\r\nSo it\'s only the case of ""worker hang"" that might not be there anymore.Let me think', 'created_at': datetime.datetime(2024, 12, 17, 12, 6, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2548283279, 'issue_id': 2588791245, 'author': 'ashb', 'body': 'For 1: https://docs.gunicorn.org/en/stable/settings.html#timeout I think?', 'created_at': datetime.datetime(2024, 12, 17, 12, 7, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2549396045, 'issue_id': 2588791245, 'author': 'potiuk', 'body': 'Just one comment here -> I\'ve heard (but it\'s mostly through grapevine) that for quite a long time, uvicorn has the capability (and it\'s more and more recommended in production) - to manage multiple processes and handle sync requests directly - on their own and there is basically no need to use gunicorn at all. \r\n\r\nAgain it\'s more of ""overheard"" thing but looking at https://www.uvicorn.org/deployment/#using-a-process-manager , maybe that\'s what we are looking for? (or maybe I misunderstood what we want to do, just wanted to mention that gunicorn might not be needed at all maybe)', 'created_at': datetime.datetime(2024, 12, 17, 19, 14, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2553914293, 'issue_id': 2588791245, 'author': 'vatsrahul1001', 'body': 'To perform the comparison. I replaced Gunicorn code in else [block](https://github.com/apache/airflow/blob/main/airflow/cli/commands/local_commands/fastapi_api_command.py#L95) with  below ` uvicorn.run` command\r\n\r\n```\r\n  uvicorn.run(""airflow.api_fastapi.main:app"", host=args.hostname, port=args.port, workers=num_workers,\r\n                    timeout_keep_alive=worker_timeout, timeout_graceful_shutdown=worker_timeout, ssl_keyfile=ssl_key,\r\n                    ssl_certfile=ssl_cert, access_log=access_logfile)\r\n```\r\nI used locust for performance testing with below configuration\r\n\r\nThese are the stats comparing `uvicorn.run()` with` Gunicorn + GunicornMonitor`\r\n\r\n<html><head></head><body>\r\n<hr>\r\n<h3><strong>Comparison: Uvicorn vs. Gunicorn Performance</strong></h3>\r\n<h4><strong>Request Statistics</strong></h4>\r\n\r\nMetric | Uvicorn | Gunicorn\r\n-- | -- | --\r\nTotal Requests | 14,714 | 14,726\r\nTotal Failures | 0 | 13\r\nAverage Response Time | 12.05 ms | 13.46 ms\r\nMin Response Time | 7 ms | 1 ms\r\nMax Response Time | 195 ms | 216 ms\r\nAverage Size (bytes) | 4,608 | 4,603.93\r\nRequests Per Second (RPS) | 49.05 | 49.09\r\nFailures Per Second | 0 | 0.04\r\n\r\n\r\n<hr>\r\n<h3><strong>Observations</strong></h3>\r\n<ol>\r\n<li>\r\n<p><strong>Response Times</strong>:</p>\r\n<ul>\r\n<li>Uvicorn demonstrates slightly lower average and maximum response times compared to Gunicorn.</li>\r\n<li>Percentile analysis shows Uvicorn\'s response times are more consistent, with fewer extreme values at higher percentiles.</li>\r\n</ul>\r\n</li>\r\n<li>\r\n<p><strong>Failures</strong>:</p>\r\n<ul>\r\n<li>Uvicorn had <strong>no failures</strong>, whereas Gunicorn recorded <strong>13 failures</strong> caused by <code inline="""">RemoteDisconnected</code> errors. This could indicate potential issues in connection handling under load.</li>\r\n</ul>\r\n</li>\r\n<li>\r\n<p><strong>Performance Consistency</strong>:</p>\r\n<ul>\r\n<li>Uvicorn offers better consistency and reliability based on the above data.</li>\r\n</ul>\r\n</li>\r\n</ol>\r\n<hr>\r\n</html>', 'created_at': datetime.datetime(2024, 12, 19, 13, 16, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2553967846, 'issue_id': 2588791245, 'author': 'potiuk', 'body': 'Nice!.', 'created_at': datetime.datetime(2024, 12, 19, 13, 26, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2613991037, 'issue_id': 2588791245, 'author': 'vatsrahul1001', 'body': '[Replacing gunicornmontor with uvicorn.run() #45103](https://github.com/apache/airflow/pull/45103) merged.', 'created_at': datetime.datetime(2025, 1, 25, 14, 51, 43, tzinfo=datetime.timezone.utc)}]","vatsrahul1001 (Assginee) on (2024-12-17 09:49:57 UTC): The current GunicornMonitor provides the following capabilities:

1. [Automatic worker restarts if workers crash or hang](https://github.com/apache/airflow/blob/main/airflow/cli/commands/local_commands/webserver_command.py#L257):  
   Ensures that if a worker crashes or becomes unresponsive, it is automatically restarted.

3. [Graceful worker scaling and reloads](https://github.com/apache/airflow/blob/main/airflow/cli/commands/local_commands/webserver_command.py#L162):
  This allows for addition and removal of workers and reloads workers gracefully when needed.

4. [Timeout management for unresponsive workers](https://github.com/apache/airflow/blob/main/airflow/cli/commands/local_commands/webserver_command.py#L154):
Gunicorn monitors workers for unresponsiveness and can terminate them if they exceed a set timeout, preventing hangs.


If we switch to `uvicorn.run()`, we would lose these features since `uvicorn.run() `lacks built-in process management. Specifically:

If a worker dies, there's no master process to restart it.
There will be no automatic scaling of workers, and no handling of worker timeouts or periodic restarts.
To replicate this functionality, we would need an external process manager like systemd or supervisord, which adds additional complexity and overhead.

cc: @kaxil @ashb

ashb on (2024-12-17 12:06:18 UTC): For 2: https://docs.gunicorn.org/en/stable/signals.html



Doesn't Gunicorn do that itself? https://docs.gunicorn.org/en/stable/design.html#master


So it's only the case of ""worker hang"" that might not be there anymore.Let me think

ashb on (2024-12-17 12:07:06 UTC): For 1: https://docs.gunicorn.org/en/stable/settings.html#timeout I think?

potiuk on (2024-12-17 19:14:36 UTC): Just one comment here -> I've heard (but it's mostly through grapevine) that for quite a long time, uvicorn has the capability (and it's more and more recommended in production) - to manage multiple processes and handle sync requests directly - on their own and there is basically no need to use gunicorn at all. 

Again it's more of ""overheard"" thing but looking at https://www.uvicorn.org/deployment/#using-a-process-manager , maybe that's what we are looking for? (or maybe I misunderstood what we want to do, just wanted to mention that gunicorn might not be needed at all maybe)

vatsrahul1001 (Assginee) on (2024-12-19 13:16:58 UTC): To perform the comparison. I replaced Gunicorn code in else [block](https://github.com/apache/airflow/blob/main/airflow/cli/commands/local_commands/fastapi_api_command.py#L95) with  below ` uvicorn.run` command

```
  uvicorn.run(""airflow.api_fastapi.main:app"", host=args.hostname, port=args.port, workers=num_workers,
                    timeout_keep_alive=worker_timeout, timeout_graceful_shutdown=worker_timeout, ssl_keyfile=ssl_key,
                    ssl_certfile=ssl_cert, access_log=access_logfile)
```
I used locust for performance testing with below configuration

These are the stats comparing `uvicorn.run()` with` Gunicorn + GunicornMonitor`

<html><head></head><body>
<hr>
<h3><strong>Comparison: Uvicorn vs. Gunicorn Performance</strong></h3>
<h4><strong>Request Statistics</strong></h4>

Metric | Uvicorn | Gunicorn
-- | -- | --
Total Requests | 14,714 | 14,726
Total Failures | 0 | 13
Average Response Time | 12.05 ms | 13.46 ms
Min Response Time | 7 ms | 1 ms
Max Response Time | 195 ms | 216 ms
Average Size (bytes) | 4,608 | 4,603.93
Requests Per Second (RPS) | 49.05 | 49.09
Failures Per Second | 0 | 0.04


<hr>
<h3><strong>Observations</strong></h3>
<ol>
<li>
<p><strong>Response Times</strong>:</p>
<ul>
<li>Uvicorn demonstrates slightly lower average and maximum response times compared to Gunicorn.</li>
<li>Percentile analysis shows Uvicorn's response times are more consistent, with fewer extreme values at higher percentiles.</li>
</ul>
</li>
<li>
<p><strong>Failures</strong>:</p>
<ul>
<li>Uvicorn had <strong>no failures</strong>, whereas Gunicorn recorded <strong>13 failures</strong> caused by <code inline="""">RemoteDisconnected</code> errors. This could indicate potential issues in connection handling under load.</li>
</ul>
</li>
<li>
<p><strong>Performance Consistency</strong>:</p>
<ul>
<li>Uvicorn offers better consistency and reliability based on the above data.</li>
</ul>
</li>
</ol>
<hr>
</html>

potiuk on (2024-12-19 13:26:41 UTC): Nice!.

vatsrahul1001 (Assginee) on (2025-01-25 14:51:43 UTC): [Replacing gunicornmontor with uvicorn.run() #45103](https://github.com/apache/airflow/pull/45103) merged.

"
2588494198,issue,closed,not_planned,Extra long value in `op_args` and `op_kwargs` in OpenLineage event,"### Apache Airflow Provider(s)

openlineage

### Versions of Apache Airflow Providers

""openlineageAdapterVersion"": ""1.6.0"",
""version"": ""2.9.0""

### Apache Airflow version

2.9.0

### Operating System

mac

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

deployment details are not present

### What happened

I am getting extra long value in op_args and op_kwargs
inside run.facets.unknownSourceAttribute.unknownItems[]


### What you think should happen instead

there should be a flag which can be set to true incase we require op_args and op_kwargs value in the OL event.
default value of the flag should be false and only enabled when required.

The information in the op_kwargs and op_args is sensitive in some cases - getting emailIDs of people in the org.

### How to reproduce

There must be some way to get data into op_args and op_kwargs. I am not really sure on how to reproduce the issue.
The data in these keys is huge.

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rahul-madaan,2024-10-15 11:35:10+00:00,[],2024-10-15 12:59:40+00:00,2024-10-15 12:59:40+00:00,https://github.com/apache/airflow/issues/43031,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:openlineage', 'AIP-53')]","[{'comment_id': 2413825340, 'issue_id': 2588494198, 'author': 'kacpermuda', 'body': 'Hey, have you been able to see if the `include_full_task_info` [config](https://airflow.apache.org/docs/apache-airflow-providers-openlineage/stable/configurations-ref.html#include-full-task-info) introduced in provider 1.10 can help you out? It should limit the number of fields included in the event.\r\n\r\nEDIT:\r\nMy bad. In 1.7 we removed most of large fields from the event by default. In 1.10 we added a config that lets you include them in the event, if you need them (false by default).\r\n\r\nSo the solution here would be to upgrade the OL provider, i think.', 'created_at': datetime.datetime(2024, 10, 15, 12, 49, 4, tzinfo=datetime.timezone.utc)}]","kacpermuda on (2024-10-15 12:49:04 UTC): Hey, have you been able to see if the `include_full_task_info` [config](https://airflow.apache.org/docs/apache-airflow-providers-openlineage/stable/configurations-ref.html#include-full-task-info) introduced in provider 1.10 can help you out? It should limit the number of fields included in the event.

EDIT:
My bad. In 1.7 we removed most of large fields from the event by default. In 1.10 we added a config that lets you include them in the event, if you need them (false by default).

So the solution here would be to upgrade the OL provider, i think.

"
2588068577,issue,closed,completed,add display_name to rendered fields for Vertex AI training operators,"### Description

I would like the 'display_name' arg be added to the list of rendered_fields for the Vertex AI CreateHyperparameterTuningJobOperator and CreateCustomContainerTrainingJobOperator operators.

### Use case/motivation

I would like to be able to dynamically build the display_name of my custom Vertex AI (hyperparameter tuning) training jobs. At the moment, I have to create a custom operator that adds the display_name to the list of rendered fields to achieve this. I'd rather just use the base operators directly.


### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",WSHoekstra,2024-10-15 08:45:32+00:00,['WSHoekstra'],2024-10-17 07:43:56+00:00,2024-10-17 07:43:56+00:00,https://github.com/apache/airflow/issues/43027,"[('provider:google', 'Google (including GCP) related issues'), ('kind:feature', 'Feature Requests'), ('good first issue', '')]","[{'comment_id': 2413267499, 'issue_id': 2588068577, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 15, 8, 45, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415188463, 'issue_id': 2588068577, 'author': 'potiuk', 'body': 'Feel free to work on it - assigned you', 'created_at': datetime.datetime(2024, 10, 15, 21, 20, 8, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-15 08:45:35 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-10-15 21:20:08 UTC): Feel free to work on it - assigned you

"
2587465537,issue,open,,Option to hide Task Groups from Gantt chart,"### Description

(Written as of Airflow 2.10.2)

Hi
I recently worked with a DAG that has 120+ tasks over 40+ nested Task Groups
Gantt chart for such dags looks somewhat hard to read because it displays not only tasks, but Task Groups as well - all hierarchy from the table on the left is shown there, too. So it's getting hard to visually check which task has the longest execution time among longer Task Groups' lines - and the goal of Gantt chart is missing

![image](https://github.com/user-attachments/assets/42886ff9-3990-4fd6-b188-dc7c263636a7)

So, my suggestion is a some kind of UI checkbox that can hide Task Groups' lines in the Gantt chart, so only tasks' timeline could be pictured there

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Nick-Nal,2024-10-15 02:47:08+00:00,[],2024-11-02 20:01:25+00:00,,https://github.com/apache/airflow/issues/43019,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2453112204, 'issue_id': 2587465537, 'author': 'jscheffl', 'body': 'Thanks for the report. In Airflow 2 line we will only fix bugs and currently focus on Airflow 3 which will 100% re-write the UI. I propose to put this on the feature wish list.\r\n\r\nIf you like you could contribute this feature as well - whereas today the GANTT chart feature is not implemented in the new UI.', 'created_at': datetime.datetime(2024, 11, 2, 20, 1, 19, tzinfo=datetime.timezone.utc)}]","jscheffl on (2024-11-02 20:01:19 UTC): Thanks for the report. In Airflow 2 line we will only fix bugs and currently focus on Airflow 3 which will 100% re-write the UI. I propose to put this on the feature wish list.

If you like you could contribute this feature as well - whereas today the GANTT chart feature is not implemented in the new UI.

"
2586330230,issue,closed,completed,Extract dag definition object models to Task SDK - Phase 1,Everything that is needed to represent DAG and tasks should be iteratively moved from airflow core codebase to Task SDK,kaxil,2024-10-14 15:05:36+00:00,['ashb'],2024-10-30 18:20:57+00:00,2024-10-30 18:20:56+00:00,https://github.com/apache/airflow/issues/43011,"[('area:core', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]","[{'comment_id': 2411541982, 'issue_id': 2586330230, 'author': 'kaxil', 'body': 'WIP at https://github.com/apache/airflow/tree/task-sdk-first-code', 'created_at': datetime.datetime(2024, 10, 14, 15, 6, 21, tzinfo=datetime.timezone.utc)}]","kaxil (Issue Creator) on (2024-10-14 15:06:21 UTC): WIP at https://github.com/apache/airflow/tree/task-sdk-first-code

"
2586306174,issue,open,,Docs: Replace `airflow.sdk.definitions` to `airflow.sdk` for public facing docs,"Currently the code lives in the following, which is right
```python
from airflow.sdk.definitions.dag import DAG as DAG
```

but we want users to just see `from airflow.sdk import DAG`

So we might have to write a Sphinx extension to not show first one and just second one (intended for public usage)",kaxil,2024-10-14 14:56:21+00:00,['Bowrna'],2024-11-11 13:03:18+00:00,,https://github.com/apache/airflow/issues/43010,"[('kind:documentation', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]","[{'comment_id': 2411552856, 'issue_id': 2586306174, 'author': 'Bowrna', 'body': 'can i pick this one @kaxil ?', 'created_at': datetime.datetime(2024, 10, 14, 15, 10, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2411664761, 'issue_id': 2586306174, 'author': 'kaxil', 'body': 'Hey @Bowrna , this one is still a tricky one and will evolve based on https://github.com/apache/airflow/issues/43011\r\n\r\nBut once https://github.com/apache/airflow/issues/43011 is done, absolutely.', 'created_at': datetime.datetime(2024, 10, 14, 15, 57, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2411810398, 'issue_id': 2586306174, 'author': 'Bowrna', 'body': '@kaxil let me keep an eye on that one.', 'created_at': datetime.datetime(2024, 10, 14, 17, 8, 27, tzinfo=datetime.timezone.utc)}]","Bowrna (Assginee) on (2024-10-14 15:10:10 UTC): can i pick this one @kaxil ?

kaxil (Issue Creator) on (2024-10-14 15:57:59 UTC): Hey @Bowrna , this one is still a tricky one and will evolve based on https://github.com/apache/airflow/issues/43011

But once https://github.com/apache/airflow/issues/43011 is done, absolutely.

Bowrna (Assginee) on (2024-10-14 17:08:27 UTC): @kaxil let me keep an eye on that one.

"
2586288266,issue,closed,completed,"Create simple ""Task API"" server based on FastAPI","Need to work with Pierre on this to work out the right code and URI layout for this.

I think there have been some people who want to run them separately (it was certainly asked about by someone in my talk), so I think they should be structured as two separate FastAPI/ASGI apps that can optionally be run in a single binary (i.e. ASGI ""mounting"" etc.). At this stage I'm thinking that we should not separate out the dependencies/make them stand-alone projects (that can happen later if there proves to be enough desire)",kaxil,2024-10-14 14:50:41+00:00,['kaxil'],2024-10-17 10:44:49+00:00,2024-10-17 10:44:49+00:00,https://github.com/apache/airflow/issues/43009,"[('area:API', ""Airflow's REST/HTTP API""), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]","[{'comment_id': 2412735044, 'issue_id': 2586288266, 'author': 'potiuk', 'body': ""I think it really depends on how we want to treat providers - this is the big difference of dependencies. Technically - if we solve base links and connection issue, we could have the same dependencies for both - none of the task api calls nor web API should need providers at all in this case. On the other hand, if we don't solve it, then both will have providers if they are run together - but task API technically could be separate and have no provider dependencies. \r\n\r\nI think we should aim on having the same dependencies in both for simplicity, and we should see if we can remove providers from both - but this can also be done later - either before 3.0 or after - this only changes security property of the deployment, so it won't be a breaking change."", 'created_at': datetime.datetime(2024, 10, 15, 3, 4, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2414008401, 'issue_id': 2586288266, 'author': 'kaxil', 'body': 'Yea, we absolutely need to solve Operator links & Connection deps. Jens & Vikram have a proposal. One of the last dev calls ([Sep 19](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+3+Dev+call%3A+Meeting+Notes#Airflow3Devcall:MeetingNotes-Summary.9)) we discussed about routing it through an API (and stored it in DB):', 'created_at': datetime.datetime(2024, 10, 15, 14, 1, 12, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-10-15 03:04:25 UTC): I think it really depends on how we want to treat providers - this is the big difference of dependencies. Technically - if we solve base links and connection issue, we could have the same dependencies for both - none of the task api calls nor web API should need providers at all in this case. On the other hand, if we don't solve it, then both will have providers if they are run together - but task API technically could be separate and have no provider dependencies. 

I think we should aim on having the same dependencies in both for simplicity, and we should see if we can remove providers from both - but this can also be done later - either before 3.0 or after - this only changes security property of the deployment, so it won't be a breaking change.

kaxil (Issue Creator) on (2024-10-15 14:01:12 UTC): Yea, we absolutely need to solve Operator links & Connection deps. Jens & Vikram have a proposal. One of the last dev calls ([Sep 19](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+3+Dev+call%3A+Meeting+Notes#Airflow3Devcall:MeetingNotes-Summary.9)) we discussed about routing it through an API (and stored it in DB):

"
2585465348,issue,closed,completed,"""Main"" tip caching of CI image is broken","It looks like after the provider move in #42505, ""main"" tip caching for airflow is broken. Before the change installing airflow had use two stages caching:

1) Airflow has been installed from main first (which allowed to perform pre-installation without invalidation of docker layer)
2) Then additional installation was done after copying latest sources and pyproject.toml/hatch

This allowed for very fast rebuilds of the image using remote cache built on ghcr.io, when only source files or dependencies of airlfow changed.

It usually took 2-5 seconds to rebuild such image. Currently it looks like second installation step is running again full installation of airlfow (it takes about 2 minutes), which means that likely some mechanism of caching is broken.

![Image](https://github.com/user-attachments/assets/d4ef2d3e-4136-4586-ac9b-1b0b7b50eb6b)

We should review and updated Dockerfile.ci and scripts to bring the speed back.",potiuk,2024-10-14 09:51:34+00:00,['potiuk'],2024-12-29 21:58:28+00:00,2024-12-29 21:58:28+00:00,https://github.com/apache/airflow/issues/42999,"[('area:CI', ""Airflow's tests and continious integration"")]","[{'comment_id': 2416058277, 'issue_id': 2585465348, 'author': 'potiuk', 'body': 'cc: @ashb @kaxil @gopidesupavan @romsharon98 @shahar1 @eladkal @jscheffl. \r\n\r\nI am slowly looking at the ways how we can again speed up the CI image build and caching.\r\nOne of the reasons I **thought** caching is not as good is the UV version bump. Whenever we increase the UV version it will invalidate the cache layer **just** before installing airflow from the branch tip.\r\n\r\nStarting from here https://github.com/apache/airflow/blob/main/Dockerfile.ci#L1281\r\n\r\n```Dockefile\r\nARG AIRFLOW_PIP_VERSION=24.2\r\nARG AIRFLOW_UV_VERSION=0.4.22\r\nARG AIRFLOW_USE_UV=""true""\r\n\r\nRUN echo ""Airflow version: ${AIRFLOW_VERSION}""\r\n\r\n# Copy all scripts required for installation - changing any of those should lead to\r\n# rebuilding from here\r\nCOPY --from=scripts install_packaging_tools.sh install_airflow_dependencies_from_branch_tip.sh \\\r\n    common.sh /scripts/docker/\r\nRUN bash /scripts/docker/install_packaging_tools.sh; \\\r\n    if [[ ${AIRFLOW_PRE_CACHED_PIP_PACKAGES} == ""true"" ]]; then \\\r\n        bash /scripts/docker/install_airflow_dependencies_from_branch_tip.sh; \\\r\n    fi\r\n```\r\n\r\nWhen we change default UV_VERSION - all the layers below that get invalidated - so the COPY that copies the script and installing dependencies from the `main` branch tip will run again and reinstalling of the whole airlfow from ""main"" branch runs from scratch. Even with `uv` where it is as fast as possible, it takes 180s on my machine (3 minutes) - which is pretty slow. And the only reason in this case is that we changed the UV version.\r\n\r\nI think about optimizing it a bit - since UV version is changing so fast - maybe we can optimize it  by installing ""latest"" version of UV for the ""branch tip"" installation and only then install the ""fixed"" version of UV.\r\n\r\nWe are fixing UV version in order to get stability in place - there is a risk that UV upgrade will break things (happened in the past) as they are ""moving fast and break things"". So fixing UV in the image (and manual updates and merging after PR passes) makes sense - but for the optimisation purpose, it might make sense that branch tip installation is happening with latest UV without specifying version.\r\n\r\nThis should be generally quite save and stable. The ""install_airlfow_from_branch_tip"" is only optimization of installation - preinstalling airflow from ""some"" good version of airflow. If we do not change UV_VERSION before it, the layer would usually not get invalidated for builds - because there is nothing that could trigger the invalidation (no other previous Dockerfile lines would change) - so once such a build succeeds in main and the remote cache is updated, it will NOT reinstall uv with latest version - it will keep the uv installed in the same version it succeed last time - because the cache will not get invalidated.\r\n\r\nSo the first time when the cache will be build it will roughly work this way:\r\n\r\n1) install latest UV version\r\n2) install airlfow from the latest main branch (this takes 3 minutes)\r\n3) fix UV to the version (say 0.4.22) and install it\r\n4) COPY airflow sources\r\n5) proceed with installing airlfow from sources (incremental)\r\n \r\nThen whenever we have modified airflow sources - for all the subsequent image build operations, if base python version and no Dockerfile above those line change and no UV version change - it will reuse the cache effectively:\r\n\r\n1) -> from cache\r\n2) -> from cache \r\n3) -> from cache\r\n4) COPY airflow sources\r\n5) proceed with installing airlfow from sources (incremental)\r\n\r\nIf we update UV to 0.4.23 for example\r\n\r\n1) -> from cache\r\n2) -> from cache (this is where we save 3 minutes) \r\n3) fix UV to the version (say 0.4.23) and install it\r\n4) COPY airflow sources\r\n5) proceed with installing airlfow from sources (incremental)\r\n\r\nThis means that we are saving 3 minutes for rebuilds of CI image (locally and on CI) - when only UV version change.\r\n\r\nWDYT?', 'created_at': datetime.datetime(2024, 10, 16, 8, 16, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2416391221, 'issue_id': 2585465348, 'author': 'kaxil', 'body': 'Yeah supportive of that change -- any time we can speed up is a bonus!', 'created_at': datetime.datetime(2024, 10, 16, 10, 25, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2416534035, 'issue_id': 2585465348, 'author': 'shahar1', 'body': ""Sounds like a good idea, so I'm supportive of that change either :)"", 'created_at': datetime.datetime(2024, 10, 16, 11, 30, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2416871480, 'issue_id': 2585465348, 'author': 'gopidesupavan', 'body': 'Great approach! installing the latest UV version first and locking the fixed version afterward is a smart way to optimize the build process.', 'created_at': datetime.datetime(2024, 10, 16, 13, 38, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2417181619, 'issue_id': 2585465348, 'author': 'potiuk', 'body': 'Good. Let me attempt doing so', 'created_at': datetime.datetime(2024, 10, 16, 15, 30, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2417475217, 'issue_id': 2585465348, 'author': 'jscheffl', 'body': '+1 as well - and if latest uv versions fails as a workaround we would need to (temporariy) fix the UV version in lowest layer... which  is a small residual risk only.', 'created_at': datetime.datetime(2024, 10, 16, 17, 32, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2418076797, 'issue_id': 2585465348, 'author': 'potiuk', 'body': 'With #43101 rebuilding the image after bumping `uv` version on my Mac takes 30 seconds instead of 3 minutes.', 'created_at': datetime.datetime(2024, 10, 16, 22, 29, 14, tzinfo=datetime.timezone.utc)}]","potiuk (Issue Creator) on (2024-10-16 08:16:36 UTC): cc: @ashb @kaxil @gopidesupavan @romsharon98 @shahar1 @eladkal @jscheffl. 

I am slowly looking at the ways how we can again speed up the CI image build and caching.
One of the reasons I **thought** caching is not as good is the UV version bump. Whenever we increase the UV version it will invalidate the cache layer **just** before installing airflow from the branch tip.

Starting from here https://github.com/apache/airflow/blob/main/Dockerfile.ci#L1281

```Dockefile
ARG AIRFLOW_PIP_VERSION=24.2
ARG AIRFLOW_UV_VERSION=0.4.22
ARG AIRFLOW_USE_UV=""true""

RUN echo ""Airflow version: ${AIRFLOW_VERSION}""

# Copy all scripts required for installation - changing any of those should lead to
# rebuilding from here
COPY --from=scripts install_packaging_tools.sh install_airflow_dependencies_from_branch_tip.sh \
    common.sh /scripts/docker/
RUN bash /scripts/docker/install_packaging_tools.sh; \
    if [[ ${AIRFLOW_PRE_CACHED_PIP_PACKAGES} == ""true"" ]]; then \
        bash /scripts/docker/install_airflow_dependencies_from_branch_tip.sh; \
    fi
```

When we change default UV_VERSION - all the layers below that get invalidated - so the COPY that copies the script and installing dependencies from the `main` branch tip will run again and reinstalling of the whole airlfow from ""main"" branch runs from scratch. Even with `uv` where it is as fast as possible, it takes 180s on my machine (3 minutes) - which is pretty slow. And the only reason in this case is that we changed the UV version.

I think about optimizing it a bit - since UV version is changing so fast - maybe we can optimize it  by installing ""latest"" version of UV for the ""branch tip"" installation and only then install the ""fixed"" version of UV.

We are fixing UV version in order to get stability in place - there is a risk that UV upgrade will break things (happened in the past) as they are ""moving fast and break things"". So fixing UV in the image (and manual updates and merging after PR passes) makes sense - but for the optimisation purpose, it might make sense that branch tip installation is happening with latest UV without specifying version.

This should be generally quite save and stable. The ""install_airlfow_from_branch_tip"" is only optimization of installation - preinstalling airflow from ""some"" good version of airflow. If we do not change UV_VERSION before it, the layer would usually not get invalidated for builds - because there is nothing that could trigger the invalidation (no other previous Dockerfile lines would change) - so once such a build succeeds in main and the remote cache is updated, it will NOT reinstall uv with latest version - it will keep the uv installed in the same version it succeed last time - because the cache will not get invalidated.

So the first time when the cache will be build it will roughly work this way:

1) install latest UV version
2) install airlfow from the latest main branch (this takes 3 minutes)
3) fix UV to the version (say 0.4.22) and install it
4) COPY airflow sources
5) proceed with installing airlfow from sources (incremental)
 
Then whenever we have modified airflow sources - for all the subsequent image build operations, if base python version and no Dockerfile above those line change and no UV version change - it will reuse the cache effectively:

1) -> from cache
2) -> from cache 
3) -> from cache
4) COPY airflow sources
5) proceed with installing airlfow from sources (incremental)

If we update UV to 0.4.23 for example

1) -> from cache
2) -> from cache (this is where we save 3 minutes) 
3) fix UV to the version (say 0.4.23) and install it
4) COPY airflow sources
5) proceed with installing airlfow from sources (incremental)

This means that we are saving 3 minutes for rebuilds of CI image (locally and on CI) - when only UV version change.

WDYT?

kaxil on (2024-10-16 10:25:36 UTC): Yeah supportive of that change -- any time we can speed up is a bonus!

shahar1 on (2024-10-16 11:30:23 UTC): Sounds like a good idea, so I'm supportive of that change either :)

gopidesupavan on (2024-10-16 13:38:59 UTC): Great approach! installing the latest UV version first and locking the fixed version afterward is a smart way to optimize the build process.

potiuk (Issue Creator) on (2024-10-16 15:30:18 UTC): Good. Let me attempt doing so

jscheffl on (2024-10-16 17:32:01 UTC): +1 as well - and if latest uv versions fails as a workaround we would need to (temporariy) fix the UV version in lowest layer... which  is a small residual risk only.

potiuk (Issue Creator) on (2024-10-16 22:29:14 UTC): With #43101 rebuilding the image after bumping `uv` version on my Mac takes 30 seconds instead of 3 minutes.

"
2585322421,issue,closed,completed,Adding Elestio as deployment option Elestio,"### Description

Hey team,
I am Kaiwalya, Developer Advocate at Elestio. Elestio has been providing options of fully deploying and managing Airflow application as shown [here](https://elest.io/open-source/airflow). I think it would be a great idea if we can add it to official readme/documentation here.
🔔 In addition to this, if you are interested we provide partnership opportunities with tools we support by **revenue share** upon addition of this method in docs. If you would like to collaborate, just shoot me an email at [kaiwalya@elest.io](mailto:kaiwalya@elest.io) :)

### Use case/motivation

Example of one-click deploy button:

[![Deploy on Elestio](https://elest.io/images/logos/deploy-to-elestio-btn.png)](https://elest.io/open-source/airflow)

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",kaiwalyakoparkar,2024-10-14 08:57:49+00:00,['kaiwalyakoparkar'],2024-10-23 11:53:35+00:00,2024-10-23 11:49:35+00:00,https://github.com/apache/airflow/issues/42998,"[('kind:feature', 'Feature Requests'), ('kind:documentation', '')]","[{'comment_id': 2410505074, 'issue_id': 2585322421, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 14, 8, 57, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2411276767, 'issue_id': 2585322421, 'author': 'raphaelauv', 'body': 'you could PR on the doc , https://airflow.apache.org/ecosystem/', 'created_at': datetime.datetime(2024, 10, 14, 13, 28, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412667368, 'issue_id': 2585322421, 'author': 'potiuk', 'body': 'And just to comment  - any kind of revenue sharing/etc. opportunities are not possible. This is a vendor-neutral PMC of the Apache Software Foundation and since ASF is a registerd 501 c3 charity in Delaware (https://apache.org/foundation/) that has the motto of ""Releasing Software for Public Good"", the Airflow PMC cannot endorse or promote any vendors on the software the ASF releases. This is why Ecosystem page has a clear disclaimer that those are unverified, public links that are neither endorsed nor affiliated with Airflow PMC.\r\n\r\nAnd one - even more important thing - as a PMC member of the Apache Airlfow, my responsibility is to notice and follow up on branding issues when Airlfow registered trademark is used by 3rd parties.\r\n\r\nYour website is violating branding rules and you MUST fix it, otherwise we will involve Apache Software Foundation trademarks team to follow up with you - you should fix it promptly.\r\n\r\nAirflow (R) is a registered trademark of the Apache Software Foundation and if you want to refer or use the trademark you need to follow the ASF Branding policy. The policy is here https://apache.org/foundation/marks/ and you and your lawyers should read it in detail and you should follow all the detailed guidelines if you want to use ""Airflow"" trademark.\r\n\r\nExample things that are wrong here https://elest.io/open-source/airflow \r\n\r\n* The wording suggest that Airlfow is ""by Elestio"" - this is huge misuse, Airlfow is an Apache project and suggesting that other entity owns it (which this statement does) is strictly forbidden and ASF trademark team is very strict about following and enforcing it. The fact that Airflow is a registered trademark in US makes it very, very enforceable. You should refer to ""Apache Airflow"" in the first prominent place as ""Apache Airlfow (R)"" and name of your offering should be a ""nominative use"" - for example ""Managed Workflows for Apache Airlfow"" (MWAA) is a good name from Amazon because ""for Apache Airflow"" is a nominative use. ""Airlfow by Elestio"" is definitely not.\r\n\r\n* There are a number of other branding rules (again link here https://apache.org/foundation/marks/) - and you (and your lawyers) have to read it in detail and you have to apply all the rules explained there to your website.\r\n\r\nAlso you have other ASF projects in your managed application list (I did not check all but for example I saw Superset) - and you have exactly the same issues - so you should implement the changes for all ASF projects you manage. \r\n\r\nPlease do it promptly and let us know when done, to avoid unnecessary escalation and lawyer involvement. I will let the trademark team in the ASF know that we noticed the issue and get you some time to react but I want you to know that the ASF treats such violation of their trademarks very, very seriously, so my advice is to quickly comply with those requirements.', 'created_at': datetime.datetime(2024, 10, 15, 2, 8, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412669259, 'issue_id': 2585322421, 'author': 'potiuk', 'body': '@kaiwalyakoparkar - I renamed the issue to ""Elestio is violating ASF trademarks"" and assigned you to it.', 'created_at': datetime.datetime(2024, 10, 15, 2, 10, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2418820800, 'issue_id': 2585322421, 'author': 'kaiwalyakoparkar', 'body': 'Hey @potiuk, I appreciate your comment. I have forwarded your suggestions to the appropriate teams internally and we will resolve it at the soonest and get back to you here. Thanks :)', 'created_at': datetime.datetime(2024, 10, 17, 7, 53, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2420666196, 'issue_id': 2585322421, 'author': 'potiuk', 'body': 'Let us know please, this is pretty important topic that the ASF treats with all seriousness.', 'created_at': datetime.datetime(2024, 10, 17, 21, 51, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2421582812, 'issue_id': 2585322421, 'author': 'jbenguira', 'body': '> * s statement does) is strictly forbidden and ASF trademark team is very strict about following and enforcing it. The fact that Airflow is a registered trademark in US makes it very, very enforceable. You should refer to ""Apache Airflow"" in the first prominent place as ""Apache Airlfow (R)"" and name of your offering should be a ""nominative use"" - for example ""Managed Workflows for Apache Airlfow"" (MWAA) is a good name from Amazon because ""for Apache Airflow"" is a nominative use. ""Airlfow by Elestio"" is definitely not.\r\n\r\nHey @potiuk, I\'ve double checked and I\'m not seeing ANY places where Elestio claims it\'s ""The wording suggest that Airlfow is ""by Elestio"". \r\n\r\n**It\'s interesting how you built your argument by truncating ""fully managed by elestio"" to just ""by elestio""\r\nYour whole argument now seems to be done in BAD FAITH!** And I also noticed you (Jarek Potiuk) trying to make some damage to our brand by renaming this incident to ""Elestio is violating"" without even discussing first with us or even consulting your legal team.\r\n\r\n**ON THE CONTRARY** it\'s very clearly indicated everywhere: ""Airflow, fully managed by Elestio"" which seems ok based on the allowed example you provided ""Managed Workflows for Apache Airlfow"" and we are linking directly to the official repo on the same page.\r\n\r\nPlease forward this case to your legal team so we can ensure our implementation is OK and not infringing anything. Our goal is to comply. In the meantime please stay professional instead of going to conclusions, you are not a lawyer nor a judge. So don\'t say we are not compliant when you are not in a position to decide that.', 'created_at': datetime.datetime(2024, 10, 18, 6, 55, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2426432261, 'issue_id': 2585322421, 'author': 'potiuk', 'body': 'First of all @jbenguira thanks for already responding to our request - even if you failed to mention it in your response.\r\n\r\nI see that you already corrected some of the things I mentioned. Namely you started to use Apache Airflow, instead of Airflow. I did copy the previous screenshots with only ""Airflow"" - now it shows ""Apache Airflow"". Thanks for being so receptive:\r\n\r\nIt was:\r\n\r\n![image](https://github.com/user-attachments/assets/1d331567-f955-4656-aa48-ebb89d3b7494)\r\n\r\nNow it is:\r\n\r\n![image](https://github.com/user-attachments/assets/cf064209-8021-4683-8b06-2fe73afcdf9a)\r\n\r\n\r\nWhich is way better.\r\n\r\nBut I am afraid it\'s not nearly enough.\r\n\r\nFirst of all there are other Apache Software Foundation projects that you have not corrected - for example https://elest.io/open-source/superset and https://elest.io/open-source/kafka (probably there is more - you should fix it in the same way for all the ASF projects).\r\n\r\nBut secondly - there are many more rules that I pointed out before that you still have to fix.\r\n\r\nI suggest you take a look at the ASF branding rules which I linked to above. The problem is @jbenguira that Airflow is not used in Nominative use https://en.wikipedia.org/wiki/Nominative_use - if you need your legal team to look at the ""nominative use"" definition, this is the right time. This is US legal doctrine, and since Airflow and other trademarks are registered trademarks in the US, you should follow it.\r\n\r\nIn short - you can use ""Powered by Apache Airflow"" or ""Managed Elastio **for** Apache Airflow"" - this is the ""nominative use"" case. Using ""Airflow managed by Elastio"" is not. I suggest to consult your legal team about the ""nominative use"" there. Also in the documents I linked you can clearly see the description of it:\r\n\r\n> What is nominative use?\r\n\r\n> Anyone can use ASF trademarks if that use of the trademark is nominative. The ""nominative use"" (or ""nominative fair use"") defense to trademark infringement is a legal doctrine that authorizes everyone (even commercial companies) to use another person\'s trademark as long as the use meets three requirements:\r\n\r\n> The product or service in question must be one not readily identifiable without use of the trademark (for example, it is not easy to identify Apache HadoopCertain ASF trademarks are reserved exclusive for official Apache Software Foundation activities. For example, ""ApacheCon"" is our exclusive trademark for our regular ASF conferences, and the ASF feather is intended for ASF use at events in which we participate.\r\n\r\n> ® software without using the trademark ""Hadoop"").\r\n\r\n> Only so much of the mark or marks may be used as is reasonably necessary to identify the product or service.\r\n\r\n> The organization using the mark must do nothing that would, in conjunction with the mark, suggest sponsorship or endorsement by the trademark holder.\r\n\r\n> The trademark nominative fair use defense is intended to encourage people to refer to trademarked goods and services by using the trademark itself. This trademark defense has nothing to do with copyright fair use and should not be confused with those rules.\r\n\r\n> What is the ""confusing similarity"" or ""likelihood of confusion"" test?\r\n\r\n> Some uses of another person\'s trademark are nominative fair use, but some uses are simply infringing. Indeed, if someone uses a trademark in a way that the relevant consuming public will likely be confused or mistaken about the source of a product or service sold or provided, likelihood of confusion exists and the mark has been infringed.\r\n\r\n> Note that, even if there is no likelihood of confusion, you may still be liable for using another company\'s trademark if you are blurring or tarnishing their mark under the state and/or federal dilution laws.\r\n\r\n> To avoid infringing ASF\'s marks, you should verify that your use of our marks is nominative and that you are not likely to confuse software consumers that your software is the same as ASF\'s software or is endorsed by ASF. This policy is already summarized in section 6 of the [Apache License](https://apache.org/licenses/) , and so it is a condition for your use of ASF software:\r\n\r\nAlso this is rather clear:\r\n\r\n> Using ASF Trademarks in software product branding\r\n\r\n> In general you may [not use ASF trademarks in any software product branding](https://apache.org/foundation/marks/faq/#products). However in very specific situations you may use the [Powered By naming form](https://apache.org/foundation/marks/faq/#poweredby) for software products.\r\n\r\nAnd the link here explains more https://apache.org/foundation/marks/faq/#poweredby - including the fact that you should not really use Airflow Logo without the ""powered by"" mark unless you get specific permission. You can request your permission by sending the request to trademarks@apache.org and again - I highly recommend you start discussing your case and permissions there, before we pass the case for them to handle. It would show that you want to play nicely with the community and that your intention is not take an unfair advantage of the trademarks owned by the Foundation.\r\n\r\nAlso this is very clear:\r\n\r\n> You needn\'t ask us for permission to use the ASF\'s graphics logos (the versions published on individual project\'s websites) on your own website solely as a hyperlink to the specific ASF project website or to [www.apache.org](https://apache.org/). The VP, Brand Management, a member of the Brand Management Committee, or the relevant Apache project\'s VP must [approve in writing](https://apache.org/foundation/marks/contact) all other uses of Apache Foo (and similar) graphic logos.\r\n\r\nThe graphic logo of yours does not link to Apache Airflow website and you need written permission if you want to use the logo for different purpose.\r\n\r\nAlso Apache Branding rules that I linked to explicitly expect you to use ""Apache Airflow (R)"" and link from the name directly to https://apache.airflow.org in the first prominent place you refer to it. Also you do not provide the required attributions in the footers or elsewhere: \r\n\r\nIn case you missed that in the branding rules, here are the right chapters quoted (https://apache.org/foundation/marks/faq/#attribution) \r\n\r\n\r\n> To properly attribute Apache marks, and to ensure that the volunteer communities that build Apache software get the credit they deserve, place prominent trademark attributions wherever you use Apache marks. On websites, add hyperlinks to the relevant project homepage and to the ASF. For example, to provide an attribution for Apache Hadoop and its yellow elephant logo (using basic HTML with hyperlinks):\r\n\r\n> Apache®, [Apache Hadoop, Hadoop®](http://hadoop.apache.org/), and the yellow elephant logo are either registered trademarks or trademarks of the [Apache Software Foundation](https://apache.org/) in the United States and/or other countries.\r\n\r\n> In text, use Apache Lucene® in the first and most prominent uses because it is a registered trademark; some other Apache project names like Apache Zookeeper™ are unregistered but are still considered trademarks of the ASF.\r\n\r\n\r\nThe branding rules are pretty clear and  pretty clearly your use is violating them - so I suggest that you read them closely and follow - those are just excerpts where your usage is clearly not following the rules, but maybe there are more', 'created_at': datetime.datetime(2024, 10, 21, 11, 40, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2426465870, 'issue_id': 2585322421, 'author': 'potiuk', 'body': '> It\'s interesting how you built your argument by truncating ""fully managed by elestio"" to just ""by elestio""\r\nYour whole argument now seems to be done in BAD FAITH! And I also noticed you (Jarek Potiuk) trying to make some damage to our brand by renaming this incident to ""Elestio is violating"" without even discussing first with us or even consulting your legal team.\r\n\r\nIt was in response to public issue you created which clearly violated the rules, so I just responded to it, and made it clear that this issue is really driver to fix the issues you have.\r\n\r\n> Please forward this case to your legal team so we can ensure our implementation is OK and not infringing anything. Our goal is to comply.\r\n\r\nThen please comply.\r\n\r\n>  In the meantime please stay professional instead of going to conclusions, you are not a lawyer nor a judge. So don\'t say we are not compliant when you are not in a position to decide that.\r\n\r\nOh. I am very much in the position to point it out. As a PMC member, it\'s not only my right, but also a responsibility to point out such clear violations, that are explicitly against what\'s written in the branding rules. \r\n\r\nIf you look here - reporting such issues is clearly within the expectations the ASF has for PMC members. https://www.apache.org/dev/pmc.html#brand-policy\r\n\r\n> PMCs SHALL ensure that they manage their projects\' brand and treat all Apache® marks properly as defined in the overview of\r\n[PMC Branding Responsibilities](https://www.apache.org/foundation/marks/responsibility) and the [Apache Project Branding Requirements](https://www.apache.org/foundation/marks/pmcs) for project websites.\r\n\r\n> Responsibly report misuses of Apache brands\r\nPMCs SHALL review use of their Apache project brand by third parties and follow the [Apache trademark use reporting guidelines](https://www.apache.org/foundation/marks/reporting.html) when appropriate.', 'created_at': datetime.datetime(2024, 10, 21, 11, 57, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2426563114, 'issue_id': 2585322421, 'author': 'potiuk', 'body': ""Also @jbenguira  I would really appreciate if you stop threatening me. Your original 'issue' was not an airflow iasuE at all it was  mostly a marketing campaign. Normally we report such issues as spam to GitHub and authors of such messages are usually quickly disabled and need to ask GitHub to reinstate their accounts - so renaming the issue to clarify the actual issue that came out of us seeing the message ( I.e. elestio violating branding rules) is actually better course of action that allows you to correct the issue in a more gentle way"", 'created_at': datetime.datetime(2024, 10, 21, 12, 42, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2426587129, 'issue_id': 2585322421, 'author': 'jbenguira', 'body': ""> Also @jbenguira I would really appreciate if you stop threatening me. Your original 'issue' was not an airflow iasuE at all it was mostly a marketing campaign. Normally we report such issues as spam to GitHub and authors of such messages are usually quickly disabled and need to ask GitHub to reinstate their accounts - so renaming the issue to clarify the actual issue that came out of us seeing the message ( I.e. elestio violating branding rules) is actually better course of action that allows you to correct the issue in a more gentle way\r\n\r\n@potiuk I didn't threaten you, I simply pointed how you turned this original post that was a partnership request with sharing part of our profits to apache into a a very aggressive attack on our company and trying to hurt our brand reputation by your actions to weaponize the title of this github issue.  \r\n\r\nMaybe you should read again the apache guidelines you cited before? It seems you are not following them! (https://www.apache.org/foundation/marks/reporting.html#pmcs)\r\nMore specifically:\r\n- Discuss questions in private: We recommend discussing these issues in private because it is easier to clear up many unintentional misuses of marks in this way. \r\n- The intent is to send a polite, non-confrontational email to the infringing party, reminding them of the proper treatment of our trademarks. Do not imply any legal action or suggest that lawyers need to be involved\r\n\r\nAbout the other points you cited, as I said in my first answer we intend to be compliant and are working on it."", 'created_at': datetime.datetime(2024, 10, 21, 12, 52, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2426792469, 'issue_id': 2585322421, 'author': 'potiuk', 'body': '> @potiuk I didn\'t threaten you, I simply pointed how you turned this original post that was a partnership request with sharing part of our profits to apache into a a very aggressive attack on our company and trying to hurt our brand reputation by your actions to weaponize the title of this github issue.\r\n\r\nMaybe you have not read the instruction that were printed to you when you opened the marketing issue here. Issues are for reporitng any issues with Apache Airflow, not about promoting your company and product, and not about partnership with Apache - which as I explained clearly is not how things work here - you apparently have not understood what Apache Software Foundation is. So rather than reporting it, I gently pointed out - very objectively and without a slight hint of being aggreesive all the issues you should fix - in all the messages of mine there is no aggression, just objective facts.\r\n\r\nAnd well, it\'s you who started to publicly post lins to the website that clearly violates the rules in an attempt to drag traffic to your site, so I think pointing it out in the same issue, made a lot of sense. I would definitely do it in private if I noticed that accidentally - but here I had few choices:\r\n\r\n* report your messasge as spam\r\n* delete it altogether (which I think would be quite aggressive way of protecting people here for getting into the trademark-violating site\r\n* or point it out objectively\r\n\r\nWhich I think I did.\r\n\r\n> About the other points you cited, as I said in my first answer we intend to be compliant and are working on it.\r\n\r\nI sincerely hope it will happen. But your message was different - you wrote ""we do not think we are doing wrong"" and ""you are not qualified to talk to me about it"", so I am a little confused what intent of your message was.', 'created_at': datetime.datetime(2024, 10, 21, 14, 7, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2426938105, 'issue_id': 2585322421, 'author': 'jbenguira', 'body': '> And well, it\'s you who started to publicly post lins to the website that clearly violates the rules in an attempt to drag traffic to your site, so I think pointing it out in the same issue, made a lot of sense. I would definitely do it in private if I noticed that accidentally - but here I had few choices:\r\n> \r\n> * report your messasge as spam\r\n> * delete it altogether (which I think would be quite aggressive way of protecting people here for getting into the trademark-violating site\r\n> * or point it out objectively\r\n> \r\n\r\nYou are making the assumption it was a marketing campaign, it was clearly not, goal was to do a partnership and share revenues with the authors / foundations of open source software, since we don\'t have any contact yet we opened a github issue to try contact the authors.\r\n\r\nIt would be clearly preferable to delete this issue when it\'s resolved (as I said we are working on others points related to airflow and the other apache software we support), If you are not trying to weaponize this ticket as you claim and you are acting in good faith you will agree to delete this issue once the points are resolved? \r\n\r\n> \r\n> > About the other points you cited, as I said in my first answer we intend to be compliant and are working on it.\r\n> \r\n> I sincerely hope it will happen. But your message was different - you wrote ""we do not think we are doing wrong"" and ""you are not qualified to talk to me about it"", so I am a little confused what intent of your message was.\r\n\r\nMaybe read my messages again? **I said clearly twice already that we want to be compliant and are working on it.** \r\n\r\nPlease read AGAIN apache guidelines and apply their recommendations.\r\n- ""The intent is to send a polite, non-confrontational"" **this is NOT at all what you are doing here.**  \r\n- We recommend discussing these issues in private because it is easier to clear up many unintentional misuses of marks in this way. **This is obviously preferable to an approach that causes unnecessary damage to our brand and reputation. As you have already identified, when our unintentional oversights were brought to our attention we immediately commenced addressing and rectifying them**', 'created_at': datetime.datetime(2024, 10, 21, 14, 59, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2426955510, 'issue_id': 2585322421, 'author': 'potiuk', 'body': ""> It would be clearly preferable to delete this issue when it's resolved (as I said we are working on others points related to airflow and the other apache software we support), If you are not trying to weaponize this ticket as you claim and you are acting in good faith you will agree to delete this issue once the points are resolved?\r\n\r\nI am happy to do it when the issue is solved"", 'created_at': datetime.datetime(2024, 10, 21, 15, 6, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2426977662, 'issue_id': 2585322421, 'author': 'jbenguira', 'body': ""> I am happy to do it when the issue is solved\r\n\r\nGreat, I'll let you know when the changes are completed."", 'created_at': datetime.datetime(2024, 10, 21, 15, 14, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2430036293, 'issue_id': 2585322421, 'author': 'jbenguira', 'body': 'Hey @potiuk \r\nWe have reviewed and fixed our page for Apache Airflow and also for other Apache software\r\nPlease have a look when you can', 'created_at': datetime.datetime(2024, 10, 22, 19, 5, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2431865135, 'issue_id': 2585322421, 'author': 'potiuk', 'body': 'This is fantastic. Thank you - I have no more issues withe the page you addressed all my concerns, I will delete/hide that issue then.\r\n\r\nIf you want to be absolutely sure you - now or in the future - that you are doing the right thing, reach out to treademarks@apache.org please and ask them for opinion', 'created_at': datetime.datetime(2024, 10, 23, 11, 49, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2431868838, 'issue_id': 2585322421, 'author': 'potiuk', 'body': 'And thank you for being so receptive and following all the guidelines and rules. Much appreciated.', 'created_at': datetime.datetime(2024, 10, 23, 11, 50, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2431879052, 'issue_id': 2585322421, 'author': 'potiuk', 'body': 'I cannot delete the issue myself I closed it and restored the original title. If you want the issue to be deleted I will need to as the ASF infrastructure - who own the repo. But I think closed like that might be good - it will show you are receptive to the ASF requirements, so this one is ""good"" eventually.', 'created_at': datetime.datetime(2024, 10, 23, 11, 53, 34, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-14 08:57:53 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

raphaelauv on (2024-10-14 13:28:04 UTC): you could PR on the doc , https://airflow.apache.org/ecosystem/

potiuk on (2024-10-15 02:08:19 UTC): And just to comment  - any kind of revenue sharing/etc. opportunities are not possible. This is a vendor-neutral PMC of the Apache Software Foundation and since ASF is a registerd 501 c3 charity in Delaware (https://apache.org/foundation/) that has the motto of ""Releasing Software for Public Good"", the Airflow PMC cannot endorse or promote any vendors on the software the ASF releases. This is why Ecosystem page has a clear disclaimer that those are unverified, public links that are neither endorsed nor affiliated with Airflow PMC.

And one - even more important thing - as a PMC member of the Apache Airlfow, my responsibility is to notice and follow up on branding issues when Airlfow registered trademark is used by 3rd parties.

Your website is violating branding rules and you MUST fix it, otherwise we will involve Apache Software Foundation trademarks team to follow up with you - you should fix it promptly.

Airflow (R) is a registered trademark of the Apache Software Foundation and if you want to refer or use the trademark you need to follow the ASF Branding policy. The policy is here https://apache.org/foundation/marks/ and you and your lawyers should read it in detail and you should follow all the detailed guidelines if you want to use ""Airflow"" trademark.

Example things that are wrong here https://elest.io/open-source/airflow 

* The wording suggest that Airlfow is ""by Elestio"" - this is huge misuse, Airlfow is an Apache project and suggesting that other entity owns it (which this statement does) is strictly forbidden and ASF trademark team is very strict about following and enforcing it. The fact that Airflow is a registered trademark in US makes it very, very enforceable. You should refer to ""Apache Airflow"" in the first prominent place as ""Apache Airlfow (R)"" and name of your offering should be a ""nominative use"" - for example ""Managed Workflows for Apache Airlfow"" (MWAA) is a good name from Amazon because ""for Apache Airflow"" is a nominative use. ""Airlfow by Elestio"" is definitely not.

* There are a number of other branding rules (again link here https://apache.org/foundation/marks/) - and you (and your lawyers) have to read it in detail and you have to apply all the rules explained there to your website.

Also you have other ASF projects in your managed application list (I did not check all but for example I saw Superset) - and you have exactly the same issues - so you should implement the changes for all ASF projects you manage. 

Please do it promptly and let us know when done, to avoid unnecessary escalation and lawyer involvement. I will let the trademark team in the ASF know that we noticed the issue and get you some time to react but I want you to know that the ASF treats such violation of their trademarks very, very seriously, so my advice is to quickly comply with those requirements.

potiuk on (2024-10-15 02:10:30 UTC): @kaiwalyakoparkar - I renamed the issue to ""Elestio is violating ASF trademarks"" and assigned you to it.

kaiwalyakoparkar (Issue Creator) on (2024-10-17 07:53:18 UTC): Hey @potiuk, I appreciate your comment. I have forwarded your suggestions to the appropriate teams internally and we will resolve it at the soonest and get back to you here. Thanks :)

potiuk on (2024-10-17 21:51:18 UTC): Let us know please, this is pretty important topic that the ASF treats with all seriousness.

jbenguira on (2024-10-18 06:55:53 UTC): Hey @potiuk, I've double checked and I'm not seeing ANY places where Elestio claims it's ""The wording suggest that Airlfow is ""by Elestio"". 

**It's interesting how you built your argument by truncating ""fully managed by elestio"" to just ""by elestio""
Your whole argument now seems to be done in BAD FAITH!** And I also noticed you (Jarek Potiuk) trying to make some damage to our brand by renaming this incident to ""Elestio is violating"" without even discussing first with us or even consulting your legal team.

**ON THE CONTRARY** it's very clearly indicated everywhere: ""Airflow, fully managed by Elestio"" which seems ok based on the allowed example you provided ""Managed Workflows for Apache Airlfow"" and we are linking directly to the official repo on the same page.

Please forward this case to your legal team so we can ensure our implementation is OK and not infringing anything. Our goal is to comply. In the meantime please stay professional instead of going to conclusions, you are not a lawyer nor a judge. So don't say we are not compliant when you are not in a position to decide that.

potiuk on (2024-10-21 11:40:48 UTC): First of all @jbenguira thanks for already responding to our request - even if you failed to mention it in your response.

I see that you already corrected some of the things I mentioned. Namely you started to use Apache Airflow, instead of Airflow. I did copy the previous screenshots with only ""Airflow"" - now it shows ""Apache Airflow"". Thanks for being so receptive:

It was:

![image](https://github.com/user-attachments/assets/1d331567-f955-4656-aa48-ebb89d3b7494)

Now it is:

![image](https://github.com/user-attachments/assets/cf064209-8021-4683-8b06-2fe73afcdf9a)


Which is way better.

But I am afraid it's not nearly enough.

First of all there are other Apache Software Foundation projects that you have not corrected - for example https://elest.io/open-source/superset and https://elest.io/open-source/kafka (probably there is more - you should fix it in the same way for all the ASF projects).

But secondly - there are many more rules that I pointed out before that you still have to fix.

I suggest you take a look at the ASF branding rules which I linked to above. The problem is @jbenguira that Airflow is not used in Nominative use https://en.wikipedia.org/wiki/Nominative_use - if you need your legal team to look at the ""nominative use"" definition, this is the right time. This is US legal doctrine, and since Airflow and other trademarks are registered trademarks in the US, you should follow it.

In short - you can use ""Powered by Apache Airflow"" or ""Managed Elastio **for** Apache Airflow"" - this is the ""nominative use"" case. Using ""Airflow managed by Elastio"" is not. I suggest to consult your legal team about the ""nominative use"" there. Also in the documents I linked you can clearly see the description of it:












Also this is rather clear:



And the link here explains more https://apache.org/foundation/marks/faq/#poweredby - including the fact that you should not really use Airflow Logo without the ""powered by"" mark unless you get specific permission. You can request your permission by sending the request to trademarks@apache.org and again - I highly recommend you start discussing your case and permissions there, before we pass the case for them to handle. It would show that you want to play nicely with the community and that your intention is not take an unfair advantage of the trademarks owned by the Foundation.

Also this is very clear:


The graphic logo of yours does not link to Apache Airflow website and you need written permission if you want to use the logo for different purpose.

Also Apache Branding rules that I linked to explicitly expect you to use ""Apache Airflow (R)"" and link from the name directly to https://apache.airflow.org in the first prominent place you refer to it. Also you do not provide the required attributions in the footers or elsewhere: 

In case you missed that in the branding rules, here are the right chapters quoted (https://apache.org/foundation/marks/faq/#attribution) 






The branding rules are pretty clear and  pretty clearly your use is violating them - so I suggest that you read them closely and follow - those are just excerpts where your usage is clearly not following the rules, but maybe there are more

potiuk on (2024-10-21 11:57:24 UTC): Your whole argument now seems to be done in BAD FAITH! And I also noticed you (Jarek Potiuk) trying to make some damage to our brand by renaming this incident to ""Elestio is violating"" without even discussing first with us or even consulting your legal team.

It was in response to public issue you created which clearly violated the rules, so I just responded to it, and made it clear that this issue is really driver to fix the issues you have.


Then please comply.


Oh. I am very much in the position to point it out. As a PMC member, it's not only my right, but also a responsibility to point out such clear violations, that are explicitly against what's written in the branding rules. 

If you look here - reporting such issues is clearly within the expectations the ASF has for PMC members. https://www.apache.org/dev/pmc.html#brand-policy

[PMC Branding Responsibilities](https://www.apache.org/foundation/marks/responsibility) and the [Apache Project Branding Requirements](https://www.apache.org/foundation/marks/pmcs) for project websites.

PMCs SHALL review use of their Apache project brand by third parties and follow the [Apache trademark use reporting guidelines](https://www.apache.org/foundation/marks/reporting.html) when appropriate.

potiuk on (2024-10-21 12:42:02 UTC): Also @jbenguira  I would really appreciate if you stop threatening me. Your original 'issue' was not an airflow iasuE at all it was  mostly a marketing campaign. Normally we report such issues as spam to GitHub and authors of such messages are usually quickly disabled and need to ask GitHub to reinstate their accounts - so renaming the issue to clarify the actual issue that came out of us seeing the message ( I.e. elestio violating branding rules) is actually better course of action that allows you to correct the issue in a more gentle way

jbenguira on (2024-10-21 12:52:29 UTC): @potiuk I didn't threaten you, I simply pointed how you turned this original post that was a partnership request with sharing part of our profits to apache into a a very aggressive attack on our company and trying to hurt our brand reputation by your actions to weaponize the title of this github issue.  

Maybe you should read again the apache guidelines you cited before? It seems you are not following them! (https://www.apache.org/foundation/marks/reporting.html#pmcs)
More specifically:
- Discuss questions in private: We recommend discussing these issues in private because it is easier to clear up many unintentional misuses of marks in this way. 
- The intent is to send a polite, non-confrontational email to the infringing party, reminding them of the proper treatment of our trademarks. Do not imply any legal action or suggest that lawyers need to be involved

About the other points you cited, as I said in my first answer we intend to be compliant and are working on it.

potiuk on (2024-10-21 14:07:17 UTC): Maybe you have not read the instruction that were printed to you when you opened the marketing issue here. Issues are for reporitng any issues with Apache Airflow, not about promoting your company and product, and not about partnership with Apache - which as I explained clearly is not how things work here - you apparently have not understood what Apache Software Foundation is. So rather than reporting it, I gently pointed out - very objectively and without a slight hint of being aggreesive all the issues you should fix - in all the messages of mine there is no aggression, just objective facts.

And well, it's you who started to publicly post lins to the website that clearly violates the rules in an attempt to drag traffic to your site, so I think pointing it out in the same issue, made a lot of sense. I would definitely do it in private if I noticed that accidentally - but here I had few choices:

* report your messasge as spam
* delete it altogether (which I think would be quite aggressive way of protecting people here for getting into the trademark-violating site
* or point it out objectively

Which I think I did.


I sincerely hope it will happen. But your message was different - you wrote ""we do not think we are doing wrong"" and ""you are not qualified to talk to me about it"", so I am a little confused what intent of your message was.

jbenguira on (2024-10-21 14:59:40 UTC): You are making the assumption it was a marketing campaign, it was clearly not, goal was to do a partnership and share revenues with the authors / foundations of open source software, since we don't have any contact yet we opened a github issue to try contact the authors.

It would be clearly preferable to delete this issue when it's resolved (as I said we are working on others points related to airflow and the other apache software we support), If you are not trying to weaponize this ticket as you claim and you are acting in good faith you will agree to delete this issue once the points are resolved? 


Maybe read my messages again? **I said clearly twice already that we want to be compliant and are working on it.** 

Please read AGAIN apache guidelines and apply their recommendations.
- ""The intent is to send a polite, non-confrontational"" **this is NOT at all what you are doing here.**  
- We recommend discussing these issues in private because it is easier to clear up many unintentional misuses of marks in this way. **This is obviously preferable to an approach that causes unnecessary damage to our brand and reputation. As you have already identified, when our unintentional oversights were brought to our attention we immediately commenced addressing and rectifying them**

potiuk on (2024-10-21 15:06:01 UTC): I am happy to do it when the issue is solved

jbenguira on (2024-10-21 15:14:13 UTC): Great, I'll let you know when the changes are completed.

jbenguira on (2024-10-22 19:05:13 UTC): Hey @potiuk 
We have reviewed and fixed our page for Apache Airflow and also for other Apache software
Please have a look when you can

potiuk on (2024-10-23 11:49:30 UTC): This is fantastic. Thank you - I have no more issues withe the page you addressed all my concerns, I will delete/hide that issue then.

If you want to be absolutely sure you - now or in the future - that you are doing the right thing, reach out to treademarks@apache.org please and ask them for opinion

potiuk on (2024-10-23 11:50:36 UTC): And thank you for being so receptive and following all the guidelines and rules. Much appreciated.

potiuk on (2024-10-23 11:53:34 UTC): I cannot delete the issue myself I closed it and restored the original title. If you want the issue to be deleted I will need to as the ASF infrastructure - who own the repo. But I think closed like that might be good - it will show you are receptive to the ASF requirements, so this one is ""good"" eventually.

"
2584481464,issue,closed,completed,Flaky test tests/ti_deps/deps/test_trigger_rule_dep.py,"This test is flaky:

https://github.com/apache/airflow/actions/runs/11317119713/job/31470297914

```python
=================================== FAILURES ===================================
_ test_setup_constraint_mapped_task_upstream_removed_and_success[3-True-removed] _

dag_maker = <dev.tests_common.pytest_plugin.dag_maker.<locals>.DagFactory object at 0x7f65c1117e20>
session = <sqlalchemy.orm.session.Session object at 0x7f65ba711d80>
get_mapped_task_dagrun = <function get_mapped_task_dagrun.<locals>._get_dagrun at 0x7f6578ac3250>
map_index = 3, flag_upstream_failed = True
expected_ti_state = <TaskInstanceState.REMOVED: 'removed'>

    @pytest.mark.parametrize(
        ""map_index, flag_upstream_failed, expected_ti_state"",
        [(2, True, None), (3, True, REMOVED), (4, True, REMOVED), (3, False, None)],
    )
    def test_setup_constraint_mapped_task_upstream_removed_and_success(
        dag_maker,
        session,
        get_mapped_task_dagrun,
        map_index,
        flag_upstream_failed,
        expected_ti_state,
    ):
        """"""
        Dynamically mapped setup task with successful and removed upstream tasks. Expect rule to be
        successful. State is set to REMOVED for map index >= n success
        """"""
        dr, _, setup_task = get_mapped_task_dagrun(add_setup_tasks=True)
    
        ti = dr.get_task_instance(task_id=""setup_3"", map_index=map_index, session=session)
        ti.task = setup_task
    
>       _test_trigger_rule(
            ti=ti,
            session=session,
            flag_upstream_failed=flag_upstream_failed,
            expected_ti_state=expected_ti_state,
        )

tests/ti_deps/deps/test_trigger_rule_dep.py:1518: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

ti = <TaskInstance: test_dag.setup_3 test map_index=3 [None]>
session = <sqlalchemy.orm.session.Session object at 0x7f65ba711d80>
flag_upstream_failed = True, wait_for_past_depends_before_skipping = False
expected_reason = '', expected_ti_state = <TaskInstanceState.REMOVED: 'removed'>

    def _test_trigger_rule(
        ti: TaskInstance,
        session: Session,
        flag_upstream_failed: bool,
        wait_for_past_depends_before_skipping: bool = False,
        expected_reason: str = """",
        expected_ti_state: TaskInstanceState | None = None,
    ) -> None:
        assert ti.state is None
        dep_statuses = tuple(
            TriggerRuleDep()._evaluate_trigger_rule(
                ti=ti,
                dep_context=DepContext(
                    flag_upstream_failed=flag_upstream_failed,
                    wait_for_past_depends_before_skipping=wait_for_past_depends_before_skipping,
                ),
                session=session,
            )
        )
        if expected_reason:
            assert len(dep_statuses) == 1
            assert not dep_statuses[0].passed
            assert expected_reason in dep_statuses[0].reason
        else:
>           assert not dep_statuses
E           assert not (TIDepStatus(dep_name='Trigger Rule', passed=False, reason=""Task's trigger rule 'all_success' requires all upstream ta...ailed=0, removed=4, done=10, success_setup=6, skipped_setup=0), upstream_task_ids={'setup_3', 'setup_1', 'setup_2'}""),)

tests/ti_deps/deps/test_trigger_rule_dep.py:1645: AssertionError
```",potiuk,2024-10-14 01:11:00+00:00,['gopidesupavan'],2024-11-30 13:21:03+00:00,2024-10-14 09:30:39+00:00,https://github.com/apache/airflow/issues/42990,"[('area:CI', ""Airflow's tests and continious integration"")]",[],
2584479494,issue,closed,completed,Some test dependencies have no lower bounds,"Some test dependencies have no ""lower bounds"" - this is not very good because ""lowest-direct"" dependency resolution might run much longer. Currently when uv is making the resolution, it prints warnings


- [x] warning: Missing version constraint (e.g., a lower bound) for `asgiref`
- [x] warning: Missing version constraint (e.g., a lower bound) for `requests-toolbelt`
- [x] warning: Missing version constraint (e.g., a lower bound) for `cloudpickle`
- [x] warning: Missing version constraint (e.g., a lower bound) for `python-ldap`
- [x] warning: Missing version constraint (e.g., a lower bound) for `plyvel`
- [x] warning: Missing version constraint (e.g., a lower bound) for `opentelemetry-exporter-prometheus`
- [x] warning: Missing version constraint (e.g., a lower bound) for `amqp`
- [x] warning: Missing version constraint (e.g., a lower bound) for `virtualenv`
- [x] warning: Missing version constraint (e.g., a lower bound) for `types-markdown`
- [x] warning: Missing version constraint (e.g., a lower bound) for `types-deprecated`
- [x] warning: Missing version constraint (e.g., a lower bound) for `types-pymysql`
- [x] warning: Missing version constraint (e.g., a lower bound) for `types-pyyaml`
- [x] warning: Missing version constraint (e.g., a lower bound) for `types-aiofiles`
- [x] warning: Missing version constraint (e.g., a lower bound) for `types-certifi`
- [x] warning: Missing version constraint (e.g., a lower bound) for `types-croniter`
- [x] warning: Missing version constraint (e.g., a lower bound) for `types-docutils`
- [x] warning: Missing version constraint (e.g., a lower bound) for `types-paramiko`
- [x] warning: Missing version constraint (e.g., a lower bound) for `types-protobuf`
- [x] warning: Missing version constraint (e.g., a lower bound) for `types-python-dateutil`
- [x] warning: Missing version constraint (e.g., a lower bound) for `types-python-slugify`
- [x] warning: Missing version constraint (e.g., a lower bound) for `types-pytz`
- [x] warning: Missing version constraint (e.g., a lower bound) for `types-redis`
- [x] warning: Missing version constraint (e.g., a lower bound) for `types-requests`
- [x] warning: Missing version constraint (e.g., a lower bound) for `types-setuptools`
- [x] warning: Missing version constraint (e.g., a lower bound) for `types-tabulate`
- [x] warning: Missing version constraint (e.g., a lower bound) for `types-termcolor`
- [x] warning: Missing version constraint (e.g., a lower bound) for `types-toml`
- [x] warning: Missing version constraint (e.g., a lower bound) for `ipykernel`
- [x] warning: Missing version constraint (e.g., a lower bound) for `pywinrm`
- [x] warning: Missing version constraint (e.g., a lower bound) for `scrapbook`


We should make sure that all those dependencies have reasonable lower-bounds . Ideally versions that are not too old (6 months?). We shoudl look at them individually though, to see if there might be potential issues when we are choosing the lower bounds.",potiuk,2024-10-14 01:08:25+00:00,['rawwar'],2024-11-02 10:11:17+00:00,2024-11-02 10:10:52+00:00,https://github.com/apache/airflow/issues/42989,"[('area:dependencies', 'Issues related to dependencies problems')]","[{'comment_id': 2409890369, 'issue_id': 2584479494, 'author': 'rawwar', 'body': '@potiuk , I think, ""lower bound"" was given as an example in the warnings.~~ \r\n\r\nFor example, asgiref is currently set to be `""asgiref>=2.3.0"",` in hatch_build.py.\r\n\r\n\r\n~~Looks like, the issue is with constraints.txt and that\'s where we need to put the lowerbound~~\r\n\r\n@potiuk, I\'m a bit confused here. All of the pins in constraints.txt use exact matches (`==`). Not sure where the issue is. I tried to set an upper bound to asgiref and yet uv is printing same warning', 'created_at': datetime.datetime(2024, 10, 14, 3, 50, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2410874018, 'issue_id': 2584479494, 'author': 'potiuk', 'body': '> @potiuk , I think, ""lower bound"" was given as an example in the warnings.~~\r\n> \r\n> For example, asgiref is currently set to be `""asgiref>=2.3.0"",` in hatch_build.py.\r\n> \r\n> ~Looks like, the issue is with constraints.txt and that\'s where we need to put the lowerbound~\r\n> \r\n> @potiuk, I\'m a bit confused here. All of the pins in constraints.txt use exact matches (`==`). Not sure where the issue is. I tried to set an upper bound to asgiref and yet uv is printing same warning\r\n\r\nInteresting. Worth taking a closer look. Constraints do not matter here. When we use ""lowest-direct"", we do not use constraints at all. Those are only used when we (or our users) want to reproducibly install airflow, but for `--resolution lowest-direct` we do not specify constraints (and it would make no sense) - we let `uv` lower all dependencies matching the requirements.\r\n\r\nI think the asgiref warning comes from http provider dependencies (see provider.yaml for http provider)\r\n\r\n```\r\ndependencies:\r\n  - apache-airflow>=2.8.0\r\n  # The 2.26.0 release of requests got rid of the chardet LGPL mandatory dependency, allowing us to\r\n  # release it as a requirement for airflow\r\n  - requests>=2.27.0,<3\r\n  - requests_toolbelt\r\n  - aiohttp>=3.9.2\r\n  - asgiref\r\n ```\r\n\r\nI think fix for that particular one will be to make sure all asgiref dependencies we have use the same lower-bound.\r\n\r\nBTW. It\'s also surprising to see asgiref as http provider dependencies - so maybe we should take a look if it is needed at all @rawwar :) ?', 'created_at': datetime.datetime(2024, 10, 14, 11, 3, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2410878654, 'issue_id': 2584479494, 'author': 'potiuk', 'body': 'BTW. I see those warnings as a great opportunity to guide us with reviewing some of our dependencies - because like in this case we might have some surprising and not entirely proper dependencies configured in some of our providers :)', 'created_at': datetime.datetime(2024, 10, 14, 11, 5, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2410907409, 'issue_id': 2584479494, 'author': 'kaxil', 'body': '> BTW. I see those warnings as a great opportunity to guide us with reviewing some of our dependencies - because like in this case we might have some surprising and not entirely proper dependencies configured in some of our providers :)\r\n\r\nAgreed', 'created_at': datetime.datetime(2024, 10, 14, 11, 17, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2410916163, 'issue_id': 2584479494, 'author': 'rawwar', 'body': ""> I think the asgiref warning comes from http provider dependencies (see provider.yaml for http provider)\r\n\r\nI tried this in the PR: #43001 and it still prints warning for asgiref.\r\n\r\n> BTW. It's also surprising to see asgiref as http provider dependencies - so maybe we should take a look if it is needed at all @rawwar :) ?\r\n\r\nWe only use it here([Link](https://github.com/apache/airflow/blob/7421af8d15643784ee961bf510f08f6707533309/providers/src/airflow/providers/http/hooks/http.py#L352)) . \r\n\r\nI will look into it and see if its possible to change this and remove the dependency."", 'created_at': datetime.datetime(2024, 10, 14, 11, 21, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415632368, 'issue_id': 2584479494, 'author': 'rawwar', 'body': '@potiuk , can you assign me this issue?', 'created_at': datetime.datetime(2024, 10, 16, 3, 8, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2416463577, 'issue_id': 2584479494, 'author': 'potiuk', 'body': '> @potiuk , can you assign me this issue?\r\n\r\nsure :) \r\n\r\nBTW. I can add you to the triage team so that you can do it yourself (and help with triage/setting labels as well) - WDYT @rawwar ?', 'created_at': datetime.datetime(2024, 10, 16, 10, 56, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2416467339, 'issue_id': 2584479494, 'author': 'potiuk', 'body': 'https://github.com/apache/airflow/pull/43074 ?', 'created_at': datetime.datetime(2024, 10, 16, 10, 58, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2424103117, 'issue_id': 2584479494, 'author': 'rawwar', 'body': '@potiuk , i think i misunderstood when you mentioned, ""we need to look into packages individually, to avoid issues""\n\nCan i raise PRs for miltiple package pins together? As long as tests pass, this should be fine right? \n\nRaising a PR for each package separately will take a lot of time to finish this issue.', 'created_at': datetime.datetime(2024, 10, 19, 17, 37, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2425605623, 'issue_id': 2584479494, 'author': 'jedcunningham', 'body': 'I don\'t see an issue with batching them.\r\n\r\n@potiuk I\'m slightly hesitant to set the minimum purely based on ""~6 months old"" though. Are we planning to moving those lower bounds for everything, on an ongoing basis?\r\n\r\nIf we do do it based on time alone (like in #43189 seemingly), we should definitely state there is no ""real"" reason for that being the minimum and we chose something arbitrary, for posterity sake.', 'created_at': datetime.datetime(2024, 10, 21, 5, 13, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2435684614, 'issue_id': 2584479494, 'author': 'potiuk', 'body': '> I don\'t see an issue with batching them.\r\n\r\nMe neither.\r\n\r\n> @potiuk I\'m slightly hesitant to set the minimum purely based on ""~6 months old"" though. Are we planning to moving those lower bounds for everything, on an ongoing basis?\r\n\r\nWe already have a working solution to detect if we are starting to use features that is not available in ""lowest"" supported version - there is the ""Lowest Dependency test"" that does it. So this exercise is really to get ""some"" baselines on those dependencies. I think we we will generally bring them up usually when we start using something that does not pass test with the lowest possible versions brought down for all dependencies (of partucular provider or airflow core).\r\n\r\nI am not tied to ""6 months"" but that sounded like a good baseline. If there is any other proposal how to get the baseline - I am happy to any reasonable proposal there @jedcunningham \r\n\r\nAnd generally we avoid to document lower-bounds - it\'s generally not needed, we pretty much never go down. We must document upper-bounds, but as long as lower-binding works, it\'s generally ok - unless of course someone has conflicting upper bound limit. But I look at those deps that we miss lower binds, and I don\'t think there is high risk for that.\r\n\r\nBut if there is any other proposal - I am all ears.', 'created_at': datetime.datetime(2024, 10, 24, 16, 1, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2435686651, 'issue_id': 2584479494, 'author': 'potiuk', 'body': ""BTW. We already have a number of some arbitrary lower-bounds - not documented - and I can't recall a single case where it cause problems."", 'created_at': datetime.datetime(2024, 10, 24, 16, 2, 16, tzinfo=datetime.timezone.utc)}]","rawwar (Assginee) on (2024-10-14 03:50:48 UTC): @potiuk , I think, ""lower bound"" was given as an example in the warnings.~~ 

For example, asgiref is currently set to be `""asgiref>=2.3.0"",` in hatch_build.py.


~~Looks like, the issue is with constraints.txt and that's where we need to put the lowerbound~~

@potiuk, I'm a bit confused here. All of the pins in constraints.txt use exact matches (`==`). Not sure where the issue is. I tried to set an upper bound to asgiref and yet uv is printing same warning

potiuk (Issue Creator) on (2024-10-14 11:03:54 UTC): Interesting. Worth taking a closer look. Constraints do not matter here. When we use ""lowest-direct"", we do not use constraints at all. Those are only used when we (or our users) want to reproducibly install airflow, but for `--resolution lowest-direct` we do not specify constraints (and it would make no sense) - we let `uv` lower all dependencies matching the requirements.

I think the asgiref warning comes from http provider dependencies (see provider.yaml for http provider)

```
dependencies:
  - apache-airflow>=2.8.0
  # The 2.26.0 release of requests got rid of the chardet LGPL mandatory dependency, allowing us to
  # release it as a requirement for airflow
  - requests>=2.27.0,<3
  - requests_toolbelt
  - aiohttp>=3.9.2
  - asgiref
 ```

I think fix for that particular one will be to make sure all asgiref dependencies we have use the same lower-bound.

BTW. It's also surprising to see asgiref as http provider dependencies - so maybe we should take a look if it is needed at all @rawwar :) ?

potiuk (Issue Creator) on (2024-10-14 11:05:30 UTC): BTW. I see those warnings as a great opportunity to guide us with reviewing some of our dependencies - because like in this case we might have some surprising and not entirely proper dependencies configured in some of our providers :)

kaxil on (2024-10-14 11:17:30 UTC): Agreed

rawwar (Assginee) on (2024-10-14 11:21:37 UTC): I tried this in the PR: #43001 and it still prints warning for asgiref.


We only use it here([Link](https://github.com/apache/airflow/blob/7421af8d15643784ee961bf510f08f6707533309/providers/src/airflow/providers/http/hooks/http.py#L352)) . 

I will look into it and see if its possible to change this and remove the dependency.

rawwar (Assginee) on (2024-10-16 03:08:24 UTC): @potiuk , can you assign me this issue?

potiuk (Issue Creator) on (2024-10-16 10:56:38 UTC): sure :) 

BTW. I can add you to the triage team so that you can do it yourself (and help with triage/setting labels as well) - WDYT @rawwar ?

potiuk (Issue Creator) on (2024-10-16 10:58:30 UTC): https://github.com/apache/airflow/pull/43074 ?

rawwar (Assginee) on (2024-10-19 17:37:25 UTC): @potiuk , i think i misunderstood when you mentioned, ""we need to look into packages individually, to avoid issues""

Can i raise PRs for miltiple package pins together? As long as tests pass, this should be fine right? 

Raising a PR for each package separately will take a lot of time to finish this issue.

jedcunningham on (2024-10-21 05:13:48 UTC): I don't see an issue with batching them.

@potiuk I'm slightly hesitant to set the minimum purely based on ""~6 months old"" though. Are we planning to moving those lower bounds for everything, on an ongoing basis?

If we do do it based on time alone (like in #43189 seemingly), we should definitely state there is no ""real"" reason for that being the minimum and we chose something arbitrary, for posterity sake.

potiuk (Issue Creator) on (2024-10-24 16:01:20 UTC): Me neither.


We already have a working solution to detect if we are starting to use features that is not available in ""lowest"" supported version - there is the ""Lowest Dependency test"" that does it. So this exercise is really to get ""some"" baselines on those dependencies. I think we we will generally bring them up usually when we start using something that does not pass test with the lowest possible versions brought down for all dependencies (of partucular provider or airflow core).

I am not tied to ""6 months"" but that sounded like a good baseline. If there is any other proposal how to get the baseline - I am happy to any reasonable proposal there @jedcunningham 

And generally we avoid to document lower-bounds - it's generally not needed, we pretty much never go down. We must document upper-bounds, but as long as lower-binding works, it's generally ok - unless of course someone has conflicting upper bound limit. But I look at those deps that we miss lower binds, and I don't think there is high risk for that.

But if there is any other proposal - I am all ears.

potiuk (Issue Creator) on (2024-10-24 16:02:16 UTC): BTW. We already have a number of some arbitrary lower-bounds - not documented - and I can't recall a single case where it cause problems.

"
2584045707,issue,closed,completed,AIP-84 Migrate XCom get entries public endpoint to FastAPI,"### Description

Migrate the XCom get entries public endpoint from connexion to FastAPI

### Use case/motivation

[AIP-84](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API)

### Related issues

Relates to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",michaeljs-c,2024-10-13 15:16:02+00:00,['michaeljs-c'],2024-11-26 20:21:11+00:00,2024-11-26 20:21:10+00:00,https://github.com/apache/airflow/issues/42980,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2488904668, 'issue_id': 2584045707, 'author': 'pierrejeambrun', 'body': 'Hello @michaeljs-c,\r\n\r\nAre you still working on this one ? Let me know if you need help to get started.', 'created_at': datetime.datetime(2024, 11, 20, 15, 33, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2498774731, 'issue_id': 2584045707, 'author': 'michaeljs-c', 'body': 'Hi @pierrejeambrun - PR is up now 🙂', 'created_at': datetime.datetime(2024, 11, 25, 18, 42, 11, tzinfo=datetime.timezone.utc)}]","pierrejeambrun on (2024-11-20 15:33:15 UTC): Hello @michaeljs-c,

Are you still working on this one ? Let me know if you need help to get started.

michaeljs-c (Issue Creator) on (2024-11-25 18:42:11 UTC): Hi @pierrejeambrun - PR is up now 🙂

"
2584006246,issue,closed,completed,AIP-84 Migrate XCom get entry public endpoint to FastAPI,"### Description

Migrate the XCom get entry public endpoint from connexion to FastAPI

### Use case/motivation

[AIP-84](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API)

### Related issues

Relates to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",michaeljs-c,2024-10-13 14:24:00+00:00,['michaeljs-c'],2024-11-12 09:04:38+00:00,2024-11-12 09:04:37+00:00,https://github.com/apache/airflow/issues/42978,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2408999998, 'issue_id': 2584006246, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 13, 14, 24, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2447433509, 'issue_id': 2584006246, 'author': 'pierrejeambrun', 'body': ""Hello @michaeljs-c,\r\n\r\nAny progress on this issue ? Don't hesitate if you need help to get started."", 'created_at': datetime.datetime(2024, 10, 30, 14, 59, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2447854707, 'issue_id': 2584006246, 'author': 'michaeljs-c', 'body': ""Hi @pierrejeambrun, thanks I'm almost finished with it, just working through CI"", 'created_at': datetime.datetime(2024, 10, 30, 17, 16, 14, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-13 14:24:03 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

pierrejeambrun on (2024-10-30 14:59:10 UTC): Hello @michaeljs-c,

Any progress on this issue ? Don't hesitate if you need help to get started.

michaeljs-c (Issue Creator) on (2024-10-30 17:16:14 UTC): Hi @pierrejeambrun, thanks I'm almost finished with it, just working through CI

"
2583223748,issue,open,,Add more metadata to Airflow Task SDK Project's pyproject.toml,"Since https://github.com/apache/airflow/pull/42904 is merged. Before we release the first version of that package, we should add more metadata including populating the README.md and things like licenses and such in [`pyproject.toml`](https://github.com/apache/airflow/blob/main/task_sdk/pyproject.toml)",kaxil,2024-10-12 16:00:43+00:00,['kaxil'],2024-11-11 13:03:17+00:00,,https://github.com/apache/airflow/issues/42962,"[('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2582289420,issue,closed,completed,Kubernetes task decorator default namespace behaves different than KubernetesPodOperator when in cluster,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.2

### What happened?

Given these settings for KuberneteseExecutor:

Section | Key | Value
-- | -- | --
kubernetes_executor | in_cluster | True |
kubernetes_executor | namespace | redacted_not_default |

the behavior of `@task.kubernetes()` vs `KubernetesOperator()` when not setting namespace differs:

- `@task.kubernetes()` defaults to `namespace=""default""`
- `KuberntesPodOperator()` defaults to `namespace=None` which uses the cluster namespace when `in_cluster` is true.

### What you think should happen instead?

`KubernetesPodOperator` and `@task.kubernetes` should use the in-cluster namespace by default (i.e., the worker pod's namespace).

### How to reproduce

1. Use a namespace that isn't `default`, let's call it `foobar` for this example
2. Set the service account to only allow pod API access to `foobar` namespace.
3. Run this DAG

    ```python
    with DAG(...):
        @task.kubernetes(image=""hello-world""):
        def foo():
            ...
        taskA = foo()
    
        taskB = KubernetesPodOperator(image=""hello-world"")
    ```

You'll see K8s API error for the wrong namespace. In my case, the KPO decorator is trying to access the `airflow` namespace despite the pod is running in the correct namespace

```
HTTP response body: {""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""pods is forbidden: User \""system:serviceaccount:airflow:airflow\"" cannot list resource \""pods\"" in API group \""\"" in the namespace \""airflow\"""",""reason"":""Forbidden"",""details"":{""kind"":""pods""},""code"":403}
```

Here's the KPO decorator task's pod spec:
```
metadata:
  annotations:
    dag_id: lensing_model_classification
    run_id: manual__2024-10-11T22:11:31.074729+00:00
    task_id: redacted
    try_number: '4'
  labels:
    airflow-worker: '0'
    airflow_version: 2.9.2
    dag_id: redacted
    kubernetes_executor: 'True'
    run_id: manual__2024-10-11T221131.0747290000-e1a8551c6
    task_id: redacted
    try_number: '4'
  name: redacted
  namespace: 'foobar'
...
```

### Operating System

Amazon Linux 2

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes==7.14.0

### Deployment

Other 3rd-party Helm chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",ketozhang,2024-10-11 22:42:30+00:00,[],2024-11-05 12:22:30+00:00,2024-11-05 12:22:30+00:00,https://github.com/apache/airflow/issues/42957,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]","[{'comment_id': 2408207150, 'issue_id': 2582289420, 'author': 'ketozhang', 'body': 'In the meantime you can fix this by explicitly setting the namespace to `None`:\r\n\r\n```python\r\nwith DAG(...):\r\n  @task.kubernetes(image=""hello-world"", namespace=None)\r\n  def foo():\r\n      ...\r\n```', 'created_at': datetime.datetime(2024, 10, 11, 22, 43, 23, tzinfo=datetime.timezone.utc)}]","ketozhang (Issue Creator) on (2024-10-11 22:43:23 UTC): In the meantime you can fix this by explicitly setting the namespace to `None`:

```python
with DAG(...):
  @task.kubernetes(image=""hello-world"", namespace=None)
  def foo():
      ...
```

"
2581141050,issue,closed,completed,Skip a list of sequentially executing task,"### Description

## Requirement

- To skip a set of task downstream and continue with other 
- I do not want to branch by task (with branchPython operator) flows neither stop the flow conditionally  (with shorCircutOperator)
- Introduce a feature to conditionally skip a specific set of downstream tasks in the Airflow DAG.
- The logic should allow flexibility to define which downstream tasks to skip based on the outcome or conditions set by previous tasks.
- Ensure that the tasks that follow the skipped set resume execution as per the DAG's original sequence, without requiring manual intervention.


## Example

```
task A => task B => task C => task D => task E
```
Suppose this is the DAG initially 

- I want to have an operator that gets a conditional based on which it can skip task C and D and proceed directly with E like
```
task A => taskB => SkipOperator (true)=> taskC (skipped) => task D(skipped) => task E
```




### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",samirsilwal,2024-10-11 11:28:36+00:00,[],2024-10-14 00:34:07+00:00,2024-10-14 00:34:06+00:00,https://github.com/apache/airflow/issues/42939,"[('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2407213834, 'issue_id': 2581141050, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 11, 11, 28, 38, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-11 11:28:38 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2580988335,issue,closed,completed,AIP-84 Migrate public get Health info to FastAPI,Migrate the public rest api GET /health endpoint to fastAPI,bbovenzi,2024-10-11 10:09:49+00:00,['bbovenzi'],2024-10-15 19:05:41+00:00,2024-10-15 19:05:41+00:00,https://github.com/apache/airflow/issues/42937,"[('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]",[],
2580930431,issue,closed,completed,AIP-84 Add UI batch recent_dag_runs endpoint,"In the new Dags list view we show a chart of recent dag runs:

![Image](https://github.com/user-attachments/assets/2374ce73-7cb9-48ac-b3a6-637dbc049434)

We should add a new endpoint `ui/dags/recent_dag_runs` to return an array of recent dag run arrays like so:
```
[{ dag_id: 'string', dag_runs: []}]
```

It should accept the same filters and pagination as the public get dags endpoint so that we can match them together.

This replaces the viewspy `/last_dagruns`.",bbovenzi,2024-10-11 09:43:16+00:00,['jason810496'],2024-10-25 10:08:43+00:00,2024-10-25 10:08:43+00:00,https://github.com/apache/airflow/issues/42933,"[('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2408361760, 'issue_id': 2580930431, 'author': 'jason810496', 'body': ""Hi, I've just finished #42713. Could you please assign this ticket to me? Thanks!"", 'created_at': datetime.datetime(2024, 10, 12, 4, 23, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2414479747, 'issue_id': 2580930431, 'author': 'bbovenzi', 'body': '@jason810496 all yours!', 'created_at': datetime.datetime(2024, 10, 15, 16, 24, 41, tzinfo=datetime.timezone.utc)}]","jason810496 (Assginee) on (2024-10-12 04:23:11 UTC): Hi, I've just finished #42713. Could you please assign this ticket to me? Thanks!

bbovenzi (Issue Creator) on (2024-10-15 16:24:41 UTC): @jason810496 all yours!

"
2580900617,issue,closed,completed,Potentially unintended behaviour with TaskGroups and user-made attributes attached to them,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.5.3

### What happened?

Is this the intended behaviour? It seems that airflow clones the task group object which is different from regular python semantic, leading to potentially hidden bugs. I got bitten by it and spent a good few hours before realizing. I can try newer airflow versions too, if requested.

```python
class RunnerTaskGroup(TaskGroup):
  def __init__(self...):
    super().__init__()
    self.skipped_because_not_trained = True

  @task(task_group=self)
  def some_task():
    if random.random() < 0.5:
      self.skipped_because_not_trained = False
    print(self, id(self))
    # <RunnerTaskGroup object at 0x7f7a2d3c5be0> 140162721668064 <- see object address

  (
    some_task
  )

.... in main dag .....

with DAG(dag_id=....):
  def update_inference_table_label(_groups):
    for group in _groups:
      print(group, id(group),  group.skipped_because_not_trained)
      # <RunnerTaskGroup object at 0x7f7a2d3cbb50> 140162721692496 <- OTHER OBJECT 
      # group.skipped_because_not_trained=True <- always TRUE

  affinities_groups = []
    for affinity in customer.
       affn_runner_tg = RunnerTaskGroup()
       affinities_group.append(affn_runner_tg)
  ( 
    affinities_groups >>
    update_inference_table_label(affinities_groups)
  )
```


### What you think should happen instead?

Same TaskGroup object is maintained during task group execution and main dag execution. If I were to make these regular python classes, it'd work like that.

### How to reproduce

run the code above

### Operating System

Ubuntu 22.04.3 LTS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Meehai,2024-10-11 09:29:42+00:00,[],2024-10-14 00:17:10+00:00,2024-10-14 00:17:10+00:00,https://github.com/apache/airflow/issues/42931,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:TaskGroup', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2407015593, 'issue_id': 2580900617, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 11, 9, 29, 45, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-11 09:29:45 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2580856194,issue,closed,completed,[Mongodb Connection] AttributeError: 'bool' object has no attribute 'lower',"### Apache Airflow Provider(s)

mongo

### Versions of Apache Airflow Providers

_No response_

### Apache Airflow version

2.10.2

### Operating System

Redhat 9

### Deployment

Virtualenv installation

### Deployment details

_No response_

### What happened

I create a new mongo connection on Airflow with the following input: 
![image](https://github.com/user-attachments/assets/8f6f8c6b-44a2-4837-bf11-4a2f5b7fe35c)

When I try to call the connection on my task on Airflow:

```
from airflow.providers.mongo.hooks.mongo import MongoHook
@task
def drop_table_from_mongo():
    mongo_conn = MongoHook(conn_id='QDATALAKE')
    #My code ...
    cnxn.close()
```

I have this error:
[2024-10-11, 08:44:07 UTC] {base.py:84} INFO - Retrieving connection 'QDATALAKE'
[2024-10-11, 08:44:07 UTC] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/opt/airflow/venv/lib/python3.9/site-packages/airflow/models/taskinstance.py"", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File ""/opt/airflow/venv/lib/python3.9/site-packages/airflow/models/taskinstance.py"", line 733, in _execute_callable
    return ExecutionCallableRunner(
  File ""/opt/airflow/venv/lib/python3.9/site-packages/airflow/utils/operator_helpers.py"", line 252, in run
    return self.func(*args, **kwargs)
  File ""/opt/airflow/venv/lib/python3.9/site-packages/airflow/models/baseoperator.py"", line 406, in wrapper
    return func(self, *args, **kwargs)
  File ""/opt/airflow/venv/lib/python3.9/site-packages/airflow/decorators/base.py"", line 266, in execute
    return_value = super().execute(context)
  File ""/opt/airflow/venv/lib/python3.9/site-packages/airflow/models/baseoperator.py"", line 406, in wrapper
    return func(self, *args, **kwargs)
  File ""/opt/airflow/venv/lib/python3.9/site-packages/airflow/operators/python.py"", line 238, in execute
    return_value = self.execute_callable()
  File ""/opt/airflow/venv/lib/python3.9/site-packages/airflow/operators/python.py"", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
  File ""/opt/airflow/venv/lib/python3.9/site-packages/airflow/utils/operator_helpers.py"", line 252, in run
    return self.func(*args, **kwargs)
  File ""/opt/airflow/dags/Erebia_Datalake_Source_Manuel.py"", line 42, in drop_table_from_mongo
    mongo_conn = MongoHook(conn_id='QDATALAKE')
  File ""/opt/airflow/venv/lib/python3.9/site-packages/airflow/providers/mongo/hooks/mongo.py"", line 139, in __init__
    self.allow_insecure = self.extras.pop(""allow_insecure"", ""false"").lower() == ""true""
AttributeError: 'bool' object has no attribute 'lower'


### What you think should happen instead

_No response_

### How to reproduce

1. Create a new connection 
2. Choose Mongo as type of connection
3. Add Connection name, host, username, passeword and port
4. Do not put anything in extra and do not check any option after extra
5. Save the connection
6. n a new Dag, import MongoHook 
7. Create a @task and call the mongo connection (like in the example below)
8. When the Dag is exacuted, the exception is raised as in the logs below. 

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",FarouziAbir,2024-10-11 09:06:02+00:00,['josix'],2024-10-16 18:57:50+00:00,2024-10-16 18:57:50+00:00,https://github.com/apache/airflow/issues/42930,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:mongo', '')]","[{'comment_id': 2406970590, 'issue_id': 2580856194, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 11, 9, 6, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2407335618, 'issue_id': 2580856194, 'author': 'build3r', 'body': ""Looks like the issue might be because of the newly introduced ssl check boxes for mongo connection.\r\nMinimal way to reproduce:\r\n1. Setup mongo connection in Airflow UI \r\n2. Open Python CLI on airflow server\r\n3.  Import mongo hook `from airflow.providers.mongo.hooks.mongo import MongoHook`\r\n4. Try to connect `mongo_hook = MongoHook(mongo_conn_id='mongo_default')`\r\nConnection fails with \r\n`AttributeError: 'bool' object has no attribute 'lower'`\r\n\r\nRemoving the `.lower` from package works"", 'created_at': datetime.datetime(2024, 10, 11, 12, 42, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408456549, 'issue_id': 2580856194, 'author': 'AkhilVinayakp', 'body': ""Try adding the connection via cli. \r\n```\r\nairflow connections add '<connection_id_name>' \\\r\n    --conn-type 'mongo' \\\r\n    --conn-host '<host-ip>' \\\r\n    --conn-port '27017' \\\r\n    --conn-login '<your-username>' \\\r\n    --conn-password '<your-password>' \r\n\r\n```"", 'created_at': datetime.datetime(2024, 10, 12, 8, 27, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2409586771, 'issue_id': 2580856194, 'author': 'josix', 'body': ""It's probably caused from an inconsistent validation between [get_extra_dejson](https://github.com/apache/airflow/blob/20f82901f4437b8a6ce2831e4b0f9d245056ce7d/airflow/models/connection.py#L413) and MongoHook [initialization](https://github.com/apache/airflow/blob/20f82901f4437b8a6ce2831e4b0f9d245056ce7d/providers/src/airflow/providers/mongo/hooks/mongo.py#L141) when building the connection, I could help revamp the validation logics here, feel free to assign it to me, thanks.\r\n\r\nMy previous workaround is adding quotes to the ssl field to make it as string."", 'created_at': datetime.datetime(2024, 10, 14, 1, 19, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2409592514, 'issue_id': 2580856194, 'author': 'potiuk', 'body': 'Assigned!', 'created_at': datetime.datetime(2024, 10, 14, 1, 21, 15, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-11 09:06:05 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

build3r on (2024-10-11 12:42:35 UTC): Looks like the issue might be because of the newly introduced ssl check boxes for mongo connection.
Minimal way to reproduce:
1. Setup mongo connection in Airflow UI 
2. Open Python CLI on airflow server
3.  Import mongo hook `from airflow.providers.mongo.hooks.mongo import MongoHook`
4. Try to connect `mongo_hook = MongoHook(mongo_conn_id='mongo_default')`
Connection fails with 
`AttributeError: 'bool' object has no attribute 'lower'`

Removing the `.lower` from package works

AkhilVinayakp on (2024-10-12 08:27:33 UTC): Try adding the connection via cli. 
```
airflow connections add '<connection_id_name>' \
    --conn-type 'mongo' \
    --conn-host '<host-ip>' \
    --conn-port '27017' \
    --conn-login '<your-username>' \
    --conn-password '<your-password>' 

```

josix (Assginee) on (2024-10-14 01:19:05 UTC): It's probably caused from an inconsistent validation between [get_extra_dejson](https://github.com/apache/airflow/blob/20f82901f4437b8a6ce2831e4b0f9d245056ce7d/airflow/models/connection.py#L413) and MongoHook [initialization](https://github.com/apache/airflow/blob/20f82901f4437b8a6ce2831e4b0f9d245056ce7d/providers/src/airflow/providers/mongo/hooks/mongo.py#L141) when building the connection, I could help revamp the validation logics here, feel free to assign it to me, thanks.

My previous workaround is adding quotes to the ssl field to make it as string.

potiuk on (2024-10-14 01:21:15 UTC): Assigned!

"
2580223343,issue,closed,completed,missing pyarrow dependency in google provider?,"### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

`6.1` but looks it's the same in `>10`

### Apache Airflow version

2.2

### Operating System

tested linux, macos

### Deployment

Virtualenv installation

### Deployment details

linux/macos & `uv pip` to install the packages

### What happened

I dont know if im missing some obvious reason for this, but `pyarrow` is not specified as a dependency for the google provider, while it definetly depends on it: https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/google/cloud/transfers/sql_to_gcs.py#L29

If i do an install in a venv with:
```
dependencies = [
	""apache-airflow==2.2"",
	""apache-airflow-providers-google==6.3"",
	""google-cloud-bigquery>=1""
]
```

pyarrow won't be installed, and importing from sql_to_gcs will raise a exception

if i remove google-cloud-bigquery, it WILL be installed, i have no idea what causes this behavior since google-cloud-bigquery does list pyarrow as a dependency. But the version is installed is due to `pandas-gbq` and  depending on it

### What you think should happen instead

IMO if a package is used directly, then it's a direct dependency and it shouldn't rely on it being available via indirect dependencies

I can just add pyarrow myself and solve my problem, but i think the dependency should be explicitly defined in the provider

### How to reproduce

```
[project]
name = ""tests""
version = ""0.1.0""
description = ""Add your description here""
readme = ""README.md""
requires-python = "">=3.8""
dependencies = [
	""apache-airflow==2.2"",
	""apache-airflow-providers-google==6.3"",
	""google-cloud-bigquery>=1""
]
```

`uv pip install .`

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",saucoide,2024-10-11 01:52:29+00:00,['saucoide'],2024-10-15 11:43:52+00:00,2024-10-15 11:43:52+00:00,https://github.com/apache/airflow/issues/42924,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('area:providers', ''), ('good first issue', ''), ('area:dependencies', 'Issues related to dependencies problems'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2406383776, 'issue_id': 2580223343, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 11, 1, 52, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2409181780, 'issue_id': 2580223343, 'author': 'potiuk', 'body': 'Sure - feel free to add it to ""provider.yaml"" in ""providers/google/..."" folder. Marked it as a good first issue to fix. Just add it in the same way as in other provider.yaml files:\r\n\r\n```\r\n - pyarrow>=14.0.1\r\n ```', 'created_at': datetime.datetime(2024, 10, 13, 22, 11, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2409184360, 'issue_id': 2580223343, 'author': 'potiuk', 'body': 'BTW. Temporary workaround - until you fix it and we relase a new provider - is o explicitly add ""pyarrow>=14.0.1"" in your dependencies.', 'created_at': datetime.datetime(2024, 10, 13, 22, 12, 57, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-11 01:52:32 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-10-13 22:11:32 UTC): Sure - feel free to add it to ""provider.yaml"" in ""providers/google/..."" folder. Marked it as a good first issue to fix. Just add it in the same way as in other provider.yaml files:

```
 - pyarrow>=14.0.1
 ```

potiuk on (2024-10-13 22:12:57 UTC): BTW. Temporary workaround - until you fix it and we relase a new provider - is o explicitly add ""pyarrow>=14.0.1"" in your dependencies.

"
2579997192,issue,closed,not_planned,KPO pod completes before pod_manager.py is able finish streaming pod logs,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.1

### What happened?

We have a particular task that spawns a pod that is scheduled by itself to a GPU node. What I've observed is that occasionally, the k8s pod reaches a status of ""Completed"" before `pod_manager.py` finishes streaming logs from said pod to Airflow's logging system. Yes, this task produces A LOT of logs. 

Sometimes it can take 2 minutes or more for `pod_manager.py` to actually catch up. The problem happens when AWS karpenter reclaims the GPU node before it finishes streaming the pod logs to airflow, resulting in a 404 error from the kubernetes API like the one below, and the airflow task being failed.

```
kubernetes.client.exceptions.ApiException: (404)
```

We're already using the `karpenter.sh/do-not-disrupt:true` annotation, but that isn't effective at preventing node reclamation once the pod reaches a state of ""Completed"".

We've been able to get around this for now by setting `get_logs=false` for that particular task, however we shouldn't have to do that. 

The other possibility would be to increase the amount of time before karpenter can reclaim a node, but again, these are gpu nodes, so they're expensive and we run 300k tasks a day in just 1 of 8 regions we have airflow deployed to.

### What you think should happen instead?

pod_manager.py should be able to set the airflow task to ""success"" once the pod reaches a state of ""Completed"" and then it can continue to stream logs to Airflow under a ""Best Effort"" basis. In other words, if there is a kube api error received while getting pod logs AFTER the pod has reached a ""Completed"" state, then those errors should be ignored.

Perhaps there is a way to improve log stream performance? If so, I'd love to know how.

### How to reproduce

create a pod that produces a lot of logs and then kill the eks node within a minute or 2 of pod reaching a state of ""completed""

### Operating System

Official Airflow Image on python3.10 (debian)

### Versions of Apache Airflow Providers

we don't have any providers pinned, so whatever versions ship with 2.10.1

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Deployed via ArgoCD.

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",andrew-stein-sp,2024-10-10 22:51:17+00:00,[],2024-11-06 00:14:52+00:00,2024-11-06 00:14:51+00:00,https://github.com/apache/airflow/issues/42923,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('area:providers', ''), ('area:logging', ''), ('pending-response', ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2406175703, 'issue_id': 2579997192, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 10, 22, 51, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2409169454, 'issue_id': 2579997192, 'author': 'potiuk', 'body': 'Can you please try upgrading the provider to https://pypi.org/project/apache-airflow-providers-cncf-kubernetes/9.0.0rc1/ - there were some fixes to log streaming implemented in the latest version that is about to be relased.', 'created_at': datetime.datetime(2024, 10, 13, 22, 4, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412024480, 'issue_id': 2579997192, 'author': 'andrew-stein-sp', 'body': ""I'll get an image built in our dev environment to test and let you know."", 'created_at': datetime.datetime(2024, 10, 14, 19, 7, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2442910498, 'issue_id': 2579997192, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 10, 29, 0, 15, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2458451548, 'issue_id': 2579997192, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 11, 6, 0, 14, 51, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-10 22:51:20 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-10-13 22:04:59 UTC): Can you please try upgrading the provider to https://pypi.org/project/apache-airflow-providers-cncf-kubernetes/9.0.0rc1/ - there were some fixes to log streaming implemented in the latest version that is about to be relased.

andrew-stein-sp (Issue Creator) on (2024-10-14 19:07:30 UTC): I'll get an image built in our dev environment to test and let you know.

github-actions[bot] on (2024-10-29 00:15:27 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-11-06 00:14:51 UTC): This issue has been closed because it has not received response from the issue author.

"
2579886853,issue,closed,completed,Cannot view logs for running task in Airflow standalone,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When viewing the logs of a running task in the UI of an airflow standalone instance, the logs do not show. The output of the standalone command shows a lot of SIGSEGV signals being sent to the worker. Once the task completed, the SIGSEGV signals stopped and everything functioned normally, including viewing logs. Additionally, the following dialog was displayed.

![image](https://github.com/user-attachments/assets/19485d0d-5b1e-4e20-bd25-ac50c179929c)


### What you think should happen instead?

When viewing the logs for a running task in the UI of an Airflow instance run with `airflow standalone`, the logs are shown.

### How to reproduce

Initialize a new Airflow environment using the following commands:

```bash
python3.10 -m venv venv
source venv/bin/activate
pip install apache-airflow==2.10.2
AIRFLOW_HOME=$(pwd)/.airflow venv/bin/airflow standalone
```

Modify the file ./venv/lib/python3.10/site-packages/airflow/example_dags/tutorial_taskflow_api.py to include some code that takes a while to complete. I have tried both sleeping using the `sleep` function in the `time` package, and logging thousands of lines in a loop and both caused the issue.

Trigger the DAG and view the logs for the modified task.

### Operating System

OSX

### Versions of Apache Airflow Providers

None

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

<details><summary>standalone.log</summary>
<pre>
❯ AIRFLOW_HOME=$(pwd)/.airflow venv/bin/airflow standalone
standalone | Starting Airflow Standalone
standalone | Checking database is initialized
INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
WARNI [airflow.models.crypto] empty cryptography key - values will not be stored encrypted.
standalone | Database ready
/Users/rcheatham/workspace/airflow/bug-test/venv/lib/python3.10/site-packages/flask_limiter/extension.py:333 UserWarning: Using the in-memory storage for tracking rate limits as no storage was explicitly specified. This is not recommended for production use. See: https://flask-limiter.readthedocs.io#configuring-a-storage-backend for documentation about configuring the storage backend.
WARNI [airflow.example_dags.plugins.workday] Could not import pandas. Holidays will not be considered.
triggerer  | ____________       _____________
triggerer  | ____    |__( )_________  __/__  /________      __
triggerer  | ____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
triggerer  | ___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
triggerer  | _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
triggerer  | [2024-10-10T15:23:44.871-0600] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
triggerer  | [2024-10-10T15:23:44.873-0600] {triggerer_job_runner.py:181} INFO - Setting up TriggererHandlerWrapper with handler <FileTaskHandler (NOTSET)>
triggerer  | [2024-10-10T15:23:44.873-0600] {triggerer_job_runner.py:237} INFO - Setting up logging queue listener with handlers [<RedirectStdHandler <stdout> (NOTSET)>, <TriggererHandlerWrapper (NOTSET)>]
triggerer  | [2024-10-10T15:23:44.876-0600] {triggerer_job_runner.py:338} INFO - Starting the triggerer
scheduler  | ____________       _____________
scheduler  | ____    |__( )_________  __/__  /________      __
scheduler  | ____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
scheduler  | ___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
scheduler  | _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
scheduler  | [2024-10-10T15:23:44.972-0600] {_client.py:1038} INFO - HTTP Request: GET https://apacheairflow.gateway.scarf.sh/scheduler?version=2.10.2&python_version=3.10&platform=Darwin&arch=arm64&database=sqlite&db_version=3.46&executor=SequentialExecutor ""HTTP/1.1 200 OK""
scheduler  | [2024-10-10T15:23:45.010-0600] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
scheduler  | [2024-10-10T15:23:45.011-0600] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
scheduler  | [2024-10-10T15:23:45.061-0600] {scheduler_job_runner.py:935} INFO - Starting the scheduler
scheduler  | [2024-10-10T15:23:45.062-0600] {scheduler_job_runner.py:942} INFO - Processing each file at most -1 times
scheduler  | [2024-10-10T15:23:45.065-0600] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 90224
scheduler  | [2024-10-10T15:23:45.066-0600] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
triggerer  | [2024-10-10 15:23:45 -0600] [90221] [INFO] Starting gunicorn 23.0.0
triggerer  | [2024-10-10 15:23:45 -0600] [90221] [INFO] Listening at: http://[::]:8794 (90221)
triggerer  | [2024-10-10 15:23:45 -0600] [90221] [INFO] Using worker: sync
triggerer  | [2024-10-10 15:23:45 -0600] [90225] [INFO] Booting worker with pid: 90225
triggerer  | [2024-10-10 15:23:45 -0600] [90226] [INFO] Booting worker with pid: 90226
webserver  | /Users/rcheatham/workspace/airflow/bug-test/venv/lib/python3.10/site-packages/flask_limiter/extension.py:333 UserWarning: Using the in-memory storage for tracking rate limits as no storage was explicitly specified. This is not recommended for production use. See: https://flask-limiter.readthedocs.io#configuring-a-storage-backend for documentation about configuring the storage backend.
scheduler  | [2024-10-10 15:23:45 -0600] [90223] [INFO] Starting gunicorn 23.0.0
scheduler  | [2024-10-10 15:23:45 -0600] [90223] [INFO] Listening at: http://[::]:8793 (90223)
scheduler  | [2024-10-10 15:23:45 -0600] [90223] [INFO] Using worker: sync
scheduler  | [2024-10-10 15:23:45 -0600] [90227] [INFO] Booting worker with pid: 90227
scheduler  | [2024-10-10T15:23:45.798-0600] {settings.py:63} INFO - Configured default timezone UTC
scheduler  | [2024-10-10 15:23:45 -0600] [90228] [INFO] Booting worker with pid: 90228
scheduler  | [2024-10-10T15:23:45.806-0600] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
webserver  | [2024-10-10T15:23:46.003-0600] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
webserver  | /Users/rcheatham/workspace/airflow/bug-test/venv/lib/python3.10/site-packages/airflow/providers/fab/auth_manager/fab_auth_manager.py:547 FutureWarning: section/key [webserver/update_fab_perms] has been deprecated, you should use[fab/update_fab_perms] instead. Please update your `conf.get*` call to use the new name
webserver  | [2024-10-10 15:23:46 -0600] [90219] [INFO] Starting gunicorn 23.0.0
webserver  | [2024-10-10 15:23:46 -0600] [90219] [INFO] Listening at: http://0.0.0.0:8080 (90219)
webserver  | [2024-10-10 15:23:46 -0600] [90219] [INFO] Using worker: sync
webserver  | [2024-10-10 15:23:46 -0600] [90230] [INFO] Booting worker with pid: 90230
webserver  | [2024-10-10 15:23:46 -0600] [90231] [INFO] Booting worker with pid: 90231
webserver  | [2024-10-10 15:23:46 -0600] [90232] [INFO] Booting worker with pid: 90232
webserver  | [2024-10-10 15:23:46 -0600] [90233] [INFO] Booting worker with pid: 90233
standalone | Airflow is ready
standalone | Login with username: admin  password: [REDACTED]
standalone | Airflow Standalone is for development purposes only. Do not use this in production!
webserver  | 127.0.0.1 - - [10/Oct/2024:15:23:51 -0600] ""GET /dags/tutorial_taskflow_api/grid HTTP/1.1"" 200 9755 ""http://localhost:8080/home?status=active"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | 127.0.0.1 - - [10/Oct/2024:15:23:51 -0600] ""GET /object/grid_data?dag_id=tutorial_taskflow_api&num_runs=25 HTTP/1.1"" 200 3222 ""http://localhost:8080/dags/tutorial_taskflow_api/grid"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | 127.0.0.1 - - [10/Oct/2024:15:23:51 -0600] ""GET /object/next_run_datasets/tutorial_taskflow_api HTTP/1.1"" 200 39 ""http://localhost:8080/dags/tutorial_taskflow_api/grid"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | 127.0.0.1 - - [10/Oct/2024:15:23:51 -0600] ""GET /api/v1/dags/tutorial_taskflow_api HTTP/1.1"" 200 1189 ""http://localhost:8080/dags/tutorial_taskflow_api/grid"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | 127.0.0.1 - - [10/Oct/2024:15:23:51 -0600] ""GET /api/v1/dags/tutorial_taskflow_api/details HTTP/1.1"" 200 1962 ""http://localhost:8080/dags/tutorial_taskflow_api/grid"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | 127.0.0.1 - - [10/Oct/2024:15:23:55 -0600] ""POST /dags/tutorial_taskflow_api/trigger?origin=%2Fdags%2Ftutorial_taskflow_api%2Fgrid HTTP/1.1"" 302 413 ""http://localhost:8080/dags/tutorial_taskflow_api/grid"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | 127.0.0.1 - - [10/Oct/2024:15:23:55 -0600] ""GET /dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00 HTTP/1.1"" 200 9885 ""http://localhost:8080/dags/tutorial_taskflow_api/grid"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | 127.0.0.1 - - [10/Oct/2024:15:23:55 -0600] ""GET /object/grid_data?dag_id=tutorial_taskflow_api&num_runs=25 HTTP/1.1"" 200 4159 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | 127.0.0.1 - - [10/Oct/2024:15:23:55 -0600] ""GET /object/next_run_datasets/tutorial_taskflow_api HTTP/1.1"" 200 39 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | 127.0.0.1 - - [10/Oct/2024:15:23:57 -0600] ""GET /api/v1/dags/tutorial_taskflow_api/tasks/transform HTTP/1.1"" 200 1116 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | 127.0.0.1 - - [10/Oct/2024:15:23:57 -0600] ""GET /api/v1/dags/tutorial_taskflow_api/dagRuns/manual__2024-10-10T21:23:55.317397+00:00/taskInstances/transform HTTP/1.1"" 200 751 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | 127.0.0.1 - - [10/Oct/2024:15:23:57 -0600] ""GET /api/v1/dags/tutorial_taskflow_api/dagRuns/manual__2024-10-10T21:23:55.317397+00:00/taskInstances/transform/dependencies HTTP/1.1"" 200 608 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
scheduler  | Dag run  in running state
scheduler  | Dag information Queued at: 2024-10-10 21:23:55.333863+00:00 hash info: 38e56a72b18ab9d01e9f2f0cd6695560
scheduler  | [2024-10-10T15:23:57.796-0600] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
scheduler  | <TaskInstance: tutorial_taskflow_api.extract manual__2024-10-10T21:23:55.317397+00:00 [scheduled]>
scheduler  | [2024-10-10T15:23:57.797-0600] {scheduler_job_runner.py:495} INFO - DAG tutorial_taskflow_api has 0/16 running and queued tasks
scheduler  | [2024-10-10T15:23:57.797-0600] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
scheduler  | <TaskInstance: tutorial_taskflow_api.extract manual__2024-10-10T21:23:55.317397+00:00 [scheduled]>
scheduler  | [2024-10-10T15:23:57.798-0600] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: tutorial_taskflow_api.extract manual__2024-10-10T21:23:55.317397+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
scheduler  | [2024-10-10T15:23:57.798-0600] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='tutorial_taskflow_api', task_id='extract', run_id='manual__2024-10-10T21:23:55.317397+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 3 and queue default
scheduler  | [2024-10-10T15:23:57.798-0600] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'tutorial_taskflow_api', 'extract', 'manual__2024-10-10T21:23:55.317397+00:00', '--local', '--subdir', '/Users/rcheatham/workspace/airflow/bug-test/venv/lib/python3.10/site-packages/airflow/example_dags/tutorial_taskflow_api.py']
scheduler  | [2024-10-10T15:23:57.799-0600] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'tutorial_taskflow_api', 'extract', 'manual__2024-10-10T21:23:55.317397+00:00', '--local', '--subdir', '/Users/rcheatham/workspace/airflow/bug-test/venv/lib/python3.10/site-packages/airflow/example_dags/tutorial_taskflow_api.py']
scheduler  | [2024-10-10T15:23:58.459-0600] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
scheduler  | [2024-10-10T15:23:58.463-0600] {dagbag.py:588} INFO - Filling up the DagBag from /Users/rcheatham/workspace/airflow/bug-test/venv/lib/python3.10/site-packages/airflow/example_dags/tutorial_taskflow_api.py
scheduler  | [2024-10-10T15:23:58.491-0600] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
scheduler  | Traceback (most recent call last):
scheduler  | File ""/Users/rcheatham/workspace/airflow/bug-test/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py"", line 38, in <module>
scheduler  | from kubernetes.client import models as k8s
scheduler  | ModuleNotFoundError: No module named 'kubernetes'
scheduler  | [2024-10-10T15:23:58.492-0600] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
webserver  | 127.0.0.1 - - [10/Oct/2024:15:23:58 -0600] ""GET /object/grid_data?dag_id=tutorial_taskflow_api&num_runs=25 HTTP/1.1"" 200 4254 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
scheduler  | [2024-10-10T15:23:58.636-0600] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
scheduler  | [2024-10-10T15:23:58.654-0600] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
scheduler  | [2024-10-10T15:23:58.669-0600] {task_command.py:467} INFO - Running <TaskInstance: tutorial_taskflow_api.extract manual__2024-10-10T21:23:55.317397+00:00 [queued]> on host Q-QKQT6Y31P2-rcheatham
webserver  | 127.0.0.1 - - [10/Oct/2024:15:23:58 -0600] ""GET /api/v1/dags/tutorial_taskflow_api/dagRuns/manual__2024-10-10T21:23:55.317397+00:00/taskInstances/transform/logs/1?full_content=false HTTP/1.1"" 200 2 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
scheduler  | [2024-10-10T15:23:58.975-0600] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='tutorial_taskflow_api', task_id='extract', run_id='manual__2024-10-10T21:23:55.317397+00:00', try_number=1, map_index=-1)
scheduler  | [2024-10-10T15:23:58.980-0600] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=tutorial_taskflow_api, task_id=extract, run_id=manual__2024-10-10T21:23:55.317397+00:00, map_index=-1, run_start_date=2024-10-10 21:23:58.727434+00:00, run_end_date=2024-10-10 21:23:58.804088+00:00, run_duration=0.076654, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=17, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2024-10-10 21:23:57.797448+00:00, queued_by_job_id=16, pid=90241
scheduler  | [2024-10-10T15:23:59.765-0600] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
scheduler  | <TaskInstance: tutorial_taskflow_api.transform manual__2024-10-10T21:23:55.317397+00:00 [scheduled]>
scheduler  | [2024-10-10T15:23:59.765-0600] {scheduler_job_runner.py:495} INFO - DAG tutorial_taskflow_api has 0/16 running and queued tasks
scheduler  | [2024-10-10T15:23:59.765-0600] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
scheduler  | <TaskInstance: tutorial_taskflow_api.transform manual__2024-10-10T21:23:55.317397+00:00 [scheduled]>
scheduler  | [2024-10-10T15:23:59.766-0600] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: tutorial_taskflow_api.transform manual__2024-10-10T21:23:55.317397+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
scheduler  | [2024-10-10T15:23:59.766-0600] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='tutorial_taskflow_api', task_id='transform', run_id='manual__2024-10-10T21:23:55.317397+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 2 and queue default
scheduler  | [2024-10-10T15:23:59.766-0600] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'tutorial_taskflow_api', 'transform', 'manual__2024-10-10T21:23:55.317397+00:00', '--local', '--subdir', '/Users/rcheatham/workspace/airflow/bug-test/venv/lib/python3.10/site-packages/airflow/example_dags/tutorial_taskflow_api.py']
scheduler  | [2024-10-10T15:23:59.767-0600] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'tutorial_taskflow_api', 'transform', 'manual__2024-10-10T21:23:55.317397+00:00', '--local', '--subdir', '/Users/rcheatham/workspace/airflow/bug-test/venv/lib/python3.10/site-packages/airflow/example_dags/tutorial_taskflow_api.py']
scheduler  | [2024-10-10T15:24:00.435-0600] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
scheduler  | [2024-10-10T15:24:00.440-0600] {dagbag.py:588} INFO - Filling up the DagBag from /Users/rcheatham/workspace/airflow/bug-test/venv/lib/python3.10/site-packages/airflow/example_dags/tutorial_taskflow_api.py
scheduler  | [2024-10-10T15:24:00.468-0600] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
scheduler  | Traceback (most recent call last):
scheduler  | File ""/Users/rcheatham/workspace/airflow/bug-test/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py"", line 38, in <module>
scheduler  | from kubernetes.client import models as k8s
scheduler  | ModuleNotFoundError: No module named 'kubernetes'
scheduler  | [2024-10-10T15:24:00.468-0600] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
scheduler  | [2024-10-10T15:24:00.600-0600] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
scheduler  | [2024-10-10T15:24:00.618-0600] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
scheduler  | [2024-10-10T15:24:00.634-0600] {task_command.py:467} INFO - Running <TaskInstance: tutorial_taskflow_api.transform manual__2024-10-10T21:23:55.317397+00:00 [queued]> on host Q-QKQT6Y31P2-rcheatham
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:01 -0600] ""GET /object/grid_data?dag_id=tutorial_taskflow_api&num_runs=25 HTTP/1.1"" 200 4380 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:01 -0600] ""GET /api/v1/dags/tutorial_taskflow_api/dagRuns/manual__2024-10-10T21:23:55.317397+00:00/taskInstances/transform HTTP/1.1"" 200 1001 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:04 -0600] ""GET /object/grid_data?dag_id=tutorial_taskflow_api&num_runs=25 HTTP/1.1"" 200 4380 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | [2024-10-10T15:24:04.649-0600] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
webserver  | [2024-10-10 15:24:04 -0600] [90219] [ERROR] Worker (pid:90232) was sent SIGSEGV!
webserver  | [2024-10-10 15:24:04 -0600] [90245] [INFO] Booting worker with pid: 90245
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:04 -0600] ""GET /api/v1/dags/tutorial_taskflow_api/dagRuns/manual__2024-10-10T21:23:55.317397+00:00/taskInstances/transform HTTP/1.1"" 200 1001 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | [2024-10-10T15:24:05.284-0600] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
webserver  | [2024-10-10 15:24:05 -0600] [90219] [ERROR] Worker (pid:90233) was sent SIGSEGV!
webserver  | [2024-10-10 15:24:05 -0600] [90246] [INFO] Booting worker with pid: 90246
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:07 -0600] ""GET /object/grid_data?dag_id=tutorial_taskflow_api&num_runs=25 HTTP/1.1"" 200 4380 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:07 -0600] ""GET /api/v1/dags/tutorial_taskflow_api/dagRuns/manual__2024-10-10T21:23:55.317397+00:00/taskInstances/transform HTTP/1.1"" 200 1001 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | [2024-10-10T15:24:08.331-0600] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
webserver  | [2024-10-10 15:24:08 -0600] [90219] [ERROR] Worker (pid:90231) was sent SIGSEGV!
webserver  | [2024-10-10 15:24:08 -0600] [90247] [INFO] Booting worker with pid: 90247
webserver  | [2024-10-10T15:24:08.919-0600] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
webserver  | [2024-10-10 15:24:08 -0600] [90219] [ERROR] Worker (pid:90246) was sent SIGSEGV!
webserver  | [2024-10-10 15:24:08 -0600] [90250] [INFO] Booting worker with pid: 90250
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:10 -0600] ""GET /object/grid_data?dag_id=tutorial_taskflow_api&num_runs=25 HTTP/1.1"" 200 4380 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:10 -0600] ""GET /api/v1/dags/tutorial_taskflow_api/dagRuns/manual__2024-10-10T21:23:55.317397+00:00/taskInstances/transform HTTP/1.1"" 200 1001 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | [2024-10-10T15:24:11.993-0600] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
webserver  | [2024-10-10 15:24:11 -0600] [90219] [ERROR] Worker (pid:90245) was sent SIGSEGV!
webserver  | [2024-10-10 15:24:12 -0600] [90252] [INFO] Booting worker with pid: 90252
webserver  | [2024-10-10T15:24:12.554-0600] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
webserver  | [2024-10-10 15:24:12 -0600] [90219] [ERROR] Worker (pid:90252) was sent SIGSEGV!
webserver  | [2024-10-10 15:24:12 -0600] [90255] [INFO] Booting worker with pid: 90255
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:13 -0600] ""GET /object/grid_data?dag_id=tutorial_taskflow_api&num_runs=25 HTTP/1.1"" 200 4380 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:13 -0600] ""GET /api/v1/dags/tutorial_taskflow_api/dagRuns/manual__2024-10-10T21:23:55.317397+00:00/taskInstances/transform HTTP/1.1"" 200 1001 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | [2024-10-10T15:24:15.623-0600] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
webserver  | [2024-10-10 15:24:15 -0600] [90219] [ERROR] Worker (pid:90250) was sent SIGSEGV!
webserver  | [2024-10-10 15:24:15 -0600] [90260] [INFO] Booting worker with pid: 90260
webserver  | [2024-10-10T15:24:16.194-0600] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
webserver  | [2024-10-10 15:24:16 -0600] [90219] [ERROR] Worker (pid:90230) was sent SIGSEGV!
webserver  | [2024-10-10 15:24:16 -0600] [90261] [INFO] Booting worker with pid: 90261
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:16 -0600] ""GET /object/grid_data?dag_id=tutorial_taskflow_api&num_runs=25 HTTP/1.1"" 200 4380 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:16 -0600] ""GET /api/v1/dags/tutorial_taskflow_api/dagRuns/manual__2024-10-10T21:23:55.317397+00:00/taskInstances/transform HTTP/1.1"" 200 1001 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | [2024-10-10T15:24:19.532-0600] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
webserver  | [2024-10-10 15:24:19 -0600] [90219] [ERROR] Worker (pid:90255) was sent SIGSEGV!
webserver  | [2024-10-10 15:24:19 -0600] [90273] [INFO] Booting worker with pid: 90273
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:19 -0600] ""GET /object/grid_data?dag_id=tutorial_taskflow_api&num_runs=25 HTTP/1.1"" 200 4380 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:19 -0600] ""GET /api/v1/dags/tutorial_taskflow_api/dagRuns/manual__2024-10-10T21:23:55.317397+00:00/taskInstances/transform HTTP/1.1"" 200 1001 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | [2024-10-10T15:24:20.073-0600] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
webserver  | [2024-10-10 15:24:20 -0600] [90219] [ERROR] Worker (pid:90260) was sent SIGSEGV!
webserver  | [2024-10-10 15:24:20 -0600] [90274] [INFO] Booting worker with pid: 90274
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:22 -0600] ""GET /object/grid_data?dag_id=tutorial_taskflow_api&num_runs=25 HTTP/1.1"" 200 4380 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:22 -0600] ""GET /api/v1/dags/tutorial_taskflow_api/dagRuns/manual__2024-10-10T21:23:55.317397+00:00/taskInstances/transform HTTP/1.1"" 200 1001 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | [2024-10-10T15:24:23.110-0600] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
webserver  | [2024-10-10 15:24:23 -0600] [90219] [ERROR] Worker (pid:90261) was sent SIGSEGV!
webserver  | [2024-10-10 15:24:23 -0600] [90275] [INFO] Booting worker with pid: 90275
webserver  | [2024-10-10T15:24:23.672-0600] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
webserver  | [2024-10-10 15:24:23 -0600] [90219] [ERROR] Worker (pid:90273) was sent SIGSEGV!
webserver  | [2024-10-10 15:24:23 -0600] [90276] [INFO] Booting worker with pid: 90276
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:26 -0600] ""GET /object/grid_data?dag_id=tutorial_taskflow_api&num_runs=25 HTTP/1.1"" 200 4380 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:26 -0600] ""GET /api/v1/dags/tutorial_taskflow_api/dagRuns/manual__2024-10-10T21:23:55.317397+00:00/taskInstances/transform HTTP/1.1"" 200 1001 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | [2024-10-10T15:24:26.733-0600] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
webserver  | [2024-10-10 15:24:26 -0600] [90219] [ERROR] Worker (pid:90274) was sent SIGSEGV!
webserver  | [2024-10-10 15:24:26 -0600] [90277] [INFO] Booting worker with pid: 90277
webserver  | [2024-10-10T15:24:27.301-0600] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
webserver  | [2024-10-10 15:24:27 -0600] [90219] [ERROR] Worker (pid:90247) was sent SIGSEGV!
webserver  | [2024-10-10 15:24:27 -0600] [90278] [INFO] Booting worker with pid: 90278
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:29 -0600] ""GET /api/v1/dags/tutorial_taskflow_api/dagRuns/manual__2024-10-10T21:23:55.317397+00:00/taskInstances/transform HTTP/1.1"" 200 1001 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:29 -0600] ""GET /object/grid_data?dag_id=tutorial_taskflow_api&num_runs=25 HTTP/1.1"" 200 4380 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | [2024-10-10T15:24:30.373-0600] {executor_loader.py:254} INFO - Loaded executor: SequentialExecutor
webserver  | [2024-10-10 15:24:30 -0600] [90219] [ERROR] Worker (pid:90277) was sent SIGSEGV!
webserver  | [2024-10-10 15:24:30 -0600] [90279] [INFO] Booting worker with pid: 90279
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:30 -0600] ""GET /api/v1/dags/tutorial_taskflow_api/dagRuns/manual__2024-10-10T21:23:55.317397+00:00/taskInstances/transform/logs/1?full_content=false HTTP/1.1"" 200 3950 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
scheduler  | [2024-10-10T15:24:30.957-0600] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='tutorial_taskflow_api', task_id='transform', run_id='manual__2024-10-10T21:23:55.317397+00:00', try_number=1, map_index=-1)
scheduler  | [2024-10-10T15:24:30.960-0600] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=tutorial_taskflow_api, task_id=transform, run_id=manual__2024-10-10T21:23:55.317397+00:00, map_index=-1, run_start_date=2024-10-10 21:24:00.685070+00:00, run_end_date=2024-10-10 21:24:30.765587+00:00, run_duration=30.080517, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=18, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2024-10-10 21:23:59.765812+00:00, queued_by_job_id=16, pid=90244
scheduler  | [2024-10-10T15:24:30.974-0600] {job.py:229} INFO - Heartbeat recovered after 35.07 seconds
scheduler  | [2024-10-10T15:24:31.917-0600] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
scheduler  | <TaskInstance: tutorial_taskflow_api.load manual__2024-10-10T21:23:55.317397+00:00 [scheduled]>
scheduler  | [2024-10-10T15:24:31.917-0600] {scheduler_job_runner.py:495} INFO - DAG tutorial_taskflow_api has 0/16 running and queued tasks
scheduler  | [2024-10-10T15:24:31.917-0600] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
scheduler  | <TaskInstance: tutorial_taskflow_api.load manual__2024-10-10T21:23:55.317397+00:00 [scheduled]>
scheduler  | [2024-10-10T15:24:31.918-0600] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: tutorial_taskflow_api.load manual__2024-10-10T21:23:55.317397+00:00 [scheduled]>] for executor: SequentialExecutor(parallelism=32)
scheduler  | [2024-10-10T15:24:31.918-0600] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='tutorial_taskflow_api', task_id='load', run_id='manual__2024-10-10T21:23:55.317397+00:00', try_number=1, map_index=-1) to SequentialExecutor with priority 1 and queue default
scheduler  | [2024-10-10T15:24:31.918-0600] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'tutorial_taskflow_api', 'load', 'manual__2024-10-10T21:23:55.317397+00:00', '--local', '--subdir', '/Users/rcheatham/workspace/airflow/bug-test/venv/lib/python3.10/site-packages/airflow/example_dags/tutorial_taskflow_api.py']
scheduler  | [2024-10-10T15:24:31.919-0600] {sequential_executor.py:84} INFO - Executing command: ['airflow', 'tasks', 'run', 'tutorial_taskflow_api', 'load', 'manual__2024-10-10T21:23:55.317397+00:00', '--local', '--subdir', '/Users/rcheatham/workspace/airflow/bug-test/venv/lib/python3.10/site-packages/airflow/example_dags/tutorial_taskflow_api.py']
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:32 -0600] ""GET /api/v1/dags/tutorial_taskflow_api/dagRuns/manual__2024-10-10T21:23:55.317397+00:00/taskInstances/transform HTTP/1.1"" 200 1036 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:32 -0600] ""GET /object/grid_data?dag_id=tutorial_taskflow_api&num_runs=25 HTTP/1.1"" 200 4444 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
scheduler  | [2024-10-10T15:24:32.603-0600] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
scheduler  | [2024-10-10T15:24:32.607-0600] {dagbag.py:588} INFO - Filling up the DagBag from /Users/rcheatham/workspace/airflow/bug-test/venv/lib/python3.10/site-packages/airflow/example_dags/tutorial_taskflow_api.py
scheduler  | [2024-10-10T15:24:32.639-0600] {example_local_kubernetes_executor.py:40} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
scheduler  | Traceback (most recent call last):
scheduler  | File ""/Users/rcheatham/workspace/airflow/bug-test/venv/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py"", line 38, in <module>
scheduler  | from kubernetes.client import models as k8s
scheduler  | ModuleNotFoundError: No module named 'kubernetes'
scheduler  | [2024-10-10T15:24:32.640-0600] {example_local_kubernetes_executor.py:41} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
scheduler  | [2024-10-10T15:24:32.783-0600] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
scheduler  | [2024-10-10T15:24:32.800-0600] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
scheduler  | [2024-10-10T15:24:32.815-0600] {task_command.py:467} INFO - Running <TaskInstance: tutorial_taskflow_api.load manual__2024-10-10T21:23:55.317397+00:00 [queued]> on host Q-QKQT6Y31P2-rcheatham
scheduler  | [2024-10-10T15:24:33.099-0600] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='tutorial_taskflow_api', task_id='load', run_id='manual__2024-10-10T21:23:55.317397+00:00', try_number=1, map_index=-1)
scheduler  | [2024-10-10T15:24:33.102-0600] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=tutorial_taskflow_api, task_id=load, run_id=manual__2024-10-10T21:23:55.317397+00:00, map_index=-1, run_start_date=2024-10-10 21:24:32.866040+00:00, run_end_date=2024-10-10 21:24:32.936800+00:00, run_duration=0.07076, state=success, executor=SequentialExecutor(parallelism=32), executor_state=success, try_number=1, max_tries=0, job_id=19, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2024-10-10 21:24:31.917929+00:00, queued_by_job_id=16, pid=90282
scheduler  | [2024-10-10T15:24:33.877-0600] {dagrun.py:854} INFO - Marking run <DagRun tutorial_taskflow_api @ 2024-10-10 21:23:55.317397+00:00: manual__2024-10-10T21:23:55.317397+00:00, state:running, queued_at: 2024-10-10 21:23:55.333863+00:00. externally triggered: True> successful
scheduler  | Dag run in success state
scheduler  | Dag run start:2024-10-10 21:23:57.783848+00:00 end:2024-10-10 21:24:33.878082+00:00
scheduler  | [2024-10-10T15:24:33.878-0600] {dagrun.py:905} INFO - DagRun Finished: dag_id=tutorial_taskflow_api, execution_date=2024-10-10 21:23:55.317397+00:00, run_id=manual__2024-10-10T21:23:55.317397+00:00, run_start_date=2024-10-10 21:23:57.783848+00:00, run_end_date=2024-10-10 21:24:33.878082+00:00, run_duration=36.094234, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-10-10 21:23:55.317397+00:00, data_interval_end=2024-10-10 21:23:55.317397+00:00, dag_hash=38e56a72b18ab9d01e9f2f0cd6695560
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:33 -0600] ""GET /api/v1/dags/tutorial_taskflow_api/dagRuns/manual__2024-10-10T21:23:55.317397+00:00/taskInstances/transform/logs/1?full_content=false HTTP/1.1"" 200 3950 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:35 -0600] ""GET /api/v1/dags/tutorial_taskflow_api/dagRuns/manual__2024-10-10T21:23:55.317397+00:00/taskInstances/transform HTTP/1.1"" 200 1036 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
webserver  | 127.0.0.1 - - [10/Oct/2024:15:24:35 -0600] ""GET /object/grid_data?dag_id=tutorial_taskflow_api&num_runs=25 HTTP/1.1"" 200 4535 ""http://localhost:8080/dags/tutorial_taskflow_api/grid?dag_run_id=manual__2024-10-10T21%3A23%3A55.317397%2B00%3A00&task_id=transform&tab=logs"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
scheduler  | [2024-10-10T15:24:37.819-0600] {workday.py:41} WARNING - Could not import pandas. Holidays will not be considered.
triggerer  | [2024-10-10T15:24:44.938-0600] {triggerer_job_runner.py:510} INFO - 0 triggers currently running
triggerer  | [2024-10-10T15:25:44.999-0600] {triggerer_job_runner.py:510} INFO - 0 triggers currently running
triggerer  | [2024-10-10T15:26:45.066-0600] {triggerer_job_runner.py:510} INFO - 0 triggers currently running
</pre>
</details>

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rcheatham-q,2024-10-10 21:32:02+00:00,[],2024-10-13 21:43:55+00:00,2024-10-13 21:43:55+00:00,https://github.com/apache/airflow/issues/42920,"[('kind:bug', 'This is a clearly a bug'), ('area:logging', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2579836527,issue,open,,"Helm chart: Git-sync v4 incorrect mix of branch, ref, rev","### Official Helm Chart version

1.15.0 (latest released)

### Apache Airflow version

2.5.3

### Kubernetes Version

1.30.4-gke.1348000

### Helm Chart configuration

```yaml
dags:
  gitSync:
    branch: main
    rev: HEAD
    ref: v1.2.3
    repo: git@github.com:<ORG>/<REPO>.git
    depth: 1
    enabled: true
    sshKeySecret: <my-key>
    subPath: src/<MY>/<PATH>
    knownHosts: |
      github.com ssh-rsa <KEY>
```

### Docker Image customizations

_No response_

### What happened

I pushed code to the `main` branch, and this was picked up by the `gitSync` sidecar and it updated the Airflow Dags to use the new code

### What you think should happen instead

It should not have updated the code, because the `ref` is ""frozen"" to version `v1.2.3`. This is a production environment, and that's the whole reason I froze it by pointing it at a Git tag - so that pushing new code won't automatically deploy until it is reviewed/tagged/released.

### How to reproduce

1. Have your git repo have a branch named `main`, and a tag named `v1.2.3` that points to `main`.
2. Set this in your `values.yaml`

 ```yaml
images:
  gitSync:
    repository: registry.k8s.io/git-sync/git-sync
    tag: v4.1.0
dags:
  gitSync:
    branch: main
    rev: HEAD
    ref: v1.2.3
    repo: git@github.com:<ORG>/<REPO>.git
    enabled: true
```
3. Deploy this
4. Make a new commit to the `main` branch, and push it
5. You will see that the gitSync container updates the code with the new commit, even though it is supposed to be ""frozen"" to `v1.2.3`

### Anything else

I dug into this, and it's caused by incorrectly mapping 3 parameters to the gitSync container. The latest Helm chart (v1.15) by default uses this image
https://github.com/apache/airflow/blob/ff7463b1624d91fb6478ec367bbf84b88fb9b83d/chart/values.yaml#L124-L127
which I believe is from this source code here https://github.com/kubernetes/git-sync/tree/v4.1.0.

The 3 incorrect params are:
- branch
- rev
- ref

In v4.x of gitSync, the params `branch` and `rev` are deprecated, and only `ref` should be used. But if any of the deprecated params are specified, they override `ref`, which was the root cause of my issue.

These are explicitly marked as deprecated (from v3 to v4) in the migration guide [here](https://github.com/kubernetes/git-sync/blob/v4.1.0/v3-to-v4.md#sync-target---branch-and---rev-----ref), and inspecting the source code you can see they are:
- https://github.com/kubernetes/git-sync/blob/97c0d585a83057184821549b30945f2105cc8966/main.go#L306-L309
- https://github.com/kubernetes/git-sync/blob/97c0d585a83057184821549b30945f2105cc8966/main.go#L327-L329
- https://github.com/kubernetes/git-sync/blob/97c0d585a83057184821549b30945f2105cc8966/main.go#L421-L432

I fixed it using a hack, by setting `branch` and `rev` to empty strings, but this is not ideal.

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",yehoshuadimarsky,2024-10-10 21:01:36+00:00,['yehoshuadimarsky'],2024-10-23 15:41:25+00:00,,https://github.com/apache/airflow/issues/42918,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:helm-chart', 'Airflow Helm Chart')]","[{'comment_id': 2409141051, 'issue_id': 2579836527, 'author': 'potiuk', 'body': 'Nice. Feel free to make PR to fix it', 'created_at': datetime.datetime(2024, 10, 13, 21, 40, 14, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-10-13 21:40:14 UTC): Nice. Feel free to make PR to fix it

"
2579775795,issue,closed,completed,Allow `RedshiftDataOperator` to take in an Airflow `Connection` object,"### Description

I would like to pass in an Airflow `Connection` object (or connection ID) to the `RedshiftDataOperator` and be able to execute  with that connection.  Happy to contribute!

Unsure if https://github.com/apache/airflow/issues/35278 is related

If this is not already implemented, I am happy to contribute so please don't hesitate to assign it to me!

### Use case/motivation

It is a cleaner way to pass in connection parameters to the operator

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jayceslesar,2024-10-10 20:28:05+00:00,[],2024-10-10 21:33:45+00:00,2024-10-10 21:33:45+00:00,https://github.com/apache/airflow/issues/42917,"[('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2405981046, 'issue_id': 2579775795, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 10, 20, 28, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2406080054, 'issue_id': 2579775795, 'author': 'jayceslesar', 'body': ""Actually looks like redshift_data is using the boto3 api and an Airflow `Connection` object wouldn't exactly play nicely there...goal is to make vacuum operations on massive databases deferrable so this is not the right place it seems"", 'created_at': datetime.datetime(2024, 10, 10, 21, 33, 41, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-10 20:28:08 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jayceslesar (Issue Creator) on (2024-10-10 21:33:41 UTC): Actually looks like redshift_data is using the boto3 api and an Airflow `Connection` object wouldn't exactly play nicely there...goal is to make vacuum operations on massive databases deferrable so this is not the right place it seems

"
2579429056,issue,open,,"Dynamic task mapping on certain operators, results in broken UI grid/graph view","### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When creating a dag with, for example, `TriggerDagRunOperator` or `EmrTerminateJobFlowOperator` and performing an expand on it, the dag is parsed and it appears on the UI but when I try to look at the grid or graph view, the following error appears:

![image](https://github.com/user-attachments/assets/9d37781c-d708-4627-87ac-d0fdb9d672b0)

 There are two workarounds to this, inserting the operator in a task_group and performing expand on the task_group or setting `operator_extra_links = []`

### What you think should happen instead?

I noticed that this error only appears when attempting to perform dynamic task mapping with operators that make use of operator_extra_links. But in certain conditions, using the workarounds mentioned above, it still works. And according to the documentation it should work in all of them.

### How to reproduce

```
from pendulum import datetime
from airflow.models.dag import DAG
from airflow.decorators import task
from airflow.operators.python import PythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator

with DAG(
    dag_id=""test_dynamic_trigger_error"",
    start_date=datetime(2024, 10, 4),
    catchup=False,
    default_args={""owner"": ""TESTING""},
    tags=[""TESTING"",""MANUAL""],
    schedule_interval=None,
) as dag:

    @task
    def init_params(**context):
        config = context[""dag_run""].conf
        number_of_executions = config.get(""number_of_executions"", 5)
        return number_of_executions

    def producer(**context):
        number_of_executions = context[""ti""].xcom_pull(""init_params"", key=""return_value"")
        result = []
        for x in range(1, number_of_executions+1):
            result.append({
                    ""vars"": {
                        ""part"": str(x)
                    }
                })
        return result

    producer_task = PythonOperator(
        task_id=""producer"",
        python_callable=producer,
    )

    consumer_task = TriggerDagRunOperator.partial(
        task_id=""trigger"",
        trigger_dag_id=""example_simple_dummy"",
    ).expand(conf=producer_task.output)

    init_params() >> producer_task >> consumer_task
```

### Operating System

Amazon Linux 2

### Versions of Apache Airflow Providers

```
apache-airflow-providers-amazon==8.28.0
apache-airflow-providers-apprise==1.4.0
apache-airflow-providers-atlassian-jira==2.7.0
apache-airflow-providers-celery==3.8.1
apache-airflow-providers-cncf-kubernetes==8.4.1
apache-airflow-providers-common-compat==1.2.0
apache-airflow-providers-common-io==1.4.0
apache-airflow-providers-common-sql==1.16.0
apache-airflow-providers-docker==3.13.0
apache-airflow-providers-elasticsearch==5.5.0
apache-airflow-providers-fab==1.3.0
apache-airflow-providers-ftp==3.11.0
apache-airflow-providers-github==2.7.0
apache-airflow-providers-google==10.22.0
apache-airflow-providers-grpc==3.6.0
apache-airflow-providers-hashicorp==3.8.0
apache-airflow-providers-http==4.13.0
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-microsoft-azure==10.4.0
apache-airflow-providers-mysql==5.7.0
apache-airflow-providers-odbc==4.7.0
apache-airflow-providers-openai==1.3.0
apache-airflow-providers-openlineage==1.11.0
apache-airflow-providers-postgres==5.12.0
apache-airflow-providers-redis==3.8.0
apache-airflow-providers-sendgrid==3.6.0
apache-airflow-providers-sftp==4.11.0
apache-airflow-providers-slack==8.9.0
apache-airflow-providers-smtp==1.8.0
apache-airflow-providers-snowflake==5.7.0
apache-airflow-providers-sqlite==3.9.0
apache-airflow-providers-ssh==3.13.1
```

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Deployment on EKS using the official Helm Chart with ArgoCD

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",FelipeRamos-neuro,2024-10-10 17:26:09+00:00,['brunoCCOS'],2024-11-12 16:52:37+00:00,,https://github.com/apache/airflow/issues/42912,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('area:dynamic-task-mapping', 'AIP-42')]","[{'comment_id': 2405954186, 'issue_id': 2579429056, 'author': 'FelipeRamos-neuro', 'body': 'Here is the traceback that I found on the webserver logs\r\n\r\n![image](https://github.com/user-attachments/assets/c71c01cd-eae3-4911-bfda-4dae78078962)', 'created_at': datetime.datetime(2024, 10, 10, 20, 10, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453108637, 'issue_id': 2579429056, 'author': 'jscheffl', 'body': 'Thanks for the bug report. I can re-produce the problem on current main as well as on 2.10.2. So it will be in next 2.10.3 as well,', 'created_at': datetime.datetime(2024, 11, 2, 19, 46, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466252671, 'issue_id': 2579429056, 'author': 'brunoCCOS', 'body': 'Can i be assigned to this one ? I would like to take as my first contribution', 'created_at': datetime.datetime(2024, 11, 9, 15, 11, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466735308, 'issue_id': 2579429056, 'author': 'brunoCCOS', 'body': 'It seens like the PythonOperator which is used to reproduce the error is not currently working in the main branch. All the imports references are wrong and not found. You were able to reproduce the error with the provided code?', 'created_at': datetime.datetime(2024, 11, 10, 13, 25, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466812147, 'issue_id': 2579429056, 'author': 'jscheffl', 'body': 'To re-produce exactly you may need to check-out the tag 2.10.3. Some parts of code will have moved nowadays but I assume it might be still the same.\r\nIf not you can try to re-produce on branch v2-10-test which is the branch we maintain for 2.10.x line - that has less difference to 2.10.3 than main', 'created_at': datetime.datetime(2024, 11, 10, 17, 8, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469146494, 'issue_id': 2579429056, 'author': 'brunoCCOS', 'body': 'Yeah i tried using the exactly same code using the 2.10.3 stable version and didn\'t get the error.  Maybe just updating from 2.10.2 to 2.10.3 already solves the problem. Another thing to mention here is that PythonOperator is deprecated, should change for the @task decorator according to docs (https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/python.html#pythonoperator)\r\n       \r\n\r\n<img width=""1440"" alt=""Captura de Tela 2024-11-11 às 18 50 00"" src=""https://github.com/user-attachments/assets/8fbf9abd-9c68-4d1c-992f-3c69f877a599"">\r\n\r\nI even tried changing the dag passed to the TriggerDagRunOperator but still got no problems\r\n\r\n<img width=""1440"" alt=""Captura de Tela 2024-11-11 às 19 01 15"" src=""https://github.com/user-attachments/assets/d86e8dfc-5a82-4890-b9f2-e42b21cb4a9b"">\r\n\r\n\r\nJust updating from 2.10.2 to 2.10.3 already fix the issue', 'created_at': datetime.datetime(2024, 11, 11, 22, 1, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2471064664, 'issue_id': 2579429056, 'author': 'FelipeRamos-neuro', 'body': 'Yes, @brunoCCOS, it indeed is working now. But I have noticed a new problem, the dag passed at the trigger_dag_id parameter is being triggered, but the **Triggered DAG button** is not pointing to it. If you click it, it redirects to the same ""origin"" dag, in this case:  **test_dynamic_trigger_error**.', 'created_at': datetime.datetime(2024, 11, 12, 16, 52, 35, tzinfo=datetime.timezone.utc)}]","FelipeRamos-neuro (Issue Creator) on (2024-10-10 20:10:25 UTC): Here is the traceback that I found on the webserver logs

![image](https://github.com/user-attachments/assets/c71c01cd-eae3-4911-bfda-4dae78078962)

jscheffl on (2024-11-02 19:46:35 UTC): Thanks for the bug report. I can re-produce the problem on current main as well as on 2.10.2. So it will be in next 2.10.3 as well,

brunoCCOS (Assginee) on (2024-11-09 15:11:27 UTC): Can i be assigned to this one ? I would like to take as my first contribution

brunoCCOS (Assginee) on (2024-11-10 13:25:02 UTC): It seens like the PythonOperator which is used to reproduce the error is not currently working in the main branch. All the imports references are wrong and not found. You were able to reproduce the error with the provided code?

jscheffl on (2024-11-10 17:08:49 UTC): To re-produce exactly you may need to check-out the tag 2.10.3. Some parts of code will have moved nowadays but I assume it might be still the same.
If not you can try to re-produce on branch v2-10-test which is the branch we maintain for 2.10.x line - that has less difference to 2.10.3 than main

brunoCCOS (Assginee) on (2024-11-11 22:01:52 UTC): Yeah i tried using the exactly same code using the 2.10.3 stable version and didn't get the error.  Maybe just updating from 2.10.2 to 2.10.3 already solves the problem. Another thing to mention here is that PythonOperator is deprecated, should change for the @task decorator according to docs (https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/python.html#pythonoperator)
       

<img width=""1440"" alt=""Captura de Tela 2024-11-11 às 18 50 00"" src=""https://github.com/user-attachments/assets/8fbf9abd-9c68-4d1c-992f-3c69f877a599"">

I even tried changing the dag passed to the TriggerDagRunOperator but still got no problems

<img width=""1440"" alt=""Captura de Tela 2024-11-11 às 19 01 15"" src=""https://github.com/user-attachments/assets/d86e8dfc-5a82-4890-b9f2-e42b21cb4a9b"">


Just updating from 2.10.2 to 2.10.3 already fix the issue

FelipeRamos-neuro (Issue Creator) on (2024-11-12 16:52:35 UTC): Yes, @brunoCCOS, it indeed is working now. But I have noticed a new problem, the dag passed at the trigger_dag_id parameter is being triggered, but the **Triggered DAG button** is not pointing to it. If you click it, it redirects to the same ""origin"" dag, in this case:  **test_dynamic_trigger_error**.

"
2578507727,issue,closed,completed,"Task assigned with queue ""heavy"" landed on worker assigned to ""default"" queue","### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I'm currently using an airflow instance setup with `CeleryExecutor`.

I have a task assigned to a ""heavy"" queue:

```python
    @task.virtualenv(
        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,
        requirements=[
            ""my_package=1.1.1""
        ],
        index_urls=[""my_index_url""],
        venv_cache_path=""/tmp"",
        queue=""heavy""
    )
    def my_task(arg1, arg2, params=None):
        ...
```

but for some reason the task is running on a worker configured to only handle the default queue?

Worker Queues in flower:
![image](https://github.com/user-attachments/assets/64b9b6a8-2c9c-40f1-a55b-518452999852)

DAG RUN details:
![image](https://github.com/user-attachments/assets/036ccc27-33eb-4047-861c-ca065eae8750)


**-> IMPORTANT: The DAG Run existed prior to the addition of the `queue=""heavy""` to the dag's task. And the Task state was cleared in order to restart the task.**

### What you think should happen instead?

Tasks assigned to a specific queue should only run in workers assigned to that specific queue.

### How to reproduce

Not sure how.

### Operating System

Ubuntu 22

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

`CeleryExecutor`
2 workers:
```
celery worker --concurrency ${DEFAULT_WORKER_CONCURRENCY}
celery worker -q heavy --concurrency ${HEAVY_WORKER_CONCURRENCY}
```

### Anything else?

**-> IMPORTANT: The DAG Run existed prior to the addition of the `queue=""heavy""` to the dag's task. And the Task state was cleared in order to restart the task.**

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",pedro-cf,2024-10-10 11:09:24+00:00,[],2024-10-13 15:54:17+00:00,2024-10-13 15:54:17+00:00,https://github.com/apache/airflow/issues/42894,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:celery', '')]","[{'comment_id': 2409029853, 'issue_id': 2578507727, 'author': 'potiuk', 'body': '> -> IMPORTANT: The DAG Run existed prior to the addition of the queue=""heavy"" to the dag\'s task. And the Task state was cleared in order to restart the task.\r\n\r\n\r\nThat\'s the reason. Past dag-run queues cannot be changed after dag_run and specifically ""task instance"" object entry gets created.\r\n\r\nSee https://airflow.apache.org/docs/apache-airflow/stable/database-erd-ref.html - the task_instance model contains ""queue"" so once task instance has the queue set there, re-running the same task instance will use what is there. \r\n\r\nYou could likely modify it manually, but currently there is no way - I think to modify it from UI - other than running backfill (@dstandish ?) which I think should do what you want.\r\n\r\nWhich could be possibly a good idea to change in Airlfow 3. There are similar discussions happening about other features of backfill and pool behaviours at the devlist that might get improved in Airflow 3 -  https://lists.apache.org/thread/zbm6tvlcz62nc9hl1mzrzz9t4bcrjngc , https://lists.apache.org/thread/jmj842wsw78clk9twdrz1t71ogsbk10s and others - so if you think it\'s a good idea to introduce such feature, feel free to start a new thread at the devlist.\r\n\r\nConverting it into a discussion in case more discussion here is needed, but I encourage you to continue at the devlist.', 'created_at': datetime.datetime(2024, 10, 13, 15, 54, 11, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-10-13 15:54:11 UTC): That's the reason. Past dag-run queues cannot be changed after dag_run and specifically ""task instance"" object entry gets created.

See https://airflow.apache.org/docs/apache-airflow/stable/database-erd-ref.html - the task_instance model contains ""queue"" so once task instance has the queue set there, re-running the same task instance will use what is there. 

You could likely modify it manually, but currently there is no way - I think to modify it from UI - other than running backfill (@dstandish ?) which I think should do what you want.

Which could be possibly a good idea to change in Airlfow 3. There are similar discussions happening about other features of backfill and pool behaviours at the devlist that might get improved in Airflow 3 -  https://lists.apache.org/thread/zbm6tvlcz62nc9hl1mzrzz9t4bcrjngc , https://lists.apache.org/thread/jmj842wsw78clk9twdrz1t71ogsbk10s and others - so if you think it's a good idea to introduce such feature, feel free to start a new thread at the devlist.

Converting it into a discussion in case more discussion here is needed, but I encourage you to continue at the devlist.

"
2578357135,issue,closed,completed,Handle request errors in new UI,"We need to handle request errors in the UI.

1. We need to make sure we are properly receiving errors
2. Make sure errors are correctly typed
3. Alert banner or toast to show errors to the user",bbovenzi,2024-10-10 10:22:26+00:00,['bbovenzi'],2024-10-11 11:53:27+00:00,2024-10-11 11:53:27+00:00,https://github.com/apache/airflow/issues/42893,"[('area:UI', 'Related to UI/UX. For Frontend Developers.')]",[],
2578340678,issue,closed,completed,unicodecsv dependency makes apache-airflow Python package un-installable,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Airflow cannot be installed in a venv with any system that properly solves dependencies and version requirements.

This happens because the lowest supported Python version of `apache-airflow` is 3.8, while the highest supported version of `unicodecsv` is 3.5. So there is no Python version that would be supported by `apache-airflow` and all of its dependencies, making it un-installable.

### What you think should happen instead?

I know `unicodecsv` has been dropped as a dependency very recently, but no new version has been released since then.

### How to reproduce

I encountered the issue with Poetry, but any tool that ensures all requirements for dependency packages are met will encounter the same issue.  
I encountered the issue targeting Python 3.8, but since there is no overlap between supported Python versions between `unicecsv` and `apache-airflow` it doesn't really matter.

Paste the following `pyproject.toml` into an empty directory:
```toml
[tool.poetry]
name = ""airflow-dependency-test""
version = ""0.1.0""
description = """"
authos = [""Your Name <you@example.com>""]

[tool.poetry.dependencies]
python = ""~3.8""
apache-airflow = ""2.10.2""

[tool.poetry.dev-dependencies]

[build-system]
requires = [""poetry-core >= 1.0.0""]
build-backend = ""poetry-core.masonry.api""
```

In a terminal navigate to the directory and run `poetry install`

### Operating System

Ubuntu 22.04

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Chais,2024-10-10 10:14:55+00:00,[],2024-10-13 02:37:05+00:00,2024-10-13 02:28:33+00:00,https://github.com/apache/airflow/issues/42892,"[('kind:bug', 'This is a clearly a bug'), ('invalid', ''), ('area:core', ''), ('area:dependencies', 'Issues related to dependencies problems'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2404681915, 'issue_id': 2578340678, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 10, 10, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408790168, 'issue_id': 2578340678, 'author': 'potiuk', 'body': 'This is invalid. \r\n\r\nAgree - we missed that we have an old and not really used package before, but it should not be prevented from being installed on higher airflow version.\r\n\r\nWhile unicodecsv had indeed only specified classifiers for up to Python 3.5 it had no requirements set to be only used for Python < 3.5 and since it had no dependencies on its own and it did not have python_requires that prevented it from being installed.\r\n\r\nWhat you are likely referring to when you mane are Trove Classifiers:\r\n\r\n```\r\nClassifier: Development Status :: 5 - Production/Stable\r\nClassifier: Intended Audience :: Developers\r\nClassifier: License :: OSI Approved :: BSD License\r\nClassifier: Natural Language :: English\r\nClassifier: Programming Language :: Python :: 2.6\r\nClassifier: Programming Language :: Python :: 2.7\r\nClassifier: Programming Language :: Python :: 3.3\r\nClassifier: Programming Language :: Python :: 3.4\r\nClassifier: Programming Language :: Python :: 3.5\r\nClassifier: Programming Language :: Python :: Implementation :: PyPy\r\nClassifier: Programming Language :: Python :: Implementation :: CPython\r\n```\r\n\r\nThe trove classifiers are defined in https://peps.python.org/pep-0301/ and they are only there to classify projects when you search for them, but they should not be used by packaging tools to decide if they are installed for specific python version (`python_requires is the way to do that). \r\n\r\nI am not sure what errors you had an which tools you used besides poetry - you missed to explain what kind of error you had when you tried to install airflow. But I am quite sure it is installable for higher python versio. In fact I just did it and it works (python 3.11 in this case):\r\n\r\n```\r\n[jarek:~/code/airflow] fix-dependencies-removal+ 1 ± python --version\r\nPython 3.11.6\r\n[jarek:~/code/airflow] fix-dependencies-removal+ ± pip install unicodecsv\r\nDEPRECATION: Loading egg at /Users/jarek/.pyenv/versions/3.11.6/lib/python3.11/site-packages/Flask_OpenID-1.3.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\r\nDEPRECATION: Loading egg at /Users/jarek/.pyenv/versions/3.11.6/lib/python3.11/site-packages/defusedxml-0.8.0rc2-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\r\nDEPRECATION: Loading egg at /Users/jarek/.pyenv/versions/3.11.6/lib/python3.11/site-packages/python3_openid-3.2.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\r\nCollecting unicodecsv\r\n  Using cached unicodecsv-0.14.1-py3-none-any.whl\r\nInstalling collected packages: unicodecsv\r\nSuccessfully installed unicodecsv-0.14.1\r\n```\r\n\r\n\r\nThe setup.py of unicodecsv does not contain python_requires so generally no tool should be limited to installing it on higher python version than 3.5 (and since it is very simple code, it generally installs and works fine on higher versions).\r\n\r\nThis is setup.py of latest `unicode.csv` - there is no limitation of Python version:\r\n\r\n```python\r\nsetup(\r\n    name=\'unicodecsv\',\r\n    version=version,\r\n    description=""Python2\'s stdlib csv module is nice, but it doesn\'t support unicode. This module is a drop-in replacement which *does*."",\r\n    long_description=open(os.path.join(os.path.dirname(__file__), \'README.rst\'), \'rb\').read().decode(\'utf-8\'),\r\n    author=\'Jeremy Dunck\',\r\n    author_email=\'jdunck@gmail.com\',\r\n    url=\'https://github.com/jdunck/python-unicodecsv\',\r\n    packages=find_packages(),\r\n    tests_require=[\'unittest2>=0.5.1\'],\r\n    test_suite=\'runtests.get_suite\',\r\n    license=\'BSD License\',\r\n    classifiers=[\'Development Status :: 5 - Production/Stable\',\r\n                \'Intended Audience :: Developers\',\r\n                \'License :: OSI Approved :: BSD License\',\r\n                \'Natural Language :: English\',\r\n                \'Programming Language :: Python :: 2.6\',\r\n                \'Programming Language :: Python :: 2.7\',\r\n                \'Programming Language :: Python :: 3.3\',\r\n                \'Programming Language :: Python :: 3.4\',\r\n                \'Programming Language :: Python :: 3.5\',\r\n                \'Programming Language :: Python :: Implementation :: PyPy\',\r\n                \'Programming Language :: Python :: Implementation :: CPython\',],\r\n```\r\n\r\n\r\nI am not sure how you came to the conclusion that unicodecsv is not installable for Python 3.8+ - maybe you can explain what exactly error you experienced and what exactly you did in a follow-up discussion but I can assure you Airflow is instalable so this issue is invalid. \r\n\r\nConverting to discussion if you need more. I will also make sure to check if unicodecsv removal is cherry-picked to 2-10 branch - so also we will remove it in the next release, but this is more of a cleanup than problem with installation.', 'created_at': datetime.datetime(2024, 10, 13, 2, 28, 27, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-10 10:15:00 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-10-13 02:28:27 UTC): This is invalid. 

Agree - we missed that we have an old and not really used package before, but it should not be prevented from being installed on higher airflow version.

While unicodecsv had indeed only specified classifiers for up to Python 3.5 it had no requirements set to be only used for Python < 3.5 and since it had no dependencies on its own and it did not have python_requires that prevented it from being installed.

What you are likely referring to when you mane are Trove Classifiers:

```
Classifier: Development Status :: 5 - Production/Stable
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: BSD License
Classifier: Natural Language :: English
Classifier: Programming Language :: Python :: 2.6
Classifier: Programming Language :: Python :: 2.7
Classifier: Programming Language :: Python :: 3.3
Classifier: Programming Language :: Python :: 3.4
Classifier: Programming Language :: Python :: 3.5
Classifier: Programming Language :: Python :: Implementation :: PyPy
Classifier: Programming Language :: Python :: Implementation :: CPython
```

The trove classifiers are defined in https://peps.python.org/pep-0301/ and they are only there to classify projects when you search for them, but they should not be used by packaging tools to decide if they are installed for specific python version (`python_requires is the way to do that). 

I am not sure what errors you had an which tools you used besides poetry - you missed to explain what kind of error you had when you tried to install airflow. But I am quite sure it is installable for higher python versio. In fact I just did it and it works (python 3.11 in this case):

```
[jarek:~/code/airflow] fix-dependencies-removal+ 1 ± python --version
Python 3.11.6
[jarek:~/code/airflow] fix-dependencies-removal+ ± pip install unicodecsv
DEPRECATION: Loading egg at /Users/jarek/.pyenv/versions/3.11.6/lib/python3.11/site-packages/Flask_OpenID-1.3.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330
DEPRECATION: Loading egg at /Users/jarek/.pyenv/versions/3.11.6/lib/python3.11/site-packages/defusedxml-0.8.0rc2-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330
DEPRECATION: Loading egg at /Users/jarek/.pyenv/versions/3.11.6/lib/python3.11/site-packages/python3_openid-3.2.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330
Collecting unicodecsv
  Using cached unicodecsv-0.14.1-py3-none-any.whl
Installing collected packages: unicodecsv
Successfully installed unicodecsv-0.14.1
```


The setup.py of unicodecsv does not contain python_requires so generally no tool should be limited to installing it on higher python version than 3.5 (and since it is very simple code, it generally installs and works fine on higher versions).

This is setup.py of latest `unicode.csv` - there is no limitation of Python version:

```python
setup(
    name='unicodecsv',
    version=version,
    description=""Python2's stdlib csv module is nice, but it doesn't support unicode. This module is a drop-in replacement which *does*."",
    long_description=open(os.path.join(os.path.dirname(__file__), 'README.rst'), 'rb').read().decode('utf-8'),
    author='Jeremy Dunck',
    author_email='jdunck@gmail.com',
    url='https://github.com/jdunck/python-unicodecsv',
    packages=find_packages(),
    tests_require=['unittest2>=0.5.1'],
    test_suite='runtests.get_suite',
    license='BSD License',
    classifiers=['Development Status :: 5 - Production/Stable',
                'Intended Audience :: Developers',
                'License :: OSI Approved :: BSD License',
                'Natural Language :: English',
                'Programming Language :: Python :: 2.6',
                'Programming Language :: Python :: 2.7',
                'Programming Language :: Python :: 3.3',
                'Programming Language :: Python :: 3.4',
                'Programming Language :: Python :: 3.5',
                'Programming Language :: Python :: Implementation :: PyPy',
                'Programming Language :: Python :: Implementation :: CPython',],
```


I am not sure how you came to the conclusion that unicodecsv is not installable for Python 3.8+ - maybe you can explain what exactly error you experienced and what exactly you did in a follow-up discussion but I can assure you Airflow is instalable so this issue is invalid. 

Converting to discussion if you need more. I will also make sure to check if unicodecsv removal is cherry-picked to 2-10 branch - so also we will remove it in the next release, but this is more of a cleanup than problem with installation.

"
2577918665,issue,closed,completed,SlackAPIFileOperator is not propagating filetype API call,"### Apache Airflow Provider(s)

slack

### Versions of Apache Airflow Providers

2.9.2

### Apache Airflow version

2.9.2

### Operating System

MacOS 14.3.1

### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### What happened

`SlackAPIFileOperator` not propagating correctly filetype ending up sending csv file and json as plain text uploads.

As you can see here `filetype` is passed to initial params but not used than in execution.
https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/slack/operators/slack.py#L263-L271

### What you think should happen instead

Should correctly pass down `filetype` value.

Can be fixed as following
```python
class SlackAPIFileOperator(SlackAPIOperator):
    ...
    def execute(self, context: Context):
        self._method_resolver(
            channels=self.channels,
            # For historical reason SlackAPIFileOperator use filename as reference to file
            file=self.filename,
            filetype=self.filetype,   # here missing filetype
            content=self.content,
            initial_comment=self.initial_comment,
            title=self.title,
        )

```

### How to reproduce
```python
slack = SlackAPIFileOperator(
    task_id=""slack_file_upload"",
    dag=dag,
    slack_conn_id=""slack"",
    channel=""#general"",
    initial_comment=""Hello World!"",
    filename=""hello_world.csv"",
    filetype=""csv"",
    content=""hello,world,csv,file"",
)
```

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",hzlmn,2024-10-10 07:32:37+00:00,[],2024-10-17 12:19:45+00:00,2024-10-17 12:19:45+00:00,https://github.com/apache/airflow/issues/42889,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:slack', '')]","[{'comment_id': 2404284537, 'issue_id': 2577918665, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 10, 7, 32, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2416055541, 'issue_id': 2577918665, 'author': 'Bowrna', 'body': '@hzlmn On debugging further with the data you shared above, could you tell me what method_version you are using to upload files using SlackOperator? The default one is ""v2"", while you can still pass the old version ""v1"". \r\n\r\nhttps://github.com/apache/airflow/blob/e20146d44b340f719f7fb432f93741e011690558/providers/src/airflow/providers/slack/operators/slack.py#L232-L234\r\n\r\nHere the execute method to invoke the Slack API chooses the upload based on the method version.\r\n\r\nhttps://github.com/apache/airflow/blob/e20146d44b340f719f7fb432f93741e011690558/providers/src/airflow/providers/slack/operators/slack.py#L257-L271\r\n\r\nThe send_file method call of v1 function invokes API call where we send the filetype. The above issue fix that you have recommended applies here.\r\n\r\nThen there is another version v2 which is the updated version, where we upload file with different SDK method call. We pass a dict of file_uploads. ( in this dict we can pass file(or content) (here file represents the filepath, content represents the actual filecontent), filename, title, snippet_type, alt_txt)\r\n\r\nhttps://github.com/apache/airflow/blob/e20146d44b340f719f7fb432f93741e011690558/providers/src/airflow/providers/slack/hooks/slack.py#L289-L322\r\n\r\nIn the above method we are passing the filetype into snippet_type. But snippet_type is an optional argument for syntax highlighting in case code content is uploaded. ( It can support types like python, HTML etc. Passing the filetype into snippet_type seems to be incorrect). In V2 version, there is no explicit way to pass the filetype, as I think with file upload directly as multipart binary, the metadata or MIME of the file content can be used to find the file type.\r\n\r\n@potiuk Can you tell me if I can raise a fix where we can get another param for snippet_type, rather than passing the filetype into it?', 'created_at': datetime.datetime(2024, 10, 16, 8, 15, 19, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-10 07:32:40 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

Bowrna on (2024-10-16 08:15:19 UTC): @hzlmn On debugging further with the data you shared above, could you tell me what method_version you are using to upload files using SlackOperator? The default one is ""v2"", while you can still pass the old version ""v1"". 

https://github.com/apache/airflow/blob/e20146d44b340f719f7fb432f93741e011690558/providers/src/airflow/providers/slack/operators/slack.py#L232-L234

Here the execute method to invoke the Slack API chooses the upload based on the method version.

https://github.com/apache/airflow/blob/e20146d44b340f719f7fb432f93741e011690558/providers/src/airflow/providers/slack/operators/slack.py#L257-L271

The send_file method call of v1 function invokes API call where we send the filetype. The above issue fix that you have recommended applies here.

Then there is another version v2 which is the updated version, where we upload file with different SDK method call. We pass a dict of file_uploads. ( in this dict we can pass file(or content) (here file represents the filepath, content represents the actual filecontent), filename, title, snippet_type, alt_txt)

https://github.com/apache/airflow/blob/e20146d44b340f719f7fb432f93741e011690558/providers/src/airflow/providers/slack/hooks/slack.py#L289-L322

In the above method we are passing the filetype into snippet_type. But snippet_type is an optional argument for syntax highlighting in case code content is uploaded. ( It can support types like python, HTML etc. Passing the filetype into snippet_type seems to be incorrect). In V2 version, there is no explicit way to pass the filetype, as I think with file upload directly as multipart binary, the metadata or MIME of the file content can be used to find the file type.

@potiuk Can you tell me if I can raise a fix where we can get another param for snippet_type, rather than passing the filetype into it?

"
2577295658,issue,closed,completed,"Status of testing Providers that were prepared on October 10, 2024","### Body

I have a kind request for all the contributors to the latest provider packages release.
Could you please help us to test the RC versions of the providers?

The guidelines on how to test providers can be found in

[Verify providers by contributors](https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDER_PACKAGES.md#verify-the-release-candidate-by-contributors)

Let us know in the comment, whether the issue is addressed.

Those are providers that require testing as there were some substantial changes introduced:


## Provider [amazon: 9.0.0rc1](https://pypi.org/project/apache-airflow-providers-amazon/9.0.0rc1)
   - [ ] [Remove deprecated stuff from Amazon provider package (#42450)](https://github.com/apache/airflow/pull/42450): @vincbeck
     Linked issues:
       - [ ] [Linked Issue #42218](https://github.com/apache/airflow/pull/42218): @borismo
   - [ ] [Support session reuse in RedshiftDataOperator (#42218)](https://github.com/apache/airflow/pull/42218): @borismo
   - [ ] [Add STOPPED to the failure cases for Sagemaker Training Jobs (#42423)](https://github.com/apache/airflow/pull/42423): @ferruzzi
   - [ ] [S3DeleteObjects Operator: Handle dates passed as strings (#42464)](https://github.com/apache/airflow/pull/42464): @ellisms
     Linked issues:
       - [ ] [Linked Issue #42363](https://github.com/apache/airflow/issues/42363): @mgorsk1
   - [ ] [Small fix to AWS AVP cli init script (#42479)](https://github.com/apache/airflow/pull/42479): @o-nikolas
   - [x] [Make `AwsTaskLogFetcher` faster by reducing the amount of sleep (#42449)](https://github.com/apache/airflow/pull/42449): @smsm1-ito
   - [ ] [Fix logout in AWS auth manager (#42447)](https://github.com/apache/airflow/pull/42447): @vincbeck
   - [ ] [handle ClientError raised after key is missing during DyanmoDB table.get_item (#42408)](https://github.com/apache/airflow/pull/42408): @Lee-W
   - [x] [Drop python3.8 support core and providers (#42766)](https://github.com/apache/airflow/pull/42766): @jscheffl
     Linked issues:
       - [x] [Linked Issue #42742](https://github.com/apache/airflow/pull/42742): @jscheffl
   - [ ] [Removed conditional check for task context logging in airflow version 2.8.0 and above (#42764)](https://github.com/apache/airflow/pull/42764): @dirrao
   - [x] [Rename dataset related python variable names to asset (#41348)](https://github.com/apache/airflow/pull/41348): @Lee-W
   - [ ] [Remove identity center auth manager cli (#42481)](https://github.com/apache/airflow/pull/42481): @o-nikolas
   - [ ] [Refactor AWS Auth manager user output (#42454)](https://github.com/apache/airflow/pull/42454): @o-nikolas
   - [ ] [Remove `sqlalchemy-redshift` dependency from Amazon provider (#42830)](https://github.com/apache/airflow/pull/42830): @vincbeck
   - [ ] [Revert ""Remove `sqlalchemy-redshift` dependency from Amazon provider"" (#42864)](https://github.com/apache/airflow/pull/42864): @mobuchowski
## Provider [apache.beam: 5.8.1rc1](https://pypi.org/project/apache-airflow-providers-apache-beam/5.8.1rc1)
   - [ ] [Bugfix/dataflow job location passing (#41887)](https://github.com/apache/airflow/pull/41887): @lukas-mi
## Provider [apache.kafka: 1.6.1rc1](https://pypi.org/project/apache-airflow-providers-apache-kafka/1.6.1rc1)
   - [x] [Remove callable functions parameter from kafka operator template_fields (#42555)](https://github.com/apache/airflow/pull/42555): @gopidesupavan
     Linked issues:
       - [x] [Linked Issue #42502](https://github.com/apache/airflow/issues/42502): @mxmrlt
## Provider [apache.spark: 4.11.1rc1](https://pypi.org/project/apache-airflow-providers-apache-spark/4.11.1rc1)
   - [ ] [The spark hook resolve_kerberos_principal function code update when airflow version 2.8.0 and above (#42777)](https://github.com/apache/airflow/pull/42777): @dirrao
## Provider [celery: 3.8.3rc1](https://pypi.org/project/apache-airflow-providers-celery/3.8.3rc1)
   - [ ] [All executors should inherit from BaseExecutor (#41904)](https://github.com/apache/airflow/pull/41904): @dstandish
   - [ ] [Remove state sync during celery task processing (#41870)](https://github.com/apache/airflow/pull/41870): @Kytha
   - [x] [Standard provider bash operator (#42252)](https://github.com/apache/airflow/pull/42252): @gopidesupavan
## Provider [cloudant: 4.0.1rc1](https://pypi.org/project/apache-airflow-providers-cloudant/4.0.1rc1)
   - [x] [Drop python3.8 support core and providers (#42766)](https://github.com/apache/airflow/pull/42766): @jscheffl
     Linked issues:
       - [x] [Linked Issue #42742](https://github.com/apache/airflow/pull/42742): @jscheffl
## Provider [cncf.kubernetes: 9.0.0rc1](https://pypi.org/project/apache-airflow-providers-cncf-kubernetes/9.0.0rc1)
   - [ ] [kubernetes executor cleanup_stuck_queued_tasks optimization (#41220)](https://github.com/apache/airflow/pull/41220): @dirrao
   - [ ] [All executors should inherit from BaseExecutor (#41904)](https://github.com/apache/airflow/pull/41904): @dstandish
   - [ ] [Fix mark as success when pod fails while fetching log (#42815)](https://github.com/apache/airflow/pull/42815): @romsharon98
   - [x] [Fix SparkKubernetesOperator spark name. (#42427)](https://github.com/apache/airflow/pull/42427): @gopidesupavan
     Linked issues:
       - [x] [Linked Issue #41188](https://github.com/apache/airflow/issues/41188): @andallo
   - [ ] [KubernetesPodOperator never stops if credentials are refreshed (#42361)](https://github.com/apache/airflow/pull/42361): @paolo-moriello
   - [ ] [Added unit tests and restructred `await_xcom_sidecar_container_start` method. (#42504)](https://github.com/apache/airflow/pull/42504): @harjeevanmaan
     Linked issues:
       - [ ] [Linked Issue #42132](https://github.com/apache/airflow/issues/42132): @captify-mkambur
   - [ ] [KubernetesHook kube_config extra can take dict (#41413)](https://github.com/apache/airflow/pull/41413): @dstandish
   - [x] [Drop python3.8 support core and providers (#42766)](https://github.com/apache/airflow/pull/42766): @jscheffl
     Linked issues:
       - [x] [Linked Issue #42742](https://github.com/apache/airflow/pull/42742): @jscheffl
   - [ ] [Remove airflow_version from k8s executor pod selector (#42751)](https://github.com/apache/airflow/pull/42751): @dstandish
## Provider [common.compat: 1.2.1rc1](https://pypi.org/project/apache-airflow-providers-common-compat/1.2.1rc1)
   - [x] [Rename dataset related python variable names to asset (#41348)](https://github.com/apache/airflow/pull/41348): @Lee-W
## Provider [common.io: 1.4.2rc1](https://pypi.org/project/apache-airflow-providers-common-io/1.4.2rc1)
   - [x] [Drop python3.8 support core and providers (#42766)](https://github.com/apache/airflow/pull/42766): @jscheffl
     Linked issues:
       - [x] [Linked Issue #42742](https://github.com/apache/airflow/pull/42742): @jscheffl
   - [x] [Rename dataset related python variable names to asset (#41348)](https://github.com/apache/airflow/pull/41348): @Lee-W
## Provider [common.sql: 1.18.0rc1](https://pypi.org/project/apache-airflow-providers-common-sql/1.18.0rc1)
   - [x] [feat(providers/common/sql): add warning to connection setter (#42736)](https://github.com/apache/airflow/pull/42736): @Lee-W
   - [x] [FIX: Only pass connection to sqlalchemy engine in JdbcHook (#42705)](https://github.com/apache/airflow/pull/42705): @dabla
     Linked issues:
       - [x] [Linked Issue #42664](https://github.com/apache/airflow/issues/42664): @emredjan
## Provider [databricks: 6.11.0rc1](https://pypi.org/project/apache-airflow-providers-databricks/6.11.0rc1)
   - [ ] [Add `on_kill` to Databricks Workflow Operator (#42115)](https://github.com/apache/airflow/pull/42115): @R7L208
   - [ ] [Add warning log in`DatabricksTaskBaseOperator`  when task_key>100 (#42813)](https://github.com/apache/airflow/pull/42813): @rawwar
     Linked issues:
       - [ ] [Linked Issue #41816](https://github.com/apache/airflow/issues/41816): @rawwar
   - [ ] [Add debug logs to print Request/Response data in  Databricks provider (#42662)](https://github.com/apache/airflow/pull/42662): @rawwar
## Provider [dbt.cloud: 3.11.0rc1](https://pypi.org/project/apache-airflow-providers-dbt-cloud/3.11.0rc1)
   - [x] [Add ability to provide proxy for dbt Cloud connection (#42737)](https://github.com/apache/airflow/pull/42737): @b-per
   - [ ] [Simplify code for recent dbt provider change (#42840)](https://github.com/apache/airflow/pull/42840): @kaxil
## Provider [elasticsearch: 5.5.2rc1](https://pypi.org/project/apache-airflow-providers-elasticsearch/5.5.2rc1)
   - [ ] [Removed conditional check for task context logging in airflow version 2.8.0 and above (#42764)](https://github.com/apache/airflow/pull/42764): @dirrao
## Provider [fab: 1.4.1rc1](https://pypi.org/project/apache-airflow-providers-fab/1.4.1rc1)
   - [ ] [Update Rest API tests to no longer rely on FAB auth manager. Move tests specific to FAB permissions to FAB provider (#42523)](https://github.com/apache/airflow/pull/42523): @vincbeck
   - [x] [Rename dataset related python variable names to asset (#41348)](https://github.com/apache/airflow/pull/41348): @Lee-W
   - [ ] [Simplify expression for get_permitted_dag_ids query (#42484)](https://github.com/apache/airflow/pull/42484): @dstandish
## Provider [google: 10.24.0rc1](https://pypi.org/project/apache-airflow-providers-google/10.24.0rc1)
   - [ ] [Add 'retry_if_resource_not_ready' logic for DataprocCreateClusterOperator and DataprocCreateBatchOperator (#42703)](https://github.com/apache/airflow/pull/42703): @MaksYermak
   - [ ] [Publish Dataproc Serverless Batch link after it starts if batch_id was provided (#41153)](https://github.com/apache/airflow/pull/41153): @rafalh
   - [x] [Fix gcp_conn_id in PubsubPullTrigger (#42671)](https://github.com/apache/airflow/pull/42671): @gopidesupavan
     Linked issues:
       - [x] [Linked Issue #42160](https://github.com/apache/airflow/issues/42160): @nickmarx12345678
   - [x] [Fix consistent return response from PubSubPullSensor (#42080)](https://github.com/apache/airflow/pull/42080): @gopidesupavan
     Linked issues:
       - [x] [Linked Issue #41877](https://github.com/apache/airflow/issues/41877): @arpit-maheshwari1
   - [ ] [Undo partition exclusion from the table name when splitting a full BigQuery table name (#42541)](https://github.com/apache/airflow/pull/42541): @moiseenkov
   - [ ] [Fix GCP text to speech operator uri fetch (#42309)](https://github.com/apache/airflow/pull/42309): @olegkachur-e
   - [x] [Refactor ``bucket.get_blob`` calls in ``GCSHook`` to handle validation for non-existent objects. (#42474)](https://github.com/apache/airflow/pull/42474): @jsjasonseba
     Linked issues:
       - [x] [Linked Issue #42439](https://github.com/apache/airflow/issues/42439): @shahar1
   - [ ] [Bugfix/dataflow job location passing (#41887)](https://github.com/apache/airflow/pull/41887): @lukas-mi
   - [ ] [Removed conditional check for task context logging in airflow version 2.8.0 and above (#42764)](https://github.com/apache/airflow/pull/42764): @dirrao
   - [x] [Rename dataset related python variable names to asset (#41348)](https://github.com/apache/airflow/pull/41348): @Lee-W
   - [ ] [Deprecate `AutoMLBatchPredictOperator` and refactor AutoML system tests (#42260)](https://github.com/apache/airflow/pull/42260): @olegkachur-e
## Provider [jdbc: 4.5.2rc1](https://pypi.org/project/apache-airflow-providers-jdbc/4.5.2rc1)
   - [x] [FIX: Only pass connection to sqlalchemy engine in JdbcHook (#42705)](https://github.com/apache/airflow/pull/42705): @dabla
     Linked issues:
       - [x] [Linked Issue #42664](https://github.com/apache/airflow/issues/42664): @emredjan
## Provider [microsoft.azure: 10.5.1rc1](https://pypi.org/project/apache-airflow-providers-microsoft-azure/10.5.1rc1)
   - [x] [BUGFIX: Paginated results in MSGraphAsyncOperator (#42414)](https://github.com/apache/airflow/pull/42414): @dabla
   - [x] [Bugfix/42575 workaround pin azure kusto data (#42576)](https://github.com/apache/airflow/pull/42576): @jscheffl
     Linked issues:
       - [x] [Linked Issue #42575](https://github.com/apache/airflow/issues/42575): @jscheffl
   - [ ] [Removed conditional check for task context logging in airflow version 2.8.0 and above (#42764)](https://github.com/apache/airflow/pull/42764): @dirrao
## Provider [mysql: 5.7.2rc1](https://pypi.org/project/apache-airflow-providers-mysql/5.7.2rc1)
   - [x] [Rename dataset related python variable names to asset (#41348)](https://github.com/apache/airflow/pull/41348): @Lee-W
## Provider [openlineage: 1.12.2rc1](https://pypi.org/project/apache-airflow-providers-openlineage/1.12.2rc1)
   - [x] [Standard provider bash operator (#42252)](https://github.com/apache/airflow/pull/42252): @gopidesupavan
   - [x] [Drop python3.8 support core and providers (#42766)](https://github.com/apache/airflow/pull/42766): @jscheffl
     Linked issues:
       - [x] [Linked Issue #42742](https://github.com/apache/airflow/pull/42742): @jscheffl
   - [x] [Rename dataset related python variable names to asset (#41348)](https://github.com/apache/airflow/pull/41348): @Lee-W
## Provider [opensearch: 1.5.0rc1](https://pypi.org/project/apache-airflow-providers-opensearch/1.5.0rc1)
   - [x] [Add feature to read log from opensearch (#41799)](https://github.com/apache/airflow/pull/41799): @Owen-CH-Leung
     Linked issues:
       - [x] [Linked Issue #33619](https://github.com/apache/airflow/issues/33619): @djadeau
   - [x] [Don't pass auth to opensearch client with empty login and password (#39982)](https://github.com/apache/airflow/pull/39982): @pdebelak
     Linked issues:
       - [x] [Linked Issue #39979](https://github.com/apache/airflow/issues/39979): @pdebelak
   - [ ] [Removed conditional check for task context logging in airflow version 2.8.0 and above (#42764)](https://github.com/apache/airflow/pull/42764): @dirrao
## Provider [postgres: 5.13.1rc1](https://pypi.org/project/apache-airflow-providers-postgres/5.13.1rc1)
   - [x] [Rename dataset related python variable names to asset (#41348)](https://github.com/apache/airflow/pull/41348): @Lee-W
## Provider [snowflake: 5.8.0rc1](https://pypi.org/project/apache-airflow-providers-snowflake/5.8.0rc1)
   - [x] [Add Snowpark operator and decorator (#42457)](https://github.com/apache/airflow/pull/42457): @sfc-gh-jdu
     Linked issues:
       - [x] [Linked Issue #24456](https://github.com/apache/airflow/issues/24456): @sfc-gh-madkins
   - [ ] [Fixes: SnowflakeSqlApiOperator not resolving parameters in SQL (#42719)](https://github.com/apache/airflow/pull/42719): @harjeevanmaan
     Linked issues:
       - [ ] [Linked Issue #42033](https://github.com/apache/airflow/issues/42033): @chris-okorodudu
   - [ ] [Make `private_key_content` a sensitive field  in Snowflake connection (#42649)](https://github.com/apache/airflow/pull/42649): @rawwar
     Linked issues:
       - [ ] [Linked Issue #42496](https://github.com/apache/airflow/issues/42496): @TJaniF
## Provider [trino: 5.8.1rc1](https://pypi.org/project/apache-airflow-providers-trino/5.8.1rc1)
   - [x] [Rename dataset related python variable names to asset (#41348)](https://github.com/apache/airflow/pull/41348): @Lee-W
## Provider [ydb: 1.4.0rc1](https://pypi.org/project/apache-airflow-providers-ydb/1.4.0rc1)
   - [ ] [Add an ability to use scan queries via new YDB operator (#42311)](https://github.com/apache/airflow/pull/42311): @vgvoleg

<!--

NOTE TO RELEASE MANAGER:

You can move here the providers that have doc-only changes or for which changes are trivial, and
you could assess that they are OK.

-->
All users involved in the PRs:
@Lee-W @jscheffl @romsharon98 @borismo @sfc-gh-jdu @smsm1-ito @o-nikolas @gopidesupavan @Kytha @paolo-moriello @lukas-mi @Owen-CH-Leung @dirrao @olegkachur-e @rafalh @mobuchowski @kaxil @pdebelak @MaksY

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2024-10-10 01:33:32+00:00,[],2024-10-14 07:15:52+00:00,2024-10-14 07:15:51+00:00,https://github.com/apache/airflow/issues/42882,"[('area:providers', ''), ('kind:meta', 'High-level information important to the community'), ('testing status', 'Status of testing releases')]","[{'comment_id': 2404576669, 'issue_id': 2577295658, 'author': 'dabla', 'body': '[PR 42414](https://github.com/apache/airflow/pull/42414) working as expected', 'created_at': datetime.datetime(2024, 10, 10, 9, 28, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2405215524, 'issue_id': 2577295658, 'author': 'dabla', 'body': '[PR 42705](https://github.com/apache/airflow/pull/42705) working as expected, so all my PR are good', 'created_at': datetime.datetime(2024, 10, 10, 14, 14, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2405967113, 'issue_id': 2577295658, 'author': 'pdebelak', 'body': '[PR 39982](https://github.com/apache/airflow/pull/39982) working as expected for opensearch.', 'created_at': datetime.datetime(2024, 10, 10, 20, 18, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2406024296, 'issue_id': 2577295658, 'author': 'sfc-gh-jdu', 'body': 'PR #42457 working as expected', 'created_at': datetime.datetime(2024, 10, 10, 20, 56, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2406065494, 'issue_id': 2577295658, 'author': 'jscheffl', 'body': 'OK for the packages... but the changelog ""Drop python3.8 support core and providers"" is only added for _some_ providers - but actually this yields for all. Do we need to fix changelog for all? Pypi (correctly) un-lists 3.8 support... but changelog most probably was not generated as I did no touch all providers with the change.', 'created_at': datetime.datetime(2024, 10, 10, 21, 25, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2406438994, 'issue_id': 2577295658, 'author': 'Owen-CH-Leung', 'body': 'PR #41799 working as expected', 'created_at': datetime.datetime(2024, 10, 11, 2, 44, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2406491118, 'issue_id': 2577295658, 'author': 'Lee-W', 'body': 'Tested https://github.com/apache/airflow/pull/42736 and https://github.com/apache/airflow/pull/41348', 'created_at': datetime.datetime(2024, 10, 11, 3, 32, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2407149067, 'issue_id': 2577295658, 'author': 'smsm1-ito', 'body': ""Regarding #42449 I've tested it on our system, and it looks good, with much faster logging stopping the builds failing due to the tasks going missing. \r\n\r\nI've not tested the original issue around multiple log items in quick succession going missing from the AWS logs."", 'created_at': datetime.datetime(2024, 10, 11, 10, 49, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408040018, 'issue_id': 2577295658, 'author': 'b-per', 'body': 'Tested #42737 for dbt Cloud, and it works as expected', 'created_at': datetime.datetime(2024, 10, 11, 19, 54, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408506179, 'issue_id': 2577295658, 'author': 'gopidesupavan', 'body': 'Tested #42427, #42671 and #42080. working fine..', 'created_at': datetime.datetime(2024, 10, 12, 10, 3, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408527273, 'issue_id': 2577295658, 'author': 'gopidesupavan', 'body': 'Tested with sample dag no impact on openlineage #42252', 'created_at': datetime.datetime(2024, 10, 12, 11, 18, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408536802, 'issue_id': 2577295658, 'author': 'gopidesupavan', 'body': 'Tested with sample dag no impact on celery executor #42252 , working fine..', 'created_at': datetime.datetime(2024, 10, 12, 11, 54, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408538187, 'issue_id': 2577295658, 'author': 'gopidesupavan', 'body': 'Tested #42555 , working fine..', 'created_at': datetime.datetime(2024, 10, 12, 12, 0, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2410237045, 'issue_id': 2577295658, 'author': 'eladkal', 'body': 'Thank you everyone. Providers are released.\r\n\r\nI invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).', 'created_at': datetime.datetime(2024, 10, 14, 7, 15, 52, tzinfo=datetime.timezone.utc)}]","dabla on (2024-10-10 09:28:27 UTC): [PR 42414](https://github.com/apache/airflow/pull/42414) working as expected

dabla on (2024-10-10 14:14:31 UTC): [PR 42705](https://github.com/apache/airflow/pull/42705) working as expected, so all my PR are good

pdebelak on (2024-10-10 20:18:56 UTC): [PR 39982](https://github.com/apache/airflow/pull/39982) working as expected for opensearch.

sfc-gh-jdu on (2024-10-10 20:56:58 UTC): PR #42457 working as expected

jscheffl on (2024-10-10 21:25:43 UTC): OK for the packages... but the changelog ""Drop python3.8 support core and providers"" is only added for _some_ providers - but actually this yields for all. Do we need to fix changelog for all? Pypi (correctly) un-lists 3.8 support... but changelog most probably was not generated as I did no touch all providers with the change.

Owen-CH-Leung on (2024-10-11 02:44:05 UTC): PR #41799 working as expected

Lee-W on (2024-10-11 03:32:39 UTC): Tested https://github.com/apache/airflow/pull/42736 and https://github.com/apache/airflow/pull/41348

smsm1-ito on (2024-10-11 10:49:23 UTC): Regarding #42449 I've tested it on our system, and it looks good, with much faster logging stopping the builds failing due to the tasks going missing. 

I've not tested the original issue around multiple log items in quick succession going missing from the AWS logs.

b-per on (2024-10-11 19:54:24 UTC): Tested #42737 for dbt Cloud, and it works as expected

gopidesupavan on (2024-10-12 10:03:58 UTC): Tested #42427, #42671 and #42080. working fine..

gopidesupavan on (2024-10-12 11:18:17 UTC): Tested with sample dag no impact on openlineage #42252

gopidesupavan on (2024-10-12 11:54:57 UTC): Tested with sample dag no impact on celery executor #42252 , working fine..

gopidesupavan on (2024-10-12 12:00:42 UTC): Tested #42555 , working fine..

eladkal (Issue Creator) on (2024-10-14 07:15:52 UTC): Thank you everyone. Providers are released.

I invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).

"
2577254014,issue,open,,Metrics Improvement Project,"### Description

From @ferruzzi:

> Currently when you add a new metric to the codebase, you must also manually update the docs page.  The docs page inevitably gets out of date and misses some details.  We want an automated system to generate the docs page based on the actual metrics.  There are also known instances where the same metric is being created and emitted in more than one place, causing duplicate data.  These will have to be fixed manually and an automated check might possibly (stretch goal?)  include checking for same or ”too similar” names while collecting the names for the docs page.

> Phase 1
> Situation:
> We support multiple different Metrics backends [0].  The two main ones are StatsD and OpenTelemetry.  This is managed though an interface class [1] which is implemented for each backend (examples:  StatsD[2] and OTel[3]).   StatsD was the only supported version well into Airflow 2.x and the entire codebase was designed with StatsD in mind so it was a good chunk of work to abstract it out and there are a few remaining tasks to perfect the new implementation.
> Task 1:
> StatsD has a name length limit of around 300 characters.  OTel limits names to 34 characters, but allows tagging.  Our temporary solution was to emit almost everything twice, once in the long format for StatsD and again in the short format with tags for OTel.  We also had to add code [4] to make sure the name is safe for OTel, and other hacks to make it work.
> The first task in this project is to understand the difference in how the two implementations handle their names and them add a ""get_name"" method to the interface: `def get_name(metric_name: str, tags: dict[str: str])`.  In the statsd_logger [2] implementation it will concatenate the tags onto the name and in the OTel implementation it will just return name.
> Once that is implemented, it can be used in the various emit methods (incr, decr, etc) instead of all the name validation code, and search the code for places where we are emitting things more than once and clean it up.
> Example:
> You can see an example in local_task_job_runner [5].  We emit `local_task_job.task_exit.<job_id>.<dag_id>.<task_id>.<return_code>` for StatsD but that results in a name too long for OTel so we also emit `local_task_job.task_exit`, and the name validation method [4] in the OTel implementation catches the one that is too long and just swallows it.  What we should do instead is pass incr() the name and the tags and let StatsD and OTel handle them accordingly.
> [0] https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/metrics.html#metric-descriptions
> [1] https://github.com/apache/airflow/blob/main/airflow/metrics/base_stats_logger.py
> [2] https://github.com/apache/airflow/blob/main/airflow/metrics/statsd_logger.py
> [3] https://github.com/apache/airflow/blob/main/airflow/metrics/otel_logger.py
> [4] https://github.com/apache/airflow/blob/main/airflow/metrics/otel_logger.py#L128
> [5] https://github.com/apache/airflow/blob/main/airflow/jobs/local_task_job_runner.py#L352


### Use case/motivation

From @ferruzzi:

> Currently when you add a new metric to the codebase, you must also manually update the docs page.  The docs page inevitably gets out of date and misses some details.  We want an automated system to generate the docs page based on the actual metrics.  There are also known instances where the same metric is being created and emitted in more than one place, causing duplicate data.  These will have to be fixed manually and an automated check might possibly (stretch goal?)  include checking for same or ”too similar” names while collecting the names for the docs page.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",dannyl1u,2024-10-10 00:43:42+00:00,['dannyl1u'],2024-10-16 17:46:51+00:00,,https://github.com/apache/airflow/issues/42881,"[('kind:feature', 'Feature Requests'), ('area:metrics', '')]","[{'comment_id': 2403677884, 'issue_id': 2577254014, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 10, 0, 43, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2403679558, 'issue_id': 2577254014, 'author': 'dannyl1u', 'body': ""Hello @potiuk @howardyoo @kaxil,\r\nI'm planning on working on this project, and just wanted to confirm that this is indeed something we want for Airflow 3.0 before I begin implementing"", 'created_at': datetime.datetime(2024, 10, 10, 0, 45, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2404437398, 'issue_id': 2577254014, 'author': 'ashb', 'body': 'This sounds great. Could you sketch out the sort of API you are thinking of, and give an example use from somewhere in scheduler job please?', 'created_at': datetime.datetime(2024, 10, 10, 8, 26, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2405924758, 'issue_id': 2577254014, 'author': 'dannyl1u', 'body': ""Hi @ashb, could you clarify what you mean by the use in the scheduler job? \r\n\r\nThe first step is refactoring the metrics code so that https://github.com/apache/airflow/blob/main/airflow/metrics/base_stats_logger.py will have a `get_name` method that https://github.com/apache/airflow/blob/main/airflow/metrics/datadog_logger.py, https://github.com/apache/airflow/blob/main/airflow/metrics/otel_logger.py, https://github.com/apache/airflow/blob/main/airflow/metrics/statsd_logger.py, etc. will inherit the method for their own specific naming conventions. This will improve the code by not needing specific name validators for each implementation https://github.com/apache/airflow/blob/main/airflow/metrics/otel_logger.py#L128.\r\n\r\ncc'ing @ferruzzi in case I'm missing something\r\n\r\nThen, once the above is done, we will plan the next step to build an automated system to generate docs based on the actual metrics.\r\n\r\ncc @arshiazr"", 'created_at': datetime.datetime(2024, 10, 10, 19, 56, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2409050782, 'issue_id': 2577254014, 'author': 'potiuk', 'body': '@ferruzzi @howardyoo -> I think you two should be quite a bit involved in the review and ideas here. \r\n\r\nInitially when we implemented open-telemetry metrics  we thought we could do it in the way that we could emit legacy \'statsd"" metrics using opentelemetry interface (basically implement or use some kind of `opentelemetry -> statsd` bridge), because of some implementation details it turned out to be impossible (or difficult).\r\n\r\nHowever that can still be explored, maybe  that is still a possiblity? Implementing our own interface that wraps both Opentelemetry Metrics and Statsd one is of course a possibility, but (at least intuitively - without knowing all the details) - it could be that opentelemetry API could be used to emit the legacy statsd events in ""mostly"" compatible way. \r\n\r\nAnd implementing it in Airflow 3 gives us also an opportunity for the ""mostly"" part. If we can get 90% of the backwards-compatible statsd metrics in place and only ""few"", ""less important"" metrics changed to be incompatible, maybe that is a way to go?', 'created_at': datetime.datetime(2024, 10, 13, 17, 0, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2411298677, 'issue_id': 2577254014, 'author': 'howardyoo', 'body': 'Hi Jarek (and Dennis),\r\n\r\nYes, in the beginning we hoped that how Airflow used statsd could also be\r\napplied to Airflow Otel in the same way (having that classic statsd metrics\r\nname having much of the context - with its long length), and now it looks\r\nlike we may have to do it the alternate way, but better way for the future\r\n(because to be fair, how stats d were naming metrics inherently had\r\nproblems).\r\nfor state in State.task_states:\r\nStats.incr(\r\nf""ti.finish.{ti.task.dag_id}.{ti.task.task_id}.{state}"",\r\ncount=0,\r\ntags=ti.stats_tags,\r\n)\r\n# Same metric with tagging\r\nStats.incr(\r\n""ti.finish"",\r\ncount=0,\r\ntags={**ti.stats_tags, ""state"": str(state)},\r\n)\r\n\r\nAs for the naming problem, I do believe that Dennis has added codes to\r\nsupport two versions, so even though they get the warning in the OTEL side,\r\nwe are not losing too much of the information (see the example code above),\r\nbut I would say we\'ll be getting some unwanted trimmed metrics flowing into\r\nOTEL.\r\n\r\nThe only problem in that case would be these unwanted and unnecessary\r\nmetrics that would flow into OTEL SDK and eventually getting trimmed.\r\nHowever, I believe we can be able to fix it by applying processor to filter\r\nthem out in the collector level (that\'s why I do believe having collector\r\nin the architecture is important - for it could act as another processing\r\nlayer separated from the source of telemetry).\r\n\r\nThose would be my two cents for now...\r\nAny thoughts, Dennis? Maybe I might be missing something.\r\n\r\nOn Sun, Oct 13, 2024 at 12:00\u202fPM Jarek Potiuk ***@***.***>\r\nwrote:\r\n\r\n> @ferruzzi <https://github.com/ferruzzi> @howardyoo\r\n> <https://github.com/howardyoo> -> I think you two should be quite a bit\r\n> involved in the review and ideas here.\r\n>\r\n> Initially when we implemented open-telemetry metrics we thought we could\r\n> do it in the way that we could emit legacy \'statsd"" metrics using\r\n> opentelemetry interface (basically implement or use some kind of opentelemetry\r\n> -> statsd bridge), because of some implementation details it turned out\r\n> to be impossible (or difficult).\r\n>\r\n> However that can still be explored, maybe that is still a possiblity?\r\n> Implementing our own interface that wraps both Opentelemetry Metrics and\r\n> Statsd one is of course a possibility, but (at least intuitively - without\r\n> knowing all the details) - it could be that opentelemetry API could be used\r\n> to emit the legacy statsd events in ""mostly"" compatible way.\r\n>\r\n> And implementing it in Airflow 3 gives us also an opportunity for the\r\n> ""mostly"" part. If we can get 90% of the backwards-compatible statsd\r\n> interface in place and only ""few"", ""less important"" metrics changed to be\r\n> incompatible, maybe that is a way to go?\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/apache/airflow/issues/42881#issuecomment-2409050782>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AHZNLLUYRUHZ7VWY3HTPWHTZ3KRLLAVCNFSM6AAAAABPVUUOT2VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDIMBZGA2TANZYGI>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>', 'created_at': datetime.datetime(2024, 10, 14, 13, 37, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2411537502, 'issue_id': 2577254014, 'author': 'ashb', 'body': 'Scheduler job = https://github.com/apache/airflow/blob/main/airflow/jobs/scheduler_job_runner.py', 'created_at': datetime.datetime(2024, 10, 14, 15, 4, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412626545, 'issue_id': 2577254014, 'author': 'ArshiaZr', 'body': ""I have just raised a new PR  (https://github.com/apache/airflow/pull/43018). I'd be grateful if you can review it.\r\n\r\ncc @ferruzzi"", 'created_at': datetime.datetime(2024, 10, 15, 1, 19, 22, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-10 00:43:45 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

dannyl1u (Issue Creator) on (2024-10-10 00:45:52 UTC): Hello @potiuk @howardyoo @kaxil,
I'm planning on working on this project, and just wanted to confirm that this is indeed something we want for Airflow 3.0 before I begin implementing

ashb on (2024-10-10 08:26:47 UTC): This sounds great. Could you sketch out the sort of API you are thinking of, and give an example use from somewhere in scheduler job please?

dannyl1u (Issue Creator) on (2024-10-10 19:56:34 UTC): Hi @ashb, could you clarify what you mean by the use in the scheduler job? 

The first step is refactoring the metrics code so that https://github.com/apache/airflow/blob/main/airflow/metrics/base_stats_logger.py will have a `get_name` method that https://github.com/apache/airflow/blob/main/airflow/metrics/datadog_logger.py, https://github.com/apache/airflow/blob/main/airflow/metrics/otel_logger.py, https://github.com/apache/airflow/blob/main/airflow/metrics/statsd_logger.py, etc. will inherit the method for their own specific naming conventions. This will improve the code by not needing specific name validators for each implementation https://github.com/apache/airflow/blob/main/airflow/metrics/otel_logger.py#L128.

cc'ing @ferruzzi in case I'm missing something

Then, once the above is done, we will plan the next step to build an automated system to generate docs based on the actual metrics.

cc @arshiazr

potiuk on (2024-10-13 17:00:15 UTC): @ferruzzi @howardyoo -> I think you two should be quite a bit involved in the review and ideas here. 

Initially when we implemented open-telemetry metrics  we thought we could do it in the way that we could emit legacy 'statsd"" metrics using opentelemetry interface (basically implement or use some kind of `opentelemetry -> statsd` bridge), because of some implementation details it turned out to be impossible (or difficult).

However that can still be explored, maybe  that is still a possiblity? Implementing our own interface that wraps both Opentelemetry Metrics and Statsd one is of course a possibility, but (at least intuitively - without knowing all the details) - it could be that opentelemetry API could be used to emit the legacy statsd events in ""mostly"" compatible way. 

And implementing it in Airflow 3 gives us also an opportunity for the ""mostly"" part. If we can get 90% of the backwards-compatible statsd metrics in place and only ""few"", ""less important"" metrics changed to be incompatible, maybe that is a way to go?

howardyoo on (2024-10-14 13:37:11 UTC): Hi Jarek (and Dennis),

Yes, in the beginning we hoped that how Airflow used statsd could also be
applied to Airflow Otel in the same way (having that classic statsd metrics
name having much of the context - with its long length), and now it looks
like we may have to do it the alternate way, but better way for the future
(because to be fair, how stats d were naming metrics inherently had
problems).
for state in State.task_states:
Stats.incr(
f""ti.finish.{ti.task.dag_id}.{ti.task.task_id}.{state}"",
count=0,
tags=ti.stats_tags,
)
# Same metric with tagging
Stats.incr(
""ti.finish"",
count=0,
tags={**ti.stats_tags, ""state"": str(state)},
)

As for the naming problem, I do believe that Dennis has added codes to
support two versions, so even though they get the warning in the OTEL side,
we are not losing too much of the information (see the example code above),
but I would say we'll be getting some unwanted trimmed metrics flowing into
OTEL.

The only problem in that case would be these unwanted and unnecessary
metrics that would flow into OTEL SDK and eventually getting trimmed.
However, I believe we can be able to fix it by applying processor to filter
them out in the collector level (that's why I do believe having collector
in the architecture is important - for it could act as another processing
layer separated from the source of telemetry).

Those would be my two cents for now...
Any thoughts, Dennis? Maybe I might be missing something.

On Sun, Oct 13, 2024 at 12:00 PM Jarek Potiuk ***@***.***>
wrote:

ashb on (2024-10-14 15:04:37 UTC): Scheduler job = https://github.com/apache/airflow/blob/main/airflow/jobs/scheduler_job_runner.py

ArshiaZr on (2024-10-15 01:19:22 UTC): I have just raised a new PR  (https://github.com/apache/airflow/pull/43018). I'd be grateful if you can review it.

cc @ferruzzi

"
2576890134,issue,closed,completed,AIP-84 Migrate the public endpoint Get Airflow Version Info to FastAPI,"### Description

Currently the Get Airflow Version Info public endpoint is at `api_connexion/endpoints/version_endpoint.py` under `get_version`. We need to migrate it to the `api_fastapi/views/public/versions.py` under a `get_airflow_version_info` or similar.

Features and functionality of the endpoint will remain unchanged.

### Use case/motivation

AIP-84 cWiki Link: https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API

### Related issues

Related to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-10-09 20:30:28+00:00,['omkar-foss'],2024-10-24 13:55:16+00:00,2024-10-24 13:55:16+00:00,https://github.com/apache/airflow/issues/42879,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]",[],
2576882031,issue,closed,completed,AIP-84 Migrate the public endpoint Get DAG Stats to FastAPI,"### Description

Currently the Get DAG Stats public endpoint is at `api_connexion/endpoints/dag_stats_endpoint.py` under `get_dag_stats`. We need to migrate it to the `api_fastapi/views/public/dag_stats.py` under a `get_dag_stats` or similar.

### Use case/motivation

AIP-84 cWiki Link: https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API

### Related issues

Related to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-10-09 20:25:22+00:00,['omkar-foss'],2024-11-05 15:58:13+00:00,2024-11-05 15:58:13+00:00,https://github.com/apache/airflow/issues/42877,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]",[],
2576878389,issue,closed,completed,AIP-84 Migrate the public endpoint Get DAG Source to FastAPI,"### Description

Currently the Get DAG Source public endpoint is at `api_connexion/endpoints/dag_source_endpoint.py` under `get_dag_source`. We need to migrate it to the `api_fastapi/views/public/dag_sources.py` under a `get_dag_source` or similar.

Features and functionality of the endpoint will remain unchanged.

### Use case/motivation

AIP-84 cWiki Link: https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API

### Related issues

Related to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-10-09 20:23:03+00:00,['omkar-foss'],2024-10-30 14:57:55+00:00,2024-10-30 14:57:55+00:00,https://github.com/apache/airflow/issues/42876,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]",[],
2576871856,issue,closed,completed,AIP-84 Migrate the public endpoint Get Tasks to FastAPI,"### Description

Currently the Get Tasks public endpoint is at `api_connexion/endpoints/task_endpoint.py` under `get_tasks`. We need to migrate it to the `api_fastapi/views/public/tasks.py` under a `get_tasks` or similar.

Features and functionality of the endpoint will remain unchanged.

### Use case/motivation

AIP-84 cWiki Link: https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API

### Related issues

Related to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-10-09 20:18:59+00:00,['omkar-foss'],2024-11-15 13:13:10+00:00,2024-11-15 13:13:09+00:00,https://github.com/apache/airflow/issues/42875,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]",[],
2576868442,issue,closed,completed,AIP-84 Migrate the public endpoint Get Task to FastAPI,"### Description

Currently the Get Task public endpoint is at `api_connexion/endpoints/task_endpoint.py` under `get_task`. We need to migrate it to the `api_fastapi/views/public/tasks.py` under a `get_task` or similar.

Features and functionality of the endpoint will remain unchanged.

### Use case/motivation

AIP-84 cWiki Link: https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API

### Related issues

Related to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-10-09 20:16:48+00:00,['omkar-foss'],2024-11-13 08:52:48+00:00,2024-11-13 08:52:48+00:00,https://github.com/apache/airflow/issues/42874,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]",[],
2576498083,issue,closed,completed,xcom saved value of type numpy.bool_ is changed from False to True,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

A `False` value of `numpy.bool_` type pushed to xcom is changed to `True` on pull. A `True` `numpy.bool_` value stays `True`.
A regular Python `bool` value works fine.


### What you think should happen instead?

`False` value of `numpy.bool_` type pushed to xcom should stay `False`.

### How to reproduce

    from datetime import datetime
    import logging
    from airflow.models.dag import DAG
    from airflow.operators.python import PythonOperator
    import numpy
    
    
    def xcom_test(ti):
        value_in = numpy.bool_(False)
        ti.xcom_push(key=""test_value"", value=value_in)
        value_out = ti.xcom_pull(key=""test_value"", task_ids=""xcom_test"")
        logging.info(""In: "" + str(value_in))
        logging.info(""Out: "" + str(value_out))
    
    
    with DAG(
            ""test"",
            start_date=datetime(2024, 8, 27),
            schedule=None,
            catchup=False,
            max_active_runs=1
    ) as dag:
        PythonOperator(
            task_id=""xcom_test"",
            python_callable=xcom_test
        )

Logs:

    [2024-10-09, 18:56:12 CEST] {test.py:12} INFO - In: False
    [2024-10-09, 18:56:12 CEST] {test.py:13} INFO - Out: True

### Operating System

Ubuntu 24.04

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",kkeiichi,2024-10-09 17:15:10+00:00,[],2024-10-13 01:16:47+00:00,2024-10-13 01:16:47+00:00,https://github.com/apache/airflow/issues/42868,"[('kind:bug', 'This is a clearly a bug'), ('pending-response', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2402874978, 'issue_id': 2576498083, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 9, 17, 15, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2403193794, 'issue_id': 2576498083, 'author': 'gopidesupavan', 'body': 'hm, strange, looking into that.', 'created_at': datetime.datetime(2024, 10, 9, 19, 7, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2403305431, 'issue_id': 2576498083, 'author': 'gopidesupavan', 'body': 'Alright the value numpy type not supported currently in xcom. any reason your using xcom for numpy types?', 'created_at': datetime.datetime(2024, 10, 9, 19, 47, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2403443569, 'issue_id': 2576498083, 'author': 'kkeiichi', 'body': 'No reason. It just seemed odd to me. I intended to use a regular bool, but accidentally used numpy. I was surprised by the result.', 'created_at': datetime.datetime(2024, 10, 9, 21, 16, 17, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-09 17:15:13 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

gopidesupavan on (2024-10-09 19:07:32 UTC): hm, strange, looking into that.

gopidesupavan on (2024-10-09 19:47:18 UTC): Alright the value numpy type not supported currently in xcom. any reason your using xcom for numpy types?

kkeiichi (Issue Creator) on (2024-10-09 21:16:17 UTC): No reason. It just seemed odd to me. I intended to use a regular bool, but accidentally used numpy. I was surprised by the result.

"
2576446788,issue,closed,completed,Refactor FastAPI tests for dag and dag_run into class based tests,"### Description

Intended to rewrite tests for dag and dag_run to be class-based.

Reference conversation: https://github.com/apache/airflow/pull/42725#discussion_r1793204496


- [x] test_dag.py - #42949
- [x] test_dag_run.py - #42949

### Use case/motivation

To maintain consistency.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rawwar,2024-10-09 16:47:52+00:00,['rawwar'],2024-10-16 03:48:25+00:00,2024-10-16 03:48:19+00:00,https://github.com/apache/airflow/issues/42866,"[('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2576014902,issue,closed,completed,"Re-organize repo into multiple projects using uv's workspaces ""core"" (scheduler + web/api servers), ""providers"" (a single project for all providers for now), ""task-sdk"" (new dist for this project)","List discussion https://lists.apache.org/thread/dyv5jhvt65xs6l5o2byc2b67f4wlwf6r

We will try out uv's workspace feature for this.

The projects we want are:

 - ""core"", containing the scheduler, web/api servers, dag parser etc.
 - ""providers"", a single project containing all the provivders
 - ""task-sdk"", the new dist that is key to this AIP.

There will be other top level folders such as the helm chart etc.

Tests should _probably_ be split to live alongside the projects.

Example code layout (picking a few representative files in each group):

```
core/pyproject.toml/
core/src/airflow/__init__.py
core/src/airflow/models/…
core/src/airflow/api_fastapi/…
core/src/airflow/ui/package.json # React UI etc.
core/tests/jobs/test_scheduler_job.py
providers/src/airflow/providers/celery/provider.yaml
providers/src/airflow/providers/celery/__init__.py
providers/tests/airflow/providers/celery/executors/test_celery_executor.py
providers/src/airflow/providers/cncf/kubernetes/provuder.yaml
providers/src/airflow/providers/cncf/kubernetes/__init__.py
task-sdk/pyproject.toml
task-sdk/src/airflow/sdk/__init__.py
task-sdk/src/airflow/sdk/defintions/dag.py
task-sdk/src/airflow/sdk/defintions/task_group.py
task-sdk/tests/airflow/sdk/defintions/test_task_group.py
```",kaxil,2024-10-09 13:53:36+00:00,['ashb'],2024-10-14 14:51:19+00:00,2024-10-09 19:24:54+00:00,https://github.com/apache/airflow/issues/42857,"[('area:providers', ''), ('area:core', ''), ('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2576014491,issue,closed,completed,Create `airflow-sdk` dist containing DAG definition level code,This becomes a top-level folder (namespace package) within Airflow package -- including pyproject.toml for building a package and tests,kaxil,2024-10-09 13:53:27+00:00,['kaxil'],2024-10-14 14:51:31+00:00,2024-10-12 14:40:11+00:00,https://github.com/apache/airflow/issues/42856,"[('area:task-execution-interface-aip72', 'AIP-72: Task Execution Interface (TEI) aka Task SDK')]",[],
2575831925,issue,open,,"Decorator for Task Flow (@skip_if, @run_if) to make it simple to apply whether or not to skip a Task -> Implementation also for Task Groups","### Description

It would be really helpful, if the new feature @skip_if and @run_if would be available also for whole TaskGroups. With that I would be able to easily toggle the computation of whole TaskGroups with a simple check.

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",weidinger-c,2024-10-09 12:47:06+00:00,[],2024-10-23 15:55:12+00:00,,https://github.com/apache/airflow/issues/42853,"[('kind:feature', 'Feature Requests'), ('area:TaskGroup', ''), ('airflow3.x candidate', 'Candidates for Airlfow 3.x (beyond Airflow 3.0)')]","[{'comment_id': 2402227213, 'issue_id': 2575831925, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 9, 12, 47, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408721653, 'issue_id': 2575831925, 'author': 'potiuk', 'body': 'This is currently not possible in Airflow 2 - because task group is mostly a grouping construct, not logical context that can be used as dependencies - the dependencies are between tasks. If you have the whole group with it\'s ""entry""point"" task  - you can decorate this task with skip_if, or run-if and this is the way how you can do what you want I guess.\r\n\r\nBut maybe that\'s something that could be implemented after we release Airlfow 3.\r\n\r\nI created a new label for such things `airflow3.x candidate` and marked it with it.', 'created_at': datetime.datetime(2024, 10, 12, 22, 45, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2422319084, 'issue_id': 2575831925, 'author': 'weidinger-c', 'body': 'Thanks for the reply, will test your suggestion by setting the first task of the task group with the skip_if flag.', 'created_at': datetime.datetime(2024, 10, 18, 12, 6, 33, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-09 12:47:09 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-10-12 22:45:28 UTC): This is currently not possible in Airflow 2 - because task group is mostly a grouping construct, not logical context that can be used as dependencies - the dependencies are between tasks. If you have the whole group with it's ""entry""point"" task  - you can decorate this task with skip_if, or run-if and this is the way how you can do what you want I guess.

But maybe that's something that could be implemented after we release Airlfow 3.

I created a new label for such things `airflow3.x candidate` and marked it with it.

weidinger-c (Issue Creator) on (2024-10-18 12:06:33 UTC): Thanks for the reply, will test your suggestion by setting the first task of the task group with the skip_if flag.

"
2575398616,issue,open,,Publish JSON schema for airflow.cfg,"### Description

There is already a good structured YAML file providing metadata about all valid configuration options in `airflow.cfg`: [airflow/config_templates/config.yml](https://github.com/apache/airflow/blob/9c4b81d71f3caee3a3b1c6d9c626ee5d16f0db7c/airflow/config_templates/config.yml).

I think publishing the same data as a JSON schema and eventually to https://www.schemastore.org/json/ could be very useful.

### Use case/motivation

- People could use extensions like [Even Better TOML](https://marketplace.visualstudio.com/items?itemName=tamasfe.even-better-toml) with their IDE to benefit from validation and powerful auto-completion while editing `airflow.cfg`
- It would be easy to leverage pre-commit hooks or other CI tools to catch mistakes in the config file.

Airflow won't complain if the configuration file contains a typo or a non-existent configuration key making it easy to make mistakes. It could also make it easier to catch invalid values earlier.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",ghjklw,2024-10-09 09:47:54+00:00,[],2024-12-21 07:24:36+00:00,,https://github.com/apache/airflow/issues/42850,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('kind:feature', 'Feature Requests'), ('good first issue', '')]","[{'comment_id': 2401853350, 'issue_id': 2575398616, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 9, 9, 47, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408718580, 'issue_id': 2575398616, 'author': 'potiuk', 'body': 'The (small) problem is that airflow.cfg file is not json. It\'s \'ini"" format. I am not sure if you can validate such format easily. Do you know any tools that can do it and tested it with Airlfow .cfg file @ghjklw ? \r\n\r\nAlso be aware that we are planning (as part of https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-67+Multi-team+deployment+of+Airflow+components to migrate the format from "".ini"" format to "".toml"" format which is de-facto standard for configuration for many python projects now.  Will that work with it? Any tools that can do it?\r\n\r\n Maybe it should be made as part of that move and maybe you would like to contribute to that effort and actually take part in the .toml conversion and adding validation for the toml file @ghjklw ?', 'created_at': datetime.datetime(2024, 10, 12, 22, 33, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408719259, 'issue_id': 2575398616, 'author': 'potiuk', 'body': 'BTW. I know you mentioned ""even better toml"", but I am asking about CLI tools - somethign that can be used in our pre-commits ad validate the schema in CI. The big problem with such tooling that is IDE-only - is that we are not able to verify if such schema is actually ""correct"" and validating config files generated automatically during testing would be a good test.', 'created_at': datetime.datetime(2024, 10, 12, 22, 35, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2413124861, 'issue_id': 2575398616, 'author': 'ghjklw', 'body': ""Hi @potiuk \r\n\r\nMy mistake for assuming `airflow.cfg` was toml and not ini 🙈\r\n\r\nRegarding the tooling for JSON schema with TOML, a fairly easy alternative relying only on largely used robust projects/stdlib would be to read the toml file as a `dict` using [`tomllib.load`](https://docs.python.org/3/library/tomllib.html) and then validating the `dict` using [`jsonschema.validate`](https://python-jsonschema.readthedocs.io/en/stable/) which actually validates a mapping/dictionary/object and not a string.\r\n\r\nSee also: https://python-jsonschema.readthedocs.io/en/stable/faq/#can-jsonschema-be-used-to-validate-yaml-toml-etc\r\n\r\nAn even more powerful solution, but which might require more work depending on how the configuration is implemented today would be to leverage [pydantic-settings](https://docs.pydantic.dev/latest/concepts/pydantic_settings/). We would define the configuration as Pydantic models, creating the JSON schema would be straightforward. Pydantic could handle itself the parsing of the TOML file through the [`TomlConfigSettingsSource`](https://docs.pydantic.dev/latest/concepts/pydantic_settings/#other-settings-source). An added benefit of that approach is that it would create an abstraction layer between the definition of the settings structure and the format they're stored in/how they're parsed. It would then be quite easy to use YAML/JSON... `pydantic-settings` can also take care of variables defined through environment variables.\r\n\r\nLast but not least, [`check-jsonschema`](https://github.com/python-jsonschema/check-jsonschema) has [support for TOML](https://check-jsonschema.readthedocs.io/en/stable/optional_parsers.html#toml). It can be used both as a [CLI tool](https://check-jsonschema.readthedocs.io/en/stable/usage.html) and as a [pre-commit hook](https://check-jsonschema.readthedocs.io/en/stable/precommit_usage.html).\r\n\r\nUnfortunately, I really do not have the bandwidth nor the experience with Airflow's development to offer my help with the implementation, but if anyone wants to work on it, I'd be happy to be a sparring partner/help with testing."", 'created_at': datetime.datetime(2024, 10, 15, 7, 39, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415185129, 'issue_id': 2575398616, 'author': 'potiuk', 'body': 'Marked it as ""good first issue"" - hopefully will pick it up', 'created_at': datetime.datetime(2024, 10, 15, 21, 17, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2472123654, 'issue_id': 2575398616, 'author': 'dannyl1u', 'body': 'I can try implementing this, feel free to assign this to me if no one else has started on this, thanks', 'created_at': datetime.datetime(2024, 11, 13, 1, 34, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2472161571, 'issue_id': 2575398616, 'author': 'potiuk', 'body': 'Assigned :)', 'created_at': datetime.datetime(2024, 11, 13, 1, 54, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2475390390, 'issue_id': 2575398616, 'author': 'dannyl1u', 'body': 'Hey @ghjklw, thanks for the detailed feature request, agreed that having validation for `airflow.cfg` would be very useful.\r\n\r\n> Also be aware that we are planning (as part of https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-67+Multi-team+deployment+of+Airflow+components to migrate the format from "".ini"" format to "".toml"" format which is de-facto standard for configuration for many python projects now. Will that work with it? Any tools that can do it?\r\n\r\nQuestion for @potiuk : should I focus my efforts on validating the existing `airflow.cfg` in its current "".ini"" format, or creating a validation for the newly planned "".toml"" migration? \r\n\r\nCreating the JSON schema from the [airflow/config_templates/config.yml](https://github.com/apache/airflow/blob/9c4b81d71f3caee3a3b1c6d9c626ee5d16f0db7c/airflow/config_templates/config.yml) and publishing it to https://www.schemastore.org/json/ should be pretty straightforward as well and I can start on that if everyone agrees.', 'created_at': datetime.datetime(2024, 11, 14, 4, 28, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2476158951, 'issue_id': 2575398616, 'author': 'potiuk', 'body': '> Question for @potiuk : should I focus my efforts on validating the existing airflow.cfg in its current "".ini"" format, or creating a validation for the newly planned "".toml"" migration?\r\n\r\nYes. I think toml might not happen', 'created_at': datetime.datetime(2024, 11, 14, 11, 54, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556299589, 'issue_id': 2575398616, 'author': 'dannyl1u', 'body': 'Hi @potiuk, apologies for the earlier unassignment. I am still interested in this topic and have a question about the JSON schema:\r\n\r\nThe file config.yml.schema.json already exists and appears to use the JSON Schema Draft 07 specification (published at http://json-schema.org/draft-07/schema#). Is this schema also related to the collection on https://www.schemastore.org/json/?\r\n\r\nWould appreciate any clarification. Thank you!', 'created_at': datetime.datetime(2024, 12, 20, 5, 10, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556345638, 'issue_id': 2575398616, 'author': 'ghjklw', 'body': ""Hi @dannyl1u \r\n\r\nThank you very much for looking into it! The file config.yml.schema.json is a JSON Schema describing the structure of config.yml itself, so not really what we're after 😉\r\n\r\nAs for schemastore.org, the way it works is that when a JSON Schema has been defined, we can ask them to publish it: https://github.com/SchemaStore/schemastore/blob/master/CONTRIBUTING.md#how-to-add-a-json-schema-thats-self-hostedremoteexternal\r\nThe point of doing this is that many tools (including most IDEs) will then automatically match it to `airflow.cfg` so that you get validation and auto-completiom without having to do any manual configuration."", 'created_at': datetime.datetime(2024, 12, 20, 6, 0, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556463519, 'issue_id': 2575398616, 'author': 'dannyl1u', 'body': '@ghjklw \r\nIf my understanding is correct:\r\n1. Create JSON Schema file using [airflow/config_templates/config.yml](https://github.com/apache/airflow/blob/9c4b81d71f3caee3a3b1c6d9c626ee5d16f0db7c/airflow/config_templates/config.yml) and publish to https://www.schemastore.org/json/ \r\n2. Use `pydantic` or some other IDE tool to validate `airflow.cfg` using the json schema\r\n\r\nRegarding (2), do you know of any tools that can be used to validate the `.ini` format from the json schema? I generated a `schema.json` file locally using the [airflow/config_templates/config.yml](https://github.com/apache/airflow/blob/9c4b81d71f3caee3a3b1c6d9c626ee5d16f0db7c/airflow/config_templates/config.yml)  and would like to test if my `airflow.cfg` passes the validation.', 'created_at': datetime.datetime(2024, 12, 20, 7, 50, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2556748387, 'issue_id': 2575398616, 'author': 'potiuk', 'body': '> Create JSON Schema file using [airflow/config_templates/config.yml](https://github.com/apache/airflow/blob/9c4b81d71f3caee3a3b1c6d9c626ee5d16f0db7c/airflow/config_templates/config.yml) and publish to https://www.schemastore.org/json/\r\n\r\nWe could yes - no need to publish it there, it could be likely pointed at directly from Airflow Repository, but eventually submitting it there might be a good idea, however this should be done with Airflow PMC as a driving/controlling entity.\r\n\r\n> Use pydantic or some other IDE tool to validate airflow.cfg using the json schema\r\n>\r\n> Regarding (2), do you know of any tools that can be used to validate the `.ini` format from the json schema? I generated a `schema.json` file locally using the [airflow/config_templates/config.yml](https://github.com/apache/airflow/blob/9c4b81d71f3caee3a3b1c6d9c626ee5d16f0db7c/airflow/config_templates/config.yml)  and would like to test if my `airflow.cfg` passes the validation.\r\n\r\nSince TOML is a superset of .ini - this likely coud work (initially proposed in that issue) https://marketplace.visualstudio.com/items?itemName=tamasfe.even-better-toml - or any other toml validation solutions.', 'created_at': datetime.datetime(2024, 12, 20, 10, 47, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2557874332, 'issue_id': 2575398616, 'author': 'dannyl1u', 'body': '> Since TOML is a superset of .ini - this likely coud work (initially proposed in that issue) https://marketplace.visualstudio.com/items?itemName=tamasfe.even-better-toml - or any other toml validation solutions.\r\n\r\ntoml expects strings to be wrapped in quotes (`""`) and some other differences between .ini and .toml (e.g. `True` in .ini -> `true` in .toml). So I\'m not sure if simply using a TOML validator will work.\r\n\r\nAny suggestions on bridging this gap?', 'created_at': datetime.datetime(2024, 12, 20, 23, 21, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558032299, 'issue_id': 2575398616, 'author': 'ghjklw', 'body': ""If you already have a JSON Schema, feel free to share it, I'd love to play with it. Something that would be relatively easy to achieve is building a pre-commit hook by just writing a simple python script that parses the `airflow.cfg` file with `configparser` and then validates it with `jsonschema`. That's something I'd be happy to contribute if you want."", 'created_at': datetime.datetime(2024, 12, 21, 7, 18, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2558033829, 'issue_id': 2575398616, 'author': 'dannyl1u', 'body': ""@ghjklw \r\nHere's a JSON Schema I generated using the config.yml\r\nhttps://github.com/apache/airflow/blob/fdc14432475e3a34b574caf4a98d3e4102083909/airflow/config_templates/schema.json\r\n\r\nLet me know if any issues 👍"", 'created_at': datetime.datetime(2024, 12, 21, 7, 24, 35, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-09 09:47:56 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-10-12 22:33:06 UTC): The (small) problem is that airflow.cfg file is not json. It's 'ini"" format. I am not sure if you can validate such format easily. Do you know any tools that can do it and tested it with Airlfow .cfg file @ghjklw ? 

Also be aware that we are planning (as part of https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-67+Multi-team+deployment+of+Airflow+components to migrate the format from "".ini"" format to "".toml"" format which is de-facto standard for configuration for many python projects now.  Will that work with it? Any tools that can do it?

 Maybe it should be made as part of that move and maybe you would like to contribute to that effort and actually take part in the .toml conversion and adding validation for the toml file @ghjklw ?

potiuk on (2024-10-12 22:35:33 UTC): BTW. I know you mentioned ""even better toml"", but I am asking about CLI tools - somethign that can be used in our pre-commits ad validate the schema in CI. The big problem with such tooling that is IDE-only - is that we are not able to verify if such schema is actually ""correct"" and validating config files generated automatically during testing would be a good test.

ghjklw (Issue Creator) on (2024-10-15 07:39:37 UTC): Hi @potiuk 

My mistake for assuming `airflow.cfg` was toml and not ini 🙈

Regarding the tooling for JSON schema with TOML, a fairly easy alternative relying only on largely used robust projects/stdlib would be to read the toml file as a `dict` using [`tomllib.load`](https://docs.python.org/3/library/tomllib.html) and then validating the `dict` using [`jsonschema.validate`](https://python-jsonschema.readthedocs.io/en/stable/) which actually validates a mapping/dictionary/object and not a string.

See also: https://python-jsonschema.readthedocs.io/en/stable/faq/#can-jsonschema-be-used-to-validate-yaml-toml-etc

An even more powerful solution, but which might require more work depending on how the configuration is implemented today would be to leverage [pydantic-settings](https://docs.pydantic.dev/latest/concepts/pydantic_settings/). We would define the configuration as Pydantic models, creating the JSON schema would be straightforward. Pydantic could handle itself the parsing of the TOML file through the [`TomlConfigSettingsSource`](https://docs.pydantic.dev/latest/concepts/pydantic_settings/#other-settings-source). An added benefit of that approach is that it would create an abstraction layer between the definition of the settings structure and the format they're stored in/how they're parsed. It would then be quite easy to use YAML/JSON... `pydantic-settings` can also take care of variables defined through environment variables.

Last but not least, [`check-jsonschema`](https://github.com/python-jsonschema/check-jsonschema) has [support for TOML](https://check-jsonschema.readthedocs.io/en/stable/optional_parsers.html#toml). It can be used both as a [CLI tool](https://check-jsonschema.readthedocs.io/en/stable/usage.html) and as a [pre-commit hook](https://check-jsonschema.readthedocs.io/en/stable/precommit_usage.html).

Unfortunately, I really do not have the bandwidth nor the experience with Airflow's development to offer my help with the implementation, but if anyone wants to work on it, I'd be happy to be a sparring partner/help with testing.

potiuk on (2024-10-15 21:17:44 UTC): Marked it as ""good first issue"" - hopefully will pick it up

dannyl1u on (2024-11-13 01:34:30 UTC): I can try implementing this, feel free to assign this to me if no one else has started on this, thanks

potiuk on (2024-11-13 01:54:59 UTC): Assigned :)

dannyl1u on (2024-11-14 04:28:51 UTC): Hey @ghjklw, thanks for the detailed feature request, agreed that having validation for `airflow.cfg` would be very useful.


Question for @potiuk : should I focus my efforts on validating the existing `airflow.cfg` in its current "".ini"" format, or creating a validation for the newly planned "".toml"" migration? 

Creating the JSON schema from the [airflow/config_templates/config.yml](https://github.com/apache/airflow/blob/9c4b81d71f3caee3a3b1c6d9c626ee5d16f0db7c/airflow/config_templates/config.yml) and publishing it to https://www.schemastore.org/json/ should be pretty straightforward as well and I can start on that if everyone agrees.

potiuk on (2024-11-14 11:54:32 UTC): Yes. I think toml might not happen

dannyl1u on (2024-12-20 05:10:08 UTC): Hi @potiuk, apologies for the earlier unassignment. I am still interested in this topic and have a question about the JSON schema:

The file config.yml.schema.json already exists and appears to use the JSON Schema Draft 07 specification (published at http://json-schema.org/draft-07/schema#). Is this schema also related to the collection on https://www.schemastore.org/json/?

Would appreciate any clarification. Thank you!

ghjklw (Issue Creator) on (2024-12-20 06:00:30 UTC): Hi @dannyl1u 

Thank you very much for looking into it! The file config.yml.schema.json is a JSON Schema describing the structure of config.yml itself, so not really what we're after 😉

As for schemastore.org, the way it works is that when a JSON Schema has been defined, we can ask them to publish it: https://github.com/SchemaStore/schemastore/blob/master/CONTRIBUTING.md#how-to-add-a-json-schema-thats-self-hostedremoteexternal
The point of doing this is that many tools (including most IDEs) will then automatically match it to `airflow.cfg` so that you get validation and auto-completiom without having to do any manual configuration.

dannyl1u on (2024-12-20 07:50:59 UTC): @ghjklw 
If my understanding is correct:
1. Create JSON Schema file using [airflow/config_templates/config.yml](https://github.com/apache/airflow/blob/9c4b81d71f3caee3a3b1c6d9c626ee5d16f0db7c/airflow/config_templates/config.yml) and publish to https://www.schemastore.org/json/ 
2. Use `pydantic` or some other IDE tool to validate `airflow.cfg` using the json schema

Regarding (2), do you know of any tools that can be used to validate the `.ini` format from the json schema? I generated a `schema.json` file locally using the [airflow/config_templates/config.yml](https://github.com/apache/airflow/blob/9c4b81d71f3caee3a3b1c6d9c626ee5d16f0db7c/airflow/config_templates/config.yml)  and would like to test if my `airflow.cfg` passes the validation.

potiuk on (2024-12-20 10:47:03 UTC): We could yes - no need to publish it there, it could be likely pointed at directly from Airflow Repository, but eventually submitting it there might be a good idea, however this should be done with Airflow PMC as a driving/controlling entity.


Since TOML is a superset of .ini - this likely coud work (initially proposed in that issue) https://marketplace.visualstudio.com/items?itemName=tamasfe.even-better-toml - or any other toml validation solutions.

dannyl1u on (2024-12-20 23:21:41 UTC): toml expects strings to be wrapped in quotes (`""`) and some other differences between .ini and .toml (e.g. `True` in .ini -> `true` in .toml). So I'm not sure if simply using a TOML validator will work.

Any suggestions on bridging this gap?

ghjklw (Issue Creator) on (2024-12-21 07:18:11 UTC): If you already have a JSON Schema, feel free to share it, I'd love to play with it. Something that would be relatively easy to achieve is building a pre-commit hook by just writing a simple python script that parses the `airflow.cfg` file with `configparser` and then validates it with `jsonschema`. That's something I'd be happy to contribute if you want.

dannyl1u on (2024-12-21 07:24:35 UTC): @ghjklw 
Here's a JSON Schema I generated using the config.yml
https://github.com/apache/airflow/blob/fdc14432475e3a34b574caf4a98d3e4102083909/airflow/config_templates/schema.json

Let me know if any issues 👍

"
2574653062,issue,open,,Dataset DEL/POST API should also check the access_control at DAG level if defined,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Let's say we have a DAG called DAG_A with several tasks and one of the task will trigger a dataset update. 

- In the same DAG, we have added an additional access_control to achieve DAG level access control and defined only users belong to Role_A can create DAG run on this DAG.
- the downstream DAGs also have DAG level access control defined, let's call it Role_B and Role_C

Right now, one user with a role which ""can create on Datasets"" will have the permission to trigger an event for this dataset, even this user doesn't have any role with dag run permissions to the DAG_A or DAG_A's downstream DAGs

### What you think should happen instead?

To support DAG level access control, in order to trigger a dataset update event, besides the ""can create on Datasets"" permission, the user should also:
- As the upstream or datasets generator, user should have permission to create dag_run on the DAGs which actually generate the dataset event if no human intervention.  Because upstream DAG owners can always re-run their DAG to create a new event
- As the downstream of the dataset, user will need to have permission to create dag_run on ALL the downtreams DAGs.

So in this case, in order to call the API to create a dataset event, beside a role with permission to ""can create on Datasets"", this user need to be in Role_A( if he/she is the upstream owner), or both Role_B and Role_C (If he/she is the downstream owner )

### How to reproduce

create 3 users with 3 roles:

- User C with role_C: ""can create on Datasets"" 
- User A with role_A: can create dag runs on ""DAG_A""
- User B with role_B: can create dag runs on ""DAG_B""

Create 2 DAGs with dag level access control defined in DAG:

- DAG_A: only Role_A can create dag runs, and define a dataset outlet
- DAG_B: only Role_B can create dag runs, and schedule based on dataset defined in DAG_A

Then use user C to call the Airflow API

### Operating System

Debian 12

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",nicolasge,2024-10-09 03:43:28+00:00,[],2024-10-09 03:45:45+00:00,,https://github.com/apache/airflow/issues/42846,"[('kind:bug', 'This is a clearly a bug'), ('area:API', ""Airflow's REST/HTTP API""), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('area:datasets', 'Issues related to the datasets feature')]",[],
2573632159,issue,closed,completed,Make DAG.max_active_runs evaluated at dag run scope,"### Body

Currently it is at DAG scope

But it should be per-dag-run

This proposal was accepted by lazy consensus: https://lists.apache.org/thread/9o84d3yn934m32gtlpokpwtbbmtxj47l

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2024-10-08 16:05:19+00:00,[],2024-10-18 14:04:06+00:00,2024-10-18 14:04:06+00:00,https://github.com/apache/airflow/issues/42833,"[('kind:meta', 'High-level information important to the community'), ('area:core', '')]",[],
2573567979,issue,closed,completed,Exception when executing SchedulerJob._run_scheduler_loop using KubernetesExecutor [NoneType],"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Scheduler Exception:

```
/home/airflow/.local/lib/python3.11/site-packages/airflow/configuration.py:765 FutureWarning: The auth_backends setting in [api] has had airflow.api.auth.backend.session added in the running config, which is needed by the UI. Please update your config before Apache Airflow 3.0.
/home/airflow/.local/lib/python3.11/site-packages/airflow/metrics/validators.py:95 DeprecationWarning: The statsd_allow_list option in [metrics] has been renamed to metrics_allow_list - the old setting has been used, but please update your config.
/home/airflow/.local/lib/python3.11/site-packages/airflow/metrics/statsd_logger.py:184 RemovedInAirflow3Warning: The basic metric validator will be deprecated in the future in favor of pattern-matching.  You can try this now by setting config option metrics_use_pattern_match to True.
DataHurb2LineageBackend is not available. Please install the required dependencies.
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2024-10-08T15:26:01.247+0000] {_client.py:1026} INFO - HTTP Request: GET https://apacheairflow.gateway.scarf.sh/scheduler?version=2.10.2&python_version=3.11&platform=Linux&arch=x86_64&database=postgresql&db_version=12.20&executor=KubernetesExecutor ""HTTP/1.1 200 OK""
/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py:143 FutureWarning: The config section [kubernetes] has been renamed to [kubernetes_executor]. Please update your `conf.get*` call to use the new name
[2024-10-08T15:26:01.361+0000] {executor_loader.py:254} INFO - Loaded executor: KubernetesExecutor
[2024-10-08T15:26:01.437+0000] {scheduler_job_runner.py:935} INFO - Starting the scheduler
[2024-10-08T15:26:01.437+0000] {scheduler_job_runner.py:942} INFO - Processing each file at most -1 times
[2024-10-08T15:26:01.438+0000] {kubernetes_executor.py:287} INFO - Start Kubernetes executor
[2024-10-08T15:26:01.471+0000] {kubernetes_executor_utils.py:140} INFO - Event: and now my watch begins starting at resource_version: 0
[2024-10-08T15:26:01.491+0000] {kubernetes_executor.py:208} INFO - Found 0 queued task instances
[2024-10-08T15:26:01.493+0000] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-08T15:26:01.653+0000] {scheduler_job_runner.py:1001} ERROR - Exception when executing SchedulerJob._run_scheduler_loop
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py"", line 984, in _execute
    self._run_scheduler_loop()
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1067, in _run_scheduler_loop
    self.adopt_or_reset_orphaned_tasks()
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1850, in adopt_or_reset_orphaned_tasks
    for attempt in run_with_db_retries(logger=self.log):
  File ""/home/airflow/.local/lib/python3.11/site-packages/tenacity/__init__.py"", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/tenacity/__init__.py"", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/tenacity/__init__.py"", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/concurrent/futures/_base.py"", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/concurrent/futures/_base.py"", line 401, in __get_result
    raise self._exception
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1894, in adopt_or_reset_orphaned_tasks
    to_reset.extend(executor.try_adopt_task_instances(tis))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py"", line 571, in try_adopt_task_instances
    pod_list = self._list_pods(query_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py"", line 162, in _list_pods
    pods.extend(dynamic_client.get(resource=pod_resource, namespace=namespace, **query_kwargs).items)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/kubernetes/dynamic/client.py"", line 112, in get
    return self.request('get', path, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/kubernetes/dynamic/client.py"", line 62, in inner
    return serializer(self, json.loads(resp.data.decode('utf8')))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/kubernetes/dynamic/resource.py"", line 294, in __init__
    for item in instance['items']:
TypeError: 'NoneType' object is not iterable
[2024-10-08T15:26:01.656+0000] {kubernetes_executor.py:752} INFO - Shutting down Kubernetes executor
[2024-10-08T15:26:01.700+0000] {scheduler_job_runner.py:1014} INFO - Exited execute loop
Traceback (most recent call last):
  File ""/home/airflow/.local/bin/airflow"", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/__main__.py"", line 62, in main
    args.func(args)
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/cli_config.py"", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/cli.py"", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/providers_configuration_loader.py"", line 55, in wrapped_function
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/commands/scheduler_command.py"", line 59, in scheduler
    run_command_with_daemon_option(
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/commands/daemon_utils.py"", line 86, in run_command_with_daemon_option
    callback()
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/commands/scheduler_command.py"", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
                     ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/commands/scheduler_command.py"", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/job.py"", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/job.py"", line 450, in execute_job
    ret = execute_callable()
          ^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py"", line 984, in _execute
    self._run_scheduler_loop()
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1067, in _run_scheduler_loop
    self.adopt_or_reset_orphaned_tasks()
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1850, in adopt_or_reset_orphaned_tasks
    for attempt in run_with_db_retries(logger=self.log):
  File ""/home/airflow/.local/lib/python3.11/site-packages/tenacity/__init__.py"", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/tenacity/__init__.py"", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/tenacity/__init__.py"", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/concurrent/futures/_base.py"", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/concurrent/futures/_base.py"", line 401, in __get_result
    raise self._exception
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1894, in adopt_or_reset_orphaned_tasks
    to_reset.extend(executor.try_adopt_task_instances(tis))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py"", line 571, in try_adopt_task_instances
    pod_list = self._list_pods(query_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py"", line 162, in _list_pods
    pods.extend(dynamic_client.get(resource=pod_resource, namespace=namespace, **query_kwargs).items)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/kubernetes/dynamic/client.py"", line 112, in get
    return self.request('get', path, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/kubernetes/dynamic/client.py"", line 62, in inner
    return serializer(self, json.loads(resp.data.decode('utf8')))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/kubernetes/dynamic/resource.py"", line 294, in __init__
    for item in instance['items']:
TypeError: 'NoneType' object is not iterable
```

### What you think should happen instead?

Scheduler running

### How to reproduce

I don't know how I reproduced it, I just started using version 2.10.2 and this error has been occurring

### Operating System

Kubernetes GKE

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.28.0
apache-airflow-providers-celery==3.8.1
apache-airflow-providers-cncf-kubernetes==8.4.1
apache-airflow-providers-common-compat==1.2.0
apache-airflow-providers-common-io==1.4.0
apache-airflow-providers-common-sql==1.16.0
apache-airflow-providers-docker==3.13.0
apache-airflow-providers-elasticsearch==5.5.0
apache-airflow-providers-fab==1.3.0
apache-airflow-providers-ftp==3.11.0
apache-airflow-providers-google==10.19.0
apache-airflow-providers-grpc==3.6.0
apache-airflow-providers-hashicorp==3.8.0
apache-airflow-providers-http==4.13.0
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-microsoft-azure==10.4.0
apache-airflow-providers-mysql==5.7.0
apache-airflow-providers-odbc==4.7.0
apache-airflow-providers-openlineage==1.11.0
apache-airflow-providers-postgres==5.12.0
apache-airflow-providers-redis==3.8.0
apache-airflow-providers-sendgrid==3.6.0
apache-airflow-providers-sftp==4.11.0
apache-airflow-providers-slack==8.9.0
apache-airflow-providers-smtp==1.8.0
apache-airflow-providers-snowflake==5.7.0
apache-airflow-providers-sqlite==3.9.0
apache-airflow-providers-ssh==3.13.1


### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Helm Chart + ArgoCD

### Anything else?


This issue is crashing our production environment

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",alexsanderp,2024-10-08 15:38:26+00:00,[],2024-10-15 20:14:17+00:00,2024-10-15 20:14:17+00:00,https://github.com/apache/airflow/issues/42831,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('area:providers', ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2400193875, 'issue_id': 2573567979, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 8, 15, 38, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2414918844, 'issue_id': 2573567979, 'author': 'alexsanderp', 'body': 'I’ve identified the cause of the issue with our Scheduler. It stemmed from a bug in the Python Kubernetes library, which attempted to iterate over a None variable. This error was fixed in version 29.0.0 of the library. Airflow was using this library and handled the exception in previous versions, but starting with version 2.10.2, it stopped handling this exception due to an update of the Kubernetes library in the provider.\r\n\r\nSince we were pinning the version to 27.2.0, the error began to surface again, as it wasn’t being handled or fixed. By updating our environments to use the latest version of the library, we resolved the issue.', 'created_at': datetime.datetime(2024, 10, 15, 20, 14, 15, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-08 15:38:29 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

alexsanderp (Issue Creator) on (2024-10-15 20:14:15 UTC): I’ve identified the cause of the issue with our Scheduler. It stemmed from a bug in the Python Kubernetes library, which attempted to iterate over a None variable. This error was fixed in version 29.0.0 of the library. Airflow was using this library and handled the exception in previous versions, but starting with version 2.10.2, it stopped handling this exception due to an update of the Kubernetes library in the provider.

Since we were pinning the version to 27.2.0, the error began to surface again, as it wasn’t being handled or fixed. By updating our environments to use the latest version of the library, we resolved the issue.

"
2572981671,issue,closed,completed,AIP-38 | New Dashboard homepage,"Create a new homepage dashboard to give an overview of what is happening on the airflow deployment. Essentially, migrate and expand upon Cluster Activity

- Add a new Dashboard page
- Show cluster health stats
- #42700
- https://github.com/apache/airflow/issues/43328
- Show DAG import errors
- Quick links to DAGs with failed runs or running runs
- Add an event timeline to help with date range selection",bbovenzi,2024-10-08 11:53:27+00:00,[],2024-11-05 18:09:57+00:00,2024-11-05 18:09:55+00:00,https://github.com/apache/airflow/issues/42825,"[('kind:meta', 'High-level information important to the community'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2401615070, 'issue_id': 2572981671, 'author': 'raphaelauv', 'body': 'in `Show cluster health stats`\r\n\r\nit would be very nice to also show the number of components ( scheduler(s) , celery-worker(s) , triggerer(s) , dag-processor(s) )', 'created_at': datetime.datetime(2024, 10, 9, 7, 59, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2457854435, 'issue_id': 2572981671, 'author': 'bbovenzi', 'body': 'Closing in favor of larger tracking issue: https://github.com/apache/airflow/issues/43712', 'created_at': datetime.datetime(2024, 11, 5, 18, 9, 55, tzinfo=datetime.timezone.utc)}]","raphaelauv on (2024-10-09 07:59:15 UTC): in `Show cluster health stats`

it would be very nice to also show the number of components ( scheduler(s) , celery-worker(s) , triggerer(s) , dag-processor(s) )

bbovenzi (Issue Creator) on (2024-11-05 18:09:55 UTC): Closing in favor of larger tracking issue: https://github.com/apache/airflow/issues/43712

"
2572927309,issue,closed,completed,`skip_if` decorator evaluated after pool slot allocation,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

The `skip_if` decorator is currently evaluated after a task has already been allocated a pool slot. This leads to inefficient use of pool resources, as tasks that end up being skipped still occupy pool slots unnecessarily.

### What you think should happen instead?

Ideally, the `skip_if` decorator should be evaluated before the task attempts to acquire a pool slot.

### How to reproduce

1. Create a Pool with the name `limited_pool` and 0 slots.
2. Create the following example DAG:

```python
from airflow.decorators import dag, task
from datetime import datetime

@dag(
    dag_id='skip_if_pool_issue',
    schedule_interval=None,
    start_date=datetime(2023, 1, 1),
    catchup=False,
    is_paused_upon_creation=False,
)
def skip_if_pool_issue():

    @task.skip_if(lambda context: True)
    @task(pool='limited_pool')
    def task_with_pool():
        print(""This task is in a pool and should always be skipped"")

    task_with_pool()

skip_if_pool_issue()
```

3. Trigger a DAG Run for this DAG.

### Operating System

Ubuntu 22.04.4 LTS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

Used `LocalExecutor`.

### Anything else?

Hopefully this is possible.
If this is considered a feature request not a bug, please change the tags.
Related PR: 
- https://github.com/apache/airflow/pull/41116


### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",pedro-cf,2024-10-08 11:31:38+00:00,[],2024-10-13 13:30:35+00:00,2024-10-13 13:30:35+00:00,https://github.com/apache/airflow/issues/42824,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('airflow3.x candidate', 'Candidates for Airlfow 3.x (beyond Airflow 3.0)')]","[{'comment_id': 2399952433, 'issue_id': 2572927309, 'author': 'pedro-cf', 'body': 'Hey @phi-friday , maybe you could share some thoughts abouts this, thank you in advance for the great work!', 'created_at': datetime.datetime(2024, 10, 8, 14, 4, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2400087005, 'issue_id': 2572927309, 'author': 'phi-friday', 'body': ""> Hey @phi-friday , maybe you could share some thoughts abouts this, thank you in advance for the great work!\r\n\r\nThis feature is entirely dependent on `pre_execute`, so if `pre_execute` can be executed before the `slot` allocation, then the scenario you mentioned should be possible. \r\n\r\nHowever, I believe (though I don't know exactly) that having a `Task` assigned to an `executor` and getting a `slot` are isomorphic, so wouldn't this scenario be impossible."", 'created_at': datetime.datetime(2024, 10, 8, 14, 56, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2402712424, 'issue_id': 2572927309, 'author': 'eladkal', 'body': ""I don't think this is a bug(?) The way I see it this feature is a shorter way to write conditions. Normally you would use a branch operator or ShortCircuitOperator and both occupy a pool slot."", 'created_at': datetime.datetime(2024, 10, 9, 15, 52, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408733825, 'issue_id': 2572927309, 'author': 'potiuk', 'body': 'Currently custom code provided by DAG author cannot be executed in scheduler for securiy reasons - so what you want is not possible because it violates airflow security model  - specifically capabilities of DAG authors https://airflow.apache.org/docs/apache-airflow/stable/security/security_model.html#dag-authors \r\n\r\nIn Airflow 3.+ we will likely have a possibility to run different types of workflows, so we could potentially execute such condition somewhere else than in pre-execute of a task - and not occupy a pool slot for such a check.\r\n\r\nIf you would like to work on it @pedro-cf - that would likely require discussion on the devlist and getting consensus on a scope of it - because this is quite a big change in architecture. And if you would like to come back to it after we know how task sdk and executing of non-task workflows will look like, because this is not likely anyone would even look at it before Airflow 3 is out.', 'created_at': datetime.datetime(2024, 10, 12, 23, 2, 26, tzinfo=datetime.timezone.utc)}]","pedro-cf (Issue Creator) on (2024-10-08 14:04:28 UTC): Hey @phi-friday , maybe you could share some thoughts abouts this, thank you in advance for the great work!

phi-friday on (2024-10-08 14:56:11 UTC): This feature is entirely dependent on `pre_execute`, so if `pre_execute` can be executed before the `slot` allocation, then the scenario you mentioned should be possible. 

However, I believe (though I don't know exactly) that having a `Task` assigned to an `executor` and getting a `slot` are isomorphic, so wouldn't this scenario be impossible.

eladkal on (2024-10-09 15:52:58 UTC): I don't think this is a bug(?) The way I see it this feature is a shorter way to write conditions. Normally you would use a branch operator or ShortCircuitOperator and both occupy a pool slot.

potiuk on (2024-10-12 23:02:26 UTC): Currently custom code provided by DAG author cannot be executed in scheduler for securiy reasons - so what you want is not possible because it violates airflow security model  - specifically capabilities of DAG authors https://airflow.apache.org/docs/apache-airflow/stable/security/security_model.html#dag-authors 

In Airflow 3.+ we will likely have a possibility to run different types of workflows, so we could potentially execute such condition somewhere else than in pre-execute of a task - and not occupy a pool slot for such a check.

If you would like to work on it @pedro-cf - that would likely require discussion on the devlist and getting consensus on a scope of it - because this is quite a big change in architecture. And if you would like to come back to it after we know how task sdk and executing of non-task workflows will look like, because this is not likely anyone would even look at it before Airflow 3 is out.

"
2572926189,issue,closed,completed,AIP-38 Move Light/Dark Mode Toggle to User Settings,"Swap the light/dakr mode toggle button with a User Settings button with an `import { FiUser } from ""react-icons/fi"";` icon. Clicking on it can open up a menu with toggling dark/light mode as an option.",bbovenzi,2024-10-08 11:31:04+00:00,['AryanK1511'],2024-10-15 15:17:04+00:00,2024-10-15 15:17:04+00:00,https://github.com/apache/airflow/issues/42823,"[('good first issue', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2400352827, 'issue_id': 2572926189, 'author': 'AryanK1511', 'body': 'Hello @bbovenzi ! R u able to assign this issue to me as I would really like to work on this', 'created_at': datetime.datetime(2024, 10, 8, 16, 40, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401641584, 'issue_id': 2572926189, 'author': 'bbovenzi', 'body': '@AryanK1511 all yours!', 'created_at': datetime.datetime(2024, 10, 9, 8, 11, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2405710386, 'issue_id': 2572926189, 'author': 'AryanK1511', 'body': '@bbovenzi Hello! I was working on this issue and have managed to implement the icon. However, I am not sure about the exact UI that you want for the menu. How do you want me to go forward with this?', 'created_at': datetime.datetime(2024, 10, 10, 17, 48, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2405754143, 'issue_id': 2572926189, 'author': 'AryanK1511', 'body': '@bbovenzi This is what I have right now. I know it is not pretty but I wanted to ask you what you want me to do with this or was the point of this issue just to add the icon and not to work on the menu?\r\n\r\n\r\nhttps://github.com/user-attachments/assets/70444692-7350-404a-b0d8-fc35fe69d93d\r\n\r\nThis is simply a menu item in the navbar.\r\n\r\n```js\r\n<Menu>\r\n  <MenuButton\r\n    as={NavButton}\r\n    icon={<FiUser size=""1.75rem"" />}\r\n  />\r\n  <MenuList>\r\n    <MenuItem onClick={toggleColorMode}>\r\n      {colorMode === ""light"" ? (\r\n        <>\r\n          <FiMoon size=""1.25rem"" style={{ marginRight: \'8px\' }} />\r\n          Switch to Dark Mode\r\n        </>\r\n      ) : (\r\n        <>\r\n          <FiSun size=""1.25rem"" style={{ marginRight: \'8px\' }} />\r\n          Switch to Light Mode\r\n        </>\r\n      )}\r\n    </MenuItem>\r\n  </MenuList>\r\n</Menu>\r\n```', 'created_at': datetime.datetime(2024, 10, 10, 18, 14, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2406840483, 'issue_id': 2572926189, 'author': 'bbovenzi', 'body': ""Oh I had a similar issue when adding the [docs button](https://github.com/apache/airflow/blob/main/airflow/ui/src/layouts/Nav/DocsButton.tsx)\r\n\r\nLet's pass just the same props as a NavButton instead of `as={}` and make sure to specify the menu placement"", 'created_at': datetime.datetime(2024, 10, 11, 8, 8, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408639108, 'issue_id': 2572926189, 'author': 'AryanK1511', 'body': 'Hello @bbovenzi! I have fixed this in #42964. Take a look and lemme know if it is good.', 'created_at': datetime.datetime(2024, 10, 12, 17, 40, 9, tzinfo=datetime.timezone.utc)}]","AryanK1511 (Assginee) on (2024-10-08 16:40:59 UTC): Hello @bbovenzi ! R u able to assign this issue to me as I would really like to work on this

bbovenzi (Issue Creator) on (2024-10-09 08:11:26 UTC): @AryanK1511 all yours!

AryanK1511 (Assginee) on (2024-10-10 17:48:35 UTC): @bbovenzi Hello! I was working on this issue and have managed to implement the icon. However, I am not sure about the exact UI that you want for the menu. How do you want me to go forward with this?

AryanK1511 (Assginee) on (2024-10-10 18:14:02 UTC): @bbovenzi This is what I have right now. I know it is not pretty but I wanted to ask you what you want me to do with this or was the point of this issue just to add the icon and not to work on the menu?


https://github.com/user-attachments/assets/70444692-7350-404a-b0d8-fc35fe69d93d

This is simply a menu item in the navbar.

```js
<Menu>
  <MenuButton
    as={NavButton}
    icon={<FiUser size=""1.75rem"" />}
  />
  <MenuList>
    <MenuItem onClick={toggleColorMode}>
      {colorMode === ""light"" ? (
        <>
          <FiMoon size=""1.25rem"" style={{ marginRight: '8px' }} />
          Switch to Dark Mode
        </>
      ) : (
        <>
          <FiSun size=""1.25rem"" style={{ marginRight: '8px' }} />
          Switch to Light Mode
        </>
      )}
    </MenuItem>
  </MenuList>
</Menu>
```

bbovenzi (Issue Creator) on (2024-10-11 08:08:50 UTC): Oh I had a similar issue when adding the [docs button](https://github.com/apache/airflow/blob/main/airflow/ui/src/layouts/Nav/DocsButton.tsx)

Let's pass just the same props as a NavButton instead of `as={}` and make sure to specify the menu placement

AryanK1511 (Assginee) on (2024-10-12 17:40:09 UTC): Hello @bbovenzi! I have fixed this in #42964. Take a look and lemme know if it is good.

"
2572589735,issue,open,,"Databricks Provider - Task within a Workflow to handle different ""run if dependencies"" configuration (currently only supports default ALL_SUCCEEDED)","### Description

Concerns [airflow.providers.databricks.operators.databricks](https://github.com/apache/airflow/blob/main/airflow/providers/databricks/operators/databricks.py)

When creating a task inside a Workflow in Databricks, you can choose ""**Run if dependencies**"", see screenshot below.

https://docs.databricks.com/en/jobs/run-if.html

![image](https://github.com/user-attachments/assets/3ec6451b-6386-4645-88ee-681268d15bf9)

The workflow json contains the information at the task level, for example:

```json
{
      ""task_key"": ""C"",
      ""depends_on"": [
        {
          ""task_key"": ""A""
        },
        {
          ""task_key"": ""B""
        }
      ],
      ""run_if"": ""ALL_SUCCEEDED"",
     ...
}
```

It is not supported using Databricks provider for now, in the api call to create the workflow it's ignored and thus default value is used: ""ALL_SUCCEEDED"".
Would be awesome to be able to feed that information at the task level so that we can handle more dependency types. 

I think the best would be to be able to leverage Airflow operator generic [trigger_rule](https://airflow.apache.org/docs/apache-airflow/1.10.9/concepts.html#trigger-rules) but i'm not too sure how to implement that or if that's doable.

I think the easiest would be add a parameter in the DatabricksNotebookOperator that would override the **run_if** field in the job json object

I'm happy to help with a PR

### Use case/motivation

I have this complex job in Databricks that I am trying to migrate as code and I'm blocked because I can't reproduce the dependency issue that I mentioned.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",RafaelCartenet,2024-10-08 09:10:45+00:00,[],2024-10-08 09:13:02+00:00,,https://github.com/apache/airflow/issues/42822,"[('kind:feature', 'Feature Requests'), ('provider:databricks', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2399296235, 'issue_id': 2572589735, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 8, 9, 10, 49, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-08 09:10:49 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2572544370,issue,open,,AIP-38 | User profile settings,"In our designs the bottom-most button of the Nav is a user profile.

- First we should build the user profile menu: #42823
- Add Timezone to the menu #42817

And then an Advanced option to open a modal with:
- User profile
- Color blind mode #43054",bbovenzi,2024-10-08 08:52:57+00:00,[],2024-10-15 20:28:12+00:00,,https://github.com/apache/airflow/issues/42821,"[('kind:meta', 'High-level information important to the community'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]",[],
2572420684,issue,closed,completed,backfill ,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

![image](https://github.com/user-attachments/assets/a8db096b-7510-4867-a261-231302419f55)
In normal run, this would be considered normal or skipped tasks, but in backfill it would be considered a failure. (I can't keep using the --continue-on-failures argument.)

### What you think should happen instead?

_No response_

### How to reproduce

Run the history task with backfill and build the task with task.virtualenv, venv_cache_path

### Operating System

ubuntu

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bangbangDong,2024-10-08 08:04:17+00:00,[],2024-10-09 12:34:02+00:00,2024-10-09 12:34:02+00:00,https://github.com/apache/airflow/issues/42818,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:backfill', 'Specifically for backfill related'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2399138729, 'issue_id': 2572420684, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 8, 8, 4, 19, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-08 08:04:19 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2572209222,issue,closed,completed,Add a clock widget with timezone support in new UI,"### Description

The clock widget in the navigation bar was useful to change time from UTC to other timezones like local timezone in the old UI. The change was also reflected in all other datetime related components. It will be helpful to have the widget in the new UI like the side bar below dark theme toggle button. I opened this issue since I didn't see it in the designs. Please feel free to close this if it's tracked in other issue.

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",tirkarthi,2024-10-08 06:25:36+00:00,['bbovenzi'],2024-10-17 22:00:04+00:00,2024-10-17 22:00:04+00:00,https://github.com/apache/airflow/issues/42817,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]","[{'comment_id': 2398955307, 'issue_id': 2572209222, 'author': 'tirkarthi', 'body': 'cc: @bbovenzi @pierrejeambrun', 'created_at': datetime.datetime(2024, 10, 8, 6, 25, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2399263391, 'issue_id': 2572209222, 'author': 'bbovenzi', 'body': ""Yes, let's put that and the light/dark mode in a User Profile settings modal."", 'created_at': datetime.datetime(2024, 10, 8, 8, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2414943245, 'issue_id': 2572209222, 'author': 'bbovenzi', 'body': 'We will also have to copy over the `useTimezone` hook and provider from the previous UI and the `<Time />` component. But both should use `dayjs` instead of `moment`', 'created_at': datetime.datetime(2024, 10, 15, 20, 27, tzinfo=datetime.timezone.utc)}]","tirkarthi (Issue Creator) on (2024-10-08 06:25:52 UTC): cc: @bbovenzi @pierrejeambrun

bbovenzi (Assginee) on (2024-10-08 08:57:00 UTC): Yes, let's put that and the light/dark mode in a User Profile settings modal.

bbovenzi (Assginee) on (2024-10-15 20:27:00 UTC): We will also have to copy over the `useTimezone` hook and provider from the previous UI and the `<Time />` component. But both should use `dayjs` instead of `moment`

"
2571234442,issue,open,,Airflow 2.9.1 : AirflowContextDeprecationWarning: Accessing 'yesterday_ds_nodash' from the template is deprecated and will be removed in a future version.,"### Discussed in https://github.com/apache/airflow/discussions/40113

<div type='discussions-op-text'>

<sup>Originally posted by **michaelritsema** June  6, 2024</sup>
### Apache Airflow version

2.9.1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

related: #20603 

I installed from PIP and in development mode on a mac. I'm experiencing this same issue when calling get_context()

I'm new to airflow but it seems like there are still references to this in the code:

class _BasePythonVirtualenvOperator(PythonOperator, metaclass=ABCMeta):
    BASE_SERIALIZABLE_CONTEXT_KEYS = {
        ""ds"",
        ""ds_nodash"",
        ""expanded_ti_count"",
        ""inlets"",
        ""map_index_template"",
        ""next_ds"",
        ""next_ds_nodash"",
        ""outlets"",
        ""prev_ds"",
        ""prev_ds_nodash"",
        ""run_id"",
        ""task_instance_key_str"",
        ""test_mode"",
        ""tomorrow_ds"",
        ""tomorrow_ds_nodash"",
        ""ts"",
        ""ts_nodash"",
        ""ts_nodash_with_tz"",
        ""yesterday_ds"",
        ""yesterday_ds_nodash"",
    }


expanded log:

/Users/mr/airflow/lib/python3.11/site-packages/airflow/utils/context.py:212 AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
/Users/mr/airflow/lib/python3.11/site-packages/airflow/utils/context.py:212 AirflowContextDeprecationWarning: Accessing 'next_ds' from the template is deprecated and will be removed in a future version. Please use '{{ data_interval_end | ds }}' instead.
/Users/mr/airflow/lib/python3.11/site-packages/airflow/utils/context.py:212 AirflowContextDeprecationWarning: Accessing 'next_ds_nodash' from the template is deprecated and will be removed in a future version. Please use '{{ data_interval_end | ds_nodash }}' instead.
/Users/mr/airflow/lib/python3.11/site-packages/airflow/utils/context.py:212 AirflowContextDeprecationWarning: Accessing 'next_execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_end' instead.
/Users/mr/airflow/lib/python3.11/site-packages/airflow/utils/context.py:212 AirflowContextDeprecationWarning: Accessing 'prev_ds' from the template is deprecated and will be removed in a future version.
/Users/mr/airflow/lib/python3.11/site-packages/airflow/utils/context.py:212 AirflowContextDeprecationWarning: Accessing 'prev_ds_nodash' from the template is deprecated and will be removed in a future version.
/Users/mr/airflow/lib/python3.11/site-packages/airflow/utils/context.py:212 AirflowContextDeprecationWarning: Accessing 'prev_execution_date' from the template is deprecated and will be removed in a future version.
/Users/mr/airflow/lib/python3.11/site-packages/airflow/utils/context.py:212 AirflowContextDeprecationWarning: Accessing 'prev_execution_date_success' from the template is deprecated and will be removed in a future version. Please use 'prev_data_interval_start_success' instead.
/Users/mr/airflow/lib/python3.11/site-packages/airflow/utils/context.py:212 AirflowContextDeprecationWarning: Accessing 'tomorrow_ds' from the template is deprecated and will be removed in a future version.
/Users/mr/airflow/lib/python3.11/site-packages/airflow/utils/context.py:212 AirflowContextDeprecationWarning: Accessing 'tomorrow_ds_nodash' from the template is deprecated and will be removed in a future version.
/Users/mr/airflow/lib/python3.11/site-packages/airflow/utils/context.py:212 AirflowContextDeprecationWarning: Accessing 'yesterday_ds' from the template is deprecated and will be removed in a future version.
/Users/mr/airflow/lib/python3.11/site-packages/airflow/utils/context.py:212 AirflowContextDeprecationWarning: Accessing 'yesterday_ds_nodash' from the template is deprecated and will be removed in a future version.

### What you think should happen instead?

_No response_

### How to reproduce

any call to get_context()

### Operating System

Sonoma 14.5

### Versions of Apache Airflow Providers

airflow-clickhouse-plugin==1.3.0
apache-airflow==2.9.1
apache-airflow-providers-common-io==1.3.2
apache-airflow-providers-common-sql==1.14.0
apache-airflow-providers-fab==1.1.1
apache-airflow-providers-ftp==3.9.1
apache-airflow-providers-http==4.11.1
apache-airflow-providers-imap==3.6.1
apache-airflow-providers-smtp==1.7.1
apache-airflow-providers-sqlite==3.8.1

### Deployment

Astronomer

### Deployment details

local pip

### Anything else?

,

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
</div>",shahar1,2024-10-07 19:04:30+00:00,[],2024-10-07 19:22:20+00:00,,https://github.com/apache/airflow/issues/42806,"[('kind:bug', 'This is a clearly a bug'), ('AIP-31', 'Task Flow API for nicer DAG definition'), ('area:core', '')]","[{'comment_id': 2397674919, 'issue_id': 2571234442, 'author': 'shahar1', 'body': 'Recreated the issue after a reproducible example was provided in the discussion, see:\r\nhttps://github.com/apache/airflow/discussions/40113#discussioncomment-10869110', 'created_at': datetime.datetime(2024, 10, 7, 19, 5, 6, tzinfo=datetime.timezone.utc)}]","shahar1 (Issue Creator) on (2024-10-07 19:05:06 UTC): Recreated the issue after a reproducible example was provided in the discussion, see:
https://github.com/apache/airflow/discussions/40113#discussioncomment-10869110

"
2570917829,issue,open,,max_active_tis_per_dagrun is somewhat inaptly named,"### Body

The actual thing it controls is more like **max_active_tis-_per-task_-per-dagrun**.  There may be a better naming choice.  And there may not.

The PR that added it https://github.com/apache/airflow/pull/29094


cc @hussein-awala  @ashb 

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2024-10-07 16:26:43+00:00,[],2024-10-07 16:27:46+00:00,,https://github.com/apache/airflow/issues/42801,"[('kind:meta', 'High-level information important to the community')]",[],
2570440449,issue,closed,completed,PythonVirtualenvOperator - SyntaxError: cannot assign to operator,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.8.4 and 2.9.3

### What happened?

I’m not sure if this error is known, but when I use the PythonVirtualenvOperator, I get the following error: ""SyntaxError: cannot assign to operator"" if the file where I create the DAG contains a hyphen in its name. When I remove the hyphen, the error disappears.


### How to reproduce

1.	Create a new DAG file in your Airflow project with a hyphen (-) in its name (e.g., my-dag.py).
2.	Define a DAG inside the file and include the PythonVirtualenvOperator in one of the tasks.
3.	Trigger the DAG or run it.
4.	You will encounter the following error: SyntaxError: cannot assign to operator.
5.	Rename the DAG file to remove the hyphen (e.g., my_dag.py), then run it again.
6.	The error no longer occurs.

### Operating System

Debian GNU/Linux 12.6

### Deployment

Official Apache Airflow Helm Chart

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",k-slash,2024-10-07 13:24:48+00:00,['jason810496'],2024-10-15 08:10:07+00:00,2024-10-13 01:55:18+00:00,https://github.com/apache/airflow/issues/42796,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:core', ''), ('area:core-operators', 'Operators, Sensors and hooks within Core Airflow')]","[{'comment_id': 2396929261, 'issue_id': 2570440449, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 7, 13, 24, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2397938417, 'issue_id': 2570440449, 'author': 'potiuk', 'body': 'Likely because the generated dynamically Python script generates a name that is not valid Python identifier. Should be easy to fix by slugifying the identifier generated, so maybe someone will take a look. Marked it as ""good first issue"".', 'created_at': datetime.datetime(2024, 10, 7, 21, 32, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2399028338, 'issue_id': 2570440449, 'author': 'k-slash', 'body': 'Thanks for the explanation. I hope this issue will be useful to others who encounter this problem in the meantime.', 'created_at': datetime.datetime(2024, 10, 8, 7, 12, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2404552508, 'issue_id': 2570440449, 'author': 'jason810496', 'body': ""Hi @potiuk , could you please assign this issue to me?I am a new contributor and have just reproduced the issue. I'm currently investigating it."", 'created_at': datetime.datetime(2024, 10, 10, 9, 16, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2413186950, 'issue_id': 2570440449, 'author': 'k-slash', 'body': 'Thank you for solving the issue, @jason810496. Looking forward to seeing the fix in the next version of Airflow! :)', 'created_at': datetime.datetime(2024, 10, 15, 8, 10, 6, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-07 13:24:52 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-10-07 21:32:18 UTC): Likely because the generated dynamically Python script generates a name that is not valid Python identifier. Should be easy to fix by slugifying the identifier generated, so maybe someone will take a look. Marked it as ""good first issue"".

k-slash (Issue Creator) on (2024-10-08 07:12:34 UTC): Thanks for the explanation. I hope this issue will be useful to others who encounter this problem in the meantime.

jason810496 (Assginee) on (2024-10-10 09:16:55 UTC): Hi @potiuk , could you please assign this issue to me?I am a new contributor and have just reproduced the issue. I'm currently investigating it.

k-slash (Issue Creator) on (2024-10-15 08:10:06 UTC): Thank you for solving the issue, @jason810496. Looking forward to seeing the fix in the next version of Airflow! :)

"
2569090300,issue,open,,_OwnersFilter is using  wildcard match to filter DAGs,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

https://github.com/apache/airflow/blob/b0a18d9019a1a12d58435185b0c86e28af050685/airflow/api_fastapi/parameters.py#L219

In the above code, we are using a wildcard match to filter DAGs. But, it should be an exact match. It's being used as a filter here:

https://github.com/apache/airflow/blob/1524283227fa00fbc01efb48ec5afb9620fa0b80/airflow/api_fastapi/views/public/dags.py#L61C5-L61C11

Because of that, when invoking GET Dags with a filter for ""NotAnAdmin"", it will also return DAGs with the owner ""Admin""
It's

### What you think should happen instead?

_OwnersFilter should be using exact match.

### How to reproduce

create two DAGs, one with the owner ""Admin"" and another with the owner ""NotAnAdmin"". Query GET Dags with the filter on owner filet to get only ""Admin"" DAGs. Use the latest FastAPI swagger UI to test this quickly

### Operating System

MacOS - 15.0.1 (24A348)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rawwar,2024-10-07 01:49:37+00:00,['rawwar'],2024-11-04 10:15:13+00:00,,https://github.com/apache/airflow/issues/42790,"[('kind:bug', 'This is a clearly a bug'), ('area:API', ""Airflow's REST/HTTP API""), ('area:core', '')]","[{'comment_id': 2395953230, 'issue_id': 2569090300, 'author': 'rawwar', 'body': 'I started writing tests for a PR to update this and then realised it\'s not straightforward. \r\n\r\nWe allow the owner field to be a string. On UI, we do a basic comma split to show owners. \r\n\r\nHere\'s an example:\r\n\r\n```\r\ndefault_args = {\r\n    \'owner\': \'Admin\',\r\n    \'depends_on_past\': False,\r\n    \'email_on_failure\': False,\r\n    \'email_on_retry\': False,\r\n    \'retries\': 1,\r\n    \'retry_delay\': timedelta(minutes=5),\r\n}\r\n\r\n# DAG A\r\ndag_a = DAG(\r\n    \'dag_owner_admin\',\r\n    default_args=default_args,\r\n    description=\'A DAG owned by Admin\',\r\n    schedule=\'@daily\',\r\n    start_date=datetime(2023, 7, 10),\r\n    catchup=False,\r\n    tags=[\'example\', \'owner_admin\'],\r\n)\r\n\r\nwith dag_a:\r\n    task_a1 = BashOperator(\r\n        task_id=\'print_date\',\r\n        bash_command=\'date\',\r\n        owner=""devops,""u,pstr$eam\',\r\n    )\r\n\r\n    task_a2 = BashOperator(\r\n        task_id=\'echo_hello\',\r\n        bash_command=\'echo ""Hello from Admin DAG""\',\r\n    )\r\n\r\n    task_a1 >> task_a2\r\n\r\n```\r\n\r\nAbove DAG will create the following entry in DB:\r\n![image](https://github.com/user-attachments/assets/23990774-f580-4fe2-a0a7-c9ad94c7ecc2)\r\n\r\nOn UI:\r\n![image](https://github.com/user-attachments/assets/84ab04e9-9a79-4833-9e71-2d9f2e5cf735)\r\n\r\n\r\nA question here: Do we want to allow users to filter by individual users or the entire owner string? Or maybe simply call allow users to filter by an owner pattern?', 'created_at': datetime.datetime(2024, 10, 7, 5, 40, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2396087318, 'issue_id': 2569090300, 'author': 'pierrejeambrun', 'body': 'Also we most likely want to fix the `api_connexion` endpoint as well, and backport that to 2.10.x', 'created_at': datetime.datetime(2024, 10, 7, 7, 7, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2396092723, 'issue_id': 2569090300, 'author': 'pierrejeambrun', 'body': ""> A question here: Do we want to allow users to filter by individual users or the entire owner string? Or maybe simply call allow users to filter by an owner pattern?\r\n\r\n\r\nI think a 'strict' match against the db field is enough to fix the mentioned bug ? (on the whole owner string then ?)"", 'created_at': datetime.datetime(2024, 10, 7, 7, 10, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2396189006, 'issue_id': 2569090300, 'author': 'rawwar', 'body': '@pierrejeambrun , the issue here is that the user has no way of knowing what exactly is in DB. If you notice, the value in DB is a concatenation of task level + DAG level `owners` values.', 'created_at': datetime.datetime(2024, 10, 7, 7, 56, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2396279972, 'issue_id': 2569090300, 'author': 'pierrejeambrun', 'body': '> @pierrejeambrun , the issue here is that the user has no way of knowing what exactly is in DB. If you notice, the value in DB is a concatenation of task level + DAG level owners values.\r\n\r\nHow things are stored in the DB is more of an implementation detail. The user can see the different owners on the homepage table for instance (as a list of string), and as long as he is able to filter on that I think that\'s good. Returning wrong results because of the wildcard matching is an issue though. Do you have a case in mind @rawwar where that could be an issue ?\r\n\r\nNow that I think about this, due to the fact that it is stored in the DB as a comma separated string makes the implementation of an ""exact match of values"" not straightforward. That might be the reason for the wildcard match in the first place.', 'created_at': datetime.datetime(2024, 10, 7, 8, 37, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2396388200, 'issue_id': 2569090300, 'author': 'rawwar', 'body': '> Do you have a case in mind @rawwar where that could be an issue ?\r\n\r\n@pierrejeambrun , You answered it in the edit. I too was thinking from the perspective of matching it in the filter.\r\n\r\nI am wondering if an Owner linked to an actual user should make this straightforward. However, implementing it requires more work. Which is to connect owners with an existing user in the users table. For arbitary filters, we already have tags, where user can add anything like ""department: devops"" or some random key-value pair\r\n\r\n\r\nFor now, I think we can let it be a wildcard match. But, have a note that we do a wildcard match in the API field description', 'created_at': datetime.datetime(2024, 10, 7, 9, 23, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2454308482, 'issue_id': 2569090300, 'author': 'rawwar', 'body': 'One idea is to denormalize the author values in DB so that its easier to match', 'created_at': datetime.datetime(2024, 11, 4, 10, 15, 8, tzinfo=datetime.timezone.utc)}]","rawwar (Issue Creator) on (2024-10-07 05:40:42 UTC): I started writing tests for a PR to update this and then realised it's not straightforward. 

We allow the owner field to be a string. On UI, we do a basic comma split to show owners. 

Here's an example:

```
default_args = {
    'owner': 'Admin',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# DAG A
dag_a = DAG(
    'dag_owner_admin',
    default_args=default_args,
    description='A DAG owned by Admin',
    schedule='@daily',
    start_date=datetime(2023, 7, 10),
    catchup=False,
    tags=['example', 'owner_admin'],
)

with dag_a:
    task_a1 = BashOperator(
        task_id='print_date',
        bash_command='date',
        owner=""devops,""u,pstr$eam',
    )

    task_a2 = BashOperator(
        task_id='echo_hello',
        bash_command='echo ""Hello from Admin DAG""',
    )

    task_a1 >> task_a2

```

Above DAG will create the following entry in DB:
![image](https://github.com/user-attachments/assets/23990774-f580-4fe2-a0a7-c9ad94c7ecc2)

On UI:
![image](https://github.com/user-attachments/assets/84ab04e9-9a79-4833-9e71-2d9f2e5cf735)


A question here: Do we want to allow users to filter by individual users or the entire owner string? Or maybe simply call allow users to filter by an owner pattern?

pierrejeambrun on (2024-10-07 07:07:12 UTC): Also we most likely want to fix the `api_connexion` endpoint as well, and backport that to 2.10.x

pierrejeambrun on (2024-10-07 07:10:20 UTC): I think a 'strict' match against the db field is enough to fix the mentioned bug ? (on the whole owner string then ?)

rawwar (Issue Creator) on (2024-10-07 07:56:58 UTC): @pierrejeambrun , the issue here is that the user has no way of knowing what exactly is in DB. If you notice, the value in DB is a concatenation of task level + DAG level `owners` values.

pierrejeambrun on (2024-10-07 08:37:55 UTC): How things are stored in the DB is more of an implementation detail. The user can see the different owners on the homepage table for instance (as a list of string), and as long as he is able to filter on that I think that's good. Returning wrong results because of the wildcard matching is an issue though. Do you have a case in mind @rawwar where that could be an issue ?

Now that I think about this, due to the fact that it is stored in the DB as a comma separated string makes the implementation of an ""exact match of values"" not straightforward. That might be the reason for the wildcard match in the first place.

rawwar (Issue Creator) on (2024-10-07 09:23:40 UTC): @pierrejeambrun , You answered it in the edit. I too was thinking from the perspective of matching it in the filter.

I am wondering if an Owner linked to an actual user should make this straightforward. However, implementing it requires more work. Which is to connect owners with an existing user in the users table. For arbitary filters, we already have tags, where user can add anything like ""department: devops"" or some random key-value pair


For now, I think we can let it be a wildcard match. But, have a note that we do a wildcard match in the API field description

rawwar (Issue Creator) on (2024-11-04 10:15:08 UTC): One idea is to denormalize the author values in DB so that its easier to match

"
2568733059,issue,closed,not_planned,High load issues in Airflow DAGs when running on EKS cluster,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.8.2

### What happened?

Description:

I am encountering significant load spikes when running DAGs in an Airflow setup on an EKS cluster with the following configuration:

EKS Node Type: c5.9xlarge
OS (Architecture): linux (amd64)
OS Image: Amazon Linux 2
Kernel Version: 5.10.225-213.878.amzn2.x86_64
Container Runtime: containerd://1.7.11
Kubelet Version: v1.28.13-eks-a737599
The DAGs are responsible for ingesting data from Kafka to S3 and are triggered both periodically and by orchestration DAGs. However, during their execution, I observe a sharp increase in resource usage, particularly CPU and memory, on the worker nodes.

The configuration for some DAGs includes relatively high max_active_runs, which may contribute to the overload. I have attached a simplified version of the DAGs below.

Steps to Reproduce:

Deploy an Airflow instance in an EKS cluster with at least two c5.9xlarge worker nodes.
Run the provided DAGs that trigger data ingestion from Kafka to S3.
Monitor resource usage on the worker nodes.
Expected Behavior:

The load on the worker nodes should remain stable and manageable, even during the execution of multiple DAGs.

Actual Behavior:

The load on the worker nodes increases significantly, causing performance degradation across the cluster.

Environment:

Kubernetes version: v1.28.13-eks-a737599
Airflow version: 2.5.0
Worker nodes: c5.9xlarge
Container runtime: containerd://1.7.11

### What you think should happen instead?

_No response_

### How to reproduce

Deploy an Airflow instance in an EKS cluster with at least two c5.9xlarge worker nodes running Amazon Linux 2.
Configure Airflow with the DAGs that handle ingestion of data from Kafka to S3. Use a high number of max_active_runs (e.g., 10) for DAGs that trigger orchestration or batch tasks.
Trigger the orchestration DAG, which will start the data ingestion processes from Kafka to S3.
Monitor CPU and memory usage on the worker nodes during DAG execution.
Observe the load spike during execution, particularly when multiple DAGs run in parallel.
Expected behavior: Stable resource usage on the worker nodes without sharp increases in load.

Actual behavior: Significant increase in CPU and memory usage, causing performance degradation on the worker nodes.

### Operating System

Ubuntu 22

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

import logging
from datetime import datetime
from CommonDag.L1RawCommonDag import DagRawKafkaS3Msg
from CommonUtil import CommonUtilClass

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

dag_config = CommonUtilClass.DagConfig(
    dag_id=""example_dag"",
    default_args=CommonUtilClass.DagDefaultArgs(
        owner=""airflow_user"",
        start_date=datetime(2024, 9, 25)
    ),
    max_active_runs=5,
    tags=[""L0"", ""kafka"", ""cdc""]
)

dag_object = DagRawKafkaS3Msg(dag_config=dag_config, log=logger)
dag = dag_object.prepare_dag()


### Anything else?

I would appreciate any guidance on how to manage these load spikes more effectively, or suggestions for optimizing DAGs in this scenario.



### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",sairan-01,2024-10-06 16:03:19+00:00,[],2024-10-29 00:15:29+00:00,2024-10-29 00:15:29+00:00,https://github.com/apache/airflow/issues/42784,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('area:performance', ''), ('pending-response', ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2395490254, 'issue_id': 2568733059, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 6, 16, 3, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2395500529, 'issue_id': 2568733059, 'author': 'gopidesupavan', 'body': ""How are you handling Kafka to S3 processing? It appears that you've implemented a custom operator. In general, if your implementation involves resource-intensive operations, it will certainly impact memory and CPU usage. Additionally, you're using version 2.8, which is outdated. I recommend upgrading to a more recent version, as it includes optimizations."", 'created_at': datetime.datetime(2024, 10, 6, 16, 34, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2427963729, 'issue_id': 2568733059, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 10, 22, 0, 15, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2442910523, 'issue_id': 2568733059, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 10, 29, 0, 15, 29, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-06 16:03:21 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

gopidesupavan on (2024-10-06 16:34:30 UTC): How are you handling Kafka to S3 processing? It appears that you've implemented a custom operator. In general, if your implementation involves resource-intensive operations, it will certainly impact memory and CPU usage. Additionally, you're using version 2.8, which is outdated. I recommend upgrading to a more recent version, as it includes optimizations.

github-actions[bot] on (2024-10-22 00:15:12 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-10-29 00:15:29 UTC): This issue has been closed because it has not received response from the issue author.

"
2568501928,issue,open,,Add write log capacity to ElasticsearchTaskHandler & OpensearchTaskHandler,"### Description

According to the aws provider doc, when enabling remote logging, the `CloudwatchTaskHandler` and `S3TaskHandler` supports both reading & writing task logs 

https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/logging/index.html

As for remote logging with Elasticsearch / Opensearch, currently only reading log is supported. Users need to deploy other softwares (such as filebeat & logstash) to ship Airflow task logs to Elasticsearch / Opensearch. Also, user would need to ensure the log messages contain a valid `log_id` of format `{dag_id}-{task_id}-{execution_date}-{try_number}` in order for reading remote log to work.

Wouldn't be nice if Airflow supports writing each task log to Elasticsearch / Opensearch, after each DAG task is completed ? Similar to `S3TaskHandler`, once remote logging is properly configured, DAG task log will automatically be written to, and read from Elasticsearch / Opensearch, and users need not deploy additional software to ship task logs



### Use case/motivation

Similar to `S3TaskHandler`, `ElasticsearchTaskHandler` and `OpensearchTaskHandler` should support automatically writing task logs to destination.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Owen-CH-Leung,2024-10-06 06:57:24+00:00,[],2024-10-08 13:39:16+00:00,,https://github.com/apache/airflow/issues/42780,"[('area:providers', ''), ('area:logging', ''), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2395320970, 'issue_id': 2568501928, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 6, 6, 57, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2398912434, 'issue_id': 2568501928, 'author': 'romsharon98', 'body': 'I would love this feature. \r\nWhen I tried to implement it myself by setting up Logstash and Filebeat as you mentioned, I encountered an issue. \r\nFilebeat struggled to read the log files due to the default [Airflow log filename template](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#log-filename-template), where the hierarchy was structured like this:\r\n\r\n`dag_id={ ti.dag_id }/run_id={ ti.run_id }/task_id={ ti.task_id }/{%% if ti.map_index >= 0 %%}map_index={ ti.map_index }/{%% endif %%}attempt={ try_number }.log`\r\n\r\nThe hierarchy was too deep and contained too many folders (around 600,000) for Filebeat to process efficiently, causing a significant delay (hours) before the first log file was sent to Logstash. As a result, there was a major lag in log delivery.\r\n\r\nI believe with this structure, it’s challenging to implement effectively. However, with your proposed approach (placing all log files in a single flat directory with no depth), it would definitely work better!\r\n\r\n(We didn’t change the log format because we didn’t want to risk losing logs.)', 'created_at': datetime.datetime(2024, 10, 8, 5, 53, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2399727077, 'issue_id': 2568501928, 'author': 'Owen-CH-Leung', 'body': ""Yeah what I propose here is - user shouldn't need to deploy additional software like filebeat, and airflow will handle shipping logs to elasticsearch for you. Just like what `S3TaskHandler` does"", 'created_at': datetime.datetime(2024, 10, 8, 12, 36, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2399878854, 'issue_id': 2568501928, 'author': 'romsharon98', 'body': ""> Yeah what I propose here is - user shouldn't need to deploy additional software like filebeat, and airflow will handle shipping logs to elasticsearch for you. Just like what `S3TaskHandler` does \n\nIf you will be able to do it also for the default log file template, it will be fantastic!"", 'created_at': datetime.datetime(2024, 10, 8, 13, 39, 15, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-06 06:57:26 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

romsharon98 on (2024-10-08 05:53:52 UTC): I would love this feature. 
When I tried to implement it myself by setting up Logstash and Filebeat as you mentioned, I encountered an issue. 
Filebeat struggled to read the log files due to the default [Airflow log filename template](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#log-filename-template), where the hierarchy was structured like this:

`dag_id={ ti.dag_id }/run_id={ ti.run_id }/task_id={ ti.task_id }/{%% if ti.map_index >= 0 %%}map_index={ ti.map_index }/{%% endif %%}attempt={ try_number }.log`

The hierarchy was too deep and contained too many folders (around 600,000) for Filebeat to process efficiently, causing a significant delay (hours) before the first log file was sent to Logstash. As a result, there was a major lag in log delivery.

I believe with this structure, it’s challenging to implement effectively. However, with your proposed approach (placing all log files in a single flat directory with no depth), it would definitely work better!

(We didn’t change the log format because we didn’t want to risk losing logs.)

Owen-CH-Leung (Issue Creator) on (2024-10-08 12:36:10 UTC): Yeah what I propose here is - user shouldn't need to deploy additional software like filebeat, and airflow will handle shipping logs to elasticsearch for you. Just like what `S3TaskHandler` does

romsharon98 on (2024-10-08 13:39:15 UTC): If you will be able to do it also for the default log file template, it will be fantastic!

"
2568401965,issue,closed,completed,Include Unit Test for Legacy UI and API Selective Checks in Breeze,"### Description

Follow-up work of #42758.

### Use case/motivation

_No response_

### Related issues

#42758

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-10-06 01:02:55+00:00,['bugraoz93'],2024-11-11 14:16:37+00:00,2024-11-11 14:16:37+00:00,https://github.com/apache/airflow/issues/42774,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('kind:feature', 'Feature Requests')]","[{'comment_id': 2395501065, 'issue_id': 2568401965, 'author': 'gopidesupavan', 'body': '@bugraoz93 can i assign it to you ? :)', 'created_at': datetime.datetime(2024, 10, 6, 16, 36, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2395504315, 'issue_id': 2568401965, 'author': 'bugraoz93', 'body': 'That would be great, @gopidesupavan! Thanks! :)', 'created_at': datetime.datetime(2024, 10, 6, 16, 46, 25, tzinfo=datetime.timezone.utc)}]","gopidesupavan on (2024-10-06 16:36:10 UTC): @bugraoz93 can i assign it to you ? :)

bugraoz93 (Issue Creator) on (2024-10-06 16:46:25 UTC): That would be great, @gopidesupavan! Thanks! :)

"
2567875541,issue,closed,completed,Rerun only certain TASKs,"### Description

If you change certain logic and need to restart from the past, only certain TASKs may need to work.
For example, in s3 to gcs, gcs to bigquery, ETL, load process

In that case, it would be nice if there was a function that only certain tasks could be restarted

![image](https://github.com/user-attachments/assets/2ac2d2ed-69fa-49d8-96d1-ed38f959c75f)

Run only that task(also_run_this) from the past

### Use case/motivation

I want a function that can only apply a specific task to the backfill

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee2532,2024-10-05 11:11:12+00:00,[],2024-10-11 13:59:19+00:00,2024-10-11 13:59:19+00:00,https://github.com/apache/airflow/issues/42768,"[('kind:feature', 'Feature Requests'), ('area:backfill', 'Specifically for backfill related'), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2567511568,issue,open,,"Indicate warnings or other ""non-failing"" indicators in Airflow UI and callbacks","### Description

Sometimes a task shouldn't fail but could contain a warning that the user should know about and fix; for example, a deprecation warning or a user-logged warning. These warnings don't ""bubble up"" anywhere in the Airflow UI.

Making warnings more visible could have the impact of making upgrades (like the upcoming 3.0 upgrade) much easier for developers.

Possible but not optimal current solutions:

- You can log a warning-level task log, but aggregating all warning-level task logs in the Airflow UI is (as far as I know) not possible.
- You could also customize the log handler to handle WARNING-level messages across all DAGs, but sending these messages to Slack or otherwise might be expensive within this process
- Aggregate using third party tool

A solution could look like all otherwise successful task instances in a DAG run labeled by a yellow (or other color) dot instead of a green dot. It could also help to aggregate warning-level or error-level messages in the UI.

Implementation wise - Maybe there could be a separate task instance state (warning) or different UI indicator on the current success state that could be leveraged to visually indicate this situation. If it were a separate warning state, an on_warning_callback could also be added to send more information to the user's existing Slack alert workflow.

### Use case/motivation

Better indication that something in a task went wrong, or something happened the user should be notified about 

### Related issues

@RNHTTR recommend I copy here from this discussion:

https://github.com/apache/airflow/discussions/42188

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",matthewblock,2024-10-04 22:58:59+00:00,[],2024-10-04 23:04:59+00:00,,https://github.com/apache/airflow/issues/42757,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2394769774, 'issue_id': 2567511568, 'author': 'matthewblock', 'body': ""Some more thoughts:\r\n\r\nI'm realizing that for a TaskInstance, both success and failed states can have warnings, where a warning is at least one call of warning.warn() during task execution. Maybe a TaskInstance should have an additional attribute has_warnings if any warnings were issued during its execution.\r\n\r\nWe can collect any warnings during task execution using the Python built in warnings.catch_warnings context manager, at least when using PythonOperator and similar. We could then set ti.has_warnings and handle these warnings in an on_warning_callback, or even aggregated somewhere else in the UI.\r\n\r\nThis would be useful to help developers stay on top of Airflow-communicated deprecation warnings and avoid unpleasant surprises when upgrading.\r\n\r\nIs there a better alternative to see all deprecation warnings across all DAGs/tasks that currently exists?"", 'created_at': datetime.datetime(2024, 10, 4, 23, 0, 9, tzinfo=datetime.timezone.utc)}]","matthewblock (Issue Creator) on (2024-10-04 23:00:09 UTC): Some more thoughts:

I'm realizing that for a TaskInstance, both success and failed states can have warnings, where a warning is at least one call of warning.warn() during task execution. Maybe a TaskInstance should have an additional attribute has_warnings if any warnings were issued during its execution.

We can collect any warnings during task execution using the Python built in warnings.catch_warnings context manager, at least when using PythonOperator and similar. We could then set ti.has_warnings and handle these warnings in an on_warning_callback, or even aggregated somewhere else in the UI.

This would be useful to help developers stay on top of Airflow-communicated deprecation warnings and avoid unpleasant surprises when upgrading.

Is there a better alternative to see all deprecation warnings across all DAGs/tasks that currently exists?

"
2567499440,issue,closed,not_planned,EcsRunTaskOperator should cast rendered networkConfiguration template field,"### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.16.0

### Apache Airflow version

2.8.1

### Operating System

N/A

### Deployment

Amazon (AWS) MWAA

### Deployment details

N/A

### What happened

Hi! I'm submitting this issue about the EcsRunTaskOperator and its templated fields, which are difficult to work with at my current job. Multiple data engineers have tried to find an elegant workaround, but I think we're at wits' end.

Passing in a templated string to the EcsRunTaskOperator `network_configuration` argument, whether using `'{{ var.json... }}'` or `'{{ var.value... }}'`, passes it as a string which fails parameter validation:

```
(Parameter validation failed:
Invalid type for parameter networkConfiguration, value: { ""awsvpcConfiguration"": { ""securityGroups"": [ ""sg-***"" ], ""subnets"": [ ""subnet-***"", ""subnet-***"", ""subnet-***"" ] } }, type: <class 'str'>, valid types: <class 'dict'>; 3150)
```

We've been able to get around this by:

1. Using `Variable.get(...)` in the root code, which is not ideal since we have a secrets backend that charges us for every call, which is made whenever a DAG is compiled. I think we'd have to create a new task in every DAG to properly render this variable in XCOM, which is unsustainable and clutters our DAGs.
2. Setting `render_template_as_native_obj=True` at the DAG level, which is not ideal since `EcsRunTaskOperator` also renders overrides, which we use to pass environment variables to our containers. Any variable that could be inferred as an `int` is automatically cast to `int`, which fails parameter validation. I think we'd have to update our variables in the secrets backend to explicitly render the strings, which is not ideal.

### What you think should happen instead

I think `EcsRunTaskOperator` should try to cast any string input to a dict object. Otherwise, the template fields are fairly combersome to use.

### How to reproduce

Create an `EcsRunTaskOperator` and pass in a template string as a parameter. For example:

```python
from datetime import datetime

from airflow import DAG
from airflow.providers.amazon.aws.operators.ecs import EcsRunTaskOperator

from utils import default_args, dag_callback, datadog_ecs_container

with DAG(
    dag_id='test_dag',
    schedule=None,
    start_date=datetime(2024, 10, 4),
    catchup=False,
    tags=[ENVIRONMENT],
    render_template_as_native_obj=False
) as dag:
    test_task = EcsRunTaskOperator(
        task_id = 'test_task',
        cluster='my-cluster',
        task_definition='my-task-definition',
        launch_type='FARGATE',
        overrides={
            'containerOverrides': [
                {
                    'name': 'my-container',
                    'environment': [
                        {
                            'name': 'ENVIRONMENT_VARIABLE_A',
                            'value': 'environment_variable_a_string_value'
                        },
                        {
                            'name': 'ENVIRONMENT_VARIABLE_B',
                            'value': '123456789'
                        }
                    ]
                }
            ]
        },
        network_configuration='{{ var.value.ecs_network_configuration_json }}'
    )

    test_task
```


### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",mmyers5,2024-10-04 22:41:29+00:00,[],2024-12-02 00:17:17+00:00,2024-12-02 00:17:17+00:00,https://github.com/apache/airflow/issues/42756,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('kind:feature', 'Feature Requests'), ('pending-response', '')]","[{'comment_id': 2394756951, 'issue_id': 2567499440, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 4, 22, 41, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2400971745, 'issue_id': 2567499440, 'author': 'jayceslesar', 'body': 'One fix could be making a cast here (or some private method to correctly cast the overrides before they are passed to the run_task call) https://github.com/apache/airflow/blob/main/airflow/providers/amazon/aws/operators/ecs.py#L461', 'created_at': datetime.datetime(2024, 10, 8, 23, 12, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2450750331, 'issue_id': 2567499440, 'author': 'o-nikolas', 'body': 'Hey! Thanks for reaching out with this ticket.\r\n\r\nI hear your pain, but I\'m not sure we want to go down the road of manually casting individual types where they suit each usecase best. There are a lot of other templated fields and we\'d have to cast them all, across all operators. This is really what the `render_template_as_native_obj` behaviour is meant for. If that doesn\'t work for you because you have some types you _don\'t_ want casted (the ints you mentioned) then that\'s a bespoke/tricky spot.\r\n\r\nI think the most natural approach is one you mostly outlined about having a task read in the data and cast it exactly in the way that you need for your usecase, since again it\'s quite specific. It\'s not as unreasonable and unmanageable as it may look at the outset, this is a fairly common paradigm actually, we use it all the time in our dags. You can wrap the data extractor tasks and the task they extract data for in a task group as well to reduce the ""clutter"". I mocked up this example quickly based of the dag you provided and it\'s quite clean in the code and the airflow UI:\r\n\r\n```python\r\nwith DAG(\r\n    dag_id=\'test_dag\',          \r\n    schedule=None,                                                            \r\n    start_date=datetime(2024, 10, 4),                                         \r\n    catchup=False,                   \r\n    render_template_as_native_obj=False\r\n) as dag:                              \r\n                                                                              \r\n    with TaskGroup(group_id=\'EcsRunTask\') as ecs_run_task_group:              \r\n        @task()\r\n        def network_config() -> dict:                                         \r\n            return json.loads(Variable.get(\'ecs_network_configuration_json\')) \r\n                                                                              \r\n        test_task: Unknown  = EcsRunTaskOperator(                             \r\n            task_id=\'test_task\',                                              \r\n            cluster=\'my-cluster\',                                             \r\n            task_definition=\'my-task-definition\',                             \r\n            launch_type=\'FARGATE\',                                            \r\n            overrides={                                                       \r\n                \'containerOverrides\': [                                       \r\n                    {                                                         \r\n                        \'name\': \'my-container\',                               \r\n                        \'environment\': [                                      \r\n                            {                                                 \r\n                                \'name\': \'ENVIRONMENT_VARIABLE_A\',             \r\n                                \'value\': \'environment_variable_a_string_value\'\r\n                            },                                                \r\n                            {                                                 \r\n                                \'name\': \'ENVIRONMENT_VARIABLE_B\',             \r\n                                \'value\': \'123456789\'                          \r\n                            }                                                 \r\n                        ]                                                     \r\n                    }                                                         \r\n                ]                                                             \r\n            },                                                                \r\n            network_configuration=network_config()                            \r\n        )                                                                     \r\n```', 'created_at': datetime.datetime(2024, 10, 31, 20, 24, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2496442748, 'issue_id': 2567499440, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 11, 25, 0, 16, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2510325846, 'issue_id': 2567499440, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 12, 2, 0, 17, 16, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-04 22:41:32 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jayceslesar on (2024-10-08 23:12:18 UTC): One fix could be making a cast here (or some private method to correctly cast the overrides before they are passed to the run_task call) https://github.com/apache/airflow/blob/main/airflow/providers/amazon/aws/operators/ecs.py#L461

o-nikolas on (2024-10-31 20:24:10 UTC): Hey! Thanks for reaching out with this ticket.

I hear your pain, but I'm not sure we want to go down the road of manually casting individual types where they suit each usecase best. There are a lot of other templated fields and we'd have to cast them all, across all operators. This is really what the `render_template_as_native_obj` behaviour is meant for. If that doesn't work for you because you have some types you _don't_ want casted (the ints you mentioned) then that's a bespoke/tricky spot.

I think the most natural approach is one you mostly outlined about having a task read in the data and cast it exactly in the way that you need for your usecase, since again it's quite specific. It's not as unreasonable and unmanageable as it may look at the outset, this is a fairly common paradigm actually, we use it all the time in our dags. You can wrap the data extractor tasks and the task they extract data for in a task group as well to reduce the ""clutter"". I mocked up this example quickly based of the dag you provided and it's quite clean in the code and the airflow UI:

```python
with DAG(
    dag_id='test_dag',          
    schedule=None,                                                            
    start_date=datetime(2024, 10, 4),                                         
    catchup=False,                   
    render_template_as_native_obj=False
) as dag:                              
                                                                              
    with TaskGroup(group_id='EcsRunTask') as ecs_run_task_group:              
        @task()
        def network_config() -> dict:                                         
            return json.loads(Variable.get('ecs_network_configuration_json')) 
                                                                              
        test_task: Unknown  = EcsRunTaskOperator(                             
            task_id='test_task',                                              
            cluster='my-cluster',                                             
            task_definition='my-task-definition',                             
            launch_type='FARGATE',                                            
            overrides={                                                       
                'containerOverrides': [                                       
                    {                                                         
                        'name': 'my-container',                               
                        'environment': [                                      
                            {                                                 
                                'name': 'ENVIRONMENT_VARIABLE_A',             
                                'value': 'environment_variable_a_string_value'
                            },                                                
                            {                                                 
                                'name': 'ENVIRONMENT_VARIABLE_B',             
                                'value': '123456789'                          
                            }                                                 
                        ]                                                     
                    }                                                         
                ]                                                             
            },                                                                
            network_configuration=network_config()                            
        )                                                                     
```

github-actions[bot] on (2024-11-25 00:16:33 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-12-02 00:17:16 UTC): This issue has been closed because it has not received response from the issue author.

"
2567486622,issue,closed,completed,Webserver cannot read logs of pods launched by a scheduler of mismatching Airflow version,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

The webserver was unable to fetch the logs of task instance.

```
*** No logs found in GCS; ti=%s <TaskInstance: long_running_tasks_poc_copy.simple_kpo scheduled__2024-10-03T00:00:00+00:00 [running]>
*** Attempting to fetch logs from pod long-running-tasks-poc-copy-simple-kpo-k5v9qx6i through kube API
*** Reading from k8s pod logs failed: ('Cannot find pod for ti %s', <TaskInstance: long_running_tasks_poc_copy.simple_kpo scheduled__2024-10-03T00:00:00+00:00 [running]>)
```

The corresponding Kubernetes worker pod was in the running phase but the weberver could not find the pod in the namespace. It turns out it was because the label selector was not matching for the label `airflow-version`. The pod had the label `airflow-version=2.9.3+astro.4` and the webserver was using the Airflow version `2.9.3+astro.5` which created the label selector `airflow-version=2.9.3+astro.5`.

The reason why the version were different is because the deployment's Airflow version was upgraded from `2.9.3+astro.4` to `2.9.3+astro.5` after the task instance was launched.

---

Closed by #42751

### What you think should happen instead?

The Airflow version should not matter in terms of the webserver's ability to read the logs of a pod.

### How to reproduce

1. Launch a long running task using the KubernetesExecutor
2. Check task logs in webserver (should be visible)
3. Upgrade the deployment to a different version 
4. Check task logs in webserver (should now be missing due to failure to read)

### Operating System

n/a

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",wolfier,2024-10-04 22:23:12+00:00,[],2024-10-05 00:47:23+00:00,2024-10-05 00:47:23+00:00,https://github.com/apache/airflow/issues/42753,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('area:logging', ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2567227925,issue,closed,completed,AIP-84 Migrate DagWarning public endpoint to FastAPI,"### Description

Migrate List dag warnings endpoint

### Use case/motivation

[AIP-84](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API)

### Related issues

Relates to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rawwar,2024-10-04 19:53:26+00:00,['rawwar'],2024-11-04 14:25:37+00:00,2024-11-04 14:25:37+00:00,https://github.com/apache/airflow/issues/42748,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2401160127, 'issue_id': 2567227925, 'author': 'rawwar', 'body': 'While I was working on this issue,  I noticed, legacy API \'s list dag warning endpoint\'s Responses schema shows ""import_errors"" . I think, this should have been DagWarnings.\r\n\r\nhttps://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html#operation/get_dag_warnings\r\n\r\n\r\n@pierrejeambrun , do you think this is a bug?', 'created_at': datetime.datetime(2024, 10, 9, 2, 44, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2402433528, 'issue_id': 2567227925, 'author': 'pierrejeambrun', 'body': 'Yes this is a mistake in the legacy API. The actual response has `dag_warnings`:\r\n\r\n![Screenshot 2024-10-09 at 15 57 15](https://github.com/user-attachments/assets/577685ed-97fc-4d87-a120-4aeb3393da93)\r\n\r\nPR here:\r\nhttps://github.com/apache/airflow/pull/42858', 'created_at': datetime.datetime(2024, 10, 9, 14, 0, 7, tzinfo=datetime.timezone.utc)}]","rawwar (Issue Creator) on (2024-10-09 02:44:43 UTC): While I was working on this issue,  I noticed, legacy API 's list dag warning endpoint's Responses schema shows ""import_errors"" . I think, this should have been DagWarnings.

https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html#operation/get_dag_warnings


@pierrejeambrun , do you think this is a bug?

pierrejeambrun on (2024-10-09 14:00:07 UTC): Yes this is a mistake in the legacy API. The actual response has `dag_warnings`:

![Screenshot 2024-10-09 at 15 57 15](https://github.com/user-attachments/assets/577685ed-97fc-4d87-a120-4aeb3393da93)

PR here:
https://github.com/apache/airflow/pull/42858

"
2566737235,issue,closed,completed,AIP-84 Migrate Config related public endpoint to FastAPI ,"### Description

Migrate the following endpoint:

- [x] Get Current configuration
- [x] Get a option from configuration


### Use case/motivation

[AIP-84](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API)

### Related issues

Relates to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rawwar,2024-10-04 16:25:16+00:00,['jason810496'],2024-11-20 15:30:04+00:00,2024-11-20 15:29:49+00:00,https://github.com/apache/airflow/issues/42745,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2394082690, 'issue_id': 2566737235, 'author': 'rawwar', 'body': 'For Get config,\r\n\r\nhttps://github.com/apache/airflow/blob/b0a18d9019a1a12d58435185b0c86e28af050685/airflow/api_connexion/endpoints/config_endpoint.py#L84\r\n\r\n[Docs](https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html#operation/get_config) does not mention about 406(HTTPStatus.NOT_ACCEPTABLE) status code. Should we include 406 as well?\r\n\r\n---\r\nhttps://github.com/apache/airflow/blob/b0a18d9019a1a12d58435185b0c86e28af050685/airflow/api_connexion/endpoints/config_endpoint.py#L71\r\nAnother question about the response. Do we only return JSON or also consider text response?', 'created_at': datetime.datetime(2024, 10, 4, 16, 38, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2402509173, 'issue_id': 2566737235, 'author': 'pierrejeambrun', 'body': ""I think we should support all the `content-type` that the legacy endpoint was supporting, so yes I would say multiple of them (JSON, text etc.) we need to actually check what should be supported.\r\n\r\nWe can add 406 that's not a problem. Though I would tend to return a general 422 ValidationError, like the client response is wrong because the mimetype suggested is not supported. I'm not really for having a multitute of response code from the API, the 406 one is really singular I would say.\r\n\r\nWe can even add `breaking` to the PR and document that the response code error changed. So it will be documented in airflow 3 breaking changes."", 'created_at': datetime.datetime(2024, 10, 9, 14, 29, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2455336028, 'issue_id': 2566737235, 'author': 'jason810496', 'body': 'Hi @rawwar, I can take this on. Could you please assign the issue to me? Thanks!', 'created_at': datetime.datetime(2024, 11, 4, 17, 44, 54, tzinfo=datetime.timezone.utc)}]","rawwar (Issue Creator) on (2024-10-04 16:38:14 UTC): For Get config,

https://github.com/apache/airflow/blob/b0a18d9019a1a12d58435185b0c86e28af050685/airflow/api_connexion/endpoints/config_endpoint.py#L84

[Docs](https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html#operation/get_config) does not mention about 406(HTTPStatus.NOT_ACCEPTABLE) status code. Should we include 406 as well?

---
https://github.com/apache/airflow/blob/b0a18d9019a1a12d58435185b0c86e28af050685/airflow/api_connexion/endpoints/config_endpoint.py#L71
Another question about the response. Do we only return JSON or also consider text response?

pierrejeambrun on (2024-10-09 14:29:20 UTC): I think we should support all the `content-type` that the legacy endpoint was supporting, so yes I would say multiple of them (JSON, text etc.) we need to actually check what should be supported.

We can add 406 that's not a problem. Though I would tend to return a general 422 ValidationError, like the client response is wrong because the mimetype suggested is not supported. I'm not really for having a multitute of response code from the API, the 406 one is really singular I would say.

We can even add `breaking` to the PR and document that the response code error changed. So it will be documented in airflow 3 breaking changes.

jason810496 (Assginee) on (2024-11-04 17:44:54 UTC): Hi @rawwar, I can take this on. Could you please assign the issue to me? Thanks!

"
2566718573,issue,closed,completed,Revoking stale permissions breaks DAG import,"### Apache Airflow Provider(s)

fab

### Versions of Apache Airflow Providers

apache-airflow-providers-fab==1.4.0

### Apache Airflow version

2.9.3

### Operating System

Oracle Linux Server 8.10

### Deployment

Virtualenv installation

### Deployment details

_No response_

### What happened

In DAG I use the ""old"" format of the access_control parameter. ""DAG Import Errors"" appears in the web interface when revoking stale permissions
```
Traceback(most recent call last):
  File ""/opt/venv/lib64/python3.11/site-packages/airflow/providers/fab/auth_manager/security_manager/override.py"", line 1128, in sync_perm_for_dag
    self._sync_dag_view_permissions(dag_id, access_control.copy())
  File ""/opt/venv/lib64/python3.11/site-packages/airflow/providers/fab/auth_manager/security_manager/override.py"", line 1175, in _sync_dag_view_permissions
    target_perms_for_role = access_control.get(role.name, {}).get(resource_name, set())
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'set' object has no attribute 'get'
```

### What you think should happen instead

According to the [provider's documentation](https://airflow.apache.org/docs/apache-airflow-providers-fab/stable/auth-manager/access-control.html#order-of-precedence-for-dag-level-permissions), it supports the old format, I think its support should be added

### How to reproduce

In DAG use the ""old"" format of the access_control parameter:
`with DAG(
        ...
        access_control={'BASIC_ROLE': {'can_read', 'can_edit', 'can_delete'}, 'BASIC_ROLE_VIEW': {'can_read'}},
        ...
)`

Wait until all permissions are added. Then remove `can_delete` permission, then when revoking obsolete permissions fab crashes and `DAG Import Errors` message appears in web interface

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",RostD,2024-10-04 16:13:03+00:00,[],2024-10-25 19:41:25+00:00,2024-10-25 19:41:25+00:00,https://github.com/apache/airflow/issues/42743,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('provider:fab', '')]","[{'comment_id': 2394040810, 'issue_id': 2566718573, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 4, 16, 13, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2394819462, 'issue_id': 2566718573, 'author': 'potiuk', 'body': 'Can. you take a look @joaopamaral ? It looks legit.', 'created_at': datetime.datetime(2024, 10, 5, 0, 33, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2400964509, 'issue_id': 2566718573, 'author': 'joaopamaral', 'body': 'Sorry for taking too long, I only had time to check it today. Here is the fix https://github.com/apache/airflow/pull/42844', 'created_at': datetime.datetime(2024, 10, 8, 23, 4, 7, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-04 16:13:06 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-10-05 00:33:16 UTC): Can. you take a look @joaopamaral ? It looks legit.

joaopamaral on (2024-10-08 23:04:07 UTC): Sorry for taking too long, I only had time to check it today. Here is the fix https://github.com/apache/airflow/pull/42844

"
2566225039,issue,closed,completed,AIP-38 Add skeleton loading state to table component,Add loading state to table and card views of our list component.,bbovenzi,2024-10-04 12:10:33+00:00,['bbovenzi'],2024-10-08 11:38:30+00:00,2024-10-08 11:38:28+00:00,https://github.com/apache/airflow/issues/42734,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]","[{'comment_id': 2395364003, 'issue_id': 2566225039, 'author': 'tirkarthi', 'body': 'See also https://github.com/TanStack/table/discussions/2386', 'created_at': datetime.datetime(2024, 10, 6, 9, 21, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2396751900, 'issue_id': 2566225039, 'author': 'bbovenzi', 'body': ""> See also [TanStack/table#2386](https://github.com/TanStack/table/discussions/2386)\r\n\r\nThanks. The approach in my PR isn't too different except that we add extra options to the `column.meta` object. Then we only have to write the Skeleton components once."", 'created_at': datetime.datetime(2024, 10, 7, 12, 9, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2399605297, 'issue_id': 2566225039, 'author': 'bbovenzi', 'body': 'Closed in #42711', 'created_at': datetime.datetime(2024, 10, 8, 11, 38, 29, tzinfo=datetime.timezone.utc)}]","tirkarthi on (2024-10-06 09:21:40 UTC): See also https://github.com/TanStack/table/discussions/2386

bbovenzi (Issue Creator) on (2024-10-07 12:09:44 UTC): Thanks. The approach in my PR isn't too different except that we add extra options to the `column.meta` object. Then we only have to write the Skeleton components once.

bbovenzi (Issue Creator) on (2024-10-08 11:38:29 UTC): Closed in #42711

"
2566220839,issue,open,,AIP-38 Sort table by multiple fields,"Right now, we only sort by Dag Id.

We should enable sorting for more fields on the DAGs list and to allow multiple sorts at the same time.",bbovenzi,2024-10-04 12:08:23+00:00,['bbovenzi'],2024-10-04 12:09:22+00:00,,https://github.com/apache/airflow/issues/42732,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]",[],
2566176145,issue,closed,completed,AIP-84 | Add last run details to dags list,"Add the latest dag run to the returned dag in the public get `/dags` endpoint in fastapi.

`dag.latest_dagrun` should be a serialized DagRun or None",bbovenzi,2024-10-04 11:43:34+00:00,['ArshiaZr'],2024-12-03 13:10:20+00:00,2024-11-29 15:48:26+00:00,https://github.com/apache/airflow/issues/42730,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('good first issue', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2394715394, 'issue_id': 2566176145, 'author': 'ArshiaZr', 'body': 'Can you please assign it to me?', 'created_at': datetime.datetime(2024, 10, 4, 21, 49, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2396703048, 'issue_id': 2566176145, 'author': 'bbovenzi', 'body': 'Thanks @ArshiaZr  Instead of listing a few separate columns of the dag run. We should probably just return `dag.last_dagrun` as a dict of the DagRun model.', 'created_at': datetime.datetime(2024, 10, 7, 11, 46, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2414540913, 'issue_id': 2566176145, 'author': 'bbovenzi', 'body': '@ArshiaZr this is the issue to add `last_dagrun` to the fast api dags list. So please feel free to work on it!', 'created_at': datetime.datetime(2024, 10, 15, 16, 54, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2505638383, 'issue_id': 2566176145, 'author': 'pierrejeambrun', 'body': 'Hello @ArshiaZr,\r\n\r\nAre you still working on it? Do you need help to get started?', 'created_at': datetime.datetime(2024, 11, 28, 9, 24, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508063542, 'issue_id': 2566176145, 'author': 'pierrejeambrun', 'body': 'Closing. We are now using the private endpoint for `recent_dag_runs` which has more information.', 'created_at': datetime.datetime(2024, 11, 29, 15, 48, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2514505943, 'issue_id': 2566176145, 'author': 'tirkarthi', 'body': ""The PR was stale and closed. It's probably good to reopen this issue and have the latest dagrun as part of the dag response which is more efficient and useful in pages like dag detail instead of fetching last 14 dagruns and getting the last item.\r\n\r\nEdit : Please ignore my comment. It seems the endpoint has dag_runs_limit to limit the dagruns to latest and also achieve my use case. Thanks."", 'created_at': datetime.datetime(2024, 12, 3, 13, 3, 57, tzinfo=datetime.timezone.utc)}]","ArshiaZr (Assginee) on (2024-10-04 21:49:58 UTC): Can you please assign it to me?

bbovenzi (Issue Creator) on (2024-10-07 11:46:46 UTC): Thanks @ArshiaZr  Instead of listing a few separate columns of the dag run. We should probably just return `dag.last_dagrun` as a dict of the DagRun model.

bbovenzi (Issue Creator) on (2024-10-15 16:54:01 UTC): @ArshiaZr this is the issue to add `last_dagrun` to the fast api dags list. So please feel free to work on it!

pierrejeambrun on (2024-11-28 09:24:58 UTC): Hello @ArshiaZr,

Are you still working on it? Do you need help to get started?

pierrejeambrun on (2024-11-29 15:48:26 UTC): Closing. We are now using the private endpoint for `recent_dag_runs` which has more information.

tirkarthi on (2024-12-03 13:03:57 UTC): The PR was stale and closed. It's probably good to reopen this issue and have the latest dagrun as part of the dag response which is more efficient and useful in pages like dag detail instead of fetching last 14 dagruns and getting the last item.

Edit : Please ignore my comment. It seems the endpoint has dag_runs_limit to limit the dagruns to latest and also achieve my use case. Thanks.

"
2564924451,issue,open,,XCom Search filter on value errors,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.3

### What happened?

An error occurs in AF2.9 when attempting to filter XCom by value

### What you think should happen instead?

_No response_

### How to reproduce

1. Go to Admin > XCom
2. Add a filter for 'value', put in any string
3. Hit search 

Stack trace included:
```
[03/Oct/2024:18:11:34 +0000] ""GET /dus78e67/xcom/list/?_flt_2_value=CQQG28MH9 HTTP/1.1"" 500 1595 ""https://clkvh3b46003m01kbalgwwdcy.astronomer.run/dus78e67/xcom/list/"" ""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
TypeError: can't escape str to binary
    cursor.execute(statement, parameters)
  File ""/usr/local/lib/python3.11/site-packages/sqlalchemy/engine/default.py"", line 736, in do_execute
    self.dialect.do_execute(
  File ""/usr/local/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context
    raise exception
  File ""/usr/local/lib/python3.11/site-packages/sqlalchemy/util/compat.py"", line 211, in raise_
    util.raise_(exc_info[1], with_traceback=exc_info[2])
  File ""/usr/local/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 2138, in _handle_dbapi_exception
    self._handle_dbapi_exception(
  File ""/usr/local/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1953, in _execute_context
          ^^^^^^^^^^^^^^^^^^^^^^
    ret = self._execute_context(
  File ""/usr/local/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1577, in _execute_clauseelement
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    return connection._execute_clauseelement(
  File ""/usr/local/lib/python3.11/site-packages/sqlalchemy/sql/elements.py"", line 334, in _execute_on_connection
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    return meth(self, args_10style, kwargs_10style, execution_options)
  File ""/usr/local/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1710, in _execute_20
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    result = conn._execute_20(statement, params or {}, execution_options)
  File ""/usr/local/lib/python3.11/site-packages/sqlalchemy/orm/session.py"", line 1717, in execute
             ^^^^^^^^^^^^^^^^^^^^^
    result = self.session.execute(
  File ""/usr/local/lib/python3.11/site-packages/sqlalchemy/orm/query.py"", line 2916, in _iter
           ^^^^^^^^^^^^
    return self._iter().one()
  File ""/usr/local/lib/python3.11/site-packages/sqlalchemy/orm/query.py"", line 2870, in one
          ^^^^^^^^^^
    ret = self.one()
  File ""/usr/local/lib/python3.11/site-packages/sqlalchemy/orm/query.py"", line 2893, in scalar
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    return self._from_self(col).enable_eagerloads(False).scalar()
  File ""/usr/local/lib/python3.11/site-packages/sqlalchemy/orm/query.py"", line 3176, in count
      ^^^^^^^
    ).count()
  File ""/usr/local/lib/python3.11/site-packages/flask_appbuilder/models/sqla/interface.py"", line 398, in query_count
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    count = self.query_count(query, filters, select_columns)
  File ""/usr/local/lib/python3.11/site-packages/flask_appbuilder/models/sqla/interface.py"", line 489, in query
                 ^^^^^^^^^^^^^^^^^^^^^
    count, lst = self.datamodel.query(
  File ""/usr/local/lib/python3.11/site-packages/flask_appbuilder/baseviews.py"", line 1078, in _get_list_widget
              ^^^^^^^^^^^^^^^^^^^^^^
    widgets = self._get_list_widget(
  File ""/usr/local/lib/python3.11/site-packages/flask_appbuilder/baseviews.py"", line 1179, in _list
              ^^^^^^^^^^^^
    widgets = self._list()
  File ""/usr/local/lib/python3.11/site-packages/flask_appbuilder/views.py"", line 550, in list
           ^^^^^^^^^^^^^^^^^^^^^^^^
    return f(self, *args, **kwargs)
  File ""/usr/local/lib/python3.11/site-packages/flask_appbuilder/security/decorators.py"", line 137, in wraps
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
  File ""/usr/local/lib/python3.11/site-packages/flask/app.py"", line 1799, in dispatch_request
         ^^^^^^^^^^^^^^^^^^^^^^^
    rv = self.dispatch_request()
  File ""/usr/local/lib/python3.11/site-packages/flask/app.py"", line 1823, in full_dispatch_request
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python3.11/site-packages/flask/app.py"", line 1825, in full_dispatch_request
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python3.11/site-packages/flask/app.py"", line 2529, in wsgi_app
Traceback (most recent call last):
{app.py:1744} ERROR - Exception on /xcom/list/ [GET]
```

### Operating System

Astro

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",fritz-astronomer,2024-10-03 20:13:44+00:00,"['josix', 'wei-juncheng']",2025-02-07 04:51:36+00:00,,https://github.com/apache/airflow/issues/42720,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2392742969, 'issue_id': 2564924451, 'author': 'josix', 'body': 'I can investigate the root cause first, feel free to assign it to me.', 'created_at': datetime.datetime(2024, 10, 4, 3, 58, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2419173701, 'issue_id': 2564924451, 'author': 'josix', 'body': ""The issue occurs in the admin XComs interface when searching by the 'value' field. Any such search fails because when the XCom list view (built with FAB's ModelView) retrieves filter parameters from the URL, these parameters are passed as strings. These parameters then travel through 2-3 classes in the ORM layer. The problem specifically manifests when SQLAlchemy attempts to execute a count query on the XCom table - it fails because the `xcom.value` column is binary, while the filter parameters are strings.\r\n\r\nTo put more detail, I found that SQLAlchemy fails to execute the following query because `value_1` is being passed as a string type when query_count is called in [SQLInterface.query](https://github.com/dpgaspar/Flask-AppBuilder/blob/master/flask_appbuilder/models/sqla/interface.py#L489). (which is the datamodel of XComModelView inherits from and also the interface to retrieve the XComs from db). Since I'm not very familiar with FAB, I'm still working on finding a way to pass the value as a non-string type. Any suggestions would be appreciated.\r\n```\r\nSELECT\r\n  xcom.dag_run_id AS xcom_dag_run_id,\r\n  xcom.task_id AS xcom_task_id,\r\n  xcom.map_index AS xcom_map_index,\r\n  xcom.key AS xcom_key,\r\n  xcom.dag_id AS xcom_dag_id,\r\n  xcom.run_id AS xcom_run_id,\r\n  xcom.value AS xcom_value,\r\n  xcom.timestamp AS xcom_timestamp,\r\n  dag_run_1.state AS dag_run_1_state,\r\n  dag_run_1.id AS dag_run_1_id,\r\n  dag_run_1.dag_id AS dag_run_1_dag_id,\r\n  dag_run_1.queued_at AS dag_run_1_queued_at,\r\n  dag_run_1.execution_date AS dag_run_1_execution_date,\r\n  dag_run_1.start_date AS dag_run_1_start_date,\r\n  dag_run_1.end_date AS dag_run_1_end_date,\r\n  dag_run_1.run_id AS dag_run_1_run_id,\r\n  dag_run_1.creating_job_id AS dag_run_1_creating_job_id,\r\n  dag_run_1.external_trigger AS dag_run_1_external_trigger,\r\n  dag_run_1.run_type AS dag_run_1_run_type,\r\n  dag_run_1.conf AS dag_run_1_conf,\r\n  dag_run_1.data_interval_start AS dag_run_1_data_interval_start,\r\n  dag_run_1.data_interval_end AS dag_run_1_data_interval_end,\r\n  dag_run_1.last_scheduling_decision AS dag_run_1_last_scheduling_decision,\r\n  dag_run_1.dag_hash AS dag_run_1_dag_hash,\r\n  dag_run_1.log_template_id AS dag_run_1_log_template_id,\r\n  dag_run_1.updated_at AS dag_run_1_updated_at,\r\n  dag_run_1.clear_number AS dag_run_1_clear_number\r\nFROM\r\n  xcom\r\n  LEFT OUTER JOIN dag_run AS dag_run_1 ON xcom.dag_run_id = dag_run_1.id\r\nWHERE\r\n  xcom.value ILIKE %(value_1) s\r\n```"", 'created_at': datetime.datetime(2024, 10, 17, 10, 38, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2590757216, 'issue_id': 2564924451, 'author': 'josix', 'body': 'I discovered that the SQL is dynamically generated based on filters provided by FAB (e.g., [FilterContains](https://github.com/dpgaspar/Flask-AppBuilder/blob/master/flask_appbuilder/models/sqla/filters.py#L119)). Therefore, I think we need to create a custom filter converter for `xcom.value` using the `conversion_table` in `AirflowFilterConverter`, similar to what we did in `CustomSQLAInterface`, to ensure the filter query works correctly for the input value.', 'created_at': datetime.datetime(2025, 1, 14, 18, 8, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614263663, 'issue_id': 2564924451, 'author': 'wei-juncheng', 'body': 'Hi @josix \nI can assist with the testing part of this feature. Could you assign the issue to me?', 'created_at': datetime.datetime(2025, 1, 26, 8, 2, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2614885804, 'issue_id': 2564924451, 'author': 'josix', 'body': ""Sure, but I don't have the permissions to do it. @jedcunningham could you please help include @wei-juncheng in the assignee? Thanks!"", 'created_at': datetime.datetime(2025, 1, 27, 5, 37, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2641942540, 'issue_id': 2564924451, 'author': 'josix', 'body': 'Resolved by #46053', 'created_at': datetime.datetime(2025, 2, 7, 4, 51, 35, tzinfo=datetime.timezone.utc)}]","josix (Assginee) on (2024-10-04 03:58:41 UTC): I can investigate the root cause first, feel free to assign it to me.

josix (Assginee) on (2024-10-17 10:38:54 UTC): The issue occurs in the admin XComs interface when searching by the 'value' field. Any such search fails because when the XCom list view (built with FAB's ModelView) retrieves filter parameters from the URL, these parameters are passed as strings. These parameters then travel through 2-3 classes in the ORM layer. The problem specifically manifests when SQLAlchemy attempts to execute a count query on the XCom table - it fails because the `xcom.value` column is binary, while the filter parameters are strings.

To put more detail, I found that SQLAlchemy fails to execute the following query because `value_1` is being passed as a string type when query_count is called in [SQLInterface.query](https://github.com/dpgaspar/Flask-AppBuilder/blob/master/flask_appbuilder/models/sqla/interface.py#L489). (which is the datamodel of XComModelView inherits from and also the interface to retrieve the XComs from db). Since I'm not very familiar with FAB, I'm still working on finding a way to pass the value as a non-string type. Any suggestions would be appreciated.
```
SELECT
  xcom.dag_run_id AS xcom_dag_run_id,
  xcom.task_id AS xcom_task_id,
  xcom.map_index AS xcom_map_index,
  xcom.key AS xcom_key,
  xcom.dag_id AS xcom_dag_id,
  xcom.run_id AS xcom_run_id,
  xcom.value AS xcom_value,
  xcom.timestamp AS xcom_timestamp,
  dag_run_1.state AS dag_run_1_state,
  dag_run_1.id AS dag_run_1_id,
  dag_run_1.dag_id AS dag_run_1_dag_id,
  dag_run_1.queued_at AS dag_run_1_queued_at,
  dag_run_1.execution_date AS dag_run_1_execution_date,
  dag_run_1.start_date AS dag_run_1_start_date,
  dag_run_1.end_date AS dag_run_1_end_date,
  dag_run_1.run_id AS dag_run_1_run_id,
  dag_run_1.creating_job_id AS dag_run_1_creating_job_id,
  dag_run_1.external_trigger AS dag_run_1_external_trigger,
  dag_run_1.run_type AS dag_run_1_run_type,
  dag_run_1.conf AS dag_run_1_conf,
  dag_run_1.data_interval_start AS dag_run_1_data_interval_start,
  dag_run_1.data_interval_end AS dag_run_1_data_interval_end,
  dag_run_1.last_scheduling_decision AS dag_run_1_last_scheduling_decision,
  dag_run_1.dag_hash AS dag_run_1_dag_hash,
  dag_run_1.log_template_id AS dag_run_1_log_template_id,
  dag_run_1.updated_at AS dag_run_1_updated_at,
  dag_run_1.clear_number AS dag_run_1_clear_number
FROM
  xcom
  LEFT OUTER JOIN dag_run AS dag_run_1 ON xcom.dag_run_id = dag_run_1.id
WHERE
  xcom.value ILIKE %(value_1) s
```

josix (Assginee) on (2025-01-14 18:08:24 UTC): I discovered that the SQL is dynamically generated based on filters provided by FAB (e.g., [FilterContains](https://github.com/dpgaspar/Flask-AppBuilder/blob/master/flask_appbuilder/models/sqla/filters.py#L119)). Therefore, I think we need to create a custom filter converter for `xcom.value` using the `conversion_table` in `AirflowFilterConverter`, similar to what we did in `CustomSQLAInterface`, to ensure the filter query works correctly for the input value.

wei-juncheng (Assginee) on (2025-01-26 08:02:28 UTC): Hi @josix 
I can assist with the testing part of this feature. Could you assign the issue to me?

josix (Assginee) on (2025-01-27 05:37:38 UTC): Sure, but I don't have the permissions to do it. @jedcunningham could you please help include @wei-juncheng in the assignee? Thanks!

josix (Assginee) on (2025-02-07 04:51:35 UTC): Resolved by #46053

"
2564717644,issue,closed,completed,Improve startup of K8S tests,"When K8S tests start, sometimes, very rarely, example DAGs are not parsed on time for the first unit tests to run. 

This results in (example https://github.com/apache/airflow/actions/runs/11157751788/job/31013822370#step:9:1497)


```python
_______________ TestKubernetesExecutor.test_integration_run_dag ________________
  
  self = <kubernetes_tests.test_kubernetes_executor.TestKubernetesExecutor object at 0x7f700fbdd100>
  
      @pytest.mark.execution_timeout(300)
      def test_integration_run_dag(self):
          dag_id = ""example_kubernetes_executor""
  >       dag_run_id, execution_date = self.start_job_in_kubernetes(dag_id, self.host)
  
  test_kubernetes_executor.py:31: 
  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
  test_base.py:250: in start_job_in_kubernetes
      result_json = self.start_dag(dag_id=dag_id, host=host)
  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
  
  self = <kubernetes_tests.test_kubernetes_executor.TestKubernetesExecutor object at 0x7f700fbdd100>
  dag_id = 'example_kubernetes_executor', host = 'localhost:27399'
  
      def start_dag(self, dag_id, host):
          patch_string = f""http://{host}/api/v1/dags/{dag_id}""
          print(f""Calling [start_dag]#1 {patch_string}"")
          result = self.session.patch(patch_string, json={""is_paused"": False})
          try:
              result_json = result.json()
          except ValueError:
              result_json = str(result)
          print(f""Received [start_dag]#1 {result_json}"")
  >       assert result.status_code == 200, f""Could not enable DAG: {result_json}""
  E       AssertionError: Could not enable DAG: {'detail': None, 'status': 404, 'title': ""Dag with id: 'example_kubernetes_executor' not found"", 'type': 'http://apache-airflow-docs.s3-website.eu-central-1.amazonaws.com/docs/apache-airflow/stable/stable-rest-api-ref.html#section/Errors/NotFound'}
  E       assert 404 == 200
  E        +  where 404 = <Response [404]>.status_code
```

This can likely be improved but some additional checks before we start k8S tests that DAGs are already parsed and serialized.

",potiuk,2024-10-03 18:27:20+00:00,['gopidesupavan'],2024-11-30 13:20:31+00:00,2024-10-04 01:31:28+00:00,https://github.com/apache/airflow/issues/42718,"[('area:providers', ''), ('good first issue', ''), ('area:system-tests', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]",[],
2564463107,issue,closed,completed,AIP-38 | DAGs List |  Filter by last_dag_run_state,"We now have `last_dag_run_state` in FastAPI's get dags endpoint. Let's use that in the UI. And enable the disabled buttons that are already in the UI:

![Image](https://github.com/user-attachments/assets/f52498a5-4392-4f55-95d3-beeeff3f9e2b)
",bbovenzi,2024-10-03 16:24:30+00:00,['tirkarthi'],2024-10-09 08:15:16+00:00,2024-10-09 08:15:16+00:00,https://github.com/apache/airflow/issues/42715,"[('kind:feature', 'Feature Requests'), ('good first issue', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]","[{'comment_id': 2395304310, 'issue_id': 2564463107, 'author': 'tirkarthi', 'body': 'I am working on a patch for this. Please assign this to me.', 'created_at': datetime.datetime(2024, 10, 6, 5, 50, 17, tzinfo=datetime.timezone.utc)}]","tirkarthi (Assginee) on (2024-10-06 05:50:17 UTC): I am working on a patch for this. Please assign this to me.

"
2564456285,issue,closed,completed,AIP-38 | DAGs List | Search by dag_display_name_pattern,"The public get dags endpoints in FastAPI now supports `dag_display_name_pattern`. We should use that to search DAGs in the UI and activate the disabled search bar that is already there.

We can leave ""Advanced Search"" disabled for now.

See top of page for the search bar:
![Image](https://github.com/user-attachments/assets/368ff14d-20b3-46c8-a4ec-053edf40b429)
",bbovenzi,2024-10-03 16:20:32+00:00,['luyangliuable'],2024-11-25 23:20:15+00:00,2024-10-21 13:25:48+00:00,https://github.com/apache/airflow/issues/42714,"[('kind:feature', 'Feature Requests'), ('good first issue', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]","[{'comment_id': 2425571000, 'issue_id': 2564456285, 'author': 'luyangliuable', 'body': 'Hi I was just wondering is this issue ready to be closed or is there anything more that needs to be fixed :)', 'created_at': datetime.datetime(2024, 10, 21, 4, 42, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2426680190, 'issue_id': 2564456285, 'author': 'bbovenzi', 'body': 'Closed via #42896', 'created_at': datetime.datetime(2024, 10, 21, 13, 25, 48, tzinfo=datetime.timezone.utc)}]","luyangliuable (Assginee) on (2024-10-21 04:42:30 UTC): Hi I was just wondering is this issue ready to be closed or is there anything more that needs to be fixed :)

bbovenzi (Issue Creator) on (2024-10-21 13:25:48 UTC): Closed via #42896

"
2564447948,issue,closed,completed,AIP-84 | Public list tags endpoint,"In the public API, we should have a get `/dag/tags` endpoint that return a list of all tags across all DAGs. We can then use this in the UI to filter the dags list by tag #42712


Views.py code to migrate: https://github.com/apache/airflow//blob/main/airflow/www/views.py#L1035",bbovenzi,2024-10-03 16:15:46+00:00,['jason810496'],2024-10-21 09:23:15+00:00,2024-10-21 09:23:14+00:00,https://github.com/apache/airflow/issues/42713,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2405732713, 'issue_id': 2564447948, 'author': 'jason810496', 'body': ""Hi, I'm interested in working on this issue. Could you please assign this ticket to me? Thanks !"", 'created_at': datetime.datetime(2024, 10, 10, 18, 1, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2406845128, 'issue_id': 2564447948, 'author': 'bbovenzi', 'body': '@jason810496 all yours!', 'created_at': datetime.datetime(2024, 10, 11, 8, 10, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408836571, 'issue_id': 2564447948, 'author': 'rawwar', 'body': '@bbovenzi, Can we also add support for the offset, limit?', 'created_at': datetime.datetime(2024, 10, 13, 5, 56, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2414256872, 'issue_id': 2564447948, 'author': 'bbovenzi', 'body': 'Yes. And then we should have a `tag_name_pattern` search param too.', 'created_at': datetime.datetime(2024, 10, 15, 15, 16, 51, tzinfo=datetime.timezone.utc)}]","jason810496 (Assginee) on (2024-10-10 18:01:42 UTC): Hi, I'm interested in working on this issue. Could you please assign this ticket to me? Thanks !

bbovenzi (Issue Creator) on (2024-10-11 08:10:04 UTC): @jason810496 all yours!

rawwar on (2024-10-13 05:56:17 UTC): @bbovenzi, Can we also add support for the offset, limit?

bbovenzi (Issue Creator) on (2024-10-15 15:16:51 UTC): Yes. And then we should have a `tag_name_pattern` search param too.

"
2564444207,issue,closed,completed,AIP-38 | DAGs List | Filter by tags,"We should add the ability to search for and filter the Dags list by tag.

First, we need an endpoint to list all tags: #42713
Then we should use `chakra-react-select` to create a multiselect of tags.",bbovenzi,2024-10-03 16:13:38+00:00,['josix'],2024-10-23 20:05:02+00:00,2024-10-23 20:05:02+00:00,https://github.com/apache/airflow/issues/42712,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]","[{'comment_id': 2395820602, 'issue_id': 2564444207, 'author': 'josix', 'body': ""I'm interested in this, may I get this ticket? Thanks!"", 'created_at': datetime.datetime(2024, 10, 7, 3, 26, 41, tzinfo=datetime.timezone.utc)}]","josix (Assginee) on (2024-10-07 03:26:41 UTC): I'm interested in this, may I get this ticket? Thanks!

"
2564005866,issue,open,,Create iframe UI plugins,"Simpler than react components for plugins, we allow users to just render a webpage inside of an iframe.

This will take the same spec as react components as specified in https://github.com/apache/airflow/issues/42702 but the `ui_view.type` needs to be `""iframe""` and an `iframe_link` needs to be specified.

Then a `GET /plugins` endpoint on FastAPI will need to read from the plugins manager and send the plugin spec to the UI. The UI can then render the url in an iframe.

Later on, we will need to add support passing props to the iframe via url parameters to load the right page.",bbovenzi,2024-10-03 13:00:17+00:00,[],2024-10-03 13:07:31+00:00,,https://github.com/apache/airflow/issues/42708,"[('area:plugins', ''), ('area:API', ""Airflow's REST/HTTP API""), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-84', 'Modern Rest API'), ('AIP-68', 'Extended Plugin Interface for React Views')]",[],
2563983434,issue,closed,completed,AIP-68 | Migrate plugin menu_items to FastAPI and new UI,"- Rename `appbuilder_menu_items` to `menu_items`
- Add a `GET /plugins` endpoint to FastAPI that looks up the menu items and sends them as an array to the UI
- Have the UI consume the `GET /plugins` endpoint and render the menu items links just like how the DocsButton component works: https://github.com/apache/airflow/blob/main/airflow/ui/src/layouts/Nav/DocsButton.tsx",bbovenzi,2024-10-03 12:50:06+00:00,['Brijeshthummar02'],2024-10-18 15:42:43+00:00,2024-10-18 15:42:43+00:00,https://github.com/apache/airflow/issues/42706,"[('area:plugins', ''), ('area:API', ""Airflow's REST/HTTP API""), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-84', 'Modern Rest API'), ('AIP-68', 'Extended Plugin Interface for React Views')]","[{'comment_id': 2391637703, 'issue_id': 2563983434, 'author': 'Brijeshthummar02', 'body': '@bbovenzi i can fix this, assign me the task.', 'created_at': datetime.datetime(2024, 10, 3, 14, 56, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2391800829, 'issue_id': 2563983434, 'author': 'bbovenzi', 'body': '@Brijeshthummar02 assigned! Let me know if you need help with any part here.', 'created_at': datetime.datetime(2024, 10, 3, 16, 8, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2391852421, 'issue_id': 2563983434, 'author': 'Brijeshthummar02', 'body': 'Guide me for 2nd and 3rd point,can you clarify in brief with the path.', 'created_at': datetime.datetime(2024, 10, 3, 16, 35, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2393511050, 'issue_id': 2563983434, 'author': 'bbovenzi', 'body': '1. just rename the plugin menu_items property\n2. create a new GET `/plugins` endpoint in the new FastAPI that will return a list of all plugin menu_items. You can the PRs tagged here to see how to build endpoints for FastAPI: https://github.com/apache/airflow/issues/42370\n3. Once there is an endpoint in FastAPI, we autogenerate all the types and queries. Then you call the function, something like `usePluginServiceGetPlugins()`, inside the Nav component to render the MenuItems. Probably reusing the code from the DocsButton component.', 'created_at': datetime.datetime(2024, 10, 4, 11, 41, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2393922686, 'issue_id': 2563983434, 'author': 'Brijeshthummar02', 'body': 'can you provide the path of file for the first point.', 'created_at': datetime.datetime(2024, 10, 4, 15, 7, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2396928801, 'issue_id': 2563983434, 'author': 'bbovenzi', 'body': ""Actually, let's focus on point 2. Renaming can be handled later."", 'created_at': datetime.datetime(2024, 10, 7, 13, 24, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2422753212, 'issue_id': 2563983434, 'author': 'bbovenzi', 'body': 'Actually we have the menu items provided in #43125.', 'created_at': datetime.datetime(2024, 10, 18, 15, 39, 19, tzinfo=datetime.timezone.utc)}]","Brijeshthummar02 (Assginee) on (2024-10-03 14:56:16 UTC): @bbovenzi i can fix this, assign me the task.

bbovenzi (Issue Creator) on (2024-10-03 16:08:16 UTC): @Brijeshthummar02 assigned! Let me know if you need help with any part here.

Brijeshthummar02 (Assginee) on (2024-10-03 16:35:30 UTC): Guide me for 2nd and 3rd point,can you clarify in brief with the path.

bbovenzi (Issue Creator) on (2024-10-04 11:41:41 UTC): 1. just rename the plugin menu_items property
2. create a new GET `/plugins` endpoint in the new FastAPI that will return a list of all plugin menu_items. You can the PRs tagged here to see how to build endpoints for FastAPI: https://github.com/apache/airflow/issues/42370
3. Once there is an endpoint in FastAPI, we autogenerate all the types and queries. Then you call the function, something like `usePluginServiceGetPlugins()`, inside the Nav component to render the MenuItems. Probably reusing the code from the DocsButton component.

Brijeshthummar02 (Assginee) on (2024-10-04 15:07:07 UTC): can you provide the path of file for the first point.

bbovenzi (Issue Creator) on (2024-10-07 13:24:42 UTC): Actually, let's focus on point 2. Renaming can be handled later.

bbovenzi (Issue Creator) on (2024-10-18 15:39:19 UTC): Actually we have the menu items provided in #43125.

"
2563885140,issue,closed,completed,DAG priority parsing request is not inserted correctly on Postgres,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

https://github.com/apache/airflow/blob/9ec21405c0abe23f6de2e0acd704faa6631fbe76/airflow/assets/manager.py#L256-L276

We use a faster SQL on Postgres to insert DagPriorityParsingRequest rows. From what I can see though the logic seems wrong? The `slow` version inserts one DPPR for each fileloc, but the Postgres version only inserts _one_ row (at most, it can insert none on conflict, but that’s besides the point).

```pycon
>>> file_locs = [""a"", ""b"", ""c""]  # Mock data.
>>> {""fileloc"": fileloc for fileloc in file_locs})
{'fileloc': 'c'}
```

This can’t be right… but there’s no bug reports on this from what I can tell.

### What you think should happen instead?

The line should probably be

```python
session.execute(stmt, ({""fileloc"": fileloc} for fileloc in file_locs))
```

instead.

### How to reproduce

I was just reading the code. Didn’t actually have any behavioural observations.

### Operating System

Any

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",uranusjr,2024-10-03 12:06:17+00:00,['Lee-W'],2024-12-26 00:42:14+00:00,2024-12-26 00:42:14+00:00,https://github.com/apache/airflow/issues/42704,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:datasets', 'Issues related to the datasets feature'), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2562042323, 'issue_id': 2563885140, 'author': 'Lee-W', 'body': 'The PRs have been merged. Closing this issue', 'created_at': datetime.datetime(2024, 12, 26, 0, 42, 14, tzinfo=datetime.timezone.utc)}]","Lee-W (Assginee) on (2024-12-26 00:42:14 UTC): The PRs have been merged. Closing this issue

"
2563618827,issue,open,,AIP-68 | Develop new UI plugins,"In an Airflow plugin `appbuilder_views` will be deprecated. They need to be replaced with `react_views`. 

Fields to include when registering `ui_views`
```
{
  name: string, // plugin name
  type: 'react' | 'iframe', // is the plugin a react component or an iframe?
  title: string, // display name to show on buttons
  icon: string, // (optional) path to an icon file to display next to the title
  url_route: string, // (optional) what to display in the url route, defaults to the plugin name
  iframe_link: string // url of the page to load in an iframe
  src: string, // path to the react component as a built javascript bundle file
  location: string[] // where in the UI the component can live:
    // ""nav"" add as a top-level nav button
    // ""dag"" add as a tab in the DAG details view
    // ""dag_run"" add as a tab in the DAG details view but only for dag runs
    // ""task"" add as a tab in the DAG details view but only for that task instance
    // ""dashboard"" add a panel on the dashboard homepage
    // ""wildcard"" add to the BaseLayout component so that it can appear on any/all pages
  conditions: {key: value } // (optional) object of key/value pairs to match against the host page to determine if the plugin should mount (ex: only render for a certain operator or asset type)
  props: string[ ] // (optional) keys of properties to pass to the react component or iframe 
}
```

The plugins manager then needs to add the icon and component static files to be hosted by FastAPI.
We can dynamically lazy load the components with `lazy(() => import(/* @vite-ignore */ plugin.src)`
FastAPI will need a `GET /plugins` component to send all react-views data to the UI.


Other issues: 

- Rename `appbuilder_menu_items` to `menu_items` and send the array to the UI to render in the main nav page #42706
- Add support for plugins that are simply urls to render inside of an iframe, saving developers the hassle of building a custom react component just to render a 3rd part website anyway #42708
- Add a CLI command to build all the boilerplate code for a react component project so plugin developers don't need to worry about project setup
- Develop an API for what props a react plugin component can expect from the UI depending on its location. 

Other questions:
- Should we still have a webserver run when the FAB provider exists in order to host legacy plugins and render them as iframes?
- What props should we specify to pass to either react components or to iframe plugins?


",bbovenzi,2024-10-03 09:58:42+00:00,['bbovenzi'],2024-10-11 09:22:54+00:00,,https://github.com/apache/airflow/issues/42702,"[('kind:meta', 'High-level information important to the community'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-68', 'Extended Plugin Interface for React Views')]","[{'comment_id': 2407002756, 'issue_id': 2563618827, 'author': 'bbovenzi', 'body': 'POC of a react plugin on the Nav level:\n\n![Image](https://github.com/user-attachments/assets/c1ae8d99-8ea0-4a57-95b2-7bc44221767d)\n\n\nI also used https://bolt.new/ as an AI to generate frontend code. I wonder if we can suggest similar tools to help developers who might not know frontend technologies well.', 'created_at': datetime.datetime(2024, 10, 11, 9, 22, 52, tzinfo=datetime.timezone.utc)}]","bbovenzi (Issue Creator) on (2024-10-11 09:22:52 UTC): POC of a react plugin on the Nav level:

![Image](https://github.com/user-attachments/assets/c1ae8d99-8ea0-4a57-95b2-7bc44221767d)


I also used https://bolt.new/ as an AI to generate frontend code. I wonder if we can suggest similar tools to help developers who might not know frontend technologies well.

"
2563596910,issue,closed,completed,AIP-84 Migrate Dag Run related public endpoint to FastAPI ,"### Description

Migrate following endpoint 

- [x] List DAG runs - #43506
- [x] Trigger a new DAG run - #43875
- [x] List DAG runs(batch) - #44170
- [x] Get a DAG run - #42725 
- [x] Delete a DAG run - #42910
- [x] Modify a DAG run - #42973
- [x] Clear a DAG run - #42975
- [x] Get ~~dataset~~ asset events for a DAG run - ~~#43876~~ #43874
- [x] ~~Update the DAGRun note - #42974~~ 
- [x] update dag run note in PATCH dag_run endpoint - #43508

### Use case/motivation

[AIP-84](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API)

### Related issues

Relates to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rawwar,2024-10-03 09:47:50+00:00,['rawwar'],2024-11-27 09:25:06+00:00,2024-11-27 07:45:00+00:00,https://github.com/apache/airflow/issues/42701,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2459785818, 'issue_id': 2563596910, 'author': 'bbovenzi', 'body': 'Important to note. For triggering a dagrun, the api_connexion endpoint is missing a lot of the form fields that @jscheffl added to [views.py](https://github.com/apache/airflow/blob/main/airflow/www/views.py#L2005). I think if its best if we consolidate to a single public trigger dag run endpoint in FastAPI.', 'created_at': datetime.datetime(2024, 11, 6, 13, 41, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2460444004, 'issue_id': 2563596910, 'author': 'jscheffl', 'body': '> Important to note. For triggering a dagrun, the api_connexion endpoint is missing a lot of the form fields that @jscheffl added to [views.py](https://github.com/apache/airflow/blob/main/airflow/www/views.py#L2005). I think if its best if we consolidate to a single public trigger dag run endpoint in FastAPI.\r\n\r\nI was always looking at `/api/v1/ui/#/DAG/get_dag_details` (airflow/api_connexion/endpoints/dag_endpoint.py:get_dag_details()` which contains a dict of all params definitions. I _think_ this should contain all needed information to generate the form views.\r\n\r\nHave not checked all default but looking at the fast API I feel like this is the same, have not checked for alle details of complete-ness. But this could be added-in there... I _think_ --> http://localhost:29091/docs#/DAG/get_dag_details (Try it with `example_params_ui_tutorial` and take a look to `params` in the response.\r\n\r\nAnd for the triggering after the form is rendered... I assume a ""plain dict"" just need to be passed as `conf`to the trigger DAG run endpoint like today... so in my view the public API _should_ be sufficient.\r\n\r\nFYI @shubhamraj-git', 'created_at': datetime.datetime(2024, 11, 6, 18, 3, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2503134206, 'issue_id': 2563596910, 'author': 'rawwar', 'body': 'Thank you so much for your help @pierrejeambrun , @bugraoz93 , @omkar-foss . I learned a lot while contributing to this endpoint.', 'created_at': datetime.datetime(2024, 11, 27, 7, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2503353974, 'issue_id': 2563596910, 'author': 'pierrejeambrun', 'body': 'Thanks @rawwar for you contributions :)', 'created_at': datetime.datetime(2024, 11, 27, 9, 25, 4, tzinfo=datetime.timezone.utc)}]","bbovenzi on (2024-11-06 13:41:17 UTC): Important to note. For triggering a dagrun, the api_connexion endpoint is missing a lot of the form fields that @jscheffl added to [views.py](https://github.com/apache/airflow/blob/main/airflow/www/views.py#L2005). I think if its best if we consolidate to a single public trigger dag run endpoint in FastAPI.

jscheffl on (2024-11-06 18:03:29 UTC): I was always looking at `/api/v1/ui/#/DAG/get_dag_details` (airflow/api_connexion/endpoints/dag_endpoint.py:get_dag_details()` which contains a dict of all params definitions. I _think_ this should contain all needed information to generate the form views.

Have not checked all default but looking at the fast API I feel like this is the same, have not checked for alle details of complete-ness. But this could be added-in there... I _think_ --> http://localhost:29091/docs#/DAG/get_dag_details (Try it with `example_params_ui_tutorial` and take a look to `params` in the response.

And for the triggering after the form is rendered... I assume a ""plain dict"" just need to be passed as `conf`to the trigger DAG run endpoint like today... so in my view the public API _should_ be sufficient.

FYI @shubhamraj-git

rawwar (Issue Creator) on (2024-11-27 07:45:00 UTC): Thank you so much for your help @pierrejeambrun , @bugraoz93 , @omkar-foss . I learned a lot while contributing to this endpoint.

pierrejeambrun on (2024-11-27 09:25:04 UTC): Thanks @rawwar for you contributions :)

"
2563592738,issue,closed,completed,AIP-38 | Show historic metrics in Dashboard,"Once https://github.com/apache/airflow/pull/42629 is merged, we can initialize our new homepage dashboard.

Right now `historical_metrics` only includes total counts. So we can build this part of the Dashboard mockups except for the bar graphs:

![Image](https://github.com/user-attachments/assets/9a99857f-9d43-4d6e-84c3-24d8bc797ca5)

That includes:
- a date range selector
- horizontal bar graphs showing the total count of dag run and task instance states and their overall percentage

We'll ignore dag run type for now.
",bbovenzi,2024-10-03 09:45:46+00:00,['tirkarthi'],2025-01-31 18:09:49+00:00,2025-01-31 18:09:49+00:00,https://github.com/apache/airflow/issues/42700,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2466180381, 'issue_id': 2563592738, 'author': 'tirkarthi', 'body': ""@bbovenzi I can take this up if you are okay. I have prototyped below versions as per design without the checkbox and the bar charts which don't have data from API. I can raise this as PR for the base part and fine tune the UI/UX during review or subsequent PRs.\r\n\r\n![image](https://github.com/user-attachments/assets/2e814d6f-fe7a-49ef-9322-cc361090906e)\r\n\r\n![image](https://github.com/user-attachments/assets/4414e9bf-e481-4e9d-a0d8-1e325c5e7e73)"", 'created_at': datetime.datetime(2024, 11, 9, 11, 28, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2468465492, 'issue_id': 2563592738, 'author': 'bbovenzi', 'body': 'Nice! One nit, let\'s keep the phrase ""Task Instances"". \r\nWe\'ll probably want to update the historical_metrics endpoint to better fit this view. But that\'s a separate issue', 'created_at': datetime.datetime(2024, 11, 11, 15, 38, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2543195145, 'issue_id': 2563592738, 'author': 'tirkarthi', 'body': '@bbovenzi I am building the asset events section in the dashboard. As part of the dashboard designs the asset events was also to the right of historical metrics so my question is do they belong to the same section and are affected by the time range filter selected? If I select last 1 hour does historical metrics along with the asset events display the last 1 hour metrics only?\r\n\r\nhttps://github.com/apache/airflow/issues/42371#issuecomment-2390830661', 'created_at': datetime.datetime(2024, 12, 14, 17, 31, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2548917356, 'issue_id': 2563592738, 'author': 'bbovenzi', 'body': ""Yeah, I guess they would be. I'll be honest though, I think the dashboard view is subject to a lot of change from what the designs are now and what we will have when 3.0 comes out."", 'created_at': datetime.datetime(2024, 12, 17, 16, 16, 6, tzinfo=datetime.timezone.utc)}]","tirkarthi (Assginee) on (2024-11-09 11:28:58 UTC): @bbovenzi I can take this up if you are okay. I have prototyped below versions as per design without the checkbox and the bar charts which don't have data from API. I can raise this as PR for the base part and fine tune the UI/UX during review or subsequent PRs.

![image](https://github.com/user-attachments/assets/2e814d6f-fe7a-49ef-9322-cc361090906e)

![image](https://github.com/user-attachments/assets/4414e9bf-e481-4e9d-a0d8-1e325c5e7e73)

bbovenzi (Issue Creator) on (2024-11-11 15:38:09 UTC): Nice! One nit, let's keep the phrase ""Task Instances"". 
We'll probably want to update the historical_metrics endpoint to better fit this view. But that's a separate issue

tirkarthi (Assginee) on (2024-12-14 17:31:53 UTC): @bbovenzi I am building the asset events section in the dashboard. As part of the dashboard designs the asset events was also to the right of historical metrics so my question is do they belong to the same section and are affected by the time range filter selected? If I select last 1 hour does historical metrics along with the asset events display the last 1 hour metrics only?

https://github.com/apache/airflow/issues/42371#issuecomment-2390830661

bbovenzi (Issue Creator) on (2024-12-17 16:16:06 UTC): Yeah, I guess they would be. I'll be honest though, I think the dashboard view is subject to a lot of change from what the designs are now and what we will have when 3.0 comes out.

"
2563575885,issue,open,,Add advanced search input + dropdown for table views,"Create a search input to filter a list view.

1. Accepts a simple string for a default search (ex: for the dags list, search for dag_display_names that match the search string)
2. Add an Advanced button which adds a dropdown to add any supported filters for the list of data
3. Filling out the advanced filters dropdown generates an advanced query string which a user can also type (think power search features in a search engine)

![Image](https://github.com/user-attachments/assets/ff28f38d-0069-4aac-98bc-a1e7b3a84640)
",bbovenzi,2024-10-03 09:37:31+00:00,[],2024-11-25 08:00:19+00:00,,https://github.com/apache/airflow/issues/42699,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2425578500, 'issue_id': 2563575885, 'author': 'luyangliuable', 'body': 'Mind if I take this one on as well.', 'created_at': datetime.datetime(2024, 10, 21, 4, 49, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2426789612, 'issue_id': 2563575885, 'author': 'bbovenzi', 'body': 'We left this in ""To Scope"" because @pierrejeambrun and I need to figure out the syntax of the power user search strings and how it combines with FastAPI. \r\nI\'m adding extra issues to the project board today for you to pick up if you want.', 'created_at': datetime.datetime(2024, 10, 21, 14, 6, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2430488954, 'issue_id': 2563575885, 'author': 'luyangliuable', 'body': 'Thanks i am happy to contribute more to this.', 'created_at': datetime.datetime(2024, 10, 22, 23, 16, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2458016662, 'issue_id': 2563575885, 'author': 'bbovenzi', 'body': 'I appreciate the enthusiasm. I just created a new meta issue to track UI tasks. https://github.com/apache/airflow/issues/43712\r\n\r\nFeel free to pick up any open issue!', 'created_at': datetime.datetime(2024, 11, 5, 19, 42, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2497148713, 'issue_id': 2563575885, 'author': 'hterik', 'body': 'If the search is being rehauled, can i suggest to make the filters **optimized for text first**, rather than visual dropdown menus and query-builders. With support for AND and OR operators.\r\n\r\nComparing with the situation of the current search, if i have to make a simple query like `state not in [""queued"", ""running""]`, every operator involves 6 mouse clicks + scrolling in the dropdown to find the essential `Not equal to`-operator + typing the text. Meaning a total of at least 14 mouse operations in different locations. It\'s very slow and painful to use. \r\n> Old UI example\r\n>  ![image](https://github.com/user-attachments/assets/efd15a72-3d17-4768-af48-d0e9b3df0ed3)\r\n\r\n\r\nThe new mockup looks a lot nicer for sure, but IMO the the mode of input is a more important improvement here, not the visuals.\r\n\r\nText is a lot easier to type, copy and edit. With AND/OR operators, much more powerful queries can be constructed and with text, the order of precedence or changing the order of precedence with parenthesis become easier and more obvious. \r\n\r\nTo ease for newcomers, all the supported operations and their possible values can be listed under the query-box with examples.', 'created_at': datetime.datetime(2024, 11, 25, 8, 0, 17, tzinfo=datetime.timezone.utc)}]","luyangliuable on (2024-10-21 04:49:34 UTC): Mind if I take this one on as well.

bbovenzi (Issue Creator) on (2024-10-21 14:06:39 UTC): We left this in ""To Scope"" because @pierrejeambrun and I need to figure out the syntax of the power user search strings and how it combines with FastAPI. 
I'm adding extra issues to the project board today for you to pick up if you want.

luyangliuable on (2024-10-22 23:16:05 UTC): Thanks i am happy to contribute more to this.

bbovenzi (Issue Creator) on (2024-11-05 19:42:15 UTC): I appreciate the enthusiasm. I just created a new meta issue to track UI tasks. https://github.com/apache/airflow/issues/43712

Feel free to pick up any open issue!

hterik on (2024-11-25 08:00:17 UTC): If the search is being rehauled, can i suggest to make the filters **optimized for text first**, rather than visual dropdown menus and query-builders. With support for AND and OR operators.

Comparing with the situation of the current search, if i have to make a simple query like `state not in [""queued"", ""running""]`, every operator involves 6 mouse clicks + scrolling in the dropdown to find the essential `Not equal to`-operator + typing the text. Meaning a total of at least 14 mouse operations in different locations. It's very slow and painful to use. 


The new mockup looks a lot nicer for sure, but IMO the the mode of input is a more important improvement here, not the visuals.

Text is a lot easier to type, copy and edit. With AND/OR operators, much more powerful queries can be constructed and with text, the order of precedence or changing the order of precedence with parenthesis become easier and more obvious. 

To ease for newcomers, all the supported operations and their possible values can be listed under the query-box with examples.

"
2563567257,issue,closed,completed,AIP-38 | DAGs List | Add a card list view to table component,"In the old UI https://github.com/apache/airflow/blob/main/airflow/www/static/js/components/Table/CardList.tsx, we made a basic card list to show Dataset Events.

We should implement a version of this in the new UI to allow any table view to specify its own card component and allow a user to switch between a card view and table view. See https://github.com/apache/airflow/issues/42371#issuecomment-2390834444 for a design mockup

![Image](https://github.com/user-attachments/assets/33346468-78a4-485f-a191-ff183e45800f)",bbovenzi,2024-10-03 09:33:21+00:00,['bbovenzi'],2024-10-08 10:08:14+00:00,2024-10-08 10:08:14+00:00,https://github.com/apache/airflow/issues/42698,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]",[],
2563423041,issue,closed,completed,Add Plugin support to FastAPI,The new FastAPI should also support user-built plugins for custom endpoints. This will replace the `flask_blueprints` part of the Airflow Plugin. We can use FastAPI's subapp: https://fastapi.tiangolo.com/advanced/sub-applications/,bbovenzi,2024-10-03 08:21:24+00:00,['pierrejeambrun'],2024-10-04 19:12:52+00:00,2024-10-04 19:12:52+00:00,https://github.com/apache/airflow/issues/42696,"[('area:plugins', ''), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]",[],
2563415202,issue,closed,completed,Host new UI from FastAPI,"In order to get the new UI project started quickly. We added a Flask Blueprint in the webserver to host the static files. This was only temporary and comes with a few problems:
1. UI doesn't need to talk to the webserver at all
2. CORS issues with the FastAPI and UI being at separate origins
3. Manual effort to register what routes are allowed in the new UI

Instead, FastAPI should host the new UI files and then we don't even need the webserver to run in order for the new UI and API to work.",bbovenzi,2024-10-03 08:17:19+00:00,['pierrejeambrun'],2024-10-03 16:50:31+00:00,2024-10-03 16:50:31+00:00,https://github.com/apache/airflow/issues/42695,"[('area:API', ""Airflow's REST/HTTP API""), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-84', 'Modern Rest API')]",[],
2562649117,issue,open,,XCom and Log views broken for DAG runs with `/` in run id,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.0

### What happened?

The UI does not work for Xcom and Log views for DAG runs with `/` in the run id. There are 404s from the page for API calls like `https://<airflow>/api/v1/dags/<dag id>/dagRuns/<run id>`. The `<run id>` is not url encoded so a slash in there certainly screws up this API call.

### What you think should happen instead?

_No response_

### How to reproduce

Create a DAG run with a `/` character in it and try to view the logs in the UI.

### Operating System

Linux (Ubuntu 22.04)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jkramer-ginkgo,2024-10-02 21:13:17+00:00,['josix'],2025-01-23 12:59:45+00:00,,https://github.com/apache/airflow/issues/42679,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2389745053, 'issue_id': 2562649117, 'author': 'jscheffl', 'body': ""As we have excluded the `/` from the standard run-id pattern in config `allowed_run_id_pattern` this might be a un-tested side effect.\r\nSo probably I'd recommend to limit and change your run-ids to use safe characters. As we are only maintaining the 2.10 version until a totally new UI is implemented, I assume it needs a volunteer (or your?) contribution to have a fix."", 'created_at': datetime.datetime(2024, 10, 2, 21, 43, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2390706450, 'issue_id': 2562649117, 'author': 'josix', 'body': '🙋🏻\u200d♂️ I can take this issue', 'created_at': datetime.datetime(2024, 10, 3, 7, 24, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2392724474, 'issue_id': 2562649117, 'author': 'josix', 'body': ""Hi @jkramer-ginkgo, could you elaborate on how you triggered a DAG run with a '/' in its id? I tried to reproduce it, but Airflow seems to have a verification process in place. It shows that a dagrun ID containing a slash, created via [dag.create_dagrun](https://github.com/apache/airflow/blob/main/airflow/models/dag.py#L2606), is prohibited"", 'created_at': datetime.datetime(2024, 10, 4, 3, 31, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2393710835, 'issue_id': 2562649117, 'author': 'jkramer-ginkgo', 'body': ""@josix We have `allowed_run_id_pattern = .*`. Given it's a user config, I imagine it should be assumed URL-escapable chars can be expected in the run ids."", 'created_at': datetime.datetime(2024, 10, 4, 13, 28, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2393715239, 'issue_id': 2562649117, 'author': 'jkramer-ginkgo', 'body': 'I appreciate you taking a look into this', 'created_at': datetime.datetime(2024, 10, 4, 13, 30, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2516674491, 'issue_id': 2562649117, 'author': 'Paniraj2010', 'body': '@josix we have upgraded from airflow 2.9.3 to 2.10.3, we have similar issue. where we dont see logs on any task. We getting error. ""No task logs found. Try the Event Log tab for more context.""\r\n\r\nMay I know is this resolved?', 'created_at': datetime.datetime(2024, 12, 4, 9, 27, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2517663669, 'issue_id': 2562649117, 'author': 'Paniraj2010', 'body': '> @josix we have upgraded from airflow 2.9.3 to 2.10.3, we have similar issue. where we dont see logs on any task. We getting error. ""No task logs found. Try the Event Log tab for more context.""\r\n> \r\n> May I know is this resolved?\r\n\r\nbotocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden', 'created_at': datetime.datetime(2024, 12, 4, 14, 53, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2609615318, 'issue_id': 2562649117, 'author': 'eladkal', 'body': '@josix are you still working on this issue?', 'created_at': datetime.datetime(2025, 1, 23, 11, 52, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2609749159, 'issue_id': 2562649117, 'author': 'josix', 'body': 'Oh yes, I’ve found some potential root causes. Let me provide the details here.', 'created_at': datetime.datetime(2025, 1, 23, 12, 59, 44, tzinfo=datetime.timezone.utc)}]","jscheffl on (2024-10-02 21:43:10 UTC): As we have excluded the `/` from the standard run-id pattern in config `allowed_run_id_pattern` this might be a un-tested side effect.
So probably I'd recommend to limit and change your run-ids to use safe characters. As we are only maintaining the 2.10 version until a totally new UI is implemented, I assume it needs a volunteer (or your?) contribution to have a fix.

josix (Assginee) on (2024-10-03 07:24:05 UTC): 🙋🏻‍♂️ I can take this issue

josix (Assginee) on (2024-10-04 03:31:01 UTC): Hi @jkramer-ginkgo, could you elaborate on how you triggered a DAG run with a '/' in its id? I tried to reproduce it, but Airflow seems to have a verification process in place. It shows that a dagrun ID containing a slash, created via [dag.create_dagrun](https://github.com/apache/airflow/blob/main/airflow/models/dag.py#L2606), is prohibited

jkramer-ginkgo (Issue Creator) on (2024-10-04 13:28:17 UTC): @josix We have `allowed_run_id_pattern = .*`. Given it's a user config, I imagine it should be assumed URL-escapable chars can be expected in the run ids.

jkramer-ginkgo (Issue Creator) on (2024-10-04 13:30:30 UTC): I appreciate you taking a look into this

Paniraj2010 on (2024-12-04 09:27:28 UTC): @josix we have upgraded from airflow 2.9.3 to 2.10.3, we have similar issue. where we dont see logs on any task. We getting error. ""No task logs found. Try the Event Log tab for more context.""

May I know is this resolved?

Paniraj2010 on (2024-12-04 14:53:10 UTC): botocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden

eladkal on (2025-01-23 11:52:43 UTC): @josix are you still working on this issue?

josix (Assginee) on (2025-01-23 12:59:44 UTC): Oh yes, I’ve found some potential root causes. Let me provide the details here.

"
2561768447,issue,closed,completed,"In MsSqlHook, SQLAlchemy engine scheme is overriden by the change in #40669","### Apache Airflow Provider(s)

common-sql

### Versions of Apache Airflow Providers

apache-airflow-providers-common-sql==1.16.0
apache-airflow-providers-microsoft-mssql==3.9.0
apache-airflow-providers-odbc==4.7.0

### Apache Airflow version

2.10.2

### Operating System

RHEL 8.10

### Deployment

Virtualenv installation

### Deployment details

_No response_

### What happened

We're using mssql+pyodbc connections in airflow to connect to our MS SQL Server. We're creating a hook using `MsSqlHook` and using `.get_sqlalchemy_engine()` to get an sql alchemy engine object to pass along our database tasks.
Prior to apache-airflow-providers-common-sql 1.15.0, this was working as expected (last known working version is 1.14.1, please see below for a working example)
With a change introduced in 1.15.0 (https://github.com/apache/airflow/pull/40669), `.get_sqlalchemy_engine()` passes the `creator` argument with the value `self.get_conn` to the `.create_engine()` function, which overrides the engine creation with the default connection scheme, which uses `pymssql`, even though we specifically define the scheme as `pyodbc` in the connection, and in the hook.

Resulting engine object becomes a weird combination of two, with a `mssql+pyodbc` scheme in the connection uri, but tries to connect to using the `pymssql` dialect internally, which results in fatal errors:

![image](https://github.com/user-attachments/assets/bd25e5d4-c8d0-4e12-8177-e1467dd9f57c)

As if it's trying to make a `pymssql` connection, but with a `pyodbc` URL.

### What you think should happen instead

The connection should work, as it does prior to the change.

![image](https://github.com/user-attachments/assets/dbc176e6-2817-42cc-95a7-767161059e3c)

If we comment out / delete the line `engine_kwargs[""creator""] = self.get_conn` in `‎airflow/providers/common/sql/hooks/sql.py` (https://github.com/apache/airflow/commit/f6c7388cfa70874d84f312a5859a4f510fef0084#diff-6e1b2f961cb951d05d66d2d814ef5f6d8f8bf8f43c40fb5d40e27a031fed8dd7R246), connections works as expected.

### How to reproduce

This is the connection that's used (values changed for privacy)

Basically use any MSSQL connection with a `mssql+pyodbc` scheme

```
{
    ""conn_type"": ""mssql"",
    ""host"": ""<redacted>"",
    ""login"": ""user"",
    ""password"": ""pass"",
    ""schema"": ""master"",
    ""port"": 1433,
    ""extra"": {
        ""driver"": ""ODBC Driver 18 for SQL Server"",
        ""encrypt"": ""yes"",
        ""sqlalchemy_scheme"": ""mssql+pyodbc""
    }
}
```

Try to connect through a hook and sqlalchemy engine:

```python
from airflow.providers.microsoft.mssql.hooks.mssql import MsSqlHook
hook = MsSqlHook(mssql_conn_id='conn1')  # conn1: the id for above connection
#hook = MsSqlHook(mssql_conn_id='conn1', sqlalchemy_scheme='mssql+pyodbc') # optional, same result
engine = hook.get_sqlalchemy_engine()
engine.connect()
```

This fails on > 1.15.0
Works on <= 1.14.1

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",emredjan,2024-10-02 13:57:35+00:00,[],2024-10-07 15:30:28+00:00,2024-10-07 15:30:28+00:00,https://github.com/apache/airflow/issues/42664,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('provider:common-sql', ''), ('provider:microsoft-mssql', '')]","[{'comment_id': 2390973072, 'issue_id': 2561768447, 'author': 'dabla', 'body': 'Weird, because we are also using MSSQL with ODBC and are using the get_sqlalchemy_engine() method as well and having the same connection config as you posted without issues.', 'created_at': datetime.datetime(2024, 10, 3, 9, 42, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2391007611, 'issue_id': 2561768447, 'author': 'emredjan', 'body': '> Weird, because we are also using MSSQL with ODBC and are using the get_sqlalchemy_engine() method as well and having the same connection config as you posted without issues.\r\n\r\nAre you using `""sqlalchemy_scheme"": ""mssql+pyodbc""` in the connection or the hook? Or you just use the ODBC Driver with the default SQLAlchemy scheme (`mssql+pymssql`)', 'created_at': datetime.datetime(2024, 10, 3, 10, 0, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2391012380, 'issue_id': 2561768447, 'author': 'dabla', 'body': 'This error is weird, but now I understand why, what you are doing is wrong, and in the past this wouldn\'t have caused any issues, but now it is.\r\n\r\nDon\'t do this as you show above:\r\n\r\n```\r\nfrom airflow.providers.microsoft.mssql.hooks.mssql import MsSqlHook\r\nhook = MsSqlHook(mssql_conn_id=\'conn1\')  # conn1: the id for above connection\r\n#hook = MsSqlHook(mssql_conn_id=\'conn1\', sqlalchemy_scheme=\'mssql+pyodbc\') # optional, same result\r\nengine = hook.get_sqlalchemy_engine()\r\nengine.connect()\r\n```\r\n\r\nBut do this instead, as this will work, and is also better way of doing it:\r\n\r\n```\r\nhook = DbApiHook.get_hook(conn_id=\'conn1\')\r\nengine = hook.get_sqlalchemy_engine()\r\nengine.connect()\r\n```\r\n\r\nI will explain why your example fails now.  As you stated, you use MSSQL through ODBC connection (and thus ODBC driver), but in your example you use the MsSqlHook instead of OdbcHook, so here you try to us an ODBC connection with a PyMSSQL based hook (e.g. MsSqlHook), which of course won\'t work.  It could have worked in the past but then you were lucky, now it won\'t as the get_sqlalchemy_engine method is now a generic method in DbApiHook which isn\'t overridden anymore in MsSqlHook.\r\n\r\n**Important:**: **_It is always a good practice not to instantiate the specialized hook yourself, it\'s better to use the DbApiHook.get_hook(conn_id=""conn_id"") classmethod, as this will return you the specialized hook depending on the connection type of the connection id you passed. Depending of the connection type defined in the connection, Airflow will be smart enough to return you the correct Hook._**\r\n\r\nSo please try my suggestion and let me know.', 'created_at': datetime.datetime(2024, 10, 3, 10, 3, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2391021576, 'issue_id': 2561768447, 'author': 'dabla', 'body': '> > Weird, because we are also using MSSQL with ODBC and are using the get_sqlalchemy_engine() method as well and having the same connection config as you posted without issues.\r\n> \r\n> Are you using `""sqlalchemy_scheme"": ""mssql+pyodbc""` in the connection or the hook? Or you just use the ODBC Driver with the default SQLAlchemy scheme (`mssql+pymssql`)\r\n\r\nYes, I also defined the ""sqlalchemy_scheme"": ""mssql+pyodbc"" in the extra of the connection as you showed above, the configuration of your connection is correct, but the instantiation of the hook isn\'t.  Please check my answer above.', 'created_at': datetime.datetime(2024, 10, 3, 10, 7, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2391167139, 'issue_id': 2561768447, 'author': 'emredjan', 'body': 'Tried this:\r\n\r\n```python\r\nfrom airflow.providers.common.sql.hooks.sql import DbApiHook\r\nhook = DbApiHook.get_hook(conn_id=\'conn1\')\r\nengine = hook.get_sqlalchemy_engine()\r\nengine.connect()\r\n```\r\n\r\nIt results in the same error as above: `AttributeError: \'pymssql._pymssql.Connection\' object has no attribute \'add_output_converter\'` (This is an error from SQLAlchemy, when it internally tries to pass a pymssql connection object into a pyodbc dialect based connection script)\r\n\r\nWhile I understand `DbApiHook.get_hook()` is the proper way to instantiate a database hook, it just creates a `MsSqlHook` with the same parameters as above. In practice, nothing changes on the engine creation side with `.get_sqlalchemy_engine()`.\r\n\r\nAs I don\'t use any other databases, I\'m not entirely sure about the benefits of passing the connection to the engine with the `creator` argument (`engine_kwargs[""creator""] = self.get_conn`), but that causes any connection parameters to be ignored. As the passed connection with the `get_conn` just returns the default MS SQL connection with pymssql, it creates a basic connection, and any other scheme or other parameters in the connection are bypassed. See the `get_conn` definition:\r\n\r\n```python\r\ndef get_conn(self):\r\n    """"""Return a connection object.""""""\r\n    db = self.get_connection(self.get_conn_id())\r\n    return self.connector.connect(host=db.host, port=db.port, username=db.login, schema=db.schema)\r\n```\r\n\r\nFrom SQLAlchemy `create_engine()` docs:\r\n\r\n![image](https://github.com/user-attachments/assets/4ed645e6-a196-4750-b749-fb2e001174c4)\r\n\r\nThe resulting engine is a weird object with a pyodbc driver and dialect, but with a pymssql connection underlying.', 'created_at': datetime.datetime(2024, 10, 3, 11, 21, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2391202420, 'issue_id': 2561768447, 'author': 'dabla', 'body': '> Tried this:\r\n> \r\n> ```python\r\n> from airflow.providers.common.sql.hooks.sql import DbApiHook\r\n> hook = DbApiHook.get_hook(conn_id=\'conn1\')\r\n> engine = hook.get_sqlalchemy_engine()\r\n> engine.connect()\r\n> ```\r\n> \r\n> It results in the same error as above: `AttributeError: \'pymssql._pymssql.Connection\' object has no attribute \'add_output_converter\'` (This is an error from SQLAlchemy, when it internally tries to pass a pymssql connection object into a pyodbc dialect based connection script)\r\n> \r\n> While I understand `DbApiHook.get_hook()` is the proper way to instantiate a database hook, it just creates a `MsSqlHook` with the same parameters as above. In practice, nothing changes on the engine creation side with `.get_sqlalchemy_engine()`.\r\n> \r\n> As I don\'t use any other databases, I\'m not entirely sure about the benefits of passing the connection to the engine with the `creator` argument (`engine_kwargs[""creator""] = self.get_conn`), but that causes any connection parameters to be ignored. As the passed connection with the `get_conn` just returns the default MS SQL connection with pymssql, it creates a basic connection, and any other scheme or other parameters in the connection are bypassed. See the `get_conn` definition:\r\n> \r\n> ```python\r\n> def get_conn(self):\r\n>     """"""Return a connection object.""""""\r\n>     db = self.get_connection(self.get_conn_id())\r\n>     return self.connector.connect(host=db.host, port=db.port, username=db.login, schema=db.schema)\r\n> ```\r\n> \r\n> From SQLAlchemy `create_engine()` docs:\r\n> \r\n> ![image](https://private-user-images.githubusercontent.com/389091/373229841-4ed645e6-a196-4750-b749-fb2e001174c4.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjc5NTU2ODEsIm5iZiI6MTcyNzk1NTM4MSwicGF0aCI6Ii8zODkwOTEvMzczMjI5ODQxLTRlZDY0NWU2LWExOTYtNDc1MC1iNzQ5LWZiMmUwMDExNzRjNC5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMDAzJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTAwM1QxMTM2MjFaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT0wYjNkYWQzY2Y1MDU5Y2Q3ZmM4Y2RmNDJmZGEwNzYwZWFjZWRlZjJmNTk2YmJiODdjNTIxYjcxNDA4MmFlMGIzJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.q3T-7d7bt_E3P_nZ8br3dM9_dkRuajVGxkj1G6d8pTQ)\r\n> \r\n> The resulting engine is a weird object with a pyodbc driver and dialect, but with a pymssql connection underlying.\r\n\r\nOk now I\'m confused...  Are you trying to access MSSQL through ODBC or PyMSQL?  Because in your initial post you stated ODBC, but now you\'re referring to PyMSSQL?  If you want to connect through PDBC, make sure the connection type is ODBC and not MSSQL which will use pymssql underneath.', 'created_at': datetime.datetime(2024, 10, 3, 11, 38, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2391210166, 'issue_id': 2561768447, 'author': 'dabla', 'body': 'This config is also contradicting:\r\n\r\n```\r\n{\r\n    ""conn_type"": ""mssql"",\r\n    ""host"": ""<redacted>"",\r\n    ""login"": ""user"",\r\n    ""password"": ""pass"",\r\n    ""schema"": ""master"",\r\n    ""port"": 1433,\r\n    ""extra"": {\r\n        ""driver"": ""ODBC Driver 18 for SQL Server"",\r\n        ""encrypt"": ""yes"",\r\n        ""sqlalchemy_scheme"": ""mssql+pyodbc""\r\n    }\r\n}\r\n```\r\n\r\nConnection type is mssql (so pymssql), but in your extras you specify you want to use the ODBC driver and ""mssql+pyodbc"" sqlalchemy_scheme while the connection type is mssql which is actually pymssql, not ODBC.  Try changing the connection type to ODBC.', 'created_at': datetime.datetime(2024, 10, 3, 11, 42, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2391214405, 'issue_id': 2561768447, 'author': 'emredjan', 'body': ""I'm trying to access MSSQL through Microsoft ODBC driver, using the `mssql+pyodbc` sqlalchemy driver / dialect. I'm aware that MSSQL connection uses pymssql  driver / dialect underneath, but you were always able to override that for getting  sqlalchemy engine objects with the `sqlchemy_scheme` argument, which is the exact reason that this argument exists as far as I'm aware.\r\n\r\nIt should be possible to change the driver in the MSSQL connections without changing the connection type to ODBC. See MSSQL provider docs:\r\n\r\nhttps://airflow.apache.org/docs/apache-airflow-providers-microsoft-mssql/stable/_api/airflow/providers/microsoft/mssql/hooks/mssql/index.html\r\n\r\nThis was always possible, and according to documentation should still be an available option, but this change breaks existing connections that use a different driver."", 'created_at': datetime.datetime(2024, 10, 3, 11, 45, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2391219261, 'issue_id': 2561768447, 'author': 'dabla', 'body': ""> I'm trying to access MSSQL through Microsoft ODBC driver, using the `mssql+pyodbc` sqlalchemy driver / dialect. I'm aware that MSSQL connection uses pymssql driver / dialect underneath, but you were always able to override that for getting sqlalchemy engine objects with the `sqlchemy_scheme` argument, which is the exact reason that this argument exists as far as I'm aware.\r\n> \r\n> It should be possible to change the driver in the MSSQL connections without changing the connection type to ODBC. See MSSQL provider docs:\r\n> \r\n> https://airflow.apache.org/docs/apache-airflow-providers-microsoft-mssql/stable/_api/airflow/providers/microsoft/mssql/hooks/mssql/index.html\r\n\r\nAs far as I know, you should always use ODBC as connection type when using ODBC connections, not using a native connection type and then still override it as ODBC.  @potiuk what do you think? Or I'm seeing/understood this wrongly?\r\n\r\nThis is also the reason why I'm working on this [PR](https://github.com/apache/airflow/pull/41327), so you could have notion of dialects (used by insert_rows method in DbApiHook) that can be used across different connection types pointing to the same database, so the dialect which would generate the replace statement (e.g. MERGE INTO for MSSQL), would work as well for PyMSSQL as ODBC with MSSQL, as the dialect wouldn't be tied to the connection type, which would also not be feasible for a generic connection type like ODBC, as ODBCHook isn't database aware, the driver is while MsSqlHook is specific to MSSQL. At the moment the replace statement is only supported in native [MsSqlHook](https://github.com/apache/airflow/pull/40836), but this [PR](https://github.com/apache/airflow/pull/41327) will make it work independently of which connection type you use (native/odbc)."", 'created_at': datetime.datetime(2024, 10, 3, 11, 48, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2391266384, 'issue_id': 2561768447, 'author': 'emredjan', 'body': 'Think about this:\r\n\r\nI modified the connection to remove the ""contradiction"":\r\n\r\n```json\r\n{\r\n    ""conn_type"": ""mssql"",\r\n    ""host"": ""<redacted>"",\r\n    ""login"": ""user"",\r\n    ""password"": ""pass"",\r\n    ""schema"": ""master"",\r\n    ""port"": 1433,\r\n    ""extra"": {\r\n        ""driver"": ""ODBC Driver 18 for SQL Server"",\r\n        ""encrypt"": ""yes""\r\n    }\r\n}\r\n```\r\n\r\nCreating an MSSQL hook, overriding the sqlalchemy_scheme in the hook instantiation instead, which is an allowed option according to the documentation:\r\n\r\n![image](https://github.com/user-attachments/assets/aaeb66f5-3ad7-47b9-b4f8-be16bc082346)\r\n\r\n```python\r\nfrom airflow.providers.microsoft.mssql.hooks.mssql import MsSqlHook\r\nhook = MsSqlHook(mssql_conn_id=\'conn1\', sqlalchemy_scheme=\'mssql+pyodbc\')\r\nengine = hook.get_sqlalchemy_engine()\r\n\r\nengine.connect()\r\n```\r\n\r\n^ This is not working as well, which means this change is a breaking one. \r\n\r\nUsing pymssql as an internal connection is fine, but it should respect the choice of `sqlalchemy_scheme` argument in the hook, in `get_sqlalchemy_engine` and `get_sqlalchemy_connection` methods. Changing the connection type to ODBC should not be needed in my opinion as the abstraction is done by sqlalchemy anyway, which allows different connection dialects with MSSQL (`mssql+pyodbc`, `mssql+pymssql`, etc.).\r\n\r\nIf this is the way MSSQL connections in Airflow will move forward, and will not allow any other driver to be used than `pymssql`, then this was never communicated as such in the changelogs and documentations, and there needs to be more changes (like removing `sqlalchemy_scheme` override option in mssql provider), and would be better communicated as a breaking change for these use cases.', 'created_at': datetime.datetime(2024, 10, 3, 12, 13, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2391269905, 'issue_id': 2561768447, 'author': 'dabla', 'body': '> Think about this:\r\n> \r\n> I modified the connection to remove the ""contradiction"":\r\n> \r\n> ```json\r\n> {\r\n>     ""conn_type"": ""mssql"",\r\n>     ""host"": ""<redacted>"",\r\n>     ""login"": ""user"",\r\n>     ""password"": ""pass"",\r\n>     ""schema"": ""master"",\r\n>     ""port"": 1433,\r\n>     ""extra"": {\r\n>         ""driver"": ""ODBC Driver 18 for SQL Server"",\r\n>         ""encrypt"": ""yes""\r\n>     }\r\n> }\r\n> ```\r\n> \r\n> Creating an MSSQL hook, overriding the sqlalchemy_scheme in the hook instantiation instead, which is an allowed option according to the documentation:\r\n> \r\n> ![image](https://private-user-images.githubusercontent.com/389091/373244343-aaeb66f5-3ad7-47b9-b4f8-be16bc082346.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjc5NTc5MjQsIm5iZiI6MTcyNzk1NzYyNCwicGF0aCI6Ii8zODkwOTEvMzczMjQ0MzQzLWFhZWI2NmY1LTNhZDctNDdiOS1iNGY4LWJlMTZiYzA4MjM0Ni5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMDAzJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTAwM1QxMjEzNDRaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1hOWM0YmZkZTk1MWYyNDc4MTI0NjRhMGZiNjIzYzAwYzhiNWZiMDVjODE5NzEyODFmMjdjZjNmZThkMjQyMGE5JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.DvVpcGaz0JOod36MDY11AiZixGz5tjjOlcRaUYJ-aZE)\r\n> \r\n> ```python\r\n> from airflow.providers.microsoft.mssql.hooks.mssql import MsSqlHook\r\n> hook = MsSqlHook(mssql_conn_id=\'conn1\', sqlalchemy_scheme=\'mssql+pyodbc\')\r\n> engine = hook.get_sqlalchemy_engine()\r\n> \r\n> engine.connect()\r\n> ```\r\n> \r\n> ^ This is not working as well, which means this change is a breaking one.\r\n> \r\n> Using pymssql as an internal connection is fine, but it should respect the choice of `sqlalchemy_scheme` argument in the hook, in `get_sqlalchemy_engine` and `get_sqlalchemy_connection` methods. Changing the connection type to ODBC should not be needed in my opinion as the abstraction is done by sqlalchemy anyway, which allows different connection dialects with MSSQL (`mssql+pyodbc`, `mssql+pymssql`, etc.).\r\n> \r\n> If this is the way MSSQL connections in Airflow will move forward, and will not allow any other driver to be used than `pymssql`, then this was never communicated as such in the changelogs and documentations, and there needs to be more changes (like removing `sqlalchemy_scheme` override option in mssql provider), and would be better communicated as a breaking change for these use cases.\r\n\r\nIt should actually be like this:\r\n\r\n```\r\n{\r\n    ""conn_type"": ""odbc"",\r\n    ""host"": ""<redacted>"",\r\n    ""login"": ""user"",\r\n    ""password"": ""pass"",\r\n    ""schema"": ""master"",\r\n    ""port"": 1433,\r\n    ""extra"": {\r\n        ""driver"": ""ODBC Driver 18 for SQL Server"",\r\n        ""encrypt"": ""yes"",\r\n        ""sqlalchemy_scheme"": ""mssql+pyodbc""\r\n    }\r\n}\r\n```\r\n\r\nThis is what we are using and works.', 'created_at': datetime.datetime(2024, 10, 3, 12, 15, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2391282616, 'issue_id': 2561768447, 'author': 'dabla', 'body': 'I\'ve digged a bit deeper regarding following line of code:\r\n\r\n` engine_kwargs[""creator""] = self.get_conn`\r\n\r\nThis was to support JDBC connections with sqlalchemy, I could remove it from the DbApiHook and override it in the JdbcHook, then you would still be able to use it as you intended to.', 'created_at': datetime.datetime(2024, 10, 3, 12, 21, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2391298399, 'issue_id': 2561768447, 'author': 'emredjan', 'body': '> \r\n> It should actually be like this:\r\n> \r\n> ```\r\n> {\r\n>     ""conn_type"": ""odbc"",\r\n>     ""host"": ""<redacted>"",\r\n>     ""login"": ""user"",\r\n>     ""password"": ""pass"",\r\n>     ""schema"": ""master"",\r\n>     ""port"": 1433,\r\n>     ""extra"": {\r\n>         ""driver"": ""ODBC Driver 18 for SQL Server"",\r\n>         ""encrypt"": ""yes"",\r\n>         ""sqlalchemy_scheme"": ""mssql+pyodbc""\r\n>     }\r\n> }\r\n> ```\r\n> \r\n> This is what we are using and works.\r\n\r\nThis works, yes (Didn\'t test further than engine connection, but at least no failure, and uses correct driver and connection type). But it doesn\'t change the fact that it should be working for mssql connection type as well, and currently it\'s not.\r\n\r\n> I\'ve digged a bit deeper regarding following line of code:\r\n> \r\n> ` engine_kwargs[""creator""] = self.get_conn`\r\n> \r\n> This was to support JDBC connections with sqlalchemy, I could remove it from the DbApiHook and override it in the JdbcHook, then you would still be able to use it as you intended to.\r\n\r\nI believe that would be the correct approach. This JDBC specific change should not break existing connection types.', 'created_at': datetime.datetime(2024, 10, 3, 12, 30, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2391371690, 'issue_id': 2561768447, 'author': 'dabla', 'body': '> I believe that would be the correct approach. This JDBC specific change should not break existing connection types.\r\n\r\nIndeed, fully agree on the part that it should only be passed there where needed (e.g. JdbcHook).', 'created_at': datetime.datetime(2024, 10, 3, 13, 4, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2391391331, 'issue_id': 2561768447, 'author': 'dabla', 'body': '[PR ](https://github.com/apache/airflow/pull/42705)succeeded, so it could be merged.', 'created_at': datetime.datetime(2024, 10, 3, 13, 13, 9, tzinfo=datetime.timezone.utc)}]","dabla on (2024-10-03 09:42:36 UTC): Weird, because we are also using MSSQL with ODBC and are using the get_sqlalchemy_engine() method as well and having the same connection config as you posted without issues.

emredjan (Issue Creator) on (2024-10-03 10:00:46 UTC): Are you using `""sqlalchemy_scheme"": ""mssql+pyodbc""` in the connection or the hook? Or you just use the ODBC Driver with the default SQLAlchemy scheme (`mssql+pymssql`)

dabla on (2024-10-03 10:03:22 UTC): This error is weird, but now I understand why, what you are doing is wrong, and in the past this wouldn't have caused any issues, but now it is.

Don't do this as you show above:

```
from airflow.providers.microsoft.mssql.hooks.mssql import MsSqlHook
hook = MsSqlHook(mssql_conn_id='conn1')  # conn1: the id for above connection
#hook = MsSqlHook(mssql_conn_id='conn1', sqlalchemy_scheme='mssql+pyodbc') # optional, same result
engine = hook.get_sqlalchemy_engine()
engine.connect()
```

But do this instead, as this will work, and is also better way of doing it:

```
hook = DbApiHook.get_hook(conn_id='conn1')
engine = hook.get_sqlalchemy_engine()
engine.connect()
```

I will explain why your example fails now.  As you stated, you use MSSQL through ODBC connection (and thus ODBC driver), but in your example you use the MsSqlHook instead of OdbcHook, so here you try to us an ODBC connection with a PyMSSQL based hook (e.g. MsSqlHook), which of course won't work.  It could have worked in the past but then you were lucky, now it won't as the get_sqlalchemy_engine method is now a generic method in DbApiHook which isn't overridden anymore in MsSqlHook.

**Important:**: **_It is always a good practice not to instantiate the specialized hook yourself, it's better to use the DbApiHook.get_hook(conn_id=""conn_id"") classmethod, as this will return you the specialized hook depending on the connection type of the connection id you passed. Depending of the connection type defined in the connection, Airflow will be smart enough to return you the correct Hook._**

So please try my suggestion and let me know.

dabla on (2024-10-03 10:07:42 UTC): Yes, I also defined the ""sqlalchemy_scheme"": ""mssql+pyodbc"" in the extra of the connection as you showed above, the configuration of your connection is correct, but the instantiation of the hook isn't.  Please check my answer above.

emredjan (Issue Creator) on (2024-10-03 11:21:43 UTC): Tried this:

```python
from airflow.providers.common.sql.hooks.sql import DbApiHook
hook = DbApiHook.get_hook(conn_id='conn1')
engine = hook.get_sqlalchemy_engine()
engine.connect()
```

It results in the same error as above: `AttributeError: 'pymssql._pymssql.Connection' object has no attribute 'add_output_converter'` (This is an error from SQLAlchemy, when it internally tries to pass a pymssql connection object into a pyodbc dialect based connection script)

While I understand `DbApiHook.get_hook()` is the proper way to instantiate a database hook, it just creates a `MsSqlHook` with the same parameters as above. In practice, nothing changes on the engine creation side with `.get_sqlalchemy_engine()`.

As I don't use any other databases, I'm not entirely sure about the benefits of passing the connection to the engine with the `creator` argument (`engine_kwargs[""creator""] = self.get_conn`), but that causes any connection parameters to be ignored. As the passed connection with the `get_conn` just returns the default MS SQL connection with pymssql, it creates a basic connection, and any other scheme or other parameters in the connection are bypassed. See the `get_conn` definition:

```python
def get_conn(self):
    """"""Return a connection object.""""""
    db = self.get_connection(self.get_conn_id())
    return self.connector.connect(host=db.host, port=db.port, username=db.login, schema=db.schema)
```

From SQLAlchemy `create_engine()` docs:

![image](https://github.com/user-attachments/assets/4ed645e6-a196-4750-b749-fb2e001174c4)

The resulting engine is a weird object with a pyodbc driver and dialect, but with a pymssql connection underlying.

dabla on (2024-10-03 11:38:26 UTC): Ok now I'm confused...  Are you trying to access MSSQL through ODBC or PyMSQL?  Because in your initial post you stated ODBC, but now you're referring to PyMSSQL?  If you want to connect through PDBC, make sure the connection type is ODBC and not MSSQL which will use pymssql underneath.

dabla on (2024-10-03 11:42:59 UTC): This config is also contradicting:

```
{
    ""conn_type"": ""mssql"",
    ""host"": ""<redacted>"",
    ""login"": ""user"",
    ""password"": ""pass"",
    ""schema"": ""master"",
    ""port"": 1433,
    ""extra"": {
        ""driver"": ""ODBC Driver 18 for SQL Server"",
        ""encrypt"": ""yes"",
        ""sqlalchemy_scheme"": ""mssql+pyodbc""
    }
}
```

Connection type is mssql (so pymssql), but in your extras you specify you want to use the ODBC driver and ""mssql+pyodbc"" sqlalchemy_scheme while the connection type is mssql which is actually pymssql, not ODBC.  Try changing the connection type to ODBC.

emredjan (Issue Creator) on (2024-10-03 11:45:26 UTC): I'm trying to access MSSQL through Microsoft ODBC driver, using the `mssql+pyodbc` sqlalchemy driver / dialect. I'm aware that MSSQL connection uses pymssql  driver / dialect underneath, but you were always able to override that for getting  sqlalchemy engine objects with the `sqlchemy_scheme` argument, which is the exact reason that this argument exists as far as I'm aware.

It should be possible to change the driver in the MSSQL connections without changing the connection type to ODBC. See MSSQL provider docs:

https://airflow.apache.org/docs/apache-airflow-providers-microsoft-mssql/stable/_api/airflow/providers/microsoft/mssql/hooks/mssql/index.html

This was always possible, and according to documentation should still be an available option, but this change breaks existing connections that use a different driver.

dabla on (2024-10-03 11:48:22 UTC): As far as I know, you should always use ODBC as connection type when using ODBC connections, not using a native connection type and then still override it as ODBC.  @potiuk what do you think? Or I'm seeing/understood this wrongly?

This is also the reason why I'm working on this [PR](https://github.com/apache/airflow/pull/41327), so you could have notion of dialects (used by insert_rows method in DbApiHook) that can be used across different connection types pointing to the same database, so the dialect which would generate the replace statement (e.g. MERGE INTO for MSSQL), would work as well for PyMSSQL as ODBC with MSSQL, as the dialect wouldn't be tied to the connection type, which would also not be feasible for a generic connection type like ODBC, as ODBCHook isn't database aware, the driver is while MsSqlHook is specific to MSSQL. At the moment the replace statement is only supported in native [MsSqlHook](https://github.com/apache/airflow/pull/40836), but this [PR](https://github.com/apache/airflow/pull/41327) will make it work independently of which connection type you use (native/odbc).

emredjan (Issue Creator) on (2024-10-03 12:13:41 UTC): Think about this:

I modified the connection to remove the ""contradiction"":

```json
{
    ""conn_type"": ""mssql"",
    ""host"": ""<redacted>"",
    ""login"": ""user"",
    ""password"": ""pass"",
    ""schema"": ""master"",
    ""port"": 1433,
    ""extra"": {
        ""driver"": ""ODBC Driver 18 for SQL Server"",
        ""encrypt"": ""yes""
    }
}
```

Creating an MSSQL hook, overriding the sqlalchemy_scheme in the hook instantiation instead, which is an allowed option according to the documentation:

![image](https://github.com/user-attachments/assets/aaeb66f5-3ad7-47b9-b4f8-be16bc082346)

```python
from airflow.providers.microsoft.mssql.hooks.mssql import MsSqlHook
hook = MsSqlHook(mssql_conn_id='conn1', sqlalchemy_scheme='mssql+pyodbc')
engine = hook.get_sqlalchemy_engine()

engine.connect()
```

^ This is not working as well, which means this change is a breaking one. 

Using pymssql as an internal connection is fine, but it should respect the choice of `sqlalchemy_scheme` argument in the hook, in `get_sqlalchemy_engine` and `get_sqlalchemy_connection` methods. Changing the connection type to ODBC should not be needed in my opinion as the abstraction is done by sqlalchemy anyway, which allows different connection dialects with MSSQL (`mssql+pyodbc`, `mssql+pymssql`, etc.).

If this is the way MSSQL connections in Airflow will move forward, and will not allow any other driver to be used than `pymssql`, then this was never communicated as such in the changelogs and documentations, and there needs to be more changes (like removing `sqlalchemy_scheme` override option in mssql provider), and would be better communicated as a breaking change for these use cases.

dabla on (2024-10-03 12:15:16 UTC): It should actually be like this:

```
{
    ""conn_type"": ""odbc"",
    ""host"": ""<redacted>"",
    ""login"": ""user"",
    ""password"": ""pass"",
    ""schema"": ""master"",
    ""port"": 1433,
    ""extra"": {
        ""driver"": ""ODBC Driver 18 for SQL Server"",
        ""encrypt"": ""yes"",
        ""sqlalchemy_scheme"": ""mssql+pyodbc""
    }
}
```

This is what we are using and works.

dabla on (2024-10-03 12:21:35 UTC): I've digged a bit deeper regarding following line of code:

` engine_kwargs[""creator""] = self.get_conn`

This was to support JDBC connections with sqlalchemy, I could remove it from the DbApiHook and override it in the JdbcHook, then you would still be able to use it as you intended to.

emredjan (Issue Creator) on (2024-10-03 12:30:02 UTC): This works, yes (Didn't test further than engine connection, but at least no failure, and uses correct driver and connection type). But it doesn't change the fact that it should be working for mssql connection type as well, and currently it's not.


I believe that would be the correct approach. This JDBC specific change should not break existing connection types.

dabla on (2024-10-03 13:04:07 UTC): Indeed, fully agree on the part that it should only be passed there where needed (e.g. JdbcHook).

dabla on (2024-10-03 13:13:09 UTC): [PR ](https://github.com/apache/airflow/pull/42705)succeeded, so it could be merged.

"
2560797654,issue,closed,completed,AIP-84 Migrate the public endpoint Get DAG to FastAPI,"### Description

Currently the Get DAG public endpoint is at `api_connexion/endpoints/dag_endpoint.py` under `get_dag`. It provides basic information of a DAG. We need to migrate it to the `api_fastapi/views/public/dags.py` under a `get_dag` or similar.

Features and functionality of the endpoint will remain unchanged.

### Use case/motivation

AIP-84 cWiki Link: https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API

### Related issues

Related to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-10-02 06:54:45+00:00,['omkar-foss'],2024-10-10 07:47:51+00:00,2024-10-10 07:47:50+00:00,https://github.com/apache/airflow/issues/42652,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]",[],
2560790266,issue,closed,completed,AIP-84 Migrate the public endpoint Delete DAG to FastAPI,"### Description

The Delete DAG public endpoint is at `api_connexion/endpoints/dag_endpoint.py` under `delete_dag`. We need to migrate it to the `api_fastapi/views/public/dags.py` under a `delete_dag` or similar.

Features and functionality of the endpoint will remain unchanged.

### Use case/motivation

AIP-84 cWiki Link: https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API

### Related issues

Related to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-10-02 06:49:48+00:00,['omkar-foss'],2024-10-15 08:45:52+00:00,2024-10-15 08:45:52+00:00,https://github.com/apache/airflow/issues/42650,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]",[],
2560705429,issue,closed,completed,Dataset and VirtualenvOperator without system_site_package,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.1

### What happened?

Hello everyone,

I am trying to use the dataset feature together with the Python Virtualenv Operator in Airflow. Specifically, I want to trigger the Virtualenv Operator in one DAG upon the completion of a task in another DAG, which I am doing through a dataset. In the Virtualenv Operator, I would like to use the scheduled datetime of the dataset, so I am trying to pass the triggering_dataset_events argument.

However, when I set system_site_package=False, I encounter an error where the Virtualenv Operator cannot accept triggering_dataset_events as an argument. I believe this is because when system_site_package=False, the Airflow package is not available inside the Virtualenv, and as a result, DatasetEvents cannot be passed to it.

As a workaround, I am currently using the Python Operator to receive the triggering_dataset_events and then passing it to the Virtualenv Operator via XCom. However, this approach complicates the code, and I am wondering if there is a simpler solution to this issue.


### What you think should happen instead?

_No response_

### How to reproduce

I used composer-local-dev to reproduce.

```bash
$ git clone https://github.com/GoogleCloudPlatform/composer-local-dev.git
$ cd composer-local-dev
```

```bash
$ python --version
Python 3.11.9
$ python -m venv venv
$ . venv/bin/activate
$ pip install -U pip
$ pip install .
$ pip install requests==2.31.0  # to avoid docker-py error
$ composer-dev create \
--from-image-version composer-2.9.1-airflow-2.9.1 \
--project your-project-id \
--port 8888 example-local-env
```

Put this file under `composer/example-local-env`.

```python
from airflow import DAG
from airflow.datasets import Dataset
from airflow.decorators import task
from pendulum.datetime import DateTime

example_dataset = Dataset(""example_dataset"")


@task(outlets=[example_dataset])
def source_task():
    print(""task source"")


@task.virtualenv(system_site_packages=True)  # This is the default. Success
def venvtask_w_ssp(triggering_dataset_events):
    print(""##################################"")
    for key in triggering_dataset_events:
        print(f""{key} {triggering_dataset_events[key]}"")
    print(""##################################"")


@task.virtualenv(system_site_packages=False)  # Error
def venvtask_wo_ssp(triggering_dataset_events):
    print(""##################################"")
    for key in triggering_dataset_events:
        print(f""{key} {triggering_dataset_events[key]}"")
    print(""##################################"")


# Work Around
@task()
def pytask(triggering_dataset_events) -> str:
    event = triggering_dataset_events[example_dataset.uri][0]
    return event.source_dag_run.logical_date.strftime(""%Y-%m-%d"")


@task.virtualenv(system_site_packages=False)
def venvtask_wo_ssp_workaround(target_date):  # Success
    print(""##################################"")
    print(target_date)
    print(""##################################"")


with DAG(
    dag_id=""source"",
    start_date=DateTime(2022, 1, 1),
    schedule=""* * * * *"",
) as dag:
    source_task()


with DAG(
    dag_id=""reference"",
    start_date=DateTime(2022, 1, 1),
    schedule=[example_dataset],
) as dag:
    venvtask_w_ssp()
    venvtask_wo_ssp()
    target_date = pytask()
    venvtask_wo_ssp_workaround(target_date)
```

Then, `venvtask_wo_ssp` task shows this error message.

```
[2024-10-02, 04:55:37 UTC] {process_utils.py:191} INFO - Traceback (most recent call last):
[2024-10-02, 04:55:37 UTC] {process_utils.py:191} INFO -   File ""/tmp/venv-callu3jpd96w/script.py"", line 48, in <module>
[2024-10-02, 04:55:37 UTC] {process_utils.py:191} INFO -     res = venvtask_wo_ssp(*arg_dict[""args""], **arg_dict[""kwargs""])
[2024-10-02, 04:55:37 UTC] {process_utils.py:191} INFO -           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-10-02, 04:55:37 UTC] {process_utils.py:191} INFO - TypeError: venvtask_wo_ssp() missing 1 required positional argument: 'triggering_dataset_events'
```


### Operating System

Mac OS 14.6.1（23G93）

### Versions of Apache Airflow Providers

apache-airflow-providers-apache-beam==5.7.1
apache-airflow-providers-celery==3.7.2
apache-airflow-providers-cncf-kubernetes==8.3.3
apache-airflow-providers-common-io==1.3.2
apache-airflow-providers-common-sql==1.14.2
apache-airflow-providers-dbt-cloud==3.9.0
apache-airflow-providers-fab==1.2.2
apache-airflow-providers-ftp==3.10.0
apache-airflow-providers-google==10.21.0
apache-airflow-providers-hashicorp==3.7.1
apache-airflow-providers-http==4.12.0
apache-airflow-providers-imap==3.6.1
apache-airflow-providers-mysql==5.6.2
apache-airflow-providers-postgres==5.11.2
apache-airflow-providers-redis==3.7.1
apache-airflow-providers-sendgrid==3.5.1
apache-airflow-providers-smtp==1.7.1
apache-airflow-providers-sqlite==3.8.1
apache-airflow-providers-ssh==3.11.2

### Deployment

Google Cloud Composer

### Deployment details

image version: composer-2.9.1-airflow-2.9.1

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rkawajiri,2024-10-02 05:32:08+00:00,[],2024-10-03 00:00:08+00:00,2024-10-03 00:00:08+00:00,https://github.com/apache/airflow/issues/42648,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:core-operators', 'Operators, Sensors and hooks within Core Airflow'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('area:datasets', 'Issues related to the datasets feature')]","[{'comment_id': 2387663979, 'issue_id': 2560705429, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 2, 5, 32, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2389748714, 'issue_id': 2560705429, 'author': 'jscheffl', 'body': 'Yes, I also very much assume (w/o manually making a regression) that the Airflow packages are needed in the venv to pass the param. If you don\'t want to have system site packages, would it work if you add `apache-airflow==<the version>"" to the requirements of the venv?', 'created_at': datetime.datetime(2024, 10, 2, 21, 46, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2390049410, 'issue_id': 2560705429, 'author': 'potiuk', 'body': '> Yes, I also very much assume (w/o manually making a regression) that the Airflow packages are needed in the venv to pass the param. If you don\'t want to have system site packages, would it work if you add `apache-airflow=="" to the requirements of the venv?\r\n\r\nYes. I guess what happens is that you have an older version of airflow installed as ""system"" package and this one is used in this case. Making sure to add ""apache-airflow==YOUR_CURRENT_AIRFLOW_VERSION"" should hopefully fix the problem. And you can get it from `airflow.version.version` for example', 'created_at': datetime.datetime(2024, 10, 2, 23, 59, 59, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-02 05:32:10 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jscheffl on (2024-10-02 21:46:10 UTC): Yes, I also very much assume (w/o manually making a regression) that the Airflow packages are needed in the venv to pass the param. If you don't want to have system site packages, would it work if you add `apache-airflow==<the version>"" to the requirements of the venv?

potiuk on (2024-10-02 23:59:59 UTC): Yes. I guess what happens is that you have an older version of airflow installed as ""system"" package and this one is used in this case. Making sure to add ""apache-airflow==YOUR_CURRENT_AIRFLOW_VERSION"" should hopefully fix the problem. And you can get it from `airflow.version.version` for example

"
2560565632,issue,closed,completed,MsSQLHook and pymssql Adaptive Server connection failed,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Connecting to mssql server caused an adaptive server issue and database could not be accessed.

> pymssql.exceptions.OperationalError: (20002, b'DB-Lib error message 20002, severity 9:\nAdaptive Server connection failed

### What you think should happen instead?

Connection hook should be successful if the Airflow connection variable is readable and filled in correctly.

Logs below:

> [2024-10-02, 02:05:48 UTC] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""src/pymssql/_pymssql.pyx"", line 650, in pymssql._pymssql.connect
  File ""src/pymssql/_mssql.pyx"", line 2158, in pymssql._mssql.connect
  File ""src/pymssql/_mssql.pyx"", line 712, in pymssql._mssql.MSSQLConnection.__init__
  File ""src/pymssql/_mssql.pyx"", line 1884, in pymssql._mssql.maybe_raise_MSSQLDatabaseException
  File ""src/pymssql/_mssql.pyx"", line 1901, in pymssql._mssql.raise_MSSQLDatabaseException
pymssql._mssql.MSSQLDatabaseException: (20002, b'DB-Lib error message 20002, severity 9:\nAdaptive Server connection failed \n')
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/home/etl/anaconda3/lib/python3.9/site-packages/airflow/models/taskinstance.py"", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File ""/home/etl/anaconda3/lib/python3.9/site-packages/airflow/models/taskinstance.py"", line 733, in _execute_callable
    return ExecutionCallableRunner(
  File ""/home/etl/anaconda3/lib/python3.9/site-packages/airflow/utils/operator_helpers.py"", line 252, in run
    return self.func(*args, **kwargs)
  File ""/home/etl/anaconda3/lib/python3.9/site-packages/airflow/models/baseoperator.py"", line 406, in wrapper
    return func(self, *args, **kwargs)
  File ""/home/etl/airflow/dags/platypus_etl/mssqltopostgresoperator.py"", line 59, in execute
    with closing(source.get_conn()) as conn:
  File ""/home/etl/anaconda3/lib/python3.9/site-packages/airflow/providers/microsoft/mssql/hooks/mssql.py"", line 98, in get_conn
    conn = pymssql.connect(
  File ""src/pymssql/_pymssql.pyx"", line 659, in pymssql._pymssql.connect
pymssql.exceptions.OperationalError: (20002, b'DB-Lib error message 20002, severity 9:\nAdaptive Server connection failed

### How to reproduce

apache-airflow-providers-microsoft-mssql-3.9.1
pymssql-2.3.1
apache-airflow-providers-common-sql-1.17.1

Use MsSQL hook to connect to database using a MSSQL connection from the Airflow connection menu.

### Operating System

CentOS 7

### Versions of Apache Airflow Providers

apache-airflow-providers-microsoft-mssql-3.9.1
pymssql-2.3.1
apache-airflow-providers-common-sql-1.17.1

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

Problem occurs using apache-airflow-providers-microsoft-mssql-3.9.1 and pymssql-2.3.1 and apache-airflow-providers-common-sql-1.17.1

Reverting back to  apache-airflow-providers-microsoft-mssql-3.4.2, pymssql-2.2.4  and apache-airflow-providers-common-sql-1.7.2 fixed my issue.

See similar issue here: https://github.com/pymssql/pymssql/issues/913

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",anthony-joyce,2024-10-02 02:45:47+00:00,[],2024-10-02 23:46:05+00:00,2024-10-02 23:46:05+00:00,https://github.com/apache/airflow/issues/42642,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:microsoft-mssql', '')]","[{'comment_id': 2388692504, 'issue_id': 2560565632, 'author': 'gordthompson', 'body': ""@anthony-joyce - As in https://github.com/pymssql/pymssql/issues/913 , are you using an environment variable similar to this?\r\n\r\n```bash\r\nexport AIRFLOW_CONN_MSSQL_DEFAULT='mssql+pymssql://usr:pwd@server.example.com/dbname'\r\n```"", 'created_at': datetime.datetime(2024, 10, 2, 13, 48, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2388710745, 'issue_id': 2560565632, 'author': 'anthony-joyce', 'body': '@gordthompson Hi Gord. Thanks for the reply. I am not, currently. I was using the connection variable from the Airflow environment (Airflow menu --> Connections). Unless that connection is getting rendered incorrectly before being passed to mssql.....\r\n\r\nShould I be using an environment variable similar to what you have posted?\r\n\r\nThanks!', 'created_at': datetime.datetime(2024, 10, 2, 13, 55, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2388740998, 'issue_id': 2560565632, 'author': 'gordthompson', 'body': ""I have zero experience with Airflow so I don't know the best approach to use in your case. I was really asking if you were using a connection URL of that form. If you were, then I would suggest trying\r\n\r\n```\r\nmssql+pymssql://usr:pwd@server.example.com/dbname?tds_version=7.0\r\n```\r\n\r\nto see if that helps. Can you add the extra `tds_version` connection parameter using your method?"", 'created_at': datetime.datetime(2024, 10, 2, 14, 7, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2389273803, 'issue_id': 2560565632, 'author': 'anthony-joyce', 'body': 'Added {tds_version: 7.0} to the connection but still receiving errors.\r\n\r\nConnection URI looks like: `INFO - mssql+pymssql://{username}:***@{host}:{port}/{db_name}?tds_version=7.0`\r\nI blocked out the important DB info for security.\r\nConnection URI looks exactly the same as when I revert to get things working again.\r\n\r\nHopefully others will chime in as the is getting triaged. Thanks again @gordthompson.', 'created_at': datetime.datetime(2024, 10, 2, 17, 54, 45, tzinfo=datetime.timezone.utc)}]","gordthompson on (2024-10-02 13:48:05 UTC): @anthony-joyce - As in https://github.com/pymssql/pymssql/issues/913 , are you using an environment variable similar to this?

```bash
export AIRFLOW_CONN_MSSQL_DEFAULT='mssql+pymssql://usr:pwd@server.example.com/dbname'
```

anthony-joyce (Issue Creator) on (2024-10-02 13:55:38 UTC): @gordthompson Hi Gord. Thanks for the reply. I am not, currently. I was using the connection variable from the Airflow environment (Airflow menu --> Connections). Unless that connection is getting rendered incorrectly before being passed to mssql.....

Should I be using an environment variable similar to what you have posted?

Thanks!

gordthompson on (2024-10-02 14:07:39 UTC): I have zero experience with Airflow so I don't know the best approach to use in your case. I was really asking if you were using a connection URL of that form. If you were, then I would suggest trying

```
mssql+pymssql://usr:pwd@server.example.com/dbname?tds_version=7.0
```

to see if that helps. Can you add the extra `tds_version` connection parameter using your method?

anthony-joyce (Issue Creator) on (2024-10-02 17:54:45 UTC): Added {tds_version: 7.0} to the connection but still receiving errors.

Connection URI looks like: `INFO - mssql+pymssql://{username}:***@{host}:{port}/{db_name}?tds_version=7.0`
I blocked out the important DB info for security.
Connection URI looks exactly the same as when I revert to get things working again.

Hopefully others will chime in as the is getting triaged. Thanks again @gordthompson.

"
2560141133,issue,open,,Separate non-db-tests (and others) to providers/non-providers/task_sdk,"Currently `non-db-tests` execute all test type specified via `--test-types` in a single process using xdist (`-n processors`) parallelism in order to make efficient use of parallelisation of the tests. This means that both ""core"" and ""providers"" types of tests are run in a single `pytest` command. For example:

```bash
breeze testing non-db-tests \
      --parallel-test-types ""API Providers[google]""
```

Where PARALLELL_TEST_TYPES are produced by the ""selective check"" to determine which tests should be run.

However the #42505 splits out providers to a separate directory and we cannot run providers and non-provider tests any longer in the same process, because pytest has a limitation where test module names should not overlap (and tests/conftest.py is different for providers and non-providers, so they cannot be run in the same process as it will result in 

```
_pytest.pathlib.ImportPathMismatchError: ('tests.conftest', '/opt/airflow/tests/conftest.py', PosixPath('/opt/airflow/providers/tests/conftest.py'))
```

One solution to that (probably best approach) is to introduce a required flag (`--providers/--no-providers`) for `non-db-tests` and filter out / core provider test folders when respective flag is used, and run two separate jobs  in our CI - one for providers and one for core. 

That would require to have to separate jobs in `ci.yml`. ""Non-db Core tests"", ""Non-db Providers tests"" - each of them passing the flag down to `run-unit-tests.yml` composite workflow.





",potiuk,2024-10-01 20:21:14+00:00,"['potiuk', 'gopidesupavan']",2024-11-15 09:19:00+00:00,,https://github.com/apache/airflow/issues/42632,[],"[{'comment_id': 2387232530, 'issue_id': 2560141133, 'author': 'gopidesupavan', 'body': '@potiuk I am interested in this should I take this on? Might take bit time to workout this, this area new to me , but this will help me to understand more on the airflow ci/cd . :)', 'created_at': datetime.datetime(2024, 10, 1, 23, 2, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2449350710, 'issue_id': 2560141133, 'author': 'potiuk', 'body': '> @potiuk I am interested in this should I take this on? Might take bit time to workout this, this area new to me , but this will help me to understand more on the airflow ci/cd . :)\r\n\r\nIt\'s going to be a little more complex it seems - we likely wil have to redesign a bit the way how tests are executed in CI with different  test ""groups"" now so i will start a wider discussion on slack channel to discuss how to do it and involve others.', 'created_at': datetime.datetime(2024, 10, 31, 8, 54, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2449403134, 'issue_id': 2560141133, 'author': 'gopidesupavan', 'body': '> > @potiuk I am interested in this should I take this on? Might take bit time to workout this, this area new to me , but this will help me to understand more on the airflow ci/cd . :)\r\n> \r\n> It\'s going to be a little more complex it seems - we likely wil have to redesign a bit the way how tests are executed in CI with different test ""groups"" now so i will start a wider discussion on slack channel to discuss how to do it and involve others.\r\n\r\nAbsolutely! I just began analyzing this yesterday to see which components can be grouped or not. I noticed some existing options, like skip-provider-tests, and now that we have the task SDK, we may need to support this for executing tests or providing skipping options as well. I completely agree that some redesign is necessary.', 'created_at': datetime.datetime(2024, 10, 31, 9, 24, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2449654038, 'issue_id': 2560141133, 'author': 'potiuk', 'body': 'Hey Here. I would like to start a discussion and propose some redesign of our CI - to address some of the issues mentioned here - and involve others so that we come up together with a better design and then we get it implemented together.\r\n\r\nFirst let me state the problems and what is wrong currently. And I will write a separte comment on what would be the first proposal to address it.\r\n\r\n* Before #42505 and #43319 all the tests we had were in ""tests"" folder. Now we have three test folders for unit tests: ""tests"", ""providers/tests"", ""task_sdk/tests"".\r\n\r\nAlso ""system"" tests are a separate type - but they are under ""tests"" in ""airflow"" and ""provider\' tests.\r\n\r\n* Those tests cannot be run together in single pytest command - because all of them have a conftest.py file at the top folder and pytest refuses to run them together because it does not know which `conftest.py` is the ""top-level"" one.\r\n\r\n* Currently we have ""non-db-tests"" that runs all the ""non-db"" tests together in a single xdist/parallel fashion and it currenlty runs together ""tests"" and ""providers/tests"" together - but this is really thanks to a workaround for it in ""providers"" (there is an `__init__.py` in ""providers"" folder that makes the test belong to package ""providers.tests"" - but this is really a hack because ""providers"" is where the whole ""providers"" project is, it\'s not a python ""package"".\r\n\r\n* Both ""airflow"" and ""providers"" tests  are currently run with `breeze testing tests` command (which by default used to run ALL tests (""airflow + providers"") and only when you specified particular packages or modules to run as additional arguments woudl run only those tests you specified. This was a bit ""hacky"" approach how it was done and one of the reasons why running system tests were broken after provider\'s separation (i *temporarily* fixed it in #43529 but this is really a band-aid). \r\n\r\n* Also the same `breeze testing tests` command is used to run ""helm"" unit tests  -  and there is another ""hack"" implemented  - we are also using breeze container to run the tests but  ""helm"" tests do not need to be run in breeze container at all - they need a small local venv we already have from k8s where `helm` is installed - and we can run them there\r\n\r\n* Actually `airflow` and `task_sdk` tests technically (eventually when we fully remove all provider references from airflow core and tests) will not have to use breeze container as well. With UV we could technicaly run the tests in a venv created dynamically in CI (with main constraints / `uv lock` for reproducibility) rather than using the CI image - because we do not need alll the 700+ deps - they should be ""reproducible-enough"" without the CI imaage of ours. \r\n\r\n* We alredy have separate ""TaskSDK tests"" that are running in the same environment as ""Non-DB tests"" - but only for ""task_sdk"" and they have separate `breeze testing task-sdk-tests` - it looks like a good direction,', 'created_at': datetime.datetime(2024, 10, 31, 11, 44, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2449671014, 'issue_id': 2560141133, 'author': 'potiuk', 'body': 'My proposal (high level) - it needs some detailed thinking and prototyping regarding the parallel execution of tests in CI is to follow the `breeze testing task-sdk-tests` and introduce three more testing commands:\r\n\r\n* `breeze testing provider-tests` command\r\n* `breeze testing helm-tests` command\r\n* `breeze testint system-tests` command\r\n\r\nThis means that we will loose some of the parallelism benefifts when we run `provider` tests and `airflow` tests together (but instead we will increase a number of jobs we have in CI, whch we still should be able to do without exceeding the limits that INFRA has on a number of parallel jobs we can run in CI jobs).\r\n\r\nWe can also gradually move tests to local venvs where they do not need database containers from docker compose and Pytthon / System dependencies that we currently standardise in the CI image.\r\n\r\nAlso System tests should have a \'dry-run"" mode - where we can run them as part of the CI so that we know that they ar e not broken. We are not able to run system tests for all external systems, but at least we should be able to have some ""dry-run"" system tests that we should be able to run with all PRs.\r\n\r\nSo high level we have:\r\n\r\n##  NOW\r\n\r\n### CI image bound tests:\r\n\r\n* Airflow + Providers DB tests\r\n* Airflow + Providers Non-DB Tests\r\n* TaskSDK Non-DB tests (no DB tests here)\r\n* Helm Unit local virtualenv tests\r\n\r\nSystem tests are not currently executed.\r\n\r\nAnd we can do it this way:\r\n\r\n## ~Stage 1~  :green_circle:   COMPLETED by #43979\r\n\r\n~Likely we could attempt to make TaskSDK and Helm tests to run in venv rather than in CI image even now - I think Task SDK  (@kaxil @ashb ?) there are very little dependencies there and we could easily make a local venv there instead of the whole CI image as there are no specific system or provider dependencies there.~\r\n\r\n~Also spliting out tests to ""Airflow"" and ""Providers"" without moving them out of CI image will hel us to address the issues we have now with conftest.py and actual ""separation"" of the different test groups.~\r\n\r\n### ~CI image bound tests:~\r\n\r\n~* Core DB tests~\r\n~* Core Non-DB Tests~\r\n~* Providers DB tests~\r\n~* Providers Non-DB tests~\r\n~* System tests dry-run tests~\r\n\r\n### Local virtualenv tests\r\n\r\n~* Helm Unit local virtualenv tests~\r\n~* TaskSDK Non-DB tests~\r\n\r\n---------------------------------\r\n\r\n## Stage 2 \r\n\r\nWhen we remove all the remaining references to providers from Airflow we might move Airflow Non-DB tests to local virtualenv.\r\n\r\n### CI image bound tests:\r\n\r\n* Core DB tests\r\n* Providers DB tests\r\n* Providers Non-DB tests\r\n* Core System tests dry-run tests \r\n* Providers System tests dry-run tests \r\n\r\n### Local virtualenv tests:\r\n\r\n* Core Non-DB Tests\r\n* TaskSDK Non-DB tests (no DB tests here)\r\n* Helm Unit local virtualenv tests\r\n\r\n---------------------------------\r\n\r\n## Stage 3\r\n\r\nWe switch the providers tests to use task sdk - and at this stage all db tests should be converted to non-db tests.\r\n\r\n### CI image bound tests:\r\n\r\n\r\n* Providers Non-DB tests\r\n* System tests dry-run tests \r\n* Core DB tests\r\n\r\n### Local virtualenv tests:\r\n\r\n\r\n* Core Non-DB Tests\r\n* TaskSDK Non-DB tests (no DB tests here)\r\n* Helm Unit local virtualenv tests\r\n\r\nWe will still need provider tests to run in CI image and use the image to calculate constraints etc. - for quite some time (maybe for ever) but I see a clear path and stages we could take to simplify the ""Airflow"" and Task SDK ones.\r\n\r\n\r\n---------------------------------\r\n\r\n## Stage 4\r\n\r\nWhen we add the capability of testcontainers (see #43514)  the db tests might eventually be run in local virtualenv\r\n\r\n\r\n### CI image bound tests:\r\n\r\n\r\n* Providers Non-DB tests\r\n* System tests dry-run tests \r\n\r\n### Local virtualenv tests:\r\n\r\n* Core DB tests\r\n* Core Non-DB Tests\r\n* TaskSDK Non-DB tests (no DB tests here)\r\n* Helm Unit local virtualenv tests\r\n\r\nWe will still need provider tests to run in CI image and use the image to calculate constraints etc. - for quite some time (maybe for ever) but I see a clear path and stages we could take to simplify the ""Airflow"" and Task SDK ones.', 'created_at': datetime.datetime(2024, 10, 31, 11, 54, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2449683148, 'issue_id': 2560141133, 'author': 'potiuk', 'body': 'I made a few refinements to the initial proposal', 'created_at': datetime.datetime(2024, 10, 31, 12, 1, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2449700465, 'issue_id': 2560141133, 'author': 'ashb', 'body': ""> I think Task SDK (@kaxil @ashb ?) there are very little dependencies there and we could easily make a local venv there instead of the whole CI image as there are no specific system or provider dependencies there.\n\nYes, there should be no deps on a any providers, not even the new standard provider as all of those need to depend on the SDK.\n\nGiven the ability of UV to create predictable venvs with the right versions installed, yeah I agree. Also if it's relevant I think every* test in the task SDK will be stateless and could be parallelized - i.e. they are all non-db style tests\n\n* Might not be 100% true, but my gut says it is"", 'created_at': datetime.datetime(2024, 10, 31, 12, 11, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2449710727, 'issue_id': 2560141133, 'author': 'potiuk', 'body': ""> Given the ability of UV to create predictable venvs with the right versions installed, yeah I agree. Also if it's relevant I think every* test in the task SDK will be stateless and could be parallelized - i.e. they are all non-db style tests\r\n\r\nYeah and starting to have it in a freshly created venv without any DB or whatever deps for Task SDK is actually even GOOD because there we will not have even accidental dependencies that we missed."", 'created_at': datetime.datetime(2024, 10, 31, 12, 17, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452372418, 'issue_id': 2560141133, 'author': 'o-nikolas', 'body': '#### Thoughts on your problem statement: \r\n> Now we have three test folders for unit tests: ""tests"", ""providers/tests"", ""task_sdk/tests""\r\nDo we want to include `helm_tests/` in that list to make it 4?\r\n\r\n> Also the same breeze testing tests command is used to run ""helm"" unit tests\r\n\r\nAlso system tests are run by the breeze testing tests command as of now which caused some confusion and issues recently. I think that\'s worth having a bullet point for in your explanation comment.\r\n\r\n#### Thoughts on proposal:\r\n> and introduce two more testing commands:\r\n\r\nAgain, should we make system tests a top level command as well (`breeze testing system-tests`)? There wouldn\'t be much there now but I think it\'s a nice future proof solution. Because right now the system test stuff works in a very messy way, and I think making it a top level command would add much needed separation.\r\n\r\nOtherwise I like the plan!', 'created_at': datetime.datetime(2024, 11, 1, 18, 26, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452561821, 'issue_id': 2560141133, 'author': 'potiuk', 'body': ""> Also system tests are run by the breeze testing tests command as of now which caused some confusion and issues recently. I think that's worth having a bullet point for in your explanation comment.\r\n\r\nAgree. Will Update the plan accordingly."", 'created_at': datetime.datetime(2024, 11, 1, 20, 42, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452570547, 'issue_id': 2560141133, 'author': 'potiuk', 'body': 'I updated it. I added something that was long time overdue - ""dry run system tests"". While we wil not be able to run all the system tests from Amazon/Google/Astronomer/Teradata/Microsoftt in the future, we should be able to have some ""canonical"" non-externally-bound system tests that we should run to ""mimic"" what Amazon/Google/Astronomer/Teradata/Microsoft in the future will be doing to make sure that at least ""execution framework"" works.', 'created_at': datetime.datetime(2024, 11, 1, 20, 49, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2462148046, 'issue_id': 2560141133, 'author': 'potiuk', 'body': 'I updated it following the discussion on slack - adding the stage where we get rid of all db and only use task sdk in all provider tests (stage 3)', 'created_at': datetime.datetime(2024, 11, 7, 12, 44, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2463646400, 'issue_id': 2560141133, 'author': 'gopidesupavan', 'body': '@potiuk Going to attempt this one  stage 1: `Providers Non-DB tests` over this weekend, would you have an suggestions or thoughts for this, at present am thinking just have similar setup like task_sdk as mentioned above it is good example direction?', 'created_at': datetime.datetime(2024, 11, 8, 2, 43, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2463647874, 'issue_id': 2560141133, 'author': 'gopidesupavan', 'body': '> I updated it following the discussion on slack - adding the stage where we get rid of all db and only use task sdk in all provider tests (stage 3)\r\n\r\nyeah that make sense. :)', 'created_at': datetime.datetime(2024, 11, 8, 2, 45, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2468060751, 'issue_id': 2560141133, 'author': 'potiuk', 'body': '> @potiuk Going to attempt this one stage 1: `Providers Non-DB tests` over this weekend, would you have an suggestions or thoughts for this, at present am thinking just have similar setup like task_sdk as mentioned above it is good example direction?\r\n\r\nSorry - only see it now (Hackathon) I thought about having completely separate `test-` command for each test type. \r\n\r\n* `breeze testing core-tests-db`\r\n* `breeze testing core-tests-non-db`\r\n* `breeze testing providers-tests-db`\r\n* `breeze testing providers-tests-non-db`\r\n* `breeze testing providers-task-sdk`\r\n* `breeze testing system`\r\n\r\nThen each of the tests will have it\'s own (specific for the test command) set of test types it can use - core tests will have all the ""airflow sub-tests"", providers will be able to use `Providers` or `Providers[google]` or `Providers[-amazon,-google]` as they can do today. Task-sdk will not have any type. System tests will have the type based on which provider system tests they are on.\r\n\r\nDoing it this way has the nice effect that you can easily parallelise them as needed following our paralllel framework but also when you enter breeze with `breeze`  command you should be able to run any test or group of tests by just `pytest tests/*` or `pytest providers/tests/*` (both DB and non-DB tests should also work when DB is available). Test type would only be used when you want to run a predefined ""set"" of tests from outside. \r\n\r\nI actually have a draft version of this in my local branch, already and I wanted to make a PR today - so if you have not started on it, I might want to set it up for review for you :D', 'created_at': datetime.datetime(2024, 11, 11, 12, 27, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469174262, 'issue_id': 2560141133, 'author': 'gopidesupavan', 'body': '> > @potiuk Going to attempt this one stage 1: `Providers Non-DB tests` over this weekend, would you have an suggestions or thoughts for this, at present am thinking just have similar setup like task_sdk as mentioned above it is good example direction?\r\n> \r\n> Sorry - only see it now (Hackathon) I thought about having completely separate `test-` command for each test type.\r\n> \r\n> * `breeze testing core-tests-db`\r\n> * `breeze testing core-tests-non-db`\r\n> * `breeze testing providers-tests-db`\r\n> * `breeze testing providers-tests-non-db`\r\n> * `breeze testing providers-task-sdk`\r\n> * `breeze testing system`\r\n> \r\n> Then each of the tests will have it\'s own (specific for the test command) set of test types it can use - core tests will have all the ""airflow sub-tests"", providers will be able to use `Providers` or `Providers[google]` or `Providers[-amazon,-google]` as they can do today. Task-sdk will not have any type. System tests will have the type based on which provider system tests they are on.\r\n> \r\n> Doing it this way has the nice effect that you can easily parallelise them as needed following our paralllel framework but also when you enter breeze with `breeze` command you should be able to run any test or group of tests by just `pytest tests/*` or `pytest providers/tests/*` (both DB and non-DB tests should also work when DB is available). Test type would only be used when you want to run a predefined ""set"" of tests from outside.\r\n> \r\n> I actually have a draft version of this in my local branch, already and I wanted to make a PR today - so if you have not started on it, I might want to set it up for review for you :D\r\n\r\nNo worries :). super thats great if you have it already, please go ahead, have not yet started, actually was looking over the weekend, backporting and testcontainer things.\r\n\r\nTestcontainers is incredibly helpful for reducing the need for numerous Docker files and configurations. It’s straightforward to use (we can define some test kind of interfaces to extend or implement to any testconatainer in our tests) —I experimented with Kafka and Airflow, and while it’s not yet fully functional setup, I managed to get it to a point where it connects a Kafka Testcontainer and runs a DAG from my local setup. I think this is definitely worth using. will try this runnig with GHA to see how it performs :)', 'created_at': datetime.datetime(2024, 11, 11, 22, 11, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469334203, 'issue_id': 2560141133, 'author': 'potiuk', 'body': '> Testcontainers is incredibly helpful for reducing the need for numerous Docker files and configurations. It’s straightforward to use (we can define some test kind of interfaces to extend or implement to any testconatainer in our tests) —I experimented with Kafka and Airflow, and while it’s not yet fully functional setup, I managed to get it to a point where it connects a Kafka Testcontainer and runs a DAG from my local setup. I think this is definitely worth using. will try this runnig with GHA to see how it performs :)\r\n\r\ncool!', 'created_at': datetime.datetime(2024, 11, 12, 0, 14, 30, tzinfo=datetime.timezone.utc)}]","gopidesupavan (Assginee) on (2024-10-01 23:02:58 UTC): @potiuk I am interested in this should I take this on? Might take bit time to workout this, this area new to me , but this will help me to understand more on the airflow ci/cd . :)

potiuk (Issue Creator) on (2024-10-31 08:54:40 UTC): It's going to be a little more complex it seems - we likely wil have to redesign a bit the way how tests are executed in CI with different  test ""groups"" now so i will start a wider discussion on slack channel to discuss how to do it and involve others.

gopidesupavan (Assginee) on (2024-10-31 09:24:29 UTC): Absolutely! I just began analyzing this yesterday to see which components can be grouped or not. I noticed some existing options, like skip-provider-tests, and now that we have the task SDK, we may need to support this for executing tests or providing skipping options as well. I completely agree that some redesign is necessary.

potiuk (Issue Creator) on (2024-10-31 11:44:06 UTC): Hey Here. I would like to start a discussion and propose some redesign of our CI - to address some of the issues mentioned here - and involve others so that we come up together with a better design and then we get it implemented together.

First let me state the problems and what is wrong currently. And I will write a separte comment on what would be the first proposal to address it.

* Before #42505 and #43319 all the tests we had were in ""tests"" folder. Now we have three test folders for unit tests: ""tests"", ""providers/tests"", ""task_sdk/tests"".

Also ""system"" tests are a separate type - but they are under ""tests"" in ""airflow"" and ""provider' tests.

* Those tests cannot be run together in single pytest command - because all of them have a conftest.py file at the top folder and pytest refuses to run them together because it does not know which `conftest.py` is the ""top-level"" one.

* Currently we have ""non-db-tests"" that runs all the ""non-db"" tests together in a single xdist/parallel fashion and it currenlty runs together ""tests"" and ""providers/tests"" together - but this is really thanks to a workaround for it in ""providers"" (there is an `__init__.py` in ""providers"" folder that makes the test belong to package ""providers.tests"" - but this is really a hack because ""providers"" is where the whole ""providers"" project is, it's not a python ""package"".

* Both ""airflow"" and ""providers"" tests  are currently run with `breeze testing tests` command (which by default used to run ALL tests (""airflow + providers"") and only when you specified particular packages or modules to run as additional arguments woudl run only those tests you specified. This was a bit ""hacky"" approach how it was done and one of the reasons why running system tests were broken after provider's separation (i *temporarily* fixed it in #43529 but this is really a band-aid). 

* Also the same `breeze testing tests` command is used to run ""helm"" unit tests  -  and there is another ""hack"" implemented  - we are also using breeze container to run the tests but  ""helm"" tests do not need to be run in breeze container at all - they need a small local venv we already have from k8s where `helm` is installed - and we can run them there

* Actually `airflow` and `task_sdk` tests technically (eventually when we fully remove all provider references from airflow core and tests) will not have to use breeze container as well. With UV we could technicaly run the tests in a venv created dynamically in CI (with main constraints / `uv lock` for reproducibility) rather than using the CI image - because we do not need alll the 700+ deps - they should be ""reproducible-enough"" without the CI imaage of ours. 

* We alredy have separate ""TaskSDK tests"" that are running in the same environment as ""Non-DB tests"" - but only for ""task_sdk"" and they have separate `breeze testing task-sdk-tests` - it looks like a good direction,

potiuk (Issue Creator) on (2024-10-31 11:54:35 UTC): My proposal (high level) - it needs some detailed thinking and prototyping regarding the parallel execution of tests in CI is to follow the `breeze testing task-sdk-tests` and introduce three more testing commands:

* `breeze testing provider-tests` command
* `breeze testing helm-tests` command
* `breeze testint system-tests` command

This means that we will loose some of the parallelism benefifts when we run `provider` tests and `airflow` tests together (but instead we will increase a number of jobs we have in CI, whch we still should be able to do without exceeding the limits that INFRA has on a number of parallel jobs we can run in CI jobs).

We can also gradually move tests to local venvs where they do not need database containers from docker compose and Pytthon / System dependencies that we currently standardise in the CI image.

Also System tests should have a 'dry-run"" mode - where we can run them as part of the CI so that we know that they ar e not broken. We are not able to run system tests for all external systems, but at least we should be able to have some ""dry-run"" system tests that we should be able to run with all PRs.

So high level we have:

##  NOW

### CI image bound tests:

* Airflow + Providers DB tests
* Airflow + Providers Non-DB Tests
* TaskSDK Non-DB tests (no DB tests here)
* Helm Unit local virtualenv tests

System tests are not currently executed.

And we can do it this way:

## ~Stage 1~  :green_circle:   COMPLETED by #43979

~Likely we could attempt to make TaskSDK and Helm tests to run in venv rather than in CI image even now - I think Task SDK  (@kaxil @ashb ?) there are very little dependencies there and we could easily make a local venv there instead of the whole CI image as there are no specific system or provider dependencies there.~

~Also spliting out tests to ""Airflow"" and ""Providers"" without moving them out of CI image will hel us to address the issues we have now with conftest.py and actual ""separation"" of the different test groups.~

### ~CI image bound tests:~

~* Core DB tests~
~* Core Non-DB Tests~
~* Providers DB tests~
~* Providers Non-DB tests~
~* System tests dry-run tests~

### Local virtualenv tests

~* Helm Unit local virtualenv tests~
~* TaskSDK Non-DB tests~

---------------------------------

## Stage 2 

When we remove all the remaining references to providers from Airflow we might move Airflow Non-DB tests to local virtualenv.

### CI image bound tests:

* Core DB tests
* Providers DB tests
* Providers Non-DB tests
* Core System tests dry-run tests 
* Providers System tests dry-run tests 

### Local virtualenv tests:

* Core Non-DB Tests
* TaskSDK Non-DB tests (no DB tests here)
* Helm Unit local virtualenv tests

---------------------------------

## Stage 3

We switch the providers tests to use task sdk - and at this stage all db tests should be converted to non-db tests.

### CI image bound tests:


* Providers Non-DB tests
* System tests dry-run tests 
* Core DB tests

### Local virtualenv tests:


* Core Non-DB Tests
* TaskSDK Non-DB tests (no DB tests here)
* Helm Unit local virtualenv tests

We will still need provider tests to run in CI image and use the image to calculate constraints etc. - for quite some time (maybe for ever) but I see a clear path and stages we could take to simplify the ""Airflow"" and Task SDK ones.


---------------------------------

## Stage 4

When we add the capability of testcontainers (see #43514)  the db tests might eventually be run in local virtualenv


### CI image bound tests:


* Providers Non-DB tests
* System tests dry-run tests 

### Local virtualenv tests:

* Core DB tests
* Core Non-DB Tests
* TaskSDK Non-DB tests (no DB tests here)
* Helm Unit local virtualenv tests

We will still need provider tests to run in CI image and use the image to calculate constraints etc. - for quite some time (maybe for ever) but I see a clear path and stages we could take to simplify the ""Airflow"" and Task SDK ones.

potiuk (Issue Creator) on (2024-10-31 12:01:37 UTC): I made a few refinements to the initial proposal

ashb on (2024-10-31 12:11:48 UTC): Yes, there should be no deps on a any providers, not even the new standard provider as all of those need to depend on the SDK.

Given the ability of UV to create predictable venvs with the right versions installed, yeah I agree. Also if it's relevant I think every* test in the task SDK will be stateless and could be parallelized - i.e. they are all non-db style tests

* Might not be 100% true, but my gut says it is

potiuk (Issue Creator) on (2024-10-31 12:17:28 UTC): Yeah and starting to have it in a freshly created venv without any DB or whatever deps for Task SDK is actually even GOOD because there we will not have even accidental dependencies that we missed.

o-nikolas on (2024-11-01 18:26:59 UTC): #### Thoughts on your problem statement: 
Do we want to include `helm_tests/` in that list to make it 4?


Also system tests are run by the breeze testing tests command as of now which caused some confusion and issues recently. I think that's worth having a bullet point for in your explanation comment.

#### Thoughts on proposal:

Again, should we make system tests a top level command as well (`breeze testing system-tests`)? There wouldn't be much there now but I think it's a nice future proof solution. Because right now the system test stuff works in a very messy way, and I think making it a top level command would add much needed separation.

Otherwise I like the plan!

potiuk (Issue Creator) on (2024-11-01 20:42:49 UTC): Agree. Will Update the plan accordingly.

potiuk (Issue Creator) on (2024-11-01 20:49:06 UTC): I updated it. I added something that was long time overdue - ""dry run system tests"". While we wil not be able to run all the system tests from Amazon/Google/Astronomer/Teradata/Microsoftt in the future, we should be able to have some ""canonical"" non-externally-bound system tests that we should run to ""mimic"" what Amazon/Google/Astronomer/Teradata/Microsoft in the future will be doing to make sure that at least ""execution framework"" works.

potiuk (Issue Creator) on (2024-11-07 12:44:46 UTC): I updated it following the discussion on slack - adding the stage where we get rid of all db and only use task sdk in all provider tests (stage 3)

gopidesupavan (Assginee) on (2024-11-08 02:43:51 UTC): @potiuk Going to attempt this one  stage 1: `Providers Non-DB tests` over this weekend, would you have an suggestions or thoughts for this, at present am thinking just have similar setup like task_sdk as mentioned above it is good example direction?

gopidesupavan (Assginee) on (2024-11-08 02:45:18 UTC): yeah that make sense. :)

potiuk (Issue Creator) on (2024-11-11 12:27:24 UTC): Sorry - only see it now (Hackathon) I thought about having completely separate `test-` command for each test type. 

* `breeze testing core-tests-db`
* `breeze testing core-tests-non-db`
* `breeze testing providers-tests-db`
* `breeze testing providers-tests-non-db`
* `breeze testing providers-task-sdk`
* `breeze testing system`

Then each of the tests will have it's own (specific for the test command) set of test types it can use - core tests will have all the ""airflow sub-tests"", providers will be able to use `Providers` or `Providers[google]` or `Providers[-amazon,-google]` as they can do today. Task-sdk will not have any type. System tests will have the type based on which provider system tests they are on.

Doing it this way has the nice effect that you can easily parallelise them as needed following our paralllel framework but also when you enter breeze with `breeze`  command you should be able to run any test or group of tests by just `pytest tests/*` or `pytest providers/tests/*` (both DB and non-DB tests should also work when DB is available). Test type would only be used when you want to run a predefined ""set"" of tests from outside. 

I actually have a draft version of this in my local branch, already and I wanted to make a PR today - so if you have not started on it, I might want to set it up for review for you :D

gopidesupavan (Assginee) on (2024-11-11 22:11:57 UTC): No worries :). super thats great if you have it already, please go ahead, have not yet started, actually was looking over the weekend, backporting and testcontainer things.

Testcontainers is incredibly helpful for reducing the need for numerous Docker files and configurations. It’s straightforward to use (we can define some test kind of interfaces to extend or implement to any testconatainer in our tests) —I experimented with Kafka and Airflow, and while it’s not yet fully functional setup, I managed to get it to a point where it connects a Kafka Testcontainer and runs a DAG from my local setup. I think this is definitely worth using. will try this runnig with GHA to see how it performs :)

potiuk (Issue Creator) on (2024-11-12 00:14:30 UTC): cool!

"
2559593324,issue,closed,completed,AIP-84 Migrate /object/historical_metrics_data from views to FastAPI,"### Description

Migrate views `/object/historical_metrics_data` to Fast API.



### Use case/motivation

[AIP-84](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API)

### Related issues

Relates to: https://github.com/apache/airflow/issues/42366

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-10-01 15:30:24+00:00,[],2024-10-09 08:09:41+00:00,2024-10-09 08:09:41+00:00,https://github.com/apache/airflow/issues/42623,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]",[],
2559042861,issue,open,,Add SSMRunCommandOperator,"### Description

Amazon Systems Manager (SSM) is used by system admins to manage AWS resources like EC2. It also provided ability for companies to run scripts on the EC2 machines securely using it's RunCommand feature. Adding SSM RunCommand Operator to the Amazon provider package will unlock migration to Airflow from legacy job scheduling frameworks like AutoSys

### Use case/motivation

I work with a few customers that are in the process of migrating jobs from AutoSys and other legacy frameworks to Airflow. For these customers, a lot of their existing jobs are in shell scripts running on EC2. When they migrate to Airflow, they are looking for an easy way to lift those scripts and run on Airflow. SSHOperator could be an option for this but in many corporate environments, SSH is disabled for security reasons. One of the secure ways AWS recommends executing a remote script on EC2 is by using SSM RunCommand.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",muraria,2024-10-01 12:12:45+00:00,['Shlomit-B'],2024-11-26 17:41:19+00:00,,https://github.com/apache/airflow/issues/42619,"[('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('kind:feature', 'Feature Requests'), ('good first issue', '')]","[{'comment_id': 2385614997, 'issue_id': 2559042861, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 1, 12, 12, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2397672754, 'issue_id': 2559042861, 'author': 'mr1holmes', 'body': '@vincbeck  I would like to work on this, can you please assign it to me', 'created_at': datetime.datetime(2024, 10, 7, 19, 3, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2397680558, 'issue_id': 2559042861, 'author': 'vincbeck', 'body': 'Sure!', 'created_at': datetime.datetime(2024, 10, 7, 19, 8, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2471440376, 'issue_id': 2559042861, 'author': 'shubham22', 'body': ""@mr1holmes - curious if you're still working on it?"", 'created_at': datetime.datetime(2024, 11, 12, 19, 52, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2472278460, 'issue_id': 2559042861, 'author': 'mr1holmes', 'body': ""Sorry \nI didn't get time to work on it.\nYou can assign it to someone else. \nApologies"", 'created_at': datetime.datetime(2024, 11, 13, 3, 9, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2500384289, 'issue_id': 2559042861, 'author': 'Shlomit-B', 'body': ""@eladkal @vincbeck - Hey, I would like to work on this one if that's ok"", 'created_at': datetime.datetime(2024, 11, 26, 11, 25, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2501565191, 'issue_id': 2559042861, 'author': 'vincbeck', 'body': 'Sure :)', 'created_at': datetime.datetime(2024, 11, 26, 17, 41, 13, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-01 12:12:48 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

mr1holmes on (2024-10-07 19:03:58 UTC): @vincbeck  I would like to work on this, can you please assign it to me

vincbeck on (2024-10-07 19:08:17 UTC): Sure!

shubham22 on (2024-11-12 19:52:19 UTC): @mr1holmes - curious if you're still working on it?

mr1holmes on (2024-11-13 03:09:04 UTC): Sorry 
I didn't get time to work on it.
You can assign it to someone else. 
Apologies

Shlomit-B (Assginee) on (2024-11-26 11:25:02 UTC): @eladkal @vincbeck - Hey, I would like to work on this one if that's ok

vincbeck on (2024-11-26 17:41:13 UTC): Sure :)

"
2558953754,issue,closed,not_planned,Wrong task order in Airflow UI Grid,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.7.2

### What happened?

I have a problem with the order of tasks after adding the branch operator. After adding it, the tasks on the ‘Grid’ view have an incorrect order, while on the ‘Graph’ they look fine and are run in the correct order. Tasks after branch operator are placed at the bottom

### What you think should happen instead?

![image](https://github.com/user-attachments/assets/f8a638da-8075-41ff-a9db-36e19b417fc0)


### How to reproduce
from airflow.decorators import dag
from pendulum import today
from airflow.operators.empty import EmptyOperator
from airflow.decorators import task
from airflow.exceptions import AirflowSkipException
from airflow.utils.state import State
import random


@dag(
    dag_id='grid_bug',
    # For now it's hardcoded, maybe we will use this in a future ?
    start_date=today(""UTC"").add(days=-31),
    schedule=None,
    is_paused_upon_creation=True,
    # For now it's hardcoded, maybe we will use this in a future ?
    catchup=False,
    render_template_as_native_obj=False
)
def dynamic_generated_dag():
    first_task = EmptyOperator(task_id='first_task')
    second_task = EmptyOperator(task_id='second_task')
    end_task = EmptyOperator(task_id='end_task')

    first_task >> second_task
    for i in range(1,10):

        # Data ingestion task
        data_ingestion_task = EmptyOperator(task_id=f'data_ingestion_{i}')

        @task(task_id=f'skipping_task_{i}')
        def skipping_task_func():
            if random.randint(0, 1) == 1:
                raise AirflowSkipException('')
        
        # Skipping task (randomly)
        skipping_task = skipping_task_func()
        second_task >> data_ingestion_task >> skipping_task

        # Baseline task
        #if execute_baseline:
        baseline_task = EmptyOperator(task_id=f'baseline_{i}')
        current_flow = skipping_task >> baseline_task

        # Soft delete task
        soft_delete_task = EmptyOperator(task_id=f'soft_delete_{i}', trigger_rule='none_failed')
        current_flow = current_flow >> soft_delete_task

        # Update delta
        update_delta_task = EmptyOperator(task_id=f'update_delta_{i}')

        # Save delta
        save_delta_task = EmptyOperator(task_id=f'save_delta_{i}')

        # Dummy task to skip for branch task
        dummy_task = EmptyOperator(task_id=f'dummy_{i}')

        @task.branch(task_id=f'is_delta_to_execute_{i}')
        def choose_flow(upstream_task_id, follow_if_true, follow_if_false, dag_run=None):
            upstream_task_state = dag_run.get_task_instance(upstream_task_id).state
            if upstream_task_state == State.SUCCESS:
                return follow_if_true
            else:
                return follow_if_false
            
        choose_flow_branch_task = choose_flow(
            upstream_task_id=f'skipping_task_{i}'
            , follow_if_true=[update_delta_task.task_id, save_delta_task.task_id]
            , follow_if_false=dummy_task.task_id
        )

        # First branch logic to follow
        current_flow >> choose_flow_branch_task >> update_delta_task >> save_delta_task >> end_task
        # Second branch logic to follow
        current_flow >> choose_flow_branch_task >> dummy_task >> end_task


dynamic_generated_dag()

### Operating System

AKS 

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",BT2,2024-10-01 11:30:23+00:00,[],2024-11-24 00:17:28+00:00,2024-11-24 00:17:27+00:00,https://github.com/apache/airflow/issues/42617,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('pending-response', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2385532070, 'issue_id': 2558953754, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 1, 11, 30, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453111712, 'issue_id': 2558953754, 'author': 'jscheffl', 'body': 'You reported this on Airflow 2.7.2, can you re-test on the most recent version, currently 2.10.2?', 'created_at': datetime.datetime(2024, 11, 2, 19, 59, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2480862629, 'issue_id': 2558953754, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 11, 17, 0, 17, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2495713870, 'issue_id': 2558953754, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 11, 24, 0, 17, 27, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-01 11:30:26 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jscheffl on (2024-11-02 19:59:21 UTC): You reported this on Airflow 2.7.2, can you re-test on the most recent version, currently 2.10.2?

github-actions[bot] on (2024-11-17 00:17:05 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-11-24 00:17:27 UTC): This issue has been closed because it has not received response from the issue author.

"
2558943684,issue,closed,completed,Help Needed: Running Airflow DAG with Dependencies in multiple Conda Environments,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?


Hi, I’m seeking assistance with my Airflow setup. My Airflow instance runs in an environment named `prod_env`, but the modules required for a particular DAG only work in a separate Conda environment named  (`ind_env`). 

I require  guidance on managing dependencies across different environments to work within  Airflow 


### What you think should happen instead?

_No response_

### How to reproduce

install airflow in one environment and create some dags,and create another environment and there you have some python modules related to a particular dag ;you have to call them in ur current environment.Remember the python packages of ur cuurent environment not directly support those packages currently setup in another environment.

### Operating System

Linux

### Versions of Apache Airflow Providers

apache-airflow==2.10.2
apache-airflow-providers-amazon==8.28.0
apache-airflow-providers-celery==3.8.1
apache-airflow-providers-common-compat==1.2.0
apache-airflow-providers-common-io==1.4.0
apache-airflow-providers-common-sql==1.14.2
apache-airflow-providers-elasticsearch==5.5.0
apache-airflow-providers-fab==1.2.2
apache-airflow-providers-ftp==3.11.0
apache-airflow-providers-google==10.21.1
apache-airflow-providers-http==4.12.0
apache-airflow-providers-imap==3.6.1
apache-airflow-providers-smtp==1.7.1
apache-airflow-providers-sqlite==3.8.1


### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",HarikrishnanK9,2024-10-01 11:26:21+00:00,[],2024-10-02 02:46:19+00:00,2024-10-02 02:46:19+00:00,https://github.com/apache/airflow/issues/42616,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2385523864, 'issue_id': 2558943684, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 1, 11, 26, 23, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-01 11:26:23 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2558884948,issue,closed,completed,GCSToGCSOperator is not able to copy files with wildcard to destination bucket in airflow 2.7.3 version,"### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

apache-airflow-providers-google==10.12.0

### Apache Airflow version

2.7.3

### Operating System

Debian GNU/Linux 11 (bullseye)

### Deployment

Astronomer

### Deployment details

_No response_

### What happened

_No response_

### What you think should happen instead

_No response_

### How to reproduce

`from datetime import timedelta, datetime
from airflow import DAG
from airflow.providers.google.cloud.transfers.gcs_to_gcs import GCSToGCSOperator
 
CONN_ID = 'bfddxvenu1-dev-sa1'
SOURCE_BUCKET =  'bfdaf-dags-bfddxvenu1-test-sm'
DESTINATION_BUCKET =  'bfdaf-dags-bfddxvenu1-test-sm'
 
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2022, 1, 1),
    'email': ['shyam.shaw@gmail.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}
 
dag = DAG(
    'move_gcs_files',
    default_args=default_args,
    description='A simple DAG to move files from one GCS bucket to another',
    schedule_interval=None,
)
 
move_files = GCSToGCSOperator(
    task_id='move_files',
    source_bucket=SOURCE_BUCKET,
    source_object='load/incremental/ca/*.csv',
    destination_bucket=DESTINATION_BUCKET,
    destination_object='processing/incremental/ca/',
    move_object=True,
    dag=dag,
    gcp_conn_id=CONN_ID
)
 
move_files`

Airflow task is marking as SUCCESS however files are not getting copied to destination bucket. This works fine with airflow 2.4.3 version and apache-airflow-providers-google==8.4.0

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",shyamshaw,2024-10-01 10:59:18+00:00,[],2024-10-03 13:54:00+00:00,2024-10-03 13:54:00+00:00,https://github.com/apache/airflow/issues/42615,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('area:providers', ''), ('pending-response', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2385471962, 'issue_id': 2558884948, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 10, 1, 10, 59, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2385712898, 'issue_id': 2558884948, 'author': 'RNHTTR', 'body': '`apache-airflow-providers-google==10.12.0` is almost a year old. After connecting with @kandharvishnu, I believe this issue has been resolved in `apache-airflow-providers-google>=10.18.0` -- Is that correct?', 'created_at': datetime.datetime(2024, 10, 1, 12, 57, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2386205377, 'issue_id': 2558884948, 'author': 'shyamshaw', 'body': '@RNHTTR \r\n\r\nYes, it seems issue has been resolved in `apache-airflow-providers-google>=10.18.0` \r\nI could not find any bug or behaviour changes related to this issue as it was working in previous version of `apache-airflow-providers-google`', 'created_at': datetime.datetime(2024, 10, 1, 14, 45, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2391488280, 'issue_id': 2558884948, 'author': 'RNHTTR', 'body': ""Got it. Since this is already fixed, I'm going to close this out."", 'created_at': datetime.datetime(2024, 10, 3, 13, 54, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-10-01 10:59:20 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

RNHTTR on (2024-10-01 12:57:18 UTC): `apache-airflow-providers-google==10.12.0` is almost a year old. After connecting with @kandharvishnu, I believe this issue has been resolved in `apache-airflow-providers-google>=10.18.0` -- Is that correct?

shyamshaw (Issue Creator) on (2024-10-01 14:45:37 UTC): @RNHTTR 

Yes, it seems issue has been resolved in `apache-airflow-providers-google>=10.18.0` 
I could not find any bug or behaviour changes related to this issue as it was working in previous version of `apache-airflow-providers-google`

RNHTTR on (2024-10-03 13:54:00 UTC): Got it. Since this is already fixed, I'm going to close this out.

"
2558487045,issue,closed,completed,Redirect loop on /home with session-persisted tags/lastrun filter,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When I go to /home
And I filter by tag (e.g. example)
And I filter by lastrun (e.g. `Running`)
When I go to /home (no filter parameters in the URL)
Then I end up in a redirect loop

This issue was introduced with https://github.com/apache/airflow/pull/39701 and also mentioned in https://github.com/apache/airflow/issues/42111#issuecomment-2343146235.

### What you think should happen instead?

No redirect loop

### How to reproduce

See above

### Operating System

debian 12 (bookworm)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jmaicher,2024-10-01 08:07:30+00:00,[],2024-10-01 19:23:42+00:00,2024-10-01 19:23:42+00:00,https://github.com/apache/airflow/issues/42607,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2557217908,issue,closed,completed,AIP-84 Migrate /object/grid_data from views to FastAPI,"### Description

Migrate `views` `/object/grid_data` to Fast API.



### Use case/motivation

[AIP-84](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API)

### Related issues

Relates to: https://github.com/apache/airflow/issues/42366

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-09-30 17:07:41+00:00,['bugraoz93'],2024-12-23 14:57:27+00:00,2024-12-23 14:57:27+00:00,https://github.com/apache/airflow/issues/42595,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2387861309, 'issue_id': 2557217908, 'author': 'bbovenzi', 'body': 'I mentioned in the main epic how we want to refactor this endpoint. I think we can have `graph_data` handle all of the task order and structure as I mentioned in #42367. Then we can simplify `grid_data` to only include dag run history. Each dag run includes an array of all its task instances. \n\nAlso, it would be nice to calculate summaries of mapped tasks and task groups (determine the overall state, give a breakdown of counts of children states, determine earliest queued, earliest start_date and latest end_date). \n\nBy separting runs from the task structure we can add pagination and filtering to the`dag_runs_with_task_summaries` endpoint, then we can do stuff like only refresh on active dag runs.', 'created_at': datetime.datetime(2024, 10, 2, 8, 2, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465535964, 'issue_id': 2557217908, 'author': 'bugraoz93', 'body': ""@bbovenzi I am not sure if I grasp all the parts. I will work on this and create a PR soon. Let's discuss in the PR to make this work how you imagined"", 'created_at': datetime.datetime(2024, 11, 8, 18, 52, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465699153, 'issue_id': 2557217908, 'author': 'bbovenzi', 'body': ""That's alright. This is a complicated endpoint. Something like:\r\n\r\n```\r\n[{\r\n  version_number?: number\r\n  data_enterval_end?: string | null;\r\n  data_interval_start?: string | null;\r\n  end_date?: string | null;\r\n  run_id: string;\r\n  run_type: string;\r\n  queued_at?: string | null;\r\n  start_date?: string | null;\r\n  state: string;\r\n  task_instances?: TaskInstanceSummary[];\r\n}]\r\n```\r\n\r\n\r\nWhere the `TaskInstanceSummary` can be either a single task instance, or summarizing multiple tasks as a Task Group or Mapped Task. For a summary, we need:\r\n\r\n```\r\n  end_date: string | null; // latest end date\r\n  overall_state: string | null // Check `priority` in `airflow/www/utils.py`\r\n  start_date: string | null; // earliest start date\r\n  queued_at: string | null // earliest queued at date\r\n  task_count: number; // total number of children tasks inside this group or mapped task\r\n  states: // dict of states and the number of tasks with each state\r\n  task_id: string;\r\n  try_number: number;\r\n ```"", 'created_at': datetime.datetime(2024, 11, 8, 20, 29, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466240558, 'issue_id': 2557217908, 'author': 'bugraoz93', 'body': ""Thanks for the details! It's clear now."", 'created_at': datetime.datetime(2024, 11, 9, 14, 32, 52, tzinfo=datetime.timezone.utc)}]","bbovenzi on (2024-10-02 08:02:53 UTC): I mentioned in the main epic how we want to refactor this endpoint. I think we can have `graph_data` handle all of the task order and structure as I mentioned in #42367. Then we can simplify `grid_data` to only include dag run history. Each dag run includes an array of all its task instances. 

Also, it would be nice to calculate summaries of mapped tasks and task groups (determine the overall state, give a breakdown of counts of children states, determine earliest queued, earliest start_date and latest end_date). 

By separting runs from the task structure we can add pagination and filtering to the`dag_runs_with_task_summaries` endpoint, then we can do stuff like only refresh on active dag runs.

bugraoz93 (Issue Creator) on (2024-11-08 18:52:04 UTC): @bbovenzi I am not sure if I grasp all the parts. I will work on this and create a PR soon. Let's discuss in the PR to make this work how you imagined

bbovenzi on (2024-11-08 20:29:19 UTC): That's alright. This is a complicated endpoint. Something like:

```
[{
  version_number?: number
  data_enterval_end?: string | null;
  data_interval_start?: string | null;
  end_date?: string | null;
  run_id: string;
  run_type: string;
  queued_at?: string | null;
  start_date?: string | null;
  state: string;
  task_instances?: TaskInstanceSummary[];
}]
```


Where the `TaskInstanceSummary` can be either a single task instance, or summarizing multiple tasks as a Task Group or Mapped Task. For a summary, we need:

```
  end_date: string | null; // latest end date
  overall_state: string | null // Check `priority` in `airflow/www/utils.py`
  start_date: string | null; // earliest start date
  queued_at: string | null // earliest queued at date
  task_count: number; // total number of children tasks inside this group or mapped task
  states: // dict of states and the number of tasks with each state
  task_id: string;
  try_number: number;
 ```

bugraoz93 (Issue Creator) on (2024-11-09 14:32:52 UTC): Thanks for the details! It's clear now.

"
2557208948,issue,closed,completed,AIP-84 Migrate the public endpoint Test Connection to FastAPI,"### Description

Migrate the 'POST' test connection to Fast API.



### Use case/motivation

[AIP-84](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API)

### Related issues

Relates to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-09-30 17:03:42+00:00,['bugraoz93'],2024-11-12 17:00:17+00:00,2024-11-12 17:00:17+00:00,https://github.com/apache/airflow/issues/42594,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]",[],
2557205271,issue,closed,completed,AIP-84 Migrate the public endpoint Post Connection to FastAPI ,"### Description

Migrate the 'POST' connection to Fast API.



### Use case/motivation

[AIP-84](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API)

### Related issues

Relates to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-09-30 17:02:17+00:00,['bugraoz93'],2024-11-05 15:29:19+00:00,2024-11-05 15:29:19+00:00,https://github.com/apache/airflow/issues/42593,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]",[],
2557203107,issue,closed,completed,AIP-84 Migrate the public endpoint Patch Connection to FastAPI,"### Description

Migrate the 'PATCH' connection to Fast API.



### Use case/motivation

[AIP-84](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API)

### Related issues

Relates to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-09-30 17:01:37+00:00,['bugraoz93'],2024-11-06 08:43:57+00:00,2024-11-06 08:43:57+00:00,https://github.com/apache/airflow/issues/42592,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]",[],
2557201538,issue,closed,completed,AIP-84 Migrate the public endpoint Get Connections to FastAPI ,"### Description

Migrate the 'GET' connections to Fast API.



### Use case/motivation

[AIP-84](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API)

### Related issues

Relates to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-09-30 17:00:47+00:00,['bugraoz93'],2024-10-15 16:06:00+00:00,2024-10-15 16:05:59+00:00,https://github.com/apache/airflow/issues/42591,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]",[],
2557199887,issue,closed,completed,AIP-84 Migrate the public endpoint Get Connection to FastAPI,"### Description

Migrate the 'GET' connection to Fast API.



### Use case/motivation

[AIP-84](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API)

### Related issues

Relates to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-09-30 16:59:52+00:00,['rawwar'],2024-10-04 07:26:37+00:00,2024-10-04 07:26:37+00:00,https://github.com/apache/airflow/issues/42590,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2389367893, 'issue_id': 2557199887, 'author': 'rawwar', 'body': 'I would like to work on this issue.', 'created_at': datetime.datetime(2024, 10, 2, 18, 15, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2390867036, 'issue_id': 2557199887, 'author': 'pierrejeambrun', 'body': '@rawwar Thanks for your PR. I think @bugraoz93 is taking care of other `Connection` related endpoints. There are plenty of other endpoints to migrate and we need your help, you can find them under `api_connexion/endpoints` just pick whichever you like, besides `dag` and `connection` everything is up for a grab. :)\r\n\r\nLet me know if you want to work on any of them so I can assign you :)\r\n\r\n\r\n@bugraoz93 I assigned you all the other tickets related to the `Connection` endpoint. (As requested, sorry for the delay)', 'created_at': datetime.datetime(2024, 10, 3, 8, 50, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2390956685, 'issue_id': 2557199887, 'author': 'rawwar', 'body': ""I apologize for not looking through history and picking up this issue on a whim.  @bugraoz93 , I've initiated PRs to get a connection and list connection. I am closing the list of connections PR as that's just a draft. \r\n\r\n@pierrejeambrun , thanks so much!"", 'created_at': datetime.datetime(2024, 10, 3, 9, 34, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2391812531, 'issue_id': 2557199887, 'author': 'bugraoz93', 'body': 'Thanks @pierrejeambrun! (No problem :) :pray:) \r\n\r\nThanks for jumping in @rawwar! We need all the help we can get. No problem at all, there is a huge backlog we should cover anyway :)', 'created_at': datetime.datetime(2024, 10, 3, 16, 13, 43, tzinfo=datetime.timezone.utc)}]","rawwar (Assginee) on (2024-10-02 18:15:43 UTC): I would like to work on this issue.

pierrejeambrun on (2024-10-03 08:50:10 UTC): @rawwar Thanks for your PR. I think @bugraoz93 is taking care of other `Connection` related endpoints. There are plenty of other endpoints to migrate and we need your help, you can find them under `api_connexion/endpoints` just pick whichever you like, besides `dag` and `connection` everything is up for a grab. :)

Let me know if you want to work on any of them so I can assign you :)


@bugraoz93 I assigned you all the other tickets related to the `Connection` endpoint. (As requested, sorry for the delay)

rawwar (Assginee) on (2024-10-03 09:34:44 UTC): I apologize for not looking through history and picking up this issue on a whim.  @bugraoz93 , I've initiated PRs to get a connection and list connection. I am closing the list of connections PR as that's just a draft. 

@pierrejeambrun , thanks so much!

bugraoz93 (Issue Creator) on (2024-10-03 16:13:43 UTC): Thanks @pierrejeambrun! (No problem :) :pray:) 

Thanks for jumping in @rawwar! We need all the help we can get. No problem at all, there is a huge backlog we should cover anyway :)

"
2556437863,issue,closed,completed,one_success trigger_rule scheduling exception,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

![image](https://github.com/user-attachments/assets/f22b46c8-7937-40b3-93eb-cba748e9bf41)
The trigger_rule of `task_one_success` is `one_success`. When the upstream node of `task_one_success` has not yet run, `task_one_success` is skipped. According to the semantics of `one_success`, `task_one_success` should be able to run.

The final result is as follows
![image](https://github.com/user-attachments/assets/f8706536-88d9-4b9b-aea7-96bd33a7348f)

My code is
```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import BranchPythonOperator

with DAG(dag_id='error_test', schedule=None, default_args={}, ) as dag:
    branch = BranchPythonOperator(task_id='branch', python_callable=lambda: 'task_run')
    task_run = BashOperator(task_id='task_run', bash_command='echo 0')
    task_skip = BashOperator(task_id='task_skip', bash_command='echo 0')
    task_1 = BashOperator(task_id='task_1', bash_command='echo 0')
    task_one_success = BashOperator(task_id='task_one_success', bash_command='echo 0', trigger_rule='one_success')
    task_2 = BashOperator(task_id='task_2', bash_command='echo 0')

    task_1 >> task_2
    branch >> task_skip
    branch >> task_run
    task_run >> task_one_success
    task_skip >> task_one_success
    task_one_success >> task_2
    task_skip >> task_2
```


### What you think should happen instead?

_No response_

### How to reproduce

The task_one_success node should be run

### Operating System

centos and macos

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",luoyuliuyin,2024-09-30 12:00:03+00:00,[],2024-10-23 08:35:25+00:00,2024-10-23 08:35:25+00:00,https://github.com/apache/airflow/issues/42581,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('area:core', ''), ('affected_version:main_branch', 'Issues Reported for main branch')]",[],
2556264386,issue,closed,not_planned,Listener called twice with multiple schedulers in Airflow 2.7,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.7.1

### What happened?

When running two schedulers in Airflow, I have encountered an issue where the listener responsible for sending custom metrics using the StatsD exporter is triggered twice. This happens specifically when the DAG state changes, and the listener is called in both `on_dag_run_failed` and `on_dag_run_success`.

**Key Observations**:
1. The duplication only occurs when **two schedulers** are running. With a single scheduler, the bug does not occur.
2. If a task within a DAG fails while it is in the **queued** state, the bug does not occur, and the metric is sent correctly.
3. When a task fails while it is in the **running** state, the bug occurs, and the metric is sent twice.
4. The listener can be triggered twice either:
a) **On the same scheduler**, with a time difference of 5-10 seconds between calls
b) **On two separate schedulers**, nearly simultaneously.



### What you think should happen instead?

Listener should be called only once per event, even if two schedulers are running. The metric should only be sent once when the DAG state changes, such as on DAG success or failure.

### How to reproduce

1. Deploy Airflow 2.7.0 using the official **Helm chart** on **Kubernetes**, and configure two active schedulers.
2. Add a listener that sends a custom metric to a StatsD exporter during DAG state changes (`on_dag_run_failed`, `on_dag_run_success`).
3. Run DAGs with tasks that may fail in the running state and monitor the metrics sent during DAG state changes.
4. Observe that the listener is sometimes called twice, resulting in duplicate metric entries.

### Operating System

Ubuntu

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

Listener code:
```
@hookimpl
def on_dag_run_success(dag_run: DagRun, msg: str) -> None:
    """"""
    This method is called when dag run state changes to SUCCESS.
    """"""
    dag_id = dag_run.dag_id
    metrics.send_dags_starting_delay(dag_run=dag_run)

@hookimpl
def on_dag_run_failed(dag_run: DagRun, msg: str) -> None:
    """"""
    This method is called when dag run state changes to FAILED.
    """"""
    dag_id = dag_run.dag_id
    run_id = dag_run.run_id
    external_trigger = dag_run.external_trigger
    metrics.send_dags_starting_delay(dag_run=dag_run)
```

Metric sending code:
```
from airflow.stats import Stats

def send_dags_starting_delay(dag_run: DagRun) -> None:
    dag_id = dag_run.dag_id

    task_instanses: list[TaskInstance] = dag_run.task_instances
    tasks_queued_duration = timedelta()
    for ti in task_instanses:
        task_queued_duration = ti.start_date - ti.queued_dttm
        tasks_queued_duration += task_queued_duration

    Stats.timing(f""dagrun.tasks.starting.delay.{dag_id}"", tasks_queued_duration)
```

* The issue only occurs when two schedulers are running. It does not occur with a single scheduler.
* No issue arises when a task fails while queued. The duplication happens only when a task fails while running.
* I’m using a custom metric listener integrated with StatsD exporter for metric collection.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Mint2702,2024-09-30 10:45:16+00:00,[],2024-10-22 00:15:15+00:00,2024-10-22 00:15:14+00:00,https://github.com/apache/airflow/issues/42580,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('pending-response', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2382833576, 'issue_id': 2556264386, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 30, 10, 45, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2384106829, 'issue_id': 2556264386, 'author': 'jscheffl', 'body': ""Version 2.7 of Airflow is pretty old and un-maintained. I'd propose in general to update. There might numerous fixes and improvements since then.\r\n\r\nCan you test against the most recent Airflow 2.10 version?"", 'created_at': datetime.datetime(2024, 9, 30, 20, 40, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412570361, 'issue_id': 2556264386, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 10, 15, 0, 15, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2427963755, 'issue_id': 2556264386, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 10, 22, 0, 15, 14, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-30 10:45:19 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jscheffl on (2024-09-30 20:40:45 UTC): Version 2.7 of Airflow is pretty old and un-maintained. I'd propose in general to update. There might numerous fixes and improvements since then.

Can you test against the most recent Airflow 2.10 version?

github-actions[bot] on (2024-10-15 00:15:07 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-10-22 00:15:14 UTC): This issue has been closed because it has not received response from the issue author.

"
2555128657,issue,closed,completed,azure-kusto-data 4.6.0 breaks provider tests and static checks,"### Apache Airflow Provider(s)

microsoft-azure

### Versions of Apache Airflow Providers

See: https://github.com/apache/airflow/actions/runs/11093100295

Static checks fail in:
```
Found 3 errors in providers
Error: The `airflow.providers.microsoft.azure.hooks.adx.AzureDataExplorerHook` object in connection-types list in airflow/providers/microsoft/azure/provider.yaml does not exist or is not a class: No module named 'build'
Error: The `airflow.providers.microsoft.azure.operators.adx` object in operators list in airflow/providers/microsoft/azure/provider.yaml does not exist or is not a module: No module named 'build'
Error: The `airflow.providers.microsoft.azure.hooks.adx` object in hooks list in airflow/providers/microsoft/azure/provider.yaml does not exist or is not a module: No module named 'build'
Error 1 returned
```
...with traceback...
```
 Traceback (most recent call last):
  File ""/opt/airflow/scripts/in_container/run_template_fields_check.py"", line 174, in <module>
    [iter_check_template_fields(module) for module in modules_to_validate]
  File ""/opt/airflow/scripts/in_container/run_template_fields_check.py"", line 174, in <listcomp>
    [iter_check_template_fields(module) for module in modules_to_validate]
  File ""/opt/airflow/scripts/in_container/run_template_fields_check.py"", line 148, in iter_check_template_fields
    imported_module = importlib.import_module(module)
  File ""/usr/local/lib/python3.8/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1014, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 843, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/opt/airflow/airflow/providers/microsoft/azure/operators/adx.py"", line 30, in <module>
    from airflow.providers.microsoft.azure.hooks.adx import AzureDataExplorerHook
  File ""/opt/airflow/airflow/providers/microsoft/azure/hooks/adx.py"", line 33, in <module>
    from azure.kusto.data import ClientRequestProperties, KustoClient, KustoConnectionStringBuilder
  File ""/usr/local/lib/python3.8/site-packages/azure/kusto/data/__init__.py"", line 5, in <module>
    from .client import KustoClient
  File ""/usr/local/lib/python3.8/site-packages/azure/kusto/data/client.py"", line 17, in <module>
    from build.lib.azure.kusto.data.exceptions import KustoServiceError
ModuleNotFoundError: No module named 'build'
```

Provider tests fail in:
```

==================================== ERRORS ====================================
______ ERROR collecting tests/providers/microsoft/azure/hooks/test_adx.py ______
ImportError while importing test module '/opt/airflow/tests/providers/microsoft/azure/hooks/test_adx.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
/usr/local/lib/python3.8/importlib/__init__.py:127: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
tests/providers/microsoft/azure/hooks/test_adx.py:23: in <module>
    from azure.kusto.data import ClientRequestProperties, KustoClient
/usr/local/lib/python3.8/site-packages/azure/kusto/data/__init__.py:5: in <module>
    from .client import KustoClient
/usr/local/lib/python3.8/site-packages/azure/kusto/data/client.py:17: in <module>
    from build.lib.azure.kusto.data.exceptions import KustoServiceError
E   ModuleNotFoundError: No module named 'build'
____ ERROR collecting tests/providers/microsoft/azure/operators/test_adx.py ____
ImportError while importing test module '/opt/airflow/tests/providers/microsoft/azure/operators/test_adx.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
/usr/local/lib/python3.8/importlib/__init__.py:127: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
tests/providers/microsoft/azure/operators/test_adx.py:23: in <module>
    from azure.kusto.data._models import KustoResultTable
/usr/local/lib/python3.8/site-packages/azure/kusto/data/__init__.py:5: in <module>
    from .client import KustoClient
/usr/local/lib/python3.8/site-packages/azure/kusto/data/client.py:17: in <module>
    from build.lib.azure.kusto.data.exceptions import KustoServiceError
E   ModuleNotFoundError: No module named 'build'
- generated xml file: /files/test_result-providers_-amazon_google-postgres.xml -
=================== Warning summary. Total: 348, Unique: 334 ===================
other: total 347, unique 333
  collect: total 347, unique 333
tests: total 1, unique 1
  collect: total 1, unique 1
Warnings saved into /files/warnings-providers_-amazon_google-postgres.txt file.
=========================== short test summary info ============================
ERROR tests/providers/microsoft/azure/hooks/test_adx.py
ERROR tests/providers/microsoft/azure/operators/test_adx.py
!!!!!!!!!!!!!!!!!!! Interrupted: 2 errors during collection !!!!!!!!!!!!!!!!!!!!
========================= 2 errors in 68.63s (0:01:08) =========================
```

### Apache Airflow version

any

### Operating System

any

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### What happened

_No response_

### What you think should happen instead

_No response_

### How to reproduce

See https://github.com/apache/airflow/actions/runs/11093100295

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jscheffl,2024-09-29 19:56:06+00:00,[],2024-10-01 07:11:04+00:00,2024-10-01 07:10:36+00:00,https://github.com/apache/airflow/issues/42575,"[('kind:bug', 'This is a clearly a bug'), ('provider:microsoft-azure', 'Azure-related issues'), ('area:providers', ''), ('good first issue', '')]","[{'comment_id': 2382125947, 'issue_id': 2555128657, 'author': 'Pyasma', 'body': 'Hi @jscheffl, is this issue beginner-friendly? Is it still open? I’d also appreciate any advice or suggestions on how to approach it.', 'created_at': datetime.datetime(2024, 9, 30, 5, 10, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2383797021, 'issue_id': 2555128657, 'author': 'jscheffl', 'body': 'I hope and assume so. 50% sure :-D\r\nSo yesterday I fixed main branch by pinning the package in PR https://github.com/apache/airflow/pull/42576 to be !=4.6.0. If using 4.6.0 then some tests fail. So if you check out main and remove the pinning, rebuild CI image and execute tests... then you should see the failure. The ones printed above in the description. Follow the link to the failed run and check which commands were called leading to the failure.\r\n\r\nStatic checks should be something like `breeze static-checks --all-files --show-diff-on-failure --color always --initialize-environment`\r\n\r\nThe pytests will be `breeze testing db-tests --parallel-test-types ""API Always BranchExternalPython BranchPythonVenv CLI Core ExternalPython Operators Other PlainAsserts Providers[-amazon,google] Providers[amazon] Providers[google] PythonVenv Serialization WWW""`\r\n\r\nI assume some API in the kusto package has moved in 4.6.0 and pytests need to be re-worked for changed API.', 'created_at': datetime.datetime(2024, 9, 30, 17, 40, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2384969483, 'issue_id': 2555128657, 'author': 'potiuk', 'body': 'The `azure-kusto-data` 4.6.1 have been released yesterday and seems that canary tests passed and constraints were updated, so we can close this one because !=4.6.0 is the **right** exclusion', 'created_at': datetime.datetime(2024, 10, 1, 7, 10, 36, tzinfo=datetime.timezone.utc)}]","Pyasma on (2024-09-30 05:10:52 UTC): Hi @jscheffl, is this issue beginner-friendly? Is it still open? I’d also appreciate any advice or suggestions on how to approach it.

jscheffl (Issue Creator) on (2024-09-30 17:40:26 UTC): I hope and assume so. 50% sure :-D
So yesterday I fixed main branch by pinning the package in PR https://github.com/apache/airflow/pull/42576 to be !=4.6.0. If using 4.6.0 then some tests fail. So if you check out main and remove the pinning, rebuild CI image and execute tests... then you should see the failure. The ones printed above in the description. Follow the link to the failed run and check which commands were called leading to the failure.

Static checks should be something like `breeze static-checks --all-files --show-diff-on-failure --color always --initialize-environment`

The pytests will be `breeze testing db-tests --parallel-test-types ""API Always BranchExternalPython BranchPythonVenv CLI Core ExternalPython Operators Other PlainAsserts Providers[-amazon,google] Providers[amazon] Providers[google] PythonVenv Serialization WWW""`

I assume some API in the kusto package has moved in 4.6.0 and pytests need to be re-worked for changed API.

potiuk on (2024-10-01 07:10:36 UTC): The `azure-kusto-data` 4.6.1 have been released yesterday and seems that canary tests passed and constraints were updated, so we can close this one because !=4.6.0 is the **right** exclusion

"
2554991216,issue,open,,Add batch_create_links method in Weaviate hook,"### Description

Add the `batch_create_links` method in the Weaviate hook to allow users batch create links from an object to anothe object through cross-references. 

An example will be:

```
data = {
                    ""collection_name"": ""TestCollection1"",
                    ""from_property"": ""property1"",
                    ""from_uuid"": 1234455,
                    ""to_uuid"": 1245555,
                }
weaviate_hook.batch_create_links(data)
```

### Use case/motivation

Currently basic batch import is already implemented. Using this method, users can create links between objects and also assign `tenant` as a kwarg.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",harjeevanmaan,2024-09-29 15:32:28+00:00,['harjeevanmaan'],2024-09-30 12:24:06+00:00,,https://github.com/apache/airflow/issues/42568,"[('area:providers', ''), ('kind:feature', 'Feature Requests'), ('provider:weaviate', '')]","[{'comment_id': 2383041628, 'issue_id': 2554991216, 'author': 'Lee-W', 'body': 'https://github.com/apache/airflow/pull/42569 created for this issue. Removing `needs-triage` label.', 'created_at': datetime.datetime(2024, 9, 30, 12, 23, 56, tzinfo=datetime.timezone.utc)}]","Lee-W on (2024-09-30 12:23:56 UTC): https://github.com/apache/airflow/pull/42569 created for this issue. Removing `needs-triage` label.

"
2554945797,issue,open,,Add delete_by_property method in weaviate hook ,"### Description

Currently `delete_collections` is implemented in weaviate hook which deletes all or specific collections if collection_names are provided.
Delete collections by_property method would allow users to delete by providing the property names and deletion of objects that match a set of criteria.
This feature was added in Weaviate v1.21.

example:
```
collection_names = [""collection_a"", ""collection_b"", ""collection_c""]
by_property = [""question"", ""answer"", ""category""]
weaviate_hook.delete_collections(collection_names=collection_names, by_property=by_property)
```

### Use case/motivation

By adding the delete_by_property method, users can delete collections based on various filtering criteria, making data management more targeted and efficient.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",harjeevanmaan,2024-09-29 13:48:36+00:00,['harjeevanmaan'],2024-09-30 12:23:37+00:00,,https://github.com/apache/airflow/issues/42565,"[('area:providers', ''), ('kind:feature', 'Feature Requests'), ('provider:weaviate', '')]","[{'comment_id': 2381364806, 'issue_id': 2554945797, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 29, 13, 48, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2383040700, 'issue_id': 2554945797, 'author': 'Lee-W', 'body': 'https://github.com/apache/airflow/pull/42566 created for this issue. Removing `needs-triage` label.', 'created_at': datetime.datetime(2024, 9, 30, 12, 23, 36, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-29 13:48:38 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

Lee-W on (2024-09-30 12:23:36 UTC): https://github.com/apache/airflow/pull/42566 created for this issue. Removing `needs-triage` label.

"
2554473844,issue,open,,AIP-81 Dynamic generation of API requests to be Used in CLI,"### Description

_No response_

### Use case/motivation

[AIP-81](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-81+Enhanced+Security+in+CLI+via+Integration+of+API)

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-09-28 19:55:15+00:00,['bugraoz93'],2024-11-16 13:37:06+00:00,,https://github.com/apache/airflow/issues/42562,"[('area:CLI', ''), ('kind:feature', 'Feature Requests'), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]",[],
2554471892,issue,open,,AIP-81 Central API Communication Mechanism for CLI,"### Description

_No response_

### Use case/motivation

[AIP-81](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-81+Enhanced+Security+in+CLI+via+Integration+of+API)

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-09-28 19:49:03+00:00,['bugraoz93'],2024-11-16 13:37:11+00:00,,https://github.com/apache/airflow/issues/42561,"[('area:CLI', ''), ('kind:feature', 'Feature Requests'), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]",[],
2554470534,issue,open,,AIP-81 Implement Missing Bulk Insert API Endpoints will be used from CLI Commands,"### Description

[AIP-81](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-81+Enhanced+Security+in+CLI+via+Integration+of+API)

To enhance the API functionality, this umbrella ticket organizes the development of missing endpoints in FastAPI for key features like connections, variables, and more. Implementing these endpoints will enable users to manage resources more effectively through the API, improving usability and reducing the need for multiple workarounds. 
The workload aims to help secure CLI by using these endpoints.

### Missing Endpoints
* Import Connection(s) in single endpoint -> #43652 @bugraoz93 
* Import Pool(s) in single endpoint -> #43896 @jason810496 
* Import Variables(s) in single endpoint -> #43897 @jason810496 (Aims v3.1, this part will be covered from CLI to finalise AIP-81)

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-09-28 19:44:42+00:00,['bugraoz93'],2024-12-01 12:33:21+00:00,,https://github.com/apache/airflow/issues/42560,"[('area:CLI', ''), ('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-81', 'Enhanced Security in CLI via Integration of API')]","[{'comment_id': 2465602084, 'issue_id': 2554470534, 'author': 'bugraoz93', 'body': 'Hey @pierrejeambrun, I need your help on AIP-81 if you have time. It is related to new set endpoints in FastAPI.\n\n**&TLDR**\nThis is part of AIP-81. The aim is to use these endpoints from CLI. There are missing ones and one set of missing ones are these. I would like to get your help on reviews and your take on this before implementing because impacting more than one model and multiple endpoint in FastAPI. \n\nI created these issues as file but I am not sure if we should accept files as an input in the API since they are harder to validate if the file is safe in APIs and brought more security concerns. I am thinking we can create `POST` endpoints that accept the models in bulks like `post_connections` endpoint and parse the `files` in CLI before sending and receiving request/response from API. What do you think?\n\nMany thanks!', 'created_at': datetime.datetime(2024, 11, 8, 19, 31, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466670790, 'issue_id': 2554470534, 'author': 'jason810496', 'body': ""Hi @bugraoz93, if I may suggest, I would +1 for the second option (parsing the files at the CLI level).\r\nThis approach would allow Pydantic data models to serve as the interface within the codebase for both the CLI and API, decoupling CLI-specific file-parsing logic from the API. Additionally, this would enable the CLI to directly utilize the public APIs.\r\n\r\nBy the way, I'm available to take on other missing APIs for AIP-81 if needed."", 'created_at': datetime.datetime(2024, 11, 10, 10, 7, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466741090, 'issue_id': 2554470534, 'author': 'bugraoz93', 'body': "">Hi @bugraoz93, if I may suggest, I would +1 for the second option (parsing the files at the CLI level).\r\nThis approach would allow Pydantic data models to serve as the interface within the codebase for both the CLI and API, decoupling CLI-specific file-parsing logic from the API. Additionally, this would enable the CLI to directly utilize the public APIs.\r\n\r\nHi @jason810496, great to hear your +1 on this! I believe this will make both the CLI and API endpoints more secure and happy 😄\r\n\r\n>By the way, I'm available to take on other missing APIs for AIP-81 if needed.\r\n\r\nAmazing! Once the approach is aligned, I’ll break these down further. Any help would be much appreciated!"", 'created_at': datetime.datetime(2024, 11, 10, 13, 41, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2468972005, 'issue_id': 2554470534, 'author': 'bugraoz93', 'body': ""I've removed the export functionality, as we can retrieve all necessary data via the `list endpoints` and parse them to a given file from the `CLI`. This approach will reduce the workload and improve security for the `API` and the `CLI`. The more I looked at these endpoints and the approach, the more I felt this wasn’t even up for discussion, as this is clearly the better approach.\n\nAdditionally, I've created the remaining issues for anyone who wants to pick them up. I haven’t labelled them as `good first issue` yet. \ncc: @jason810496 @josix if you're interested in more issues :)"", 'created_at': datetime.datetime(2024, 11, 11, 20, 30, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469400921, 'issue_id': 2554470534, 'author': 'jason810496', 'body': 'Sounds reasonable. I can take on both issues. Could you please assign them to me? Thanks !\r\nBy the way, I noticed a possible typo for the Pools endpoint. Should it be `Insert Multiple Variables`?', 'created_at': datetime.datetime(2024, 11, 12, 1, 26, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469410476, 'issue_id': 2554470534, 'author': 'bugraoz93', 'body': '> Sounds reasonable. I can take on both issues. Could you please assign them to me? Thanks !\n> By the way, I noticed a possible typo for the Pools endpoint. Should it be `Insert Multiple Variables`?\n> \n\nNice one! Fixed. Assigning both. Thanks for your effort! :)', 'created_at': datetime.datetime(2024, 11, 12, 1, 31, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469411501, 'issue_id': 2554470534, 'author': 'bugraoz93', 'body': 'Can you comment them @jason810496? This is the only way I can see you in the issue.', 'created_at': datetime.datetime(2024, 11, 12, 1, 32, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2470760753, 'issue_id': 2554470534, 'author': 'pierrejeambrun', 'body': 'Most of these resources `Connection` `Pool` `Variable` already have a `POST` endpoint to create 1.\r\n\r\nThe CLI can either re-use with a loop and async calls to create multiple of them quickly as needed. (which can be enough at this point). Or we can develop a `bulk` insert endpoint, not sure if this is needed.', 'created_at': datetime.datetime(2024, 11, 12, 14, 57, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2470921805, 'issue_id': 2554470534, 'author': 'bugraoz93', 'body': '> Most of these resources `Connection` `Pool` `Variable` already have a `POST` endpoint to create 1.\n> \n> The CLI can either re-use with a loop and async calls to create multiple of them quickly as needed. (which can be enough at this point). Or we can develop a `bulk` insert endpoint, not sure if this is needed.\n\nIn that sense, those endpoints are indeed not necessary. I agree that even hundreds of calls per second shouldn’t be an issue for FastAPI. However, for better exception handling and an improved user experience, I’d prefer to handle all inserts in a single request. Otherwise, if the request fails for multiple entries, users would need to split the file, identify each issue, and then retry each entry individually or through the UI/CLI as a single resource.\n\nLet’s proceed with implementing the bulk insert endpoints, as it seems you’re not strictly opposed.', 'created_at': datetime.datetime(2024, 11, 12, 16, 0, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2471085816, 'issue_id': 2554470534, 'author': 'pierrejeambrun', 'body': 'Yes of course if we need a bulk insert for that use case we can definitely implement it.\r\n\r\nHow do we plan on handling failures ? (Even for bulk insert). Do we fail the entire transaction, or do we create those that could be created and return back the whole bunch of errors for the failing ones ?', 'created_at': datetime.datetime(2024, 11, 12, 16, 58, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2471128036, 'issue_id': 2554470534, 'author': 'bugraoz93', 'body': 'Awesome, thanks! \n\nThese endpoints will primarily be used when users are importing models from a file via the CLI. \n\nMy thinking is to fail the entire transaction and return a list of errors for any entries that caused issues. This way, users can quickly locate and fix specific items in the file or request, and then resend it. Creating only a portion of the entries would still be inconvenient for both API and CLI users. Not only is editing but also removing partial entries difficult if they’re spread across different sections of the list.\n\nFrom the CLI perspective, we can return the response message to the user as we do with the API if it fails, so both are aligned.\n\nWhat do you think?', 'created_at': datetime.datetime(2024, 11, 12, 17, 16, 40, tzinfo=datetime.timezone.utc)}]","bugraoz93 (Issue Creator) on (2024-11-08 19:31:19 UTC): Hey @pierrejeambrun, I need your help on AIP-81 if you have time. It is related to new set endpoints in FastAPI.

**&TLDR**
This is part of AIP-81. The aim is to use these endpoints from CLI. There are missing ones and one set of missing ones are these. I would like to get your help on reviews and your take on this before implementing because impacting more than one model and multiple endpoint in FastAPI. 

I created these issues as file but I am not sure if we should accept files as an input in the API since they are harder to validate if the file is safe in APIs and brought more security concerns. I am thinking we can create `POST` endpoints that accept the models in bulks like `post_connections` endpoint and parse the `files` in CLI before sending and receiving request/response from API. What do you think?

Many thanks!

jason810496 on (2024-11-10 10:07:26 UTC): Hi @bugraoz93, if I may suggest, I would +1 for the second option (parsing the files at the CLI level).
This approach would allow Pydantic data models to serve as the interface within the codebase for both the CLI and API, decoupling CLI-specific file-parsing logic from the API. Additionally, this would enable the CLI to directly utilize the public APIs.

By the way, I'm available to take on other missing APIs for AIP-81 if needed.

bugraoz93 (Issue Creator) on (2024-11-10 13:41:28 UTC): This approach would allow Pydantic data models to serve as the interface within the codebase for both the CLI and API, decoupling CLI-specific file-parsing logic from the API. Additionally, this would enable the CLI to directly utilize the public APIs.

Hi @jason810496, great to hear your +1 on this! I believe this will make both the CLI and API endpoints more secure and happy 😄


Amazing! Once the approach is aligned, I’ll break these down further. Any help would be much appreciated!

bugraoz93 (Issue Creator) on (2024-11-11 20:30:27 UTC): I've removed the export functionality, as we can retrieve all necessary data via the `list endpoints` and parse them to a given file from the `CLI`. This approach will reduce the workload and improve security for the `API` and the `CLI`. The more I looked at these endpoints and the approach, the more I felt this wasn’t even up for discussion, as this is clearly the better approach.

Additionally, I've created the remaining issues for anyone who wants to pick them up. I haven’t labelled them as `good first issue` yet. 
cc: @jason810496 @josix if you're interested in more issues :)

jason810496 on (2024-11-12 01:26:04 UTC): Sounds reasonable. I can take on both issues. Could you please assign them to me? Thanks !
By the way, I noticed a possible typo for the Pools endpoint. Should it be `Insert Multiple Variables`?

bugraoz93 (Issue Creator) on (2024-11-12 01:31:31 UTC): Nice one! Fixed. Assigning both. Thanks for your effort! :)

bugraoz93 (Issue Creator) on (2024-11-12 01:32:29 UTC): Can you comment them @jason810496? This is the only way I can see you in the issue.

pierrejeambrun on (2024-11-12 14:57:44 UTC): Most of these resources `Connection` `Pool` `Variable` already have a `POST` endpoint to create 1.

The CLI can either re-use with a loop and async calls to create multiple of them quickly as needed. (which can be enough at this point). Or we can develop a `bulk` insert endpoint, not sure if this is needed.

bugraoz93 (Issue Creator) on (2024-11-12 16:00:37 UTC): In that sense, those endpoints are indeed not necessary. I agree that even hundreds of calls per second shouldn’t be an issue for FastAPI. However, for better exception handling and an improved user experience, I’d prefer to handle all inserts in a single request. Otherwise, if the request fails for multiple entries, users would need to split the file, identify each issue, and then retry each entry individually or through the UI/CLI as a single resource.

Let’s proceed with implementing the bulk insert endpoints, as it seems you’re not strictly opposed.

pierrejeambrun on (2024-11-12 16:58:26 UTC): Yes of course if we need a bulk insert for that use case we can definitely implement it.

How do we plan on handling failures ? (Even for bulk insert). Do we fail the entire transaction, or do we create those that could be created and return back the whole bunch of errors for the failing ones ?

bugraoz93 (Issue Creator) on (2024-11-12 17:16:40 UTC): Awesome, thanks! 

These endpoints will primarily be used when users are importing models from a file via the CLI. 

My thinking is to fail the entire transaction and return a list of errors for any entries that caused issues. This way, users can quickly locate and fix specific items in the file or request, and then resend it. Creating only a portion of the entries would still be inconvenient for both API and CLI users. Not only is editing but also removing partial entries difficult if they’re spread across different sections of the list.

From the CLI perspective, we can return the response message to the user as we do with the API if it fails, so both are aligned.

What do you think?

"
2554468529,issue,closed,completed,AIP-84 Migrate the public endpoint Delete Connection to FastAPI,"### Description

Migrate the 'DELETE' connection to Fast API. 

### Use case/motivation

[AIP-84](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API)

### Related issues

Relates to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bugraoz93,2024-09-28 19:38:23+00:00,['bugraoz93'],2024-10-02 08:47:57+00:00,2024-10-02 08:47:57+00:00,https://github.com/apache/airflow/issues/42559,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]",[],
2554217829,issue,closed,completed,Airflow version: 2.3.3  RuntimeError: Unable to find any timezone configuration,"### Apache Airflow version

2.10.1

### If ""Other Airflow 2 version"" selected, which one?

2.3.3

### What happened?

Started airflow scheduler and webserver and opened the endpoint for 6007 from Jarvis lab.  Got below error.


Ooops!
Something bad has happened.

Airflow is used by many users, and it is very likely that others had similar problems and you can easily find
a solution to your problem.

Consider following these steps:

  * gather the relevant information (detailed logs with errors, reproduction steps, details of your deployment)

  * find similar issues using:
     * [GitHub Discussions](https://github.com/apache/airflow/discussions)
     * [GitHub Issues](https://github.com/apache/airflow/issues)
     * [Stack Overflow](https://stackoverflow.com/questions/tagged/airflow)
     * the usual search engine you use on a daily basis

  * if you run Airflow on a Managed Service, consider opening an issue using the service support channels

  * if you tried and have difficulty with diagnosing and fixing the problem yourself, consider creating a [bug report](https://github.com/apache/airflow/issues/new/choose).
    Make sure however, to include all relevant details and results of your investigation so far.

Python version: 3.8.12
Airflow version: 2.3.3
Node: f6d2b5df6971
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/flask/app.py"", line 2073, in wsgi_app
    response = self.full_dispatch_request()
  File ""/opt/conda/lib/python3.8/site-packages/flask/app.py"", line 1519, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/opt/conda/lib/python3.8/site-packages/flask/app.py"", line 1517, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/opt/conda/lib/python3.8/site-packages/flask/app.py"", line 1503, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**req.view_args)
  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/auth.py"", line 46, in decorated
    return func(*args, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/views.py"", line 964, in index
    return self.render_template(
  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/views.py"", line 709, in render_template
    return super().render_template(
  File ""/opt/conda/lib/python3.8/site-packages/flask_appbuilder/baseviews.py"", line 322, in render_template
    return render_template(
  File ""/opt/conda/lib/python3.8/site-packages/flask/templating.py"", line 154, in render_template
    return _render(
  File ""/opt/conda/lib/python3.8/site-packages/flask/templating.py"", line 128, in _render
    rv = template.render(context)
  File ""/opt/conda/lib/python3.8/site-packages/jinja2/environment.py"", line 1291, in render
    self.environment.handle_exception()
  File ""/opt/conda/lib/python3.8/site-packages/jinja2/environment.py"", line 925, in handle_exception
    raise rewrite_traceback_stack(source=source)
  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/templates/airflow/dags.html"", line 43, in top-level template code
    {% elif curr_ordering_direction == 'asc' and request.args.get('sorting_key') == attribute_name %}
  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/templates/airflow/main.html"", line 21, in top-level template code
    {% from 'airflow/_messages.html' import show_message %}
  File ""/opt/conda/lib/python3.8/site-packages/flask_appbuilder/templates/appbuilder/baselayout.html"", line 2, in top-level template code
    {% import 'appbuilder/baselib.html' as baselib %}
  File ""/opt/conda/lib/python3.8/site-packages/flask_appbuilder/templates/appbuilder/init.html"", line 37, in top-level template code
    {% block body %}
  File ""/opt/conda/lib/python3.8/site-packages/flask_appbuilder/templates/appbuilder/baselayout.html"", line 16, in block 'body'
    {% block messages %}
  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/templates/airflow/dags.html"", line 104, in block 'messages'
    {{ super() }}
  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/templates/airflow/main.html"", line 62, in block 'messages'
    {% call show_message(category='warning', dismissible=false) %}
  File ""/opt/conda/lib/python3.8/site-packages/jinja2/runtime.py"", line 828, in _invoke
    rv = self._func(*arguments)
  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/templates/airflow/_messages.html"", line 25, in template
    {{ caller() }}
  File ""/opt/conda/lib/python3.8/site-packages/jinja2/runtime.py"", line 828, in _invoke
    rv = self._func(*arguments)
  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/templates/airflow/main.html"", line 70, in template
    >{{ macros.datetime_diff_for_humans(scheduler_job.latest_heartbeat) }}</time>.
  File ""/opt/conda/lib/python3.8/site-packages/airflow/macros/__init__.py"", line 77, in datetime_diff_for_humans
    return pendulum.instance(dt).diff_for_humans(since)
  File ""/opt/conda/lib/python3.8/site-packages/pendulum/datetime.py"", line 824, in diff_for_humans
    other = self.now()
  File ""/opt/conda/lib/python3.8/site-packages/pendulum/datetime.py"", line 106, in now
    return pendulum.now(tz)
  File ""/opt/conda/lib/python3.8/site-packages/pendulum/__init__.py"", line 211, in now
    dt = _datetime.datetime.now(local_timezone())
  File ""/opt/conda/lib/python3.8/site-packages/pendulum/tz/__init__.py"", line 60, in local_timezone
    return get_local_timezone()
  File ""/opt/conda/lib/python3.8/site-packages/pendulum/tz/local_timezone.py"", line 35, in get_local_timezone
    tz = _get_system_timezone()
  File ""/opt/conda/lib/python3.8/site-packages/pendulum/tz/local_timezone.py"", line 63, in _get_system_timezone
    return _get_unix_timezone()
  File ""/opt/conda/lib/python3.8/site-packages/pendulum/tz/local_timezone.py"", line 242, in _get_unix_timezone
    raise RuntimeError(""Unable to find any timezone configuration"")
RuntimeError: Unable to find any timezone configuration

### What you think should happen instead?

UI should open

### How to reproduce

Open the UI 

### Operating System

Linux f6d2b5df6971 6.2.0-39-generic #40~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 16 10:53:04 UTC 2 x86_64 x86_64 x86_64 GNU/Linux

### Versions of Apache Airflow Providers

2.3.3 

### Deployment

Other

### Deployment details

NA

### Anything else?

NA

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",psubudhi,2024-09-28 13:19:56+00:00,[],2024-10-02 19:31:58+00:00,2024-10-02 19:31:58+00:00,https://github.com/apache/airflow/issues/42558,"[('kind:bug', 'This is a clearly a bug'), ('area:webserver', 'Webserver related Issues'), ('pending-response', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2380637856, 'issue_id': 2554217829, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 28, 13, 20, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2381457116, 'issue_id': 2554217829, 'author': 'koustreak', 'body': 'env:\r\n  - name: AIRFLOW__CORE__DEFAULT_TIMEZONE\r\n    value: ""UTC""  \r\n\r\nHave you tried this one ??', 'created_at': datetime.datetime(2024, 9, 29, 18, 39, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2384108880, 'issue_id': 2554217829, 'author': 'jscheffl', 'body': 'Version 2.3.3 of Airflow is pretty pretty old. Have you considered upgrading to a more recent version?', 'created_at': datetime.datetime(2024, 9, 30, 20, 42, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2384646442, 'issue_id': 2554217829, 'author': 'psubudhi', 'body': ""We have received the lab with pre-installed airflow, and will check with\r\nthe team if the airflow can be upgraded to the latest version.\r\n\r\nBut don't we have a solution for the above issue?\r\nThanks,\r\nPrem\r\n\r\n\r\nOn Tue, 1 Oct 2024 at 02:12, Jens Scheffler ***@***.***>\r\nwrote:\r\n\r\n> Version 2.3.3 of Airflow is pretty pretty old. Have you considered\r\n> upgrading to a more recent version?\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/apache/airflow/issues/42558#issuecomment-2384108880>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/ACDW4VCHKWMKOYZGF5SICDTZZGZTDAVCNFSM6AAAAABPAT7ZVSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGOBUGEYDQOBYGA>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>"", 'created_at': datetime.datetime(2024, 10, 1, 2, 16, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2385029901, 'issue_id': 2554217829, 'author': 'psubudhi', 'body': 'I checked the config \r\n\r\ndefault_timezone = utc', 'created_at': datetime.datetime(2024, 10, 1, 7, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2385038621, 'issue_id': 2554217829, 'author': 'psubudhi', 'body': 'fyi\r\n\r\n![image](https://github.com/user-attachments/assets/fbb051fd-064c-4ab5-8b09-0a49e3edf6be)\r\n![image](https://github.com/user-attachments/assets/e4aa4beb-39ba-4609-9dce-359e0a763156)', 'created_at': datetime.datetime(2024, 10, 1, 7, 48, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2386754415, 'issue_id': 2554217829, 'author': 'jscheffl', 'body': 'Re-reading the stack trace there is something seriously wrog with your system anyway. The error is rooted becasue the webser attempts to render an error message that the scheduler in unhealty.\r\n\r\nAre you sure that the `default_timezone` is set consistently across all nodes? Looking at the configuration reference `utc`is actually the default (see https://airflow.apache.org/docs/apache-airflow/2.3.3/configurations-ref.html)\r\n\r\nNote that the upper case version `AIRFLOW__CORE__...` in your config is the version to over-ride the CFG file from ENV variables. To debug what actually is used, can you open a shell i nthe context of the webserver and run `airflow config list` on the command line and check which value is active?', 'created_at': datetime.datetime(2024, 10, 1, 19, 6, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2387305904, 'issue_id': 2554217829, 'author': 'psubudhi', 'body': 'Thank you Jens for the reply. Here are my inputs to your asks.\r\n1. I am using a single node server.\r\n2. Here is the config list\r\n***@***.***:~# airflow config list|grep timezone\r\ndefault_timezone = utc\r\ndefault_ui_timezone = UTC\r\n***@***.***:~#\r\n\r\nFYI More log of scheduler and web server attached. Please let me know if\r\nyou have any details you might need from the host node.\r\n\r\nRegards,\r\nPrem\r\n\r\nOn Wed, 2 Oct 2024 at 00:37, Jens Scheffler ***@***.***>\r\nwrote:\r\n\r\n> Re-reading the stack trace there is something seriously wrog with your\r\n> system anyway. The error is rooted becasue the webser attempts to render an\r\n> error message that the scheduler in unhealty.\r\n>\r\n> Are you sure that the default_timezone is set consistently across all\r\n> nodes? Looking at the configuration reference utcis actually the default\r\n> (see\r\n> https://airflow.apache.org/docs/apache-airflow/2.3.3/configurations-ref.html\r\n> )\r\n>\r\n> Note that the upper case version AIRFLOW__CORE__... in your config is the\r\n> version to over-ride the CFG file from ENV variables. To debug what\r\n> actually is used, can you open a shell i nthe context of the webserver and\r\n> run airflow config list on the command line and check which value is\r\n> active?\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/apache/airflow/issues/42558#issuecomment-2386754415>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/ACDW4VB7Q75HTOUIBZVPZBDZZLXFHAVCNFSM6AAAAABPAT7ZVSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGOBWG42TINBRGU>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>\r\n\n***@***.***:~# airflow config list\r\n[core]\r\ndags_folder = /home/airflow/dags\r\nhostname_callable = socket.getfqdn\r\ndefault_timezone = utc\r\nexecutor = SequentialExecutor\r\nparallelism = 32\r\nmax_active_tasks_per_dag = 16\r\ndags_are_paused_at_creation = True\r\nmax_active_runs_per_dag = 16\r\nload_examples = True\r\nplugins_folder = /home/airflow/plugins\r\nexecute_tasks_new_python_interpreter = False\r\nfernet_key = \r\ndonot_pickle = True\r\ndagbag_import_timeout = 30.0\r\ndagbag_import_error_tracebacks = True\r\ndagbag_import_error_traceback_depth = 2\r\ndag_file_processor_timeout = 50\r\ntask_runner = StandardTaskRunner\r\ndefault_impersonation = \r\nsecurity = \r\nunit_test_mode = False\r\nenable_xcom_pickling = False\r\nkilled_task_cleanup_time = 60\r\ndag_run_conf_overrides_params = True\r\ndag_discovery_safe_mode = True\r\ndag_ignore_file_syntax = regexp\r\ndefault_task_retries = 0\r\ndefault_task_weight_rule = downstream\r\ndefault_task_execution_timeout = \r\nmin_serialized_dag_update_interval = 30\r\ncompress_serialized_dags = False\r\nmin_serialized_dag_fetch_interval = 10\r\nmax_num_rendered_ti_fields_per_task = 30\r\ncheck_slas = True\r\nxcom_backend = airflow.models.xcom.BaseXCom\r\nlazy_load_plugins = True\r\nlazy_discover_providers = True\r\nhide_sensitive_var_conn_fields = True\r\nsensitive_var_conn_names = \r\ndefault_pool_task_slot_count = 128\r\nmax_map_length = 1024\r\nAIRFLOW__CORE__DEFAULT_TIMEZONE = UTC\r\n\r\n[database]\r\nsql_alchemy_conn = sqlite:////home/airflow/airflow.db\r\nsql_engine_encoding = utf-8\r\nsql_alchemy_pool_enabled = True\r\nsql_alchemy_pool_size = 5\r\nsql_alchemy_max_overflow = 10\r\nsql_alchemy_pool_recycle = 1800\r\nsql_alchemy_pool_pre_ping = True\r\nsql_alchemy_schema = \r\nload_default_connections = True\r\nmax_db_retries = 3\r\n\r\n[logging]\r\nbase_log_folder = /home/airflow/logs\r\nremote_logging = False\r\nremote_log_conn_id = \r\ngoogle_key_path = \r\nremote_base_log_folder = \r\nencrypt_s3_logs = False\r\nlogging_level = INFO\r\ncelery_logging_level = \r\nfab_logging_level = WARNING\r\nlogging_config_class = \r\ncolored_console_log = True\r\ncolored_log_format = [%(blue)s%(asctime)s%(reset)s] {%(blue)s%(filename)s:%(reset)s%(lineno)d} %(log_color)s%(levelname)s%(reset)s - %(log_color)s%(message)s%(reset)s\r\ncolored_formatter_class = airflow.utils.log.colored_log.CustomTTYColoredFormatter\r\nlog_format = [%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s\r\nsimple_log_format = %(asctime)s %(levelname)s - %(message)s\r\ntask_log_prefix_template = \r\nlog_filename_template = dag_id={{ ti.dag_id }}/run_id={{ ti.run_id }}/task_id={{ ti.task_id }}/{% if ti.map_index >= 0 %}map_index={{ ti.map_index }}/{% endif %}attempt={{ try_number }}.log\r\nlog_processor_filename_template = {{ filename }}.log\r\ndag_processor_manager_log_location = /home/airflow/logs/dag_processor_manager/dag_processor_manager.log\r\ntask_log_reader = task\r\nextra_logger_names = \r\nworker_log_server_port = 8793\r\n\r\n[metrics]\r\nstatsd_on = False\r\nstatsd_host = localhost\r\nstatsd_port = 8125\r\nstatsd_prefix = airflow\r\nstatsd_allow_list = \r\nstat_name_handler = \r\nstatsd_datadog_enabled = False\r\nstatsd_datadog_tags = \r\n\r\n[secrets]\r\nbackend = \r\nbackend_kwargs = \r\n\r\n[cli]\r\napi_client = airflow.api.client.local_client\r\nendpoint_url = http://localhost:8080\r\n\r\n[debug]\r\nfail_fast = False\r\n\r\n[api]\r\nenable_experimental_api = False\r\nauth_backends = airflow.api.auth.backend.session\r\nmaximum_page_limit = 100\r\nfallback_page_limit = 100\r\ngoogle_oauth2_audience = \r\ngoogle_key_path = \r\naccess_control_allow_headers = \r\naccess_control_allow_methods = \r\naccess_control_allow_origins = \r\n\r\n[lineage]\r\nbackend = \r\n\r\n[atlas]\r\nsasl_enabled = False\r\nhost = \r\nport = 21000\r\nusername = \r\npassword = \r\n\r\n[operators]\r\ndefault_owner = airflow\r\ndefault_cpus = 1\r\ndefault_ram = 512\r\ndefault_disk = 512\r\ndefault_gpus = 0\r\ndefault_queue = default\r\nallow_illegal_arguments = False\r\n\r\n[hive]\r\ndefault_hive_mapred_queue = \r\n\r\n[webserver]\r\nbase_url = http://localhost:8080\r\ndefault_ui_timezone = UTC\r\nweb_server_host = 0.0.0.0\r\nweb_server_port = 6007\r\nweb_server_ssl_cert = \r\nweb_server_ssl_key = \r\nsession_backend = database\r\nweb_server_master_timeout = 120\r\nweb_server_worker_timeout = 120\r\nworker_refresh_batch_size = 1\r\nworker_refresh_interval = 6000\r\nreload_on_plugin_change = False\r\nsecret_key = CbGpipwchu6Dg00sg5m0ZA==\r\nworkers = 4\r\nworker_class = sync\r\naccess_logfile = -\r\nerror_logfile = -\r\naccess_logformat = \r\nexpose_config = False\r\nexpose_hostname = True\r\nexpose_stacktrace = True\r\ndag_default_view = grid\r\ndag_orientation = LR\r\nlog_fetch_timeout_sec = 5\r\nlog_fetch_delay_sec = 2\r\nlog_auto_tailing_offset = 30\r\nlog_animation_speed = 1000\r\nhide_paused_dags_by_default = False\r\npage_size = 100\r\nnavbar_color = #fff\r\ndefault_dag_run_display_number = 25\r\nenable_proxy_fix = False\r\nproxy_fix_x_for = 1\r\nproxy_fix_x_proto = 1\r\nproxy_fix_x_host = 1\r\nproxy_fix_x_port = 1\r\nproxy_fix_x_prefix = 1\r\ncookie_secure = False\r\ncookie_samesite = Lax\r\ndefault_wrap = False\r\nx_frame_enabled = True\r\nshow_recent_stats_for_completed_runs = True\r\nupdate_fab_perms = True\r\nsession_lifetime_minutes = 43200\r\ninstance_name_has_markup = False\r\nauto_refresh_interval = 3\r\nwarn_deployment_exposure = True\r\naudit_view_excluded_events = gantt,landing_times,tries,duration,calendar,graph,grid,tree,tree_data\r\n\r\n[email]\r\nemail_backend = airflow.utils.email.send_email_smtp\r\nemail_conn_id = smtp_default\r\ndefault_email_on_retry = True\r\ndefault_email_on_failure = True\r\n\r\n[smtp]\r\nsmtp_host = localhost\r\nsmtp_starttls = True\r\nsmtp_ssl = False\r\nsmtp_port = 25\r\nsmtp_mail_from = ***@***.***\r\nsmtp_timeout = 30\r\nsmtp_retry_limit = 5\r\n\r\n[sentry]\r\nsentry_on = False\r\nsentry_dsn = \r\n\r\n[local_kubernetes_executor]\r\nkubernetes_queue = kubernetes\r\n\r\n[celery_kubernetes_executor]\r\nkubernetes_queue = kubernetes\r\n\r\n[celery]\r\ncelery_app_name = airflow.executors.celery_executor\r\nworker_concurrency = 16\r\nworker_prefetch_multiplier = 1\r\nworker_enable_remote_control = True\r\nworker_umask = 0o077\r\nbroker_url = redis://redis:6379/0\r\nresult_backend = ***@***.***/airflow\r\nflower_host = 0.0.0.0\r\nflower_url_prefix = \r\nflower_port = 5555\r\nflower_basic_auth = \r\nsync_parallelism = 0\r\ncelery_config_options = airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG\r\nssl_active = False\r\nssl_key = \r\nssl_cert = \r\nssl_cacert = \r\npool = prefork\r\noperation_timeout = 1.0\r\ntask_track_started = True\r\ntask_adoption_timeout = 600\r\nstalled_task_timeout = 0\r\ntask_publish_max_retries = 3\r\nworker_precheck = False\r\n\r\n[celery_broker_transport_options]\r\n\r\n[dask]\r\ncluster_address = 127.0.0.1:8786\r\ntls_ca = \r\ntls_cert = \r\ntls_key = \r\n\r\n[scheduler]\r\njob_heartbeat_sec = 5\r\nscheduler_heartbeat_sec = 5\r\nnum_runs = -1\r\nscheduler_idle_sleep_time = 1\r\nmin_file_process_interval = 30\r\ndeactivate_stale_dags_interval = 60\r\ndag_dir_list_interval = 300\r\nprint_stats_interval = 30\r\npool_metrics_interval = 5.0\r\nscheduler_health_check_threshold = 30\r\norphaned_tasks_check_interval = 300.0\r\nchild_process_log_directory = /home/airflow/logs/scheduler\r\nscheduler_zombie_task_threshold = 300\r\nzombie_detection_interval = 10.0\r\ncatchup_by_default = True\r\nignore_first_depends_on_past_by_default = True\r\nmax_tis_per_query = 512\r\nuse_row_level_locking = True\r\nmax_dagruns_to_create_per_loop = 10\r\nmax_dagruns_per_loop_to_schedule = 20\r\nschedule_after_task_execution = True\r\nparsing_processes = 2\r\nfile_parsing_sort_mode = modified_time\r\nstandalone_dag_processor = False\r\nmax_callbacks_per_loop = 20\r\nuse_job_schedule = True\r\nallow_trigger_in_future = False\r\ndependency_detector = airflow.serialization.serialized_objects.DependencyDetector\r\ntrigger_timeout_check_interval = 15\r\n\r\n[triggerer]\r\ndefault_capacity = 1000\r\n\r\n[kerberos]\r\nccache = /tmp/airflow_krb5_ccache\r\nprincipal = airflow\r\nreinit_frequency = 3600\r\nkinit_path = kinit\r\nkeytab = airflow.keytab\r\nforwardable = True\r\ninclude_ip = True\r\n\r\n[github_enterprise]\r\napi_rev = v3\r\n\r\n[elasticsearch]\r\nhost = \r\nlog_id_template = {dag_id}-{task_id}-{run_id}-{map_index}-{try_number}\r\nend_of_log_mark = end_of_log\r\nfrontend = \r\nwrite_stdout = False\r\njson_format = False\r\njson_fields = asctime, filename, lineno, levelname, message\r\nhost_field = host\r\noffset_field = offset\r\n\r\n[elasticsearch_configs]\r\nuse_ssl = False\r\nverify_certs = True\r\n\r\n[kubernetes]\r\npod_template_file = \r\nworker_container_repository = \r\nworker_container_tag = \r\nnamespace = default\r\ndelete_worker_pods = True\r\ndelete_worker_pods_on_failure = False\r\nworker_pods_creation_batch_size = 1\r\nmulti_namespace_mode = False\r\nin_cluster = True\r\nkube_client_request_args = \r\ndelete_option_kwargs = \r\nenable_tcp_keepalive = True\r\ntcp_keep_idle = 120\r\ntcp_keep_intvl = 30\r\ntcp_keep_cnt = 6\r\nverify_ssl = True\r\nworker_pods_pending_timeout = 300\r\nworker_pods_pending_timeout_check_interval = 120\r\nworker_pods_queued_check_interval = 60\r\nworker_pods_pending_timeout_batch_size = 100\r\n\r\n[sensors]\r\ndefault_timeout = 604800\r\n\r\n[smart_sensor]\r\nuse_smart_sensor = False\r\nshard_code_upper_limit = 10000\r\nshards = 5\r\nsensors_enabled = NamedHivePartitionSensor\r\n\r\n***@***.***:~# airflow config list|grep time\r\ndefault_timezone = utc\r\ndagbag_import_timeout = 30.0\r\ndag_file_processor_timeout = 50\r\nkilled_task_cleanup_time = 60\r\ndefault_task_execution_timeout = \r\ncolored_log_format = [%(blue)s%(asctime)s%(reset)s] {%(blue)s%(filename)s:%(reset)s%(lineno)d} %(log_color)s%(levelname)s%(reset)s - %(log_color)s%(message)s%(reset)s\r\nlog_format = [%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s\r\nsimple_log_format = %(asctime)s %(levelname)s - %(message)s\r\ndefault_ui_timezone = UTC\r\nweb_server_master_timeout = 120\r\nweb_server_worker_timeout = 120\r\nlog_fetch_timeout_sec = 5\r\nsession_lifetime_minutes = 43200\r\naudit_view_excluded_events = gantt,landing_times,tries,duration,calendar,graph,grid,tree,tree_data\r\nsmtp_timeout = 30\r\noperation_timeout = 1.0\r\ntask_adoption_timeout = 600\r\nstalled_task_timeout = 0\r\nscheduler_idle_sleep_time = 1\r\nfile_parsing_sort_mode = modified_time\r\ntrigger_timeout_check_interval = 15\r\njson_fields = asctime, filename, lineno, levelname, message\r\nworker_pods_pending_timeout = 300\r\nworker_pods_pending_timeout_check_interval = 120\r\nworker_pods_pending_timeout_batch_size = 100\r\ndefault_timeout = 604800\r\n***@***.***:~# airflow config list|grep timezone\r\ndefault_timezone = utc\r\ndefault_ui_timezone = UTC\r\n***@***.***:~# airflow webserver --port 6007\r\n  ____________       _____________\r\n ____    |__( )_________  __/__  /________      __\r\n____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /\r\n___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /\r\n _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/\r\nRunning the Gunicorn Server with:\r\nWorkers: 4 sync\r\nHost: 0.0.0.0:6007\r\nTimeout: 120\r\nLogfiles: - -\r\nAccess Logformat: \r\n=================================================================\r\n[2024-10-02 00:14:25 +0000] [1730] [INFO] Starting gunicorn 20.1.0\r\n[2024-10-02 00:14:26 +0000] [1730] [INFO] Listening at: http://0.0.0.0:6007 (1730)\r\n[2024-10-02 00:14:26 +0000] [1730] [INFO] Using worker: sync\r\n[2024-10-02 00:14:26 +0000] [1733] [INFO] Booting worker with pid: 1733\r\n[2024-10-02 00:14:26 +0000] [1734] [INFO] Booting worker with pid: 1734\r\n[2024-10-02 00:14:26 +0000] [1735] [INFO] Booting worker with pid: 1735\r\n[2024-10-02 00:14:26 +0000] [1736] [INFO] Booting worker with pid: 1736\r\n10.232.204.18 - - [02/Oct/2024:00:14:37 +0000] ""GET / HTTP/1.1"" 302 197 ""https://jarvislabs.ai/"" ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""\r\n/opt/conda/lib/python3.8/site-packages/airflow/www/views.py:889 SADeprecationWarning: DISTINCT ON is currently supported only by the PostgreSQL dialect.  Use of DISTINCT ON for other backends is currently silently ignored, however this usage is deprecated, and will raise CompileError in a future release for all backends that do not support this syntax.\r\n[2024-10-02 00:14:37,965] {app.py:1449} ERROR - Exception on /home [GET]\r\nTraceback (most recent call last):\r\n  File ""/opt/conda/lib/python3.8/site-packages/flask/app.py"", line 2073, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n  File ""/opt/conda/lib/python3.8/site-packages/flask/app.py"", line 1519, in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\n  File ""/opt/conda/lib/python3.8/site-packages/flask/app.py"", line 1517, in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n  File ""/opt/conda/lib/python3.8/site-packages/flask/app.py"", line 1503, in dispatch_request\r\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**req.view_args)\r\n  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/auth.py"", line 46, in decorated\r\n    return func(*args, **kwargs)\r\n  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/views.py"", line 964, in index\r\n    return self.render_template(\r\n  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/views.py"", line 709, in render_template\r\n    return super().render_template(\r\n  File ""/opt/conda/lib/python3.8/site-packages/flask_appbuilder/baseviews.py"", line 322, in render_template\r\n    return render_template(\r\n  File ""/opt/conda/lib/python3.8/site-packages/flask/templating.py"", line 154, in render_template\r\n    return _render(\r\n  File ""/opt/conda/lib/python3.8/site-packages/flask/templating.py"", line 128, in _render\r\n    rv = template.render(context)\r\n  File ""/opt/conda/lib/python3.8/site-packages/jinja2/environment.py"", line 1291, in render\r\n    self.environment.handle_exception()\r\n  File ""/opt/conda/lib/python3.8/site-packages/jinja2/environment.py"", line 925, in handle_exception\r\n    raise rewrite_traceback_stack(source=source)\r\n  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/templates/airflow/dags.html"", line 43, in top-level template code\r\n    {% elif curr_ordering_direction == \'asc\' and request.args.get(\'sorting_key\') == attribute_name %}\r\n  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/templates/airflow/main.html"", line 21, in top-level template code\r\n    {% from \'airflow/_messages.html\' import show_message %}\r\n  File ""/opt/conda/lib/python3.8/site-packages/flask_appbuilder/templates/appbuilder/baselayout.html"", line 2, in top-level template code\r\n    {% import \'appbuilder/baselib.html\' as baselib %}\r\n  File ""/opt/conda/lib/python3.8/site-packages/flask_appbuilder/templates/appbuilder/init.html"", line 37, in top-level template code\r\n    {% block body %}\r\n  File ""/opt/conda/lib/python3.8/site-packages/flask_appbuilder/templates/appbuilder/baselayout.html"", line 16, in block \'body\'\r\n    {% block messages %}\r\n  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/templates/airflow/dags.html"", line 104, in block \'messages\'\r\n    {{ super() }}\r\n  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/templates/airflow/main.html"", line 62, in block \'messages\'\r\n    {% call show_message(category=\'warning\', dismissible=false) %}\r\n  File ""/opt/conda/lib/python3.8/site-packages/jinja2/runtime.py"", line 828, in _invoke\r\n    rv = self._func(*arguments)\r\n  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/templates/airflow/_messages.html"", line 25, in template\r\n    {{ caller() }}\r\n  File ""/opt/conda/lib/python3.8/site-packages/jinja2/runtime.py"", line 828, in _invoke\r\n    rv = self._func(*arguments)\r\n  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/templates/airflow/main.html"", line 70, in template\r\n    >{{ macros.datetime_diff_for_humans(scheduler_job.latest_heartbeat) }}</time>.\r\n  File ""/opt/conda/lib/python3.8/site-packages/airflow/macros/__init__.py"", line 77, in datetime_diff_for_humans\r\n    return pendulum.instance(dt).diff_for_humans(since)\r\n  File ""/opt/conda/lib/python3.8/site-packages/pendulum/datetime.py"", line 824, in diff_for_humans\r\n    other = self.now()\r\n  File ""/opt/conda/lib/python3.8/site-packages/pendulum/datetime.py"", line 106, in now\r\n    return pendulum.now(tz)\r\n  File ""/opt/conda/lib/python3.8/site-packages/pendulum/__init__.py"", line 211, in now\r\n    dt = _datetime.datetime.now(local_timezone())\r\n  File ""/opt/conda/lib/python3.8/site-packages/pendulum/tz/__init__.py"", line 60, in local_timezone\r\n    return get_local_timezone()\r\n  File ""/opt/conda/lib/python3.8/site-packages/pendulum/tz/local_timezone.py"", line 35, in get_local_timezone\r\n    tz = _get_system_timezone()\r\n  File ""/opt/conda/lib/python3.8/site-packages/pendulum/tz/local_timezone.py"", line 63, in _get_system_timezone\r\n    return _get_unix_timezone()\r\n  File ""/opt/conda/lib/python3.8/site-packages/pendulum/tz/local_timezone.py"", line 242, in _get_unix_timezone\r\n    raise RuntimeError(""Unable to find any timezone configuration"")\r\nRuntimeError: Unable to find any timezone configuration\r\n10.232.204.18 - - [02/Oct/2024:00:14:38 +0000] ""GET /home HTTP/1.1"" 500 6481 ""https://jarvislabs.ai/"" ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""\r\n\r\n\r\n\r\nScheduler log:\r\n===============\r\n***@***.***:~# airflow scheduler\r\n  ____________       _____________\r\n ____    |__( )_________  __/__  /________      __\r\n____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /\r\n___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /\r\n _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/\r\n[2024-10-02 00:13:55 +0000] [1690] [INFO] Starting gunicorn 20.1.0\r\n[2024-10-02 00:13:55 +0000] [1690] [INFO] Listening at: http://0.0.0.0:8793 (1690)\r\n[2024-10-02 00:13:55 +0000] [1690] [INFO] Using worker: sync\r\n[2024-10-02 00:13:55 +0000] [1691] [INFO] Booting worker with pid: 1691\r\n[2024-10-02 00:13:55,601] {scheduler_job.py:708} INFO - Starting the scheduler\r\n[2024-10-02 00:13:55,601] {scheduler_job.py:713} INFO - Processing each file at most -1 times\r\n[2024-10-02 00:13:55,603] {executor_loader.py:105} INFO - Loaded executor: SequentialExecutor\r\n[2024-10-02 00:13:55,608] {manager.py:160} INFO - Launched DagFileProcessorManager with pid: 1692\r\n[2024-10-02 00:13:55,610] {scheduler_job.py:1233} INFO - Resetting orphaned tasks for active dag runs\r\n[2024-10-02 00:13:55,622] {settings.py:55} INFO - Configured default timezone Timezone(\'UTC\')\r\n[2024-10-02 00:13:55,625] {scheduler_job.py:1256} INFO - Marked 1 SchedulerJob instances as failed\r\n[2024-10-02 00:13:55,643] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.\r\n[2024-10-02 00:13:55 +0000] [1693] [INFO] Booting worker with pid: 1693\r\n[2024-10-02 00:14:00,252] {dag.py:2968} INFO - Setting next_dagrun for Lead_scoring_inference_pipeline to 2024-10-01T23:00:00+00:00, run_after=2024-10-02T00:00:00+00:00\r\n[2024-10-02 00:14:00,302] {dag.py:2968} INFO - Setting next_dagrun for Lead_Scoring_Data_Engineering_Pipeline to 2024-10-02T00:00:00+00:00, run_after=2024-10-03T00:00:00+00:00\r\n[2024-10-02 00:14:00,531] {scheduler_job.py:353} INFO - 2 tasks up for execution:\r\n        <TaskInstance: Lead_Scoring_Data_Engineering_Pipeline.building_db scheduled__2024-10-01T00:00:00+00:00 [scheduled]>\r\n        <TaskInstance: Lead_scoring_inference_pipeline.encoding_categorical_variables scheduled__2024-10-01T07:00:00+00:00 [scheduled]>\r\n[2024-10-02 00:14:00,532] {scheduler_job.py:418} INFO - DAG Lead_Scoring_Data_Engineering_Pipeline has 0/16 running and queued tasks\r\n[2024-10-02 00:14:00,533] {scheduler_job.py:418} INFO - DAG Lead_scoring_inference_pipeline has 0/16 running and queued tasks\r\n[2024-10-02 00:14:00,533] {scheduler_job.py:504} INFO - Setting the following tasks to queued state:\r\n        <TaskInstance: Lead_Scoring_Data_Engineering_Pipeline.building_db scheduled__2024-10-01T00:00:00+00:00 [scheduled]>\r\n        <TaskInstance: Lead_scoring_inference_pipeline.encoding_categorical_variables scheduled__2024-10-01T07:00:00+00:00 [scheduled]>\r\n[2024-10-02 00:14:00,592] {scheduler_job.py:546} INFO - Sending TaskInstanceKey(dag_id=\'Lead_Scoring_Data_Engineering_Pipeline\', task_id=\'building_db\', run_id=\'scheduled__2024-10-01T00:00:00+00:00\', try_number=1, map_index=-1) to executor with priority 7 and queue default\r\n[2024-10-02 00:14:00,592] {base_executor.py:91} INFO - Adding to queue: [\'airflow\', \'tasks\', \'run\', \'Lead_Scoring_Data_Engineering_Pipeline\', \'building_db\', \'scheduled__2024-10-01T00:00:00+00:00\', \'--local\', \'--subdir\', \'DAGS_FOLDER/Lead_scoring_data_pipeline/.ipynb_checkpoints/lead_scoring_data_pipeline-checkpoint.py\']\r\n[2024-10-02 00:14:00,593] {scheduler_job.py:546} INFO - Sending TaskInstanceKey(dag_id=\'Lead_scoring_inference_pipeline\', task_id=\'encoding_categorical_variables\', run_id=\'scheduled__2024-10-01T07:00:00+00:00\', try_number=1, map_index=-1) to executor with priority 4 and queue default\r\n[2024-10-02 00:14:00,593] {base_executor.py:91} INFO - Adding to queue: [\'airflow\', \'tasks\', \'run\', \'Lead_scoring_inference_pipeline\', \'encoding_categorical_variables\', \'scheduled__2024-10-01T07:00:00+00:00\', \'--local\', \'--subdir\', \'DAGS_FOLDER/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py\']\r\n[2024-10-02 00:14:00,613] {sequential_executor.py:59} INFO - Executing command: [\'airflow\', \'tasks\', \'run\', \'Lead_Scoring_Data_Engineering_Pipeline\', \'building_db\', \'scheduled__2024-10-01T00:00:00+00:00\', \'--local\', \'--subdir\', \'DAGS_FOLDER/Lead_scoring_data_pipeline/.ipynb_checkpoints/lead_scoring_data_pipeline-checkpoint.py\']\r\n[2024-10-02 00:14:01,312] {dagbag.py:508} INFO - Filling up the DagBag from /home/airflow/dags/Lead_scoring_data_pipeline/.ipynb_checkpoints/lead_scoring_data_pipeline-checkpoint.py\r\n[2024-10-02 00:14:01,547] {utils.py:145} INFO - Note: NumExpr detected 64 cores but ""NUMEXPR_MAX_THREADS"" not set, so enforcing safe limit of 8.\r\n[2024-10-02 00:14:01,548] {utils.py:157} INFO - NumExpr defaulting to 8 threads.\r\n[2024-10-02 00:14:04,527] {lead_scoring_data_pipeline-checkpoint.py:47} INFO - Creating mlflow experiment\r\n[2024-10-02 00:14:05,349] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]\r\n[2024-10-02 00:14:05,350] {example_local_kubernetes_executor.py:37} WARNING - Could not import DAGs in example_local_kubernetes_executor.py\r\nTraceback (most recent call last):\r\n  File ""/opt/conda/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py"", line 35, in <module>\r\n    from kubernetes.client import models as k8s\r\nModuleNotFoundError: No module named \'kubernetes\'\r\n[2024-10-02 00:14:05,350] {example_local_kubernetes_executor.py:38} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]\r\n[2024-10-02 00:14:05,354] {example_python_operator.py:68} WARNING - The virtalenv_python example task requires virtualenv, please install it.\r\n[2024-10-02 00:14:05,388] {tutorial_taskflow_api_etl_virtualenv.py:29} WARNING - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.\r\n[2024-10-02 00:14:05,430] {task_command.py:371} INFO - Running <TaskInstance: Lead_Scoring_Data_Engineering_Pipeline.building_db scheduled__2024-10-01T00:00:00+00:00 [queued]> on host b07a3767db24\r\n[2024-10-02 00:14:06,999] {sequential_executor.py:59} INFO - Executing command: [\'airflow\', \'tasks\', \'run\', \'Lead_scoring_inference_pipeline\', \'encoding_categorical_variables\', \'scheduled__2024-10-01T07:00:00+00:00\', \'--local\', \'--subdir\', \'DAGS_FOLDER/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py\']\r\n[2024-10-02 00:14:07,807] {dagbag.py:508} INFO - Filling up the DagBag from /home/airflow/dags/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py\r\n[2024-10-02 00:14:08,731] {utils.py:145} INFO - Note: NumExpr detected 64 cores but ""NUMEXPR_MAX_THREADS"" not set, so enforcing safe limit of 8.\r\n[2024-10-02 00:14:08,731] {utils.py:157} INFO - NumExpr defaulting to 8 threads.\r\n[2024-10-02 00:14:09,066] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]\r\n[2024-10-02 00:14:09,067] {example_local_kubernetes_executor.py:37} WARNING - Could not import DAGs in example_local_kubernetes_executor.py\r\nTraceback (most recent call last):\r\n  File ""/opt/conda/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py"", line 35, in <module>\r\n    from kubernetes.client import models as k8s\r\nModuleNotFoundError: No module named \'kubernetes\'\r\n[2024-10-02 00:14:09,068] {example_local_kubernetes_executor.py:38} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]\r\n[2024-10-02 00:14:09,071] {example_python_operator.py:68} WARNING - The virtalenv_python example task requires virtualenv, please install it.\r\n[2024-10-02 00:14:09,106] {tutorial_taskflow_api_etl_virtualenv.py:29} WARNING - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.\r\n[2024-10-02 00:14:09,149] {task_command.py:371} INFO - Running <TaskInstance: Lead_scoring_inference_pipeline.encoding_categorical_variables scheduled__2024-10-01T07:00:00+00:00 [queued]> on host b07a3767db24\r\n[2024-10-02 00:14:18,873] {scheduler_job.py:599} INFO - Executor reports execution of Lead_Scoring_Data_Engineering_Pipeline.building_db run_id=scheduled__2024-10-01T00:00:00+00:00 exited with status success for try_number 1\r\n[2024-10-02 00:14:18,873] {scheduler_job.py:599} INFO - Executor reports execution of Lead_scoring_inference_pipeline.encoding_categorical_variables run_id=scheduled__2024-10-01T07:00:00+00:00 exited with status success for try_number 1\r\n[2024-10-02 00:14:18,885] {scheduler_job.py:642} INFO - TaskInstance Finished: dag_id=Lead_scoring_inference_pipeline, task_id=encoding_categorical_variables, run_id=scheduled__2024-10-01T07:00:00+00:00, map_index=-1, run_start_date=2024-10-02 00:14:09.210160+00:00, run_end_date=2024-10-02 00:14:17.928646+00:00, run_duration=8.718486, state=success, executor_state=success, try_number=1, max_tries=1, job_id=698, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-10-02 00:14:00.534756+00:00, queued_by_job_id=696, pid=1716\r\n[2024-10-02 00:14:18,885] {scheduler_job.py:642} INFO - TaskInstance Finished: dag_id=Lead_Scoring_Data_Engineering_Pipeline, task_id=building_db, run_id=scheduled__2024-10-01T00:00:00+00:00, map_index=-1, run_start_date=2024-10-02 00:14:05.489475+00:00, run_end_date=2024-10-02 00:14:05.840342+00:00, run_duration=0.350867, state=success, executor_state=success, try_number=1, max_tries=1, job_id=697, pool=default_pool, queue=default, priority_weight=7, operator=PythonOperator, queued_dttm=2024-10-02 00:14:00.534756+00:00, queued_by_job_id=696, pid=1705\r\n[2024-10-02 00:14:19,041] {dag.py:2968} INFO - Setting next_dagrun for Lead_scoring_inference_pipeline to 2024-10-02T00:00:00+00:00, run_after=2024-10-02T01:00:00+00:00\r\n[2024-10-02 00:14:19,375] {scheduler_job.py:353} INFO - 3 tasks up for execution:\r\n        <TaskInstance: Lead_Scoring_Data_Engineering_Pipeline.rawdata_schema_check scheduled__2024-10-01T00:00:00+00:00 [scheduled]>\r\n        <TaskInstance: Lead_scoring_inference_pipeline.encoding_categorical_variables scheduled__2024-10-01T23:00:00+00:00 [scheduled]>\r\n        <TaskInstance: Lead_scoring_inference_pipeline.checking_input_features scheduled__2024-10-01T07:00:00+00:00 [scheduled]>\r\n[2024-10-02 00:14:19,375] {scheduler_job.py:418} INFO - DAG Lead_Scoring_Data_Engineering_Pipeline has 0/16 running and queued tasks\r\n[2024-10-02 00:14:19,377] {scheduler_job.py:418} INFO - DAG Lead_scoring_inference_pipeline has 0/16 running and queued tasks\r\n[2024-10-02 00:14:19,377] {scheduler_job.py:418} INFO - DAG Lead_scoring_inference_pipeline has 1/16 running and queued tasks\r\n[2024-10-02 00:14:19,379] {scheduler_job.py:504} INFO - Setting the following tasks to queued state:\r\n        <TaskInstance: Lead_Scoring_Data_Engineering_Pipeline.rawdata_schema_check scheduled__2024-10-01T00:00:00+00:00 [scheduled]>\r\n        <TaskInstance: Lead_scoring_inference_pipeline.encoding_categorical_variables scheduled__2024-10-01T23:00:00+00:00 [scheduled]>\r\n        <TaskInstance: Lead_scoring_inference_pipeline.checking_input_features scheduled__2024-10-01T07:00:00+00:00 [scheduled]>\r\n[2024-10-02 00:14:19,385] {scheduler_job.py:546} INFO - Sending TaskInstanceKey(dag_id=\'Lead_Scoring_Data_Engineering_Pipeline\', task_id=\'rawdata_schema_check\', run_id=\'scheduled__2024-10-01T00:00:00+00:00\', try_number=1, map_index=-1) to executor with priority 6 and queue default\r\n[2024-10-02 00:14:19,386] {base_executor.py:91} INFO - Adding to queue: [\'airflow\', \'tasks\', \'run\', \'Lead_Scoring_Data_Engineering_Pipeline\', \'rawdata_schema_check\', \'scheduled__2024-10-01T00:00:00+00:00\', \'--local\', \'--subdir\', \'DAGS_FOLDER/Lead_scoring_data_pipeline/.ipynb_checkpoints/lead_scoring_data_pipeline-checkpoint.py\']\r\n[2024-10-02 00:14:19,387] {scheduler_job.py:546} INFO - Sending TaskInstanceKey(dag_id=\'Lead_scoring_inference_pipeline\', task_id=\'encoding_categorical_variables\', run_id=\'scheduled__2024-10-01T23:00:00+00:00\', try_number=1, map_index=-1) to executor with priority 4 and queue default\r\n[2024-10-02 00:14:19,387] {base_executor.py:91} INFO - Adding to queue: [\'airflow\', \'tasks\', \'run\', \'Lead_scoring_inference_pipeline\', \'encoding_categorical_variables\', \'scheduled__2024-10-01T23:00:00+00:00\', \'--local\', \'--subdir\', \'DAGS_FOLDER/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py\']\r\n[2024-10-02 00:14:19,389] {scheduler_job.py:546} INFO - Sending TaskInstanceKey(dag_id=\'Lead_scoring_inference_pipeline\', task_id=\'checking_input_features\', run_id=\'scheduled__2024-10-01T07:00:00+00:00\', try_number=1, map_index=-1) to executor with priority 3 and queue default\r\n[2024-10-02 00:14:19,389] {base_executor.py:91} INFO - Adding to queue: [\'airflow\', \'tasks\', \'run\', \'Lead_scoring_inference_pipeline\', \'checking_input_features\', \'scheduled__2024-10-01T07:00:00+00:00\', \'--local\', \'--subdir\', \'DAGS_FOLDER/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py\']\r\n[2024-10-02 00:14:19,411] {sequential_executor.py:59} INFO - Executing command: [\'airflow\', \'tasks\', \'run\', \'Lead_Scoring_Data_Engineering_Pipeline\', \'rawdata_schema_check\', \'scheduled__2024-10-01T00:00:00+00:00\', \'--local\', \'--subdir\', \'DAGS_FOLDER/Lead_scoring_data_pipeline/.ipynb_checkpoints/lead_scoring_data_pipeline-checkpoint.py\']\r\n[2024-10-02 00:14:20,324] {dagbag.py:508} INFO - Filling up the DagBag from /home/airflow/dags/Lead_scoring_data_pipeline/.ipynb_checkpoints/lead_scoring_data_pipeline-checkpoint.py\r\n[2024-10-02 00:14:20,569] {utils.py:145} INFO - Note: NumExpr detected 64 cores but ""NUMEXPR_MAX_THREADS"" not set, so enforcing safe limit of 8.\r\n[2024-10-02 00:14:20,570] {utils.py:157} INFO - NumExpr defaulting to 8 threads.\r\n[2024-10-02 00:14:25,191] {lead_scoring_data_pipeline-checkpoint.py:47} INFO - Creating mlflow experiment\r\n[2024-10-02 00:14:25,518] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]\r\n[2024-10-02 00:14:25,520] {example_local_kubernetes_executor.py:37} WARNING - Could not import DAGs in example_local_kubernetes_executor.py\r\nTraceback (most recent call last):\r\n  File ""/opt/conda/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py"", line 35, in <module>\r\n    from kubernetes.client import models as k8s\r\nModuleNotFoundError: No module named \'kubernetes\'\r\n[2024-10-02 00:14:25,520] {example_local_kubernetes_executor.py:38} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]\r\n[2024-10-02 00:14:25,524] {example_python_operator.py:68} WARNING - The virtalenv_python example task requires virtualenv, please install it.\r\n[2024-10-02 00:14:25,592] {tutorial_taskflow_api_etl_virtualenv.py:29} WARNING - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.\r\n[2024-10-02 00:14:25,684] {task_command.py:371} INFO - Running <TaskInstance: Lead_Scoring_Data_Engineering_Pipeline.rawdata_schema_check scheduled__2024-10-01T00:00:00+00:00 [queued]> on host b07a3767db24\r\n[2024-10-02 00:14:34,723] {sequential_executor.py:59} INFO - Executing command: [\'airflow\', \'tasks\', \'run\', \'Lead_scoring_inference_pipeline\', \'encoding_categorical_variables\', \'scheduled__2024-10-01T23:00:00+00:00\', \'--local\', \'--subdir\', \'DAGS_FOLDER/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py\']\r\n[2024-10-02 00:14:37,632] {dagbag.py:508} INFO - Filling up the DagBag from /home/airflow/dags/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py\r\n[2024-10-02 00:14:38,717] {utils.py:145} INFO - Note: NumExpr detected 64 cores but ""NUMEXPR_MAX_THREADS"" not set, so enforcing safe limit of 8.\r\n[2024-10-02 00:14:38,717] {utils.py:157} INFO - NumExpr defaulting to 8 threads.\r\n[2024-10-02 00:14:39,057] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]\r\n[2024-10-02 00:14:39,058] {example_local_kubernetes_executor.py:37} WARNING - Could not import DAGs in example_local_kubernetes_executor.py\r\nTraceback (most recent call last):\r\n  File ""/opt/conda/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py"", line 35, in <module>\r\n    from kubernetes.client import models as k8s\r\nModuleNotFoundError: No module named \'kubernetes\'\r\n[2024-10-02 00:14:39,058] {example_local_kubernetes_executor.py:38} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]\r\n[2024-10-02 00:14:39,062] {example_python_operator.py:68} WARNING - The virtalenv_python example task requires virtualenv, please install it.\r\n[2024-10-02 00:14:39,097] {tutorial_taskflow_api_etl_virtualenv.py:29} WARNING - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.\r\n[2024-10-02 00:14:39,149] {task_command.py:371} INFO - Running <TaskInstance: Lead_scoring_inference_pipeline.encoding_categorical_variables scheduled__2024-10-01T23:00:00+00:00 [queued]> on host b07a3767db24\r\n[2024-10-02 00:14:46,811] {sequential_executor.py:59} INFO - Executing command: [\'airflow\', \'tasks\', \'run\', \'Lead_scoring_inference_pipeline\', \'checking_input_features\', \'scheduled__2024-10-01T07:00:00+00:00\', \'--local\', \'--subdir\', \'DAGS_FOLDER/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py\']\r\n[2024-10-02 00:14:47,576] {dagbag.py:508} INFO - Filling up the DagBag from /home/airflow/dags/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py\r\n[2024-10-02 00:14:48,445] {utils.py:145} INFO - Note: NumExpr detected 64 cores but ""NUMEXPR_MAX_THREADS"" not set, so enforcing safe limit of 8.\r\n[2024-10-02 00:14:48,445] {utils.py:157} INFO - NumExpr defaulting to 8 threads.\r\n[2024-10-02 00:14:48,786] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]\r\n[2024-10-02 00:14:48,787] {example_local_kubernetes_executor.py:37} WARNING - Could not import DAGs in example_local_kubernetes_executor.py\r\nTraceback (most recent call last):\r\n  File ""/opt/conda/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py"", line 35, in <module>\r\n    from kubernetes.client import models as k8s\r\nModuleNotFoundError: No module named \'kubernetes\'\r\n[2024-10-02 00:14:48,788] {example_local_kubernetes_executor.py:38} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]\r\n[2024-10-02 00:14:48,791] {example_python_operator.py:68} WARNING - The virtalenv_python example task requires virtualenv, please install it.\r\n[2024-10-02 00:14:48,825] {tutorial_taskflow_api_etl_virtualenv.py:29} WARNING - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.\r\n[2024-10-02 00:14:48,866] {task_command.py:371} INFO - Running <TaskInstance: Lead_scoring_inference_pipeline.checking_input_features scheduled__2024-10-01T07:00:00+00:00 [queued]> on host b07a3767db24\r\n[2024-10-02 00:14:54,893] {scheduler_job.py:599} INFO - Executor reports execution of Lead_Scoring_Data_Engineering_Pipeline.rawdata_schema_check run_id=scheduled__2024-10-01T00:00:00+00:00 exited with status success for try_number 1\r\n[2024-10-02 00:14:54,893] {scheduler_job.py:599} INFO - Executor reports execution of Lead_scoring_inference_pipeline.encoding_categorical_variables run_id=scheduled__2024-10-01T23:00:00+00:00 exited with status success for try_number 1\r\n[2024-10-02 00:14:54,893] {scheduler_job.py:599} INFO - Executor reports execution of Lead_scoring_inference_pipeline.checking_input_features run_id=scheduled__2024-10-01T07:00:00+00:00 exited with status success for try_number 1\r\n[2024-10-02 00:14:54,899] {scheduler_job.py:642} INFO - TaskInstance Finished: dag_id=Lead_scoring_inference_pipeline, task_id=checking_input_features, run_id=scheduled__2024-10-01T07:00:00+00:00, map_index=-1, run_start_date=2024-10-02 00:14:49.204026+00:00, run_end_date=2024-10-02 00:14:54.036880+00:00, run_duration=4.832854, state=success, executor_state=success, try_number=1, max_tries=1, job_id=701, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-10-02 00:14:19.380465+00:00, queued_by_job_id=696, pid=1758\r\n[2024-10-02 00:14:54,900] {scheduler_job.py:642} INFO - TaskInstance Finished: dag_id=Lead_Scoring_Data_Engineering_Pipeline, task_id=rawdata_schema_check, run_id=scheduled__2024-10-01T00:00:00+00:00, map_index=-1, run_start_date=2024-10-02 00:14:25.895394+00:00, run_end_date=2024-10-02 00:14:30.420493+00:00, run_duration=4.525099, state=success, executor_state=success, try_number=1, max_tries=1, job_id=699, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-10-02 00:14:19.380465+00:00, queued_by_job_id=696, pid=1732\r\n[2024-10-02 00:14:54,900] {scheduler_job.py:642} INFO - TaskInstance Finished: dag_id=Lead_scoring_inference_pipeline, task_id=encoding_categorical_variables, run_id=scheduled__2024-10-01T23:00:00+00:00, map_index=-1, run_start_date=2024-10-02 00:14:39.228254+00:00, run_end_date=2024-10-02 00:14:46.043804+00:00, run_duration=6.81555, state=success, executor_state=success, try_number=1, max_tries=1, job_id=700, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-10-02 00:14:19.380465+00:00, queued_by_job_id=696, pid=1747\r\n[2024-10-02 00:14:55,185] {scheduler_job.py:353} INFO - 3 tasks up for execution:\r\n        <TaskInstance: Lead_Scoring_Data_Engineering_Pipeline.loading_data scheduled__2024-10-01T00:00:00+00:00 [scheduled]>\r\n        <TaskInstance: Lead_scoring_inference_pipeline.checking_input_features scheduled__2024-10-01T23:00:00+00:00 [scheduled]>\r\n        <TaskInstance: Lead_scoring_inference_pipeline.generating_models_prediction scheduled__2024-10-01T07:00:00+00:00 [scheduled]>\r\n[2024-10-02 00:14:55,186] {scheduler_job.py:418} INFO - DAG Lead_Scoring_Data_Engineering_Pipeline has 0/16 running and queued tasks\r\n[2024-10-02 00:14:55,186] {scheduler_job.py:418} INFO - DAG Lead_scoring_inference_pipeline has 0/16 running and queued tasks\r\n[2024-10-02 00:14:55,186] {scheduler_job.py:418} INFO - DAG Lead_scoring_inference_pipeline has 1/16 running and queued tasks\r\n[2024-10-02 00:14:55,187] {scheduler_job.py:504} INFO - Setting the following tasks to queued state:\r\n        <TaskInstance: Lead_Scoring_Data_Engineering_Pipeline.loading_data scheduled__2024-10-01T00:00:00+00:00 [scheduled]>\r\n        <TaskInstance: Lead_scoring_inference_pipeline.checking_input_features scheduled__2024-10-01T23:00:00+00:00 [scheduled]>\r\n        <TaskInstance: Lead_scoring_inference_pipeline.generating_models_prediction scheduled__2024-10-01T07:00:00+00:00 [scheduled]>\r\n[2024-10-02 00:14:55,192] {scheduler_job.py:546} INFO - Sending TaskInstanceKey(dag_id=\'Lead_Scoring_Data_Engineering_Pipeline\', task_id=\'loading_data\', run_id=\'scheduled__2024-10-01T00:00:00+00:00\', try_number=1, map_index=-1) to executor with priority 5 and queue default\r\n[2024-10-02 00:14:55,193] {base_executor.py:91} INFO - Adding to queue: [\'airflow\', \'tasks\', \'run\', \'Lead_Scoring_Data_Engineering_Pipeline\', \'loading_data\', \'scheduled__2024-10-01T00:00:00+00:00\', \'--local\', \'--subdir\', \'DAGS_FOLDER/Lead_scoring_data_pipeline/.ipynb_checkpoints/lead_scoring_data_pipeline-checkpoint.py\']\r\n[2024-10-02 00:14:55,194] {scheduler_job.py:546} INFO - Sending TaskInstanceKey(dag_id=\'Lead_scoring_inference_pipeline\', task_id=\'checking_input_features\', run_id=\'scheduled__2024-10-01T23:00:00+00:00\', try_number=1, map_index=-1) to executor with priority 3 and queue default\r\n[2024-10-02 00:14:55,194] {base_executor.py:91} INFO - Adding to queue: [\'airflow\', \'tasks\', \'run\', \'Lead_scoring_inference_pipeline\', \'checking_input_features\', \'scheduled__2024-10-01T23:00:00+00:00\', \'--local\', \'--subdir\', \'DAGS_FOLDER/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py\']\r\n[2024-10-02 00:14:55,195] {scheduler_job.py:546} INFO - Sending TaskInstanceKey(dag_id=\'Lead_scoring_inference_pipeline\', task_id=\'generating_models_prediction\', run_id=\'scheduled__2024-10-01T07:00:00+00:00\', try_number=1, map_index=-1) to executor with priority 2 and queue default\r\n[2024-10-02 00:14:55,196] {base_executor.py:91} INFO - Adding to queue: [\'airflow\', \'tasks\', \'run\', \'Lead_scoring_inference_pipeline\', \'generating_models_prediction\', \'scheduled__2024-10-01T07:00:00+00:00\', \'--local\', \'--subdir\', \'DAGS_FOLDER/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py\']\r\n[2024-10-02 00:14:55,250] {sequential_executor.py:59} INFO - Executing command: [\'airflow\', \'tasks\', \'run\', \'Lead_Scoring_Data_Engineering_Pipeline\', \'loading_data\', \'scheduled__2024-10-01T00:00:00+00:00\', \'--local\', \'--subdir\', \'DAGS_FOLDER/Lead_scoring_data_pipeline/.ipynb_checkpoints/lead_scoring_data_pipeline-checkpoint.py\']\r\n[2024-10-02 00:14:55,912] {dagbag.py:508} INFO - Filling up the DagBag from /home/airflow/dags/Lead_scoring_data_pipeline/.ipynb_checkpoints/lead_scoring_data_pipeline-checkpoint.py\r\n[2024-10-02 00:14:56,141] {utils.py:145} INFO - Note: NumExpr detected 64 cores but ""NUMEXPR_MAX_THREADS"" not set, so enforcing safe limit of 8.\r\n[2024-10-02 00:14:56,141] {utils.py:157} INFO - NumExpr defaulting to 8 threads.\r\n[2024-10-02 00:14:59,093] {lead_scoring_data_pipeline-checkpoint.py:47} INFO - Creating mlflow experiment\r\n[2024-10-02 00:14:59,253] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]\r\n[2024-10-02 00:14:59,254] {example_local_kubernetes_executor.py:37} WARNING - Could not import DAGs in example_local_kubernetes_executor.py\r\nTraceback (most recent call last):\r\n  File ""/opt/conda/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py"", line 35, in <module>\r\n    from kubernetes.client import models as k8s\r\nModuleNotFoundError: No module named \'kubernetes\'\r\n[2024-10-02 00:14:59,254] {example_local_kubernetes_executor.py:38} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]\r\n[2024-10-02 00:14:59,258] {example_python_operator.py:68} WARNING - The virtalenv_python example task requires virtualenv, please install it.\r\n[2024-10-02 00:14:59,291] {tutorial_taskflow_api_etl_virtualenv.py:29} WARNING - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.\r\n[2024-10-02 00:14:59,548] {task_command.py:371} INFO - Running <TaskInstance: Lead_Scoring_Data_Engineering_Pipeline.loading_data scheduled__2024-10-01T00:00:00+00:00 [queued]> on host b07a3767db24\r\n[2024-10-02 00:15:04,403] {sequential_executor.py:59} INFO - Executing command: [\'airflow\', \'tasks\', \'run\', \'Lead_scoring_inference_pipeline\', \'checking_input_features\', \'scheduled__2024-10-01T23:00:00+00:00\', \'--local\', \'--subdir\', \'DAGS_FOLDER/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py\']\r\n[2024-10-02 00:15:05,080] {dagbag.py:508} INFO - Filling up the DagBag from /home/airflow/dags/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py\r\n[2024-10-02 00:15:05,983] {utils.py:145} INFO - Note: NumExpr detected 64 cores but ""NUMEXPR_MAX_THREADS"" not set, so enforcing safe limit of 8.\r\n[2024-10-02 00:15:05,983] {utils.py:157} INFO - NumExpr defaulting to 8 threads.\r\n[2024-10-02 00:15:06,324] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]\r\n[2024-10-02 00:15:06,325] {example_local_kubernetes_executor.py:37} WARNING - Could not import DAGs in example_local_kubernetes_executor.py\r\nTraceback (most recent call last):\r\n  File ""/opt/conda/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py"", line 35, in <module>\r\n    from kubernetes.client import models as k8s\r\nModuleNotFoundError: No module named \'kubernetes\'\r\n[2024-10-02 00:15:06,326] {example_local_kubernetes_executor.py:38} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]\r\n[2024-10-02 00:15:06,329] {example_python_operator.py:68} WARNING - The virtalenv_python example task requires virtualenv, please install it.\r\n[2024-10-02 00:15:06,367] {tutorial_taskflow_api_etl_virtualenv.py:29} WARNING - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.\r\n[2024-10-02 00:15:06,412] {task_command.py:371} INFO - Running <TaskInstance: Lead_scoring_inference_pipeline.checking_input_features scheduled__2024-10-01T23:00:00+00:00 [queued]> on host b07a3767db24\r\n\r\n\r\nMLFLOW : Log:\r\n==================\r\n\r\n***@***.***:~/airflow/dags/Lead_scoring_data_pipeline# mlflow server --backend-store-uri=\'sqlite:///database/lead_scoring_data_cleaning.db\' --default-artifact-root=""mlruns/"" --port=6006 --host=0.0.0.0\r\n[2024-10-02 00:13:43 +0000] [1650] [INFO] Starting gunicorn 20.1.0\r\n[2024-10-02 00:13:43 +0000] [1650] [INFO] Listening at: http://0.0.0.0:6006 (1650)\r\n[2024-10-02 00:13:43 +0000] [1650] [INFO] Using worker: sync\r\n[2024-10-02 00:13:43 +0000] [1652] [INFO] Booting worker with pid: 1652\r\n[2024-10-02 00:13:44 +0000] [1653] [INFO] Booting worker with pid: 1653\r\n[2024-10-02 00:13:44 +0000] [1654] [INFO] Booting worker with pid: 1654\r\n[2024-10-02 00:13:44 +0000] [1655] [INFO] Booting worker with pid: 1655', 'created_at': datetime.datetime(2024, 10, 2, 0, 18, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2389524969, 'issue_id': 2554217829, 'author': 'jscheffl', 'body': 'Conflig looks good but still I feel like the exception is not usual.\r\n\r\nHypothesis: Something is broken in your install. One option would be a clean re-install (don\'t forget to use the right constraints) and even much better I\'d recommend to use a recent version. 2.3.3 for sure will not receive fixes. And the error is so basic that I doubt it is a SW problem but rather something in your setup or environment. Config at least looks ""proper"".', 'created_at': datetime.datetime(2024, 10, 2, 19, 31, 23, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-28 13:20:02 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

koustreak on (2024-09-29 18:39:06 UTC): env:
  - name: AIRFLOW__CORE__DEFAULT_TIMEZONE
    value: ""UTC""  

Have you tried this one ??

jscheffl on (2024-09-30 20:42:02 UTC): Version 2.3.3 of Airflow is pretty pretty old. Have you considered upgrading to a more recent version?

psubudhi (Issue Creator) on (2024-10-01 02:16:09 UTC): We have received the lab with pre-installed airflow, and will check with
the team if the airflow can be upgraded to the latest version.

But don't we have a solution for the above issue?
Thanks,
Prem


On Tue, 1 Oct 2024 at 02:12, Jens Scheffler ***@***.***>
wrote:

psubudhi (Issue Creator) on (2024-10-01 07:44:00 UTC): I checked the config 

default_timezone = utc

psubudhi (Issue Creator) on (2024-10-01 07:48:36 UTC): fyi

![image](https://github.com/user-attachments/assets/fbb051fd-064c-4ab5-8b09-0a49e3edf6be)
![image](https://github.com/user-attachments/assets/e4aa4beb-39ba-4609-9dce-359e0a763156)

jscheffl on (2024-10-01 19:06:38 UTC): Re-reading the stack trace there is something seriously wrog with your system anyway. The error is rooted becasue the webser attempts to render an error message that the scheduler in unhealty.

Are you sure that the `default_timezone` is set consistently across all nodes? Looking at the configuration reference `utc`is actually the default (see https://airflow.apache.org/docs/apache-airflow/2.3.3/configurations-ref.html)

Note that the upper case version `AIRFLOW__CORE__...` in your config is the version to over-ride the CFG file from ENV variables. To debug what actually is used, can you open a shell i nthe context of the webserver and run `airflow config list` on the command line and check which value is active?

psubudhi (Issue Creator) on (2024-10-02 00:18:57 UTC): Thank you Jens for the reply. Here are my inputs to your asks.
1. I am using a single node server.
2. Here is the config list
***@***.***:~# airflow config list|grep timezone
default_timezone = utc
default_ui_timezone = UTC
***@***.***:~#

FYI More log of scheduler and web server attached. Please let me know if
you have any details you might need from the host node.

Regards,
Prem

On Wed, 2 Oct 2024 at 00:37, Jens Scheffler ***@***.***>
wrote:


***@***.***:~# airflow config list
[core]
dags_folder = /home/airflow/dags
hostname_callable = socket.getfqdn
default_timezone = utc
executor = SequentialExecutor
parallelism = 32
max_active_tasks_per_dag = 16
dags_are_paused_at_creation = True
max_active_runs_per_dag = 16
load_examples = True
plugins_folder = /home/airflow/plugins
execute_tasks_new_python_interpreter = False
fernet_key = 
donot_pickle = True
dagbag_import_timeout = 30.0
dagbag_import_error_tracebacks = True
dagbag_import_error_traceback_depth = 2
dag_file_processor_timeout = 50
task_runner = StandardTaskRunner
default_impersonation = 
security = 
unit_test_mode = False
enable_xcom_pickling = False
killed_task_cleanup_time = 60
dag_run_conf_overrides_params = True
dag_discovery_safe_mode = True
dag_ignore_file_syntax = regexp
default_task_retries = 0
default_task_weight_rule = downstream
default_task_execution_timeout = 
min_serialized_dag_update_interval = 30
compress_serialized_dags = False
min_serialized_dag_fetch_interval = 10
max_num_rendered_ti_fields_per_task = 30
check_slas = True
xcom_backend = airflow.models.xcom.BaseXCom
lazy_load_plugins = True
lazy_discover_providers = True
hide_sensitive_var_conn_fields = True
sensitive_var_conn_names = 
default_pool_task_slot_count = 128
max_map_length = 1024
AIRFLOW__CORE__DEFAULT_TIMEZONE = UTC

[database]
sql_alchemy_conn = sqlite:////home/airflow/airflow.db
sql_engine_encoding = utf-8
sql_alchemy_pool_enabled = True
sql_alchemy_pool_size = 5
sql_alchemy_max_overflow = 10
sql_alchemy_pool_recycle = 1800
sql_alchemy_pool_pre_ping = True
sql_alchemy_schema = 
load_default_connections = True
max_db_retries = 3

[logging]
base_log_folder = /home/airflow/logs
remote_logging = False
remote_log_conn_id = 
google_key_path = 
remote_base_log_folder = 
encrypt_s3_logs = False
logging_level = INFO
celery_logging_level = 
fab_logging_level = WARNING
logging_config_class = 
colored_console_log = True
colored_log_format = [%(blue)s%(asctime)s%(reset)s] {%(blue)s%(filename)s:%(reset)s%(lineno)d} %(log_color)s%(levelname)s%(reset)s - %(log_color)s%(message)s%(reset)s
colored_formatter_class = airflow.utils.log.colored_log.CustomTTYColoredFormatter
log_format = [%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s
simple_log_format = %(asctime)s %(levelname)s - %(message)s
task_log_prefix_template = 
log_filename_template = dag_id={{ ti.dag_id }}/run_id={{ ti.run_id }}/task_id={{ ti.task_id }}/{% if ti.map_index >= 0 %}map_index={{ ti.map_index }}/{% endif %}attempt={{ try_number }}.log
log_processor_filename_template = {{ filename }}.log
dag_processor_manager_log_location = /home/airflow/logs/dag_processor_manager/dag_processor_manager.log
task_log_reader = task
extra_logger_names = 
worker_log_server_port = 8793

[metrics]
statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow
statsd_allow_list = 
stat_name_handler = 
statsd_datadog_enabled = False
statsd_datadog_tags = 

[secrets]
backend = 
backend_kwargs = 

[cli]
api_client = airflow.api.client.local_client
endpoint_url = http://localhost:8080

[debug]
fail_fast = False

[api]
enable_experimental_api = False
auth_backends = airflow.api.auth.backend.session
maximum_page_limit = 100
fallback_page_limit = 100
google_oauth2_audience = 
google_key_path = 
access_control_allow_headers = 
access_control_allow_methods = 
access_control_allow_origins = 

[lineage]
backend = 

[atlas]
sasl_enabled = False
host = 
port = 21000
username = 
password = 

[operators]
default_owner = airflow
default_cpus = 1
default_ram = 512
default_disk = 512
default_gpus = 0
default_queue = default
allow_illegal_arguments = False

[hive]
default_hive_mapred_queue = 

[webserver]
base_url = http://localhost:8080
default_ui_timezone = UTC
web_server_host = 0.0.0.0
web_server_port = 6007
web_server_ssl_cert = 
web_server_ssl_key = 
session_backend = database
web_server_master_timeout = 120
web_server_worker_timeout = 120
worker_refresh_batch_size = 1
worker_refresh_interval = 6000
reload_on_plugin_change = False
secret_key = CbGpipwchu6Dg00sg5m0ZA==
workers = 4
worker_class = sync
access_logfile = -
error_logfile = -
access_logformat = 
expose_config = False
expose_hostname = True
expose_stacktrace = True
dag_default_view = grid
dag_orientation = LR
log_fetch_timeout_sec = 5
log_fetch_delay_sec = 2
log_auto_tailing_offset = 30
log_animation_speed = 1000
hide_paused_dags_by_default = False
page_size = 100
navbar_color = #fff
default_dag_run_display_number = 25
enable_proxy_fix = False
proxy_fix_x_for = 1
proxy_fix_x_proto = 1
proxy_fix_x_host = 1
proxy_fix_x_port = 1
proxy_fix_x_prefix = 1
cookie_secure = False
cookie_samesite = Lax
default_wrap = False
x_frame_enabled = True
show_recent_stats_for_completed_runs = True
update_fab_perms = True
session_lifetime_minutes = 43200
instance_name_has_markup = False
auto_refresh_interval = 3
warn_deployment_exposure = True
audit_view_excluded_events = gantt,landing_times,tries,duration,calendar,graph,grid,tree,tree_data

[email]
email_backend = airflow.utils.email.send_email_smtp
email_conn_id = smtp_default
default_email_on_retry = True
default_email_on_failure = True

[smtp]
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
smtp_port = 25
smtp_mail_from = ***@***.***
smtp_timeout = 30
smtp_retry_limit = 5

[sentry]
sentry_on = False
sentry_dsn = 

[local_kubernetes_executor]
kubernetes_queue = kubernetes

[celery_kubernetes_executor]
kubernetes_queue = kubernetes

[celery]
celery_app_name = airflow.executors.celery_executor
worker_concurrency = 16
worker_prefetch_multiplier = 1
worker_enable_remote_control = True
worker_umask = 0o077
broker_url = redis://redis:6379/0
result_backend = ***@***.***/airflow
flower_host = 0.0.0.0
flower_url_prefix = 
flower_port = 5555
flower_basic_auth = 
sync_parallelism = 0
celery_config_options = airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG
ssl_active = False
ssl_key = 
ssl_cert = 
ssl_cacert = 
pool = prefork
operation_timeout = 1.0
task_track_started = True
task_adoption_timeout = 600
stalled_task_timeout = 0
task_publish_max_retries = 3
worker_precheck = False

[celery_broker_transport_options]

[dask]
cluster_address = 127.0.0.1:8786
tls_ca = 
tls_cert = 
tls_key = 

[scheduler]
job_heartbeat_sec = 5
scheduler_heartbeat_sec = 5
num_runs = -1
scheduler_idle_sleep_time = 1
min_file_process_interval = 30
deactivate_stale_dags_interval = 60
dag_dir_list_interval = 300
print_stats_interval = 30
pool_metrics_interval = 5.0
scheduler_health_check_threshold = 30
orphaned_tasks_check_interval = 300.0
child_process_log_directory = /home/airflow/logs/scheduler
scheduler_zombie_task_threshold = 300
zombie_detection_interval = 10.0
catchup_by_default = True
ignore_first_depends_on_past_by_default = True
max_tis_per_query = 512
use_row_level_locking = True
max_dagruns_to_create_per_loop = 10
max_dagruns_per_loop_to_schedule = 20
schedule_after_task_execution = True
parsing_processes = 2
file_parsing_sort_mode = modified_time
standalone_dag_processor = False
max_callbacks_per_loop = 20
use_job_schedule = True
allow_trigger_in_future = False
dependency_detector = airflow.serialization.serialized_objects.DependencyDetector
trigger_timeout_check_interval = 15

[triggerer]
default_capacity = 1000

[kerberos]
ccache = /tmp/airflow_krb5_ccache
principal = airflow
reinit_frequency = 3600
kinit_path = kinit
keytab = airflow.keytab
forwardable = True
include_ip = True

[github_enterprise]
api_rev = v3

[elasticsearch]
host = 
log_id_template = {dag_id}-{task_id}-{run_id}-{map_index}-{try_number}
end_of_log_mark = end_of_log
frontend = 
write_stdout = False
json_format = False
json_fields = asctime, filename, lineno, levelname, message
host_field = host
offset_field = offset

[elasticsearch_configs]
use_ssl = False
verify_certs = True

[kubernetes]
pod_template_file = 
worker_container_repository = 
worker_container_tag = 
namespace = default
delete_worker_pods = True
delete_worker_pods_on_failure = False
worker_pods_creation_batch_size = 1
multi_namespace_mode = False
in_cluster = True
kube_client_request_args = 
delete_option_kwargs = 
enable_tcp_keepalive = True
tcp_keep_idle = 120
tcp_keep_intvl = 30
tcp_keep_cnt = 6
verify_ssl = True
worker_pods_pending_timeout = 300
worker_pods_pending_timeout_check_interval = 120
worker_pods_queued_check_interval = 60
worker_pods_pending_timeout_batch_size = 100

[sensors]
default_timeout = 604800

[smart_sensor]
use_smart_sensor = False
shard_code_upper_limit = 10000
shards = 5
sensors_enabled = NamedHivePartitionSensor

***@***.***:~# airflow config list|grep time
default_timezone = utc
dagbag_import_timeout = 30.0
dag_file_processor_timeout = 50
killed_task_cleanup_time = 60
default_task_execution_timeout = 
colored_log_format = [%(blue)s%(asctime)s%(reset)s] {%(blue)s%(filename)s:%(reset)s%(lineno)d} %(log_color)s%(levelname)s%(reset)s - %(log_color)s%(message)s%(reset)s
log_format = [%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s
simple_log_format = %(asctime)s %(levelname)s - %(message)s
default_ui_timezone = UTC
web_server_master_timeout = 120
web_server_worker_timeout = 120
log_fetch_timeout_sec = 5
session_lifetime_minutes = 43200
audit_view_excluded_events = gantt,landing_times,tries,duration,calendar,graph,grid,tree,tree_data
smtp_timeout = 30
operation_timeout = 1.0
task_adoption_timeout = 600
stalled_task_timeout = 0
scheduler_idle_sleep_time = 1
file_parsing_sort_mode = modified_time
trigger_timeout_check_interval = 15
json_fields = asctime, filename, lineno, levelname, message
worker_pods_pending_timeout = 300
worker_pods_pending_timeout_check_interval = 120
worker_pods_pending_timeout_batch_size = 100
default_timeout = 604800
***@***.***:~# airflow config list|grep timezone
default_timezone = utc
default_ui_timezone = UTC
***@***.***:~# airflow webserver --port 6007
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
Running the Gunicorn Server with:
Workers: 4 sync
Host: 0.0.0.0:6007
Timeout: 120
Logfiles: - -
Access Logformat: 
=================================================================
[2024-10-02 00:14:25 +0000] [1730] [INFO] Starting gunicorn 20.1.0
[2024-10-02 00:14:26 +0000] [1730] [INFO] Listening at: http://0.0.0.0:6007 (1730)
[2024-10-02 00:14:26 +0000] [1730] [INFO] Using worker: sync
[2024-10-02 00:14:26 +0000] [1733] [INFO] Booting worker with pid: 1733
[2024-10-02 00:14:26 +0000] [1734] [INFO] Booting worker with pid: 1734
[2024-10-02 00:14:26 +0000] [1735] [INFO] Booting worker with pid: 1735
[2024-10-02 00:14:26 +0000] [1736] [INFO] Booting worker with pid: 1736
10.232.204.18 - - [02/Oct/2024:00:14:37 +0000] ""GET / HTTP/1.1"" 302 197 ""https://jarvislabs.ai/"" ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""
/opt/conda/lib/python3.8/site-packages/airflow/www/views.py:889 SADeprecationWarning: DISTINCT ON is currently supported only by the PostgreSQL dialect.  Use of DISTINCT ON for other backends is currently silently ignored, however this usage is deprecated, and will raise CompileError in a future release for all backends that do not support this syntax.
[2024-10-02 00:14:37,965] {app.py:1449} ERROR - Exception on /home [GET]
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/flask/app.py"", line 2073, in wsgi_app
    response = self.full_dispatch_request()
  File ""/opt/conda/lib/python3.8/site-packages/flask/app.py"", line 1519, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/opt/conda/lib/python3.8/site-packages/flask/app.py"", line 1517, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/opt/conda/lib/python3.8/site-packages/flask/app.py"", line 1503, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**req.view_args)
  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/auth.py"", line 46, in decorated
    return func(*args, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/views.py"", line 964, in index
    return self.render_template(
  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/views.py"", line 709, in render_template
    return super().render_template(
  File ""/opt/conda/lib/python3.8/site-packages/flask_appbuilder/baseviews.py"", line 322, in render_template
    return render_template(
  File ""/opt/conda/lib/python3.8/site-packages/flask/templating.py"", line 154, in render_template
    return _render(
  File ""/opt/conda/lib/python3.8/site-packages/flask/templating.py"", line 128, in _render
    rv = template.render(context)
  File ""/opt/conda/lib/python3.8/site-packages/jinja2/environment.py"", line 1291, in render
    self.environment.handle_exception()
  File ""/opt/conda/lib/python3.8/site-packages/jinja2/environment.py"", line 925, in handle_exception
    raise rewrite_traceback_stack(source=source)
  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/templates/airflow/dags.html"", line 43, in top-level template code
    {% elif curr_ordering_direction == 'asc' and request.args.get('sorting_key') == attribute_name %}
  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/templates/airflow/main.html"", line 21, in top-level template code
    {% from 'airflow/_messages.html' import show_message %}
  File ""/opt/conda/lib/python3.8/site-packages/flask_appbuilder/templates/appbuilder/baselayout.html"", line 2, in top-level template code
    {% import 'appbuilder/baselib.html' as baselib %}
  File ""/opt/conda/lib/python3.8/site-packages/flask_appbuilder/templates/appbuilder/init.html"", line 37, in top-level template code
    {% block body %}
  File ""/opt/conda/lib/python3.8/site-packages/flask_appbuilder/templates/appbuilder/baselayout.html"", line 16, in block 'body'
    {% block messages %}
  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/templates/airflow/dags.html"", line 104, in block 'messages'
    {{ super() }}
  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/templates/airflow/main.html"", line 62, in block 'messages'
    {% call show_message(category='warning', dismissible=false) %}
  File ""/opt/conda/lib/python3.8/site-packages/jinja2/runtime.py"", line 828, in _invoke
    rv = self._func(*arguments)
  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/templates/airflow/_messages.html"", line 25, in template
    {{ caller() }}
  File ""/opt/conda/lib/python3.8/site-packages/jinja2/runtime.py"", line 828, in _invoke
    rv = self._func(*arguments)
  File ""/opt/conda/lib/python3.8/site-packages/airflow/www/templates/airflow/main.html"", line 70, in template
  File ""/opt/conda/lib/python3.8/site-packages/airflow/macros/__init__.py"", line 77, in datetime_diff_for_humans
    return pendulum.instance(dt).diff_for_humans(since)
  File ""/opt/conda/lib/python3.8/site-packages/pendulum/datetime.py"", line 824, in diff_for_humans
    other = self.now()
  File ""/opt/conda/lib/python3.8/site-packages/pendulum/datetime.py"", line 106, in now
    return pendulum.now(tz)
  File ""/opt/conda/lib/python3.8/site-packages/pendulum/__init__.py"", line 211, in now
    dt = _datetime.datetime.now(local_timezone())
  File ""/opt/conda/lib/python3.8/site-packages/pendulum/tz/__init__.py"", line 60, in local_timezone
    return get_local_timezone()
  File ""/opt/conda/lib/python3.8/site-packages/pendulum/tz/local_timezone.py"", line 35, in get_local_timezone
    tz = _get_system_timezone()
  File ""/opt/conda/lib/python3.8/site-packages/pendulum/tz/local_timezone.py"", line 63, in _get_system_timezone
    return _get_unix_timezone()
  File ""/opt/conda/lib/python3.8/site-packages/pendulum/tz/local_timezone.py"", line 242, in _get_unix_timezone
    raise RuntimeError(""Unable to find any timezone configuration"")
RuntimeError: Unable to find any timezone configuration
10.232.204.18 - - [02/Oct/2024:00:14:38 +0000] ""GET /home HTTP/1.1"" 500 6481 ""https://jarvislabs.ai/"" ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36""



Scheduler log:
===============
***@***.***:~# airflow scheduler
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2024-10-02 00:13:55 +0000] [1690] [INFO] Starting gunicorn 20.1.0
[2024-10-02 00:13:55 +0000] [1690] [INFO] Listening at: http://0.0.0.0:8793 (1690)
[2024-10-02 00:13:55 +0000] [1690] [INFO] Using worker: sync
[2024-10-02 00:13:55 +0000] [1691] [INFO] Booting worker with pid: 1691
[2024-10-02 00:13:55,601] {scheduler_job.py:708} INFO - Starting the scheduler
[2024-10-02 00:13:55,601] {scheduler_job.py:713} INFO - Processing each file at most -1 times
[2024-10-02 00:13:55,603] {executor_loader.py:105} INFO - Loaded executor: SequentialExecutor
[2024-10-02 00:13:55,608] {manager.py:160} INFO - Launched DagFileProcessorManager with pid: 1692
[2024-10-02 00:13:55,610] {scheduler_job.py:1233} INFO - Resetting orphaned tasks for active dag runs
[2024-10-02 00:13:55,622] {settings.py:55} INFO - Configured default timezone Timezone('UTC')
[2024-10-02 00:13:55,625] {scheduler_job.py:1256} INFO - Marked 1 SchedulerJob instances as failed
[2024-10-02 00:13:55,643] {manager.py:406} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-10-02 00:13:55 +0000] [1693] [INFO] Booting worker with pid: 1693
[2024-10-02 00:14:00,252] {dag.py:2968} INFO - Setting next_dagrun for Lead_scoring_inference_pipeline to 2024-10-01T23:00:00+00:00, run_after=2024-10-02T00:00:00+00:00
[2024-10-02 00:14:00,302] {dag.py:2968} INFO - Setting next_dagrun for Lead_Scoring_Data_Engineering_Pipeline to 2024-10-02T00:00:00+00:00, run_after=2024-10-03T00:00:00+00:00
[2024-10-02 00:14:00,531] {scheduler_job.py:353} INFO - 2 tasks up for execution:
        <TaskInstance: Lead_Scoring_Data_Engineering_Pipeline.building_db scheduled__2024-10-01T00:00:00+00:00 [scheduled]>
        <TaskInstance: Lead_scoring_inference_pipeline.encoding_categorical_variables scheduled__2024-10-01T07:00:00+00:00 [scheduled]>
[2024-10-02 00:14:00,532] {scheduler_job.py:418} INFO - DAG Lead_Scoring_Data_Engineering_Pipeline has 0/16 running and queued tasks
[2024-10-02 00:14:00,533] {scheduler_job.py:418} INFO - DAG Lead_scoring_inference_pipeline has 0/16 running and queued tasks
[2024-10-02 00:14:00,533] {scheduler_job.py:504} INFO - Setting the following tasks to queued state:
        <TaskInstance: Lead_Scoring_Data_Engineering_Pipeline.building_db scheduled__2024-10-01T00:00:00+00:00 [scheduled]>
        <TaskInstance: Lead_scoring_inference_pipeline.encoding_categorical_variables scheduled__2024-10-01T07:00:00+00:00 [scheduled]>
[2024-10-02 00:14:00,592] {scheduler_job.py:546} INFO - Sending TaskInstanceKey(dag_id='Lead_Scoring_Data_Engineering_Pipeline', task_id='building_db', run_id='scheduled__2024-10-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 7 and queue default
[2024-10-02 00:14:00,592] {base_executor.py:91} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'Lead_Scoring_Data_Engineering_Pipeline', 'building_db', 'scheduled__2024-10-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/Lead_scoring_data_pipeline/.ipynb_checkpoints/lead_scoring_data_pipeline-checkpoint.py']
[2024-10-02 00:14:00,593] {scheduler_job.py:546} INFO - Sending TaskInstanceKey(dag_id='Lead_scoring_inference_pipeline', task_id='encoding_categorical_variables', run_id='scheduled__2024-10-01T07:00:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default
[2024-10-02 00:14:00,593] {base_executor.py:91} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'Lead_scoring_inference_pipeline', 'encoding_categorical_variables', 'scheduled__2024-10-01T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py']
[2024-10-02 00:14:00,613] {sequential_executor.py:59} INFO - Executing command: ['airflow', 'tasks', 'run', 'Lead_Scoring_Data_Engineering_Pipeline', 'building_db', 'scheduled__2024-10-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/Lead_scoring_data_pipeline/.ipynb_checkpoints/lead_scoring_data_pipeline-checkpoint.py']
[2024-10-02 00:14:01,312] {dagbag.py:508} INFO - Filling up the DagBag from /home/airflow/dags/Lead_scoring_data_pipeline/.ipynb_checkpoints/lead_scoring_data_pipeline-checkpoint.py
[2024-10-02 00:14:01,547] {utils.py:145} INFO - Note: NumExpr detected 64 cores but ""NUMEXPR_MAX_THREADS"" not set, so enforcing safe limit of 8.
[2024-10-02 00:14:01,548] {utils.py:157} INFO - NumExpr defaulting to 8 threads.
[2024-10-02 00:14:04,527] {lead_scoring_data_pipeline-checkpoint.py:47} INFO - Creating mlflow experiment
[2024-10-02 00:14:05,349] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-02 00:14:05,350] {example_local_kubernetes_executor.py:37} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py"", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-02 00:14:05,350] {example_local_kubernetes_executor.py:38} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-02 00:14:05,354] {example_python_operator.py:68} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2024-10-02 00:14:05,388] {tutorial_taskflow_api_etl_virtualenv.py:29} WARNING - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.
[2024-10-02 00:14:05,430] {task_command.py:371} INFO - Running <TaskInstance: Lead_Scoring_Data_Engineering_Pipeline.building_db scheduled__2024-10-01T00:00:00+00:00 [queued]> on host b07a3767db24
[2024-10-02 00:14:06,999] {sequential_executor.py:59} INFO - Executing command: ['airflow', 'tasks', 'run', 'Lead_scoring_inference_pipeline', 'encoding_categorical_variables', 'scheduled__2024-10-01T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py']
[2024-10-02 00:14:07,807] {dagbag.py:508} INFO - Filling up the DagBag from /home/airflow/dags/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py
[2024-10-02 00:14:08,731] {utils.py:145} INFO - Note: NumExpr detected 64 cores but ""NUMEXPR_MAX_THREADS"" not set, so enforcing safe limit of 8.
[2024-10-02 00:14:08,731] {utils.py:157} INFO - NumExpr defaulting to 8 threads.
[2024-10-02 00:14:09,066] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-02 00:14:09,067] {example_local_kubernetes_executor.py:37} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py"", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-02 00:14:09,068] {example_local_kubernetes_executor.py:38} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-02 00:14:09,071] {example_python_operator.py:68} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2024-10-02 00:14:09,106] {tutorial_taskflow_api_etl_virtualenv.py:29} WARNING - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.
[2024-10-02 00:14:09,149] {task_command.py:371} INFO - Running <TaskInstance: Lead_scoring_inference_pipeline.encoding_categorical_variables scheduled__2024-10-01T07:00:00+00:00 [queued]> on host b07a3767db24
[2024-10-02 00:14:18,873] {scheduler_job.py:599} INFO - Executor reports execution of Lead_Scoring_Data_Engineering_Pipeline.building_db run_id=scheduled__2024-10-01T00:00:00+00:00 exited with status success for try_number 1
[2024-10-02 00:14:18,873] {scheduler_job.py:599} INFO - Executor reports execution of Lead_scoring_inference_pipeline.encoding_categorical_variables run_id=scheduled__2024-10-01T07:00:00+00:00 exited with status success for try_number 1
[2024-10-02 00:14:18,885] {scheduler_job.py:642} INFO - TaskInstance Finished: dag_id=Lead_scoring_inference_pipeline, task_id=encoding_categorical_variables, run_id=scheduled__2024-10-01T07:00:00+00:00, map_index=-1, run_start_date=2024-10-02 00:14:09.210160+00:00, run_end_date=2024-10-02 00:14:17.928646+00:00, run_duration=8.718486, state=success, executor_state=success, try_number=1, max_tries=1, job_id=698, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-10-02 00:14:00.534756+00:00, queued_by_job_id=696, pid=1716
[2024-10-02 00:14:18,885] {scheduler_job.py:642} INFO - TaskInstance Finished: dag_id=Lead_Scoring_Data_Engineering_Pipeline, task_id=building_db, run_id=scheduled__2024-10-01T00:00:00+00:00, map_index=-1, run_start_date=2024-10-02 00:14:05.489475+00:00, run_end_date=2024-10-02 00:14:05.840342+00:00, run_duration=0.350867, state=success, executor_state=success, try_number=1, max_tries=1, job_id=697, pool=default_pool, queue=default, priority_weight=7, operator=PythonOperator, queued_dttm=2024-10-02 00:14:00.534756+00:00, queued_by_job_id=696, pid=1705
[2024-10-02 00:14:19,041] {dag.py:2968} INFO - Setting next_dagrun for Lead_scoring_inference_pipeline to 2024-10-02T00:00:00+00:00, run_after=2024-10-02T01:00:00+00:00
[2024-10-02 00:14:19,375] {scheduler_job.py:353} INFO - 3 tasks up for execution:
        <TaskInstance: Lead_Scoring_Data_Engineering_Pipeline.rawdata_schema_check scheduled__2024-10-01T00:00:00+00:00 [scheduled]>
        <TaskInstance: Lead_scoring_inference_pipeline.encoding_categorical_variables scheduled__2024-10-01T23:00:00+00:00 [scheduled]>
        <TaskInstance: Lead_scoring_inference_pipeline.checking_input_features scheduled__2024-10-01T07:00:00+00:00 [scheduled]>
[2024-10-02 00:14:19,375] {scheduler_job.py:418} INFO - DAG Lead_Scoring_Data_Engineering_Pipeline has 0/16 running and queued tasks
[2024-10-02 00:14:19,377] {scheduler_job.py:418} INFO - DAG Lead_scoring_inference_pipeline has 0/16 running and queued tasks
[2024-10-02 00:14:19,377] {scheduler_job.py:418} INFO - DAG Lead_scoring_inference_pipeline has 1/16 running and queued tasks
[2024-10-02 00:14:19,379] {scheduler_job.py:504} INFO - Setting the following tasks to queued state:
        <TaskInstance: Lead_Scoring_Data_Engineering_Pipeline.rawdata_schema_check scheduled__2024-10-01T00:00:00+00:00 [scheduled]>
        <TaskInstance: Lead_scoring_inference_pipeline.encoding_categorical_variables scheduled__2024-10-01T23:00:00+00:00 [scheduled]>
        <TaskInstance: Lead_scoring_inference_pipeline.checking_input_features scheduled__2024-10-01T07:00:00+00:00 [scheduled]>
[2024-10-02 00:14:19,385] {scheduler_job.py:546} INFO - Sending TaskInstanceKey(dag_id='Lead_Scoring_Data_Engineering_Pipeline', task_id='rawdata_schema_check', run_id='scheduled__2024-10-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 6 and queue default
[2024-10-02 00:14:19,386] {base_executor.py:91} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'Lead_Scoring_Data_Engineering_Pipeline', 'rawdata_schema_check', 'scheduled__2024-10-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/Lead_scoring_data_pipeline/.ipynb_checkpoints/lead_scoring_data_pipeline-checkpoint.py']
[2024-10-02 00:14:19,387] {scheduler_job.py:546} INFO - Sending TaskInstanceKey(dag_id='Lead_scoring_inference_pipeline', task_id='encoding_categorical_variables', run_id='scheduled__2024-10-01T23:00:00+00:00', try_number=1, map_index=-1) to executor with priority 4 and queue default
[2024-10-02 00:14:19,387] {base_executor.py:91} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'Lead_scoring_inference_pipeline', 'encoding_categorical_variables', 'scheduled__2024-10-01T23:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py']
[2024-10-02 00:14:19,389] {scheduler_job.py:546} INFO - Sending TaskInstanceKey(dag_id='Lead_scoring_inference_pipeline', task_id='checking_input_features', run_id='scheduled__2024-10-01T07:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
[2024-10-02 00:14:19,389] {base_executor.py:91} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'Lead_scoring_inference_pipeline', 'checking_input_features', 'scheduled__2024-10-01T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py']
[2024-10-02 00:14:19,411] {sequential_executor.py:59} INFO - Executing command: ['airflow', 'tasks', 'run', 'Lead_Scoring_Data_Engineering_Pipeline', 'rawdata_schema_check', 'scheduled__2024-10-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/Lead_scoring_data_pipeline/.ipynb_checkpoints/lead_scoring_data_pipeline-checkpoint.py']
[2024-10-02 00:14:20,324] {dagbag.py:508} INFO - Filling up the DagBag from /home/airflow/dags/Lead_scoring_data_pipeline/.ipynb_checkpoints/lead_scoring_data_pipeline-checkpoint.py
[2024-10-02 00:14:20,569] {utils.py:145} INFO - Note: NumExpr detected 64 cores but ""NUMEXPR_MAX_THREADS"" not set, so enforcing safe limit of 8.
[2024-10-02 00:14:20,570] {utils.py:157} INFO - NumExpr defaulting to 8 threads.
[2024-10-02 00:14:25,191] {lead_scoring_data_pipeline-checkpoint.py:47} INFO - Creating mlflow experiment
[2024-10-02 00:14:25,518] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-02 00:14:25,520] {example_local_kubernetes_executor.py:37} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py"", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-02 00:14:25,520] {example_local_kubernetes_executor.py:38} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-02 00:14:25,524] {example_python_operator.py:68} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2024-10-02 00:14:25,592] {tutorial_taskflow_api_etl_virtualenv.py:29} WARNING - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.
[2024-10-02 00:14:25,684] {task_command.py:371} INFO - Running <TaskInstance: Lead_Scoring_Data_Engineering_Pipeline.rawdata_schema_check scheduled__2024-10-01T00:00:00+00:00 [queued]> on host b07a3767db24
[2024-10-02 00:14:34,723] {sequential_executor.py:59} INFO - Executing command: ['airflow', 'tasks', 'run', 'Lead_scoring_inference_pipeline', 'encoding_categorical_variables', 'scheduled__2024-10-01T23:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py']
[2024-10-02 00:14:37,632] {dagbag.py:508} INFO - Filling up the DagBag from /home/airflow/dags/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py
[2024-10-02 00:14:38,717] {utils.py:145} INFO - Note: NumExpr detected 64 cores but ""NUMEXPR_MAX_THREADS"" not set, so enforcing safe limit of 8.
[2024-10-02 00:14:38,717] {utils.py:157} INFO - NumExpr defaulting to 8 threads.
[2024-10-02 00:14:39,057] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-02 00:14:39,058] {example_local_kubernetes_executor.py:37} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py"", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-02 00:14:39,058] {example_local_kubernetes_executor.py:38} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-02 00:14:39,062] {example_python_operator.py:68} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2024-10-02 00:14:39,097] {tutorial_taskflow_api_etl_virtualenv.py:29} WARNING - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.
[2024-10-02 00:14:39,149] {task_command.py:371} INFO - Running <TaskInstance: Lead_scoring_inference_pipeline.encoding_categorical_variables scheduled__2024-10-01T23:00:00+00:00 [queued]> on host b07a3767db24
[2024-10-02 00:14:46,811] {sequential_executor.py:59} INFO - Executing command: ['airflow', 'tasks', 'run', 'Lead_scoring_inference_pipeline', 'checking_input_features', 'scheduled__2024-10-01T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py']
[2024-10-02 00:14:47,576] {dagbag.py:508} INFO - Filling up the DagBag from /home/airflow/dags/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py
[2024-10-02 00:14:48,445] {utils.py:145} INFO - Note: NumExpr detected 64 cores but ""NUMEXPR_MAX_THREADS"" not set, so enforcing safe limit of 8.
[2024-10-02 00:14:48,445] {utils.py:157} INFO - NumExpr defaulting to 8 threads.
[2024-10-02 00:14:48,786] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-02 00:14:48,787] {example_local_kubernetes_executor.py:37} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py"", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-02 00:14:48,788] {example_local_kubernetes_executor.py:38} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-02 00:14:48,791] {example_python_operator.py:68} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2024-10-02 00:14:48,825] {tutorial_taskflow_api_etl_virtualenv.py:29} WARNING - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.
[2024-10-02 00:14:48,866] {task_command.py:371} INFO - Running <TaskInstance: Lead_scoring_inference_pipeline.checking_input_features scheduled__2024-10-01T07:00:00+00:00 [queued]> on host b07a3767db24
[2024-10-02 00:14:54,893] {scheduler_job.py:599} INFO - Executor reports execution of Lead_Scoring_Data_Engineering_Pipeline.rawdata_schema_check run_id=scheduled__2024-10-01T00:00:00+00:00 exited with status success for try_number 1
[2024-10-02 00:14:54,893] {scheduler_job.py:599} INFO - Executor reports execution of Lead_scoring_inference_pipeline.encoding_categorical_variables run_id=scheduled__2024-10-01T23:00:00+00:00 exited with status success for try_number 1
[2024-10-02 00:14:54,893] {scheduler_job.py:599} INFO - Executor reports execution of Lead_scoring_inference_pipeline.checking_input_features run_id=scheduled__2024-10-01T07:00:00+00:00 exited with status success for try_number 1
[2024-10-02 00:14:54,899] {scheduler_job.py:642} INFO - TaskInstance Finished: dag_id=Lead_scoring_inference_pipeline, task_id=checking_input_features, run_id=scheduled__2024-10-01T07:00:00+00:00, map_index=-1, run_start_date=2024-10-02 00:14:49.204026+00:00, run_end_date=2024-10-02 00:14:54.036880+00:00, run_duration=4.832854, state=success, executor_state=success, try_number=1, max_tries=1, job_id=701, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-10-02 00:14:19.380465+00:00, queued_by_job_id=696, pid=1758
[2024-10-02 00:14:54,900] {scheduler_job.py:642} INFO - TaskInstance Finished: dag_id=Lead_Scoring_Data_Engineering_Pipeline, task_id=rawdata_schema_check, run_id=scheduled__2024-10-01T00:00:00+00:00, map_index=-1, run_start_date=2024-10-02 00:14:25.895394+00:00, run_end_date=2024-10-02 00:14:30.420493+00:00, run_duration=4.525099, state=success, executor_state=success, try_number=1, max_tries=1, job_id=699, pool=default_pool, queue=default, priority_weight=6, operator=PythonOperator, queued_dttm=2024-10-02 00:14:19.380465+00:00, queued_by_job_id=696, pid=1732
[2024-10-02 00:14:54,900] {scheduler_job.py:642} INFO - TaskInstance Finished: dag_id=Lead_scoring_inference_pipeline, task_id=encoding_categorical_variables, run_id=scheduled__2024-10-01T23:00:00+00:00, map_index=-1, run_start_date=2024-10-02 00:14:39.228254+00:00, run_end_date=2024-10-02 00:14:46.043804+00:00, run_duration=6.81555, state=success, executor_state=success, try_number=1, max_tries=1, job_id=700, pool=default_pool, queue=default, priority_weight=4, operator=PythonOperator, queued_dttm=2024-10-02 00:14:19.380465+00:00, queued_by_job_id=696, pid=1747
[2024-10-02 00:14:55,185] {scheduler_job.py:353} INFO - 3 tasks up for execution:
        <TaskInstance: Lead_Scoring_Data_Engineering_Pipeline.loading_data scheduled__2024-10-01T00:00:00+00:00 [scheduled]>
        <TaskInstance: Lead_scoring_inference_pipeline.checking_input_features scheduled__2024-10-01T23:00:00+00:00 [scheduled]>
        <TaskInstance: Lead_scoring_inference_pipeline.generating_models_prediction scheduled__2024-10-01T07:00:00+00:00 [scheduled]>
[2024-10-02 00:14:55,186] {scheduler_job.py:418} INFO - DAG Lead_Scoring_Data_Engineering_Pipeline has 0/16 running and queued tasks
[2024-10-02 00:14:55,186] {scheduler_job.py:418} INFO - DAG Lead_scoring_inference_pipeline has 0/16 running and queued tasks
[2024-10-02 00:14:55,186] {scheduler_job.py:418} INFO - DAG Lead_scoring_inference_pipeline has 1/16 running and queued tasks
[2024-10-02 00:14:55,187] {scheduler_job.py:504} INFO - Setting the following tasks to queued state:
        <TaskInstance: Lead_Scoring_Data_Engineering_Pipeline.loading_data scheduled__2024-10-01T00:00:00+00:00 [scheduled]>
        <TaskInstance: Lead_scoring_inference_pipeline.checking_input_features scheduled__2024-10-01T23:00:00+00:00 [scheduled]>
        <TaskInstance: Lead_scoring_inference_pipeline.generating_models_prediction scheduled__2024-10-01T07:00:00+00:00 [scheduled]>
[2024-10-02 00:14:55,192] {scheduler_job.py:546} INFO - Sending TaskInstanceKey(dag_id='Lead_Scoring_Data_Engineering_Pipeline', task_id='loading_data', run_id='scheduled__2024-10-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 5 and queue default
[2024-10-02 00:14:55,193] {base_executor.py:91} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'Lead_Scoring_Data_Engineering_Pipeline', 'loading_data', 'scheduled__2024-10-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/Lead_scoring_data_pipeline/.ipynb_checkpoints/lead_scoring_data_pipeline-checkpoint.py']
[2024-10-02 00:14:55,194] {scheduler_job.py:546} INFO - Sending TaskInstanceKey(dag_id='Lead_scoring_inference_pipeline', task_id='checking_input_features', run_id='scheduled__2024-10-01T23:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
[2024-10-02 00:14:55,194] {base_executor.py:91} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'Lead_scoring_inference_pipeline', 'checking_input_features', 'scheduled__2024-10-01T23:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py']
[2024-10-02 00:14:55,195] {scheduler_job.py:546} INFO - Sending TaskInstanceKey(dag_id='Lead_scoring_inference_pipeline', task_id='generating_models_prediction', run_id='scheduled__2024-10-01T07:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
[2024-10-02 00:14:55,196] {base_executor.py:91} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'Lead_scoring_inference_pipeline', 'generating_models_prediction', 'scheduled__2024-10-01T07:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py']
[2024-10-02 00:14:55,250] {sequential_executor.py:59} INFO - Executing command: ['airflow', 'tasks', 'run', 'Lead_Scoring_Data_Engineering_Pipeline', 'loading_data', 'scheduled__2024-10-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/Lead_scoring_data_pipeline/.ipynb_checkpoints/lead_scoring_data_pipeline-checkpoint.py']
[2024-10-02 00:14:55,912] {dagbag.py:508} INFO - Filling up the DagBag from /home/airflow/dags/Lead_scoring_data_pipeline/.ipynb_checkpoints/lead_scoring_data_pipeline-checkpoint.py
[2024-10-02 00:14:56,141] {utils.py:145} INFO - Note: NumExpr detected 64 cores but ""NUMEXPR_MAX_THREADS"" not set, so enforcing safe limit of 8.
[2024-10-02 00:14:56,141] {utils.py:157} INFO - NumExpr defaulting to 8 threads.
[2024-10-02 00:14:59,093] {lead_scoring_data_pipeline-checkpoint.py:47} INFO - Creating mlflow experiment
[2024-10-02 00:14:59,253] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-02 00:14:59,254] {example_local_kubernetes_executor.py:37} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py"", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-02 00:14:59,254] {example_local_kubernetes_executor.py:38} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-02 00:14:59,258] {example_python_operator.py:68} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2024-10-02 00:14:59,291] {tutorial_taskflow_api_etl_virtualenv.py:29} WARNING - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.
[2024-10-02 00:14:59,548] {task_command.py:371} INFO - Running <TaskInstance: Lead_Scoring_Data_Engineering_Pipeline.loading_data scheduled__2024-10-01T00:00:00+00:00 [queued]> on host b07a3767db24
[2024-10-02 00:15:04,403] {sequential_executor.py:59} INFO - Executing command: ['airflow', 'tasks', 'run', 'Lead_scoring_inference_pipeline', 'checking_input_features', 'scheduled__2024-10-01T23:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py']
[2024-10-02 00:15:05,080] {dagbag.py:508} INFO - Filling up the DagBag from /home/airflow/dags/Lead_scoring_inference_pipeline/.ipynb_checkpoints/lead_scoring_inference_pipeline-checkpoint.py
[2024-10-02 00:15:05,983] {utils.py:145} INFO - Note: NumExpr detected 64 cores but ""NUMEXPR_MAX_THREADS"" not set, so enforcing safe limit of 8.
[2024-10-02 00:15:05,983] {utils.py:157} INFO - NumExpr defaulting to 8 threads.
[2024-10-02 00:15:06,324] {example_kubernetes_executor.py:39} WARNING - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]
[2024-10-02 00:15:06,325] {example_local_kubernetes_executor.py:37} WARNING - Could not import DAGs in example_local_kubernetes_executor.py
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py"", line 35, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[2024-10-02 00:15:06,326] {example_local_kubernetes_executor.py:38} WARNING - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]
[2024-10-02 00:15:06,329] {example_python_operator.py:68} WARNING - The virtalenv_python example task requires virtualenv, please install it.
[2024-10-02 00:15:06,367] {tutorial_taskflow_api_etl_virtualenv.py:29} WARNING - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.
[2024-10-02 00:15:06,412] {task_command.py:371} INFO - Running <TaskInstance: Lead_scoring_inference_pipeline.checking_input_features scheduled__2024-10-01T23:00:00+00:00 [queued]> on host b07a3767db24


MLFLOW : Log:
==================

***@***.***:~/airflow/dags/Lead_scoring_data_pipeline# mlflow server --backend-store-uri='sqlite:///database/lead_scoring_data_cleaning.db' --default-artifact-root=""mlruns/"" --port=6006 --host=0.0.0.0
[2024-10-02 00:13:43 +0000] [1650] [INFO] Starting gunicorn 20.1.0
[2024-10-02 00:13:43 +0000] [1650] [INFO] Listening at: http://0.0.0.0:6006 (1650)
[2024-10-02 00:13:43 +0000] [1650] [INFO] Using worker: sync
[2024-10-02 00:13:43 +0000] [1652] [INFO] Booting worker with pid: 1652
[2024-10-02 00:13:44 +0000] [1653] [INFO] Booting worker with pid: 1653
[2024-10-02 00:13:44 +0000] [1654] [INFO] Booting worker with pid: 1654
[2024-10-02 00:13:44 +0000] [1655] [INFO] Booting worker with pid: 1655

jscheffl on (2024-10-02 19:31:23 UTC): Conflig looks good but still I feel like the exception is not usual.

Hypothesis: Something is broken in your install. One option would be a clean re-install (don't forget to use the right constraints) and even much better I'd recommend to use a recent version. 2.3.3 for sure will not receive fixes. And the error is so basic that I doubt it is a SW problem but rather something in your setup or environment. Config at least looks ""proper"".

"
2553680389,issue,open,,Same task instance attempt is detected as a zombie multiple times and fails without retries ,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.3

### What happened?

A task instance's LocalTaskJobRunner exited without addressing the task instance state which is expected. The task instance will be identified as a zombie.

The issue is that the task instance is identified as a zombie three times and each time a TaskCallbackRequest is created. The reason why three callback requests were sent is because the DagFileProcessorProcess did not parse the file in a timely manner. There were about 600 seconds between each parse which allowed the zombie detection operation to find the same task instance multiple times.

Eventually when the the source file is parsed, the DagFileProcessorProcess executed all the TaskCallbackRequests and moved the task from `running` to `up_to_retry` then to `failed`.

This was previously reported in #31212

### What you think should happen instead?

Ideally, each task instance attempt should only be identified once regardless of how it is done. 

I think the following options are viable:
* deduplicating the TaskCallbackRequest / DbCallbackRequest
* tie TaskCallbackRequest to the task instance key and check before it is sent to the DatabaseCallbackSink 


### How to reproduce

1. Increase min_file_process_interval to 600 seconds
2. Create zombies by causing the LocalTaskJobRunner to terminate (delete the PgBouncer)
3. Confirm the same task instance attempt is identified as zombies between source file parsings
4. Confirm the DagFileProcessorProcess parses the file and that the on_failure_callback runs multiple times

### Operating System

n/a

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",wolfier,2024-09-27 20:25:39+00:00,[],2024-09-27 20:28:02+00:00,,https://github.com/apache/airflow/issues/42553,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2553574417,issue,closed,completed,Try number inconsistency between webserver and the actual log generated,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.2

### What happened?

Some tasks in airflow 2.10.2 are being launched with try number 0. However, the webserver is looking for logs starting with try number 1. Because of this we are having cases where tasks are running and passing however, the server throws a ""cannot read served logs"" error.

Executor=CeleryExecutor

![try number issue](https://github.com/user-attachments/assets/1f6b9ecd-a9a3-4019-a15c-ba8ed56dd9cb)


### What you think should happen instead?

Airflow should start attempt with try number 1 instead.

### How to reproduce

This is happening intermittently. Not all tasks start with try number 0. I have checked to make sure both the webserver and the celery worker are on the same version 2.10.2

### Operating System

SUSE Linux Enterprise Server 12 SP5

### Versions of Apache Airflow Providers

apache-airflow-providers-celery==3.8.1
celery==5.4.0


### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",dheerajturaga,2024-09-27 19:10:13+00:00,[],2024-10-15 20:42:52+00:00,2024-10-15 19:24:02+00:00,https://github.com/apache/airflow/issues/42549,"[('kind:bug', 'This is a clearly a bug'), ('area:logging', ''), ('pending-response', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:celery', '')]","[{'comment_id': 2379899728, 'issue_id': 2553574417, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 27, 19, 10, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2384113156, 'issue_id': 2553574417, 'author': 'jscheffl', 'body': 'I am not sure whether this is really a bug, might be rather a semantic gap or something that could be (visually) improved to just hide logs when no logs are there.\r\n\r\nIn Airflow 2.10 the internal semantic was changed to increment the try number of a task _only_ at the time the task is really scheduled/started. As long as it is just there as instance (not scheduled) the try=0 is the sign for it has never tried. In this case the visual improvement might be to hide the log panel as no logs are existing.\r\n\r\nWould you like to submit a small improvement?', 'created_at': datetime.datetime(2024, 9, 30, 20, 44, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2384146703, 'issue_id': 2553574417, 'author': 'dheerajturaga', 'body': ""@jscheffl, thats not really the case. Here the task has started and completed, and the log is captured under attempt-0.log. However, the webserver is trying to look for attempt-1.log \r\nit appears that there are cases where the try number isn't incremented on the log side even when the task has started.\r\n\r\n`du -sh attempt-0.log\r\n64K\tattempt-0.log\r\n`\r\n\r\n> [2024-09-25T14:24:04.772+0000] {taskinstance.py:1402} DEBUG - previous_execution_date was called\r\n[2024-09-25T14:24:04.822+0000] {taskinstance.py:1402} DEBUG - previous_execution_date was called\r\n[2024-09-25T14:24:04.838+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs\r\n[2024-09-25T14:24:04.854+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: [queued]>\r\n[2024-09-25T14:24:04.867+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: scheduled__2024-09-25T05:00:00+00:00 [queued]>\r\n[2024-09-25T14:24:04.868+0000] {taskinstance.py:2865} INFO - Starting attempt 0 of 1"", 'created_at': datetime.datetime(2024, 9, 30, 21, 7, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2384157163, 'issue_id': 2553574417, 'author': 'jscheffl', 'body': 'Thanks for the response. You say this is intermittently - do you have any means to re-produce this?', 'created_at': datetime.datetime(2024, 9, 30, 21, 14, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2384183299, 'issue_id': 2553574417, 'author': 'dheerajturaga', 'body': 'let me see if I can isolate this', 'created_at': datetime.datetime(2024, 9, 30, 21, 34, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2386728545, 'issue_id': 2553574417, 'author': 'jscheffl', 'body': 'Might this be a side effect of #39336 @dstandish ?', 'created_at': datetime.datetime(2024, 10, 1, 18, 51, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2387000062, 'issue_id': 2553574417, 'author': 'dstandish', 'body': 'How was this task launched / created / scheduled?', 'created_at': datetime.datetime(2024, 10, 1, 20, 23, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2387063226, 'issue_id': 2553574417, 'author': 'dheerajturaga', 'body': '@dstandish, this is essentially a bash operator The task was triggered on schedule and is part of a task_group. I suspect whats happening here is the `log_filename_template` is using `attempt={{ try_number }}.log` as its default. It could be that `{try_number}` and `{ti.try_number}` are possibly diverging. Please review PR #42633', 'created_at': datetime.datetime(2024, 10, 1, 20, 58, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2387160130, 'issue_id': 2553574417, 'author': 'dheerajturaga', 'body': '@dstandish, @jscheffl looks like my pr is exposing a bug with #39336 \r\n\r\nTake a look at https://github.com/apache/airflow/actions/runs/11133289143/job/30939468337?pr=42633', 'created_at': datetime.datetime(2024, 10, 1, 22, 7, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2387161821, 'issue_id': 2553574417, 'author': 'dstandish', 'body': ""> @dstandish, @jscheffl looks like my pr is exposing a bug with #39336\r\n> \r\n> Take a look at https://github.com/apache/airflow/actions/runs/11133289143/job/30939468337?pr=42633\r\n\r\nIt's certainly possible.  But I have not seen this.  So can you provide some repro steps?  Like, give us the simplest dag possible which also produces this behavior."", 'created_at': datetime.datetime(2024, 10, 1, 22, 8, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2388997668, 'issue_id': 2553574417, 'author': 'dheerajturaga', 'body': '@dstandish, Im trying hard to reproduce the conditions as to when this is happening. This happens intermittently. Take a look at this screenshot. This scenario has `{try_number}` and `{ti.try_number}` different\r\n\r\nI added both these templates in the log_filename_template.\r\n\r\n`log_filename_template = dag-{{ ti.dag_id }}/run-{{ ts_nodash }}/task-{{ ti.task_id }}/{%% if ti.map_index >= 0 %%}mapid-{{ ti.map_index }}/{%% endif %%}attempt-{{ try_number }}-{{ ti.try_number }}.log`\r\n\r\n![image](https://github.com/user-attachments/assets/3261435c-7567-482e-beaa-4193d4a57aa9)\r\n\r\nAlso, there are many cases where the first task of the dag starts with try_number 2', 'created_at': datetime.datetime(2024, 10, 2, 15, 38, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2389037176, 'issue_id': 2553574417, 'author': 'dheerajturaga', 'body': '@dstandish, @jscheffl  Here\'s how you reproduce this. Just run the simple dag below.  On completion, clear the first task and let it rerun. You will notice `{try_number}` and `{ti.try_number}` diverge. `{try_number}` accurately changes between 1 and 2 but `{ti.try_number}` is stuck at 2.\r\n\r\nmake sure you set this cfg option:\r\n```log_filename_template = dag-{{ ti.dag_id }}/run-{{ ts_nodash }}/task-{{ ti.task_id }}/{%% if ti.map_index >= 0 %%}mapid-{{ ti.map_index }}/{%% endif %%}attempt-{{ try_number }}-{{ ti.try_number }}.log```\r\n\r\n```\r\nfrom __future__ import annotations\r\nimport pendulum\r\n\r\nimport airflow\r\nfrom airflow.decorators import dag, task_group\r\nfrom airflow.operators.bash import BashOperator\r\nfrom datetime import timedelta\r\n\r\nimport yaml\r\nimport os\r\n\r\n@dag(\r\n    dag_id=""debug_me"",\r\n    start_date=pendulum.today(\'America/Chicago\'),\r\n    catchup=False,\r\n    tags=[""debug""]\r\n)\r\ndef single_hello():\r\n    hw = BashOperator(\r\n            task_id=""try_number_check"",\r\n            bash_command=""""""\r\n            # echo try_number:\r\n            echo ti.try_number: {{ti.try_number}}\r\n            """"""\r\n        )\r\n    hw1 = BashOperator(\r\n            task_id=""try_number_check_1"",\r\n            bash_command=""""""\r\n            # echo try_number:\r\n            echo ti.try_number: {{ti.try_number}}\r\n            """"""\r\n        )\r\n\r\n    hw >> hw1\r\n\r\nsingle_hello()\r\n```', 'created_at': datetime.datetime(2024, 10, 2, 15, 57, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2389092430, 'issue_id': 2553574417, 'author': 'dstandish', 'body': 'I tried reproing on main via clearing task but could not do\r\n\r\nhttps://github.com/user-attachments/assets/685ff05d-2b40-4019-8f3e-fef0da404b5e\r\n\r\nHere\'s my code\r\n\r\n```\r\nwith DAG(dag_id=""dag1"", schedule=""@daily"", catchup=False, start_date=pendulum.now()) as dag1:\r\n\r\n    @task(retries=2, retry_delay=timedelta(seconds=10))\r\n    def the_task(try_number=None, ti=None):\r\n        sleep(rand() * 10)\r\n\r\n    tasks = []\r\n    for num in range(3):\r\n        tasks.append(the_task.override(task_id=f""task_{num+1}"")())\r\n\r\n    chain_linear(*tasks)\r\n```\r\n\r\nCan you try it and see if you get a diff result?', 'created_at': datetime.datetime(2024, 10, 2, 16, 23, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2389106502, 'issue_id': 2553574417, 'author': 'dheerajturaga', 'body': '@dstandish, you are missing changing `attempt-{{ try_number }}.log` to `attempt-{{ ti.try_number }}.log` in your airflow.cfg\r\n\r\nonce you do that, try clicking between the logs of `try 1` and `try 2`\r\n\r\neven better would be setting the following in your airflow config:\r\n\r\n`log_filename_template = dag-{{ ti.dag_id }}/run-{{ ts_nodash }}/task-{{ ti.task_id }}/{%% if ti.map_index >= 0 %%}mapid-{{ ti.map_index }}/{%% endif %%}attempt-{{ try_number }}-{{ ti.try_number }}.log`', 'created_at': datetime.datetime(2024, 10, 2, 16, 30, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2398011136, 'issue_id': 2553574417, 'author': 'dheerajturaga', 'body': ""@dstandish, Issue is that it happens at random. I have a cluster of celery workers which keep listening for jobs from the scheduler. There maybe a case where the try number isn't incremented even when the job is running on the worker. I can't isolate this with airflow standalone and it seems to be effecting all tasks ( there doesn't seem to be a pattern here ). \r\n\r\n![image](https://github.com/user-attachments/assets/80e8b987-0c72-4403-90cc-d382d17d0d6b)\r\n\r\nClearing this task increments the try_number to 2\r\n\r\n![image](https://github.com/user-attachments/assets/f3a03cbc-5573-49b0-aa8f-32f09d0e6a89)\r\n\r\nA workaround here would be to find all attempt-0.log and softlink them to attempt-1.log to fix the gui. I have a dag that is doing this every 20min and there are many tasks scattered all over that are randomly impacted by this"", 'created_at': datetime.datetime(2024, 10, 7, 22, 11, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2414831492, 'issue_id': 2553574417, 'author': 'dheerajturaga', 'body': ""@dstandish, I figured out what's happening. I had a few workers which were still on airflow 2.7.2 which never got recycled. \r\nthe issue went away when I upgraded them to 2.10.2\r\n\r\nThanks for your help on triaging this!"", 'created_at': datetime.datetime(2024, 10, 15, 19, 24, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415038091, 'issue_id': 2553574417, 'author': 'dstandish', 'body': ""> @dstandish, I figured out what's happening. I had a few workers which were still on airflow 2.7.2 which never got recycled. the issue went away when I upgraded them to 2.10.2\r\n> \r\n> Thanks for your help on triaging this!\r\n\r\nAwesome, great result"", 'created_at': datetime.datetime(2024, 10, 15, 20, 42, 51, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-27 19:10:16 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jscheffl on (2024-09-30 20:44:52 UTC): I am not sure whether this is really a bug, might be rather a semantic gap or something that could be (visually) improved to just hide logs when no logs are there.

In Airflow 2.10 the internal semantic was changed to increment the try number of a task _only_ at the time the task is really scheduled/started. As long as it is just there as instance (not scheduled) the try=0 is the sign for it has never tried. In this case the visual improvement might be to hide the log panel as no logs are existing.

Would you like to submit a small improvement?

dheerajturaga (Issue Creator) on (2024-09-30 21:07:31 UTC): @jscheffl, thats not really the case. Here the task has started and completed, and the log is captured under attempt-0.log. However, the webserver is trying to look for attempt-1.log 
it appears that there are cases where the try number isn't incremented on the log side even when the task has started.

`du -sh attempt-0.log
64K	attempt-0.log
`

[2024-09-25T14:24:04.822+0000] {taskinstance.py:1402} DEBUG - previous_execution_date was called
[2024-09-25T14:24:04.838+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2024-09-25T14:24:04.854+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: [queued]>
[2024-09-25T14:24:04.867+0000] {taskinstance.py:2612} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: scheduled__2024-09-25T05:00:00+00:00 [queued]>
[2024-09-25T14:24:04.868+0000] {taskinstance.py:2865} INFO - Starting attempt 0 of 1

jscheffl on (2024-09-30 21:14:58 UTC): Thanks for the response. You say this is intermittently - do you have any means to re-produce this?

dheerajturaga (Issue Creator) on (2024-09-30 21:34:21 UTC): let me see if I can isolate this

jscheffl on (2024-10-01 18:51:29 UTC): Might this be a side effect of #39336 @dstandish ?

dstandish on (2024-10-01 20:23:54 UTC): How was this task launched / created / scheduled?

dheerajturaga (Issue Creator) on (2024-10-01 20:58:32 UTC): @dstandish, this is essentially a bash operator The task was triggered on schedule and is part of a task_group. I suspect whats happening here is the `log_filename_template` is using `attempt={{ try_number }}.log` as its default. It could be that `{try_number}` and `{ti.try_number}` are possibly diverging. Please review PR #42633

dheerajturaga (Issue Creator) on (2024-10-01 22:07:22 UTC): @dstandish, @jscheffl looks like my pr is exposing a bug with #39336 

Take a look at https://github.com/apache/airflow/actions/runs/11133289143/job/30939468337?pr=42633

dstandish on (2024-10-01 22:08:34 UTC): It's certainly possible.  But I have not seen this.  So can you provide some repro steps?  Like, give us the simplest dag possible which also produces this behavior.

dheerajturaga (Issue Creator) on (2024-10-02 15:38:25 UTC): @dstandish, Im trying hard to reproduce the conditions as to when this is happening. This happens intermittently. Take a look at this screenshot. This scenario has `{try_number}` and `{ti.try_number}` different

I added both these templates in the log_filename_template.

`log_filename_template = dag-{{ ti.dag_id }}/run-{{ ts_nodash }}/task-{{ ti.task_id }}/{%% if ti.map_index >= 0 %%}mapid-{{ ti.map_index }}/{%% endif %%}attempt-{{ try_number }}-{{ ti.try_number }}.log`

![image](https://github.com/user-attachments/assets/3261435c-7567-482e-beaa-4193d4a57aa9)

Also, there are many cases where the first task of the dag starts with try_number 2

dheerajturaga (Issue Creator) on (2024-10-02 15:57:05 UTC): @dstandish, @jscheffl  Here's how you reproduce this. Just run the simple dag below.  On completion, clear the first task and let it rerun. You will notice `{try_number}` and `{ti.try_number}` diverge. `{try_number}` accurately changes between 1 and 2 but `{ti.try_number}` is stuck at 2.

make sure you set this cfg option:
```log_filename_template = dag-{{ ti.dag_id }}/run-{{ ts_nodash }}/task-{{ ti.task_id }}/{%% if ti.map_index >= 0 %%}mapid-{{ ti.map_index }}/{%% endif %%}attempt-{{ try_number }}-{{ ti.try_number }}.log```

```
from __future__ import annotations
import pendulum

import airflow
from airflow.decorators import dag, task_group
from airflow.operators.bash import BashOperator
from datetime import timedelta

import yaml
import os

@dag(
    dag_id=""debug_me"",
    start_date=pendulum.today('America/Chicago'),
    catchup=False,
    tags=[""debug""]
)
def single_hello():
    hw = BashOperator(
            task_id=""try_number_check"",
            bash_command=""""""
            # echo try_number:
            echo ti.try_number: {{ti.try_number}}
            """"""
        )
    hw1 = BashOperator(
            task_id=""try_number_check_1"",
            bash_command=""""""
            # echo try_number:
            echo ti.try_number: {{ti.try_number}}
            """"""
        )

    hw >> hw1

single_hello()
```

dstandish on (2024-10-02 16:23:47 UTC): I tried reproing on main via clearing task but could not do

https://github.com/user-attachments/assets/685ff05d-2b40-4019-8f3e-fef0da404b5e

Here's my code

```
with DAG(dag_id=""dag1"", schedule=""@daily"", catchup=False, start_date=pendulum.now()) as dag1:

    @task(retries=2, retry_delay=timedelta(seconds=10))
    def the_task(try_number=None, ti=None):
        sleep(rand() * 10)

    tasks = []
    for num in range(3):
        tasks.append(the_task.override(task_id=f""task_{num+1}"")())

    chain_linear(*tasks)
```

Can you try it and see if you get a diff result?

dheerajturaga (Issue Creator) on (2024-10-02 16:30:48 UTC): @dstandish, you are missing changing `attempt-{{ try_number }}.log` to `attempt-{{ ti.try_number }}.log` in your airflow.cfg

once you do that, try clicking between the logs of `try 1` and `try 2`

even better would be setting the following in your airflow config:

`log_filename_template = dag-{{ ti.dag_id }}/run-{{ ts_nodash }}/task-{{ ti.task_id }}/{%% if ti.map_index >= 0 %%}mapid-{{ ti.map_index }}/{%% endif %%}attempt-{{ try_number }}-{{ ti.try_number }}.log`

dheerajturaga (Issue Creator) on (2024-10-07 22:11:08 UTC): @dstandish, Issue is that it happens at random. I have a cluster of celery workers which keep listening for jobs from the scheduler. There maybe a case where the try number isn't incremented even when the job is running on the worker. I can't isolate this with airflow standalone and it seems to be effecting all tasks ( there doesn't seem to be a pattern here ). 

![image](https://github.com/user-attachments/assets/80e8b987-0c72-4403-90cc-d382d17d0d6b)

Clearing this task increments the try_number to 2

![image](https://github.com/user-attachments/assets/f3a03cbc-5573-49b0-aa8f-32f09d0e6a89)

A workaround here would be to find all attempt-0.log and softlink them to attempt-1.log to fix the gui. I have a dag that is doing this every 20min and there are many tasks scattered all over that are randomly impacted by this

dheerajturaga (Issue Creator) on (2024-10-15 19:24:02 UTC): @dstandish, I figured out what's happening. I had a few workers which were still on airflow 2.7.2 which never got recycled. 
the issue went away when I upgraded them to 2.10.2

Thanks for your help on triaging this!

dstandish on (2024-10-15 20:42:51 UTC): Awesome, great result

"
2553153584,issue,closed,completed,SparkKubernetesOperator not rendering the templated kuberntes_conn_id after upgrading,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.7

### What happened?

I upgraded from airflow version 2.5.3 to 2.7.1 and with that apache-airflow-providers-cncf-kubernetes was also upgraded from 5.2.2 to 7.5.0.

Earlier I was using Kubernetes connection ID like this from Xcoms. I put it in xcoms in the first task where I load all the configs to xcoms
`kubernetes_conn_id: ""{% raw %}{{ ti.xcom_pull(key ='env_vars', task_ids='fetch_vars')['{% endraw %}{{root_key}}{% raw %}']['kubernetes_conn_id']}}{% endraw %}""`

This was working fine, but after upgrading this stopped working and I am getting the following error
`The conn_id `{{ ti.xcom_pull(key ='env_variables', task_ids='fetch_env')['cycle_process_client_2332']['kubernetes_conn_id']}}` isn't defined`

Also in the UI if I check rendered templates tag I can see the conn ID resolved correctly. I am confused, any help is appreciated.

### What you think should happen instead?

It should pull the connection ID correctly from the Xcoms.

### How to reproduce

Create a dag with and add a task using SparkKubernetesOperator, pass kubernetes_conn_id as a param and assign it Xcom value.

### Operating System

linux

### Versions of Apache Airflow Providers

 apache-airflow-providers-cncf-kubernetes - 7.5
airflow - 2.7

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",nikhilkarve,2024-09-27 15:02:02+00:00,['gopidesupavan'],2024-10-16 19:27:59+00:00,2024-10-16 19:27:59+00:00,https://github.com/apache/airflow/issues/42546,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]","[{'comment_id': 2380569095, 'issue_id': 2553153584, 'author': 'gopidesupavan', 'body': 'It would be better to share your dag example. Which your getting .', 'created_at': datetime.datetime(2024, 9, 28, 8, 41, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2384062649, 'issue_id': 2553153584, 'author': 'nikhilkarve', 'body': '```\r\n submit_task = SparkKubernetesOperator(\r\n        task_id=""submit_task"",\r\n        file_source=""URL"",\r\n        namespace=""{{ ti.xcom_pull(key =\'env_variables\', task_ids=\'fetch_env\')[\'dag_id_here\'][\'namespace\']}}"",\r\n        base_url=""{{ ti.xcom_pull(key =\'env_variables\', task_ids=\'fetch_env\')[\'dag_id_here\'][\'spark_base_url\']}}"",\r\n        endpoint=""{{ ti.xcom_pull(key =\'env_variables\', task_ids=\'fetch_env\')[\'dag_id_here\'][\'deployment_file\']}}"",\r\n        application_file=""{{ ti.xcom_pull(key =\'env_variables\', task_ids=\'fetch_env\')[\'dag_id_here\'][\'application_file\']}}"",\r\n        kubernetes_conn_id=""{{ ti.xcom_pull(key =\'env_variables\', task_ids=\'fetch_env\')[\'dag_id_here\'][\'kubernetes_conn_id\']}}"",\r\n        do_xcom_push=True,\r\n        sla=timedelta(seconds=1200),\r\n    )\r\n    submit_batch_extractor_task.set_upstream([prev_task])\r\n```\r\nI did not add the complete dag as we use many custom operators and that may cause the confusion. This is a bit boiled down version of the same task which was succeeding with 2.5.3 and not with 2.7 airflow with cncf-kub-operator also upgraded.', 'created_at': datetime.datetime(2024, 9, 30, 20, 12, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2395536962, 'issue_id': 2553153584, 'author': 'gopidesupavan', 'body': 'Thanks..', 'created_at': datetime.datetime(2024, 10, 6, 18, 31, 24, tzinfo=datetime.timezone.utc)}]","gopidesupavan (Assginee) on (2024-09-28 08:41:16 UTC): It would be better to share your dag example. Which your getting .

nikhilkarve (Issue Creator) on (2024-09-30 20:12:41 UTC): ```
 submit_task = SparkKubernetesOperator(
        task_id=""submit_task"",
        file_source=""URL"",
        namespace=""{{ ti.xcom_pull(key ='env_variables', task_ids='fetch_env')['dag_id_here']['namespace']}}"",
        base_url=""{{ ti.xcom_pull(key ='env_variables', task_ids='fetch_env')['dag_id_here']['spark_base_url']}}"",
        endpoint=""{{ ti.xcom_pull(key ='env_variables', task_ids='fetch_env')['dag_id_here']['deployment_file']}}"",
        application_file=""{{ ti.xcom_pull(key ='env_variables', task_ids='fetch_env')['dag_id_here']['application_file']}}"",
        kubernetes_conn_id=""{{ ti.xcom_pull(key ='env_variables', task_ids='fetch_env')['dag_id_here']['kubernetes_conn_id']}}"",
        do_xcom_push=True,
        sla=timedelta(seconds=1200),
    )
    submit_batch_extractor_task.set_upstream([prev_task])
```
I did not add the complete dag as we use many custom operators and that may cause the confusion. This is a bit boiled down version of the same task which was succeeding with 2.5.3 and not with 2.7 airflow with cncf-kub-operator also upgraded.

gopidesupavan (Assginee) on (2024-10-06 18:31:24 UTC): Thanks..

"
2553094008,issue,closed,completed,AIP-84 Migrate the public endpoint Batch Update DAG to FastAPI,"### Body

Part of: https://github.com/apache/airflow/issues/42370

Migrate the 'PATCH'  batch dags. Mostly use to 'pause' multiple dags.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-09-27 14:32:44+00:00,['pierrejeambrun'],2024-09-30 15:04:26+00:00,2024-09-30 15:04:26+00:00,https://github.com/apache/airflow/issues/42544,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]",[],
2552582598,issue,open,,"After deleting a dag from the dags directory, it is still displayed in the UI","### Discussed in https://github.com/apache/airflow/discussions/40331

<div type='discussions-op-text'>

<sup>Originally posted by **tatyana12345** June 19, 2024</sup>
### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.8.3

### What happened?

Dag has been removed from the airflow dags directory /opt/airflow/dags. Dag is not displayed when running the airflow dags list command. But it is still available in the Airflow UI, and it is also available in the Airflow postgres database.


### What you think should happen instead?

It is expected that dag will be automatically removed from the web interface after deleting dag from the dags directory.

### How to reproduce

Remove dag from the /opt/airflow/dags directory and it will still be available in the web interface even after a few days

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
</div>",shahar1,2024-09-27 10:29:57+00:00,[],2025-02-07 21:31:11+00:00,,https://github.com/apache/airflow/issues/42542,"[('kind:bug', 'This is a clearly a bug'), ('area:MetaDB', 'Meta Database related issues.'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2379002375, 'issue_id': 2552582598, 'author': 'shahar1', 'body': ""Following the reports in the discussion #40331 - I've reopened it as an issue.\r\nI didn't manage to [reproduce ](https://github.com/apache/airflow/discussions/40331#discussioncomment-10774202) it on breeze deployment.\r\nI would like to ask from the OP (@tatyana12345) and the commentors (@cjj1120, @falukelo, @gaelxcowi) to post the exact configuration, deployment environment, Airflow version, and steps to reproduce the issue (preferrably with accompanying timestamps and images from the UI). After we manage to reproduce the issue, I'll be happy to review PRs from the community that resolve it."", 'created_at': datetime.datetime(2024, 9, 27, 10, 53, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2379989682, 'issue_id': 2552582598, 'author': 'gaelxcowi', 'body': '@shahar1 Hopefully I can capture everything, if not let me know. \r\n\r\nHere is a minimal exmaple, hopefully I have covered enough evidence:\r\n\r\nDisclaimer: I wasn\'t yet able to reproduce 100% my behaviour - but I will comeback to that later. \r\n\r\nSo, here are the configs: \r\n```\r\n[core]\r\ndags_folder = /opt/airflow/dags/repo/\r\nhostname_callable = airflow.utils.net.getfqdn\r\nmight_contain_dag_callable = airflow.utils.file.might_contain_dag_via_default_heuristic\r\ndefault_timezone = utc\r\nexecutor = KubernetesExecutor\r\nauth_manager = airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager\r\nparallelism = 32\r\nmax_active_tasks_per_dag = 18\r\ndags_are_paused_at_creation = True\r\nmax_active_runs_per_dag = 18\r\nmax_consecutive_failed_dag_runs_per_dag = 0\r\n# mp_start_method =\r\nload_examples = false\r\nplugins_folder = /opt/airflow/plugins\r\nexecute_tasks_new_python_interpreter = False\r\nfernet_key =<removed>\r\ndonot_pickle = True\r\ndagbag_import_timeout = 100\r\ndagbag_import_error_tracebacks = True\r\ndagbag_import_error_traceback_depth = 2\r\ndag_file_processor_timeout = 150\r\ntask_runner = StandardTaskRunner\r\ndefault_impersonation =\r\nsecurity =\r\nunit_test_mode = False\r\nenable_xcom_pickling = False\r\nallowed_deserialization_classes = airflow.*\r\nallowed_deserialization_classes_regexp =\r\nkilled_task_cleanup_time = 60\r\ndag_run_conf_overrides_params = True\r\ndag_discovery_safe_mode = True\r\ndag_ignore_file_syntax = regexp\r\ndefault_task_retries = 0\r\ndefault_task_retry_delay = 300\r\nmax_task_retry_delay = 86400\r\ndefault_task_weight_rule = downstream\r\ntask_success_overtime = 20\r\ndefault_task_execution_timeout =\r\nmin_serialized_dag_update_interval = 30\r\ncompress_serialized_dags = False\r\nmin_serialized_dag_fetch_interval = 10\r\nmax_num_rendered_ti_fields_per_task = 30\r\ncheck_slas = True\r\nxcom_backend = airflow.models.xcom.BaseXCom\r\nlazy_load_plugins = True\r\nlazy_discover_providers = True\r\nhide_sensitive_var_conn_fields = True\r\nsensitive_var_conn_names =\r\ndefault_pool_task_slot_count = 128\r\nmax_map_length = 1024\r\ndaemon_umask = 0o077\r\n# dataset_manager_class =\r\n# dataset_manager_kwargs =\r\nstrict_dataset_uri_validation = False\r\ndatabase_access_isolation = False\r\n# internal_api_url =\r\ninternal_api_secret_key = <removed>\r\ntest_connection = Disabled\r\nmax_templated_field_length = 4096\r\n[database]\r\nalembic_ini_file_path = alembic.ini\r\nsql_alchemy_conn = postgresql+psycopg2://<removed>\r\n# sql_alchemy_engine_args =\r\nsql_engine_encoding = utf-8\r\n# sql_engine_collation_for_ids =\r\nsql_alchemy_pool_enabled = True\r\nsql_alchemy_pool_size = 5\r\nsql_alchemy_max_overflow = 10\r\nsql_alchemy_pool_recycle = 1800\r\nsql_alchemy_pool_pre_ping = True\r\nsql_alchemy_schema =\r\n# sql_alchemy_connect_args =\r\n# sql_alchemy_session_maker =\r\nload_default_connections = True\r\nmax_db_retries = 3\r\ncheck_migrations = True\r\n[logging]\r\nbase_log_folder = /opt/airflow/logs\r\nremote_logging = False\r\nremote_log_conn_id =\r\ndelete_local_logs = False\r\ngoogle_key_path =\r\nremote_base_log_folder =\r\nremote_task_handler_kwargs =\r\nencrypt_s3_logs = False\r\nlogging_level = INFO\r\ncelery_logging_level =\r\nfab_logging_level = WARNING\r\nlogging_config_class =\r\ncolored_console_log = True\r\ncolored_log_format = [%%(blue)s%%(asctime)s%%(reset)s] {%%(blue)s%%(filename)s:%%(reset)s%%(lineno)d} %%(log_color)s%%(levelname)s%%(reset)s - %%(log_color)s%%(message)s%%(reset)s\r\ncolored_formatter_class = airflow.utils.log.colored_log.CustomTTYColoredFormatter\r\nlog_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s\r\nsimple_log_format = %%(asctime)s %%(levelname)s - %%(message)s\r\ndag_processor_log_target = file\r\ndag_processor_log_format = [%%(asctime)s] [SOURCE:DAG_PROCESSOR] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s\r\nlog_formatter_class = airflow.utils.log.timezone_aware.TimezoneAware\r\nsecret_mask_adapter =\r\ntask_log_prefix_template =\r\nlog_filename_template = dag_id={{ ti.dag_id }}/run_id={{ ti.run_id }}/task_id={{ ti.task_id }}/{%% if ti.map_index >= 0 %%}map_index={{ ti.map_index }}/{%% endif %%}attempt={{ try_number }}.log\r\nlog_processor_filename_template = {{ filename }}.log\r\ndag_processor_manager_log_location = /opt/airflow/logs/dag_processor_manager/dag_processor_manager.log\r\ndag_processor_manager_log_stdout = False\r\ntask_log_reader = task\r\nextra_logger_names =\r\nworker_log_server_port = 8793\r\ntrigger_log_server_port = 8794\r\n# interleave_timestamp_parser =\r\nfile_task_handler_new_folder_permissions = 0o775\r\nfile_task_handler_new_file_permissions = 0o664\r\ncelery_stdout_stderr_separation = False\r\nenable_task_context_logger = True\r\ncolor_log_error_keywords = error,exception\r\ncolor_log_warning_keywords = warn\r\n[metrics]\r\nmetrics_use_pattern_match = False\r\nmetrics_allow_list =\r\nmetrics_block_list =\r\nstatsd_on = False\r\nstatsd_host = localhost\r\nstatsd_port = 8125\r\nstatsd_prefix = airflow\r\nstat_name_handler =\r\nstatsd_datadog_enabled = False\r\nstatsd_datadog_tags =\r\nstatsd_datadog_metrics_tags = True\r\n# statsd_custom_client_path =\r\nstatsd_disabled_tags = job_id,run_id\r\nstatsd_influxdb_enabled = False\r\notel_on = False\r\notel_host = localhost\r\notel_port = 8889\r\notel_prefix = airflow\r\notel_interval_milliseconds = 60000\r\notel_debugging_on = False\r\notel_ssl_active = False\r\n[traces]\r\notel_on = False\r\notel_host = localhost\r\notel_port = 8889\r\notel_service = Airflow\r\notel_debugging_on = False\r\notel_ssl_active = False\r\notel_task_log_event = False\r\n[secrets]\r\nbackend =\r\nbackend_kwargs =\r\nuse_cache = False\r\ncache_ttl_seconds = 900\r\n[cli]\r\napi_client = airflow.api.client.local_client\r\nendpoint_url = http://localhost:8080\r\n[debug]\r\nfail_fast = False\r\n[api]\r\nenable_experimental_api = False\r\nauth_backends = airflow.api.auth.backend.session\r\nmaximum_page_limit = 100\r\nfallback_page_limit = 100\r\ngoogle_oauth2_audience =\r\ngoogle_key_path =\r\naccess_control_allow_headers =\r\naccess_control_allow_methods =\r\naccess_control_allow_origins =\r\nenable_xcom_deserialize_support = False\r\n[lineage]\r\nbackend =\r\n[operators]\r\ndefault_owner = airflow\r\ndefault_deferrable = false\r\ndefault_cpus = 1\r\ndefault_ram = 512\r\ndefault_disk = 512\r\ndefault_gpus = 0\r\ndefault_queue = default\r\nallow_illegal_arguments = True\r\n[webserver]\r\naccess_denied_message = Access is Denied\r\nconfig_file = /opt/airflow/webserver_config.py\r\nbase_url = <removed>\r\ndefault_ui_timezone = UTC\r\nweb_server_host = 0.0.0.0\r\nweb_server_port = 8080\r\nweb_server_ssl_cert =\r\nweb_server_ssl_key =\r\nsession_backend = database\r\nweb_server_master_timeout = 600\r\nweb_server_worker_timeout = 600\r\nworker_refresh_batch_size = 1\r\nworker_refresh_interval = 6000\r\nreload_on_plugin_change = False\r\nsecret_key = THIS IS UNSAFE!\r\nworkers = 4\r\nworker_class = sync\r\naccess_logfile = -\r\nerror_logfile = -\r\naccess_logformat =\r\nexpose_config = non-sensitive-only\r\nexpose_hostname = False\r\nexpose_stacktrace = False\r\ndag_default_view = grid\r\ndag_orientation = LR\r\ngrid_view_sorting_order = topological\r\nlog_fetch_timeout_sec = 5\r\nlog_fetch_delay_sec = 2\r\nlog_auto_tailing_offset = 30\r\nlog_animation_speed = 1000\r\nhide_paused_dags_by_default = False\r\npage_size = 100\r\nnavbar_color = #fff\r\nnavbar_text_color = #51504f\r\nnavbar_hover_color = #eee\r\nnavbar_text_hover_color = #51504f\r\nnavbar_logo_text_color = #51504f\r\ndefault_dag_run_display_number = 25\r\nenable_proxy_fix = True\r\nproxy_fix_x_for = 1\r\nproxy_fix_x_proto = 1\r\nproxy_fix_x_host = 1\r\nproxy_fix_x_port = 1\r\nproxy_fix_x_prefix = 1\r\ncookie_secure = False\r\ncookie_samesite = Lax\r\ndefault_wrap = False\r\nx_frame_enabled = True\r\n# analytics_tool =\r\n# analytics_id =\r\n# analytics_url =\r\nshow_recent_stats_for_completed_runs = True\r\nsession_lifetime_minutes = 43200\r\n# instance_name =\r\ninstance_name_has_markup = False\r\nauto_refresh_interval = 3\r\nwarn_deployment_exposure = False\r\n# audit_view_excluded_events =\r\n# audit_view_included_events =\r\nenable_swagger_ui = True\r\nrun_internal_api = False\r\ncaching_hash_method = md5\r\nshow_trigger_form_if_no_params = False\r\nnum_recent_configurations_for_trigger = 5\r\nallow_raw_html_descriptions = False\r\nallowed_payload_size = 1.0\r\nrequire_confirmation_dag_change = False\r\n[email]\r\nemail_backend = airflow.utils.email.send_email_smtp\r\nemail_conn_id = smtp_default\r\ndefault_email_on_retry = True\r\ndefault_email_on_failure = True\r\n# subject_template =\r\n# html_content_template =\r\n# from_email =\r\nssl_context = default\r\n[smtp]\r\nsmtp_host = localhost\r\nsmtp_starttls = True\r\nsmtp_ssl = False\r\n# smtp_user =\r\n# smtp_password =\r\nsmtp_port = 25\r\nsmtp_mail_from = airflow@example.com\r\nsmtp_timeout = 30\r\nsmtp_retry_limit = 5\r\n[sentry]\r\nsentry_on = false\r\nsentry_dsn =\r\n# before_send =\r\n[scheduler]\r\njob_heartbeat_sec = 5\r\nscheduler_heartbeat_sec = 5\r\nlocal_task_job_heartbeat_sec = 0\r\nnum_runs = -1\r\nscheduler_idle_sleep_time = 1\r\nmin_file_process_interval = 30\r\nparsing_cleanup_interval = 30\r\nstale_dag_threshold = 50\r\ndag_dir_list_interval = 60\r\nprint_stats_interval = 30\r\npool_metrics_interval = 5.0\r\nscheduler_health_check_threshold = 30\r\nenable_health_check = False\r\nscheduler_health_check_server_host = 0.0.0.0\r\nscheduler_health_check_server_port = 8974\r\norphaned_tasks_check_interval = 300.0\r\nchild_process_log_directory = /opt/airflow/logs/scheduler\r\nscheduler_zombie_task_threshold = 300\r\nzombie_detection_interval = 10.0\r\ncatchup_by_default = True\r\nignore_first_depends_on_past_by_default = True\r\nmax_tis_per_query = 16\r\nuse_row_level_locking = True\r\nmax_dagruns_to_create_per_loop = 10\r\nmax_dagruns_per_loop_to_schedule = 20\r\nschedule_after_task_execution = True\r\nparsing_pre_import_modules = True\r\nparsing_processes = 2\r\nfile_parsing_sort_mode = modified_time\r\nstandalone_dag_processor = False\r\nmax_callbacks_per_loop = 20\r\ndag_stale_not_seen_duration = 600\r\nuse_job_schedule = True\r\nallow_trigger_in_future = False\r\ntrigger_timeout_check_interval = 15\r\ntask_queued_timeout = 600.0\r\ntask_queued_timeout_check_interval = 120.0\r\nallowed_run_id_pattern = ^[A-Za-z0-9_.~:+-]+$\r\ncreate_cron_data_intervals = True\r\n[triggerer]\r\ndefault_capacity = 1000\r\njob_heartbeat_sec = 5\r\ntriggerer_health_check_threshold = 30\r\n[kerberos]\r\nccache = /tmp/airflow_krb5_ccache\r\nprincipal = airflow\r\nreinit_frequency = 3600\r\nkinit_path = kinit\r\nkeytab = airflow.keytab\r\nforwardable = True\r\ninclude_ip = True\r\n[sensors]\r\ndefault_timeout = 604800\r\n[usage_data_collection]\r\nenabled = True\r\n[aws]\r\n# session_factory =\r\ncloudwatch_task_handler_json_serializer = airflow.providers.amazon.aws.log.cloudwatch_task_handler.json_serialize_legacy\r\n[aws_batch_executor]\r\nconn_id = aws_default\r\n# region_name =\r\nmax_submit_job_attempts = 3\r\ncheck_health_on_startup = True\r\n# job_name =\r\n# job_queue =\r\n# job_definition =\r\n# submit_job_kwargs =\r\n[aws_ecs_executor]\r\nconn_id = aws_default\r\n# region_name =\r\nassign_public_ip = False\r\n# cluster =\r\n# capacity_provider_strategy =\r\n# container_name =\r\n# launch_type =\r\nplatform_version = LATEST\r\n# security_groups =\r\n# subnets =\r\n# task_definition =\r\nmax_run_task_attempts = 3\r\n# run_task_kwargs =\r\ncheck_health_on_startup = True\r\n[aws_auth_manager]\r\nenable = False\r\nconn_id = aws_default\r\n# region_name =\r\n# saml_metadata_url =\r\n# avp_policy_store_id =\r\n[celery_kubernetes_executor]\r\nkubernetes_queue = kubernetes\r\n[celery]\r\ncelery_app_name = airflow.providers.celery.executors.celery_executor\r\nworker_concurrency = 16\r\n# worker_autoscale =\r\nworker_prefetch_multiplier = 1\r\nworker_enable_remote_control = true\r\nbroker_url = redis://redis:6379/0\r\n# result_backend =\r\nresult_backend_sqlalchemy_engine_options =\r\nflower_host = 0.0.0.0\r\nflower_url_prefix =\r\nflower_port = 5555\r\nflower_basic_auth =\r\nsync_parallelism = 0\r\ncelery_config_options = airflow.providers.celery.executors.default_celery.DEFAULT_CELERY_CONFIG\r\nssl_active = False\r\nssl_key =\r\nssl_cert =\r\nssl_cacert =\r\npool = prefork\r\noperation_timeout = 1.0\r\ntask_acks_late = True\r\ntask_track_started = True\r\ntask_publish_max_retries = 3\r\nworker_precheck = False\r\n[celery_broker_transport_options]\r\n# visibility_timeout =\r\n# sentinel_kwargs =\r\n[local_kubernetes_executor]\r\nkubernetes_queue = kubernetes\r\n[kubernetes_executor]\r\napi_client_retry_configuration =\r\nlogs_task_metadata = False\r\npod_template_file = /opt/airflow/pod_templates/pod_template.yaml\r\nworker_container_repository = apache/airflow\r\nworker_container_tag = 2.10.0-python3.11\r\nnamespace = airflow-alpha\r\ndelete_worker_pods = True\r\ndelete_worker_pods_on_failure = True\r\nworker_pod_pending_fatal_container_state_reasons = CreateContainerConfigError,ErrImagePull,CreateContainerError,ImageInspectError, InvalidImageName\r\nworker_pods_creation_batch_size = 10\r\nmulti_namespace_mode = False\r\nmulti_namespace_mode_namespace_list =\r\nin_cluster = True\r\n# cluster_context =\r\n# config_file =\r\nkube_client_request_args = {""_request_timeout"": [240, 240]}\r\ndelete_option_kwargs =\r\nenable_tcp_keepalive = True\r\ntcp_keep_idle = 120\r\ntcp_keep_intvl = 30\r\ntcp_keep_cnt = 6\r\nverify_ssl = True\r\nworker_pods_queued_check_interval = 60\r\nssl_ca_cert =\r\ntask_publish_max_retries = 0\r\n[common.io]\r\nxcom_objectstorage_path =\r\nxcom_objectstorage_threshold = -1\r\nxcom_objectstorage_compression =\r\n[elasticsearch]\r\nhost =\r\nlog_id_template = {dag_id}-{task_id}-{run_id}-{map_index}-{try_number}\r\nend_of_log_mark = end_of_log\r\nfrontend =\r\nwrite_stdout = False\r\njson_format = False\r\njson_fields = asctime, filename, lineno, levelname, message\r\nhost_field = host\r\noffset_field = offset\r\nindex_patterns = _all\r\nindex_patterns_callable =\r\n[elasticsearch_configs]\r\nhttp_compress = False\r\nverify_certs = True\r\n[fab]\r\nauth_rate_limited = True\r\nauth_rate_limit = 5 per 40 second\r\nupdate_fab_perms = True\r\n[imap]\r\n# ssl_context =\r\n[azure_remote_logging]\r\nremote_wasb_log_container = airflow-logs\r\n[openlineage]\r\ndisabled = False\r\ndisabled_for_operators =\r\nselective_enable = False\r\n# namespace =\r\n# extractors =\r\ncustom_run_facets =\r\nconfig_path =\r\ntransport =\r\ndisable_source_code = False\r\ndag_state_change_process_pool_size = 1\r\nexecution_timeout = 10\r\ninclude_full_task_info = False\r\n[smtp_provider]\r\n# ssl_context =\r\n# templated_email_subject_path =\r\n# templated_html_content_path =\r\n```\r\n\r\ntried with airflow 2.10.0 and 2.10.2 (but have been observing the issues since 2.8.x).\r\ndags are kept in a git repository (Azure Repos)\r\ndeployed on a k8s cluster (AKS) using the community helm chart - not sure if you have all the infor from configs or something more is needed from values as well? \r\n\r\nso here is the image of the commit - delete a dag at around 21:01:\r\n![Screenshot_1](https://github.com/user-attachments/assets/03bec17a-4554-49ab-add2-40b0cef4ac86)\r\n\r\nhere is the log from git sync: \r\n\r\n![Screenshot_3](https://github.com/user-attachments/assets/6b9c1d74-d0e9-4205-bb5b-5b606a3badf7)\r\n\r\nhere is the ui at around 21:02- still present: \r\n![Screenshot_2](https://github.com/user-attachments/assets/6a5c7824-97d3-4a21-bb5e-e853375a6cf4)\r\n\r\n\r\nhere is the dag folder (scheduler) 21:05: \r\n\r\n![Screenshot_4](https://github.com/user-attachments/assets/8ab8164c-94a6-424e-945a-e2e91a939855)\r\n\r\n\r\nhere is dag ui and dag table 21:53:\r\n\r\n![Screenshot_6](https://github.com/user-attachments/assets/a2b00dec-cc8b-4081-95f9-76b0fd2ad61a)\r\n![Screenshot_7](https://github.com/user-attachments/assets/b6c33006-baef-4147-8c74-c4a91cbadc00)\r\n\r\nhere are the logs after ~ 1h from scheduler when trying to run the dag (after restarting the scheduler: \r\n\r\n```\r\n/home/airflow/.local/lib/python3.11/site-packages/airflow/plugins_manager.py:30 DeprecationWarning: \'cgitb\' is deprecated and slated for removal in Python 3.13\r\n  ____________       _____________\r\n ____    |__( )_________  __/__  /________      __\r\n____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /\r\n___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /\r\n _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/\r\n[2024-09-27T20:08:09.049+0000] {_client.py:1026} INFO - HTTP Request: GET https://apacheairflow.gateway.scarf.sh/scheduler?version=2.10.0&python_version=3.11.9&platform=Linux&arch=x86_64&database=postgresql&db_version=14.9&executor=KubernetesExecutor ""HTTP/1.1 200 OK""\r\n/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py:143 FutureWarning: The config section [kubernetes] has been renamed to [kubernetes_executor]. Please update your `conf.get*` call to use the new name\r\n[2024-09-27T20:08:10.788+0000] {executor_loader.py:254} INFO - Loaded executor: KubernetesExecutor\r\n[2024-09-27T20:08:10.901+0000] {scheduler_job_runner.py:935} INFO - Starting the scheduler\r\n[2024-09-27T20:08:10.901+0000] {scheduler_job_runner.py:942} INFO - Processing each file at most -1 times\r\n[2024-09-27T20:08:10.902+0000] {kubernetes_executor.py:287} INFO - Start Kubernetes executor\r\n[2024-09-27T20:08:10.946+0000] {kubernetes_executor_utils.py:140} INFO - Event: and now my watch begins starting at resource_version: 0\r\n[2024-09-27T20:08:10.990+0000] {kubernetes_executor.py:208} INFO - Found 0 queued task instances\r\n[2024-09-27T20:08:10.999+0000] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 89\r\n[2024-09-27T20:08:11.002+0000] {scheduler_job_runner.py:1843} INFO - Adopting or resetting orphaned tasks for active dag runs\r\n[2024-09-27T20:08:11.011+0000] {settings.py:63} INFO - Configured default timezone UTC\r\n[2024-09-27T20:08:21.101+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:\r\n        <TaskInstance: example_dag_to_be_deleted.task-kubernetes manual__2024-09-27T20:08:20.051996+00:00 [scheduled]>\r\n[2024-09-27T20:08:21.102+0000] {scheduler_job_runner.py:495} INFO - DAG example_dag_to_be_deleted has 0/18 running and queued tasks\r\n[2024-09-27T20:08:21.102+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:\r\n        <TaskInstance: example_dag_to_be_deleted.task-kubernetes manual__2024-09-27T20:08:20.051996+00:00 [scheduled]>\r\n[2024-09-27T20:08:21.105+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: example_dag_to_be_deleted.task-kubernetes manual__2024-09-27T20:08:20.051996+00:00 [scheduled]>] for executor: KubernetesExecutor(parallelism=32)\r\n[2024-09-27T20:08:21.106+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id=\'example_dag_to_be_deleted\', task_id=\'task-kubernetes\', run_id=\'manual__2024-09-27T20:08:20.051996+00:00\', try_number=1, map_index=-1) to KubernetesExecutor with priority 1 and queue kubernetes\r\n[2024-09-27T20:08:21.106+0000] {base_executor.py:168} INFO - Adding to queue: [\'airflow\', \'tasks\', \'run\', \'example_dag_to_be_deleted\', \'task-kubernetes\', \'manual__2024-09-27T20:08:20.051996+00:00\', \'--local\', \'--subdir\', \'DAGS_FOLDER/example_dag_to_be_deleted.py\']\r\n[2024-09-27T20:08:21.109+0000] {kubernetes_executor.py:326} INFO - Add task TaskInstanceKey(dag_id=\'example_dag_to_be_deleted\', task_id=\'task-kubernetes\', run_id=\'manual__2024-09-27T20:08:20.051996+00:00\', try_number=1, map_index=-1) with command [\'airflow\', \'tasks\', \'run\', \'example_dag_to_be_deleted\', \'task-kubernetes\', \'manual__2024-09-27T20:08:20.051996+00:00\', \'--local\', \'--subdir\', \'DAGS_FOLDER/example_dag_to_be_deleted.py\']\r\n[2024-09-27T20:08:21.145+0000] {kubernetes_executor_utils.py:426} INFO - Creating kubernetes pod for job is TaskInstanceKey(dag_id=\'example_dag_to_be_deleted\', task_id=\'task-kubernetes\', run_id=\'manual__2024-09-27T20:08:20.051996+00:00\', try_number=1, map_index=-1), with pod name example-dag-to-be-deleted-task-kubernetes-tqqfawxa, annotations: <omitted>\r\n[2024-09-27T20:08:21.199+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state queued for task instance TaskInstanceKey(dag_id=\'example_dag_to_be_deleted\', task_id=\'task-kubernetes\', run_id=\'manual__2024-09-27T20:08:20.051996+00:00\', try_number=1, map_index=-1)\r\n[2024-09-27T20:08:21.206+0000] {kubernetes_executor_utils.py:265} INFO - Event: example-dag-to-be-deleted-task-kubernetes-tqqfawxa Pending, annotations: <omitted>\r\n[2024-09-27T20:08:21.211+0000] {scheduler_job_runner.py:791} INFO - Setting external_id for <TaskInstance: example_dag_to_be_deleted.task-kubernetes manual__2024-09-27T20:08:20.051996+00:00 [queued]> to 10\r\n[2024-09-27T20:08:21.215+0000] {kubernetes_executor_utils.py:265} INFO - Event: example-dag-to-be-deleted-task-kubernetes-tqqfawxa Pending, annotations: <omitted>\r\n[2024-09-27T20:08:21.234+0000] {kubernetes_executor_utils.py:265} INFO - Event: example-dag-to-be-deleted-task-kubernetes-tqqfawxa Pending, annotations: <omitted>\r\n[2024-09-27T20:08:22.202+0000] {kubernetes_executor_utils.py:265} INFO - Event: example-dag-to-be-deleted-task-kubernetes-tqqfawxa Pending, annotations: <omitted>\r\n[2024-09-27T20:08:30.227+0000] {kubernetes_executor_utils.py:265} INFO - Event: example-dag-to-be-deleted-task-kubernetes-tqqfawxa Pending, annotations: <omitted>\r\n[2024-09-27T20:08:31.230+0000] {kubernetes_executor_utils.py:299} INFO - Event: example-dag-to-be-deleted-task-kubernetes-tqqfawxa is Running, annotations: <omitted>\r\n[2024-09-27T20:08:47.288+0000] {kubernetes_executor_utils.py:299} INFO - Event: example-dag-to-be-deleted-task-kubernetes-tqqfawxa is Running, annotations: <omitted>\r\n[2024-09-27T20:08:48.480+0000] {kubernetes_executor_utils.py:269} ERROR - Event: example-dag-to-be-deleted-task-kubernetes-tqqfawxa Failed, annotations: <omitted>\r\n[2024-09-27T20:08:48.764+0000] {kubernetes_executor.py:370} INFO - Changing state of (TaskInstanceKey(dag_id=\'example_dag_to_be_deleted\', task_id=\'task-kubernetes\', run_id=\'manual__2024-09-27T20:08:20.051996+00:00\', try_number=1, map_index=-1), <TaskInstanceState.FAILED: \'failed\'>, \'example-dag-to-be-deleted-task-kubernetes-tqqfawxa\', \'airflow-alpha\', \'328757893\') to failed\r\n[2024-09-27T20:08:48.795+0000] {kubernetes_executor_utils.py:269} ERROR - Event: example-dag-to-be-deleted-task-kubernetes-tqqfawxa Failed, annotations: <omitted>\r\n[2024-09-27T20:08:48.802+0000] {kubernetes_executor.py:475} INFO - Deleted pod associated with the TI TaskInstanceKey(dag_id=\'example_dag_to_be_deleted\', task_id=\'task-kubernetes\', run_id=\'manual__2024-09-27T20:08:20.051996+00:00\', try_number=1, map_index=-1). Pod name: example-dag-to-be-deleted-task-kubernetes-tqqfawxa. Namespace: airflow-alpha\r\n[2024-09-27T20:08:48.803+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id=\'example_dag_to_be_deleted\', task_id=\'task-kubernetes\', run_id=\'manual__2024-09-27T20:08:20.051996+00:00\', try_number=1, map_index=-1)\r\n[2024-09-27T20:08:48.811+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_dag_to_be_deleted, task_id=task-kubernetes, run_id=manual__2024-09-27T20:08:20.051996+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor=KubernetesExecutor(parallelism=32), executor_state=failed, try_number=1, max_tries=0, job_id=None, pool=default_pool, queue=kubernetes, priority_weight=1, operator=PythonOperator, queued_dttm=2024-09-27 20:08:21.103374+00:00, queued_by_job_id=10, pid=None\r\n[2024-09-27T20:08:48.812+0000] {scheduler_job_runner.py:907} ERROR - Executor KubernetesExecutor(parallelism=32) reported that the task instance <TaskInstance: example_dag_to_be_deleted.task-kubernetes manual__2024-09-27T20:08:20.051996+00:00 [queued]> finished with state failed, but the task instance\'s state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally\r\n[2024-09-27T20:08:48.822+0000] {taskinstance.py:3303} ERROR - Executor KubernetesExecutor(parallelism=32) reported that the task instance <TaskInstance: example_dag_to_be_deleted.task-kubernetes manual__2024-09-27T20:08:20.051996+00:00 [queued]> finished with state failed, but the task instance\'s state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally\r\n[2024-09-27T20:08:48.843+0000] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=example_dag_to_be_deleted, task_id=task-kubernetes, run_id=manual__2024-09-27T20:08:20.051996+00:00, execution_date=20240927T200820, start_date=, end_date=20240927T200848\r\n[2024-09-27T20:08:49.127+0000] {kubernetes_executor_utils.py:269} ERROR - Event: example-dag-to-be-deleted-task-kubernetes-tqqfawxa Failed, annotations: <omitted>\r\n[2024-09-27T20:08:49.955+0000] {dagrun.py:823} ERROR - Marking run <DagRun example_dag_to_be_deleted @ 2024-09-27 20:08:20.051996+00:00: manual__2024-09-27T20:08:20.051996+00:00, state:running, queued_at: 2024-09-27 20:08:20.084580+00:00. externally triggered: True> failed\r\n[2024-09-27T20:08:49.956+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=example_dag_to_be_deleted, execution_date=2024-09-27 20:08:20.051996+00:00, run_id=manual__2024-09-27T20:08:20.051996+00:00, run_start_date=2024-09-27 20:08:20.303660+00:00, run_end_date=2024-09-27 20:08:49.956078+00:00, run_duration=29.652418, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-09-26 00:00:00+00:00, data_interval_end=2024-09-27 00:00:00+00:00, dag_hash=1a61ee6a34486df6240d5abb977f4507\r\n[2024-09-27T20:08:49.982+0000] {kubernetes_executor.py:370} INFO - Changing state of (TaskInstanceKey(dag_id=\'example_dag_to_be_deleted\', task_id=\'task-kubernetes\', run_id=\'manual__2024-09-27T20:08:20.051996+00:00\', try_number=1, map_index=-1), <TaskInstanceState.FAILED: \'failed\'>, \'example-dag-to-be-deleted-task-kubernetes-tqqfawxa\', \'airflow-alpha\', \'328757894\') to failed\r\n[2024-09-27T20:08:49.991+0000] {kubernetes_executor.py:475} INFO - Deleted pod associated with the TI TaskInstanceKey(dag_id=\'example_dag_to_be_deleted\', task_id=\'task-kubernetes\', run_id=\'manual__2024-09-27T20:08:20.051996+00:00\', try_number=1, map_index=-1). Pod name: example-dag-to-be-deleted-task-kubernetes-tqqfawxa. Namespace: airflow-alpha\r\n[2024-09-27T20:08:49.993+0000] {kubernetes_executor.py:370} INFO - Changing state of (TaskInstanceKey(dag_id=\'example_dag_to_be_deleted\', task_id=\'task-kubernetes\', run_id=\'manual__2024-09-27T20:08:20.051996+00:00\', try_number=1, map_index=-1), <TaskInstanceState.FAILED: \'failed\'>, \'example-dag-to-be-deleted-task-kubernetes-tqqfawxa\', \'airflow-alpha\', \'328757895\') to failed\r\n[2024-09-27T20:08:50.000+0000] {kubernetes_executor.py:475} INFO - Deleted pod associated with the TI TaskInstanceKey(dag_id=\'example_dag_to_be_deleted\', task_id=\'task-kubernetes\', run_id=\'manual__2024-09-27T20:08:20.051996+00:00\', try_number=1, map_index=-1). Pod name: example-dag-to-be-deleted-task-kubernetes-tqqfawxa. Namespace: airflow-alpha\r\n[2024-09-27T20:08:50.002+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id=\'example_dag_to_be_deleted\', task_id=\'task-kubernetes\', run_id=\'manual__2024-09-27T20:08:20.051996+00:00\', try_number=1, map_index=-1)\r\n[2024-09-27T20:08:50.010+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_dag_to_be_deleted, task_id=task-kubernetes, run_id=manual__2024-09-27T20:08:20.051996+00:00, map_index=-1, run_start_date=None, run_end_date=2024-09-27 20:08:48.831070+00:00, run_duration=None, state=failed, executor=KubernetesExecutor(parallelism=32), executor_state=failed, try_number=1, max_tries=0, job_id=None, pool=default_pool, queue=kubernetes, priority_weight=1, operator=PythonOperator, queued_dttm=2024-09-27 20:08:21.103374+00:00, queued_by_job_id=10, pid=None\r\n[2024-09-27T20:09:11.082+0000] {kubernetes_executor.py:208} INFO - Found 0 queued task instances\r\n[2024-09-27T20:09:19.134+0000] {kubernetes_executor_utils.py:101} INFO - Kubernetes watch timed out waiting for events. Restarting watch.\r\n[2024-09-27T20:09:20.136+0000] {kubernetes_executor_utils.py:140} INFO - Event: and now my watch begins starting at resource_version: 0\r\n```\r\n\r\nthe difference from my original setup to this is that in my original setup I run a KubernetesPodOperator - that similar to the empty operator actually runs successfully without any issues even tho the dag is not there. \r\n\r\nIn the new setup I have an EmptyOperator and a PythonOperator - the empty operator runs okay, but the python operator fails saying that the dag cannot be found. \r\n\r\nIn my original setup these are the logs I get, even tho the dag is not there: \r\n\r\n```\r\n/home/airflow/.local/lib/python3.11/site-packages/airflow/plugins_manager.py:30 DeprecationWarning: \'cgitb\' is deprecated and slated for removal in Python 3.13\r\n[2024-09-27T19:09:22.402+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/repo/A235712_gea_test.py\r\n[2024-09-27T19:09:22.404+0000] {cli.py:243} WARNING - Dag \'Example_dag\' not found in path /opt/airflow/dags/repo/Example_dag.py; trying path /opt/airflow/dags/repo/\r\n[2024-09-27T19:09:22.404+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/repo/\r\n```', 'created_at': datetime.datetime(2024, 9, 27, 20, 17, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2408258153, 'issue_id': 2552582598, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 10, 12, 0, 14, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2425479275, 'issue_id': 2552582598, 'author': 'cjj1120', 'body': 'below is the testing that I did locally, lmk if u need more info @shahar1 \r\n*I\'m using community Airflow Helm chart* \r\n\r\n## How to reproduce \r\n1. Deploy Airflow with below helm chart. \r\n2. Remove DAG file and push the changes. \r\n3. Verify that latest changes is synced.\r\n\r\nObservation: \r\nThe DAG is not deactivated, it has to be paused/ deleted manually from the UI. \r\n\r\n## Operating System \r\nMac OS\r\n\r\n## Version of Apache Airflow Providers \r\napache-airflow==2.9.3\r\napache-airflow-providers-postgres==5.11.3\r\napache-airflow-providers-cncf-kubernetes==8.3.4\r\n\r\n## Deployment \r\n [community helm chart](https://github.com/airflow-helm/charts/tree/main). \r\nTesting is done locally with cluster set up with `kind`. \r\n\r\n## Deployment details \r\n\r\nBase Docker image: `apache/airflow:slim-2.9.3-python3.11`\r\nhelm chart value file: \r\n```yaml\r\n########################################\r\n## CONFIG | Airflow Configs\r\n########################################\r\nairflow:\r\n  ## if we use legacy 1.10 airflow commands\r\n  legacyCommands: false\r\n\r\n  ## configs for the airflow container image\r\n  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/configuration/airflow-version.md\r\n  image:\r\n    # repository: localhost:5000/airflow-custom\r\n    # tag: latest\r\n    repository: airflow-custom\r\n    tag: latest\r\n\r\n  ## the airflow executor type to use\r\n  executor: KubernetesExecutor\r\n\r\n  ## the fernet encryption key (sets `AIRFLOW__CORE__FERNET_KEY`)\r\n  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/security/set-fernet-key.md\r\n  ## [WARNING] change from default value to ensure security\r\n  fernetKey: """"\r\n\r\n  ## the secret_key for flask (sets `AIRFLOW__WEBSERVER__SECRET_KEY`)\r\n  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/security/set-webserver-secret-key.md\r\n  ## [WARNING] change from default value to ensure security\r\n  webserverSecretKey: ""THIS IS UNSAFE!""\r\n\r\n  ## environment variables for airflow configs\r\n  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/configuration/airflow-configs.md\r\n  config:\r\n    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: ""False""\r\n    AIRFLOW__CORE__LOAD_EXAMPLES: ""False""\r\n    AIRFLOW__CORE__DEFAULT_TIMEZONE: ""utc""    \r\n    # AIRFLOW__KUBERNETES_EXECUTOR__DELETE_WORKER_PODS_ON_FAILURE: ""False""\r\n    AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: ""False""\r\n    AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 120\r\n    AIRFLOW__SCHEDULER__PARSING_CLEANUP_INTERVAL: 6\r\n    AIRFLOW__SCHEDULER__STALE_DAG_THRESHOLD: 6\r\n    AIRFLOW__SCHEDULER__TASK_QUEUED_TIMEOUT: 300\r\n    AIRFLOW_VAR_ENVIRONMENT: ""DEV""    \r\n    AIRFLOW__LOGGING__LOGGING_LEVEL: ""INFO""\r\n    # AIRFLOW__LOGGING__DAG_PROCESSOR_MANAGER_LOG_STDOUT: ""True""\r\n    # AIRFLOW__LOGGING__DAG_PROCESSOR_LOG_TARGET: ""stdout"" # default fo files in scheduler pod. \r\n    AIRFLOW__WEBSERVER__BASE_URL: http://localhost:8080\r\n    AIRFLOW__WEBSERVER__DEFAULT_WRAP: True\r\n    AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: True\r\n    AIRFLOW__METRICS__STATSD_ON: True\r\n    AIRFLOW__METRICS__STATSD_HOST: statsd-exporter\r\n    AIRFLOW__METRICS__STATSD_PORT: 9125\r\n    AIRFLOW__METRICS__STATSD_PREFIX: airflow\r\n\r\n  ## a list of users to create\r\n  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/security/airflow-users.md\r\n  users:\r\n    - username: admin\r\n      password: admin\r\n      role: Admin\r\n      email: admin@example.com\r\n      firstName: admin\r\n      lastName: admin\r\n\r\n  ## a list airflow connections to create\r\n  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/dags/airflow-connections.md\r\n  connections: []\r\n\r\n  ## a list airflow variables to create\r\n  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/dags/airflow-variables.md\r\n  variables: []\r\n\r\n  ## a list airflow pools to create\r\n  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/dags/airflow-pools.md\r\n  pools: []\r\n\r\n  ## extra pip packages to install in airflow Pods\r\n  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/configuration/extra-python-packages.md\r\n  ## [WARNING] this feature is not recommended for production use, see docs\r\n  extraPipPackages: []\r\n\r\n  ## extra environment variables for the airflow Pods\r\n  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/kubernetes/mount-environment-variables.md\r\n  extraEnv: []\r\n\r\n  ## extra VolumeMounts for the airflow Pods\r\n  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/kubernetes/mount-persistent-volumes.md\r\n  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/kubernetes/mount-files.md\r\n  extraVolumeMounts: []\r\n    # - name: airflow-logs-volume\r\n    #   mountPath: /airflow-logs\r\n\r\n  ## extra Volumes for the airflow Pods\r\n  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/kubernetes/mount-persistent-volumes.md\r\n  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/kubernetes/mount-files.md\r\n  extraVolumes: []\r\n    # - name: airflow-logs-volume\r\n    #   persistentVolumeClaim:\r\n    #     claimName: airflow-logs-claim\r\n\r\n  ## configs generating the `pod_template.yaml` file for `AIRFLOW__KUBERNETES__POD_TEMPLATE_FILE`\r\n  ## [NOTE] the `dags.gitSync` values will create a git-sync init-container in the pod\r\n  ## [NOTE] the `airflow.extraPipPackages` will NOT be installed\r\n  kubernetesPodTemplate:\r\n\r\n    ## the full content of the pod-template file (as a string)\r\n    ## [NOTE] all other `kubernetesPodTemplate.*` are disabled when this is set\r\n    stringOverride: """"\r\n\r\n    ## resource requests/limits for the Pod template ""base"" container\r\n    ## [SPEC] https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#resourcerequirements-v1-core\r\n    resources: {}\r\n\r\n    ## extra pip packages to install in the Pod template\r\n    ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/configuration/extra-python-packages.md\r\n    ## [WARNING] this feature is not recommended for production use, see docs\r\n    extraPipPackages: []\r\n\r\n    ## extra VolumeMounts for the Pod template\r\n    ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/kubernetes/mount-persistent-volumes.md\r\n    ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/kubernetes/mount-files.md\r\n    extraVolumeMounts: []\r\n\r\n    ## extra Volumes for the Pod template\r\n    ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/kubernetes/mount-persistent-volumes.md\r\n    ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/kubernetes/mount-files.md\r\n    extraVolumes: []\r\n\r\n###################################\r\n## COMPONENT | Airflow Scheduler\r\n###################################\r\nscheduler:\r\n  ## the number of scheduler Pods to run\r\n  replicas: 1\r\n\r\n  ## resource requests/limits for the scheduler Pods\r\n  ## [SPEC] https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#resourcerequirements-v1-core\r\n  resources: {}\r\n\r\n  ## configs for the log-cleanup sidecar of the scheduler\r\n  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/monitoring/log-cleanup.md\r\n  logCleanup:\r\n    enabled: false\r\n    retentionMinutes: 21600\r\n\r\n  ## configs for the scheduler Pods\' liveness probe\r\n  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/monitoring/scheduler-liveness-probe.md\r\n  livenessProbe:\r\n    enabled: true\r\n\r\n    ## configs for an additional check that ensures tasks are being created by the scheduler\r\n    ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/monitoring/scheduler-liveness-probe.md\r\n    taskCreationCheck:\r\n      enabled: false\r\n      thresholdSeconds: 300\r\n      schedulerAgeBeforeCheck: 180\r\n\r\n###################################\r\n## COMPONENT | Airflow Webserver\r\n###################################\r\nweb:\r\n  ## the number of web Pods to run\r\n  replicas: 1\r\n\r\n  ## resource requests/limits for the web Pods\r\n  ## [SPEC] https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#resourcerequirements-v1-core\r\n  resources: {}\r\n\r\n  ## configs for the Service of the web Pods\r\n  service:\r\n    type: ClusterIP\r\n    externalPort: 8080\r\n\r\n\r\n  # DEFAULT \r\n  ## configs generating the `webserver_config.py` file\r\n  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/configuration/airflow-configs.md#webserver_configpy\r\n  # webserverConfig:\r\n  #   ## the full content of the `webserver_config.py` file (as a string)\r\n  #   stringOverride: |\r\n  #     from airflow import configuration as conf\r\n  #     from flask_appbuilder.security.manager import AUTH_DB\r\n      \r\n  #     # the SQLAlchemy connection string\r\n  #     SQLALCHEMY_DATABASE_URI = conf.get(""core"", ""SQL_ALCHEMY_CONN"")\r\n      \r\n  #     # use embedded DB for auth\r\n  #     AUTH_TYPE = AUTH_DB\r\n\r\n  # OVERWRITE   \r\n  webserverConfig:\r\n    ## if the `webserver_config.py` file is mounted\r\n    ## - set to false if you wish to mount your own `webserver_config.py` file\r\n    ##\r\n    enabled: true\r\n\r\n    ## the full content of the `webserver_config.py` file (as a string)\r\n    ## - docs for Flask-AppBuilder security configs:\r\n    ##   https://flask-appbuilder.readthedocs.io/en/latest/security.html\r\n    ##\r\n    ## ____ EXAMPLE _______________\r\n    ##   stringOverride: |\r\n    ##     from airflow import configuration as conf\r\n    ##     from flask_appbuilder.security.manager import AUTH_DB\r\n    ##\r\n    ##     # the SQLAlchemy connection string\r\n    ##     SQLALCHEMY_DATABASE_URI = conf.get(\'core\', \'SQL_ALCHEMY_CONN\')\r\n    ##\r\n    ##     # use embedded DB for auth\r\n    ##     AUTH_TYPE = AUTH_DB\r\n    ##\r\n    stringOverride: """"\r\n\r\n    ## the name of a Secret containing a `webserver_config.py` key\r\n    ##\r\n    existingSecret: """"\r\n\r\n  ## the number of web Pods to run\r\n  ## - if you set this >1 we recommend defining a `web.podDisruptionBudget`\r\n  ##\r\n\r\n###################################\r\n## COMPONENT | Airflow Workers\r\n###################################\r\nworkers:\r\n  ## if the airflow workers StatefulSet should be deployed\r\n  enabled: false\r\n\r\n###################################\r\n## COMPONENT | Triggerer\r\n###################################\r\ntriggerer:\r\n  ## if the airflow triggerer should be deployed\r\n  enabled: true\r\n\r\n  ## the number of triggerer Pods to run\r\n  replicas: 1\r\n\r\n  ## resource requests/limits for the triggerer Pods\r\n  ## [SPEC] https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#resourcerequirements-v1-core\r\n  resources: {}\r\n\r\n  ## maximum number of triggers each triggerer will run at once (sets `AIRFLOW__TRIGGERER__DEFAULT_CAPACITY`)\r\n  capacity: 1000\r\n\r\n###################################\r\n## COMPONENT | Flower\r\n###################################\r\nflower:\r\n  ## if the airflow flower UI should be deployed\r\n  enabled: false\r\n\r\n###################################\r\n## CONFIG | Airflow Logs\r\n###################################\r\nlogs:\r\n  ## the airflow logs folder\r\n  path: /opt/airflow/logs\r\n\r\n  ## configs for the logs PVC\r\n  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/monitoring/log-persistence.md\r\n  persistence:\r\n    enabled: false\r\n    # ## the name of your existing PersistentVolumeClaim\r\n    # existingClaim: airflow-logs-claim\r\n    # ## WARNING: as multiple pods will write logs, this MUST be ReadWriteMany\r\n    # accessMode: ReadWriteMany\r\n\r\n###################################\r\n## CONFIG | Airflow DAGs\r\n###################################\r\ndags:\r\n  ## the airflow dags folder\r\n  path: /opt/airflow/dags\r\n\r\n  ## configs for the dags PVC\r\n  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/dags/load-dag-definitions.md\r\n  persistence:\r\n    enabled: false\r\n\r\n  ## configs for the git-sync sidecar\r\n  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/dags/load-dag-definitions.md\r\n  gitSync:\r\n    enabled: true\r\n    image:\r\n      repository: registry.k8s.io/git-sync/git-sync\r\n      tag: v3.6.9\r\n      pullPolicy: IfNotPresent\r\n      uid: 65533\r\n      gid: 65533\r\n    # SSH\r\n    repo: ""#REMOVED""\r\n    repoSubPath: ""dags""\r\n    sshSecret: ""#REMOVED""\r\n    sshSecretKey: #REMOVED    \r\n    branch: ""main""\r\n    revision: ""HEAD""\r\n    # depth: 1    \r\n    maxFailures: 5\r\n    syncWait: 15\r\n    # ""known_hosts"" verification can be disabled by setting to ... \r\n    sshKnownHosts: """"         \r\n    # # HTTP \r\n    # repo: https://github.com/cjj1120/local_k8s_airflow_dags.git\r\n    # ## the name of a pre-created Secret with git http credentials\r\n    # ##\r\n    # httpSecret: ""#REMOVED""\r\n\r\n    # ## the key in `dags.gitSync.httpSecret` with your git username\r\n    # ##\r\n    # httpSecretUsernameKey: #REMOVED\r\n\r\n    # ## the key in `dags.gitSync.httpSecret` with your git password/token\r\n    # ##\r\n    # httpSecretPasswordKey: #REMOVED\r\n\r\n###################################\r\n## CONFIG | Kubernetes Ingress\r\n###################################\r\ningress:\r\n  ## if we should deploy Ingress resources\r\n  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/kubernetes/ingress.md\r\n  enabled: false\r\n\r\n###################################\r\n## CONFIG | Kubernetes ServiceAccount\r\n###################################\r\nserviceAccount:\r\n\r\n  ## if a Kubernetes ServiceAccount is created\r\n  create: true\r\n\r\n  ## the name of the ServiceAccount\r\n  name: """"\r\n\r\n  ## annotations for the ServiceAccount\r\n  annotations: {}\r\n\r\n###################################\r\n## CONFIG | Kubernetes Extra Manifests\r\n###################################\r\n\r\n## a list of extra Kubernetes manifests that will be deployed alongside the chart\r\n## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/kubernetes/extra-manifests.md\r\nextraManifests: []\r\n# extraManifests:\r\n#   - |\r\n#     apiVersion: apps/v1\r\n#     kind: Deployment\r\n#     metadata:\r\n#       name: {{ include ""airflow.fullname"" . }}-busybox\r\n#       labels:\r\n#         app: {{ include ""airflow.labels.app"" . }}\r\n#         component: busybox\r\n#         chart: {{ include ""airflow.labels.chart"" . }}\r\n#         release: {{ .Release.Name }}\r\n#         heritage: {{ .Release.Service }}\r\n#     spec:\r\n#       replicas: 1\r\n#       selector:\r\n#         matchLabels:\r\n#           app: {{ include ""airflow.labels.app"" . }}\r\n#           component: busybox\r\n#           release: {{ .Release.Name }}\r\n#       template:\r\n#         metadata:\r\n#           labels:\r\n#             app: {{ include ""airflow.labels.app"" . }}\r\n#             component: busybox\r\n#             release: {{ .Release.Name }}\r\n#         spec:\r\n#           containers:\r\n#             - name: busybox\r\n#               image: busybox:1.35\r\n#               command:\r\n#                 - ""/bin/sh""\r\n#                 - ""-c""\r\n#               args:\r\n#                 - |\r\n#                   ## to break the infinite loop when we receive SIGTERM\r\n#                   trap ""exit 0"" SIGTERM;\r\n#                   ## keep the container running (so people can `kubectl exec -it` into it)\r\n#                   while true; do\r\n#                     echo ""I am alive..."";\r\n#                     sleep 30;\r\n#                   done\r\n\r\n###################################\r\n## DATABASE | PgBouncer\r\n###################################\r\npgbouncer:\r\n  ## if the pgbouncer Deployment is created\r\n  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/database/pgbouncer.md\r\n  enabled: false\r\n\r\n  ## resource requests/limits for the pgbouncer Pods\r\n  ## [SPEC] https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#resourcerequirements-v1-core\r\n  resources: {}\r\n\r\n  ## sets pgbouncer config: `auth_type`\r\n  authType: md5\r\n\r\n###################################\r\n## DATABASE | Embedded Postgres\r\n###################################\r\npostgresql:\r\n  ## if the `stable/postgresql` chart is used\r\n  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/database/embedded-database.md\r\n  ## [WARNING] the embedded Postgres is NOT SUITABLE for production deployments of Airflow\r\n  ## [WARNING] consider using an external database with `externalDatabase.*`\r\n  enabled: false\r\n  ## configs for the PVC of postgresql\r\n  persistence:\r\n    enabled: true\r\n      \r\n    storageClass: """"\r\n    size: 8Gi\r\n\r\n###################################\r\n## DATABASE | External Database\r\n###################################\r\nexternalDatabase:\r\n  type: postgres\r\n  host: #REMOVED\r\n  port: 5432\r\n  database: airflow\r\n  userSecret: #REMOVED\r\n  userSecretKey: #REMOVED\r\n  passwordSecret: #REMOVED\r\n  passwordSecretKey: #REMOVED\r\n\r\n\r\n###################################\r\n## DATABASE | Embedded Redis\r\n###################################\r\nredis:\r\n  ## if the `stable/redis` chart is used\r\n  enabled: false\r\n\r\n###################################\r\n## DATABASE | External Redis\r\n###################################\r\nexternalRedis:\r\n  ## the host of the external redis\r\n  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/database/external-redis.md\r\n  host: localhost`\r\n\r\n```', 'created_at': datetime.datetime(2024, 10, 21, 3, 21, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2436096266, 'issue_id': 2552582598, 'author': 'ahipp13', 'body': ""I can comment as another person that has this behavior in their Airflow instances. I am running Airflow's Helm Chart along with git sync and Kubernetes. I have seen this in all the Airflow versions my team has been on. We currently have been manually clicking the delete button in the UI. Would love to have a feature where we could choose if we want deactivated DAGs showing up in the UI or not."", 'created_at': datetime.datetime(2024, 10, 24, 18, 39, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2471606074, 'issue_id': 2552582598, 'author': 'gordonwells', 'body': ""I don't have time to check the source code to understand why, but enabling the standalone DAG processor in the helm chart values solved this issue for me:\r\n```yaml\r\ndagProcessor:\r\n  enabled: true\r\n```\r\n\r\nI'm using Airflow 2.9.3 and haven't tested this for later versions."", 'created_at': datetime.datetime(2024, 11, 12, 21, 19, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2490402494, 'issue_id': 2552582598, 'author': 'zhaozhaoqueue', 'body': ""> I don't have time to check the source code to understand why, but enabling the standalone DAG processor in the helm chart values solved this issue for me:\r\n> \r\n> ```yaml\r\n> dagProcessor:\r\n>   enabled: true\r\n> ```\r\n> \r\n> I'm using Airflow 2.9.3 and haven't tested this for later versions.\r\n\r\nThank you, @gordonwells, I'm using 2.9.2 Airflow, and your solution works!\r\n\r\nThe weird thing is that everything worked as expected when I used PVC for the dag folder. However, the issue happened when I switched to git-sync."", 'created_at': datetime.datetime(2024, 11, 21, 8, 46, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2506065908, 'issue_id': 2552582598, 'author': 'hpereira98', 'body': ""This is happening to me as well...\r\nThe DAGs were removed from the code base, but they are still visible in the UI.\r\nWe're using the User-Community Chart (https://github.com/airflow-helm/charts), with git-sync enabled.\r\nWe wanted to keep the historic metadata, but remove the DAGs from the UI... The documentation isn't being reflected in reality, as the DAGs were removed from the `DAGS_FOLDER` but they are not being marked as inactive. Also tried to mark them as inactive in the database, but they were immediately activated again.\r\n\r\nAirflow version: 2.9.1"", 'created_at': datetime.datetime(2024, 11, 28, 12, 57, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2549851903, 'issue_id': 2552582598, 'author': 'ketozhang', 'body': 'This issue also exists in Airflow 2.9.2\r\n\r\nIIRC, Airflow 2.6–2.8 had the expected behavior.', 'created_at': datetime.datetime(2024, 12, 17, 23, 12, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2553561130, 'issue_id': 2552582598, 'author': 'irakl1s', 'body': 'I’m encountering the same issue on Airflow 2.10.2. Specifically, after removing certain helper Python files and even some DAG files from my local dags/ directory (which is mounted into the Airflow containers), these files continue to appear inside the running containers. Although they’re deleted on the host machine, they still show up in the container as if they remain present and are being parsed by the scheduler or DAG processor.\r\n\r\nIt feels like there’s some form of caching or delayed sync at play. How can I ensure that once files are removed locally, they are also removed from the container’s view and no longer recognized by Airflow? Any guidance or known workarounds would be greatly appreciated.', 'created_at': datetime.datetime(2024, 12, 19, 11, 59, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2553919773, 'issue_id': 2552582598, 'author': 'potiuk', 'body': '> I’m encountering the same issue on Airflow 2.10.2. Specifically, after removing certain helper Python files and even some DAG files from my local dags/ directory (which is mounted into the Airflow containers), these files continue to appear inside the running containers. Although they’re deleted on the host machine, they still show up in the container as if they remain present and are being parsed by the scheduler or DAG processor.\r\n> \r\n> It feels like there’s some form of caching or delayed sync at play. How can I ensure that once files are removed locally, they are also removed from the container’s view and no longer recognized by Airflow? Any guidance or known workarounds would be greatly appreciated.\r\n\r\nIt\'s entirely dependent on your deployment - which is totally outside of Airlfow realm - we have a `git-sync` in the reference chart of ours, but how syncing of hte works for other charts or various deployments - it\'s not something that Airflow 2 controis and you need to look at details on how this mounting/syncing is done - because that totally depends on deployment manager.\r\n\r\nIn Airflow 3 it will change, when you are going to use DAG Bundles https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=294816356 - where syncing will be controlled inside Airflow and then it\'s more of an Airlfow problem - but until then it\'s more of how airflow is deployed and what syncing mechanims `You` implemented. We can try to help here if you describe your case, but when you - for example use 3rd-party chart like @hpereira98 -> the question is better answered and discussed there. Ig you @klisira describe in detail your deployment in detail and it\'s different than the other chart, this might be a good idea to explain all the details and maybe we can find something.\r\n\r\nThere are several things that could happen - one of them is that you have **some** filesystem caching that prevents the files from deletion when they are deleted locally. Another one is that pre-compiled bytecode files remain where they were and they are a) generated and b) not removed when git-sync swaps the directory (if you are using git-sync). In this case setting the variable: https://docs.python.org/3/using/cmdline.html#envvar-PYTHONDONTWRITEBYTECODE and cleaning those .pyc files might help. \r\n\r\nOther than that the investigation should happen in your container that is used for dag parsing. It could be that it is scheduler or if you are using standalone dag processor, it\'s the processor - then you have to exec to those containers and see if the python files (or .pyc files) are there and whether they seem to parsed by the scheduler/dag processor (you will see it in dag file processor logs). Finally (and this happened to a few of our users) - they might have some old version of scheduler/ dag file processor running - with old un-synced folders and connected to the same database - in such case such ""additional"" scheduler/dag file processor will continue scanning and adding those files even if they are removed in the ""regular"" scheduler/dag file processor.\r\n\r\nThose are all ideas/hypotheses that come to my mind that you could explore - but most of them are just wild guesses and well, hypotheses, that only you can verify.', 'created_at': datetime.datetime(2024, 12, 19, 13, 17, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2576193468, 'issue_id': 2552582598, 'author': 'el-aasi', 'body': '> I’m encountering the same issue on Airflow 2.10.2. Specifically, after removing certain helper Python files and even some DAG files from my local dags/ directory (which is mounted into the Airflow containers), these files continue to appear inside the running containers. Although they’re deleted on the host machine, they still show up in the container as if they remain present and are being parsed by the scheduler or DAG processor.\n> \n> It feels like there’s some form of caching or delayed sync at play. How can I ensure that once files are removed locally, they are also removed from the container’s view and no longer recognized by Airflow? Any guidance or known workarounds would be greatly appreciated.\n\nJust to make sure, this is not the behavior that I have encountered (described above), for the files are deleted and never appear, nevertheless the DAG still remains in the UI/is not marked in the DB.', 'created_at': datetime.datetime(2025, 1, 7, 20, 48, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2644166002, 'issue_id': 2552582598, 'author': 'TerryYin1777', 'body': ""We have a similar setup (kubernetes deployment with git-sync) and have the same issue. After some deep dive in the code base, I believe the root cause is the following:\n\nWhen git-sync resyncs, it changes the DAGs folder from:\n\nhash-123/dags/example_dag.py  →  hash-456/dags/example_dag.py\n\nDespite the symlink that points these directories to a contract directory, the [get_dag_directory](https://github.com/apache/airflow/blob/2.9.1/airflow/dag_processing/manager.py#L963) resolves this path to its canonical path, which will be different across re-syncs. This path get passed all the way to the [deactivate_deleted_dags](https://github.com/apache/airflow/blob/2.9.1/airflow/models/dag.py#L3830) function in the dag model, by which is used to mark the dag inactive and therefore hidden from the UI. The deletion is not happening since the processor_subdir value does not match the previously registered processor_subdir.\n\nNot fully sure the reason why we need to resolve the dag folder path to its canonical. I understand it's not an issue with Airflow itself and probably happens only when git-sync is used for dag deployment. But given it's a quite widely adopted combination, is it possible to add a configuration like  GET_DAG_FOLDER_RESOLVE=False so the symlink path is not resolved to its canonical path?"", 'created_at': datetime.datetime(2025, 2, 7, 21, 31, 10, tzinfo=datetime.timezone.utc)}]","shahar1 (Issue Creator) on (2024-09-27 10:53:37 UTC): Following the reports in the discussion #40331 - I've reopened it as an issue.
I didn't manage to [reproduce ](https://github.com/apache/airflow/discussions/40331#discussioncomment-10774202) it on breeze deployment.
I would like to ask from the OP (@tatyana12345) and the commentors (@cjj1120, @falukelo, @gaelxcowi) to post the exact configuration, deployment environment, Airflow version, and steps to reproduce the issue (preferrably with accompanying timestamps and images from the UI). After we manage to reproduce the issue, I'll be happy to review PRs from the community that resolve it.

gaelxcowi on (2024-09-27 20:17:18 UTC): @shahar1 Hopefully I can capture everything, if not let me know. 

Here is a minimal exmaple, hopefully I have covered enough evidence:

Disclaimer: I wasn't yet able to reproduce 100% my behaviour - but I will comeback to that later. 

So, here are the configs: 
```
[core]
dags_folder = /opt/airflow/dags/repo/
hostname_callable = airflow.utils.net.getfqdn
might_contain_dag_callable = airflow.utils.file.might_contain_dag_via_default_heuristic
default_timezone = utc
executor = KubernetesExecutor
auth_manager = airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
parallelism = 32
max_active_tasks_per_dag = 18
dags_are_paused_at_creation = True
max_active_runs_per_dag = 18
max_consecutive_failed_dag_runs_per_dag = 0
# mp_start_method =
load_examples = false
plugins_folder = /opt/airflow/plugins
execute_tasks_new_python_interpreter = False
fernet_key =<removed>
donot_pickle = True
dagbag_import_timeout = 100
dagbag_import_error_tracebacks = True
dagbag_import_error_traceback_depth = 2
dag_file_processor_timeout = 150
task_runner = StandardTaskRunner
default_impersonation =
security =
unit_test_mode = False
enable_xcom_pickling = False
allowed_deserialization_classes = airflow.*
allowed_deserialization_classes_regexp =
killed_task_cleanup_time = 60
dag_run_conf_overrides_params = True
dag_discovery_safe_mode = True
dag_ignore_file_syntax = regexp
default_task_retries = 0
default_task_retry_delay = 300
max_task_retry_delay = 86400
default_task_weight_rule = downstream
task_success_overtime = 20
default_task_execution_timeout =
min_serialized_dag_update_interval = 30
compress_serialized_dags = False
min_serialized_dag_fetch_interval = 10
max_num_rendered_ti_fields_per_task = 30
check_slas = True
xcom_backend = airflow.models.xcom.BaseXCom
lazy_load_plugins = True
lazy_discover_providers = True
hide_sensitive_var_conn_fields = True
sensitive_var_conn_names =
default_pool_task_slot_count = 128
max_map_length = 1024
daemon_umask = 0o077
# dataset_manager_class =
# dataset_manager_kwargs =
strict_dataset_uri_validation = False
database_access_isolation = False
# internal_api_url =
internal_api_secret_key = <removed>
test_connection = Disabled
max_templated_field_length = 4096
[database]
alembic_ini_file_path = alembic.ini
sql_alchemy_conn = postgresql+psycopg2://<removed>
# sql_alchemy_engine_args =
sql_engine_encoding = utf-8
# sql_engine_collation_for_ids =
sql_alchemy_pool_enabled = True
sql_alchemy_pool_size = 5
sql_alchemy_max_overflow = 10
sql_alchemy_pool_recycle = 1800
sql_alchemy_pool_pre_ping = True
sql_alchemy_schema =
# sql_alchemy_connect_args =
# sql_alchemy_session_maker =
load_default_connections = True
max_db_retries = 3
check_migrations = True
[logging]
base_log_folder = /opt/airflow/logs
remote_logging = False
remote_log_conn_id =
delete_local_logs = False
google_key_path =
remote_base_log_folder =
remote_task_handler_kwargs =
encrypt_s3_logs = False
logging_level = INFO
celery_logging_level =
fab_logging_level = WARNING
logging_config_class =
colored_console_log = True
colored_log_format = [%%(blue)s%%(asctime)s%%(reset)s] {%%(blue)s%%(filename)s:%%(reset)s%%(lineno)d} %%(log_color)s%%(levelname)s%%(reset)s - %%(log_color)s%%(message)s%%(reset)s
colored_formatter_class = airflow.utils.log.colored_log.CustomTTYColoredFormatter
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s
dag_processor_log_target = file
dag_processor_log_format = [%%(asctime)s] [SOURCE:DAG_PROCESSOR] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
log_formatter_class = airflow.utils.log.timezone_aware.TimezoneAware
secret_mask_adapter =
task_log_prefix_template =
log_filename_template = dag_id={{ ti.dag_id }}/run_id={{ ti.run_id }}/task_id={{ ti.task_id }}/{%% if ti.map_index >= 0 %%}map_index={{ ti.map_index }}/{%% endif %%}attempt={{ try_number }}.log
log_processor_filename_template = {{ filename }}.log
dag_processor_manager_log_location = /opt/airflow/logs/dag_processor_manager/dag_processor_manager.log
dag_processor_manager_log_stdout = False
task_log_reader = task
extra_logger_names =
worker_log_server_port = 8793
trigger_log_server_port = 8794
# interleave_timestamp_parser =
file_task_handler_new_folder_permissions = 0o775
file_task_handler_new_file_permissions = 0o664
celery_stdout_stderr_separation = False
enable_task_context_logger = True
color_log_error_keywords = error,exception
color_log_warning_keywords = warn
[metrics]
metrics_use_pattern_match = False
metrics_allow_list =
metrics_block_list =
statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow
stat_name_handler =
statsd_datadog_enabled = False
statsd_datadog_tags =
statsd_datadog_metrics_tags = True
# statsd_custom_client_path =
statsd_disabled_tags = job_id,run_id
statsd_influxdb_enabled = False
otel_on = False
otel_host = localhost
otel_port = 8889
otel_prefix = airflow
otel_interval_milliseconds = 60000
otel_debugging_on = False
otel_ssl_active = False
[traces]
otel_on = False
otel_host = localhost
otel_port = 8889
otel_service = Airflow
otel_debugging_on = False
otel_ssl_active = False
otel_task_log_event = False
[secrets]
backend =
backend_kwargs =
use_cache = False
cache_ttl_seconds = 900
[cli]
api_client = airflow.api.client.local_client
endpoint_url = http://localhost:8080
[debug]
fail_fast = False
[api]
enable_experimental_api = False
auth_backends = airflow.api.auth.backend.session
maximum_page_limit = 100
fallback_page_limit = 100
google_oauth2_audience =
google_key_path =
access_control_allow_headers =
access_control_allow_methods =
access_control_allow_origins =
enable_xcom_deserialize_support = False
[lineage]
backend =
[operators]
default_owner = airflow
default_deferrable = false
default_cpus = 1
default_ram = 512
default_disk = 512
default_gpus = 0
default_queue = default
allow_illegal_arguments = True
[webserver]
access_denied_message = Access is Denied
config_file = /opt/airflow/webserver_config.py
base_url = <removed>
default_ui_timezone = UTC
web_server_host = 0.0.0.0
web_server_port = 8080
web_server_ssl_cert =
web_server_ssl_key =
session_backend = database
web_server_master_timeout = 600
web_server_worker_timeout = 600
worker_refresh_batch_size = 1
worker_refresh_interval = 6000
reload_on_plugin_change = False
secret_key = THIS IS UNSAFE!
workers = 4
worker_class = sync
access_logfile = -
error_logfile = -
access_logformat =
expose_config = non-sensitive-only
expose_hostname = False
expose_stacktrace = False
dag_default_view = grid
dag_orientation = LR
grid_view_sorting_order = topological
log_fetch_timeout_sec = 5
log_fetch_delay_sec = 2
log_auto_tailing_offset = 30
log_animation_speed = 1000
hide_paused_dags_by_default = False
page_size = 100
navbar_color = #fff
navbar_text_color = #51504f
navbar_hover_color = #eee
navbar_text_hover_color = #51504f
navbar_logo_text_color = #51504f
default_dag_run_display_number = 25
enable_proxy_fix = True
proxy_fix_x_for = 1
proxy_fix_x_proto = 1
proxy_fix_x_host = 1
proxy_fix_x_port = 1
proxy_fix_x_prefix = 1
cookie_secure = False
cookie_samesite = Lax
default_wrap = False
x_frame_enabled = True
# analytics_tool =
# analytics_id =
# analytics_url =
show_recent_stats_for_completed_runs = True
session_lifetime_minutes = 43200
# instance_name =
instance_name_has_markup = False
auto_refresh_interval = 3
warn_deployment_exposure = False
# audit_view_excluded_events =
# audit_view_included_events =
enable_swagger_ui = True
run_internal_api = False
caching_hash_method = md5
show_trigger_form_if_no_params = False
num_recent_configurations_for_trigger = 5
allow_raw_html_descriptions = False
allowed_payload_size = 1.0
require_confirmation_dag_change = False
[email]
email_backend = airflow.utils.email.send_email_smtp
email_conn_id = smtp_default
default_email_on_retry = True
default_email_on_failure = True
# subject_template =
# html_content_template =
# from_email =
ssl_context = default
[smtp]
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
# smtp_user =
# smtp_password =
smtp_port = 25
smtp_mail_from = airflow@example.com
smtp_timeout = 30
smtp_retry_limit = 5
[sentry]
sentry_on = false
sentry_dsn =
# before_send =
[scheduler]
job_heartbeat_sec = 5
scheduler_heartbeat_sec = 5
local_task_job_heartbeat_sec = 0
num_runs = -1
scheduler_idle_sleep_time = 1
min_file_process_interval = 30
parsing_cleanup_interval = 30
stale_dag_threshold = 50
dag_dir_list_interval = 60
print_stats_interval = 30
pool_metrics_interval = 5.0
scheduler_health_check_threshold = 30
enable_health_check = False
scheduler_health_check_server_host = 0.0.0.0
scheduler_health_check_server_port = 8974
orphaned_tasks_check_interval = 300.0
child_process_log_directory = /opt/airflow/logs/scheduler
scheduler_zombie_task_threshold = 300
zombie_detection_interval = 10.0
catchup_by_default = True
ignore_first_depends_on_past_by_default = True
max_tis_per_query = 16
use_row_level_locking = True
max_dagruns_to_create_per_loop = 10
max_dagruns_per_loop_to_schedule = 20
schedule_after_task_execution = True
parsing_pre_import_modules = True
parsing_processes = 2
file_parsing_sort_mode = modified_time
standalone_dag_processor = False
max_callbacks_per_loop = 20
dag_stale_not_seen_duration = 600
use_job_schedule = True
allow_trigger_in_future = False
trigger_timeout_check_interval = 15
task_queued_timeout = 600.0
task_queued_timeout_check_interval = 120.0
allowed_run_id_pattern = ^[A-Za-z0-9_.~:+-]+$
create_cron_data_intervals = True
[triggerer]
default_capacity = 1000
job_heartbeat_sec = 5
triggerer_health_check_threshold = 30
[kerberos]
ccache = /tmp/airflow_krb5_ccache
principal = airflow
reinit_frequency = 3600
kinit_path = kinit
keytab = airflow.keytab
forwardable = True
include_ip = True
[sensors]
default_timeout = 604800
[usage_data_collection]
enabled = True
[aws]
# session_factory =
cloudwatch_task_handler_json_serializer = airflow.providers.amazon.aws.log.cloudwatch_task_handler.json_serialize_legacy
[aws_batch_executor]
conn_id = aws_default
# region_name =
max_submit_job_attempts = 3
check_health_on_startup = True
# job_name =
# job_queue =
# job_definition =
# submit_job_kwargs =
[aws_ecs_executor]
conn_id = aws_default
# region_name =
assign_public_ip = False
# cluster =
# capacity_provider_strategy =
# container_name =
# launch_type =
platform_version = LATEST
# security_groups =
# subnets =
# task_definition =
max_run_task_attempts = 3
# run_task_kwargs =
check_health_on_startup = True
[aws_auth_manager]
enable = False
conn_id = aws_default
# region_name =
# saml_metadata_url =
# avp_policy_store_id =
[celery_kubernetes_executor]
kubernetes_queue = kubernetes
[celery]
celery_app_name = airflow.providers.celery.executors.celery_executor
worker_concurrency = 16
# worker_autoscale =
worker_prefetch_multiplier = 1
worker_enable_remote_control = true
broker_url = redis://redis:6379/0
# result_backend =
result_backend_sqlalchemy_engine_options =
flower_host = 0.0.0.0
flower_url_prefix =
flower_port = 5555
flower_basic_auth =
sync_parallelism = 0
celery_config_options = airflow.providers.celery.executors.default_celery.DEFAULT_CELERY_CONFIG
ssl_active = False
ssl_key =
ssl_cert =
ssl_cacert =
pool = prefork
operation_timeout = 1.0
task_acks_late = True
task_track_started = True
task_publish_max_retries = 3
worker_precheck = False
[celery_broker_transport_options]
# visibility_timeout =
# sentinel_kwargs =
[local_kubernetes_executor]
kubernetes_queue = kubernetes
[kubernetes_executor]
api_client_retry_configuration =
logs_task_metadata = False
pod_template_file = /opt/airflow/pod_templates/pod_template.yaml
worker_container_repository = apache/airflow
worker_container_tag = 2.10.0-python3.11
namespace = airflow-alpha
delete_worker_pods = True
delete_worker_pods_on_failure = True
worker_pod_pending_fatal_container_state_reasons = CreateContainerConfigError,ErrImagePull,CreateContainerError,ImageInspectError, InvalidImageName
worker_pods_creation_batch_size = 10
multi_namespace_mode = False
multi_namespace_mode_namespace_list =
in_cluster = True
# cluster_context =
# config_file =
kube_client_request_args = {""_request_timeout"": [240, 240]}
delete_option_kwargs =
enable_tcp_keepalive = True
tcp_keep_idle = 120
tcp_keep_intvl = 30
tcp_keep_cnt = 6
verify_ssl = True
worker_pods_queued_check_interval = 60
ssl_ca_cert =
task_publish_max_retries = 0
[common.io]
xcom_objectstorage_path =
xcom_objectstorage_threshold = -1
xcom_objectstorage_compression =
[elasticsearch]
host =
log_id_template = {dag_id}-{task_id}-{run_id}-{map_index}-{try_number}
end_of_log_mark = end_of_log
frontend =
write_stdout = False
json_format = False
json_fields = asctime, filename, lineno, levelname, message
host_field = host
offset_field = offset
index_patterns = _all
index_patterns_callable =
[elasticsearch_configs]
http_compress = False
verify_certs = True
[fab]
auth_rate_limited = True
auth_rate_limit = 5 per 40 second
update_fab_perms = True
[imap]
# ssl_context =
[azure_remote_logging]
remote_wasb_log_container = airflow-logs
[openlineage]
disabled = False
disabled_for_operators =
selective_enable = False
# namespace =
# extractors =
custom_run_facets =
config_path =
transport =
disable_source_code = False
dag_state_change_process_pool_size = 1
execution_timeout = 10
include_full_task_info = False
[smtp_provider]
# ssl_context =
# templated_email_subject_path =
# templated_html_content_path =
```

tried with airflow 2.10.0 and 2.10.2 (but have been observing the issues since 2.8.x).
dags are kept in a git repository (Azure Repos)
deployed on a k8s cluster (AKS) using the community helm chart - not sure if you have all the infor from configs or something more is needed from values as well? 

so here is the image of the commit - delete a dag at around 21:01:
![Screenshot_1](https://github.com/user-attachments/assets/03bec17a-4554-49ab-add2-40b0cef4ac86)

here is the log from git sync: 

![Screenshot_3](https://github.com/user-attachments/assets/6b9c1d74-d0e9-4205-bb5b-5b606a3badf7)

here is the ui at around 21:02- still present: 
![Screenshot_2](https://github.com/user-attachments/assets/6a5c7824-97d3-4a21-bb5e-e853375a6cf4)


here is the dag folder (scheduler) 21:05: 

![Screenshot_4](https://github.com/user-attachments/assets/8ab8164c-94a6-424e-945a-e2e91a939855)


here is dag ui and dag table 21:53:

![Screenshot_6](https://github.com/user-attachments/assets/a2b00dec-cc8b-4081-95f9-76b0fd2ad61a)
![Screenshot_7](https://github.com/user-attachments/assets/b6c33006-baef-4147-8c74-c4a91cbadc00)

here are the logs after ~ 1h from scheduler when trying to run the dag (after restarting the scheduler: 

```
/home/airflow/.local/lib/python3.11/site-packages/airflow/plugins_manager.py:30 DeprecationWarning: 'cgitb' is deprecated and slated for removal in Python 3.13
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2024-09-27T20:08:09.049+0000] {_client.py:1026} INFO - HTTP Request: GET https://apacheairflow.gateway.scarf.sh/scheduler?version=2.10.0&python_version=3.11.9&platform=Linux&arch=x86_64&database=postgresql&db_version=14.9&executor=KubernetesExecutor ""HTTP/1.1 200 OK""
/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py:143 FutureWarning: The config section [kubernetes] has been renamed to [kubernetes_executor]. Please update your `conf.get*` call to use the new name
[2024-09-27T20:08:10.788+0000] {executor_loader.py:254} INFO - Loaded executor: KubernetesExecutor
[2024-09-27T20:08:10.901+0000] {scheduler_job_runner.py:935} INFO - Starting the scheduler
[2024-09-27T20:08:10.901+0000] {scheduler_job_runner.py:942} INFO - Processing each file at most -1 times
[2024-09-27T20:08:10.902+0000] {kubernetes_executor.py:287} INFO - Start Kubernetes executor
[2024-09-27T20:08:10.946+0000] {kubernetes_executor_utils.py:140} INFO - Event: and now my watch begins starting at resource_version: 0
[2024-09-27T20:08:10.990+0000] {kubernetes_executor.py:208} INFO - Found 0 queued task instances
[2024-09-27T20:08:10.999+0000] {manager.py:174} INFO - Launched DagFileProcessorManager with pid: 89
[2024-09-27T20:08:11.002+0000] {scheduler_job_runner.py:1843} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-09-27T20:08:11.011+0000] {settings.py:63} INFO - Configured default timezone UTC
[2024-09-27T20:08:21.101+0000] {scheduler_job_runner.py:423} INFO - 1 tasks up for execution:
        <TaskInstance: example_dag_to_be_deleted.task-kubernetes manual__2024-09-27T20:08:20.051996+00:00 [scheduled]>
[2024-09-27T20:08:21.102+0000] {scheduler_job_runner.py:495} INFO - DAG example_dag_to_be_deleted has 0/18 running and queued tasks
[2024-09-27T20:08:21.102+0000] {scheduler_job_runner.py:634} INFO - Setting the following tasks to queued state:
        <TaskInstance: example_dag_to_be_deleted.task-kubernetes manual__2024-09-27T20:08:20.051996+00:00 [scheduled]>
[2024-09-27T20:08:21.105+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: example_dag_to_be_deleted.task-kubernetes manual__2024-09-27T20:08:20.051996+00:00 [scheduled]>] for executor: KubernetesExecutor(parallelism=32)
[2024-09-27T20:08:21.106+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='example_dag_to_be_deleted', task_id='task-kubernetes', run_id='manual__2024-09-27T20:08:20.051996+00:00', try_number=1, map_index=-1) to KubernetesExecutor with priority 1 and queue kubernetes
[2024-09-27T20:08:21.106+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', 'example_dag_to_be_deleted', 'task-kubernetes', 'manual__2024-09-27T20:08:20.051996+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_dag_to_be_deleted.py']
[2024-09-27T20:08:21.109+0000] {kubernetes_executor.py:326} INFO - Add task TaskInstanceKey(dag_id='example_dag_to_be_deleted', task_id='task-kubernetes', run_id='manual__2024-09-27T20:08:20.051996+00:00', try_number=1, map_index=-1) with command ['airflow', 'tasks', 'run', 'example_dag_to_be_deleted', 'task-kubernetes', 'manual__2024-09-27T20:08:20.051996+00:00', '--local', '--subdir', 'DAGS_FOLDER/example_dag_to_be_deleted.py']
[2024-09-27T20:08:21.145+0000] {kubernetes_executor_utils.py:426} INFO - Creating kubernetes pod for job is TaskInstanceKey(dag_id='example_dag_to_be_deleted', task_id='task-kubernetes', run_id='manual__2024-09-27T20:08:20.051996+00:00', try_number=1, map_index=-1), with pod name example-dag-to-be-deleted-task-kubernetes-tqqfawxa, annotations: <omitted>
[2024-09-27T20:08:21.199+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state queued for task instance TaskInstanceKey(dag_id='example_dag_to_be_deleted', task_id='task-kubernetes', run_id='manual__2024-09-27T20:08:20.051996+00:00', try_number=1, map_index=-1)
[2024-09-27T20:08:21.206+0000] {kubernetes_executor_utils.py:265} INFO - Event: example-dag-to-be-deleted-task-kubernetes-tqqfawxa Pending, annotations: <omitted>
[2024-09-27T20:08:21.211+0000] {scheduler_job_runner.py:791} INFO - Setting external_id for <TaskInstance: example_dag_to_be_deleted.task-kubernetes manual__2024-09-27T20:08:20.051996+00:00 [queued]> to 10
[2024-09-27T20:08:21.215+0000] {kubernetes_executor_utils.py:265} INFO - Event: example-dag-to-be-deleted-task-kubernetes-tqqfawxa Pending, annotations: <omitted>
[2024-09-27T20:08:21.234+0000] {kubernetes_executor_utils.py:265} INFO - Event: example-dag-to-be-deleted-task-kubernetes-tqqfawxa Pending, annotations: <omitted>
[2024-09-27T20:08:22.202+0000] {kubernetes_executor_utils.py:265} INFO - Event: example-dag-to-be-deleted-task-kubernetes-tqqfawxa Pending, annotations: <omitted>
[2024-09-27T20:08:30.227+0000] {kubernetes_executor_utils.py:265} INFO - Event: example-dag-to-be-deleted-task-kubernetes-tqqfawxa Pending, annotations: <omitted>
[2024-09-27T20:08:31.230+0000] {kubernetes_executor_utils.py:299} INFO - Event: example-dag-to-be-deleted-task-kubernetes-tqqfawxa is Running, annotations: <omitted>
[2024-09-27T20:08:47.288+0000] {kubernetes_executor_utils.py:299} INFO - Event: example-dag-to-be-deleted-task-kubernetes-tqqfawxa is Running, annotations: <omitted>
[2024-09-27T20:08:48.480+0000] {kubernetes_executor_utils.py:269} ERROR - Event: example-dag-to-be-deleted-task-kubernetes-tqqfawxa Failed, annotations: <omitted>
[2024-09-27T20:08:48.764+0000] {kubernetes_executor.py:370} INFO - Changing state of (TaskInstanceKey(dag_id='example_dag_to_be_deleted', task_id='task-kubernetes', run_id='manual__2024-09-27T20:08:20.051996+00:00', try_number=1, map_index=-1), <TaskInstanceState.FAILED: 'failed'>, 'example-dag-to-be-deleted-task-kubernetes-tqqfawxa', 'airflow-alpha', '328757893') to failed
[2024-09-27T20:08:48.795+0000] {kubernetes_executor_utils.py:269} ERROR - Event: example-dag-to-be-deleted-task-kubernetes-tqqfawxa Failed, annotations: <omitted>
[2024-09-27T20:08:48.802+0000] {kubernetes_executor.py:475} INFO - Deleted pod associated with the TI TaskInstanceKey(dag_id='example_dag_to_be_deleted', task_id='task-kubernetes', run_id='manual__2024-09-27T20:08:20.051996+00:00', try_number=1, map_index=-1). Pod name: example-dag-to-be-deleted-task-kubernetes-tqqfawxa. Namespace: airflow-alpha
[2024-09-27T20:08:48.803+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='example_dag_to_be_deleted', task_id='task-kubernetes', run_id='manual__2024-09-27T20:08:20.051996+00:00', try_number=1, map_index=-1)
[2024-09-27T20:08:48.811+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_dag_to_be_deleted, task_id=task-kubernetes, run_id=manual__2024-09-27T20:08:20.051996+00:00, map_index=-1, run_start_date=None, run_end_date=None, run_duration=None, state=queued, executor=KubernetesExecutor(parallelism=32), executor_state=failed, try_number=1, max_tries=0, job_id=None, pool=default_pool, queue=kubernetes, priority_weight=1, operator=PythonOperator, queued_dttm=2024-09-27 20:08:21.103374+00:00, queued_by_job_id=10, pid=None
[2024-09-27T20:08:48.812+0000] {scheduler_job_runner.py:907} ERROR - Executor KubernetesExecutor(parallelism=32) reported that the task instance <TaskInstance: example_dag_to_be_deleted.task-kubernetes manual__2024-09-27T20:08:20.051996+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally
[2024-09-27T20:08:48.822+0000] {taskinstance.py:3303} ERROR - Executor KubernetesExecutor(parallelism=32) reported that the task instance <TaskInstance: example_dag_to_be_deleted.task-kubernetes manual__2024-09-27T20:08:20.051996+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally
[2024-09-27T20:08:48.843+0000] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=example_dag_to_be_deleted, task_id=task-kubernetes, run_id=manual__2024-09-27T20:08:20.051996+00:00, execution_date=20240927T200820, start_date=, end_date=20240927T200848
[2024-09-27T20:08:49.127+0000] {kubernetes_executor_utils.py:269} ERROR - Event: example-dag-to-be-deleted-task-kubernetes-tqqfawxa Failed, annotations: <omitted>
[2024-09-27T20:08:49.955+0000] {dagrun.py:823} ERROR - Marking run <DagRun example_dag_to_be_deleted @ 2024-09-27 20:08:20.051996+00:00: manual__2024-09-27T20:08:20.051996+00:00, state:running, queued_at: 2024-09-27 20:08:20.084580+00:00. externally triggered: True> failed
[2024-09-27T20:08:49.956+0000] {dagrun.py:905} INFO - DagRun Finished: dag_id=example_dag_to_be_deleted, execution_date=2024-09-27 20:08:20.051996+00:00, run_id=manual__2024-09-27T20:08:20.051996+00:00, run_start_date=2024-09-27 20:08:20.303660+00:00, run_end_date=2024-09-27 20:08:49.956078+00:00, run_duration=29.652418, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-09-26 00:00:00+00:00, data_interval_end=2024-09-27 00:00:00+00:00, dag_hash=1a61ee6a34486df6240d5abb977f4507
[2024-09-27T20:08:49.982+0000] {kubernetes_executor.py:370} INFO - Changing state of (TaskInstanceKey(dag_id='example_dag_to_be_deleted', task_id='task-kubernetes', run_id='manual__2024-09-27T20:08:20.051996+00:00', try_number=1, map_index=-1), <TaskInstanceState.FAILED: 'failed'>, 'example-dag-to-be-deleted-task-kubernetes-tqqfawxa', 'airflow-alpha', '328757894') to failed
[2024-09-27T20:08:49.991+0000] {kubernetes_executor.py:475} INFO - Deleted pod associated with the TI TaskInstanceKey(dag_id='example_dag_to_be_deleted', task_id='task-kubernetes', run_id='manual__2024-09-27T20:08:20.051996+00:00', try_number=1, map_index=-1). Pod name: example-dag-to-be-deleted-task-kubernetes-tqqfawxa. Namespace: airflow-alpha
[2024-09-27T20:08:49.993+0000] {kubernetes_executor.py:370} INFO - Changing state of (TaskInstanceKey(dag_id='example_dag_to_be_deleted', task_id='task-kubernetes', run_id='manual__2024-09-27T20:08:20.051996+00:00', try_number=1, map_index=-1), <TaskInstanceState.FAILED: 'failed'>, 'example-dag-to-be-deleted-task-kubernetes-tqqfawxa', 'airflow-alpha', '328757895') to failed
[2024-09-27T20:08:50.000+0000] {kubernetes_executor.py:475} INFO - Deleted pod associated with the TI TaskInstanceKey(dag_id='example_dag_to_be_deleted', task_id='task-kubernetes', run_id='manual__2024-09-27T20:08:20.051996+00:00', try_number=1, map_index=-1). Pod name: example-dag-to-be-deleted-task-kubernetes-tqqfawxa. Namespace: airflow-alpha
[2024-09-27T20:08:50.002+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='example_dag_to_be_deleted', task_id='task-kubernetes', run_id='manual__2024-09-27T20:08:20.051996+00:00', try_number=1, map_index=-1)
[2024-09-27T20:08:50.010+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=example_dag_to_be_deleted, task_id=task-kubernetes, run_id=manual__2024-09-27T20:08:20.051996+00:00, map_index=-1, run_start_date=None, run_end_date=2024-09-27 20:08:48.831070+00:00, run_duration=None, state=failed, executor=KubernetesExecutor(parallelism=32), executor_state=failed, try_number=1, max_tries=0, job_id=None, pool=default_pool, queue=kubernetes, priority_weight=1, operator=PythonOperator, queued_dttm=2024-09-27 20:08:21.103374+00:00, queued_by_job_id=10, pid=None
[2024-09-27T20:09:11.082+0000] {kubernetes_executor.py:208} INFO - Found 0 queued task instances
[2024-09-27T20:09:19.134+0000] {kubernetes_executor_utils.py:101} INFO - Kubernetes watch timed out waiting for events. Restarting watch.
[2024-09-27T20:09:20.136+0000] {kubernetes_executor_utils.py:140} INFO - Event: and now my watch begins starting at resource_version: 0
```

the difference from my original setup to this is that in my original setup I run a KubernetesPodOperator - that similar to the empty operator actually runs successfully without any issues even tho the dag is not there. 

In the new setup I have an EmptyOperator and a PythonOperator - the empty operator runs okay, but the python operator fails saying that the dag cannot be found. 

In my original setup these are the logs I get, even tho the dag is not there: 

```
/home/airflow/.local/lib/python3.11/site-packages/airflow/plugins_manager.py:30 DeprecationWarning: 'cgitb' is deprecated and slated for removal in Python 3.13
[2024-09-27T19:09:22.402+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/repo/A235712_gea_test.py
[2024-09-27T19:09:22.404+0000] {cli.py:243} WARNING - Dag 'Example_dag' not found in path /opt/airflow/dags/repo/Example_dag.py; trying path /opt/airflow/dags/repo/
[2024-09-27T19:09:22.404+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/repo/
```

github-actions[bot] on (2024-10-12 00:14:32 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

cjj1120 on (2024-10-21 03:21:12 UTC): below is the testing that I did locally, lmk if u need more info @shahar1 
*I'm using community Airflow Helm chart* 

## How to reproduce 
1. Deploy Airflow with below helm chart. 
2. Remove DAG file and push the changes. 
3. Verify that latest changes is synced.

Observation: 
The DAG is not deactivated, it has to be paused/ deleted manually from the UI. 

## Operating System 
Mac OS

## Version of Apache Airflow Providers 
apache-airflow==2.9.3
apache-airflow-providers-postgres==5.11.3
apache-airflow-providers-cncf-kubernetes==8.3.4

## Deployment 
 [community helm chart](https://github.com/airflow-helm/charts/tree/main). 
Testing is done locally with cluster set up with `kind`. 

## Deployment details 

Base Docker image: `apache/airflow:slim-2.9.3-python3.11`
helm chart value file: 
```yaml
########################################
## CONFIG | Airflow Configs
########################################
airflow:
  ## if we use legacy 1.10 airflow commands
  legacyCommands: false

  ## configs for the airflow container image
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/configuration/airflow-version.md
  image:
    # repository: localhost:5000/airflow-custom
    # tag: latest
    repository: airflow-custom
    tag: latest

  ## the airflow executor type to use
  executor: KubernetesExecutor

  ## the fernet encryption key (sets `AIRFLOW__CORE__FERNET_KEY`)
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/security/set-fernet-key.md
  ## [WARNING] change from default value to ensure security
  fernetKey: """"

  ## the secret_key for flask (sets `AIRFLOW__WEBSERVER__SECRET_KEY`)
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/security/set-webserver-secret-key.md
  ## [WARNING] change from default value to ensure security
  webserverSecretKey: ""THIS IS UNSAFE!""

  ## environment variables for airflow configs
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/configuration/airflow-configs.md
  config:
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: ""False""
    AIRFLOW__CORE__LOAD_EXAMPLES: ""False""
    AIRFLOW__CORE__DEFAULT_TIMEZONE: ""utc""    
    # AIRFLOW__KUBERNETES_EXECUTOR__DELETE_WORKER_PODS_ON_FAILURE: ""False""
    AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: ""False""
    AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 120
    AIRFLOW__SCHEDULER__PARSING_CLEANUP_INTERVAL: 6
    AIRFLOW__SCHEDULER__STALE_DAG_THRESHOLD: 6
    AIRFLOW__SCHEDULER__TASK_QUEUED_TIMEOUT: 300
    AIRFLOW_VAR_ENVIRONMENT: ""DEV""    
    AIRFLOW__LOGGING__LOGGING_LEVEL: ""INFO""
    # AIRFLOW__LOGGING__DAG_PROCESSOR_MANAGER_LOG_STDOUT: ""True""
    # AIRFLOW__LOGGING__DAG_PROCESSOR_LOG_TARGET: ""stdout"" # default fo files in scheduler pod. 
    AIRFLOW__WEBSERVER__BASE_URL: http://localhost:8080
    AIRFLOW__WEBSERVER__DEFAULT_WRAP: True
    AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: True
    AIRFLOW__METRICS__STATSD_ON: True
    AIRFLOW__METRICS__STATSD_HOST: statsd-exporter
    AIRFLOW__METRICS__STATSD_PORT: 9125
    AIRFLOW__METRICS__STATSD_PREFIX: airflow

  ## a list of users to create
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/security/airflow-users.md
  users:
    - username: admin
      password: admin
      role: Admin
      email: admin@example.com
      firstName: admin
      lastName: admin

  ## a list airflow connections to create
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/dags/airflow-connections.md
  connections: []

  ## a list airflow variables to create
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/dags/airflow-variables.md
  variables: []

  ## a list airflow pools to create
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/dags/airflow-pools.md
  pools: []

  ## extra pip packages to install in airflow Pods
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/configuration/extra-python-packages.md
  ## [WARNING] this feature is not recommended for production use, see docs
  extraPipPackages: []

  ## extra environment variables for the airflow Pods
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/kubernetes/mount-environment-variables.md
  extraEnv: []

  ## extra VolumeMounts for the airflow Pods
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/kubernetes/mount-persistent-volumes.md
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/kubernetes/mount-files.md
  extraVolumeMounts: []
    # - name: airflow-logs-volume
    #   mountPath: /airflow-logs

  ## extra Volumes for the airflow Pods
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/kubernetes/mount-persistent-volumes.md
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/kubernetes/mount-files.md
  extraVolumes: []
    # - name: airflow-logs-volume
    #   persistentVolumeClaim:
    #     claimName: airflow-logs-claim

  ## configs generating the `pod_template.yaml` file for `AIRFLOW__KUBERNETES__POD_TEMPLATE_FILE`
  ## [NOTE] the `dags.gitSync` values will create a git-sync init-container in the pod
  ## [NOTE] the `airflow.extraPipPackages` will NOT be installed
  kubernetesPodTemplate:

    ## the full content of the pod-template file (as a string)
    ## [NOTE] all other `kubernetesPodTemplate.*` are disabled when this is set
    stringOverride: """"

    ## resource requests/limits for the Pod template ""base"" container
    ## [SPEC] https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#resourcerequirements-v1-core
    resources: {}

    ## extra pip packages to install in the Pod template
    ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/configuration/extra-python-packages.md
    ## [WARNING] this feature is not recommended for production use, see docs
    extraPipPackages: []

    ## extra VolumeMounts for the Pod template
    ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/kubernetes/mount-persistent-volumes.md
    ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/kubernetes/mount-files.md
    extraVolumeMounts: []

    ## extra Volumes for the Pod template
    ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/kubernetes/mount-persistent-volumes.md
    ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/kubernetes/mount-files.md
    extraVolumes: []

###################################
## COMPONENT | Airflow Scheduler
###################################
scheduler:
  ## the number of scheduler Pods to run
  replicas: 1

  ## resource requests/limits for the scheduler Pods
  ## [SPEC] https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#resourcerequirements-v1-core
  resources: {}

  ## configs for the log-cleanup sidecar of the scheduler
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/monitoring/log-cleanup.md
  logCleanup:
    enabled: false
    retentionMinutes: 21600

  ## configs for the scheduler Pods' liveness probe
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/monitoring/scheduler-liveness-probe.md
  livenessProbe:
    enabled: true

    ## configs for an additional check that ensures tasks are being created by the scheduler
    ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/monitoring/scheduler-liveness-probe.md
    taskCreationCheck:
      enabled: false
      thresholdSeconds: 300
      schedulerAgeBeforeCheck: 180

###################################
## COMPONENT | Airflow Webserver
###################################
web:
  ## the number of web Pods to run
  replicas: 1

  ## resource requests/limits for the web Pods
  ## [SPEC] https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#resourcerequirements-v1-core
  resources: {}

  ## configs for the Service of the web Pods
  service:
    type: ClusterIP
    externalPort: 8080


  # DEFAULT 
  ## configs generating the `webserver_config.py` file
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/configuration/airflow-configs.md#webserver_configpy
  # webserverConfig:
  #   ## the full content of the `webserver_config.py` file (as a string)
  #   stringOverride: |
  #     from airflow import configuration as conf
  #     from flask_appbuilder.security.manager import AUTH_DB
      
  #     # the SQLAlchemy connection string
  #     SQLALCHEMY_DATABASE_URI = conf.get(""core"", ""SQL_ALCHEMY_CONN"")
      
  #     # use embedded DB for auth
  #     AUTH_TYPE = AUTH_DB

  # OVERWRITE   
  webserverConfig:
    ## if the `webserver_config.py` file is mounted
    ## - set to false if you wish to mount your own `webserver_config.py` file
    ##
    enabled: true

    ## the full content of the `webserver_config.py` file (as a string)
    ## - docs for Flask-AppBuilder security configs:
    ##   https://flask-appbuilder.readthedocs.io/en/latest/security.html
    ##
    ## ____ EXAMPLE _______________
    ##   stringOverride: |
    ##     from airflow import configuration as conf
    ##     from flask_appbuilder.security.manager import AUTH_DB
    ##
    ##     # the SQLAlchemy connection string
    ##     SQLALCHEMY_DATABASE_URI = conf.get('core', 'SQL_ALCHEMY_CONN')
    ##
    ##     # use embedded DB for auth
    ##     AUTH_TYPE = AUTH_DB
    ##
    stringOverride: """"

    ## the name of a Secret containing a `webserver_config.py` key
    ##
    existingSecret: """"

  ## the number of web Pods to run
  ## - if you set this >1 we recommend defining a `web.podDisruptionBudget`
  ##

###################################
## COMPONENT | Airflow Workers
###################################
workers:
  ## if the airflow workers StatefulSet should be deployed
  enabled: false

###################################
## COMPONENT | Triggerer
###################################
triggerer:
  ## if the airflow triggerer should be deployed
  enabled: true

  ## the number of triggerer Pods to run
  replicas: 1

  ## resource requests/limits for the triggerer Pods
  ## [SPEC] https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#resourcerequirements-v1-core
  resources: {}

  ## maximum number of triggers each triggerer will run at once (sets `AIRFLOW__TRIGGERER__DEFAULT_CAPACITY`)
  capacity: 1000

###################################
## COMPONENT | Flower
###################################
flower:
  ## if the airflow flower UI should be deployed
  enabled: false

###################################
## CONFIG | Airflow Logs
###################################
logs:
  ## the airflow logs folder
  path: /opt/airflow/logs

  ## configs for the logs PVC
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/monitoring/log-persistence.md
  persistence:
    enabled: false
    # ## the name of your existing PersistentVolumeClaim
    # existingClaim: airflow-logs-claim
    # ## WARNING: as multiple pods will write logs, this MUST be ReadWriteMany
    # accessMode: ReadWriteMany

###################################
## CONFIG | Airflow DAGs
###################################
dags:
  ## the airflow dags folder
  path: /opt/airflow/dags

  ## configs for the dags PVC
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/dags/load-dag-definitions.md
  persistence:
    enabled: false

  ## configs for the git-sync sidecar
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/dags/load-dag-definitions.md
  gitSync:
    enabled: true
    image:
      repository: registry.k8s.io/git-sync/git-sync
      tag: v3.6.9
      pullPolicy: IfNotPresent
      uid: 65533
      gid: 65533
    # SSH
    repo: ""#REMOVED""
    repoSubPath: ""dags""
    sshSecret: ""#REMOVED""
    sshSecretKey: #REMOVED    
    branch: ""main""
    revision: ""HEAD""
    # depth: 1    
    maxFailures: 5
    syncWait: 15
    # ""known_hosts"" verification can be disabled by setting to ... 
    sshKnownHosts: """"         
    # # HTTP 
    # repo: https://github.com/cjj1120/local_k8s_airflow_dags.git
    # ## the name of a pre-created Secret with git http credentials
    # ##
    # httpSecret: ""#REMOVED""

    # ## the key in `dags.gitSync.httpSecret` with your git username
    # ##
    # httpSecretUsernameKey: #REMOVED

    # ## the key in `dags.gitSync.httpSecret` with your git password/token
    # ##
    # httpSecretPasswordKey: #REMOVED

###################################
## CONFIG | Kubernetes Ingress
###################################
ingress:
  ## if we should deploy Ingress resources
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/kubernetes/ingress.md
  enabled: false

###################################
## CONFIG | Kubernetes ServiceAccount
###################################
serviceAccount:

  ## if a Kubernetes ServiceAccount is created
  create: true

  ## the name of the ServiceAccount
  name: """"

  ## annotations for the ServiceAccount
  annotations: {}

###################################
## CONFIG | Kubernetes Extra Manifests
###################################

## a list of extra Kubernetes manifests that will be deployed alongside the chart
## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/kubernetes/extra-manifests.md
extraManifests: []
# extraManifests:
#   - |
#     apiVersion: apps/v1
#     kind: Deployment
#     metadata:
#       name: {{ include ""airflow.fullname"" . }}-busybox
#       labels:
#         app: {{ include ""airflow.labels.app"" . }}
#         component: busybox
#         chart: {{ include ""airflow.labels.chart"" . }}
#         release: {{ .Release.Name }}
#         heritage: {{ .Release.Service }}
#     spec:
#       replicas: 1
#       selector:
#         matchLabels:
#           app: {{ include ""airflow.labels.app"" . }}
#           component: busybox
#           release: {{ .Release.Name }}
#       template:
#         metadata:
#           labels:
#             app: {{ include ""airflow.labels.app"" . }}
#             component: busybox
#             release: {{ .Release.Name }}
#         spec:
#           containers:
#             - name: busybox
#               image: busybox:1.35
#               command:
#                 - ""/bin/sh""
#                 - ""-c""
#               args:
#                 - |
#                   ## to break the infinite loop when we receive SIGTERM
#                   trap ""exit 0"" SIGTERM;
#                   ## keep the container running (so people can `kubectl exec -it` into it)
#                   while true; do
#                     echo ""I am alive..."";
#                     sleep 30;
#                   done

###################################
## DATABASE | PgBouncer
###################################
pgbouncer:
  ## if the pgbouncer Deployment is created
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/database/pgbouncer.md
  enabled: false

  ## resource requests/limits for the pgbouncer Pods
  ## [SPEC] https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#resourcerequirements-v1-core
  resources: {}

  ## sets pgbouncer config: `auth_type`
  authType: md5

###################################
## DATABASE | Embedded Postgres
###################################
postgresql:
  ## if the `stable/postgresql` chart is used
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/database/embedded-database.md
  ## [WARNING] the embedded Postgres is NOT SUITABLE for production deployments of Airflow
  ## [WARNING] consider using an external database with `externalDatabase.*`
  enabled: false
  ## configs for the PVC of postgresql
  persistence:
    enabled: true
      
    storageClass: """"
    size: 8Gi

###################################
## DATABASE | External Database
###################################
externalDatabase:
  type: postgres
  host: #REMOVED
  port: 5432
  database: airflow
  userSecret: #REMOVED
  userSecretKey: #REMOVED
  passwordSecret: #REMOVED
  passwordSecretKey: #REMOVED


###################################
## DATABASE | Embedded Redis
###################################
redis:
  ## if the `stable/redis` chart is used
  enabled: false

###################################
## DATABASE | External Redis
###################################
externalRedis:
  ## the host of the external redis
  ## [FAQ] https://github.com/airflow-helm/charts/blob/main/charts/airflow/docs/faq/database/external-redis.md
  host: localhost`

```

ahipp13 on (2024-10-24 18:39:45 UTC): I can comment as another person that has this behavior in their Airflow instances. I am running Airflow's Helm Chart along with git sync and Kubernetes. I have seen this in all the Airflow versions my team has been on. We currently have been manually clicking the delete button in the UI. Would love to have a feature where we could choose if we want deactivated DAGs showing up in the UI or not.

gordonwells on (2024-11-12 21:19:32 UTC): I don't have time to check the source code to understand why, but enabling the standalone DAG processor in the helm chart values solved this issue for me:
```yaml
dagProcessor:
  enabled: true
```

I'm using Airflow 2.9.3 and haven't tested this for later versions.

zhaozhaoqueue on (2024-11-21 08:46:43 UTC): Thank you, @gordonwells, I'm using 2.9.2 Airflow, and your solution works!

The weird thing is that everything worked as expected when I used PVC for the dag folder. However, the issue happened when I switched to git-sync.

hpereira98 on (2024-11-28 12:57:48 UTC): This is happening to me as well...
The DAGs were removed from the code base, but they are still visible in the UI.
We're using the User-Community Chart (https://github.com/airflow-helm/charts), with git-sync enabled.
We wanted to keep the historic metadata, but remove the DAGs from the UI... The documentation isn't being reflected in reality, as the DAGs were removed from the `DAGS_FOLDER` but they are not being marked as inactive. Also tried to mark them as inactive in the database, but they were immediately activated again.

Airflow version: 2.9.1

ketozhang on (2024-12-17 23:12:21 UTC): This issue also exists in Airflow 2.9.2

IIRC, Airflow 2.6–2.8 had the expected behavior.

irakl1s on (2024-12-19 11:59:37 UTC): I’m encountering the same issue on Airflow 2.10.2. Specifically, after removing certain helper Python files and even some DAG files from my local dags/ directory (which is mounted into the Airflow containers), these files continue to appear inside the running containers. Although they’re deleted on the host machine, they still show up in the container as if they remain present and are being parsed by the scheduler or DAG processor.

It feels like there’s some form of caching or delayed sync at play. How can I ensure that once files are removed locally, they are also removed from the container’s view and no longer recognized by Airflow? Any guidance or known workarounds would be greatly appreciated.

potiuk on (2024-12-19 13:17:55 UTC): It's entirely dependent on your deployment - which is totally outside of Airlfow realm - we have a `git-sync` in the reference chart of ours, but how syncing of hte works for other charts or various deployments - it's not something that Airflow 2 controis and you need to look at details on how this mounting/syncing is done - because that totally depends on deployment manager.

In Airflow 3 it will change, when you are going to use DAG Bundles https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=294816356 - where syncing will be controlled inside Airflow and then it's more of an Airlfow problem - but until then it's more of how airflow is deployed and what syncing mechanims `You` implemented. We can try to help here if you describe your case, but when you - for example use 3rd-party chart like @hpereira98 -> the question is better answered and discussed there. Ig you @klisira describe in detail your deployment in detail and it's different than the other chart, this might be a good idea to explain all the details and maybe we can find something.

There are several things that could happen - one of them is that you have **some** filesystem caching that prevents the files from deletion when they are deleted locally. Another one is that pre-compiled bytecode files remain where they were and they are a) generated and b) not removed when git-sync swaps the directory (if you are using git-sync). In this case setting the variable: https://docs.python.org/3/using/cmdline.html#envvar-PYTHONDONTWRITEBYTECODE and cleaning those .pyc files might help. 

Other than that the investigation should happen in your container that is used for dag parsing. It could be that it is scheduler or if you are using standalone dag processor, it's the processor - then you have to exec to those containers and see if the python files (or .pyc files) are there and whether they seem to parsed by the scheduler/dag processor (you will see it in dag file processor logs). Finally (and this happened to a few of our users) - they might have some old version of scheduler/ dag file processor running - with old un-synced folders and connected to the same database - in such case such ""additional"" scheduler/dag file processor will continue scanning and adding those files even if they are removed in the ""regular"" scheduler/dag file processor.

Those are all ideas/hypotheses that come to my mind that you could explore - but most of them are just wild guesses and well, hypotheses, that only you can verify.

el-aasi on (2025-01-07 20:48:50 UTC): Just to make sure, this is not the behavior that I have encountered (described above), for the files are deleted and never appear, nevertheless the DAG still remains in the UI/is not marked in the DB.

TerryYin1777 on (2025-02-07 21:31:10 UTC): We have a similar setup (kubernetes deployment with git-sync) and have the same issue. After some deep dive in the code base, I believe the root cause is the following:

When git-sync resyncs, it changes the DAGs folder from:

hash-123/dags/example_dag.py  →  hash-456/dags/example_dag.py

Despite the symlink that points these directories to a contract directory, the [get_dag_directory](https://github.com/apache/airflow/blob/2.9.1/airflow/dag_processing/manager.py#L963) resolves this path to its canonical path, which will be different across re-syncs. This path get passed all the way to the [deactivate_deleted_dags](https://github.com/apache/airflow/blob/2.9.1/airflow/models/dag.py#L3830) function in the dag model, by which is used to mark the dag inactive and therefore hidden from the UI. The deletion is not happening since the processor_subdir value does not match the previously registered processor_subdir.

Not fully sure the reason why we need to resolve the dag folder path to its canonical. I understand it's not an issue with Airflow itself and probably happens only when git-sync is used for dag deployment. But given it's a quite widely adopted combination, is it possible to add a configuration like  GET_DAG_FOLDER_RESOLVE=False so the symlink path is not resolved to its canonical path?

"
2552384709,issue,closed,completed,"Status of testing Providers that were prepared on September 27, 2024","### Body


I have a kind request for all the contributors to the latest provider packages release.
Could you please help us to test the RC versions of the providers?

The guidelines on how to test providers can be found in

[Verify providers by contributors](https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDER_PACKAGES.md#verify-the-release-candidate-by-contributors)

Let us know in the comment, whether the issue is addressed.

Those are providers that require testing as there were some substantial changes introduced:


## Provider [common.sql: 1.17.1rc1](https://pypi.org/project/apache-airflow-providers-common-sql/1.17.1rc1)
   - [x] [fix(providers/common/sql): add dummy connection setter for backward compatibility (#42490)](https://github.com/apache/airflow/pull/42490): @Lee-W
     Linked issues:
       - [x] [Linked Issue #42452](https://github.com/apache/airflow/issues/42452): @agreenburg
   - [ ] [Changed type hinting for handler function (#42275)](https://github.com/apache/airflow/pull/42275): @Ariel2400
## Provider [openlineage: 1.12.1rc1](https://pypi.org/project/apache-airflow-providers-openlineage/1.12.1rc1)
   - [x] [fix: OL dag start event not being emitted (#42448)](https://github.com/apache/airflow/pull/42448): @kacpermuda
     Linked issues:
       - [x] [Linked Issue #41690](https://github.com/apache/airflow/pull/41690): @mobuchowski
   - [x] [Fix typo in error stack trace formatting for clearer output (#42017)](https://github.com/apache/airflow/pull/42017): @aboitreaud

<!--

NOTE TO RELEASE MANAGER:

You can move here the providers that have doc-only changes or for which changes are trivial, and
you could assess that they are OK.

-->
All users involved in the PRs:
@kacpermuda @Lee-W @Ariel2400 @aboitreaud

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2024-09-27 08:53:59+00:00,[],2024-10-01 09:10:32+00:00,2024-10-01 09:10:32+00:00,https://github.com/apache/airflow/issues/42538,"[('area:providers', ''), ('kind:meta', 'High-level information important to the community'), ('testing status', 'Status of testing releases'), ('provider:common-sql', ''), ('provider:openlineage', 'AIP-53')]","[{'comment_id': 2379001596, 'issue_id': 2552384709, 'author': 'kacpermuda', 'body': 'All OL changes are good, thanks for the quick response !', 'created_at': datetime.datetime(2024, 9, 27, 10, 53, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2380420887, 'issue_id': 2552384709, 'author': 'Lee-W', 'body': 'Tested #42490 . Works as expected. Thanks @eladkal !', 'created_at': datetime.datetime(2024, 9, 28, 5, 53, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2385236737, 'issue_id': 2552384709, 'author': 'eladkal', 'body': 'Thank you everyone. Providers are released.\r\n\r\nI invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).', 'created_at': datetime.datetime(2024, 10, 1, 9, 10, 32, tzinfo=datetime.timezone.utc)}]","kacpermuda on (2024-09-27 10:53:08 UTC): All OL changes are good, thanks for the quick response !

Lee-W on (2024-09-28 05:53:22 UTC): Tested #42490 . Works as expected. Thanks @eladkal !

eladkal (Issue Creator) on (2024-10-01 09:10:32 UTC): Thank you everyone. Providers are released.

I invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).

"
2551822449,issue,closed,completed,AIP-78 make dag_run_conf from API be object in spec,I got an error in python api client tests when i did this initially,dstandish,2024-09-27 01:47:39+00:00,[],2024-10-16 13:28:25+00:00,2024-10-16 13:28:25+00:00,https://github.com/apache/airflow/issues/42531,"[('area:API', ""Airflow's REST/HTTP API"")]",[],
2551715858,issue,open,,ObjectStorage jinja template is converting GCS to a different path type,"### Apache Airflow version

2.10.1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When using the [new jinja templating in ObjectStorage](https://github.com/apache/airflow/pull/40638), it seems that it's misinterpreting the protocol of the string and prepending a forward slash ""/"" to my GCS bucket name. 
Perhaps it's [evaluating the protocol](https://github.com/apache/airflow/blob/3749dbde2f87638033a43dfdcd22664a2e20f8b6/airflow/io/path.py#L108) before rendering the jinja string. 


### What you think should happen instead?

When using jinja templating for variable in ObjectStorage, I'm getting the following:
/gs:/redox-n-airflow-workspace-us-central1 

rather than the correct variable name:
gs://redox-n-airflow-workspace-us-central1


### How to reproduce

```python
import datetime
import os
import logging

import pendulum

from airflow.decorators import dag, task
from airflow.io.path import ObjectStoragePath
from airflow.models import Variable

logger = logging.getLogger(__name__)

@dag(
    dag_id='objectstorage_jinja_variable',
    schedule=None,
    start_date=pendulum.datetime(2021, 1, 1, tz=""UTC""),
    catchup=False,
    tags=[""testing""]
)
def dag_example():
    
    @task
    def print_variable(path: ObjectStoragePath):
        variable_path = Variable.get(""gs_analytics_bucket"")
        template_in_task_path = ObjectStoragePath(""{{var.value.gs_analytics_bucket}}"", conn_id=""google_cloud_default"")
        
        logger.debug(f""Jinja Path: {path}"")
        logger.debug(f""Variable Path: {variable_path}"")
        logger.debug(f""Template in Task Path: {template_in_task_path}"")
        logger.debug(f""ENV VAR: {os.getenv('AIRFLOW_VAR_GS_ANALYTICS_BUCKET')}"")
        
    
    
    base = ObjectStoragePath(""{{var.value.gs_analytics_bucket}}"", conn_id=""google_cloud_default"")
    print_variable(base)

dag_example()
``` 

Logging prints the following:
```shell
DEBUG - Jinja Path: /gs:/redox-n-airflow-workspace-us-central1 
DEBUG - Variable Path: gs://redox-n-airflow-workspace-us-central1
DEBUG - Template in Task Path: /{{var.value.gs_analytics_bucket}}
DEBUG - ENV VAR: gs://redox-n-airflow-workspace-us-central1
``` 

EDIT:
I tested the theory that it's interpreting the protocol before rendering and I suspect that's the case given the output below. It wasn't the output I expected because it added `google_cloud_default@` I'm guessing [based on this __str__](https://github.com/apache/airflow/blob/3749dbde2f87638033a43dfdcd22664a2e20f8b6/airflow/io/path.py#L416)

```python
hardcode_protocol = ObjectStoragePath(""gs://{{var.value.gs_analytics_bucket}}"", conn_id=""google_cloud_default"")
    print_variable(hardcode_protocol)
```
```shell
DEBUG - Jinja Path: gs://google_cloud_default@gs://redox-n-airflow-workspace-us-central1/
```

### Operating System

linux amd64

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

> Open to it, but have never contributed. The jinja templating is a bit magical to me and if it's happening in the wrong order, I'm unsure it's an easy enough fix for someone of my skill level. 

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",nolan-redox,2024-09-27 00:01:18+00:00,[],2024-09-27 02:57:11+00:00,,https://github.com/apache/airflow/issues/42528,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('area:core', ''), ('AIP-58', ''), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2378141531, 'issue_id': 2551715858, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 27, 0, 1, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2378292544, 'issue_id': 2551715858, 'author': 'eladkal', 'body': 'cc @bolkedebruin', 'created_at': datetime.datetime(2024, 9, 27, 2, 34, 51, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-27 00:01:21 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2024-09-27 02:34:51 UTC): cc @bolkedebruin

"
2551653709,issue,closed,completed,AIP-78 add DagAccessEntity and resource for Backfill,"### Body

It seems it might be ""correct"" to add a backfill resource and DagAccessEntity.  But it's very confusing to know how we're supposed to implement this, and what granular access control actually means for backfill entities.

I spent some time implementing this [here](https://github.com/apache/airflow/pull/42527) but hit a bit of a dead end and will be deferring it for now.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2024-09-26 22:45:23+00:00,[],2024-12-11 15:27:55+00:00,2024-12-11 15:27:54+00:00,https://github.com/apache/airflow/issues/42526,"[('kind:meta', 'High-level information important to the community'), ('area:backfill', 'Specifically for backfill related')]",[],
2551563692,issue,open,,DAG Param incorrectly convert enum objects to strings,"### Apache Airflow version

2.10.1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Using a DAG Param with a type of `object` and `enum` set to a list of possible objects is reflected as a string in Trigger DAG UI, both in the selector and in the underlying JSON.
![image](https://github.com/user-attachments/assets/23f4c62b-6bc7-4e4d-bb8e-eadb322e8a54)

When attempting to run the DAG using the value, the DAG will fail to run
![image](https://github.com/user-attachments/assets/191486b2-2ef5-4eaf-9523-482a6377204a)


### What you think should happen instead?

JSON schemas support enums of any type 

> Elements in the array might be of any type, including null.

(source: https://json-schema.org/draft/2020-12/json-schema-validation#section-6.1.2-3) . They should be configurable and supported by DAG Parameters, and should allow for successful selection and execution of the DAG from the Trigger DAG UI.

### How to reproduce

Create the following DAG
```python
from airflow.decorators import dag, task
from pendulum import datetime
from airflow.models.param import Param

keys = [""a"", ""b"", ""c""]
values_display_list = [{""key"": key, ""value"": key.upper()} for key in keys]


@dag(
    start_date=datetime(2024, 1, 1),
    schedule=None,
    catchup=False,
    params={
        ""x"": Param(
            values_display_list[0],
            type=[""object""],
            enum=values_display_list,
        ),
    },
    tags=[""example""],
)
def example_params():
    @task
    def example_params_task(*, params, **kwargs) -> None:
        if params:
            print(f""Params: {params}"")
        else:
            print(""No Params"")

    example_params_task()


# Instantiate the DAG
example_params()
```



### Operating System

Debian 12 (Bookworm)

### Versions of Apache Airflow Providers

apache-airflow==2.10.1+astro.1

### Deployment

Other Docker-based deployment

### Deployment details

Astro CLI running Astro Runtime 12.1.0

### Anything else?

I think this issue might be caused by the the trigger templating at https://github.com/apache/airflow/blob/2.10.2/airflow/www/templates/airflow/trigger.html#L77-L86

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",cdabella,2024-09-26 21:30:46+00:00,[],2024-10-06 15:44:14+00:00,,https://github.com/apache/airflow/issues/42524,"[('type:improvement', 'Changelog: Improvements'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2384118935, 'issue_id': 2551563692, 'author': 'jscheffl', 'body': 'Yes, this described problem is a limitatio of the trigger UI form. Object element enums are not support as nobody implemented / requested this feature.\r\n\r\nObjects are currently only supported in the UI as JSON entry field. Enums only with strings.\r\n\r\nNevertheless if you trigger via API or manually adjust the generated JSON in the trigger form, JSON validation will work.\r\n\r\nAre you willing to supply an improvement?', 'created_at': datetime.datetime(2024, 9, 30, 20, 48, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2395451256, 'issue_id': 2551563692, 'author': 'cdabella', 'body': 'I\'m willing to try, though front-end work is definitely not my strength.\r\n\r\nFrom a persona perspective, the primary use case for this is when the DAG Author and DAG operator are different users, and the DAG author explicitly is controlling the available actions for their downstream DAG operator users. In this scenario, the DAG operator may not be aware of the underlying DAG code.\r\n\r\nI\'ve been playing around with how the `trigger.html` and `trigger.js` interact, and specifically the user experience for complex objects that should be considered read-only or selectable from an finite list. This has raised some questions for me.\r\n\r\n## How to select values?\r\nFor standard object params, a CodeMirror text block is added in the parameter form.\r\n\r\n![image](https://github.com/user-attachments/assets/ab47b1fc-a9bd-4dd8-befe-b9af13d50ef5)\r\n\r\nConceptually this doesn\'t align with our goal of an enum, as the textbox should be read-only, but the question becomes how to select from many read-only text boxes.\r\n\r\nProviding a key seems a appropriate. This would need to be the inverse of the existing [`values_display`](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/params.html#json-schema-validation) where the key is actually the parameter value and the value is the UX to be displayed, as complex objects may not be easily serializable into dict keys. I have a working example by adding a `keys_display` parameter to the Param object, but this could probably be renamed to `enum_keys` or similar for simplicity.\r\n\r\n## How to inform users of the selection?\r\nIf we are using keyed selection, what is the appropriate way to inform a user of the actual reflection of that selection in the parameters? There is the `Generated Configuration JSON` view, but this is:\r\n\r\n1. Collapsed by default\r\n2. Mutable\r\n\r\nWe could add a CodeMirror textarea with `readOnly` set below the enum selector, but this has the potential to greatly expand the form size. Should a view like this always be visible? Collapsable?\r\n\r\nIt feels like there should maybe be a requirement with the `enum` param to have an `enum_keys`  param (or autogenerate one).\r\n\r\n## How to store the values?\r\nToday the values for enums are stored as value attributes on the `option` tag. For complex JSON objects this leads either to malformed HTML or JSON parse errors because of single quotes. The object could be stored as a child of the option in a script tag, something like the following? \r\n\r\n```html\r\n<script id=""element_x_option_1"" type=""text/json"">{""key"":""a"",""value"":""a""}</script>\r\n```\r\n\r\n## Is there a better UX that achieves similar results?\r\nFrom the use case I\'m basing this functionality off of, it feels like there is a more complex ask here around have a Parameter Group where parameters are inter-linked, and must be set as a group. This is beyond my skillset today but I just want to callout that another option is possibly viable for the future.', 'created_at': datetime.datetime(2024, 10, 6, 14, 0, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2395484211, 'issue_id': 2551563692, 'author': 'jscheffl', 'body': 'Thanks for the write-up. I assume this level of complexity might also be the reason why this feature was not initially contributed (by me).\r\n\r\nNote that the UI is currently under re-work. The current Airflow 2.10-line will not receive additional features. A new ReactJS based UI with a new trigger form is to be developed. So most probably it makes sense to focus on Airflow 3 for this limitation.\r\n\r\nHave you considered the alternative to abstract the problem away in the UI such that the parameter selection is a ""simple"" drop-down, even with values-display which gives an abstract selection in human readable manner to the user. Based on the selection a first task in the DAG resolves the ""simple string"" into a more complex dictionary with a lookup? That would not need UI complexity and you can hide all technical details in the DAG code.\r\n\r\nSomething like:\r\n```\r\n@dag(\r\n    schedule=None,\r\n    start_date=pendulum.datetime(2021, 1, 1, tz=""UTC""),\r\n    params={\r\n        ""pick_with_label"": Param(\r\n            3,\r\n            type=""number"",\r\n            title=""Select one Number"",\r\n            description=""With drop down selections you can also have nice display labels for the values."",\r\n            enum=[*range(1, 10)],\r\n            values_display={\r\n                1: ""One"",\r\n                2: ""Two"",\r\n                3: ""Three"",\r\n                4: ""Four - is like you take three and get one for free!"",\r\n                5: ""Five"",\r\n                6: ""Six"",\r\n                7: ""Seven"",\r\n                8: ""Eight"",\r\n                9: ""Nine"",\r\n            },\r\n        ),\r\n    },\r\n)\r\ndef example_python_decorator():\r\n    @task\r\n    def select_config_details(**kwargs):\r\n        """"""Select further config parameters depending on user selection.""""""\r\n        params: ParamsDict = kwargs[""params""]\r\n        pick_with_label = params[""pick_with_label""]\r\n        my_complex_config = {\r\n            1: {\r\n                ""foo"": ""bar"",\r\n            },\r\n            ...\r\n        }\r\n        return my_complex_config[pick_with_label]\r\n```\r\n\r\nBesides all the options... from UX Perspective it might be best to have a MD per option and a complex JSON object and render a radio box selection per option in the enum? Would you expect the user wanting to see/understand the object really or would a description be more appropriate?', 'created_at': datetime.datetime(2024, 10, 6, 15, 44, 12, tzinfo=datetime.timezone.utc)}]","jscheffl on (2024-09-30 20:48:43 UTC): Yes, this described problem is a limitatio of the trigger UI form. Object element enums are not support as nobody implemented / requested this feature.

Objects are currently only supported in the UI as JSON entry field. Enums only with strings.

Nevertheless if you trigger via API or manually adjust the generated JSON in the trigger form, JSON validation will work.

Are you willing to supply an improvement?

cdabella (Issue Creator) on (2024-10-06 14:00:04 UTC): I'm willing to try, though front-end work is definitely not my strength.

From a persona perspective, the primary use case for this is when the DAG Author and DAG operator are different users, and the DAG author explicitly is controlling the available actions for their downstream DAG operator users. In this scenario, the DAG operator may not be aware of the underlying DAG code.

I've been playing around with how the `trigger.html` and `trigger.js` interact, and specifically the user experience for complex objects that should be considered read-only or selectable from an finite list. This has raised some questions for me.

## How to select values?
For standard object params, a CodeMirror text block is added in the parameter form.

![image](https://github.com/user-attachments/assets/ab47b1fc-a9bd-4dd8-befe-b9af13d50ef5)

Conceptually this doesn't align with our goal of an enum, as the textbox should be read-only, but the question becomes how to select from many read-only text boxes.

Providing a key seems a appropriate. This would need to be the inverse of the existing [`values_display`](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/params.html#json-schema-validation) where the key is actually the parameter value and the value is the UX to be displayed, as complex objects may not be easily serializable into dict keys. I have a working example by adding a `keys_display` parameter to the Param object, but this could probably be renamed to `enum_keys` or similar for simplicity.

## How to inform users of the selection?
If we are using keyed selection, what is the appropriate way to inform a user of the actual reflection of that selection in the parameters? There is the `Generated Configuration JSON` view, but this is:

1. Collapsed by default
2. Mutable

We could add a CodeMirror textarea with `readOnly` set below the enum selector, but this has the potential to greatly expand the form size. Should a view like this always be visible? Collapsable?

It feels like there should maybe be a requirement with the `enum` param to have an `enum_keys`  param (or autogenerate one).

## How to store the values?
Today the values for enums are stored as value attributes on the `option` tag. For complex JSON objects this leads either to malformed HTML or JSON parse errors because of single quotes. The object could be stored as a child of the option in a script tag, something like the following? 

```html
<script id=""element_x_option_1"" type=""text/json"">{""key"":""a"",""value"":""a""}</script>
```

## Is there a better UX that achieves similar results?
From the use case I'm basing this functionality off of, it feels like there is a more complex ask here around have a Parameter Group where parameters are inter-linked, and must be set as a group. This is beyond my skillset today but I just want to callout that another option is possibly viable for the future.

jscheffl on (2024-10-06 15:44:12 UTC): Thanks for the write-up. I assume this level of complexity might also be the reason why this feature was not initially contributed (by me).

Note that the UI is currently under re-work. The current Airflow 2.10-line will not receive additional features. A new ReactJS based UI with a new trigger form is to be developed. So most probably it makes sense to focus on Airflow 3 for this limitation.

Have you considered the alternative to abstract the problem away in the UI such that the parameter selection is a ""simple"" drop-down, even with values-display which gives an abstract selection in human readable manner to the user. Based on the selection a first task in the DAG resolves the ""simple string"" into a more complex dictionary with a lookup? That would not need UI complexity and you can hide all technical details in the DAG code.

Something like:
```
@dag(
    schedule=None,
    start_date=pendulum.datetime(2021, 1, 1, tz=""UTC""),
    params={
        ""pick_with_label"": Param(
            3,
            type=""number"",
            title=""Select one Number"",
            description=""With drop down selections you can also have nice display labels for the values."",
            enum=[*range(1, 10)],
            values_display={
                1: ""One"",
                2: ""Two"",
                3: ""Three"",
                4: ""Four - is like you take three and get one for free!"",
                5: ""Five"",
                6: ""Six"",
                7: ""Seven"",
                8: ""Eight"",
                9: ""Nine"",
            },
        ),
    },
)
def example_python_decorator():
    @task
    def select_config_details(**kwargs):
        """"""Select further config parameters depending on user selection.""""""
        params: ParamsDict = kwargs[""params""]
        pick_with_label = params[""pick_with_label""]
        my_complex_config = {
            1: {
                ""foo"": ""bar"",
            },
            ...
        }
        return my_complex_config[pick_with_label]
```

Besides all the options... from UX Perspective it might be best to have a MD per option and a complex JSON object and render a radio box selection per option in the enum? Would you expect the user wanting to see/understand the object really or would a description be more appropriate?

"
2551379269,issue,open,,Deferred ECS tasks failing when reading logs from cloudwatch throws RequestTimeoutException,"### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

8.27.0

### Apache Airflow version

2.9.3

### Operating System

Managed by Astronomer

### Deployment

Astronomer

### Deployment details

_No response_

### What happened

Deferred ECS task failed because fetching logs from Cloudwatch timed out (AWS cloudwatch logs client threw RequestTimeoutException).

### What you think should happen instead

Should probably be similar behavior as when the AWS cloudwatch logs client throws ResourceNotFoundException - it should be handled and return None instead of allowing the RequestTimeoutException to propagate.

See: https://github.com/apache/airflow/blob/84e8cdf67475c4b2eeadde99cb11eb02459cc9f5/airflow/providers/amazon/aws/triggers/ecs.py#L214

### How to reproduce

Force the AWS cloudwatch logs client to throw a RequestTimeoutException when running an ECS task in deferred mode.

### Anything else

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",pgieser,2024-09-26 19:27:53+00:00,['pgieser'],2024-10-03 10:08:26+00:00,,https://github.com/apache/airflow/issues/42521,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', '')]","[{'comment_id': 2377766608, 'issue_id': 2551379269, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 26, 19, 27, 56, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-26 19:27:56 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2551244188,issue,closed,completed,[Airbyte] Airflow Airbyte provider 4.0 not allowing disabled auth and does not work with auth,"### Apache Airflow Provider(s)

airbyte

### Versions of Apache Airflow Providers

apache-airflow-providers-airbyte | 4.0.0




### Apache Airflow version

2.8.1

### Operating System

AWS MWAA

### Deployment

Amazon (AWS) MWAA

### Deployment details

Nothing really relevant. Standard AWS MWAA installation. 

### What happened

There are two issues with the version 4.0.0 of the airbyte provider:

1 - In the airbyte provider, there is no use case for disabling authentication as an OSS instalation. You are being forced to enable authentication and pass the client_id and client_secret

2 - It does not seem to get the authentication token. Even after enabling auth and creating the user application and using the client_id and client_secret, it just gets a 404 error. None of the documented endpoints, either in airbyte or the airflow provider code/guidelines seem to work. We have also tried multiple iterations of endpoints, such as /api, /api/v1. /api/public/v1/.... in both the host and/or the Token URL (also tested with v1/applications/token):

```
[2024-09-26, 15:41:37 UTC] {{taskinstance.py:2698}} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/airbyte/hooks/airbyte.py"", line 157, in submit_sync_connection
    res = self.airbyte_api.jobs.create_job(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airbyte_api/jobs.py"", line 100, in create_job
    raise e
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airbyte_api/jobs.py"", line 95, in create_job
    req = self.sdk_configuration.get_hooks().before_request(BeforeRequestContext(hook_ctx), req)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airbyte_api/_hooks/sdkhooks.py"", line 41, in before_request
    out = hook.before_request(hook_ctx, request)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airbyte_api/_hooks/clientcredentials.py"", line 59, in before_request
    sess = self.do_token_request(credentials, self.get_scopes(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airbyte_api/_hooks/clientcredentials.py"", line 122, in do_token_request
    raise Exception(
Exception: Unexpected status code 404 from token endpoint

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/airbyte/operators/airbyte.py"", line 83, in execute
    job_object = hook.submit_sync_connection(connection_id=self.connection_id)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/airbyte/hooks/airbyte.py"", line 165, in submit_sync_connection
    raise AirflowException(e)
airflow.exceptions.AirflowException: Unexpected status code 404 from token endpoint
``` 

### What you think should happen instead

1 - There should be a use case for an OSS instalation with disabled authentication. No token needed to connect.
2 - Not have a 404 error. Or document clearly what you should input in the fields, if we are doing something wrong.

### How to reproduce

1 - Newest install of OSS Airbyte with authentication disabled. You don't have the /applications endpoint, so you cannot get a token and therefore use the provider
2 -  Install newest OSS Airbyte, use airflow's airbyte 4.0.0 provider, try to connect and submit a job using the client_id and client_secret, as well as the Token-URL and host fields.

### Anything else

Every time

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",baugarcia,2024-09-26 18:13:12+00:00,[],2024-11-20 18:42:42+00:00,2024-10-22 11:16:38+00:00,https://github.com/apache/airflow/issues/42520,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('provider:airbyte', '')]","[{'comment_id': 2447746814, 'issue_id': 2551244188, 'author': 'querbesd', 'body': 'Hello, \r\nI have exactly the same issue (OSS instalation and version 4.0.0 of the airbyte provider).\r\nI get this error `airflow.exceptions.AirflowException: Unexpected status code 404 from token endpoint`', 'created_at': datetime.datetime(2024, 10, 30, 16, 34, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2468867728, 'issue_id': 2551244188, 'author': 'ShelRoman', 'body': 'Hello @baugarcia \r\nCould you please share how did you resolve this issue?', 'created_at': datetime.datetime(2024, 11, 11, 19, 17, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489072269, 'issue_id': 2551244188, 'author': 'amardatar', 'body': 'If you use the ""Token URL"" `api/v1/applications/token` then I think that will deal with the 404 at least, but instead I get a 401. I\'ve created an issue https://github.com/airbytehq/airbyte-api-python-sdk/issues/112 to track, since the core problem seems to be from the airbyte-api SDK, rather than from the provider.\r\n\r\nAlternatively, the airbyte-api SDK _does_ seem to work with username/password auth, so possibly the change could still be made here in the provider, with the option to auth using username/password for OSS users?', 'created_at': datetime.datetime(2024, 11, 20, 16, 38, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489306026, 'issue_id': 2551244188, 'author': 'amardatar', 'body': ""For anyone looking for a workaround - this is a little involved, but I've got it working by making the following changes:\r\n\r\nThe connection in Airflow has the details:\r\nHost: `<your airbyte domain>/api/public/v1`\r\nToken URL: `v1/applications/token`\r\nClient ID: `<your client ID>`\r\nClient Secret: `<your client secret>`\r\n\r\nAnd the clientcredentials.py file needs to be updated [here](https://github.com/airbytehq/airbyte-api-python-sdk/blob/048c847914113b75a246ce122fe1cce4f48fee64/src/airbyte_api/_hooks/clientcredentials.py#L119) from `data=payload` to `data=json.dumps(payload)` (you'll also need to `import json` at the top of the file). Up to you how you do this - I created a copy of the file in my repo, and then use that to replace the normally installed file in my Dockerfile, i.e. something like `COPY clientcredentials.py /home/airflow/.local/lib/python3.12/site-packages/airbyte_api/_hooks/clientcredentials.py`.\r\n\r\nObviously - wouldn't recommend this unless you know what you're doing, fairly high chance of something breaking at some point."", 'created_at': datetime.datetime(2024, 11, 20, 18, 42, 41, tzinfo=datetime.timezone.utc)}]","querbesd on (2024-10-30 16:34:41 UTC): Hello, 
I have exactly the same issue (OSS instalation and version 4.0.0 of the airbyte provider).
I get this error `airflow.exceptions.AirflowException: Unexpected status code 404 from token endpoint`

ShelRoman on (2024-11-11 19:17:59 UTC): Hello @baugarcia 
Could you please share how did you resolve this issue?

amardatar on (2024-11-20 16:38:51 UTC): If you use the ""Token URL"" `api/v1/applications/token` then I think that will deal with the 404 at least, but instead I get a 401. I've created an issue https://github.com/airbytehq/airbyte-api-python-sdk/issues/112 to track, since the core problem seems to be from the airbyte-api SDK, rather than from the provider.

Alternatively, the airbyte-api SDK _does_ seem to work with username/password auth, so possibly the change could still be made here in the provider, with the option to auth using username/password for OSS users?

amardatar on (2024-11-20 18:42:41 UTC): For anyone looking for a workaround - this is a little involved, but I've got it working by making the following changes:

The connection in Airflow has the details:
Host: `<your airbyte domain>/api/public/v1`
Token URL: `v1/applications/token`
Client ID: `<your client ID>`
Client Secret: `<your client secret>`

And the clientcredentials.py file needs to be updated [here](https://github.com/airbytehq/airbyte-api-python-sdk/blob/048c847914113b75a246ce122fe1cce4f48fee64/src/airbyte_api/_hooks/clientcredentials.py#L119) from `data=payload` to `data=json.dumps(payload)` (you'll also need to `import json` at the top of the file). Up to you how you do this - I created a copy of the file in my repo, and then use that to replace the normally installed file in my Dockerfile, i.e. something like `COPY clientcredentials.py /home/airflow/.local/lib/python3.12/site-packages/airbyte_api/_hooks/clientcredentials.py`.

Obviously - wouldn't recommend this unless you know what you're doing, fairly high chance of something breaking at some point.

"
2551006415,issue,closed,completed,AIP-82. Manual testing,"### Description

_No response_

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vincbeck,2024-09-26 16:05:28+00:00,['vincbeck'],2024-12-05 20:29:09+00:00,2024-12-05 20:29:08+00:00,https://github.com/apache/airflow/issues/42516,"[('kind:feature', 'Feature Requests'), ('AIP-82', 'External event driven scheduling in Airflow')]","[{'comment_id': 2521332463, 'issue_id': 2551006415, 'author': 'vincbeck', 'body': 'Done', 'created_at': datetime.datetime(2024, 12, 5, 20, 29, 8, tzinfo=datetime.timezone.utc)}]","vincbeck (Issue Creator) on (2024-12-05 20:29:08 UTC): Done

"
2551003136,issue,closed,completed,AIP-82. Add test with example,"### Description

_No response_

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vincbeck,2024-09-26 16:04:01+00:00,['vincbeck'],2024-12-05 20:28:49+00:00,2024-12-05 20:28:21+00:00,https://github.com/apache/airflow/issues/42515,"[('kind:feature', 'Feature Requests'), ('AIP-82', 'External event driven scheduling in Airflow')]","[{'comment_id': 2521328567, 'issue_id': 2551003136, 'author': 'vincbeck', 'body': 'Done as part of #44664', 'created_at': datetime.datetime(2024, 12, 5, 20, 28, 21, tzinfo=datetime.timezone.utc)}]","vincbeck (Issue Creator) on (2024-12-05 20:28:21 UTC): Done as part of #44664

"
2551002114,issue,closed,completed,AIP-82. Handle triggers cleanup,"### Description

Triggers are automatically removed after they succeed. This makes sense when used with deferrable operators but in the event scheduling scenario, they should stay active. The purpose of this task is to handle that.

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vincbeck,2024-09-26 16:03:27+00:00,['vincbeck'],2024-12-05 20:28:45+00:00,2024-12-05 20:27:49+00:00,https://github.com/apache/airflow/issues/42514,"[('kind:feature', 'Feature Requests'), ('area:async-operators', 'AIP-40: Deferrable (""Async"") Operators'), ('area:Triggerer', ''), ('AIP-82', 'External event driven scheduling in Airflow')]","[{'comment_id': 2521327598, 'issue_id': 2551002114, 'author': 'vincbeck', 'body': 'Done as part of #44369', 'created_at': datetime.datetime(2024, 12, 5, 20, 27, 49, tzinfo=datetime.timezone.utc)}]","vincbeck (Issue Creator) on (2024-12-05 20:27:49 UTC): Done as part of #44369

"
2551001064,issue,closed,completed,AIP-82. Send event to asset when trigger succeeds,"### Description

_No response_

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vincbeck,2024-09-26 16:02:55+00:00,['vincbeck'],2024-12-05 20:28:42+00:00,2024-12-04 15:58:24+00:00,https://github.com/apache/airflow/issues/42513,"[('kind:feature', 'Feature Requests'), ('area:Triggerer', ''), ('AIP-82', 'External event driven scheduling in Airflow')]",[],
2551000484,issue,open,,AIP-82. Documentation,"### Description

_No response_

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vincbeck,2024-09-26 16:02:37+00:00,[],2024-10-01 06:58:23+00:00,,https://github.com/apache/airflow/issues/42512,"[('kind:feature', 'Feature Requests'), ('kind:documentation', ''), ('AIP-82', 'External event driven scheduling in Airflow')]",[],
2550999981,issue,closed,completed,AIP-82. Update models,"### Description

_No response_

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vincbeck,2024-09-26 16:02:20+00:00,['vincbeck'],2024-12-05 20:28:55+00:00,2024-11-13 16:12:49+00:00,https://github.com/apache/airflow/issues/42511,"[('kind:feature', 'Feature Requests'), ('AIP-82', 'External event driven scheduling in Airflow')]","[{'comment_id': 2474072908, 'issue_id': 2550999981, 'author': 'vincbeck', 'body': 'Resolved as part of #42511', 'created_at': datetime.datetime(2024, 11, 13, 16, 12, 49, tzinfo=datetime.timezone.utc)}]","vincbeck (Issue Creator) on (2024-11-13 16:12:49 UTC): Resolved as part of #42511

"
2550997924,issue,closed,completed,AIP-82. Save references asset <-> triggers when parsing DAGs,"### Description

_No response_

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vincbeck,2024-09-26 16:01:18+00:00,['vincbeck'],2024-11-25 16:13:34+00:00,2024-11-25 16:13:34+00:00,https://github.com/apache/airflow/issues/42510,"[('kind:feature', 'Feature Requests'), ('area:DAG-processing', ''), ('AIP-82', 'External event driven scheduling in Airflow')]",[],
2550997029,issue,closed,completed,AIP-82. Create migration to add relation between assets and triggers,"### Description

_No response_

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [x] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vincbeck,2024-09-26 16:00:51+00:00,['vincbeck'],2024-11-07 17:31:59+00:00,2024-11-07 17:31:59+00:00,https://github.com/apache/airflow/issues/42509,"[('kind:feature', 'Feature Requests'), ('area:db-migrations', 'PRs with DB migration'), ('AIP-82', 'External event driven scheduling in Airflow')]",[],
2550669811,issue,closed,completed,Apache Kafka operators ProduceToTopicOperator and ConsumeFromTopicOperator unusable since Airflow >= 2.10.0,"### Apache Airflow Provider(s)

apache-kafka

### Versions of Apache Airflow Providers

apache-airflow-providers-apache-kafka==1.6.0

### Apache Airflow version

2.10.2

### Operating System

Debian GNU/Linux 12 (bookworm)

### Deployment

Docker-Compose

### Deployment details

_No response_

### What happened

Since Apache Airflow 2.10.0 and introduction of the feature: callable for template_fields (https://github.com/apache/airflow/pull/37028) operators do not longer work as it just fails at execution.

Indeed callable field _**producer_function**_ of the ProduceToTopicOperator is part of template_fields which make fail DAG execution.
```python
template_fields = (
    ""topic"",
    ""producer_function"",
    ""producer_function_args"",
    ""producer_function_kwargs"",
    ""kafka_config_id"",
)
```

Here's the execution log :
```log
[2024-09-25, 18:13:51 CEST] {abstractoperator.py:778} ERROR - Exception rendering Jinja template for task 'produce_treats', field 'producer_function'. Template: <function prod_function at 0x7ff0cd915d00>
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/abstractoperator.py"", line 768, in _do_render_template_fields
    rendered_content = value(context=context, jinja_env=jinja_env)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: prod_function() got an unexpected keyword argument 'context'
[2024-09-25, 18:13:51 CEST] {taskinstance.py:3310} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 3114, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context, jinja_env=jinja_env)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 3533, in render_templates
    original_task.render_template_fields(context, jinja_env)
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py"", line 1419, in render_template_fields
    self._do_render_template_fields(self, self.template_fields, context, jinja_env, set())
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/abstractoperator.py"", line 768, in _do_render_template_fields
    rendered_content = value(context=context, jinja_env=jinja_env)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: prod_function() got an unexpected keyword argument 'context'
```

The same happens with _**apply_function**_ field in ConsumeFromTopicOperator :
```python
template_fields = (
    ""topics"",
    ""apply_function"",
    ""apply_function_args"",
    ""apply_function_kwargs"",
    ""kafka_config_id"",
)
```

### What you think should happen instead

_No response_

### How to reproduce

Just use the DAG example available here in an Apache Airflow instance: https://airflow.apache.org/docs/apache-airflow-providers-apache-kafka/stable/_modules/tests/system/providers/apache/kafka/example_dag_hello_kafka.html

```python
produce_treats = ProduceToTopicOperator(
    task_id=""produce_treats"",
    kafka_config_id=""kafka_default"",
    topic=KAFKA_TOPIC,
    producer_function=prod_function,
    producer_function_args=[""{{ ti.xcom_pull(task_ids='get_number_of_treats')}}""],
    producer_function_kwargs={
        ""pet_name"": ""{{ ti.xcom_pull(task_ids='get_your_pet_name')}}""
    },
    poll_timeout=10,
)
```

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",mxmrlt,2024-09-26 13:56:16+00:00,['gopidesupavan'],2024-09-29 11:40:38+00:00,2024-09-29 11:40:38+00:00,https://github.com/apache/airflow/issues/42502,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('provider:apache-kafka', '')]","[{'comment_id': 2377052072, 'issue_id': 2550669811, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 26, 13, 56, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2379573472, 'issue_id': 2550669811, 'author': 'gopidesupavan', 'body': 'Thanks for raising this. able to re produce, will workout fix.', 'created_at': datetime.datetime(2024, 9, 27, 15, 41, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2380060625, 'issue_id': 2550669811, 'author': 'gopidesupavan', 'body': '@shahar1 thanks , have created please check when you get chance. not sure the build didnt proceed got this error in ci build \r\n\r\n` TARGET_BRANCH: main\r\nUnable to find image \'bash:latest\' locally\r\ndocker: Error response from daemon: Head ""https://registry-1.docker.io/v2/library/bash/manifests/latest"": unauthorized: incorrect username or password.\r\nSee \'docker run --help\'.\r\nError: Process completed with exit code 125`\r\n\r\nAny idea about this? whats this error  or would you be able to re-trigger please?', 'created_at': datetime.datetime(2024, 9, 27, 21, 7, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2380083648, 'issue_id': 2550669811, 'author': 'shahar1', 'body': '> @shahar1 thanks , have created please check when you get chance. not sure the build didnt proceed got this error in ci build\r\n> \r\n> ` TARGET_BRANCH: main Unable to find image \'bash:latest\' locally docker: Error response from daemon: Head ""https://registry-1.docker.io/v2/library/bash/manifests/latest"": unauthorized: incorrect username or password. See \'docker run --help\'. Error: Process completed with exit code 125`\r\n> \r\n> Any idea about this? whats this error or would you be able to re-trigger please?\r\n\r\nSeems like a Docker Hub issue:\r\nhttps://www.dockerstatus.com/', 'created_at': datetime.datetime(2024, 9, 27, 21, 30, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2380091079, 'issue_id': 2550669811, 'author': 'gopidesupavan', 'body': '> > @shahar1 thanks , have created please check when you get chance. not sure the build didnt proceed got this error in ci build\r\n> > ` TARGET_BRANCH: main Unable to find image \'bash:latest\' locally docker: Error response from daemon: Head ""https://registry-1.docker.io/v2/library/bash/manifests/latest"": unauthorized: incorrect username or password. See \'docker run --help\'. Error: Process completed with exit code 125`\r\n> > Any idea about this? whats this error or would you be able to re-trigger please?\r\n> \r\n> Seems like a Docker Hub issue: https://www.dockerstatus.com/\r\n\r\nYeh correct just have seen in docker service page. There some active incidents going... :)', 'created_at': datetime.datetime(2024, 9, 27, 21, 36, 37, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-26 13:56:20 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

gopidesupavan (Assginee) on (2024-09-27 15:41:41 UTC): Thanks for raising this. able to re produce, will workout fix.

gopidesupavan (Assginee) on (2024-09-27 21:07:19 UTC): @shahar1 thanks , have created please check when you get chance. not sure the build didnt proceed got this error in ci build 

` TARGET_BRANCH: main
Unable to find image 'bash:latest' locally
docker: Error response from daemon: Head ""https://registry-1.docker.io/v2/library/bash/manifests/latest"": unauthorized: incorrect username or password.
See 'docker run --help'.
Error: Process completed with exit code 125`

Any idea about this? whats this error  or would you be able to re-trigger please?

shahar1 on (2024-09-27 21:30:12 UTC): Seems like a Docker Hub issue:
https://www.dockerstatus.com/

gopidesupavan (Assginee) on (2024-09-27 21:36:37 UTC): Yeh correct just have seen in docker service page. There some active incidents going... :)

"
2550268610,issue,closed,completed,Kubernetes Pod Operator Init container logging,"### Description

When writing a pod using an init container, I notice that you don't get any logs from the init container.  I tried adding the name of the init container to the container_logs option in KubernetesPodOperator, but it complains that the container couldn't be found.  
I also noticed that the airflow would say the pod was pending while the init container process was running.  

### Use case/motivation

- When init containers are running the task should be consider running.
- Logs from init containers should be shown 

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",johnhoran,2024-09-26 11:10:39+00:00,[],2024-12-17 22:58:29+00:00,2024-12-17 22:58:29+00:00,https://github.com/apache/airflow/issues/42498,"[('kind:bug', 'This is a clearly a bug'), ('kind:feature', 'Feature Requests'), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]","[{'comment_id': 2453111206, 'issue_id': 2550268610, 'author': 'jscheffl', 'body': 'Thanks for the proposal - I think is is a valid feature! If you implement it, you might make this configurable, maybe not everybody wants logs, maybe some only on error?', 'created_at': datetime.datetime(2024, 11, 2, 19, 57, 7, tzinfo=datetime.timezone.utc)}]","jscheffl on (2024-11-02 19:57:07 UTC): Thanks for the proposal - I think is is a valid feature! If you implement it, you might make this configurable, maybe not everybody wants logs, maybe some only on error?

"
2550247604,issue,closed,completed,WasbHook needs a copy_object method in Airflow,"### Description

Currently we have a `copy_object` method for S3Hook. It would be beneficial to have a similar method for WasbHook to be able to copy blob objects seamlessly in Azure. 

### Use case/motivation

To be able to seamlessly copy blob objects from one Blob container to another within the same Azure storage account. We can also add the native support to copy objects across different storage accounts with an optional parameter. 

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",kunaljubce,2024-09-26 11:00:25+00:00,['kunaljubce'],2024-11-01 20:30:04+00:00,2024-11-01 20:30:04+00:00,https://github.com/apache/airflow/issues/42497,"[('provider:microsoft-azure', 'Azure-related issues'), ('area:providers', ''), ('kind:feature', 'Feature Requests')]","[{'comment_id': 2376625595, 'issue_id': 2550247604, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 26, 11, 0, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2376627460, 'issue_id': 2550247604, 'author': 'kunaljubce', 'body': 'I am working on this, please assign this to me if the feature looks like a reasonable value add.', 'created_at': datetime.datetime(2024, 9, 26, 11, 1, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2376646005, 'issue_id': 2550247604, 'author': 'kunaljubce', 'body': 'The cross-storage account functionality has an added layer of complexity since, unlike in AWS, Azure implements a 3 tiered folder structure. \r\n\r\nAWS: Account (Auth) -> S3 Bucket -> Prefix -> Keys\r\nAzure: Account (Auth) -> Storage Account (Auth) -> Containers -> Blob Prefix -> Blob objects\r\n\r\nIn Azure, due to the need to have authentication setup at a storage account level, it becomes imperative to create different instances of WasbHook for different storage accounts, which is not the case in AWS with the S3Hook. My plan is to handle this with an optional parameter, wherein users can mention the destination storage account details/hook for the dest storage account and the file copy would happen accordingly. \r\n\r\nPlease let me know your thoughts!', 'created_at': datetime.datetime(2024, 9, 26, 11, 11, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2393604511, 'issue_id': 2550247604, 'author': 'Lee-W', 'body': 'roughly read though it. looks like a good idea. I just assigned it to you. Thanks', 'created_at': datetime.datetime(2024, 10, 4, 12, 33, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2452308653, 'issue_id': 2550247604, 'author': 'kunaljubce', 'body': '@Lee-W The PR is up for review now.', 'created_at': datetime.datetime(2024, 11, 1, 17, 43, 51, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-26 11:00:28 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

kunaljubce (Issue Creator) on (2024-09-26 11:01:25 UTC): I am working on this, please assign this to me if the feature looks like a reasonable value add.

kunaljubce (Issue Creator) on (2024-09-26 11:11:07 UTC): The cross-storage account functionality has an added layer of complexity since, unlike in AWS, Azure implements a 3 tiered folder structure. 

AWS: Account (Auth) -> S3 Bucket -> Prefix -> Keys
Azure: Account (Auth) -> Storage Account (Auth) -> Containers -> Blob Prefix -> Blob objects

In Azure, due to the need to have authentication setup at a storage account level, it becomes imperative to create different instances of WasbHook for different storage accounts, which is not the case in AWS with the S3Hook. My plan is to handle this with an optional parameter, wherein users can mention the destination storage account details/hook for the dest storage account and the file copy would happen accordingly. 

Please let me know your thoughts!

Lee-W on (2024-10-04 12:33:06 UTC): roughly read though it. looks like a good idea. I just assigned it to you. Thanks

kunaljubce (Issue Creator) on (2024-11-01 17:43:51 UTC): @Lee-W The PR is up for review now.

"
2550230733,issue,closed,completed,Snowflake connection UI does not hide private_key_content in the Extra field and in the Private key (Text) field,"### Apache Airflow Provider(s)

snowflake

### Versions of Apache Airflow Providers

apache-airflow-providers-snowflake==5.7.1

### Apache Airflow version

2.10.2

### Operating System

Debian GNU/Linux 12 (bookworm)

### Deployment

Astronomer

### Deployment details

_No response_

### What happened

Noticed that the private_key_content as well as the Private key (Text) field are not masked upon editing a connection

![image](https://github.com/user-attachments/assets/0c5fad9e-8b14-4f9e-a9fa-587e14fd608e)


(also the Private key (Text) field displays error and warning icons for valid keys, probably trying to display JSON)

### What you think should happen instead

I think it should be masked in the text field and use `RATHER_LONG_SENSITIVE_FIELD_PLACEHOLDER` in the Extra field
Also it should not display warning icons for a valid key.

### How to reproduce

1. Create a Snowflake connection in the Airflow UI
2. paste a value in the Private Key (Text) field
3. Save the connection
4. Edit the connection
5. See the key

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",TJaniF,2024-09-26 10:52:08+00:00,[],2024-10-02 23:56:06+00:00,2024-10-02 20:29:32+00:00,https://github.com/apache/airflow/issues/42496,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('provider:snowflake', 'Issues related to Snowflake provider'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2389662732, 'issue_id': 2550230733, 'author': 'rawwar', 'body': ""@potiuk , wondering if it's possible to hide private_key_content in the Extra field?  The only way I see it is by making extra field sensitive."", 'created_at': datetime.datetime(2024, 10, 2, 20, 43, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2390012201, 'issue_id': 2550230733, 'author': 'potiuk', 'body': ""> @potiuk , wondering if it's possible to hide private_key_content in the Extra field? The only way I see it is by making extra field sensitive.\r\n\r\nYes. Your PR implemented what's needed. Will be released in the next snowflake provider."", 'created_at': datetime.datetime(2024, 10, 2, 23, 56, 5, tzinfo=datetime.timezone.utc)}]","rawwar on (2024-10-02 20:43:17 UTC): @potiuk , wondering if it's possible to hide private_key_content in the Extra field?  The only way I see it is by making extra field sensitive.

potiuk on (2024-10-02 23:56:05 UTC): Yes. Your PR implemented what's needed. Will be released in the next snowflake provider.

"
2550206417,issue,open,,[Bug] Run `airflow dags test` with `DatasetAlias`/`Dataset` raises `sqlalchemy.orm.exc.FlushError`,"### Apache Airflow version

2.10.0, 2.10,1, 2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Given the DAG:
```
from datetime import datetime

from airflow import DAG, Dataset
from airflow.datasets import DatasetAlias
from airflow.models.baseoperator import BaseOperator
from airflow.utils.context import Context


ALIAS_NAME = ""some-alias""


class CustomOperator(BaseOperator):

    def __init__(self, *args, **kwargs):
        kwargs[""outlets""] = [DatasetAlias(name=ALIAS_NAME)]
        super().__init__(*args, **kwargs)

    def execute(self, context: Context):
        new_outlets = [Dataset(""something"")]
        for outlet in new_outlets:
            context[""outlet_events""][ALIAS_NAME].add(outlet)


with DAG(""dataset_alias_dag"", start_date=datetime(2023, 4, 20)) as dag:
    do_something = CustomOperator(task_id=""do_something"")
    do_something
```

When I try to run:
```
airflow dags test dataset_alias_dag  `date -Iseconds`
```

I get the error:
```
[2024-09-26T11:46:37.012+0300] {dag.py:3060} ERROR - Task failed; ti=<TaskInstance: dataset_alias_dag.do_something manual__2024-09-26T11:46:32+03:00 [success]>
Traceback (most recent call last):
  File ""/Users/tati/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4VBJdS-x/tests.py3.11-2.10/lib/python3.11/site-packages/airflow/models/dag.py"", line 3053, in test
    _run_task(
  File ""/Users/tati/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4VBJdS-x/tests.py3.11-2.10/lib/python3.11/site-packages/airflow/models/dag.py"", line 4357, in _run_task
    ti._run_raw_task(session=session, raise_on_defer=inline_trigger, mark_success=mark_success)
  File ""/Users/tati/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4VBJdS-x/tests.py3.11-2.10/lib/python3.11/site-packages/airflow/utils/session.py"", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/tati/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4VBJdS-x/tests.py3.11-2.10/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2995, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File ""/Users/tati/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4VBJdS-x/tests.py3.11-2.10/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 363, in _run_raw_task
    ti._register_dataset_changes(events=context[""outlet_events""], session=session)
  File ""/Users/tati/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4VBJdS-x/tests.py3.11-2.10/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 3058, in _register_dataset_changes
    dataset_manager.register_dataset_change(
  File ""/Users/tati/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4VBJdS-x/tests.py3.11-2.10/lib/python3.11/site-packages/airflow/api_internal/internal_api_call.py"", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/tati/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4VBJdS-x/tests.py3.11-2.10/lib/python3.11/site-packages/airflow/utils/session.py"", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/tati/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4VBJdS-x/tests.py3.11-2.10/lib/python3.11/site-packages/airflow/datasets/manager.py"", line 145, in register_dataset_change
    session.flush()
  File ""/Users/tati/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4VBJdS-x/tests.py3.11-2.10/lib/python3.11/site-packages/sqlalchemy/orm/session.py"", line 3449, in flush
    self._flush(objects)
  File ""/Users/tati/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4VBJdS-x/tests.py3.11-2.10/lib/python3.11/site-packages/sqlalchemy/orm/session.py"", line 3588, in _flush
    with util.safe_reraise():
  File ""/Users/tati/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4VBJdS-x/tests.py3.11-2.10/lib/python3.11/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""/Users/tati/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4VBJdS-x/tests.py3.11-2.10/lib/python3.11/site-packages/sqlalchemy/util/compat.py"", line 211, in raise_
    raise exception
  File ""/Users/tati/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4VBJdS-x/tests.py3.11-2.10/lib/python3.11/site-packages/sqlalchemy/orm/session.py"", line 3549, in _flush
    flush_context.execute()
  File ""/Users/tati/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4VBJdS-x/tests.py3.11-2.10/lib/python3.11/site-packages/sqlalchemy/orm/unitofwork.py"", line 456, in execute
    rec.execute(self)
  File ""/Users/tati/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4VBJdS-x/tests.py3.11-2.10/lib/python3.11/site-packages/sqlalchemy/orm/unitofwork.py"", line 579, in execute
    self.dependency_processor.process_saves(uow, states)
  File ""/Users/tati/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4VBJdS-x/tests.py3.11-2.10/lib/python3.11/site-packages/sqlalchemy/orm/dependency.py"", line 1136, in process_saves
    if not self._synchronize(
           ^^^^^^^^^^^^^^^^^^
  File ""/Users/tati/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4VBJdS-x/tests.py3.11-2.10/lib/python3.11/site-packages/sqlalchemy/orm/dependency.py"", line 1252, in _synchronize
    self._verify_canload(child)
  File ""/Users/tati/Library/Application Support/hatch/env/virtual/astronomer-cosmos/4VBJdS-x/tests.py3.11-2.10/lib/python3.11/site-packages/sqlalchemy/orm/dependency.py"", line 257, in _verify_canload
    raise exc.FlushError(
sqlalchemy.orm.exc.FlushError: Can't flush None value found in collection DatasetModel.aliases
```

This DAG successfully executes when not being triggered via the `dags test` command.

### What you think should happen instead?

I should be able to run `dags test` for this DAG without seeing this error message.

### How to reproduce

Already described.

### Operating System

Any

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

It is not happening during deployment (tested in Astronomer, and it worked fine).
The issue happens when running the `airflow dags test` command locally

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",tatiana,2024-09-26 10:40:21+00:00,[],2024-12-17 07:56:40+00:00,,https://github.com/apache/airflow/issues/42495,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:datasets', 'Issues related to the datasets feature')]","[{'comment_id': 2376618033, 'issue_id': 2550206417, 'author': 'uranusjr', 'body': 'Note that this only happens if the DAG is _not_ already parsed by the DAG processor. Once parsed, the DatasetAlias will have an entry in the database, and the DAG will run as expected.\r\n\r\nRunning this with a plain Dataset under the same condition also produces an unexpected result:\r\n\r\n```python\r\nwith DAG(""dataset_dag"", ...):\r\n    BashOperator(bash_command="":"", outlets=Dataset(""some_dataset""))\r\n```\r\n\r\nThe task will succeed, but no event is emitted.\r\n\r\n```console\r\n$ airflow dags test dataset_dag\r\n...\r\n[2024-09-26T09:50:38.635+0000] {manager.py:96} WARNING - DatasetModel Dataset(uri=\'some_dataset\', extra=None) not found\r\n...\r\n```\r\n\r\n------\r\n\r\nUltimately this is due to `airflow dags test` is essentially out-of-band execution that does not involve in the normal DAG parsing and scheduling. Dataset event emission is entirely implemented around the database, and thus does not work well in this mode.\r\n\r\nI think we should make a decision how the design of this entire function going forward. Since the situation is sort of similar to `airflow dags backfill`, maybe we should make this also go through the scheduler instead? (Basically make the CLI behave like triggering a manual run in web UI.) This is a relatively large undertaking though. What should we do for 2.x?', 'created_at': datetime.datetime(2024, 9, 26, 10, 56, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2381083843, 'issue_id': 2550206417, 'author': 'uranusjr', 'body': 'I thought about this a bit and arrived at the conclusion that `test` should simply act entirely differently from other forms of execution: Airflow already provides a way to run a DAG outside of its schedule with manual runs (via the web UI or `airflow dags trigger`), and `test` (either from CLI as in this issue, or `dag.test()` in a Python file) should do something different to be meaningful.\r\n\r\nJudging by the word `test`, I’m thinking this operation should further differ from `trigger` in that the result should be completely ephemeral and idendempotent (as long as the tasks themselves are). This means that the _test run_ should leave no trace in the database after execution, including run records, task logs, etc. (I believe this is already almost the case.) And equally, _outlets defined in tasks should not leave a DatasetEvent entry in the database, and threfore do not trigger downstream DAGs.\r\n\r\nDoes this sound reasonable?', 'created_at': datetime.datetime(2024, 9, 29, 3, 1, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382395047, 'issue_id': 2550206417, 'author': 'tatiana', 'body': '@uranusjr It seems sensible for the `dags test` not to leave traces in the database after it is run. Another option could be to give users a configuration option and allow them to run a cleanup task as they please.\r\n\r\nThat said, I believe it is essential that people can use this command to validate somehow:\r\n- Are the expected `Dataset` being set as outlets/inlets of the desired tasks/DAGs?\r\n- Are the expected `DatasetAliases`  being set as outlets/inlets of the desired tasks/DAGs?\r\n\r\nThese two areas are particularly important when we take into account dynamic DAG generation tools.\r\n\r\nSince the command is erroring, I understand that we cannot currently do this. \r\n\r\nCurrently, users of `dags test` can already:\r\n- Check that tasks run as expected\r\n- Check that DAG completes as expected\r\n- DAG topology meets the original design\r\n\r\nIf we want datasets/dataset aliases/assets to be first-class citizens in Airflow, we must also have a way of testing them efficiently.', 'created_at': datetime.datetime(2024, 9, 30, 8, 5, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2396312297, 'issue_id': 2550206417, 'author': 'tatiana', 'body': 'Agreed on the next steps with @uranusjr :\r\n- I\'ll try to add some if/else using airflow `test_mode` config (https://airflow.apache.org/docs/apache-airflow/2.1.0/howto/use-test-config.html)\r\n- Do the minimal possible so the command doesn\'t error, preferably without any changes to the database\r\n\r\nAs a mid-long-term task, we must rethink how we want the `dags test` to work. @ashb suggested that the command be written to a ""test"" database. Before we close this ticket, we should have a proposed plan for this longer term as a GitHub ticket.', 'created_at': datetime.datetime(2024, 10, 7, 8, 51, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2547731376, 'issue_id': 2550206417, 'author': 'kamathramu87', 'body': '@tatiana  how did you solve this in pytest?', 'created_at': datetime.datetime(2024, 12, 17, 7, 56, 39, tzinfo=datetime.timezone.utc)}]","uranusjr on (2024-09-26 10:56:32 UTC): Note that this only happens if the DAG is _not_ already parsed by the DAG processor. Once parsed, the DatasetAlias will have an entry in the database, and the DAG will run as expected.

Running this with a plain Dataset under the same condition also produces an unexpected result:

```python
with DAG(""dataset_dag"", ...):
    BashOperator(bash_command="":"", outlets=Dataset(""some_dataset""))
```

The task will succeed, but no event is emitted.

```console
$ airflow dags test dataset_dag
...
[2024-09-26T09:50:38.635+0000] {manager.py:96} WARNING - DatasetModel Dataset(uri='some_dataset', extra=None) not found
...
```

------

Ultimately this is due to `airflow dags test` is essentially out-of-band execution that does not involve in the normal DAG parsing and scheduling. Dataset event emission is entirely implemented around the database, and thus does not work well in this mode.

I think we should make a decision how the design of this entire function going forward. Since the situation is sort of similar to `airflow dags backfill`, maybe we should make this also go through the scheduler instead? (Basically make the CLI behave like triggering a manual run in web UI.) This is a relatively large undertaking though. What should we do for 2.x?

uranusjr on (2024-09-29 03:01:07 UTC): I thought about this a bit and arrived at the conclusion that `test` should simply act entirely differently from other forms of execution: Airflow already provides a way to run a DAG outside of its schedule with manual runs (via the web UI or `airflow dags trigger`), and `test` (either from CLI as in this issue, or `dag.test()` in a Python file) should do something different to be meaningful.

Judging by the word `test`, I’m thinking this operation should further differ from `trigger` in that the result should be completely ephemeral and idendempotent (as long as the tasks themselves are). This means that the _test run_ should leave no trace in the database after execution, including run records, task logs, etc. (I believe this is already almost the case.) And equally, _outlets defined in tasks should not leave a DatasetEvent entry in the database, and threfore do not trigger downstream DAGs.

Does this sound reasonable?

tatiana (Issue Creator) on (2024-09-30 08:05:22 UTC): @uranusjr It seems sensible for the `dags test` not to leave traces in the database after it is run. Another option could be to give users a configuration option and allow them to run a cleanup task as they please.

That said, I believe it is essential that people can use this command to validate somehow:
- Are the expected `Dataset` being set as outlets/inlets of the desired tasks/DAGs?
- Are the expected `DatasetAliases`  being set as outlets/inlets of the desired tasks/DAGs?

These two areas are particularly important when we take into account dynamic DAG generation tools.

Since the command is erroring, I understand that we cannot currently do this. 

Currently, users of `dags test` can already:
- Check that tasks run as expected
- Check that DAG completes as expected
- DAG topology meets the original design

If we want datasets/dataset aliases/assets to be first-class citizens in Airflow, we must also have a way of testing them efficiently.

tatiana (Issue Creator) on (2024-10-07 08:51:34 UTC): Agreed on the next steps with @uranusjr :
- I'll try to add some if/else using airflow `test_mode` config (https://airflow.apache.org/docs/apache-airflow/2.1.0/howto/use-test-config.html)
- Do the minimal possible so the command doesn't error, preferably without any changes to the database

As a mid-long-term task, we must rethink how we want the `dags test` to work. @ashb suggested that the command be written to a ""test"" database. Before we close this ticket, we should have a proposed plan for this longer term as a GitHub ticket.

kamathramu87 on (2024-12-17 07:56:39 UTC): @tatiana  how did you solve this in pytest?

"
2550099708,issue,closed,completed,Enter sequence values when using Oracle hooks,"### Description

If a sequence column exists when using Oracle Hook, there is no condition to enter the corresponding value.
Then, when the sequence is hung in the ID value, an error called ID cannot insert null into ~

### Use case/motivation

When using functions such as bulk_insert_rows, it would be nice to add a parameter that can put conditions corresponding to the sequence

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee2532,2024-09-26 09:54:37+00:00,[],2024-10-26 19:58:56+00:00,2024-10-26 19:58:56+00:00,https://github.com/apache/airflow/issues/42494,"[('area:providers', ''), ('kind:feature', 'Feature Requests'), ('provider:oracle', '')]","[{'comment_id': 2378309992, 'issue_id': 2550099708, 'author': 'Lee2532', 'body': 'What if we get the column to which the sequence is applied and the sequence name as a parameter?', 'created_at': datetime.datetime(2024, 9, 27, 2, 55, 15, tzinfo=datetime.timezone.utc)}]","Lee2532 (Issue Creator) on (2024-09-27 02:55:15 UTC): What if we get the column to which the sequence is applied and the sequence name as a parameter?

"
2549945386,issue,open,,Provide configuration option to configure path of OTEL metrics exporter,"### Description

Currently Airflow does not allow to configure path in [OTEL metrics exporter](https://github.com/apache/airflow/blob/2.10.2/airflow/metrics/otel_logger.py#L391C1-L403C56)
```python
def get_otel_logger(cls) -> SafeOtelLogger:
    host = conf.get(""metrics"", ""otel_host"")  # ex: ""breeze-otel-collector""
    port = conf.getint(""metrics"", ""otel_port"")  # ex: 4318
    prefix = conf.get(""metrics"", ""otel_prefix"")  # ex: ""airflow""
    ssl_active = conf.getboolean(""metrics"", ""otel_ssl_active"")
    # PeriodicExportingMetricReader will default to an interval of 60000 millis.
    interval = conf.getint(""metrics"", ""otel_interval_milliseconds"", fallback=None)  # ex: 30000
    debug = conf.getboolean(""metrics"", ""otel_debugging_on"")

    resource = Resource(attributes={SERVICE_NAME: ""Airflow""})

    protocol = ""https"" if ssl_active else ""http""
    endpoint = f""{protocol}://{host}:{port}/v1/metrics""
```

And some of the metrics collectors, e.g. VictoriaMetrics expect the metrics to be sent to different path
- `/opentelemetry/api/v1/push`
- `/insert/0/opentelemetry/api/v1/push`

It will be great to have the metrics path to be configurable, like the following

```python
def get_otel_logger(cls) -> SafeOtelLogger:
    host = conf.get(""metrics"", ""otel_host"")  # ex: ""breeze-otel-collector""
    port = conf.getint(""metrics"", ""otel_port"")  # ex: 4318
    path = conf.get(""metrics"", ""otel_path"", fallback=""/v1/metrics"") # ex:  ""/opentelemetry/api/v1/push""
    prefix = conf.get(""metrics"", ""otel_prefix"")  # ex: ""airflow""
    ssl_active = conf.getboolean(""metrics"", ""otel_ssl_active"")
    # PeriodicExportingMetricReader will default to an interval of 60000 millis.
    interval = conf.getint(""metrics"", ""otel_interval_milliseconds"", fallback=None)  # ex: 30000
    debug = conf.getboolean(""metrics"", ""otel_debugging_on"")

    resource = Resource(attributes={SERVICE_NAME: ""Airflow""})

    protocol = ""https"" if ssl_active else ""http""
    endpoint = f""{protocol}://{host}:{port}{path}""
```

### Use case/motivation

Ability to send metrics directly into OTEL-compatible metrics collectors, e.g. VictoriaMetrics.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",szhem,2024-09-26 08:52:39+00:00,['szhem'],2024-11-03 09:04:14+00:00,,https://github.com/apache/airflow/issues/42492,"[('kind:feature', 'Feature Requests'), ('good first issue', ''), ('telemetry', 'Telemetry-related issues')]","[{'comment_id': 2376348900, 'issue_id': 2549945386, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 26, 8, 52, 42, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-26 08:52:42 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2549407665,issue,closed,completed,The Dags Are not showing in the UI on dag search it is showing,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.2

### What happened?

The dags are not showing in the all view but is running and available in dag_bag and being accessed by dag_search

### What you think should happen instead?

_No response_

### How to reproduce

Upgrade to new version

### Operating System

kubernetive

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other 3rd-party Helm chart

### Deployment details

Installed using Community Helm Chart

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Amar1404,2024-09-26 03:13:17+00:00,[],2024-10-03 16:40:59+00:00,2024-10-03 16:40:59+00:00,https://github.com/apache/airflow/issues/42489,"[('kind:bug', 'This is a clearly a bug'), ('pending-response', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2375862646, 'issue_id': 2549407665, 'author': 'Amar1404', 'body': 'Dag are being parsed, checked all the logs of scheduler and dag parser, dags are running and only showing running dag in UI,or failed dag in UI, even i go to dag from dag search, but are not visible dag in all dags or active dags in web UI', 'created_at': datetime.datetime(2024, 9, 26, 4, 46, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2378294880, 'issue_id': 2549407665, 'author': 'eladkal', 'body': 'Can you share reproduce steps?', 'created_at': datetime.datetime(2024, 9, 27, 2, 36, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382178683, 'issue_id': 2549407665, 'author': 'Amar1404', 'body': 'Hi @eladkal  - \r\n1. I am using helm chart to deploy the airflow. Please find below the chart\r\n2 I am using Dyanmic dag generation using one single file. Have checked the scheduler have process all the dags and even dag_processor , working fine in 2.9.2\r\n[helmcharAirflow.txt](https://github.com/user-attachments/files/17184718/helmcharAirflow.txt)', 'created_at': datetime.datetime(2024, 9, 30, 5, 59, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382226891, 'issue_id': 2549407665, 'author': 'ephraimbuddy', 'body': '> Hi @eladkal -\r\n> \r\n> 1. I am using helm chart to deploy the airflow. Please find below the chart\r\n>    2 I am using Dyanmic dag generation using one single file. Have checked the scheduler have process all the dags and even dag_processor , working fine in 2.9.2\r\n>    [helmcharAirflow.txt](https://github.com/user-attachments/files/17184718/helmcharAirflow.txt)\r\n\r\nCan you share the screenshot of this part of your UI:\r\n![Screenshot 2024-09-30 at 07 32 51](https://github.com/user-attachments/assets/f8f191ed-ab74-4ecc-8299-ee9e7d17a7ca)', 'created_at': datetime.datetime(2024, 9, 30, 6, 33, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382323370, 'issue_id': 2549407665, 'author': 'cesar-vermeulen', 'body': 'Also experienced this issue. It appears that whenever you click on running or failed, filtering continues to apply after clicking active/paused/all (cfr. url in screenshot). In previous versions, the filtering did not persist when you clicked on different tiles.\r\n![image](https://github.com/user-attachments/assets/69ae70bd-d56b-4715-b052-96fe1dff08d5)\r\n\r\nWhen you click on the current active filter (running/failed), all dags appear again.', 'created_at': datetime.datetime(2024, 9, 30, 7, 29, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382856231, 'issue_id': 2549407665, 'author': 'Amar1404', 'body': 'the above screenshot is the issue the all only showing 4 dags which are running or failed. not all the active or paused dags', 'created_at': datetime.datetime(2024, 9, 30, 10, 55, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382880551, 'issue_id': 2549407665, 'author': 'ephraimbuddy', 'body': ""That's not an issue. I knew that was your confusion, but it's right that 4 is displayed in 'all' because the total number of 'running' dags is 4. You have to deselect the Running tab by clicking on it again so it's no longer applied."", 'created_at': datetime.datetime(2024, 9, 30, 11, 6, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382883964, 'issue_id': 2549407665, 'author': 'bbovenzi', 'body': 'This is working as expected. But we will be improving the UX in Airflow 3.0 to make it more intuitive.', 'created_at': datetime.datetime(2024, 9, 30, 11, 8, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2391862382, 'issue_id': 2549407665, 'author': 'Amar1404', 'body': 'Got it thanks for the information.', 'created_at': datetime.datetime(2024, 10, 3, 16, 40, 59, tzinfo=datetime.timezone.utc)}]","Amar1404 (Issue Creator) on (2024-09-26 04:46:44 UTC): Dag are being parsed, checked all the logs of scheduler and dag parser, dags are running and only showing running dag in UI,or failed dag in UI, even i go to dag from dag search, but are not visible dag in all dags or active dags in web UI

eladkal on (2024-09-27 02:36:58 UTC): Can you share reproduce steps?

Amar1404 (Issue Creator) on (2024-09-30 05:59:33 UTC): Hi @eladkal  - 
1. I am using helm chart to deploy the airflow. Please find below the chart
2 I am using Dyanmic dag generation using one single file. Have checked the scheduler have process all the dags and even dag_processor , working fine in 2.9.2
[helmcharAirflow.txt](https://github.com/user-attachments/files/17184718/helmcharAirflow.txt)

ephraimbuddy on (2024-09-30 06:33:48 UTC): Can you share the screenshot of this part of your UI:
![Screenshot 2024-09-30 at 07 32 51](https://github.com/user-attachments/assets/f8f191ed-ab74-4ecc-8299-ee9e7d17a7ca)

cesar-vermeulen on (2024-09-30 07:29:36 UTC): Also experienced this issue. It appears that whenever you click on running or failed, filtering continues to apply after clicking active/paused/all (cfr. url in screenshot). In previous versions, the filtering did not persist when you clicked on different tiles.
![image](https://github.com/user-attachments/assets/69ae70bd-d56b-4715-b052-96fe1dff08d5)

When you click on the current active filter (running/failed), all dags appear again.

Amar1404 (Issue Creator) on (2024-09-30 10:55:21 UTC): the above screenshot is the issue the all only showing 4 dags which are running or failed. not all the active or paused dags

ephraimbuddy on (2024-09-30 11:06:59 UTC): That's not an issue. I knew that was your confusion, but it's right that 4 is displayed in 'all' because the total number of 'running' dags is 4. You have to deselect the Running tab by clicking on it again so it's no longer applied.

bbovenzi on (2024-09-30 11:08:44 UTC): This is working as expected. But we will be improving the UX in Airflow 3.0 to make it more intuitive.

Amar1404 (Issue Creator) on (2024-10-03 16:40:59 UTC): Got it thanks for the information.

"
2549017715,issue,closed,not_planned,Airflow assume task context is serialized with Pydantic models ,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.2

### What happened?

The triggerer terminates when attempting to deserialize the task context dictionary.

```
[2024-09-25T13:25:57.492-0500] {triggerer_job_runner.py:338} INFO - Starting the triggerer
[2024-09-25T13:25:57.629-0500] {triggerer_job_runner.py:348} ERROR - Exception when executing TriggererJobRunner._run_trigger_loop
Traceback (most recent call last):
  File ""/usr/local/lib/python3.12/site-packages/airflow/jobs/triggerer_job_runner.py"", line 346, in _execute
    self._run_trigger_loop()
  File ""/usr/local/lib/python3.12/site-packages/airflow/jobs/triggerer_job_runner.py"", line 374, in _run_trigger_loop
    self.load_triggers()
  File ""/usr/local/lib/python3.12/site-packages/airflow/traces/tracer.py"", line 58, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/jobs/triggerer_job_runner.py"", line 400, in load_triggers
    self.trigger_runner.update_triggers(set(ids))
  File ""/usr/local/lib/python3.12/site-packages/airflow/jobs/triggerer_job_runner.py"", line 719, in update_triggers
    new_trigger_instance = trigger_class(**new_trigger_orm.kwargs)
                                           ^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/models/trigger.py"", line 94, in kwargs
    return self._decrypt_kwargs(self.encrypted_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/models/trigger.py"", line 130, in _decrypt_kwargs
    return BaseSerialization.deserialize(decrypted_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/serialization/serialized_objects.py"", line 831, in deserialize
    return {k: cls.deserialize(v, use_pydantic_models) for k, v in var.items()}
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/serialization/serialized_objects.py"", line 821, in deserialize
    d[k] = cls.deserialize(v, use_pydantic_models=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/airflow/serialization/serialized_objects.py"", line 803, in deserialize
    raise RuntimeError(
RuntimeError: Setting use_pydantic_models = True requires AIP-44 (in progress) feature flag to be true. This parameter will be removed eventually when new serialization is used by AIP-44
```

The issue started appearing when I upgraded from Airflow 2.9.3 to Airflow 2.10.2. 

Do note that I have a custom trigger where I am serializing the task context.

```
class CustomTrigger(BaseTrigger):
...
    def serialize(self) -> tuple[str, dict[str, Any]]:
        """"""
        Serializes the trigger's arguments and classpath.

        :return: A tuple containing the classpath and a dictionary of arguments.
        """"""
        return (
            ""CustomTrigger"",
            {
                ""context"": self.context,
            },
        )
```

### What you think should happen instead?

I believe the deserialize operation should not be [forcing `use_pydantic_models` to be true](https://github.com/apache/airflow/blob/2.10.2/airflow/serialization/serialized_objects.py#L816-L821C43).

Instead, it should be using the value passed as a parameter.

```
                d[k] = cls.deserialize(v, use_pydantic_models)
```

Also, when [the task context is being serialized](https://github.com/apache/airflow/blob/2.10.2/airflow/serialization/serialized_objects.py#L764-L769), it is respecting the value passed to the serialized function.

### How to reproduce

1. Create a custom Trigger that serializes the task context
2. Create a deferrable operator that uses the custom trigger
3. Run the task

### Operating System

n/a

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",wolfier,2024-09-25 21:18:30+00:00,[],2024-10-26 00:15:11+00:00,2024-10-26 00:15:11+00:00,https://github.com/apache/airflow/issues/42485,"[('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('good first issue', ''), ('pending-response', ''), ('area:core', ''), ('area:Triggerer', '')]","[{'comment_id': 2391597872, 'issue_id': 2549017715, 'author': 'gopidesupavan', 'body': ""Currently context is not supported in trigger's, if you need any values from the context object , extract and send as key-->values."", 'created_at': datetime.datetime(2024, 10, 3, 14, 39, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2392631976, 'issue_id': 2549017715, 'author': 'potiuk', 'body': 'Yeah. We might want to handle it better - so better error message should be printed in this case. Marked it as good-first-issue.', 'created_at': datetime.datetime(2024, 10, 4, 1, 45, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2423396664, 'issue_id': 2549017715, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 10, 19, 0, 14, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439071063, 'issue_id': 2549017715, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 10, 26, 0, 15, 10, tzinfo=datetime.timezone.utc)}]","gopidesupavan on (2024-10-03 14:39:59 UTC): Currently context is not supported in trigger's, if you need any values from the context object , extract and send as key-->values.

potiuk on (2024-10-04 01:45:25 UTC): Yeah. We might want to handle it better - so better error message should be printed in this case. Marked it as good-first-issue.

github-actions[bot] on (2024-10-19 00:14:59 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-10-26 00:15:10 UTC): This issue has been closed because it has not received response from the issue author.

"
2548800358,issue,closed,completed,Airflow task_fail table does not have some failures,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.2

### What happened?

We are pulling in data from Airflow's task_fail table mentioned [here](https://airflow.apache.org/docs/apache-airflow/2.9.2/database-erd-ref.html)
<img width=""235"" alt=""image"" src=""https://github.com/user-attachments/assets/74c35c07-8888-4817-bcbe-23b89574c980"">

We have a task with retires are set to 0
<img width=""313"" alt=""image"" src=""https://github.com/user-attachments/assets/dbf0a61a-cbf5-4b95-83f0-a17008e9fbf3"">

When this task fails, there are no more retries and the DAG state becomes 'failed'. When this happens , the records are not logged in task_fail table. When the retires are greater than 0, this works perfectly fine and the failed tasks are logged.

Can you please check what is happening ?


### What you think should happen instead?

_No response_

### How to reproduce

Set task retry to 0 and check task_fail table

### Operating System

Amazon Linux. VERSION=2023

### Versions of Apache Airflow Providers

_No response_

### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",dgrewal-tc,2024-09-25 19:14:04+00:00,[],2024-09-25 20:16:48+00:00,2024-09-25 20:16:48+00:00,https://github.com/apache/airflow/issues/42482,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2548643143,issue,open,,airflowignore is not hiding DAGs,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.2

### What happened?

After upgrading from 2.10.0 to 2.10.2 in order to correct the issue related to [#41699](https://github.com/apache/airflow/pull/41699), the airflow is not hiding my legacy pipelines.

Inside `dags/` folder, i have a subfolder called `archived/`, in which i have pipelines that i do not use anymore. These pipelines have the suffix `_legacy`. 
Inside the `archived/` there is the `.airflowignore` file with the _legacy keyword, in order to ignore the pipelines in which the filename ends in `_legacy`.

After the upgrade, it seems that this file is not working since no pipeline in which the filename ends in `_legacy` are hidden.

I tested moving `.airflowignore` to the `dags/` folder, and appended the keyword `archived/`, in order to hide all the pipelines on that folder. Still no legacy pipelines were hidden.

Can you help me check what is going on here?


### What you think should happen instead?

The pipelines inside `archived/` folder or the pipelines that contains `_legacy.py` in the filename should not appear on airflow dags list.

### How to reproduce

- Create .airflowignore file
- add the keywords `_legacy.py` and `archived/` on separate lines
- create a testing dag in which the name ends in `_legacy.py`
- Verify if the pipelines appear in airflow UI DAG list

### Operating System

Debian GNU/Linux 12

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.28.0
apache-airflow-providers-celery==3.8.1
apache-airflow-providers-cncf-kubernetes==8.4.1
apache-airflow-providers-common-compat==1.2.0
apache-airflow-providers-common-io==1.4.0
apache-airflow-providers-common-sql==1.16.0
apache-airflow-providers-docker==3.13.0
apache-airflow-providers-elasticsearch==5.5.0
apache-airflow-providers-fab==1.3.0
apache-airflow-providers-ftp==3.11.0
apache-airflow-providers-google==10.23.0
apache-airflow-providers-grpc==3.6.0
apache-airflow-providers-hashicorp==3.8.0
apache-airflow-providers-http==4.13.0
apache-airflow-providers-imap==3.7.0
apache-airflow-providers-microsoft-azure==10.4.0
apache-airflow-providers-mongo==4.2.1
apache-airflow-providers-mysql==5.7.0
apache-airflow-providers-odbc==4.7.0
apache-airflow-providers-openlineage==1.11.0
apache-airflow-providers-postgres==5.12.0
apache-airflow-providers-redis==3.8.0
apache-airflow-providers-sendgrid==3.6.0
apache-airflow-providers-sftp==4.11.0
apache-airflow-providers-slack==8.9.0
apache-airflow-providers-smtp==1.8.0
apache-airflow-providers-snowflake==5.7.0
apache-airflow-providers-sqlite==3.9.0
apache-airflow-providers-ssh==3.13.1

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Docker used to create the image from the original:

```
FROM apache/airflow:2.10.2

RUN pip install --no-cache-dir --upgrade ""pip""
RUN pip install --no-cache-dir --upgrade \
	""apache-airflow-providers-google"" \
	""apache-airflow-providers-mongo"" \
	""google-ads"" \
	""google-api-core"" \
	""google-api-python-client"" \
	""google-auth"" \
	""google-auth-httplib2"" \
	""google-auth-oauthlib"" \
	""google-cloud-aiplatform"" \
	""google-cloud-appengine-logging"" \
	""google-cloud-audit-log"" \
	""google-cloud-automl"" \
	""google-cloud-bigquery"" \
	""google-cloud-bigquery-datatransfer"" \
	""google-cloud-bigquery-storage"" \
	""google-cloud-bigtable"" \
	""google-cloud-build"" \
	""google-cloud-container"" \
	""google-cloud-core"" \
	""google-cloud-datacatalog"" \
	""google-cloud-dataform"" \
	""google-cloud-dataplex"" \
	""google-cloud-dataproc"" \
	""google-cloud-dataproc-metastore"" \
	""google-cloud-dlp"" \
	""google-cloud-kms"" \
	""google-cloud-language"" \
	""google-cloud-logging"" \
	""google-cloud-memcache"" \
	""google-cloud-monitoring"" \
	""google-cloud-orchestration-airflow"" \
	""google-cloud-os-login"" \
	""google-cloud-pubsub"" \
	""google-cloud-redis"" \
	""google-cloud-secret-manager"" \
	""google-cloud-spanner"" \
	""google-cloud-speech"" \
	""google-cloud-storage"" \
	""google-cloud-tasks"" \
	""google-cloud-texttospeech"" \
	""google-cloud-translate"" \
	""google-cloud-videointelligence"" \
	""google-cloud-vision"" \
	""google-cloud-workflows"" \
	""mailchimp-marketing""
```

### Anything else?

More context:

Airflow is installed in a test and production environment.
Test environment is with 2.10.2 version.
Production environment is with 2.10.0 version.

On test environment, the issue happens.
On production environment, the issue does not happen,

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",progressive-scaler,2024-09-25 17:57:06+00:00,['progressive-scaler'],2024-11-13 06:27:03+00:00,,https://github.com/apache/airflow/issues/42476,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('area:DAG-processing', '')]","[{'comment_id': 2374793825, 'issue_id': 2548643143, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 25, 17, 57, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2419416039, 'issue_id': 2548643143, 'author': 'c-thiel', 'body': 'We encountered a related issue: Airflow was parsing files it wasn\'t supposed to parse and showed ""Broken DAG"" in the UI. We then added the files to .airflowignore.\r\nAirflow still showed the Broken DAGs. We checked the code and found that currently no process is using .airflowignore to clear or filter files from the `import_error` table. So we truncated the table manually and now everything is OK again.', 'created_at': datetime.datetime(2024, 10, 17, 12, 31, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466240624, 'issue_id': 2548643143, 'author': 'progressive-scaler', 'body': 'While trying to solve this issue, i made the following validations:\r\n\r\n- First i checked if i could open or run the DAGs that were supposed to be hidden. When i tried to execute, the Dags would not run; when i tried to open, i would get the ""missing DagBag"" error;\r\n- Then i asked devops to share the `AIRFLOW__LOGGING__DAG_PROCESSOR_MANAGER_LOG_LOCATION` log. Analyzing the log, i realized that there were no information of the DAGs that were supposed to be hidden, even though they were still appearing on Airflow GUI;\r\n\r\nAfter this outcomes, i just removed the DAGs via Airflow GUI. After that, they were never re-appeared in Airflow GUI.\r\n\r\nAnother extra note: this happened after upgrading Airflow on a test environment. When we upgrade the production environment, this issue never happened. I could not get to the potential reasons on why it happened on test environment and not on production environment.', 'created_at': datetime.datetime(2024, 11, 9, 14, 33, 3, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-25 17:57:10 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

c-thiel on (2024-10-17 12:31:46 UTC): We encountered a related issue: Airflow was parsing files it wasn't supposed to parse and showed ""Broken DAG"" in the UI. We then added the files to .airflowignore.
Airflow still showed the Broken DAGs. We checked the code and found that currently no process is using .airflowignore to clear or filter files from the `import_error` table. So we truncated the table manually and now everything is OK again.

progressive-scaler (Issue Creator) on (2024-11-09 14:33:03 UTC): While trying to solve this issue, i made the following validations:

- First i checked if i could open or run the DAGs that were supposed to be hidden. When i tried to execute, the Dags would not run; when i tried to open, i would get the ""missing DagBag"" error;
- Then i asked devops to share the `AIRFLOW__LOGGING__DAG_PROCESSOR_MANAGER_LOG_LOCATION` log. Analyzing the log, i realized that there were no information of the DAGs that were supposed to be hidden, even though they were still appearing on Airflow GUI;

After this outcomes, i just removed the DAGs via Airflow GUI. After that, they were never re-appeared in Airflow GUI.

Another extra note: this happened after upgrading Airflow on a test environment. When we upgrade the production environment, this issue never happened. I could not get to the potential reasons on why it happened on test environment and not on production environment.

"
2548002580,issue,closed,completed,AIP-84 Migrate the public endpoint Update DAG to FastAPI,"Part of: https://github.com/apache/airflow/issues/42370

Migrate the 'PATCH' on the Dag. Mostly use to 'pause' the dag.


### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-09-25 13:28:22+00:00,['pierrejeambrun'],2024-09-26 10:16:13+00:00,2024-09-26 10:16:13+00:00,https://github.com/apache/airflow/issues/42468,"[('area:API', ""Airflow's REST/HTTP API""), ('kind:meta', 'High-level information important to the community'), ('AIP-84', 'Modern Rest API')]",[],
2547984297,issue,closed,completed,AIP-84 List DAGs endpoint new features (2/2) Advanced,"### Body

### Description
Related to: https://github.com/apache/airflow/issues/42370

Advanced:
- later on, we will want more advanced search features like any/all combinations of tags
- we will also want to search more by dag run: ""show dags with a dag run within this date range"", ""show dags with a running dagrun"", 
<img width=""851"" alt=""Screenshot 2024-09-11 at 8 51 43 AM"" src=""https://github.com/user-attachments/assets/36138f06-929d-4ce4-b9f5-3bc4d1caa8e2"">

We're looking to include the last X runs of a DAG, but that can be a separate endpoint to add later:
<img width=""566"" alt=""Screenshot 2024-09-11 at 9 00 03 AM"" src=""https://github.com/user-attachments/assets/fd1ada28-e6b7-485a-88f7-dc015e76e8bd"">


### Use case/motivation

Our current REST API dags list endpoint is not sufficient. It doesn't include any run history and sorting only works for a few fields
The legacy UI uses a custom FAB view which also needs to be replaced.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)


### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-09-25 13:21:08+00:00,['prabhusneha'],2025-01-22 04:04:29+00:00,2025-01-22 04:04:29+00:00,https://github.com/apache/airflow/issues/42467,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('kind:meta', 'High-level information important to the community'), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2541138782, 'issue_id': 2547984297, 'author': 'prabhusneha', 'body': 'I can work on this one', 'created_at': datetime.datetime(2024, 12, 13, 10, 41, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2541141143, 'issue_id': 2547984297, 'author': 'pierrejeambrun', 'body': 'Great @prabhusneha, assigned, thank you!', 'created_at': datetime.datetime(2024, 12, 13, 10, 42, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2572803548, 'issue_id': 2547984297, 'author': 'prabhusneha', 'body': 'PR: https://github.com/apache/airflow/pull/45420', 'created_at': datetime.datetime(2025, 1, 6, 10, 20, tzinfo=datetime.timezone.utc)}]","prabhusneha (Assginee) on (2024-12-13 10:41:35 UTC): I can work on this one

pierrejeambrun (Issue Creator) on (2024-12-13 10:42:31 UTC): Great @prabhusneha, assigned, thank you!

prabhusneha (Assginee) on (2025-01-06 10:20:00 UTC): PR: https://github.com/apache/airflow/pull/45420

"
2547858667,issue,closed,completed,FivetranOperator stuck in deferred state till it times out,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.2

### What happened?

This is in relation to https://github.com/astronomer/airflow-provider-fivetran-async/issues/109

As mentioned in the [comments](https://github.com/astronomer/airflow-provider-fivetran-async/issues/109#issuecomment-2368036922), When we upgraded Airflow from 2.8.1 to 2.9.2, FivetranOperator in airflow-provider-fivetran-async package started giving issues abruptly.

We are using the FivetranOperator to trigger a Fivetran connector. The connector completes in 5-10 minutes, but the Airflow task does not finish. It remains in a deferred state until it times out and fails.

Airflow Version : 2.9.2
airflow-provider-fivetran-async==2.1.0

Failed logs
```
2024-09-15, 10:26:53 UTC] INFO - Pausing task as DEFERRED.
[2024-09-15, 10:26:54 UTC] INFO - Task exited with return code 100 (task deferral)

[2024-09-15, 11:26:58 UTC] INFO - Resuming after deferral
[2024-09-15, 11:26:59 UTC] INFO - Executing <Task(FivetranOperator)> on 2024-09-14 08:00:00+00:00
[2024-09-15, 11:26:59 UTC] INFO - Started process 9527 to run task
[2024-09-15, 11:26:59 UTC] INFO - Job 918648: Subtask
[2024-09-15, 11:26:59 UTC] ERROR - Task failed with exception
Traceback (most recent call last):
airflow.exceptions.AirflowTaskTimeout
```

Success logs
```
[2024-09-15, 12:12:51 UTC] INFO - Pausing task as DEFERRED. 
[2024-09-15, 12:12:52 UTC] INFO - Task exited with return code 100 (task deferral)

[2024-09-15, 12:12:52 UTC] INFO - Using connection ID 'id' for task execution.
[2024-09-15, 12:12:52 UTC] INFO - sync is still running...
[2024-09-15, 12:12:52 UTC] INFO - sleeping for 120 seconds.
[2024-09-15, 12:14:53 UTC] INFO - sync is still running...
[2024-09-15, 12:14:53 UTC] INFO - sleeping for 120 seconds.
[2024-09-15, 12:16:53 UTC] INFO - sync is still running...
[2024-09-15, 12:16:53 UTC] INFO - sleeping for 120 seconds.

[2024-09-15, 12:19:00 UTC] INFO - Resuming after deferral
[2024-09-15, 12:19:00 UTC] INFO - Executing <Task(FivetranOperator):> on 2024-09-14 08:00:00+00:00
[2024-09-15, 12:19:00 UTC] INFO - Started process 11601 to run task
[2024-09-15, 12:19:00 UTC] INFO - Job 918720: Subtask
[2024-09-15, 12:19:00 UTC] INFO - Fivetran connector finished syncing at 2024-09-15 12:17:07.840000+00:00
[2024-09-15, 12:19:00 UTC] INFO - Marking task as SUCCESS.
[2024-09-15, 12:19:00 UTC] INFO - Task exited with return code 0
```

Can someone please have a look ?

Thanks

### What you think should happen instead?

_No response_

### How to reproduce

Use FivetranOperator.
This issue does not happen every time we run it. It is very random.


### Operating System

Amazon Linux. VERSION=2023

### Versions of Apache Airflow Providers

_No response_

### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",dgrewal-tc,2024-09-25 12:29:58+00:00,[],2024-10-28 06:52:33+00:00,2024-10-27 22:13:22+00:00,https://github.com/apache/airflow/issues/42465,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('pending-response', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2373951361, 'issue_id': 2547858667, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 25, 12, 30, 2, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-25 12:30:02 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2546782910,issue,closed,not_planned,Add a new task instance status: ICED,"### Description

Add a new status called iced to [taskinstance](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/tasks.html#task-instances)

When the task instance status is set to ICED via taskinstance.set_state('iced'),  Airflow will immediately stop the task instance and mark it as ""ICED"", and the downstream tasks will continue to run as if this task is ""success"".

This feature can be rollout phase by phase:
Phase 1: Add this new status in metaDB and UI, and when do scheduling treat it like the combination of ""skip"" and ""success"", the task will be ignored but the downstream thinks it has been successfully completed
Phase 2: If this task has dataset outlets, it will trigger the dataset update as if the task has been successfully completed
Phase 3: Users can leverage Airflow UI, and put a task into ICED status

 This concept is from Autosys which is another job scheduling system.

### Use case/motivation

Let's say we have a DAG with three tasks:  A >> B >> C.
During the day2day work, there might be some occurred in task B which is caused by external, ie the error is not caused by a bug of the code.  In such cases, we want to ""skip"" this task, so during the schedule the new flow looks like this: A >> C.
Please note the ""skip"" here is different with the Airflow built-in skip, this is something we'd like to call it ICED.  If a task is ICED, all the downstream tasks will continue to run even they set the trigger rules to 'all success'. 


### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",nicolasge,2024-09-25 02:54:23+00:00,[],2025-01-28 07:50:41+00:00,2025-01-28 07:50:40+00:00,https://github.com/apache/airflow/issues/42459,"[('area:MetaDB', 'Meta Database related issues.'), ('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2618144350, 'issue_id': 2546782910, 'author': 'eladkal', 'body': '> Let\'s say we have a DAG with three tasks: A >> B >> C.\nDuring the day2day work, there might be some occurred in task B which is caused by external, ie the error is not caused by a bug of the code. In such cases, we want to ""skip"" this task, so during the schedule the new flow looks like this: A >> C.\nPlease note the ""skip"" here is different with the Airflow built-in skip, this is something we\'d like to call it ICED. If a task is ICED, all the downstream tasks will continue to run even they set the trigger rules to \'all success\'.\n\nThe easy and best solution to my taste is to manually mark B as success and leave a note on the task of what you fixed. The note will make the task square be half colored makes it easy to track\n\n<img width=""101"" alt=""Image"" src=""https://github.com/user-attachments/assets/91988110-2f0e-41e6-bfc6-99cb03bcc894"" />\n\nI\'m closing this issue as your use case is solved natively without the need to introduce new task state.', 'created_at': datetime.datetime(2025, 1, 28, 7, 50, 40, tzinfo=datetime.timezone.utc)}]","eladkal on (2025-01-28 07:50:40 UTC): During the day2day work, there might be some occurred in task B which is caused by external, ie the error is not caused by a bug of the code. In such cases, we want to ""skip"" this task, so during the schedule the new flow looks like this: A >> C.
Please note the ""skip"" here is different with the Airflow built-in skip, this is something we'd like to call it ICED. If a task is ICED, all the downstream tasks will continue to run even they set the trigger rules to 'all success'.

The easy and best solution to my taste is to manually mark B as success and leave a note on the task of what you fixed. The note will make the task square be half colored makes it easy to track

<img width=""101"" alt=""Image"" src=""https://github.com/user-attachments/assets/91988110-2f0e-41e6-bfc6-99cb03bcc894"" />

I'm closing this issue as your use case is solved natively without the need to introduce new task state.

"
2546509623,issue,closed,completed,OTEL Collector exception scheduler termination,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.0

### What happened?

An OTEL Collector exception caused the scheduler to terminate unexpectedly.

```
[2024-09-13T14:12:12.681+0000] {scheduler_job_runner.py:1001} ERROR - Exception when executing SchedulerJob._run_scheduler_loop
Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py"", line 984, in _execute
    self._run_scheduler_loop()
  File ""/usr/local/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1132, in _run_scheduler_loop
    num_finished_events += self._process_executor_events(
  File ""/usr/local/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py"", line 851, in _process_executor_events
    span.add_event(name=""started"", timestamp=datetime_to_nano(ti.start_date))
  File ""/usr/local/lib/python3.10/site-packages/airflow/utils/dates.py"", line 283, in datetime_to_nano
    return int(datetime.timestamp() * 1000000000)
AttributeError: 'NoneType' object has no attribute 'timestamp'
```

The exception was due to the processing of an executor event where the tied task instance start_date attribute did not have the expected type of datetime.

### What you think should happen instead?

Instead of raising the exception and causing the schedule to terminate, the exception should be caught and an error log to be generated.

### How to reproduce

Unknown.

### Operating System

n/a

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",wolfier,2024-09-24 22:20:05+00:00,['jedcunningham'],2024-10-31 22:34:18+00:00,2024-10-31 22:34:18+00:00,https://github.com/apache/airflow/issues/42456,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2372744216, 'issue_id': 2546509623, 'author': 'jedcunningham', 'body': ""~~I wasn't able to reproduce this on main or 2.10.0, unfortunately.~~\r\n\r\n~~edit: wait, might have overlooked something. Let me try again.~~\r\n\r\nedit edit: I was able to reproduce this. I'll open a PR tomorrow."", 'created_at': datetime.datetime(2024, 9, 25, 2, 11, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2389078950, 'issue_id': 2546509623, 'author': 'pustoshilov-d', 'body': '@wolfier  @jedcunningham  @nathadfield, hi!\r\nI have the same issue on 2.10.4. I can\'t normally reproduce it, but have more info \r\n\r\n```\r\n[2024-10-02T15:09:12.515+0000] {executor_loader.py:254} INFO - Loaded executor: CeleryExecutor\r\n[2024-10-02T15:09:12.565+0000] {scheduler_job_runner.py:935} INFO - Starting the scheduler\r\n[2024-10-02T15:09:12.566+0000] {scheduler_job_runner.py:942} INFO - Processing each file at most -1 times\r\n[2024-10-02T15:09:12.570+0000] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs\r\n[2024-10-02T15:09:12.675+0000] {otel_tracer.py:265} INFO - [OTLPSpanExporter] Connecting to OpenTelemetry Collector at http://1.1.1.1:24318/v1/traces\r\n[2024-10-02T15:09:12.683+0000] {celery_executor.py:433} INFO - Adopted the following 1 tasks from a dead executor\r\n        <TaskInstance: mgi__daily.trigger_mgi_bioinformatics manual__2024-10-01T22:02:42+03:00 [running]> in state SUCCESS\r\n[2024-10-02T15:09:12.949+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id=\'mgi__daily\', task_id=\'trigger_mgi_bioinformatics\', run_id=\'manual__2024-10-01T22:02:42+03:00\', try_number=1, map_index=-1)\r\n[2024-10-02T15:09:12.960+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=mgi__daily, task_id=trigger_mgi_bioinformatics, run_id=manual__2024-10-01T22:02:42+03:00, map_index=-1, run_start_date=2024-10-01 19:03:16.803553+00:00, run_end_date=None, run_duration=None, state=running, executor=CeleryExecutor(parallelism=1024), executor_state=success, try_number=1, max_tries=1, job_id=33823, pool=default_pool, queue=default, priority_weight=1, operator=TriggerDagRunOperator, queued_dttm=2024-10-01 19:03:15.226131+00:00, queued_by_job_id=41976, pid=47466\r\n[2024-10-02T15:09:12.961+0000] {__init__.py:100} WARNING - Invalid type NoneType for attribute \'duration\' value. Expected one of [\'bool\', \'str\', \'bytes\', \'int\', \'float\'] or a sequence of those types\r\n[2024-10-02T15:09:12.966+0000] {scheduler_job_runner.py:1001} ERROR - Exception when executing SchedulerJob._run_scheduler_loop\r\nTraceback (most recent call last):\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/scheduler_job_runner.py"", line 984, in _execute\r\n    self._run_scheduler_loop()\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1132, in _run_scheduler_loop\r\n    num_finished_events += self._process_executor_events(\r\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/scheduler_job_runner.py"", line 852, in _process_executor_events\r\n    span.add_event(name=""ended"", timestamp=datetime_to_nano(ti.end_date))\r\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/dates.py"", line 283, in datetime_to_nano\r\n    return int(datetime.timestamp() * 1000000000)\r\n               ^^^^^^^^^^^^^^^^^^\r\nAttributeError: \'NoneType\' object has no attribute \'timestamp\'\r\n```\r\n\r\nas I see from the logs it happens when executor_state==success and ti.state==running is passed there https://github.com/apache/airflow/blob/1b538a9a3c0f8edb7e5ddeb7b7f1f30d9675549c/airflow/jobs/scheduler_job_runner.py#L781-L784\r\n\r\nsearching for the reasons of it in my case is an open question, I think something happens in dead scheduler/executor reincarnation\'s processes\r\nif anyone has any ideas -- you\'re welcomed\r\n\r\nbut I guess this should be handled in the code because of failed ti.end_date=None there\r\nhttps://github.com/apache/airflow/blob/1b538a9a3c0f8edb7e5ddeb7b7f1f30d9675549c/airflow/jobs/scheduler_job_runner.py#L844\r\n\r\nlet me know if I could help more or write a PR', 'created_at': datetime.datetime(2024, 10, 2, 16, 16, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2391261862, 'issue_id': 2546509623, 'author': 'pustoshilov-d', 'body': 'today caught it again with executor_state==success and ti.state==queued', 'created_at': datetime.datetime(2024, 10, 3, 12, 11, 20, tzinfo=datetime.timezone.utc)}]","jedcunningham (Assginee) on (2024-09-25 02:11:15 UTC): ~~I wasn't able to reproduce this on main or 2.10.0, unfortunately.~~

~~edit: wait, might have overlooked something. Let me try again.~~

edit edit: I was able to reproduce this. I'll open a PR tomorrow.

pustoshilov-d on (2024-10-02 16:16:56 UTC): @wolfier  @jedcunningham  @nathadfield, hi!
I have the same issue on 2.10.4. I can't normally reproduce it, but have more info 

```
[2024-10-02T15:09:12.515+0000] {executor_loader.py:254} INFO - Loaded executor: CeleryExecutor
[2024-10-02T15:09:12.565+0000] {scheduler_job_runner.py:935} INFO - Starting the scheduler
[2024-10-02T15:09:12.566+0000] {scheduler_job_runner.py:942} INFO - Processing each file at most -1 times
[2024-10-02T15:09:12.570+0000] {scheduler_job_runner.py:1847} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-10-02T15:09:12.675+0000] {otel_tracer.py:265} INFO - [OTLPSpanExporter] Connecting to OpenTelemetry Collector at http://1.1.1.1:24318/v1/traces
[2024-10-02T15:09:12.683+0000] {celery_executor.py:433} INFO - Adopted the following 1 tasks from a dead executor
        <TaskInstance: mgi__daily.trigger_mgi_bioinformatics manual__2024-10-01T22:02:42+03:00 [running]> in state SUCCESS
[2024-10-02T15:09:12.949+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='mgi__daily', task_id='trigger_mgi_bioinformatics', run_id='manual__2024-10-01T22:02:42+03:00', try_number=1, map_index=-1)
[2024-10-02T15:09:12.960+0000] {scheduler_job_runner.py:801} INFO - TaskInstance Finished: dag_id=mgi__daily, task_id=trigger_mgi_bioinformatics, run_id=manual__2024-10-01T22:02:42+03:00, map_index=-1, run_start_date=2024-10-01 19:03:16.803553+00:00, run_end_date=None, run_duration=None, state=running, executor=CeleryExecutor(parallelism=1024), executor_state=success, try_number=1, max_tries=1, job_id=33823, pool=default_pool, queue=default, priority_weight=1, operator=TriggerDagRunOperator, queued_dttm=2024-10-01 19:03:15.226131+00:00, queued_by_job_id=41976, pid=47466
[2024-10-02T15:09:12.961+0000] {__init__.py:100} WARNING - Invalid type NoneType for attribute 'duration' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types
[2024-10-02T15:09:12.966+0000] {scheduler_job_runner.py:1001} ERROR - Exception when executing SchedulerJob._run_scheduler_loop
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/scheduler_job_runner.py"", line 984, in _execute
    self._run_scheduler_loop()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1132, in _run_scheduler_loop
    num_finished_events += self._process_executor_events(
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/scheduler_job_runner.py"", line 852, in _process_executor_events
    span.add_event(name=""ended"", timestamp=datetime_to_nano(ti.end_date))
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/dates.py"", line 283, in datetime_to_nano
    return int(datetime.timestamp() * 1000000000)
               ^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'timestamp'
```

as I see from the logs it happens when executor_state==success and ti.state==running is passed there https://github.com/apache/airflow/blob/1b538a9a3c0f8edb7e5ddeb7b7f1f30d9675549c/airflow/jobs/scheduler_job_runner.py#L781-L784

searching for the reasons of it in my case is an open question, I think something happens in dead scheduler/executor reincarnation's processes
if anyone has any ideas -- you're welcomed

but I guess this should be handled in the code because of failed ti.end_date=None there
https://github.com/apache/airflow/blob/1b538a9a3c0f8edb7e5ddeb7b7f1f30d9675549c/airflow/jobs/scheduler_job_runner.py#L844

let me know if I could help more or write a PR

pustoshilov-d on (2024-10-03 12:11:20 UTC): today caught it again with executor_state==success and ti.state==queued

"
2546243563,issue,closed,completed,AIP-84 Migrate the public endpoint DAG Details to FastAPI,"### Description

Currently the DAG Details public endpoint is at `api_connexion/endpoints/dag_endpoint.py` under `get_dag_details`. We need to migrate it to the `api_fastapi/views/public/dags.py` under a `get_dag_details` or similar.

Features and functionality of the endpoint will remain unchanged. 

### Use case/motivation

AIP-84 cWiki Link: https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-84+UI+REST+API

### Related issues

Related to: https://github.com/apache/airflow/issues/42370

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-09-24 19:45:29+00:00,['omkar-foss'],2024-10-03 08:38:02+00:00,2024-10-03 08:38:02+00:00,https://github.com/apache/airflow/issues/42453,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2373949430, 'issue_id': 2546243563, 'author': 'omkar-foss', 'body': ""I've started working on this, will raise a PR in the next few days (cc: @pierrejeambrun)."", 'created_at': datetime.datetime(2024, 9, 25, 12, 29, 8, tzinfo=datetime.timezone.utc)}]","omkar-foss (Issue Creator) on (2024-09-25 12:29:08 UTC): I've started working on this, will raise a PR in the next few days (cc: @pierrejeambrun).

"
2546132373,issue,closed,completed,MySqlHook constructor fails with exception after upgrading apache-airflow-providers-common-sql from 1.16.0 to 1.17.0,"### Apache Airflow Provider(s)

common-sql, mysql

### Versions of Apache Airflow Providers

apache-airflow-providers-common-sql      1.17.0
apache-airflow-providers-mysql           5.6.1


### Apache Airflow version

2.9.2+astro.1

### Operating System

debian astronomer/astro-runtime:11.5.0

### Deployment

Astronomer

### Deployment details

This is an Astronomer-hosted deployment. Docker image is quay.io/astronomer/astro-runtime:11.5.0

### What happened

DAGs that interact with MySqlHook started failing after apache-airflow-providers-common-sql was automatically upgraded from 1.16.0 to 1.17.0.

Here is an example error from the log:

```
[2024-09-24, 18:24:30 UTC] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/baseoperator.py"", line 401, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/operators/python.py"", line 235, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/operators/python.py"", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/dags/sftp_handler.py"", line 25, in getDBConnection
    return MySqlHook(connection_name).get_conn()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/providers/mysql/hooks/mysql.py"", line 75, in __init__
    self.connection = kwargs.pop(""connection"", None)
    ^^^^^^^^^^^^^^^
AttributeError: property 'connection' of 'MySqlHook' object has no setter
```

requirements.txt had apache-airflow-providers-mysql with no version constraint.

```
apache-airflow-providers-mysql
```

Forcing downgrade to the previous version fixes the problem and allows the DAGs to run again:

```
apache-airflow-providers-mysql
apache-airflow-providers-common-sql <1.17
```

### What you think should happen instead

Because we are including apache-airflow-providers-mysql without a version constraint, I would hope and expect that it would only bring in a version of apache-airflow-providers-common-sql that is compatible with the version of apache-airflow-providers-mysql that is being used. That does not appear to be happening here.

### How to reproduce

1. Install these exact dependency versions:

apache-airflow-providers-common-sql      1.17.0
apache-airflow-providers-mysql           5.6.1

2. Inside a python operator, attempt to get an instance of a MysqlHook with a named connection:

```
conn = MySqlHook(""my_named_mysql_connection"").get_conn()
```

This will throw an AttributeError:

```
AttributeError: property 'connection' of 'MySqlHook' object has no setter
```

### Anything else

This is consistently reproducible with the listed provider versions.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",agreenburg,2024-09-24 18:54:50+00:00,[],2024-09-26 12:39:02+00:00,2024-09-26 12:39:01+00:00,https://github.com/apache/airflow/issues/42452,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('provider:common-sql', ''), ('provider:postgres', ''), ('provider:mysql', '')]","[{'comment_id': 2372065313, 'issue_id': 2546132373, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 24, 18, 54, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2373588863, 'issue_id': 2546132373, 'author': 'steffidlf', 'body': '+1, we have the same issue with the same versions', 'created_at': datetime.datetime(2024, 9, 25, 9, 43, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2373860412, 'issue_id': 2546132373, 'author': 'dweaver33', 'body': ""+1, we're having the same issue as well"", 'created_at': datetime.datetime(2024, 9, 25, 11, 49, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2373995902, 'issue_id': 2546132373, 'author': 'veer0318', 'body': ""+1 same issue;\r\n\r\nquick solution that worked for us (don't know why, to be honest), doing a pip install with this:\r\n```python\r\napache-airflow-providers-mysql>=5.7.1\r\n```"", 'created_at': datetime.datetime(2024, 9, 25, 12, 49, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2374924095, 'issue_id': 2546132373, 'author': 'tboschtxr', 'body': '+1 same issue but for postgres \r\n```  File ""/usr/local/lib/python3.10/site-packages/airflow/providers/postgres/hooks/postgres.py"", line 90, in __init__\r\n    self.connection: Connection | None = kwargs.pop(""connection"", None)\r\nAttributeError: can\'t set attribute \'connection\'```', 'created_at': datetime.datetime(2024, 9, 25, 19, 0, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2374954401, 'issue_id': 2546132373, 'author': 'rawwar', 'body': '@tboschtxr , can you use `apache-airflow-providers-postgres==5.13.0`  in you requirements.txt?', 'created_at': datetime.datetime(2024, 9, 25, 19, 6, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2374980584, 'issue_id': 2546132373, 'author': 'tboschtxr', 'body': '@rawwar \r\n\r\nI will give this a try now.', 'created_at': datetime.datetime(2024, 9, 25, 19, 10, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2375047098, 'issue_id': 2546132373, 'author': 'tboschtxr', 'body': '@rawwar this did work yes, thank you!', 'created_at': datetime.datetime(2024, 9, 25, 19, 23, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2375076114, 'issue_id': 2546132373, 'author': 'agreenburg', 'body': 'This looks like it is resolved with the latest version of apache-airflow-providers-mysql.\r\n\r\nHowever I would like to leave this open for visibility because having dependencies defined without version constraints between providers that might be updated independently could cause things like this to break again in the future due to `pip install` behavior to not update existing modules that already satisfy the requirement.\r\n\r\nBecause apache-airflow-providers-mysql has no upper version constraint on its apache-airflow-providers-common-sql dependency, any breaking change to a public API in a future version of apache-airflow-providers-common-sql can break existing airflow installs.', 'created_at': datetime.datetime(2024, 9, 25, 19, 30, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2375782275, 'issue_id': 2546132373, 'author': 'Lee-W', 'body': 'Just traced the code a bit. This was due to a change in `apache-airflow-providers-mysql==1.17.0`. [connection](https://github.com/apache/airflow/pull/40751/files#diff-6e1b2f961cb951d05d66d2d814ef5f6d8f8bf8f43c40fb5d40e27a031fed8dd7R208) is now a descriptor without a setter (and I think this is by design).\r\n\r\nThe reason why `apache-airflow-providers-mysql==5.7.1` works is due to the fix in the same [PR](https://github.com/apache/airflow/pull/40751/files#diff-b072809da92f96948d535afe96b8b6b1927a5ecca99144eaf79a603449a29406L75). Also postgres change [here](https://github.com/apache/airflow/pull/40751/files#diff-f1ec9506a1f434e38d095c1f10634243f4cce602a595cd786f0ca33aededb051L91) along with other providers in that PR', 'created_at': datetime.datetime(2024, 9, 26, 3, 30, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2375809304, 'issue_id': 2546132373, 'author': 'Lee-W', 'body': ""I'll send a PR for adding a dummy setter for backcompat."", 'created_at': datetime.datetime(2024, 9, 26, 3, 49, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2376294980, 'issue_id': 2546132373, 'author': 'dabla', 'body': 'This is probably due to my [PR 40751](https://github.com/apache/airflow/pull/40751), a dummy setter would indeed solve the issue', 'created_at': datetime.datetime(2024, 9, 26, 8, 30, 8, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-24 18:54:53 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

steffidlf on (2024-09-25 09:43:10 UTC): +1, we have the same issue with the same versions

dweaver33 on (2024-09-25 11:49:17 UTC): +1, we're having the same issue as well

veer0318 on (2024-09-25 12:49:41 UTC): +1 same issue;

quick solution that worked for us (don't know why, to be honest), doing a pip install with this:
```python
apache-airflow-providers-mysql>=5.7.1
```

tboschtxr on (2024-09-25 19:00:48 UTC): +1 same issue but for postgres 
```  File ""/usr/local/lib/python3.10/site-packages/airflow/providers/postgres/hooks/postgres.py"", line 90, in __init__
    self.connection: Connection | None = kwargs.pop(""connection"", None)
AttributeError: can't set attribute 'connection'```

rawwar on (2024-09-25 19:06:18 UTC): @tboschtxr , can you use `apache-airflow-providers-postgres==5.13.0`  in you requirements.txt?

tboschtxr on (2024-09-25 19:10:48 UTC): @rawwar 

I will give this a try now.

tboschtxr on (2024-09-25 19:23:22 UTC): @rawwar this did work yes, thank you!

agreenburg (Issue Creator) on (2024-09-25 19:30:32 UTC): This looks like it is resolved with the latest version of apache-airflow-providers-mysql.

However I would like to leave this open for visibility because having dependencies defined without version constraints between providers that might be updated independently could cause things like this to break again in the future due to `pip install` behavior to not update existing modules that already satisfy the requirement.

Because apache-airflow-providers-mysql has no upper version constraint on its apache-airflow-providers-common-sql dependency, any breaking change to a public API in a future version of apache-airflow-providers-common-sql can break existing airflow installs.

Lee-W on (2024-09-26 03:30:13 UTC): Just traced the code a bit. This was due to a change in `apache-airflow-providers-mysql==1.17.0`. [connection](https://github.com/apache/airflow/pull/40751/files#diff-6e1b2f961cb951d05d66d2d814ef5f6d8f8bf8f43c40fb5d40e27a031fed8dd7R208) is now a descriptor without a setter (and I think this is by design).

The reason why `apache-airflow-providers-mysql==5.7.1` works is due to the fix in the same [PR](https://github.com/apache/airflow/pull/40751/files#diff-b072809da92f96948d535afe96b8b6b1927a5ecca99144eaf79a603449a29406L75). Also postgres change [here](https://github.com/apache/airflow/pull/40751/files#diff-f1ec9506a1f434e38d095c1f10634243f4cce602a595cd786f0ca33aededb051L91) along with other providers in that PR

Lee-W on (2024-09-26 03:49:59 UTC): I'll send a PR for adding a dummy setter for backcompat.

dabla on (2024-09-26 08:30:08 UTC): This is probably due to my [PR 40751](https://github.com/apache/airflow/pull/40751), a dummy setter would indeed solve the issue

"
2545642456,issue,closed,not_planned,Add support for an allowlist of DAG bundle sources,"### Body

We want to give deployment managers the ability to restrict DAG bundle sources (e.g. ""only from this github org""). Since these sources will be configured from the API, having a config driven way to restrict where they can come from is an important security control.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2024-09-24 14:53:16+00:00,[],2024-12-19 16:36:10+00:00,2024-12-19 16:36:08+00:00,https://github.com/apache/airflow/issues/42446,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2554981435, 'issue_id': 2545642456, 'author': 'jedcunningham', 'body': 'These are now configured in the config, so deployment managers are already in direct control of it!', 'created_at': datetime.datetime(2024, 12, 19, 16, 36, 8, tzinfo=datetime.timezone.utc)}]","jedcunningham (Issue Creator) on (2024-12-19 16:36:08 UTC): These are now configured in the config, so deployment managers are already in direct control of it!

"
2545416919,issue,closed,completed,AWS ECS Logging very slow when lots of logging leading to task failure,"### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.28.0 is affected

apache-airflow-providers-amazon==8.27.0 is not affected

### Apache Airflow version

2.10.2

### Operating System

Debian 12 bookworm

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### What happened

After upgrading to Airflow 2.10.2 longer running ECS tasks with significant logging started failing. The logs would still be slowly appearing on Airflow, yet the ECS Task had completed. If the logging took more than an hour more than the task, then the ECS task in Airflow would fail with an error that the ECS Task was missing. This is due to the older tasks disappearing within ECS (Fargate).

Looking at the changes I came across https://github.com/apache/airflow/pull/41515/files which added a 0.1 second sleep if the timestamps were the same. On looking further at the logs of the tasks that were failing, there were 2 log times. One which was getting significantly later than the other from the application.

On rolling back the amazon provider to the previous version and still using Airflow 2.10.2 the issue went away.

Linked tickets #41515 #40875

### What you think should happen instead

Logging should be submitted in a timely manner.

Could we go for a much shorter delay such as 0.001 seconds?

### How to reproduce

Have an ECS Task that has a lot more logging than the time it takes to run the task.

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",smsm1-ito,2024-09-24 13:27:51+00:00,[],2024-11-03 09:10:28+00:00,2024-11-03 09:10:28+00:00,https://github.com/apache/airflow/issues/42442,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('area:logging', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2371277691, 'issue_id': 2545416919, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 24, 13, 27, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2371950192, 'issue_id': 2545416919, 'author': 'smsm1-ito', 'body': ""I've created a merge request for this: https://github.com/apache/airflow/pull/42449"", 'created_at': datetime.datetime(2024, 9, 24, 17, 58, 30, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-24 13:27:54 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

smsm1-ito (Issue Creator) on (2024-09-24 17:58:30 UTC): I've created a merge request for this: https://github.com/apache/airflow/pull/42449

"
2545209231,issue,closed,not_planned,Inconsistent log formatting in (TaskDetail) Logs and (See more) Log tabs,"### Apache Airflow version

2.10.1

### If ""Other Airflow 2 version"" selected, which one?

2.10.2

### What happened?

In the Logs tab, the colors for warnings and errors are used in the log text (see the pict)
![image](https://github.com/user-attachments/assets/c9c78c0b-b2cb-4835-a256-575669fded60)

But if you display the log using See more, the text will be displayed without colour formatting (see the pict of the very same log)
![image](https://github.com/user-attachments/assets/f0e328c7-40a4-49fa-b077-e65a88698ea7)



### What you think should happen instead?

The log text should be displayed equally (with colored formating) in both places

### How to reproduce

View the task log with warnings and/or errors in the task detail, and then view it using the View More feature.

### Operating System

Kubernetes on Unix platform

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Pad71,2024-09-24 12:03:43+00:00,[],2024-09-27 12:31:26+00:00,2024-09-27 12:31:05+00:00,https://github.com/apache/airflow/issues/42440,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2374554937, 'issue_id': 2545209231, 'author': 'tirkarthi', 'body': ""This was intentional since legacy log page doesn't process logs line by line like log tab. The processing is done on client side. The legacy logs page is planned to be removed so there won't be much development to add this feature.\r\n\r\nhttps://github.com/apache/airflow/pull/39006#issuecomment-2067480912"", 'created_at': datetime.datetime(2024, 9, 25, 16, 25, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2379163835, 'issue_id': 2545209231, 'author': 'Pad71', 'body': '@tirkarthi  Ok, thanks for the information. Closing this issue.', 'created_at': datetime.datetime(2024, 9, 27, 12, 30, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2379164318, 'issue_id': 2545209231, 'author': 'Pad71', 'body': '...', 'created_at': datetime.datetime(2024, 9, 27, 12, 31, 5, tzinfo=datetime.timezone.utc)}]","tirkarthi on (2024-09-25 16:25:43 UTC): This was intentional since legacy log page doesn't process logs line by line like log tab. The processing is done on client side. The legacy logs page is planned to be removed so there won't be much development to add this feature.

https://github.com/apache/airflow/pull/39006#issuecomment-2067480912

Pad71 (Issue Creator) on (2024-09-27 12:30:49 UTC): @tirkarthi  Ok, thanks for the information. Closing this issue.

Pad71 (Issue Creator) on (2024-09-27 12:31:05 UTC): ...

"
2545146288,issue,closed,completed,Implicit exception in `GCSToLocalFilesystemOperator` when reading non-existing files,"### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

10.15.0

### Apache Airflow version

2.6.3

### Operating System

Debian

### Deployment

Google Cloud Composer

### Deployment details

_No response_

### What happened

When using `GCSToLocalFilesystemOperator` on a non-existing object, the resulted exception is regarding a `NoneType` object with no `size` attribute - which is True, but it would be better to raise an exception that the file simply does not exist.

### What you think should happen instead

_No response_

### How to reproduce

Use the `GCSToLocalFilesystemOperator` with a non-existing `OBJECT_NAME`
```python
    gcs_to_fs = GCSToLocalFilesystemOperator(
        task_id=""gcs_to_fs"",
        bucket=BUCKET,  # <- exists
        object_name=OBJECT_NAME, # <- does not exist
    )
```
You'll get an exception similar to this:

```
Traceback (most recent call last):
  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/providers/google/cloud/transfers/gcs_to_local.py"", line 118, in execute
    file_size = hook.get_size(bucket_name=self.bucket, object_name=self.object_name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/providers/google/cloud/hooks/gcs.py"", line 968, in get_size
    blob_size = blob.size
                ^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'size'
```


### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",shahar1,2024-09-24 11:35:21+00:00,['jsjasonseba'],2024-09-27 08:11:09+00:00,2024-09-27 08:11:09+00:00,https://github.com/apache/airflow/issues/42439,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('area:providers', ''), ('good first issue', '')]","[{'comment_id': 2371758395, 'issue_id': 2545146288, 'author': 'jsjasonseba', 'body': 'Hi @shahar1, I’d like to take this issue. I plan to create a custom exception inheriting from AirflowException and raise it. Any suggestions?', 'created_at': datetime.datetime(2024, 9, 24, 16, 22, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2371786567, 'issue_id': 2545146288, 'author': 'shahar1', 'body': ""> Hi @shahar1, I’d like to take this issue. I plan to create a custom exception inheriting from AirflowException and raise it. Any suggestions?\r\n\r\nThanks for your willing to contribute!\r\nI just assigned you this issue.\r\nI'd avoid for now creating a customized exception class, as no other classes in this provider do so.\r\nPlease ping me in the PR when you're ready :)"", 'created_at': datetime.datetime(2024, 9, 24, 16, 35, 1, tzinfo=datetime.timezone.utc)}]","jsjasonseba (Assginee) on (2024-09-24 16:22:05 UTC): Hi @shahar1, I’d like to take this issue. I plan to create a custom exception inheriting from AirflowException and raise it. Any suggestions?

shahar1 (Issue Creator) on (2024-09-24 16:35:01 UTC): Thanks for your willing to contribute!
I just assigned you this issue.
I'd avoid for now creating a customized exception class, as no other classes in this provider do so.
Please ping me in the PR when you're ready :)

"
2545033593,issue,closed,completed,"Add ""job_clusters"" in template_fields - DatabricksWorkflowTaskGroup","### Description

It would be great if we can allow `CreateDatabricksWorkflowOperator` to support Jinja templating for `job_clusters` attribute. This will allow us to dynamically render cluster configuration at runtime based on the context, such as spark env variables or spark configurations.

https://github.com/apache/airflow/blob/e0bddbc438872277ca8de7fb794285cf546ddc0b/airflow/providers/databricks/operators/databricks_workflow.py#L96

### Use case/motivation

I have an Airflow Dag. In this dag, I am trying to use output of upstream task in `job_clusters` attribute while defining the `DatabricksWorkflowTaskGroup`. In current implementation, output of upstream task doesn't get jinjaified at runtime.

**Sample:**

```

dag = DAG(
    dag_id=""example_databricks_workflow"",
    start_date=datetime(2023, 1, 1),
    schedule=None,
    catchup=False,
    tags=[""example"", ""databricks""],
)
with dag:

 fetch_config = PythonOperator(
        task_id='fetch_config',
        python_callable=fetch_config_func,
        provide_context=True,
    )

  task_group = DatabricksWorkflowTaskGroup(
        group_id=f""test_workflow"",
        job_clusters=[
          {
        ""job_cluster_key"": ""Shared_job_cluster"",
        ""new_cluster"": {
            ""cluster_name"": """",
            ""spark_version"": ""11.3.x-scala2.12"",
            ""aws_attributes"": {
                ...
            },
            ""node_type_id"": ""i3.xlarge"",
            # Pass output of fetch_config task as spark env variables.
            ""spark_env_vars"": {""PYSPARK_PYTHON"": ""/databricks/python3/bin/python3"", ""CONFIG"": f""{fetch_config.output}""},
            ""enable_elastic_disk"": False,
            ""data_security_mode"": ""LEGACY_SINGLE_USER_STANDARD"",
            ""runtime_engine"": ""STANDARD"",
            ""num_workers"": 8,
          }
        ]
    )
    with task_group:
     
        task_operator_nb_1 = DatabricksTaskOperator(
            task_id=""nb_1"",
            databricks_conn_id=""databricks_conn"",
            job_cluster_key=""Shared_job_cluster"",
            task_config={
                ""notebook_task"": {
                    ""notebook_path"": ""/Shared/Notebook_1"",
                    ""source"": ""WORKSPACE"",
                },
                ""libraries"": [
                    {""pypi"": {""package"": ""Faker""}},
                    {""pypi"": {""package"": ""simplejson""}},
                ],
            },
        )

        sql_query = DatabricksTaskOperator(
            task_id=""sql_query"",
            databricks_conn_id=""databricks_conn"",
            task_config={
                ""sql_task"": {
                    ""query"": {
                        ""query_id"": QUERY_ID,
                    },
                    ""warehouse_id"": WAREHOUSE_ID,
                }
            },
        )

        task_operator_nb_1 >> sql_query


fetch_config >> task_group

```

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",sushil-louisa,2024-09-24 10:42:45+00:00,['kunaljubce'],2024-12-18 13:30:05+00:00,2024-12-18 13:30:04+00:00,https://github.com/apache/airflow/issues/42438,"[('area:providers', ''), ('kind:feature', 'Feature Requests'), ('good first issue', ''), ('provider:databricks', '')]","[{'comment_id': 2370908278, 'issue_id': 2545033593, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 24, 10, 42, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453353683, 'issue_id': 2545033593, 'author': 'gopidesupavan', 'body': 'cc: @pankajkoti', 'created_at': datetime.datetime(2024, 11, 3, 9, 12, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2530644713, 'issue_id': 2545033593, 'author': 'kunaljubce', 'body': ""@gopidesupavan @pankajkoti If you're not working on it, I can pick this up."", 'created_at': datetime.datetime(2024, 12, 10, 7, 25, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2530660250, 'issue_id': 2545033593, 'author': 'pankajkoti', 'body': 'Sure @kunaljubce . Assigned it to you, your contribution is most welcome. Please tag me if you would like me to review your PR once ready.', 'created_at': datetime.datetime(2024, 12, 10, 7, 30, 9, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-24 10:42:48 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

gopidesupavan on (2024-11-03 09:12:04 UTC): cc: @pankajkoti

kunaljubce (Assginee) on (2024-12-10 07:25:06 UTC): @gopidesupavan @pankajkoti If you're not working on it, I can pick this up.

pankajkoti on (2024-12-10 07:30:09 UTC): Sure @kunaljubce . Assigned it to you, your contribution is most welcome. Please tag me if you would like me to review your PR once ready.

"
2544389470,issue,closed,completed,Running `db.resetdb()` (e.g. in test suite) modifies `logging.root.level`,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.8.0

### What happened?

Running `airflow.utils.db.resetdb()` (e.g. in test suite) sets `logging.root.level = Logging.WARNING`

It appears to apply the logging config at `airflow/alembic.ini`

### What you think should happen instead?

`resetdb()` as a framework util function for clearing db state, should not modify application root logger config

### How to reproduce

```py
import logging

print(f""{logging.root.level=}"")

logging.root.setLevel(logging.INFO)
print(f""{logging.root.level=}"")

import airflow.utils.db
airflow.utils.db.resetdb()

print(f""{logging.root.level=}"")
```

```sh
mkdir test_airflow_logging
cd test_airflow_logging
python -m venv .venv
. .venv/bin/activate
pip install ""apache-airflow==2.10.1"" --constraint ""https://raw.githubusercontent.com/apache/airflow/constraints-2.10.1/constraints-3.11.txt""
export AIRFLOW_HOME=$(PWD)
python logging_test.py
```

Outputs:

```
logging.root.level=30
logging.root.level=20
[2024-09-23T22:36:21.856-0700] {db.py:1702} INFO - Dropping tables that exist
[2024-09-23T22:36:22.463-0700] {migration.py:215} INFO - Context impl SQLiteImpl.
[2024-09-23T22:36:22.463-0700] {migration.py:218} INFO - Will assume non-transactional DDL.
[2024-09-23T22:36:22.464-0700] {migration.py:215} INFO - Context impl SQLiteImpl.
[2024-09-23T22:36:22.464-0700] {migration.py:218} INFO - Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
INFO  [alembic.runtime.migration] Running stamp_revision  -> 22ed7efa9da2
WARNI [airflow.models.crypto] empty cryptography key - values will not be stored encrypted.
logging.root.level=30
```

### Operating System

Darwin COMPNAME 23.6.0 Darwin Kernel Version 23.6.0: Wed Jul 31 20:49:39 PDT 2024; root:xnu-10063.141.1.700.5~1/RELEASE_ARM64_T6000 arm64

### Versions of Apache Airflow Providers

bare/vanilla Airflow install, see repro

### Deployment

Virtualenv installation

### Deployment details

repro does not use deployment

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Kache,2024-09-24 05:41:56+00:00,[],2024-10-09 00:09:28+00:00,2024-10-09 00:09:28+00:00,https://github.com/apache/airflow/issues/42432,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2371935064, 'issue_id': 2544389470, 'author': 'harjeevanmaan', 'body': 'Working on it.', 'created_at': datetime.datetime(2024, 9, 24, 17, 50, 15, tzinfo=datetime.timezone.utc)}]","harjeevanmaan on (2024-09-24 17:50:15 UTC): Working on it.

"
2544001707,issue,closed,completed,Fix the span link of task instance to point to the correct span in the scheduler_job_loop,"### Apache Airflow version

2.10.1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

# Overview
Currently, the span link that would point to the calling origin (scheduler job loop), points to the root span of the scheduler job loop, rather than the correct span itself, sometimes making it not so obvious for user to locate the related execution point of the span:
<img width=""1275"" alt=""image"" src=""https://github.com/user-attachments/assets/0a661f1a-4193-4551-8440-6df247621570"">



### What you think should happen instead?

What the span link should have pointed to, is not the root span, but rather the actual span where the task instance was called by the scheduler:
<img width=""1276"" alt=""image"" src=""https://github.com/user-attachments/assets/59b88cde-a4a9-4cb7-9ef3-7410cd252285"">



### How to reproduce

- Enable the OTEL traces
- Run a DAG
- Monitor that the taskinstance span's link points to the root span of the scheduler job loop.

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",howardyoo,2024-09-24 00:27:43+00:00,[],2024-09-25 18:12:53+00:00,2024-09-25 18:12:53+00:00,https://github.com/apache/airflow/issues/42429,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('area:core', '')]",[],
2543709742,issue,closed,completed,Support of host.name in OTEL metrics,"### Description

Currently there is no attribute 'host.name' that is part of the metrics data emitted from Airflow, which may cause confusion when monitoring airflow metrics.

### Use case/motivation

In order to more properly identify airflow instances running on a host environment, we may need to have this common sementic covention (host.name) be available. (more info: https://opentelemetry.io/docs/specs/semconv/resource/host/)
Currently, because the metric does not have any attributes, in case there are multiple airflow instances emitting metrics, it may not be easy to distinguish them, unless explicitly defining it.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",howardyoo,2024-09-23 21:31:20+00:00,[],2024-10-01 12:55:00+00:00,2024-10-01 07:04:07+00:00,https://github.com/apache/airflow/issues/42425,"[('kind:feature', 'Feature Requests'), ('telemetry', 'Telemetry-related issues')]","[{'comment_id': 2385707410, 'issue_id': 2543709742, 'author': 'howardyoo', 'body': 'Thank you!', 'created_at': datetime.datetime(2024, 10, 1, 12, 54, 58, tzinfo=datetime.timezone.utc)}]","howardyoo (Issue Creator) on (2024-10-01 12:54:58 UTC): Thank you!

"
2543675483,issue,closed,completed,Support of ResourceAttributes and for OTEL metrics,"### Description

# What this is about
Currently, OTEL metrics does not support OTEL_RESOURCE_ATTRIBUTES for its metrics being instrumented, which can provide additional details regarding the metrics data being emitted from Airflow.

# What is the value
Resource attributes are very helpful in describing the details about what the resource is. User who wants to deploy Airflow and make sure to have a specific resource attributes available would make analyzing opentelemetry metrics helpful. A good example of resource attributes are:
- organization id
- environment
- version information
- any feature flags, or labels


### Use case/motivation

# What needs to be done
When OTEL metrics are being initialized, make sure to check the OTEL_RESOURCE_ATTRIBUTES env. variable, and if exists, add them as the metric's resource attributes, so that it can be emitted along with the metrics data.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",howardyoo,2024-09-23 21:15:20+00:00,[],2024-10-01 07:04:07+00:00,2024-10-01 07:04:07+00:00,https://github.com/apache/airflow/issues/42424,"[('kind:feature', 'Feature Requests'), ('telemetry', 'Telemetry-related issues')]","[{'comment_id': 2373923468, 'issue_id': 2543675483, 'author': 'mxmrlt', 'body': 'The same is needed for OTEL traces.\r\n\r\nIndeed we should be able to use documented OTEL_RESOURCE_ATTRIBUTES environment variable so that we can for instance set and use ResourceAttributes.DEPLOYMENT_ENVIRONMENT.\r\n\r\nThis is mandatory to make it work correctly and consistent when monitoring (Elastic/Kibana/APM for me) moreover when tracing in distributed environnements.\r\n\r\n```python\r\n\r\n# airflow/traces/otel_tracer.py\r\n\r\nclass OtelTrace:\r\n    """"""\r\n    Handle all tracing requirements such as getting the tracer, and starting a new span.\r\n\r\n    When OTEL is enabled, the Trace class will be replaced by this class.\r\n    """"""\r\n\r\n    def __init__(self, span_exporter: ConsoleSpanExporter | OTLPSpanExporter, tag_string: str | None = None):\r\n        self.span_exporter = span_exporter\r\n        self.span_processor = BatchSpanProcessor(self.span_exporter)\r\n        self.tag_string = tag_string\r\n        self.otel_service = conf.get(""traces"", ""otel_service"")\r\n\r\n    def get_tracer(\r\n        self, component: str, trace_id: int | None = None, span_id: int | None = None\r\n    ) -> OpenTelemetryTracer | Tracer:\r\n        """"""Tracer that will use special AirflowOtelIdGenerator to control producing certain span and trace id.""""""\r\n        resource = Resource(\r\n            attributes={\r\n                HOST_NAME: get_hostname(),\r\n                SERVICE_NAME: self.otel_service\r\n                \r\n                # Every other OTEL_RESOURCE_ATTRIBUTES like \'deployment.environment\'\r\n                # but also everything available in opentelemetry/semconv/resource/__init__.py\r\n                \r\n            }\r\n        )\r\n        if trace_id or span_id:\r\n            # in case where trace_id or span_id was given\r\n            tracer_provider = TracerProvider(\r\n                resource=resource, id_generator=AirflowOtelIdGenerator(span_id=span_id, trace_id=trace_id)\r\n            )\r\n        else:\r\n            tracer_provider = TracerProvider(resource=resource)\r\n        tracer_provider.add_span_processor(self.span_processor)\r\n        tracer = tracer_provider.get_tracer(component)\r\n        """"""\r\n        Tracer will produce a single ID value if value is provided. Note that this is one-time only, so any\r\n        subsequent call will produce the normal random ids.\r\n        """"""\r\n        return tracer`', 'created_at': datetime.datetime(2024, 9, 25, 12, 17, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2374027194, 'issue_id': 2543675483, 'author': 'howardyoo', 'body': 'Correct.I have recently created a PR that would do the same to otel traces, and it was reviewed and approved.So hopefully, these will get into the next Airflow release soon.HowardSent from my iPhoneOn Sep 25, 2024, at 7:17\u202fAM, mxmrlt ***@***.***> wrote:\ufeff\r\nThe same is needed for OTEL traces.\r\nIndeed we should be able to use documented OTEL_RESOURCE_ATTRIBUTES environment variable so that we can for instance set and use ResourceAttributes.DEPLOYMENT_ENVIRONMENT.\r\nThis is mandatory to make it work correctly and consistent when monitoring (Elastic/Kibana/APM for me) moreover when tracing in distributed environnements.\r\n# airflow/traces/otel_tracer.py\r\n\r\nclass OtelTrace:\r\n    """"""\r\n    Handle all tracing requirements such as getting the tracer, and starting a new span.\r\n\r\n    When OTEL is enabled, the Trace class will be replaced by this class.\r\n    """"""\r\n\r\n    def __init__(self, span_exporter: ConsoleSpanExporter | OTLPSpanExporter, tag_string: str | None = None):\r\n        self.span_exporter = span_exporter\r\n        self.span_processor = BatchSpanProcessor(self.span_exporter)\r\n        self.tag_string = tag_string\r\n        self.otel_service = conf.get(""traces"", ""otel_service"")\r\n\r\n    def get_tracer(\r\n        self, component: str, trace_id: int | None = None, span_id: int | None = None\r\n    ) -> OpenTelemetryTracer | Tracer:\r\n        """"""Tracer that will use special AirflowOtelIdGenerator to control producing certain span and trace id.""""""\r\n        resource = Resource(\r\n            attributes={\r\n                HOST_NAME: get_hostname(),\r\n                SERVICE_NAME: self.otel_service\r\n                \r\n                # Every other OTEL_RESOURCE_ATTRIBUTES like \'deployment.environment\'\r\n                # but also everything available in opentelemetry/semconv/resource/__init__.py\r\n                \r\n            }\r\n        )\r\n        if trace_id or span_id:\r\n            # in case where trace_id or span_id was given\r\n            tracer_provider = TracerProvider(\r\n                resource=resource, id_generator=AirflowOtelIdGenerator(span_id=span_id, trace_id=trace_id)\r\n            )\r\n        else:\r\n            tracer_provider = TracerProvider(resource=resource)\r\n        tracer_provider.add_span_processor(self.span_processor)\r\n        tracer = tracer_provider.get_tracer(component)\r\n        """"""\r\n        Tracer will produce a single ID value if value is provided. Note that this is one-time only, so any\r\n        subsequent call will produce the normal random ids.\r\n        """"""\r\n        return tracer`\r\n\r\n—Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you authored the thread.Message ID: ***@***.***>', 'created_at': datetime.datetime(2024, 9, 25, 13, 2, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2374103765, 'issue_id': 2543675483, 'author': 'mxmrlt', 'body': ""Glad to read it.\r\n\r\nI see you're the creator of airflow_otel_provider. Do you know if there is a way to auto-instrument our dags so that spans are created and exported for any custom code like requests calls or kafka publishing etc...?\r\nOpenTelemetry should permit that if I'm right (https://opentelemetry.io/docs/zero-code/python/) but perhaps I'm not.\r\n\r\nThis would save us from having to declare manually what you show in the README :\r\n```python\r\nRequestsInstrumentor().instrument(tracer_provider=otel_hook.tracer_provider)\r\n```\r\n\r\nIf you have any other advice on this topic please tell me.\r\n\r\nThank you"", 'created_at': datetime.datetime(2024, 9, 25, 13, 33, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2374438262, 'issue_id': 2543675483, 'author': 'howardyoo', 'body': ""Auto-instrumentation is a tricky(?) area, especially trying to instrument a\r\ncomplex system like Airflow (It does work extremely well with smaller\r\napplications like microservices, where that was the area the\r\nauto-instrumentation usually focuses on). Technically, I would say the\r\ninstrumenting the whole Airflow using otel auto-instrumentation would work,\r\nbut you may have to do it on your own risk.\r\n\r\nI would be a little worried if we did that because that would introduce a\r\nhuge amount of telemetry data (I know how much Airflow calls database\r\nqueries just to keep it idly running), and also potential impact on its\r\nperformance has not been greatly studied.\r\n\r\nSo, when I was implementing the AIP-49 (the otel traces for Airflow), I\r\npurposely scoped out the auto-instrumentation aspects.\r\n\r\nHowever, if there's a good reason / value / need to provide certain level\r\nof enabling auto-instrumentation in terms of running operators (e.g. python\r\noperators), that may be a good discussion to start with..\r\n\r\nSo, generally, the Airflow community welcomes contributions of any types as\r\nlong as those contributions have enough support (things are voted and\r\napproved), and the discussion has been made enough. If something sounds\r\nlike a good idea (or you have found something) - please share with the\r\ncommunity and then it can happen as an implementation work!\r\n\r\nRegards,\r\nHoward\r\n\r\nOn Wed, Sep 25, 2024 at 8:33\u202fAM mxmrlt ***@***.***> wrote:\r\n\r\n> Glad to read it.\r\n>\r\n> I see you're the creator of airflow_otel_provider. Do you know if there is\r\n> a way to auto-instrument our dags so that spans are created and exported\r\n> for any custom code like requests calls or kafka publishing etc...?\r\n> OpenTelemetry should permit that if I'm right (\r\n> https://opentelemetry.io/docs/zero-code/python/) but perhaps I'm not.\r\n>\r\n> This would save us from having to declare manually what you show in the\r\n> README :\r\n>\r\n> RequestsInstrumentor().instrument(tracer_provider=otel_hook.tracer_provider)\r\n>\r\n> If you have any other advice on this topic please tell me.\r\n>\r\n> Thank you\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/apache/airflow/issues/42424#issuecomment-2374103765>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AHZNLLQJY55TV6IDKRJRMD3ZYK3UJAVCNFSM6AAAAABOW5EZVOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGNZUGEYDGNZWGU>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>"", 'created_at': datetime.datetime(2024, 9, 25, 15, 36, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2374442057, 'issue_id': 2543675483, 'author': 'howardyoo', 'body': ""Your question kind of gave me some ideas, so maybe the otel airflow\r\nprovider could help making it easier for users to enable certain\r\ninstrumentations... but that would be something.\r\n\r\nOn Wed, Sep 25, 2024 at 10:35\u202fAM Howard Yoo ***@***.***> wrote:\r\n\r\n> Auto-instrumentation is a tricky(?) area, especially trying to instrument\r\n> a complex system like Airflow (It does work extremely well with smaller\r\n> applications like microservices, where that was the area the\r\n> auto-instrumentation usually focuses on). Technically, I would say the\r\n> instrumenting the whole Airflow using otel auto-instrumentation would work,\r\n> but you may have to do it on your own risk.\r\n>\r\n> I would be a little worried if we did that because that would introduce a\r\n> huge amount of telemetry data (I know how much Airflow calls database\r\n> queries just to keep it idly running), and also potential impact on its\r\n> performance has not been greatly studied.\r\n>\r\n> So, when I was implementing the AIP-49 (the otel traces for Airflow), I\r\n> purposely scoped out the auto-instrumentation aspects.\r\n>\r\n> However, if there's a good reason / value / need to provide certain level\r\n> of enabling auto-instrumentation in terms of running operators (e.g. python\r\n> operators), that may be a good discussion to start with..\r\n>\r\n> So, generally, the Airflow community welcomes contributions of any types\r\n> as long as those contributions have enough support (things are voted and\r\n> approved), and the discussion has been made enough. If something sounds\r\n> like a good idea (or you have found something) - please share with the\r\n> community and then it can happen as an implementation work!\r\n>\r\n> Regards,\r\n> Howard\r\n>\r\n> On Wed, Sep 25, 2024 at 8:33\u202fAM mxmrlt ***@***.***> wrote:\r\n>\r\n>> Glad to read it.\r\n>>\r\n>> I see you're the creator of airflow_otel_provider. Do you know if there\r\n>> is a way to auto-instrument our dags so that spans are created and exported\r\n>> for any custom code like requests calls or kafka publishing etc...?\r\n>> OpenTelemetry should permit that if I'm right (\r\n>> https://opentelemetry.io/docs/zero-code/python/) but perhaps I'm not.\r\n>>\r\n>> This would save us from having to declare manually what you show in the\r\n>> README :\r\n>>\r\n>> RequestsInstrumentor().instrument(tracer_provider=otel_hook.tracer_provider)\r\n>>\r\n>> If you have any other advice on this topic please tell me.\r\n>>\r\n>> Thank you\r\n>>\r\n>> —\r\n>> Reply to this email directly, view it on GitHub\r\n>> <https://github.com/apache/airflow/issues/42424#issuecomment-2374103765>,\r\n>> or unsubscribe\r\n>> <https://github.com/notifications/unsubscribe-auth/AHZNLLQJY55TV6IDKRJRMD3ZYK3UJAVCNFSM6AAAAABOW5EZVOVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGNZUGEYDGNZWGU>\r\n>> .\r\n>> You are receiving this because you authored the thread.Message ID:\r\n>> ***@***.***>\r\n>>\r\n>"", 'created_at': datetime.datetime(2024, 9, 25, 15, 37, 44, tzinfo=datetime.timezone.utc)}]","mxmrlt on (2024-09-25 12:17:11 UTC): The same is needed for OTEL traces.

Indeed we should be able to use documented OTEL_RESOURCE_ATTRIBUTES environment variable so that we can for instance set and use ResourceAttributes.DEPLOYMENT_ENVIRONMENT.

This is mandatory to make it work correctly and consistent when monitoring (Elastic/Kibana/APM for me) moreover when tracing in distributed environnements.

```python

# airflow/traces/otel_tracer.py

class OtelTrace:
    """"""
    Handle all tracing requirements such as getting the tracer, and starting a new span.

    When OTEL is enabled, the Trace class will be replaced by this class.
    """"""

    def __init__(self, span_exporter: ConsoleSpanExporter | OTLPSpanExporter, tag_string: str | None = None):
        self.span_exporter = span_exporter
        self.span_processor = BatchSpanProcessor(self.span_exporter)
        self.tag_string = tag_string
        self.otel_service = conf.get(""traces"", ""otel_service"")

    def get_tracer(
        self, component: str, trace_id: int | None = None, span_id: int | None = None
    ) -> OpenTelemetryTracer | Tracer:
        """"""Tracer that will use special AirflowOtelIdGenerator to control producing certain span and trace id.""""""
        resource = Resource(
            attributes={
                HOST_NAME: get_hostname(),
                SERVICE_NAME: self.otel_service
                
                # Every other OTEL_RESOURCE_ATTRIBUTES like 'deployment.environment'
                # but also everything available in opentelemetry/semconv/resource/__init__.py
                
            }
        )
        if trace_id or span_id:
            # in case where trace_id or span_id was given
            tracer_provider = TracerProvider(
                resource=resource, id_generator=AirflowOtelIdGenerator(span_id=span_id, trace_id=trace_id)
            )
        else:
            tracer_provider = TracerProvider(resource=resource)
        tracer_provider.add_span_processor(self.span_processor)
        tracer = tracer_provider.get_tracer(component)
        """"""
        Tracer will produce a single ID value if value is provided. Note that this is one-time only, so any
        subsequent call will produce the normal random ids.
        """"""
        return tracer`

howardyoo (Issue Creator) on (2024-09-25 13:02:07 UTC): Correct.I have recently created a PR that would do the same to otel traces, and it was reviewed and approved.So hopefully, these will get into the next Airflow release soon.HowardSent from my iPhoneOn Sep 25, 2024, at 7:17 AM, mxmrlt ***@***.***> wrote:﻿
The same is needed for OTEL traces.
Indeed we should be able to use documented OTEL_RESOURCE_ATTRIBUTES environment variable so that we can for instance set and use ResourceAttributes.DEPLOYMENT_ENVIRONMENT.
This is mandatory to make it work correctly and consistent when monitoring (Elastic/Kibana/APM for me) moreover when tracing in distributed environnements.
# airflow/traces/otel_tracer.py

class OtelTrace:
    """"""
    Handle all tracing requirements such as getting the tracer, and starting a new span.

    When OTEL is enabled, the Trace class will be replaced by this class.
    """"""

    def __init__(self, span_exporter: ConsoleSpanExporter | OTLPSpanExporter, tag_string: str | None = None):
        self.span_exporter = span_exporter
        self.span_processor = BatchSpanProcessor(self.span_exporter)
        self.tag_string = tag_string
        self.otel_service = conf.get(""traces"", ""otel_service"")

    def get_tracer(
        self, component: str, trace_id: int | None = None, span_id: int | None = None
    ) -> OpenTelemetryTracer | Tracer:
        """"""Tracer that will use special AirflowOtelIdGenerator to control producing certain span and trace id.""""""
        resource = Resource(
            attributes={
                HOST_NAME: get_hostname(),
                SERVICE_NAME: self.otel_service
                
                # Every other OTEL_RESOURCE_ATTRIBUTES like 'deployment.environment'
                # but also everything available in opentelemetry/semconv/resource/__init__.py
                
            }
        )
        if trace_id or span_id:
            # in case where trace_id or span_id was given
            tracer_provider = TracerProvider(
                resource=resource, id_generator=AirflowOtelIdGenerator(span_id=span_id, trace_id=trace_id)
            )
        else:
            tracer_provider = TracerProvider(resource=resource)
        tracer_provider.add_span_processor(self.span_processor)
        tracer = tracer_provider.get_tracer(component)
        """"""
        Tracer will produce a single ID value if value is provided. Note that this is one-time only, so any
        subsequent call will produce the normal random ids.
        """"""
        return tracer`

—Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you authored the thread.Message ID: ***@***.***>

mxmrlt on (2024-09-25 13:33:35 UTC): Glad to read it.

I see you're the creator of airflow_otel_provider. Do you know if there is a way to auto-instrument our dags so that spans are created and exported for any custom code like requests calls or kafka publishing etc...?
OpenTelemetry should permit that if I'm right (https://opentelemetry.io/docs/zero-code/python/) but perhaps I'm not.

This would save us from having to declare manually what you show in the README :
```python
RequestsInstrumentor().instrument(tracer_provider=otel_hook.tracer_provider)
```

If you have any other advice on this topic please tell me.

Thank you

howardyoo (Issue Creator) on (2024-09-25 15:36:06 UTC): Auto-instrumentation is a tricky(?) area, especially trying to instrument a
complex system like Airflow (It does work extremely well with smaller
applications like microservices, where that was the area the
auto-instrumentation usually focuses on). Technically, I would say the
instrumenting the whole Airflow using otel auto-instrumentation would work,
but you may have to do it on your own risk.

I would be a little worried if we did that because that would introduce a
huge amount of telemetry data (I know how much Airflow calls database
queries just to keep it idly running), and also potential impact on its
performance has not been greatly studied.

So, when I was implementing the AIP-49 (the otel traces for Airflow), I
purposely scoped out the auto-instrumentation aspects.

However, if there's a good reason / value / need to provide certain level
of enabling auto-instrumentation in terms of running operators (e.g. python
operators), that may be a good discussion to start with..

So, generally, the Airflow community welcomes contributions of any types as
long as those contributions have enough support (things are voted and
approved), and the discussion has been made enough. If something sounds
like a good idea (or you have found something) - please share with the
community and then it can happen as an implementation work!

Regards,
Howard

On Wed, Sep 25, 2024 at 8:33 AM mxmrlt ***@***.***> wrote:

howardyoo (Issue Creator) on (2024-09-25 15:37:44 UTC): Your question kind of gave me some ideas, so maybe the otel airflow
provider could help making it easier for users to enable certain
instrumentations... but that would be something.

On Wed, Sep 25, 2024 at 10:35 AM Howard Yoo ***@***.***> wrote:

"
2543261672,issue,closed,not_planned,Redact secrets from external secrets provider,"### Apache Airflow version

2.10.1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When using a third party secrets backend (e.g. AWS Secrets Manager), secret values are not redacted from task logs and appear as plaintext in the logs.

### What you think should happen instead?

When writing task logs, the worker should list all the secret values from the backend and add that to the list of things to redact.

### How to reproduce

Connect a secrets backend e.g. [AWS Secrets Manager](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/_api/airflow/providers/amazon/aws/secrets/secrets_manager/index.html#module-airflow.providers.amazon.aws.secrets.secrets_manager)
Print a value from the backend - the plaintext will appear in the task logs


### Operating System

N/A

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",stuart23,2024-09-23 17:39:58+00:00,[],2024-10-25 00:15:27+00:00,2024-10-25 00:15:26+00:00,https://github.com/apache/airflow/issues/42420,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('provider:amazon', 'AWS/Amazon - related issues'), ('security', 'Security issues that must be fixed'), ('pending-response', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2374321451, 'issue_id': 2543261672, 'author': 'wseaton', 'body': 'Not just secret values, it is also important that bootstrap variables for secrets manager configuration like `AIRFLOW__SECRETS__BACKEND_KWARGS` can be masked in task logs, since they are visible to the workers.', 'created_at': datetime.datetime(2024, 9, 25, 14, 50, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2387486532, 'issue_id': 2543261672, 'author': 'potiuk', 'body': 'Could. you please clarify and show examples of what values you are talking about ? Airflow does not mask ""ALL"" values retrieved from secret backends only those that are considered ""sensitive"" - see https://airflow.apache.org/docs/apache-airflow/stable/security/secrets/mask-sensitive-values.html. In many cases Secrets backend are used to keep non-sensitive configuration and those non-sensitive values are not masked.\r\n\r\nSo examples and explanation why things should be masked and aren\'t ?', 'created_at': datetime.datetime(2024, 10, 2, 1, 59, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2418196570, 'issue_id': 2543261672, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 10, 17, 0, 15, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2436555307, 'issue_id': 2543261672, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 10, 25, 0, 15, 26, tzinfo=datetime.timezone.utc)}]","wseaton on (2024-09-25 14:50:22 UTC): Not just secret values, it is also important that bootstrap variables for secrets manager configuration like `AIRFLOW__SECRETS__BACKEND_KWARGS` can be masked in task logs, since they are visible to the workers.

potiuk on (2024-10-02 01:59:20 UTC): Could. you please clarify and show examples of what values you are talking about ? Airflow does not mask ""ALL"" values retrieved from secret backends only those that are considered ""sensitive"" - see https://airflow.apache.org/docs/apache-airflow/stable/security/secrets/mask-sensitive-values.html. In many cases Secrets backend are used to keep non-sensitive configuration and those non-sensitive values are not masked.

So examples and explanation why things should be masked and aren't ?

github-actions[bot] on (2024-10-17 00:15:17 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-10-25 00:15:26 UTC): This issue has been closed because it has not received response from the issue author.

"
2542155785,issue,closed,completed,Add SageMakerProcessingSensor,"### Description

Hi,
we would like to switch our Sagemaker pipelines to Airflow pipelines.
There are already many useful operators and sensors - however, the sensor for processing jobs is missing. Could this be added?

All current sensors are listed here: https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/_api/airflow/providers/amazon/aws/sensors/sagemaker/index.html

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",anneadb,2024-09-23 10:05:46+00:00,[],2024-10-18 15:19:43+00:00,2024-10-18 13:27:55+00:00,https://github.com/apache/airflow/issues/42411,"[('provider:amazon', 'AWS/Amazon - related issues'), ('kind:feature', 'Feature Requests')]","[{'comment_id': 2421632794, 'issue_id': 2542155785, 'author': 'vVv-AA', 'body': '@Lee-W Have raised PR for this. Please review when you can.\r\nhttps://github.com/apache/airflow/pull/43144', 'created_at': datetime.datetime(2024, 10, 18, 7, 21, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2422644723, 'issue_id': 2542155785, 'author': 'anneadb', 'body': 'Thank you so much for implementing this!\r\nHow does the release process usually work, will this be included in the next minor automatically?', 'created_at': datetime.datetime(2024, 10, 18, 14, 44, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2422716171, 'issue_id': 2542155785, 'author': 'vincbeck', 'body': 'Yes, in the next providers release', 'created_at': datetime.datetime(2024, 10, 18, 15, 19, 41, tzinfo=datetime.timezone.utc)}]","vVv-AA on (2024-10-18 07:21:54 UTC): @Lee-W Have raised PR for this. Please review when you can.
https://github.com/apache/airflow/pull/43144

anneadb (Issue Creator) on (2024-10-18 14:44:09 UTC): Thank you so much for implementing this!
How does the release process usually work, will this be included in the next minor automatically?

vincbeck on (2024-10-18 15:19:41 UTC): Yes, in the next providers release

"
2541514696,issue,closed,completed,"Add the Git Clone command for this project to the first line of the ""Installation"" section in the ""Installation"" chapter.","### What do you see as an issue?

[01_installation.rst](https://github.com/apache/airflow/blob/main/dev/breeze/doc/01_installation.rst)

As beginners, it's easy to assume that commands like `pip install ....` or `pipx install ....` will handle everything needed, without realizing that cloning the project is required first, and we find out clone in the home directory will cause some trouble so I think we can add git clone and the warning at doc.


### Solving the problem

Add `git clone https://github.com/apache/airflow.git` and `cd airflow` to the first line of the 'Installation' section in the 'Installation' chapter.

Additionally, include a warning to ensure users are not in the home directory when running these commands.

Adding this step at the beginning of the section will help prevent any confusion.

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",SamLiaoP,2024-09-23 03:54:11+00:00,['SamLiaoP'],2024-09-23 13:43:11+00:00,2024-09-23 13:43:11+00:00,https://github.com/apache/airflow/issues/42402,"[('good first issue', ''), ('kind:documentation', ''), ('contributors-workshop', ""Issues that are good for first-time contributor's workshop""), ('type:doc-only', 'Changelog: Doc Only')]","[{'comment_id': 2367186972, 'issue_id': 2541514696, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 23, 3, 54, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2367187140, 'issue_id': 2541514696, 'author': 'SamLiaoP', 'body': '@lee-w', 'created_at': datetime.datetime(2024, 9, 23, 3, 54, 28, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-23 03:54:13 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

SamLiaoP (Issue Creator) on (2024-09-23 03:54:28 UTC): @lee-w

"
2541259011,issue,open,,EmrServerlessStartJobOperator does not cancel EMR Serverless job when waiter_max_attempts is reached,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.8.1

### What happened?

When using the EmrServerlessStartJobOperator with wait_for_completion=True, and specifying waiter_delay and waiter_max_attempts, the EMR Serverless job is not canceled when the maximum waiter attempts are reached. Instead, the Airflow task fails and a new task instance is started due to retries being configured. This results in multiple EMR Serverless jobs running concurrently, as the original job continues to run even after the Airflow task has failed and retried.

### What you think should happen instead?

When the waiter_max_attempts limit is reached, the EMR Serverless job should be automatically canceled as a result of this event, before the Airflow task proceeds to a retry. This ensures that upon retrying, Airflow starts a new EMR Serverless job, and only one job is active at any given time

### How to reproduce

- Create an Airflow DAG with a task using EmrServerlessStartJobOperator

- Configure the operator with wait_for_completion=True, and set waiter_delay and waiter_max_attempts to values that will cause a timeout before the job completes

- Use a dummy Spark job that runs longer than the total wait time (waiter_delay * waiter_max_attempts)

- Configure the Airflow task to have retries (e.g., retries=2)

- Run the DAG

- Observe that when the waiter_max_attempts limit is reached, the Airflow task fails and retries, starting a new EMR Serverless job while the previous job continues to run

### Operating System

Amazon Linux 2023

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.16.0

### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",monometa,2024-09-22 21:10:40+00:00,[],2024-12-10 11:15:45+00:00,,https://github.com/apache/airflow/issues/42401,"[('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('kind:feature', 'Feature Requests'), ('good first issue', '')]","[{'comment_id': 2366966343, 'issue_id': 2541259011, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 22, 21, 10, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2400979055, 'issue_id': 2541259011, 'author': 'jayceslesar', 'body': 'Do you ever see https://github.com/apache/airflow/blob/main/airflow/providers/amazon/aws/operators/emr.py#L1272 (""Unable to request query cancel on EMR Serverless. Exiting"") in the logs?', 'created_at': datetime.datetime(2024, 10, 8, 23, 20, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2528808047, 'issue_id': 2541259011, 'author': 'gopidesupavan', 'body': 'Yes this is the current behaviour of the operator. you may add support for this :)', 'created_at': datetime.datetime(2024, 12, 9, 17, 24, 3, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-22 21:10:43 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jayceslesar on (2024-10-08 23:20:42 UTC): Do you ever see https://github.com/apache/airflow/blob/main/airflow/providers/amazon/aws/operators/emr.py#L1272 (""Unable to request query cancel on EMR Serverless. Exiting"") in the logs?

gopidesupavan on (2024-12-09 17:24:03 UTC): Yes this is the current behaviour of the operator. you may add support for this :)

"
2540528758,issue,open,,BeamRunPythonPipelineOperator doesn't work with Google Application Default Credentials ADC,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.6.3 (problem occurs in latest version as well, will try to download latest and post log as well)

### What happened?

When manually submitting an Apache Beam Python job to Google Dataflow runner using BeamRunPythonPipelineOperator
- without specifying as operator parameter `BeamRunPythonPipelineOperator.pipeline_options.service_account_name`
- without having any Google Auth explicit environment variable set `GOOGLE_APPLICATION_CREDENTIALS`
- without having any Google Auth explicit environment variable set `GCP_PROJECT`
- with previously executing Google Auth `gcloud auth application-default login --disable-quota-project`
- with previously executing Google Auth `gcloud auth login`
- with previously executing Google Auth `gcloud config set project <project>`
- with using default airflow gcp connection `google_cloud_default` with following content 
  ```json
  {
    ""conn_type"": ""google_cloud_platform"", 
    ""extra"": {
      ""scope"": ""https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/drive,https://www.googleapis.com/auth/bigquery"", 
      ""project"": ""<project>"", 
      ""num_retries"": 5
    }
  }
  ```
- with specifying `BeamRunPythonPipelineOperator.DataflowConfiguration.project_id`

Task gets stuck in `apitools` that use **oauth2client**
- https://github.com/google/apitools/blob/v0.5.32/apitools/base/py/credentials_lib.py#L153-L154
    - https://github.com/google/apitools/blob/v0.5.32/apitools/base/py/credentials_lib.py#L551-L552


and try initiative browser Google sign-in which must fail. I don't understand why the authentication flow ends in that execution branch since the credential of type `authorized_user`exist in the well-known path  `~/.config/gcloud/application_default_credentials.json`.

```
[2024-09-21, 21:05:58 UTC] {taskinstance.py:1328} INFO - Executing <Task(BeamRunPythonPipelineOperator): submit_beam_job> on 2022-01-01 00:00:00+00:00
[2024-09-21, 21:05:58 UTC] {standard_task_runner.py:57} INFO - Started process 2105 to run task
[2024-09-21, 21:05:58 UTC] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'dag-example-beam-dataflow-python', 'submit_beam_job', 'scheduled__2022-01-01T00:00:00+00:00', '--job-id', '716', '--raw', '--subdir', 'DAGS_FOLDER/dag_example_beam_dataflow_python/dag_example_beam_dataflow_python.py', '--cfg-path', '/tmp/tmprcjz83eb']
[2024-09-21, 21:05:58 UTC] {standard_task_runner.py:85} INFO - Job 716: Subtask submit_beam_job
[2024-09-21, 21:05:58 UTC] {task_command.py:414} INFO - Running <TaskInstance: dag-example-beam-dataflow-python.submit_beam_job scheduled__2022-01-01T00:00:00+00:00 [running]> on host 9743bdb39e14
[2024-09-21, 21:05:58 UTC] {taskinstance.py:1547} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='dag-example-beam-dataflow-python' AIRFLOW_CTX_TASK_ID='submit_beam_job' AIRFLOW_CTX_EXECUTION_DATE='2022-01-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='3' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2022-01-01T00:00:00+00:00'
[2024-09-21, 21:05:58 UTC] {crypto.py:83} WARNING - empty cryptography key - values will not be stored encrypted.

[2024-09-21, 21:05:58 UTC] {base.py:73} INFO - Using connection ID 'google_cloud_default' for task execution.
[2024-09-21, 21:05:58 UTC] {beam.py:198} INFO - {'job_name': 'simple-beam-job-7d346a20', 'project': 'XXX', 'region': 'europe-west1', 'labels': {'airflow-version': 'v2-6-3-composer'}}
[2024-09-21, 21:05:59 UTC] {base.py:73} INFO - Using connection ID 'google_cloud_default' for task execution.
[2024-09-21, 21:05:59 UTC] {credentials_provider.py:353} INFO - Getting connection using `google.auth.default()` since no explicit credentials are provided.
[2024-09-21, 21:05:59 UTC] {logging_mixin.py:150} WARNING - /opt/python3.8/lib/python3.8/site-packages/google/auth/_default.py:78 UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a ""quota exceeded"" or ""API not enabled"" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds.

[2024-09-21, 21:06:05 UTC] {beam.py:271} INFO - Beam version: 2.50.0
[2024-09-21, 21:06:05 UTC] {beam.py:131} INFO - Running command: python3 /home/airflow/gcs/dags/dag_example_beam_dataflow_python/src/beam/job_example_beam_dataflow_python.py --runner=DataflowRunner --job_name=simple-beam-job-7d346a20 --project=XXX --region=europe-west1 --labels=airflow-version=v2-6-3-composer --worker_machine_type=n1-standard-1 --disk_size_gb=10 --num_workers=1
[2024-09-21, 21:06:05 UTC] {beam.py:142} INFO - Start waiting for Apache Beam process to complete.
[2024-09-21, 21:06:07 UTC] {beam.py:113} INFO - 0

[2024-09-21, 21:06:10 UTC] {beam.py:113} INFO - Generating new OAuth credentials ...
[2024-09-21, 21:06:10 UTC] {beam.py:113} INFO -
[2024-09-21, 21:06:10 UTC] {beam.py:113} INFO - Your browser has been opened to visit:
[2024-09-21, 21:06:10 UTC] {beam.py:113} INFO -
[2024-09-21, 21:06:10 UTC] {beam.py:113} INFO -     https://accounts.google.com/o/oauth2/v2/auth?client_id=XXXX.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8090%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute.readonly+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email&access_type=offline&response_type=code
[2024-09-21, 21:06:10 UTC] {beam.py:113} INFO -
[2024-09-21, 21:06:10 UTC] {beam.py:113} INFO - If your browser is on a different machine then exit and re-run this
[2024-09-21, 21:06:10 UTC] {beam.py:113} INFO - application with the command-line parameter
[2024-09-21, 21:06:10 UTC] {beam.py:113} INFO -
[2024-09-21, 21:06:10 UTC] {beam.py:113} INFO -   --noauth_local_webserver
[2024-09-21, 21:06:10 UTC] {beam.py:113} INFO -
```

### What you think should happen instead?

Airflow should submit job to Dataflow using Application Default Credentials the same way standalone Apache Beam Python (without Airflow) submits the job to Dataflow does.

I see that Apache Beam already solved that problem https://github.com/apache/beam/pull/15004, hence running Apache Beam Python without Airflow using ADC works.

### How to reproduce

Prepare env. variables:

```shell
unset GOOGLE_APPLICATION_CREDENTIALS
unset GCP_PROJECT
gcloud auth application-default login
gcloud config set project <project>
```

and execute the following DAG

```py
# -*- coding: utf-8 -*-
import os
from datetime import datetime
from airflow.models import DAG

from airflow.providers.apache.beam.operators.beam import (
    BeamRunPythonPipelineOperator,
)
from airflow.operators.empty import EmptyOperator
from airflow.providers.google.cloud.operators.dataflow import (
    DataflowConfiguration,
)

current_path = os.path.dirname(__file__)

with DAG(
    dag_id=""dag-example-beam-dataflow-python-adc"",
    default_args={""owner"": ""airflow""},
    start_date=datetime(2024, 1, 1),
    schedule_interval=""@once"",
    catchup=True,
) as dag:
    start_dag = EmptyOperator(task_id=""start_dag"")
    end_dag = EmptyOperator(task_id=""end_dag"")

    submit_beam_job = BeamRunPythonPipelineOperator(
        task_id=""submit_beam_job_with_dataflow_using_adc"",
        py_file=os.path.join(""job_example_beam_dataflow_python_adc.py""),
        runner=""DataflowRunner"",
        pipeline_options={
            ""temp_location"": ""<bucket>"",
            ""staging_location"":  ""<bucket>"",
        },
        dataflow_config=DataflowConfiguration(
            job_name=""submit_beam_job_with_dataflow_using_adc"",
            project_id=""<project_id>"",
            location=""<location>"",
            wait_until_finished=True,
        ),
        do_xcom_push=True,
    )

    start_dag >> submit_beam_job >> end_dag
```

and Apache Beam job source code `job_example_beam_dataflow_python_adc.py`
```python
# -*- coding: utf-8 -*-
import argparse
import logging
import apache_beam as beam
from apache_beam import Create
from apache_beam.options.pipeline_options import PipelineOptions, SetupOptions


class PrintElementDoFn(beam.DoFn):
    def process(self, element, *args, **kwargs):
        print(f""Processing element {element}."")


def run(argv=None):
    parser = argparse.ArgumentParser()

    known_args, pipeline_args = parser.parse_known_args(argv)
    print(known_args.sleep)

    pipeline_options = PipelineOptions(pipeline_args)
    pipeline_options.view_as(SetupOptions).save_main_session = True
    p = beam.Pipeline(options=pipeline_options)

    p | ""create dummy events"" >> Create([1]) | ""print dummy elements"" >> beam.ParDo(
        PrintElementDoFn()
    )

    p.run()


if __name__ == ""__main__"":
    logging.getLogger().setLevel(logging.DEBUG)
    run()
```

### Operating System

macOS 14.7

### Versions of Apache Airflow Providers

apache-airflow-providers-apache-beam==5.3.0

### Deployment

Docker Airflow / Composer Airflow (doesn't matter, problem occurs in latest version as well).

### Deployment details

- composer-2.5.1-airflow-2.6.3
- python 3.8.12

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",fpopic,2024-09-21 22:02:21+00:00,['MaksYermak'],2025-01-08 14:21:37+00:00,,https://github.com/apache/airflow/issues/42396,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('area:core', ''), ('provider:apache-beam', '')]","[{'comment_id': 2393598787, 'issue_id': 2540528758, 'author': 'Lee-W', 'body': 'Hi @fpopic , as you checked ""Yes I am willing to submit a PR!"", I\'ll assign this to you. But please let us know if you no longer interested in it. Thanks!', 'created_at': datetime.datetime(2024, 10, 4, 12, 29, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2393611846, 'issue_id': 2540528758, 'author': 'fpopic', 'body': 'I removed the checkmark, would appreciate help.', 'created_at': datetime.datetime(2024, 10, 4, 12, 37, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2424793223, 'issue_id': 2540528758, 'author': 'fpopic', 'body': 'Adding minimal change to latest setup to reproduce\r\n\r\nIn `docker-compose.yml` passing host ADC config with write permission (to refresh host token from within container) \r\n\r\n```yaml\r\nx-airflow-common:\r\n  &airflow-common\r\n  #...\r\n  build: .\r\n  #image: ...\r\n  environment:\r\n    &airflow-common-env\r\n    # ...\r\n    AIRFLOW__LOGGING__LOGGING_LEVEL: \'WARNING\'\r\n    AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: \'{ ""conn_type"": ""google_cloud_platform"", ""extra"": { ""scope"": ""https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/drive,https://www.googleapis.com/auth/bigquery"",  ""num_retries"": 0 } }\'\r\n    GOOGLE_CLOUD_PROJECT: <my-project>\r\n\r\n  volumes: &airflow-common-volumes\r\n    # ...\r\n    # Use ADC\r\n    - ~/.config/gcloud/:/home/airflow/.config/gcloud:rw\r\n...\r\n```\r\n\r\nIn Dockerfile install google cloud SDK (`gcloud` command) since google base hook requires gcloud CLI it:\r\nhttps://github.com/apache/airflow/blob/2.10.2/airflow/providers/google/common/hooks/base_google.py#L638-L669\r\n\r\n```Dockerfile\r\n# specifying explicit python 3.9 otherwise gcloud sdk installation complains\r\nFROM apache/airflow:2.10.2-python3.9\r\nUSER root\r\n\r\n## COPY FROM https://github.com/apache/airflow/blob/main/docs/docker-stack/docker-images-recipes/gcloud.Dockerfile\r\nARG CLOUD_SDK_VERSION=322.0.0\r\nENV GCLOUD_HOME=/home/airflow/google-cloud-sdk\r\nENV PATH=""${GCLOUD_HOME}/bin/:${PATH}""\r\n\r\nUSER airflow\r\n\r\nRUN DOWNLOAD_URL=""https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-${CLOUD_SDK_VERSION}-linux-x86_64.tar.gz"" \\\r\n    && TMP_DIR=""$(mktemp -d)"" \\\r\n    && curl -fL ""${DOWNLOAD_URL}"" --output ""${TMP_DIR}/google-cloud-sdk.tar.gz"" \\\r\n    && mkdir -p ""${GCLOUD_HOME}"" \\\r\n    && tar xzf ""${TMP_DIR}/google-cloud-sdk.tar.gz"" -C ""${GCLOUD_HOME}"" --strip-components=1 \\\r\n    && ""${GCLOUD_HOME}/install.sh"" \\\r\n       --bash-completion=false \\\r\n       --path-update=false \\\r\n       --usage-reporting=false \\\r\n       --additional-components alpha beta kubectl \\\r\n       --quiet \\\r\n    && rm -rf ""${TMP_DIR}"" \\\r\n    && rm -rf ""${GCLOUD_HOME}/.install/.backup/"" \\\r\n    && gcloud --version\r\n# END OF COPY\r\n\r\n# pip install airflow has to be run with USER airflow\r\nRUN pip install \'apache-airflow==2.10.2\' \\\r\n    apache-airflow[google_auth] \\\r\n    apache-airflow-providers-google \\\r\n    apache-airflow[google] \\\r\n    apache-airflow-providers-apache-beam\r\n\r\nRUN mkdir -p /home/airflow/.config/gcloud/\r\n```', 'created_at': datetime.datetime(2024, 10, 20, 9, 51, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453361877, 'issue_id': 2540528758, 'author': 'eladkal', 'body': 'cc @VladaZakharova', 'created_at': datetime.datetime(2024, 11, 3, 9, 42, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2509609370, 'issue_id': 2540528758, 'author': 'SuccessMoses', 'body': '@eladkal can you assign me?', 'created_at': datetime.datetime(2024, 12, 1, 7, 15, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2509627898, 'issue_id': 2540528758, 'author': 'eladkal', 'body': '@SuccessMoses assigned', 'created_at': datetime.datetime(2024, 12, 1, 8, 14, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2541191660, 'issue_id': 2540528758, 'author': 'VladaZakharova', 'body': 'Hi @SuccessMoses ! Do you have any progress in resolving this issue?', 'created_at': datetime.datetime(2024, 12, 13, 10, 59, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2543038082, 'issue_id': 2540528758, 'author': 'SuccessMoses', 'body': 'I was not able to reproduce this in version 3, sorry.', 'created_at': datetime.datetime(2024, 12, 14, 10, 5, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2545342506, 'issue_id': 2540528758, 'author': 'VladaZakharova', 'body': 'Than I think we can take this from here, okay?\r\n@eladkal please assign this one to @MaksYermak', 'created_at': datetime.datetime(2024, 12, 16, 11, 18, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2545456317, 'issue_id': 2540528758, 'author': 'eladkal', 'body': ""GitHub doesn't allow to assign tasks for users who didn't comment on the issue :)\r\nSo @MaksYermak need to comment and ask for this only then his name appear in the drop list"", 'created_at': datetime.datetime(2024, 12, 16, 12, 7, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2545661982, 'issue_id': 2540528758, 'author': 'MaksYermak', 'body': 'Hello @eladkal !\r\nCould you please assign this issue to me?', 'created_at': datetime.datetime(2024, 12, 16, 13, 40, 31, tzinfo=datetime.timezone.utc)}]","Lee-W on (2024-10-04 12:29:56 UTC): Hi @fpopic , as you checked ""Yes I am willing to submit a PR!"", I'll assign this to you. But please let us know if you no longer interested in it. Thanks!

fpopic (Issue Creator) on (2024-10-04 12:37:12 UTC): I removed the checkmark, would appreciate help.

fpopic (Issue Creator) on (2024-10-20 09:51:29 UTC): Adding minimal change to latest setup to reproduce

In `docker-compose.yml` passing host ADC config with write permission (to refresh host token from within container) 

```yaml
x-airflow-common:
  &airflow-common
  #...
  build: .
  #image: ...
  environment:
    &airflow-common-env
    # ...
    AIRFLOW__LOGGING__LOGGING_LEVEL: 'WARNING'
    AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT: '{ ""conn_type"": ""google_cloud_platform"", ""extra"": { ""scope"": ""https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/drive,https://www.googleapis.com/auth/bigquery"",  ""num_retries"": 0 } }'
    GOOGLE_CLOUD_PROJECT: <my-project>

  volumes: &airflow-common-volumes
    # ...
    # Use ADC
    - ~/.config/gcloud/:/home/airflow/.config/gcloud:rw
...
```

In Dockerfile install google cloud SDK (`gcloud` command) since google base hook requires gcloud CLI it:
https://github.com/apache/airflow/blob/2.10.2/airflow/providers/google/common/hooks/base_google.py#L638-L669

```Dockerfile
# specifying explicit python 3.9 otherwise gcloud sdk installation complains
FROM apache/airflow:2.10.2-python3.9
USER root

## COPY FROM https://github.com/apache/airflow/blob/main/docs/docker-stack/docker-images-recipes/gcloud.Dockerfile
ARG CLOUD_SDK_VERSION=322.0.0
ENV GCLOUD_HOME=/home/airflow/google-cloud-sdk
ENV PATH=""${GCLOUD_HOME}/bin/:${PATH}""

USER airflow

RUN DOWNLOAD_URL=""https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-${CLOUD_SDK_VERSION}-linux-x86_64.tar.gz"" \
    && TMP_DIR=""$(mktemp -d)"" \
    && curl -fL ""${DOWNLOAD_URL}"" --output ""${TMP_DIR}/google-cloud-sdk.tar.gz"" \
    && mkdir -p ""${GCLOUD_HOME}"" \
    && tar xzf ""${TMP_DIR}/google-cloud-sdk.tar.gz"" -C ""${GCLOUD_HOME}"" --strip-components=1 \
    && ""${GCLOUD_HOME}/install.sh"" \
       --bash-completion=false \
       --path-update=false \
       --usage-reporting=false \
       --additional-components alpha beta kubectl \
       --quiet \
    && rm -rf ""${TMP_DIR}"" \
    && rm -rf ""${GCLOUD_HOME}/.install/.backup/"" \
    && gcloud --version
# END OF COPY

# pip install airflow has to be run with USER airflow
RUN pip install 'apache-airflow==2.10.2' \
    apache-airflow[google_auth] \
    apache-airflow-providers-google \
    apache-airflow[google] \
    apache-airflow-providers-apache-beam

RUN mkdir -p /home/airflow/.config/gcloud/
```

eladkal on (2024-11-03 09:42:20 UTC): cc @VladaZakharova

SuccessMoses on (2024-12-01 07:15:43 UTC): @eladkal can you assign me?

eladkal on (2024-12-01 08:14:32 UTC): @SuccessMoses assigned

VladaZakharova on (2024-12-13 10:59:57 UTC): Hi @SuccessMoses ! Do you have any progress in resolving this issue?

SuccessMoses on (2024-12-14 10:05:38 UTC): I was not able to reproduce this in version 3, sorry.

VladaZakharova on (2024-12-16 11:18:17 UTC): Than I think we can take this from here, okay?
@eladkal please assign this one to @MaksYermak

eladkal on (2024-12-16 12:07:56 UTC): GitHub doesn't allow to assign tasks for users who didn't comment on the issue :)
So @MaksYermak need to comment and ask for this only then his name appear in the drop list

MaksYermak (Assginee) on (2024-12-16 13:40:31 UTC): Hello @eladkal !
Could you please assign this issue to me?

"
2540079317,issue,closed,completed,"Status of testing Providers that were prepared on September 21, 2024","### Body

I have a kind request for all the contributors to the latest provider packages release.
Could you please help us to test the RC versions of the providers?

The guidelines on how to test providers can be found in

[Verify providers by contributors](https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDER_PACKAGES.md#verify-the-release-candidate-by-contributors)

Let us know in the comment, whether the issue is addressed.

Those are providers that require testing as there were some substantial changes introduced:


## Provider [airbyte: 4.0.0rc1](https://pypi.org/project/apache-airflow-providers-airbyte/4.0.0rc1)
   - [ ] [Provider Airbyte: breaking change, update provider to use Airbyte API Python SDK (#41122)](https://github.com/apache/airflow/pull/41122): @marcosmarxm
   - [x] [Fix wrong casing in airbyte hook. (#42170)](https://github.com/apache/airflow/pull/42170): @potiuk
     Linked issues:
       - [x] [Linked Issue #42154](https://github.com/apache/airflow/issues/42154): @jscheffl
   - [x] [Pin airbyte-api to 0.51.0 (#42154) (#42155)](https://github.com/apache/airflow/pull/42155): @jscheffl
     Linked issues:
       - [x] [Linked Issue #42154](https://github.com/apache/airflow/issues/42154): @jscheffl
## Provider [alibaba: 2.9.1rc1](https://pypi.org/project/apache-airflow-providers-alibaba/2.9.1rc1)
   - [x] [log handler deprecated filename_template argument removal (#41552)](https://github.com/apache/airflow/pull/41552): @dirrao
## Provider [amazon: 8.29.0rc1](https://pypi.org/project/apache-airflow-providers-amazon/8.29.0rc1)
   - [ ] [Adding support for volume configurations in ECSRunTaskOperator (#42087)](https://github.com/apache/airflow/pull/42087): @srehman420
   - [ ] [Openlineage s3 to redshift operator integration (#41575)](https://github.com/apache/airflow/pull/41575): @Artuz37
   - [ ] [ECS Exec: Drop params that aren't compatible with EC2 (#42228)](https://github.com/apache/airflow/pull/42228): @o-nikolas
     Linked issues:
       - [ ] [Linked Issue #41824](https://github.com/apache/airflow/issues/41824): @AdamStrLSEC
   - [ ] [Fix GlueDataBrewStartJobOperator template fields (#42073)](https://github.com/apache/airflow/pull/42073): @ferruzzi
   - [x] [Validate aws service exceptions in waiters (#41941)](https://github.com/apache/airflow/pull/41941): @gopidesupavan
     Linked issues:
       - [x] [Linked Issue #41918](https://github.com/apache/airflow/issues/41918): @sarkch
   - [x] [Fix treatment of ""#"" in S3Hook.parse_s3_url() (#41796)](https://github.com/apache/airflow/pull/41796): @GlenboLake
   - [ ] [fix: remove part of openlineage extraction from S3ToRedshiftOperator (#41631)](https://github.com/apache/airflow/pull/41631): @kacpermuda
   - [x] [filename template arg in providers file task handlers backward compitability support (#41633)](https://github.com/apache/airflow/pull/41633): @dirrao
     Linked issues:
       - [x] [Linked Issue #41552](https://github.com/apache/airflow/pull/41552): @dirrao
   - [ ] [fix: custom query should have precedence over default query in RedshiftToS3Operator (#41634)](https://github.com/apache/airflow/pull/41634): @kacpermuda
     Linked issues:
       - [ ] [Linked Issue #37861](https://github.com/apache/airflow/pull/37861): @okirialbert
   - [ ] [Actually move saml to amazon provider (mistakenly added in papermill) (#42148)](https://github.com/apache/airflow/pull/42148): @potiuk
     Linked issues:
       - [ ] [Linked Issue #42137](https://github.com/apache/airflow/pull/42137): @potiuk
   - [x] [Use base aws classes in AWS Glue DataBrew Operators/Triggers (#41848)](https://github.com/apache/airflow/pull/41848): @gopidesupavan
     Linked issues:
       - [x] [Linked Issue #35278](https://github.com/apache/airflow/issues/35278): @Taragolis
   - [ ] [Move `register_views` to auth manager interface (#41777)](https://github.com/apache/airflow/pull/41777): @vincbeck
   - [ ] [Limit watchtower as depenendcy as 3.3.0 breaks main. (#41612)](https://github.com/apache/airflow/pull/41612): @potiuk
   - [x] [log handler deprecated filename_template argument removal (#41552)](https://github.com/apache/airflow/pull/41552): @dirrao
## Provider [apache.hdfs: 4.5.1rc1](https://pypi.org/project/apache-airflow-providers-apache-hdfs/4.5.1rc1)
   - [x] [log handler deprecated filename_template argument removal (#41552)](https://github.com/apache/airflow/pull/41552): @dirrao
## Provider [apache.impala: 1.5.1rc1](https://pypi.org/project/apache-airflow-providers-apache-impala/1.5.1rc1)
   - [ ] [Make kerberos an optional and devel dependency for impala and fab (#41616)](https://github.com/apache/airflow/pull/41616): @potiuk
## Provider [apache.livy: 3.9.1rc1](https://pypi.org/project/apache-airflow-providers-apache-livy/3.9.1rc1)
   - [x] [bugfix/livy-set-base-url (#41658)](https://github.com/apache/airflow/pull/41658): @harryshi10
## Provider [apache.spark: 4.11.0rc1](https://pypi.org/project/apache-airflow-providers-apache-spark/4.11.0rc1)
   - [x] [Add kerberos related connection fields(principal, keytab) on SparkSubmitHook (#40757)](https://github.com/apache/airflow/pull/40757): @seyoon-lim
## Provider [cloudant: 4.0.0rc1](https://pypi.org/project/apache-airflow-providers-cloudant/4.0.0rc1)
   - [x] [Switch cloudant provider from cloudant library to ibmcloudant library (#41555)](https://github.com/apache/airflow/pull/41555): @topherinternational
     Linked issues:
       - [x] [Linked Issue #21004](https://github.com/apache/airflow/issues/21004): @uranusjr
## Provider [cncf.kubernetes: 8.4.2rc1](https://pypi.org/project/apache-airflow-providers-cncf-kubernetes/8.4.2rc1)
   - [x] [Deprecated configuration removed (#42129)](https://github.com/apache/airflow/pull/42129): @dirrao
## Provider [common.io: 1.4.1rc1](https://pypi.org/project/apache-airflow-providers-common-io/1.4.1rc1)
   - [ ] [Protect against None components of universal pathlib xcom backend (#41921)](https://github.com/apache/airflow/pull/41921): @potiuk
     Linked issues:
       - [ ] [Linked Issue #41723](https://github.com/apache/airflow/issues/41723): @potiuk
## Provider [common.sql: 1.17.0rc1](https://pypi.org/project/apache-airflow-providers-common-sql/1.17.0rc1)
   - [x] [Generalize caching of connection in DbApiHook to improve performance (#40751)](https://github.com/apache/airflow/pull/40751): @dabla
   - [x] [feat: log client db messages for provider postgres (#40171)](https://github.com/apache/airflow/pull/40171): @dondaum
     Linked issues:
       - [x] [Linked Issue #40116](https://github.com/apache/airflow/issues/40116): @hpereira98
## Provider [databricks: 6.10.0rc1](https://pypi.org/project/apache-airflow-providers-databricks/6.10.0rc1)
   - [ ] [[FEAT] databricks repair run with reason match and appropriate new settings (#41412)](https://github.com/apache/airflow/pull/41412): @gaurav7261
   - [ ] [Removed deprecated method reference airflow.www.auth.has_access when min airflow version >= 2.8.0 (#41747)](https://github.com/apache/airflow/pull/41747): @dirrao
## Provider [docker: 3.14.0rc1](https://pypi.org/project/apache-airflow-providers-docker/3.14.0rc1)
   - [ ] [Added logging device and logging device options (#41416)](https://github.com/apache/airflow/pull/41416): @geraj1010
     Linked issues:
       - [ ] [Linked Issue #40533](https://github.com/apache/airflow/issues/40533): @xjchew
   - [ ] [Add retrieve output docker swarm operator (#41531)](https://github.com/apache/airflow/pull/41531): @rgriffier
## Provider [elasticsearch: 5.5.1rc1](https://pypi.org/project/apache-airflow-providers-elasticsearch/5.5.1rc1)
   - [x] [Fix ElasticSearch SQLClient deprecation warning (#41871)](https://github.com/apache/airflow/pull/41871): @Owen-CH-Leung
   - [ ] [Generalize caching of connection in DbApiHook to improve performance (#40751)](https://github.com/apache/airflow/pull/40751): @dabla
   - [x] [filename template arg in providers file task handlers backward compitability support (#41633)](https://github.com/apache/airflow/pull/41633): @dirrao
     Linked issues:
       - [x] [Linked Issue #41552](https://github.com/apache/airflow/pull/41552): @dirrao
   - [x] [log handler deprecated filename_template argument removal (#41552)](https://github.com/apache/airflow/pull/41552): @dirrao
## Provider [fab: 1.4.0rc1](https://pypi.org/project/apache-airflow-providers-fab/1.4.0rc1)
   - [x] [Add FAB migration commands (#41804)](https://github.com/apache/airflow/pull/41804): @ephraimbuddy
   - [ ] [Separate FAB migration from Core Airflow migration (#41437)](https://github.com/apache/airflow/pull/41437): @ephraimbuddy
   - [x] [Deprecated kerberos auth removed (#41693)](https://github.com/apache/airflow/pull/41693): @dirrao
   - [x] [Deprecated configuration removed (#42129)](https://github.com/apache/airflow/pull/42129): @dirrao
   - [ ] [Move `is_active` user property to FAB auth manager (#42042)](https://github.com/apache/airflow/pull/42042): @vincbeck
   - [ ] [Move `register_views` to auth manager interface (#41777)](https://github.com/apache/airflow/pull/41777): @vincbeck
   - [ ] [Revert ""Provider fab auth manager deprecated methods removed (#41720)"" (#41960)](https://github.com/apache/airflow/pull/41960): @potiuk
   - [x] [provider fab auth manager deprecated methods removed (#41720)](https://github.com/apache/airflow/pull/41720): @dirrao
   - [ ] [Make kerberos an optional and devel dependency for impala and fab (#41616)](https://github.com/apache/airflow/pull/41616): @potiuk
## Provider [google: 10.23.0rc1](https://pypi.org/project/apache-airflow-providers-google/10.23.0rc1)
   - [x] [Add ability to create Flink Jobs in dataproc cluster (#42342)](https://github.com/apache/airflow/pull/42342): @VladaZakharova
   - [x] [Add new Google Search 360 Reporting Operators (#42255)](https://github.com/apache/airflow/pull/42255): @molcay
   - [ ] [Add return_immediately parameter to PubSubPullSensor class (#41842)](https://github.com/apache/airflow/pull/41842): @arnaubadia
     Linked issues:
       - [ ] [Linked Issue #41838](https://github.com/apache/airflow/issues/41838): @arnaubadia
       - [ ] [Linked Issue #23231](https://github.com/apache/airflow/pull/23231): @eladkal
   - [ ] [Add parent_model param in `UploadModelOperator` (#42091)](https://github.com/apache/airflow/pull/42091): @jx2lee
     Linked issues:
       - [ ] [Linked Issue #41194](https://github.com/apache/airflow/issues/41194): @GLaDAP
   - [x] [Add DataflowStartYamlJobOperator (#41576)](https://github.com/apache/airflow/pull/41576): @moiseenkov
   - [ ] [Add RunEvaluationOperator for Google Vertex AI Rapid Evaluation API (#41940)](https://github.com/apache/airflow/pull/41940): @CYarros10
   - [ ] [Add CountTokensOperator for Google Generative AI CountTokensAPI (#41908)](https://github.com/apache/airflow/pull/41908): @CYarros10
   - [ ] [Add Supervised Fine Tuning Train Operator, Hook, Tests, Docs (#41807)](https://github.com/apache/airflow/pull/41807): @CYarros10
   - [ ] [Minor fixes to ensure successful Vertex AI LLMops pipeline (#41997)](https://github.com/apache/airflow/pull/41997): @CYarros10
   - [x] [Exclude partition from BigQuery table name (#42130)](https://github.com/apache/airflow/pull/42130): @moiseenkov
   - [ ] [[Fix #41763]: Redundant forward slash in SFTPToGCSOperator when destination_path is not specified or have default value (#41928)](https://github.com/apache/airflow/pull/41928): @Mayuresh16
     Linked issues:
       - [ ] [Linked Issue #41763](https://github.com/apache/airflow/issues/41763): @mpospis
   - [x] [Fix poll_interval in GKEJobTrigger (#41712)](https://github.com/apache/airflow/pull/41712): @gopidesupavan
     Linked issues:
       - [x] [Linked Issue #41705](https://github.com/apache/airflow/issues/41705): @bwatan
   - [ ] [update pattern for dataflow job id extraction (#41794)](https://github.com/apache/airflow/pull/41794): @lukas-mi
   - [ ] [Fix 'do_xcom_push' and 'get_logs' functionality for KubernetesJobOperator (#40814)](https://github.com/apache/airflow/pull/40814): @MaksYermak
   - [ ] [Mark VertexAI AutoMLText deprecation (#42251)](https://github.com/apache/airflow/pull/42251): @olegkachur-e
   - [x] [Remove system test for deprecated Google analytics operators (#41946)](https://github.com/apache/airflow/pull/41946): @moiseenkov
   - [ ] [Unpin google-cloud-bigquery package version for Google provider (#41839)](https://github.com/apache/airflow/pull/41839): @MaksYermak
   - [x] [Move away from deprecated DAG.following_schedule() method (#41773)](https://github.com/apache/airflow/pull/41773): @jscheffl
   - [x] [log handler deprecated filename_template argument removal (#41552)](https://github.com/apache/airflow/pull/41552): @dirrao
## Provider [jdbc: 4.5.1rc1](https://pypi.org/project/apache-airflow-providers-jdbc/4.5.1rc1)
   - [ ] [Generalize caching of connection in DbApiHook to improve performance (#40751)](https://github.com/apache/airflow/pull/40751): @dabla
## Provider [microsoft.azure: 10.5.0rc1](https://pypi.org/project/apache-airflow-providers-microsoft-azure/10.5.0rc1)
   - [ ] [Allow custom api versions in MSGraphAsyncOperator (#41331)](https://github.com/apache/airflow/pull/41331): @dabla
   - [ ] [ Add callback to process Azure Service Bus message contents (#41601)](https://github.com/apache/airflow/pull/41601): @perry2of5
   - [x] [log handler deprecated filename_template argument removal (#41552)](https://github.com/apache/airflow/pull/41552): @dirrao
## Provider [microsoft.mssql: 3.9.1rc1](https://pypi.org/project/apache-airflow-providers-microsoft-mssql/3.9.1rc1)
   - [ ] [Generalize caching of connection in DbApiHook to improve performance (#40751)](https://github.com/apache/airflow/pull/40751): @dabla
## Provider [mongo: 4.2.1rc1](https://pypi.org/project/apache-airflow-providers-mongo/4.2.1rc1)
   - [x] [Adjust typing in Mongo hook to prevent mypy errors (#42354)](https://github.com/apache/airflow/pull/42354): @jscheffl
   - [x] [Improve Mongo connection validation and UI (#41717)](https://github.com/apache/airflow/pull/41717): @topherinternational
     Linked issues:
       - [x] [Linked Issue #41371](https://github.com/apache/airflow/pull/41371): @topherinternational
## Provider [mysql: 5.7.1rc1](https://pypi.org/project/apache-airflow-providers-mysql/5.7.1rc1)
   - [ ] [Generalize caching of connection in DbApiHook to improve performance (#40751)](https://github.com/apache/airflow/pull/40751): @dabla
## Provider [odbc: 4.7.1rc1](https://pypi.org/project/apache-airflow-providers-odbc/4.7.1rc1)
   - [ ] [Generalize caching of connection in DbApiHook to improve performance (#40751)](https://github.com/apache/airflow/pull/40751): @dabla
## Provider [openai: 1.4.0rc1](https://pypi.org/project/apache-airflow-providers-openai/1.4.0rc1)
   - [x] [feat(providers/openai): support batch api in hook/operator/trigger (#41554)](https://github.com/apache/airflow/pull/41554): @josix
     Linked issues:
       - [x] [Linked Issue #41336](https://github.com/apache/airflow/issues/41336): @josix
## Provider [openlineage: 1.12.0rc1](https://pypi.org/project/apache-airflow-providers-openlineage/1.12.0rc1)
   - [ ] [feat: notify about potential serialization failures when sending DagRun, don't serialize unnecessary params, guard listener for exceptions (#41690)](https://github.com/apache/airflow/pull/4169
   - [ ] [fix: revert behavior of flattening lists in OpenLineage's InfoJsonEncodable (#41786)](https://github.com/apache/airflow/pull/41786): @mobuchowski
   - [ ] [chore: bump OL provider dependencies versions (#42059)](https://github.com/apache/airflow/pull/42059): @kacpermuda
   - [ ] [move to dag_run.logical_date from execution date in OpenLineage provider (#41889)](https://github.com/apache/airflow/pull/41889): @mobuchowski
   - [ ] [Unify DAG schedule args and change default to None (#41453)](https://github.com/apache/airflow/pull/41453): @uranusjr
## Provider [papermill: 3.8.1rc1](https://pypi.org/project/apache-airflow-providers-papermill/3.8.1rc1)
   - [ ] [Actually move saml to amazon provider (mistakenly added in papermill) (#42148)](https://github.com/apache/airflow/pull/42148): @potiuk
     Linked issues:
       - [ ] [Linked Issue #42137](https://github.com/apache/airflow/pull/42137): @potiuk
   - [ ] [Make SAML a required dependency of Amazon provider (#42137)](https://github.com/apache/airflow/pull/42137): @potiuk
## Provider [postgres: 5.13.0rc1](https://pypi.org/project/apache-airflow-providers-postgres/5.13.0rc1)
   - [ ] [feat: log client db messages for provider postgres (#40171)](https://github.com/apache/airflow/pull/40171): @dondaum
     Linked issues:
       - [ ] [Linked Issue #40116](https://github.com/apache/airflow/issues/40116): @hpereira98
   - [ ] [Generalize caching of connection in DbApiHook to improve performance (#40751)](https://github.com/apache/airflow/pull/40751): @dabla
## Provider [snowflake: 5.7.1rc1](https://pypi.org/project/apache-airflow-providers-snowflake/5.7.1rc1)
   - [x] [Update snowflake naming for account names and locators. (#41775)](https://github.com/apache/airflow/pull/41775): @JDarDagran

<!--

NOTE TO RELEASE MANAGER:

You can move here the providers that have doc-only changes or for which changes are trivial, and
you could assess that they are OK.

-->
All users involved in the PRs:
@gaurav7261 @ephraimbuddy @jscheffl @harryshi10 @dabla @moiseenkov @marcosmarxm @arnaubadia @o-nikolas @Artuz37 @geraj1010 @perry2of5 @josix @kacpermuda @dondaum @Mayuresh16 @olegkachur-e @ferruzzi @mob

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2024-09-21 08:38:14+00:00,[],2024-09-25 17:54:43+00:00,2024-09-24 13:54:17+00:00,https://github.com/apache/airflow/issues/42393,"[('area:providers', ''), ('kind:meta', 'High-level information important to the community'), ('testing status', 'Status of testing releases')]","[{'comment_id': 2365247406, 'issue_id': 2540079317, 'author': 'jscheffl', 'body': 'Signed-off my fixes I contributed. They were mainly related to failing main, if pytests are fine then all is good.', 'created_at': datetime.datetime(2024, 9, 21, 16, 37, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2366034036, 'issue_id': 2540079317, 'author': 'Owen-CH-Leung', 'body': 'Confirmed that the deprecation warning is gone with version `5.5.1rc1`. Thanks', 'created_at': datetime.datetime(2024, 9, 22, 8, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2366942458, 'issue_id': 2540079317, 'author': 'dondaum', 'body': 'Tested #40171. Worked as expected.', 'created_at': datetime.datetime(2024, 9, 22, 20, 4, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2367039657, 'issue_id': 2540079317, 'author': 'seyoon-lim', 'body': 'I have tested apache.spark: 4.11.0rc1(https://github.com/apache/airflow/pull/40757) and it works as intended. Thanks', 'created_at': datetime.datetime(2024, 9, 23, 0, 24, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2367596272, 'issue_id': 2540079317, 'author': 'josix', 'body': 'Tested [openai: 1.4.0rc1](https://pypi.org/project/apache-airflow-providers-openai/1.4.0rc1) #41336, it works as expeted, thanks!', 'created_at': datetime.datetime(2024, 9, 23, 8, 53, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2368016389, 'issue_id': 2540079317, 'author': 'topherinternational', 'body': 'Checked #41555 and #41717, they are good to go.', 'created_at': datetime.datetime(2024, 9, 23, 12, 4, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2368121318, 'issue_id': 2540079317, 'author': 'moiseenkov', 'body': '#41576, #42130, #41946 are good', 'created_at': datetime.datetime(2024, 9, 23, 12, 48, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2368246835, 'issue_id': 2540079317, 'author': 'molcay', 'body': '#42255 works as expected', 'created_at': datetime.datetime(2024, 9, 23, 13, 27, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2368250189, 'issue_id': 2540079317, 'author': 'VladaZakharova', 'body': 'https://github.com/apache/airflow/pull/42342 looks good', 'created_at': datetime.datetime(2024, 9, 23, 13, 28, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2368319563, 'issue_id': 2540079317, 'author': 'GlenboLake', 'body': '#41796 looks good', 'created_at': datetime.datetime(2024, 9, 23, 13, 46, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2368415610, 'issue_id': 2540079317, 'author': 'JDarDagran', 'body': 'https://github.com/apache/airflow/pull/41775 verified working', 'created_at': datetime.datetime(2024, 9, 23, 14, 16, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2368667490, 'issue_id': 2540079317, 'author': 'gopidesupavan', 'body': 'Verified #41918 working fine. Thank you for release.\r\nFor #41848 have environment issues to test, Just seen vincent message system tests are working fine on the latest provider package.', 'created_at': datetime.datetime(2024, 9, 23, 15, 37, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2368713500, 'issue_id': 2540079317, 'author': 'ephraimbuddy', 'body': 'Tested #41804, and #41437. No interactions with Airflow 2', 'created_at': datetime.datetime(2024, 9, 23, 15, 56, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2368752944, 'issue_id': 2540079317, 'author': 'gopidesupavan', 'body': 'Tested #41712 with system test and no issues, working fine.', 'created_at': datetime.datetime(2024, 9, 23, 16, 12, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2370156214, 'issue_id': 2540079317, 'author': 'harryshi10', 'body': '#41658 has been tested and works as expected.', 'created_at': datetime.datetime(2024, 9, 24, 4, 56, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2370324673, 'issue_id': 2540079317, 'author': 'dirrao', 'body': ""#42129, #41552, #41633, #41747, #41720\r\nI have validated basic things and they looks good to me. However, I couldn't able to test all functionality of auth, configuration and logging changes."", 'created_at': datetime.datetime(2024, 9, 24, 6, 41, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2371348770, 'issue_id': 2540079317, 'author': 'eladkal', 'body': 'Thank you everyone. Providers are released.\r\n\r\nI invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).', 'created_at': datetime.datetime(2024, 9, 24, 13, 54, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2374785700, 'issue_id': 2540079317, 'author': 'geraj1010', 'body': 'I tested #41416 before the PR was closed and the logging worked as expected. Apologies for the late response, my GitHub account was just recently restored.', 'created_at': datetime.datetime(2024, 9, 25, 17, 54, 41, tzinfo=datetime.timezone.utc)}]","jscheffl on (2024-09-21 16:37:55 UTC): Signed-off my fixes I contributed. They were mainly related to failing main, if pytests are fine then all is good.

Owen-CH-Leung on (2024-09-22 08:27:00 UTC): Confirmed that the deprecation warning is gone with version `5.5.1rc1`. Thanks

dondaum on (2024-09-22 20:04:44 UTC): Tested #40171. Worked as expected.

seyoon-lim on (2024-09-23 00:24:01 UTC): I have tested apache.spark: 4.11.0rc1(https://github.com/apache/airflow/pull/40757) and it works as intended. Thanks

josix on (2024-09-23 08:53:21 UTC): Tested [openai: 1.4.0rc1](https://pypi.org/project/apache-airflow-providers-openai/1.4.0rc1) #41336, it works as expeted, thanks!

topherinternational on (2024-09-23 12:04:01 UTC): Checked #41555 and #41717, they are good to go.

moiseenkov on (2024-09-23 12:48:08 UTC): #41576, #42130, #41946 are good

molcay on (2024-09-23 13:27:16 UTC): #42255 works as expected

VladaZakharova on (2024-09-23 13:28:29 UTC): https://github.com/apache/airflow/pull/42342 looks good

GlenboLake on (2024-09-23 13:46:31 UTC): #41796 looks good

JDarDagran on (2024-09-23 14:16:57 UTC): https://github.com/apache/airflow/pull/41775 verified working

gopidesupavan on (2024-09-23 15:37:17 UTC): Verified #41918 working fine. Thank you for release.
For #41848 have environment issues to test, Just seen vincent message system tests are working fine on the latest provider package.

ephraimbuddy on (2024-09-23 15:56:04 UTC): Tested #41804, and #41437. No interactions with Airflow 2

gopidesupavan on (2024-09-23 16:12:57 UTC): Tested #41712 with system test and no issues, working fine.

harryshi10 on (2024-09-24 04:56:43 UTC): #41658 has been tested and works as expected.

dirrao on (2024-09-24 06:41:38 UTC): #42129, #41552, #41633, #41747, #41720
I have validated basic things and they looks good to me. However, I couldn't able to test all functionality of auth, configuration and logging changes.

eladkal (Issue Creator) on (2024-09-24 13:54:18 UTC): Thank you everyone. Providers are released.

I invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).

geraj1010 on (2024-09-25 17:54:41 UTC): I tested #41416 before the PR was closed and the logging worked as expected. Apologies for the late response, my GitHub account was just recently restored.

"
2539718542,issue,closed,completed,GCSToGCSOperator source_objects failing to parse xcom,"### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

10.15.0

### Apache Airflow version

2.7.3

### Operating System

debian

### Deployment

Astronomer

### Deployment details

Using astro runtime 9.15.0

### What happened

The GCSToGCSOperator is throwing an error when trying to pass task output into the `source_objects` field. 

```
 File ""/usr/local/lib/python3.11/site-packages/airflow/providers/google/cloud/transfers/gcs_to_gcs.py"", line 211, in __init__
    if source_objects and any(WILDCARD in obj for obj in source_objects):
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/xcom_arg.py"", line 268, in __iter__
    raise TypeError(""'XComArg' object is not iterable"")
```

### What you think should happen instead

This operator should be able to handle xcom, and we have no issue using the `GCSToBigQueryOperator` in the same way

### How to reproduce

```
@task
def return_files():
    files = ['a','b','c','d']
    return files

files = return_files()

 extract_files = GCSToGCSOperator(
        task_id=""fetch_data"",
        source_bucket=""source_bucket_name"",
        source_objects=files,
        destination_bucket=""dest_bucket_name"",
        destination_object=""foo"",
        dag=dag,
    )

files >> extract_files
```

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",blaklaybul,2024-09-20 21:37:53+00:00,[],2024-10-02 01:07:18+00:00,2024-10-02 01:07:18+00:00,https://github.com/apache/airflow/issues/42391,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2364651534, 'issue_id': 2539718542, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 20, 21, 37, 55, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-20 21:37:55 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2539471054,issue,closed,completed,Tasks pods are getting stuck in scheduled state after open slot parallelism count is reached ,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.0

### What happened?

I recently upgraded my airflow version from 2.5.3 to 2.10.0 in our environment, and the parallelism count is set to 32 with three schedulers in place, so what happens is that when more than 96 tasks run, whenever a new task is scheduled after that, it gets stuck in scheduled state, with the open slot count being zero, even though the previous tasks that ran have completed and have been cleared.

### What you think should happen instead?

The open slot count should increase when the tasks are completed and the tasks queued up should be scheduled

### How to reproduce

Just tried it by upgrading the changes and running 5 or 6 dags with 10 task in each dag and parallelism set to 32 for each scheduler. Point to be noted is that the same set of dag works fine when it was running in airflow version 2.5.3

### Operating System

Redhat linux

### Versions of Apache Airflow Providers

 apache-airflow-providers-postgres==5.12.0 \
           apache-airflow-providers-apache-hive==8.2.0 \
           apache-airflow-providers-amazon==8.28.0 \
           apache-airflow-providers-cncf-kubernetes==8.4.1 \
           apache-airflow-providers-apache-livy==3.9.0 \
           apache-airflow-providers-presto==5.6.0 \
           apache-airflow-providers-http==4.13.0 \
           apache-airflow-providers-trino==5.8.0 \
           apache-airflow-providers-snowflake==5.7.0 \
           apache-airflow-providers-salesforce==5.8.0 \
           apache-airflow-providers-papermill==3.8.0 \
           apache-airflow-providers-google==10.22.0 \
           apache-airflow-providers-celery==3.8.1 \
           apache-airflow-providers-redis==3.8.0 \
           apache-airflow-providers-dbt-cloud==3.10.0 \
           apache-airflow-providers-openlineage==1.11.0 \

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",amrit2196,2024-09-20 18:59:53+00:00,[],2024-11-22 21:48:25+00:00,2024-11-22 21:48:25+00:00,https://github.com/apache/airflow/issues/42383,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('pending-response', ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2364348084, 'issue_id': 2539471054, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 20, 18, 59, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2364616103, 'issue_id': 2539471054, 'author': 'jscheffl', 'body': 'Can you create and post an example DAG to reproduce?\r\nI am a bit courious what effect might bring this bug to you. There are maybe hundreds of installations using 2.10 already and it would be a major bug if nobody has detected this, a moment before we release 2.10.2.\r\n\r\nCan you tell which executor you are using?', 'created_at': datetime.datetime(2024, 9, 20, 21, 4, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2364741507, 'issue_id': 2539471054, 'author': 'amrit2196', 'body': 'We are using kubernetes executor , but for task pod deletion we run a cronjob to delete task pods, which was working fine in 2.5.3, but not in this one', 'created_at': datetime.datetime(2024, 9, 20, 23, 17, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2364742212, 'issue_id': 2539471054, 'author': 'amrit2196', 'body': 'We are currently running a simple tag with multiple tasks with sleep and checking a get request in each tasks', 'created_at': datetime.datetime(2024, 9, 20, 23, 18, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2365246750, 'issue_id': 2539471054, 'author': 'jscheffl', 'body': 'So to be able to understand this - and most likely it is something in the environment - I request that you inspect the scheduler logs. In recent versions there should be logs emitted when the scheduler is at the parallelism limit. Can you check for this?\r\n\r\nCan you also please post an example DAG with which it is easy to reproduce? Then we could test it as regression.', 'created_at': datetime.datetime(2024, 9, 21, 16, 35, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2395235495, 'issue_id': 2539471054, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 10, 6, 0, 16, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2402880774, 'issue_id': 2539471054, 'author': 'amrit2196', 'body': '@jscheffl  The issue we are facing is , assume a scenario where the task pod runs and the base container gets completed and the side cars are terminated immediately along with the base container, the task event are properly sent to the kubernetes executor, whereas when I add a delay logic in termination of side cars, the information is not sent to the executor even though the containers are terminated and the pod is deleted after the delay. This results in parallelism count reaching to 0. I can see messages like skipping event for this task pod as the event has been already sent as well.\r\n\r\nThis happens even with a simple hello world dag:\r\n\r\nfrom airflow import DAG\r\nfrom airflow.operators.python_operator import PythonOperator\r\nfrom datetime import datetime\r\n\r\ndef hello_world():\r\n    print(""Hello, World!"")\r\n\r\n\r\ndefault_args = {\r\n    \'owner\': \'airflow\',\r\n    \'start_date\': datetime(2024, 10, 8),  # Update to the current date or earlier\r\n    \'retries\': 1,\r\n}\r\n\r\n\r\nwith DAG(\r\n    dag_id=\'hello_world_dag\',\r\n    default_args=default_args,\r\n    schedule_interval=None,  # Set to None to disable scheduled runs\r\n    catchup=False,  # Don\'t backfill or run previous missed schedules\r\n) as dag:\r\n\r\nhello_world_task = PythonOperator(\r\n        task_id=\'print_hello_world\',\r\n        python_callable=hello_world,\r\n    )\r\n\r\n\r\nhello_world_task', 'created_at': datetime.datetime(2024, 10, 9, 17, 18, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2406894422, 'issue_id': 2539471054, 'author': 'FleischerT', 'body': ""We seem to be experiencing the same issue, or at least a very similar one: After restarting the Airflow pods in Kubernetes, everything operates normally for a while, but then suddenly no additional tasks are executed. The logs display the following message:\r\n\r\n'Executor parallelism limit reached. 0 open slots'\r\n\r\nHowever, no tasks are actually running at that point. This issue has only arisen for us since upgrading to version 2.10."", 'created_at': datetime.datetime(2024, 10, 11, 8, 26, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2457634351, 'issue_id': 2539471054, 'author': 'potiuk', 'body': 'Airflow 2.10.3 is now out an it has fix #42932 that is likely to fix the problems you reported, please <author> upgrade, check if it fixed your problem and report back @amrit2196 @FleischerT ?', 'created_at': datetime.datetime(2024, 11, 5, 16, 26, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2487026663, 'issue_id': 2539471054, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 11, 20, 0, 15, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2491692621, 'issue_id': 2539471054, 'author': 'FleischerT', 'body': '@potiuk So far, it seems that the problem is fixed with the new release! If it appears again, then I will create a new issue.', 'created_at': datetime.datetime(2024, 11, 21, 16, 22, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2494920703, 'issue_id': 2539471054, 'author': 'potiuk', 'body': '> @potiuk So far, it seems that the problem is fixed with the new release! If it appears again, then I will create a new issue.\r\n\r\nThanks for confirming @FleischerT !', 'created_at': datetime.datetime(2024, 11, 22, 21, 48, 25, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-20 18:59:56 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jscheffl on (2024-09-20 21:04:56 UTC): Can you create and post an example DAG to reproduce?
I am a bit courious what effect might bring this bug to you. There are maybe hundreds of installations using 2.10 already and it would be a major bug if nobody has detected this, a moment before we release 2.10.2.

Can you tell which executor you are using?

amrit2196 (Issue Creator) on (2024-09-20 23:17:38 UTC): We are using kubernetes executor , but for task pod deletion we run a cronjob to delete task pods, which was working fine in 2.5.3, but not in this one

amrit2196 (Issue Creator) on (2024-09-20 23:18:55 UTC): We are currently running a simple tag with multiple tasks with sleep and checking a get request in each tasks

jscheffl on (2024-09-21 16:35:19 UTC): So to be able to understand this - and most likely it is something in the environment - I request that you inspect the scheduler logs. In recent versions there should be logs emitted when the scheduler is at the parallelism limit. Can you check for this?

Can you also please post an example DAG with which it is easy to reproduce? Then we could test it as regression.

github-actions[bot] on (2024-10-06 00:16:28 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

amrit2196 (Issue Creator) on (2024-10-09 17:18:20 UTC): @jscheffl  The issue we are facing is , assume a scenario where the task pod runs and the base container gets completed and the side cars are terminated immediately along with the base container, the task event are properly sent to the kubernetes executor, whereas when I add a delay logic in termination of side cars, the information is not sent to the executor even though the containers are terminated and the pod is deleted after the delay. This results in parallelism count reaching to 0. I can see messages like skipping event for this task pod as the event has been already sent as well.

This happens even with a simple hello world dag:

from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime

def hello_world():
    print(""Hello, World!"")


default_args = {
    'owner': 'airflow',
    'start_date': datetime(2024, 10, 8),  # Update to the current date or earlier
    'retries': 1,
}


with DAG(
    dag_id='hello_world_dag',
    default_args=default_args,
    schedule_interval=None,  # Set to None to disable scheduled runs
    catchup=False,  # Don't backfill or run previous missed schedules
) as dag:

hello_world_task = PythonOperator(
        task_id='print_hello_world',
        python_callable=hello_world,
    )


hello_world_task

FleischerT on (2024-10-11 08:26:12 UTC): We seem to be experiencing the same issue, or at least a very similar one: After restarting the Airflow pods in Kubernetes, everything operates normally for a while, but then suddenly no additional tasks are executed. The logs display the following message:

'Executor parallelism limit reached. 0 open slots'

However, no tasks are actually running at that point. This issue has only arisen for us since upgrading to version 2.10.

potiuk on (2024-11-05 16:26:35 UTC): Airflow 2.10.3 is now out an it has fix #42932 that is likely to fix the problems you reported, please <author> upgrade, check if it fixed your problem and report back @amrit2196 @FleischerT ?

github-actions[bot] on (2024-11-20 00:15:42 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

FleischerT on (2024-11-21 16:22:18 UTC): @potiuk So far, it seems that the problem is fixed with the new release! If it appears again, then I will create a new issue.

potiuk on (2024-11-22 21:48:25 UTC): Thanks for confirming @FleischerT !

"
2539251461,issue,closed,completed,Add heartbeat metric for DAG processor,"### Description

The scheduler emits a `scheduler_heartbeat`, which makes it easy to determine if the scheduler is up and running. We should add an equivalent for the DAG processor.

### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jedcunningham,2024-09-20 16:59:13+00:00,[],2024-10-01 21:59:55+00:00,2024-10-01 21:59:55+00:00,https://github.com/apache/airflow/issues/42380,"[('kind:feature', 'Feature Requests'), ('area:metrics', '')]",[],
2538864048,issue,closed,completed,AIP-38 Develop a robust table component,"Our legacy UI includes a lot of tables. We need a robust table component for us to reuse across all of these views. This should include a good UX for power features like all of the sorting and filtering we currently have. We are currently using an old version of react-table. We should make this component with the new version and make it easy to plug into a bunch of our views: https://tanstack.com/table/latest

- Add card list option: #42698
- Add advanced search bar: #42699
- Add multi-field sort: #42732
- Handle loading state #42734
- Handle errors: #42893

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-09-20 13:41:46+00:00,['bbovenzi'],2024-12-11 23:10:19+00:00,2024-12-11 23:10:19+00:00,https://github.com/apache/airflow/issues/42373,"[('kind:meta', 'High-level information important to the community'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]",[],
2538856992,issue,closed,completed,AIP-38 Programmatic form component,"Brent says:
> Let's develop a robust form component that can render many different fields based on the spec it receives. Then we > can use this for advanced dagrun trigger and connection management.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-09-20 13:39:12+00:00,"['bbovenzi', 'jscheffl']",2025-01-31 18:09:39+00:00,2025-01-31 18:09:39+00:00,https://github.com/apache/airflow/issues/42372,"[('kind:meta', 'High-level information important to the community'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application'), ('AIP-50', 'Trigger DAG UI user Form')]","[{'comment_id': 2391081186, 'issue_id': 2538856992, 'author': 'jscheffl', 'body': 'I would try to raise my hand for this. Just need a bit time to get started in the new ecosystem :-D \n\nTHe components woul dbe needed for (1) trigger form as well as (2) connection for in the feature set of the legacy UI.', 'created_at': datetime.datetime(2024, 10, 3, 10, 37, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2391283343, 'issue_id': 2538856992, 'author': 'pierrejeambrun', 'body': ""Yes the idea is to have a generic form component, flexible, reusable enough to do any type of form. A good idea would be to take a look into react form libraries out there and find one that looks good. (input validation, error handling, etcc...). In the past I have used formik, I don't know if it is still relevant today or if other libraries are better suited such as `https://tanstack.com/form/latest`"", 'created_at': datetime.datetime(2024, 10, 3, 12, 21, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2391325409, 'issue_id': 2538856992, 'author': 'bbovenzi', 'body': 'Updating my comment based on some quick research.\r\n\r\nformik: https://formik.org/ Has had long periods of not being maintained, not typescript-first\r\ntanstack/form: https://tanstack.com/form/latest His other libraries, query and table, have been very useful for us, but its still at v0.33.0 so it could change a lot after a v1.0\r\nreact-hook-form: https://react-hook-form.com/ Another popular framework.\r\n\r\nI would lean towards react-hook-form, idk if we want to rely on an outdated library or anything at a 0.X version', 'created_at': datetime.datetime(2024, 10, 3, 12, 43, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2391477378, 'issue_id': 2538856992, 'author': 'pierrejeambrun', 'body': 'I share the feeling for `0.X` versions. This is something I mentioned when we considered switching the backend web framework, but still we went with FastAPI, so I would say for consistancy that 0.X version are not a problem for us. (I guess)\r\n\r\nAnd in general tanstack lib are well done.', 'created_at': datetime.datetime(2024, 10, 3, 13, 49, 33, tzinfo=datetime.timezone.utc)}]","jscheffl (Assginee) on (2024-10-03 10:37:55 UTC): I would try to raise my hand for this. Just need a bit time to get started in the new ecosystem :-D 

THe components woul dbe needed for (1) trigger form as well as (2) connection for in the feature set of the legacy UI.

pierrejeambrun (Issue Creator) on (2024-10-03 12:21:57 UTC): Yes the idea is to have a generic form component, flexible, reusable enough to do any type of form. A good idea would be to take a look into react form libraries out there and find one that looks good. (input validation, error handling, etcc...). In the past I have used formik, I don't know if it is still relevant today or if other libraries are better suited such as `https://tanstack.com/form/latest`

bbovenzi (Assginee) on (2024-10-03 12:43:24 UTC): Updating my comment based on some quick research.

formik: https://formik.org/ Has had long periods of not being maintained, not typescript-first
tanstack/form: https://tanstack.com/form/latest His other libraries, query and table, have been very useful for us, but its still at v0.33.0 so it could change a lot after a v1.0
react-hook-form: https://react-hook-form.com/ Another popular framework.

I would lean towards react-hook-form, idk if we want to rely on an outdated library or anything at a 0.X version

pierrejeambrun (Issue Creator) on (2024-10-03 13:49:33 UTC): I share the feeling for `0.X` versions. This is something I mentioned when we considered switching the backend web framework, but still we went with FastAPI, so I would say for consistancy that 0.X version are not a problem for us. (I guess)

And in general tanstack lib are well done.

"
2538840632,issue,closed,completed,AIP-38 Create new designs for Airflow 3,"Pages to design:

- [x] New Homepage Dashboard 
- [x] DAGs list
- [ ] Assets page
- [x] DAG Details

Many table/card view pages should follow the same designs (dag runs list, task instances list, user management, connections)

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-09-20 13:31:39+00:00,"['bbovenzi', 'cmarteepants']",2025-01-31 18:09:56+00:00,2025-01-31 18:09:56+00:00,https://github.com/apache/airflow/issues/42371,"[('kind:meta', 'High-level information important to the community'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-38', 'Modern Web Application')]","[{'comment_id': 2390830661, 'issue_id': 2538840632, 'author': 'bbovenzi', 'body': 'Upgrading Cluster Activity to be the main dashboard for a new home page.\n\nStories:\n- What has happened across my entire Airflow deployment recently?\n- Can I quickly see any issues and drill down into them?\n\n![Image](https://github.com/user-attachments/assets/843e837e-39d5-41c5-baa2-5527decc1348)', 'created_at': datetime.datetime(2024, 10, 3, 8, 32, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2390834444, 'issue_id': 2538840632, 'author': 'bbovenzi', 'body': 'Card-style view of the DAGs list page.\n\nUsers can still toggle to a more condensed table view. A few pieces here are still subject to change.\n\nStories:\n- Show me a list of my dags with a summary of their activity\n- We have a big org. I want to filter the list to just the specific dags I care about.\n![Image](https://github.com/user-attachments/assets/f1c17a53-1020-4d4f-9571-adf942ba6329)', 'created_at': datetime.datetime(2024, 10, 3, 8, 34, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2390836158, 'issue_id': 2538840632, 'author': 'bbovenzi', 'body': 'Advanced queries for list views:\n![Image](https://github.com/user-attachments/assets/cf56967b-6215-4ab6-a9c2-d7ad92963c43)', 'created_at': datetime.datetime(2024, 10, 3, 8, 35, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2390837981, 'issue_id': 2538840632, 'author': 'bbovenzi', 'body': 'New vertical navigation with subnav and menu options\n\n![Image](https://github.com/user-attachments/assets/6b776b95-7e4d-4aa5-a676-e4140a0bbf85)', 'created_at': datetime.datetime(2024, 10, 3, 8, 36, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2392628197, 'issue_id': 2538840632, 'author': 'potiuk', 'body': 'Slick!', 'created_at': datetime.datetime(2024, 10, 4, 1, 41, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2426858224, 'issue_id': 2538840632, 'author': 'bbovenzi', 'body': 'DAG Details\n\nStories:\n- Show me an overview of this DAG\'s activity\n- Highlight failed task instances and let me quickly get to their logs\n\n<img width=""1457"" alt=""Screenshot 2024-10-21 at 9 35 33\u202fAM"" src=""https://github.com/user-attachments/assets/42d9783f-79b5-4752-9718-e6d556957f57"">\nWe\'re going to make that top stats bar smaller and remove ""Slow Tasks""', 'created_at': datetime.datetime(2024, 10, 21, 14, 31, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2426860509, 'issue_id': 2538840632, 'author': 'bbovenzi', 'body': 'DAG Details, view list of runs with a bar chart to help as a date range selector:\r\n\r\n<img width=""1455"" alt=""Screenshot 2024-10-21 at 9 35 45\u202fAM"" src=""https://github.com/user-attachments/assets/bd480afe-1f37-41d8-a70a-50c35f5909c8"">', 'created_at': datetime.datetime(2024, 10, 21, 14, 32, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2426863604, 'issue_id': 2538840632, 'author': 'bbovenzi', 'body': 'DAG Details, list of tasks with their own graph of recent task instances:\n\n\nStories:\n- Show me a list of my tasks with quick history of their recent instances. \n\n<img width=""1459"" alt=""Screenshot 2024-10-21 at 9 36 25\u202fAM"" src=""https://github.com/user-attachments/assets/3cb02df3-488f-4bc6-9d16-5896101aacdb"">', 'created_at': datetime.datetime(2024, 10, 21, 14, 33, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2426868265, 'issue_id': 2538840632, 'author': 'bbovenzi', 'body': 'DAG Run Details, with the ability to see task logs for the entire run at once (we need to see if this is actually feasible to load so many logs at once)\n\n<img width=""1483"" alt=""Screenshot 2024-10-21 at 9 36 14\u202fAM"" src=""https://github.com/user-attachments/assets/84a898d6-f319-43ea-9af7-e596209e41b1"">', 'created_at': datetime.datetime(2024, 10, 21, 14, 34, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2426871060, 'issue_id': 2538840632, 'author': 'bbovenzi', 'body': 'Task details to see how a task has performed across instances, same as a DAG overview.\n\n<img width=""1458"" alt=""Screenshot 2024-10-21 at 9 36 43\u202fAM"" src=""https://github.com/user-attachments/assets/9c414eae-2585-4ae5-8ccb-1611e4b695f7"">', 'created_at': datetime.datetime(2024, 10, 21, 14, 35, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2426874071, 'issue_id': 2538840632, 'author': 'bbovenzi', 'body': 'Task Instance details with logs:\r\n\r\n<img width=""1460"" alt=""Screenshot 2024-10-21 at 9 36 52\u202fAM"" src=""https://github.com/user-attachments/assets/ff1a449a-dea9-480a-b966-03cbce93882a"">', 'created_at': datetime.datetime(2024, 10, 21, 14, 36, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2426879623, 'issue_id': 2538840632, 'author': 'bbovenzi', 'body': 'Graph, Grid and Gantt open up in a full-page modal that can be accessed from any Dag/Run/Task page. The modal also has a right hand drawer for information on the selected dag/run/task.\n\nMotivation:\n- Grid/Gantt/graph can all be complicated views. Give them the space of a full-page modal but still include a drawer for some details\n\n<img width=""1460"" alt=""Screenshot 2024-10-21 at 9 37 17\u202fAM"" src=""https://github.com/user-attachments/assets/37a4e0d3-a8c1-42b6-ad90-eb36b12b5642"">\n\n<img width=""1381"" alt=""Screenshot 2024-10-21 at 9 37 44\u202fAM"" src=""https://github.com/user-attachments/assets/26ce24f9-3748-49ad-854e-e14b70e419a6"">\n\n<img width=""1456"" alt=""Screenshot 2024-10-21 at 9 37 51\u202fAM"" src=""https://github.com/user-attachments/assets/a1859521-15db-4e34-91b7-d2587a68efde"">', 'created_at': datetime.datetime(2024, 10, 21, 14, 38, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2426889366, 'issue_id': 2538840632, 'author': 'bbovenzi', 'body': 'Logs and graph side-by-side:\r\n\r\n<img width=""1457"" alt=""Screenshot 2024-10-21 at 10 41 39\u202fAM"" src=""https://github.com/user-attachments/assets/8d434252-4d4f-45cd-b387-7c8343fa1ab6"">', 'created_at': datetime.datetime(2024, 10, 21, 14, 41, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2427320741, 'issue_id': 2538840632, 'author': 'potiuk', 'body': 'Nice :)', 'created_at': datetime.datetime(2024, 10, 21, 17, 30, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2427374996, 'issue_id': 2538840632, 'author': 'jscheffl', 'body': '> New vertical navigation with subnav and menu options\r\n> \r\n> ![Image](https://private-user-images.githubusercontent.com/4600967/373178324-6b776b95-7e4d-4aa5-a676-e4140a0bbf85.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk1MzM2MDgsIm5iZiI6MTcyOTUzMzMwOCwicGF0aCI6Ii80NjAwOTY3LzM3MzE3ODMyNC02Yjc3NmI5NS03ZTRkLTRhYTUtYTY3Ni1lNDE0MGEwYmJmODUucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTAyMSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDEwMjFUMTc1NTA4WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9NTM2MTlmYzViMmVmZTdlZjE1YWU1ZjRiOWRiNmM5NWMxNzc4MzJkMzEyODdhOGNiOTFhMjE0MmMzNmQzOGQwNiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.NT3A8gSoS-N7qpEFl1iFw5uklHxRkO37H1DEQbJjtlQ)\r\n\r\nI don\'t know if this issue is the right way to discuss. let me know if there are other means...\r\n\r\nI\'d propose to move the XComs to ""Browse..."" as actually it is a legacy that you browse XComs in the Admin menu... but usually this is not an admin-specific thing.', 'created_at': datetime.datetime(2024, 10, 21, 17, 59, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2427382380, 'issue_id': 2538840632, 'author': 'jscheffl', 'body': '> Card-style view of the DAGs list page.\r\n> \r\n> Users can still toggle to a more condensed table view. A few pieces here are still subject to change. ![Image](https://private-user-images.githubusercontent.com/4600967/373177497-f1c17a53-1020-4d4f-9571-adf942ba6329.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk1MzM2MDgsIm5iZiI6MTcyOTUzMzMwOCwicGF0aCI6Ii80NjAwOTY3LzM3MzE3NzQ5Ny1mMWMxN2E1My0xMDIwLTRkNGYtOTU3MS1hZGY5NDJiYTYzMjkucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTAyMSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDEwMjFUMTc1NTA4WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZTNlN2E2MjE5NTQzOTNlZDI5ZDk4MGI5NDc3ZmI0ZGZkYjVjMjRjYzQ3ZDkyNTQ0NzRkNjMwZTc4NDk5MzBhYiZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.mwlp5Yvy5PQ3cnPeaQAIQRKFQVSIPnjbfYXUj0EA0X0)\r\n\r\nAs we might not have a ""Multie Team"" setup in most environments... and as usually you have more than a hand-full of DAGs - can we foresee an option that a user can mark DAGs (on his profile) like ""Favorite DAGs"" which are displayed first before a long long list? (to focus on). At least we have many users who are mainly interested in 3-5 of 100 DAGs.', 'created_at': datetime.datetime(2024, 10, 21, 18, 3, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2427389125, 'issue_id': 2538840632, 'author': 'jscheffl', 'body': '> DAG Run Details, with the ability to see task logs for the entire run at once:\r\n> <img alt=""Screenshot 2024-10-21 at 9 36 14\u202fAM"" width=""1483"" src=""https://private-user-images.githubusercontent.com/4600967/378471745-84a898d6-f319-43ea-9af7-e596209e41b1.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk1MzM2MDgsIm5iZiI6MTcyOTUzMzMwOCwicGF0aCI6Ii80NjAwOTY3LzM3ODQ3MTc0NS04NGE4OThkNi1mMzE5LTQzZWEtOWFmNy1lNTk2MjA5ZTQxYjEucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTAyMSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDEwMjFUMTc1NTA4WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9N2U4NzU4ZGU3YWU3YTgwNTExNTQ4OTM3M2ViYzNmY2JiNWIxZjk0ZTZkMDZiOWRhYWQyNjkyYzY3ZDQ1ZGFhMCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.YLLvX8lngzKqhtJa8MLVwPdf3z1BQQgl1bUHGYyiZKg"">\r\n\r\nOne thing that botheres me since a long time and is contained in the Lorem-Ipsum as well - there are always 3 lines where the Log is sourced from. Nice for troubleshooting bust most users are confused by this and wonder why link is most cases noch loading when clicking (because internal protected S3 backend, not public exposed...) - logs should focus on valueable content, in the summary maybe the last N lines and not the first lines.', 'created_at': datetime.datetime(2024, 10, 21, 18, 7, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2427397026, 'issue_id': 2538840632, 'author': 'jscheffl', 'body': '> Graph, Grid and Gantt open up in a full-page modal that can be accessed from any Dag/Run/Task page. The modal also has a left hand drawer for information on the selected dag/run/task.\r\n> <img alt=""Screenshot 2024-10-21 at 9 37 17\u202fAM"" width=""1460"" src=""https://private-user-images.githubusercontent.com/4600967/378472716-37a4e0d3-a8c1-42b6-ad90-eb36b12b5642.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk1MzM2MDgsIm5iZiI6MTcyOTUzMzMwOCwicGF0aCI6Ii80NjAwOTY3LzM3ODQ3MjcxNi0zN2E0ZTBkMy1hOGMxLTQyYjYtYWQ5MC1lYjM2YjEyYjU2NDIucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTAyMSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDEwMjFUMTc1NTA4WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MDEyZjg1YzQ3ZGU1MzI5MmQ2YmNiNTBhNjY5NGE2Njk3ODg3NWEwOTQwNDFlNmEyMTA0N2E0ZTFmZWNiNjE4OCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.D3LNi6F7UwoFz0h4u3bUJcQZPVOzN5zWnZfNGiB3i-g""> <img alt=""Screenshot 2024-10-21 at 9 37 44\u202fAM"" width=""1381"" src=""https://private-user-images.githubusercontent.com/4600967/378473316-26ce24f9-3748-49ad-854e-e14b70e419a6.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk1MzM2MDgsIm5iZiI6MTcyOTUzMzMwOCwicGF0aCI6Ii80NjAwOTY3LzM3ODQ3MzMxNi0yNmNlMjRmOS0zNzQ4LTQ5YWQtODU0ZS1lMTRiNzBlNDE5YTYucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTAyMSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDEwMjFUMTc1NTA4WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9MjUwYzQxZmRlZTEzY2MwZjdkYzMxMzZlYjA4MWExOWM2NDc2ZmNlOTdkZTM1Zjg0OTU0YTZmNjY3NGRlYThiYSZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.iTv8jvgDzxlCtD7SmbVs9L0YsmkpYUBcVYwD1TjSUyo""> <img alt=""Screenshot 2024-10-21 at 9 37 51\u202fAM"" width=""1456"" src=""https://private-user-images.githubusercontent.com/4600967/378473333-a1859521-15db-4e34-91b7-d2587a68efde.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3Mjk1MzM2MDgsIm5iZiI6MTcyOTUzMzMwOCwicGF0aCI6Ii80NjAwOTY3LzM3ODQ3MzMzMy1hMTg1OTUyMS0xNWRiLTRlMzQtOTFiNy1kMjU4N2E2OGVmZGUucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTAyMSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDEwMjFUMTc1NTA4WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9M2Q3MTUzMzNmOTA5NDM0YjNlYzhiODkyNjAwZWE5NDAzZWZlMzZjNDNkMTllYTk0ZTdiZTZjNzEyZmNjMjg2NyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.bRXJt6K1ogUdJp74ZZDM6IdNaEjGxqtQFfjDEjHYsGo"">\r\n\r\nI very much like the new views. What many users fail but is a common modelling pattern is to use mapped tasks. This usually needs a few more clicks to find and navigate down. Is there a way to have a direct drill-down also on mapped tasks on the new detail views? (As if it is vitually a task group with a set of tasks from UI perspective, just the volume is flexible per run).', 'created_at': datetime.datetime(2024, 10, 21, 18, 11, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2427483202, 'issue_id': 2538840632, 'author': 'bbovenzi', 'body': '@jscheffl \r\n\r\n- Good ideas on logs and xcoms. Both make sense to me\r\n- We don\'t actually have any user settings saved except in localstorage which is why, for now, we have the ""Recently viewed Dags"" section in the homepage dashboard. Happy to figure out a proper user favorites though.\r\n- Yes, any failed mapped task instance should have quick links to just like any other TI. So far I think we\'ll keep the existing UX of adding a Mapped Tasks tab to the Task Instance details page/drawer. In the case of the drawer, you can see the graph and the list of mapped tasks within the same view. If we manage to show logs across a whole dag run, perhaps we can show logs across a Task Group or all mapped tasks.', 'created_at': datetime.datetime(2024, 10, 21, 18, 56, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2427528311, 'issue_id': 2538840632, 'author': 'kaxil', 'body': 'Loving the new views', 'created_at': datetime.datetime(2024, 10, 21, 19, 19, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2427845796, 'issue_id': 2538840632, 'author': 'ldacey', 'body': 'Looks great.  Are there any new views for Datasets/Assets in the works as well? \r\n\r\nI have been splitting my DAGs up, one (or more) for extracting raw data and the other for processing/transformation. Right now, the dataset view shows a lot of dags instead of just the inlets/outlets for the DAG I am looking at (sometimes for other clients so it is not a view i can use while sharing my screen). \r\n\r\n(extract DAG):\r\nlist_files (source dataset) -> extract_files (raw dataset) -> trigger_dag (outlet = raw dataset)\r\n\r\n(transform DAG):\r\nscheduled on raw dataset\r\n\r\ntransform_files (bronze dataset) -> deduplicate (silver dataset) -> finalize (gold dataset)\r\n\r\n\r\nIt would be nice to be able to see the lineage **from** the DAG if possible. So if I am within the transform DAG then I would see something like source > raw > bronze > silver > gold and be able to click on a link to the raw DAG.', 'created_at': datetime.datetime(2024, 10, 21, 22, 20, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428132926, 'issue_id': 2538840632, 'author': 'cmarteepants', 'body': '@ldacey they are in the works! we are slotted to work on them once we wrap up the dag view designs. agreed! this is something we want to show in the new asset views', 'created_at': datetime.datetime(2024, 10, 22, 3, 26, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428637878, 'issue_id': 2538840632, 'author': 'pierrejeambrun', 'body': 'Really nice! 🎉', 'created_at': datetime.datetime(2024, 10, 22, 8, 39, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2474061771, 'issue_id': 2538840632, 'author': 'bbovenzi', 'body': 'Backfills:\n\nSee more button lists all possible Dag actions:\n![Image](https://github.com/user-attachments/assets/7105cfa3-d3b3-41ce-9a5f-d41c914369b2)\n\n""Run Backfill"" opens a modal:\n(Need to add ""dry_run"" response here of how many runs will be rerun or created based on backfill options)\n![Image](https://github.com/user-attachments/assets/88f2a175-3974-4464-879e-a5e0f3463510)\n\nAdd a banner to the Dag page when there is an active backfill\n![Image](https://github.com/user-attachments/assets/aa6d83a6-3a80-4b68-add8-3b9b812edd27)\n![Image](https://github.com/user-attachments/assets/4841ac51-bc92-4274-a417-75103ce7e48d)\n\nTab to list all backfills:\n![Image](https://github.com/user-attachments/assets/00b001c6-25b4-496b-bdb1-b264704f57b4)\n\nClicking on a singler backfill shows details about it like all dag runs its affected.\n![Image](https://github.com/user-attachments/assets/aad0e58c-44cb-469c-8d24-8ecd075a9508)', 'created_at': datetime.datetime(2024, 11, 13, 16, 10, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2486816921, 'issue_id': 2538840632, 'author': 'ausiddiqui', 'body': '> Task details to see how a task has performed across instances, same as a DAG overview.\n> \n> <img alt=""Screenshot 2024-10-21 at 9 36 43\u202fAM"" width=""1458"" src=""https://private-user-images.githubusercontent.com/4600967/378472068-9c414eae-2585-4ae5-8ccb-1611e4b695f7.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzIwNTI5NTIsIm5iZiI6MTczMjA1MjY1MiwicGF0aCI6Ii80NjAwOTY3LzM3ODQ3MjA2OC05YzQxNGVhZS0yNTg1LTRhZTUtOGNjYi0xNjExZTRiNjk1ZjcucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MTExOSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDExMTlUMjE0NDEyWiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9YmIxMTQ5ODdkYzExMWU5NzI5MWZiZmViYTQyYTIwMGQzZjk3M2ZhZmNkOGM4MGU5ZWYwYjYzMmI1NmU1OTI4MCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QifQ.PsBfMYdri0A6eCvjUsZ9bqJco6dwQ3hl7aEgwgY-_z0"">\n\nTask run duration would be a great bar graph, and having the average over the x number of runs displayed would be awesome.', 'created_at': datetime.datetime(2024, 11, 19, 21, 46, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2488952005, 'issue_id': 2538840632, 'author': 'cmarteepants', 'body': ""@ausiddiqui not sure if it's just me, but I can't see your screenshot. Agree conceptually that task run duration would make a great bar graph!"", 'created_at': datetime.datetime(2024, 11, 20, 15, 52, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2565256252, 'issue_id': 2538840632, 'author': 'ausiddiqui', 'body': ""@cmarteepants this is the screenshot from @bbovenzi from 10/21 about the Task details. \r\n\r\n![image](https://github.com/user-attachments/assets/e72307be-71b8-4462-af32-75a5446aacf2)\r\n\r\nFrom the existing Airflow 2 Task Duration charts for both the line graph and bar chart views some items that would be nice to haves:\r\n\r\n- Ability to remove/exclude failed or canceled task attempts, so we only look at succesful runs; often we are trying to gauge the best case scenario how long a particular task or tasks take, we end up having to manually calculate / exclude the outlier where it re-ran a bunch of times and failed mid-way due to some issue from the estimate\r\n- When looking at the Task duration graph for the DAG (with all the tasks and having option of line graph / bar chart) there isn't a median aggregate available, this is only available for an individual tasks's Task duration chart (the column only chart). This would be great to bring to the DAG level across all tasks and have ability to exclude failed tasks as noted above. Option to have median and average would be nice.\r\n- The current navigation between the Task-level task duration chart back to the DAG level task-duration chart isn't simple. The real flow is to click on the DAG link in the Purple box area. Would be great to have another way to access switch between task-level to DAG level task duration chart more intutively, like thorugh the Grid chart itself on the left as well. \r\n\r\n![image](https://github.com/user-attachments/assets/7a119995-ee39-4f0c-81b0-3d1efab9058c)"", 'created_at': datetime.datetime(2024, 12, 30, 9, 51, 16, tzinfo=datetime.timezone.utc)}]","bbovenzi (Assginee) on (2024-10-03 08:32:01 UTC): Upgrading Cluster Activity to be the main dashboard for a new home page.

Stories:
- What has happened across my entire Airflow deployment recently?
- Can I quickly see any issues and drill down into them?

![Image](https://github.com/user-attachments/assets/843e837e-39d5-41c5-baa2-5527decc1348)

bbovenzi (Assginee) on (2024-10-03 08:34:05 UTC): Card-style view of the DAGs list page.

Users can still toggle to a more condensed table view. A few pieces here are still subject to change.

Stories:
- Show me a list of my dags with a summary of their activity
- We have a big org. I want to filter the list to just the specific dags I care about.
![Image](https://github.com/user-attachments/assets/f1c17a53-1020-4d4f-9571-adf942ba6329)

bbovenzi (Assginee) on (2024-10-03 08:35:02 UTC): Advanced queries for list views:
![Image](https://github.com/user-attachments/assets/cf56967b-6215-4ab6-a9c2-d7ad92963c43)

bbovenzi (Assginee) on (2024-10-03 08:36:02 UTC): New vertical navigation with subnav and menu options

![Image](https://github.com/user-attachments/assets/6b776b95-7e4d-4aa5-a676-e4140a0bbf85)

potiuk on (2024-10-04 01:41:03 UTC): Slick!

bbovenzi (Assginee) on (2024-10-21 14:31:20 UTC): DAG Details

Stories:
- Show me an overview of this DAG's activity
- Highlight failed task instances and let me quickly get to their logs

<img width=""1457"" alt=""Screenshot 2024-10-21 at 9 35 33 AM"" src=""https://github.com/user-attachments/assets/42d9783f-79b5-4752-9718-e6d556957f57"">
We're going to make that top stats bar smaller and remove ""Slow Tasks""

bbovenzi (Assginee) on (2024-10-21 14:32:12 UTC): DAG Details, view list of runs with a bar chart to help as a date range selector:

<img width=""1455"" alt=""Screenshot 2024-10-21 at 9 35 45 AM"" src=""https://github.com/user-attachments/assets/bd480afe-1f37-41d8-a70a-50c35f5909c8"">

bbovenzi (Assginee) on (2024-10-21 14:33:03 UTC): DAG Details, list of tasks with their own graph of recent task instances:


Stories:
- Show me a list of my tasks with quick history of their recent instances. 

<img width=""1459"" alt=""Screenshot 2024-10-21 at 9 36 25 AM"" src=""https://github.com/user-attachments/assets/3cb02df3-488f-4bc6-9d16-5896101aacdb"">

bbovenzi (Assginee) on (2024-10-21 14:34:35 UTC): DAG Run Details, with the ability to see task logs for the entire run at once (we need to see if this is actually feasible to load so many logs at once)

<img width=""1483"" alt=""Screenshot 2024-10-21 at 9 36 14 AM"" src=""https://github.com/user-attachments/assets/84a898d6-f319-43ea-9af7-e596209e41b1"">

bbovenzi (Assginee) on (2024-10-21 14:35:24 UTC): Task details to see how a task has performed across instances, same as a DAG overview.

<img width=""1458"" alt=""Screenshot 2024-10-21 at 9 36 43 AM"" src=""https://github.com/user-attachments/assets/9c414eae-2585-4ae5-8ccb-1611e4b695f7"">

bbovenzi (Assginee) on (2024-10-21 14:36:23 UTC): Task Instance details with logs:

<img width=""1460"" alt=""Screenshot 2024-10-21 at 9 36 52 AM"" src=""https://github.com/user-attachments/assets/ff1a449a-dea9-480a-b966-03cbce93882a"">

bbovenzi (Assginee) on (2024-10-21 14:38:23 UTC): Graph, Grid and Gantt open up in a full-page modal that can be accessed from any Dag/Run/Task page. The modal also has a right hand drawer for information on the selected dag/run/task.

Motivation:
- Grid/Gantt/graph can all be complicated views. Give them the space of a full-page modal but still include a drawer for some details

<img width=""1460"" alt=""Screenshot 2024-10-21 at 9 37 17 AM"" src=""https://github.com/user-attachments/assets/37a4e0d3-a8c1-42b6-ad90-eb36b12b5642"">

<img width=""1381"" alt=""Screenshot 2024-10-21 at 9 37 44 AM"" src=""https://github.com/user-attachments/assets/26ce24f9-3748-49ad-854e-e14b70e419a6"">

<img width=""1456"" alt=""Screenshot 2024-10-21 at 9 37 51 AM"" src=""https://github.com/user-attachments/assets/a1859521-15db-4e34-91b7-d2587a68efde"">

bbovenzi (Assginee) on (2024-10-21 14:41:53 UTC): Logs and graph side-by-side:

<img width=""1457"" alt=""Screenshot 2024-10-21 at 10 41 39 AM"" src=""https://github.com/user-attachments/assets/8d434252-4d4f-45cd-b387-7c8343fa1ab6"">

potiuk on (2024-10-21 17:30:30 UTC): Nice :)

jscheffl on (2024-10-21 17:59:46 UTC): I don't know if this issue is the right way to discuss. let me know if there are other means...

I'd propose to move the XComs to ""Browse..."" as actually it is a legacy that you browse XComs in the Admin menu... but usually this is not an admin-specific thing.

jscheffl on (2024-10-21 18:03:25 UTC): As we might not have a ""Multie Team"" setup in most environments... and as usually you have more than a hand-full of DAGs - can we foresee an option that a user can mark DAGs (on his profile) like ""Favorite DAGs"" which are displayed first before a long long list? (to focus on). At least we have many users who are mainly interested in 3-5 of 100 DAGs.

jscheffl on (2024-10-21 18:07:05 UTC): One thing that botheres me since a long time and is contained in the Lorem-Ipsum as well - there are always 3 lines where the Log is sourced from. Nice for troubleshooting bust most users are confused by this and wonder why link is most cases noch loading when clicking (because internal protected S3 backend, not public exposed...) - logs should focus on valueable content, in the summary maybe the last N lines and not the first lines.

jscheffl on (2024-10-21 18:11:15 UTC): I very much like the new views. What many users fail but is a common modelling pattern is to use mapped tasks. This usually needs a few more clicks to find and navigate down. Is there a way to have a direct drill-down also on mapped tasks on the new detail views? (As if it is vitually a task group with a set of tasks from UI perspective, just the volume is flexible per run).

bbovenzi (Assginee) on (2024-10-21 18:56:02 UTC): @jscheffl 

- Good ideas on logs and xcoms. Both make sense to me
- We don't actually have any user settings saved except in localstorage which is why, for now, we have the ""Recently viewed Dags"" section in the homepage dashboard. Happy to figure out a proper user favorites though.
- Yes, any failed mapped task instance should have quick links to just like any other TI. So far I think we'll keep the existing UX of adding a Mapped Tasks tab to the Task Instance details page/drawer. In the case of the drawer, you can see the graph and the list of mapped tasks within the same view. If we manage to show logs across a whole dag run, perhaps we can show logs across a Task Group or all mapped tasks.

kaxil on (2024-10-21 19:19:11 UTC): Loving the new views

ldacey on (2024-10-21 22:20:33 UTC): Looks great.  Are there any new views for Datasets/Assets in the works as well? 

I have been splitting my DAGs up, one (or more) for extracting raw data and the other for processing/transformation. Right now, the dataset view shows a lot of dags instead of just the inlets/outlets for the DAG I am looking at (sometimes for other clients so it is not a view i can use while sharing my screen). 

(extract DAG):
list_files (source dataset) -> extract_files (raw dataset) -> trigger_dag (outlet = raw dataset)

(transform DAG):
scheduled on raw dataset

transform_files (bronze dataset) -> deduplicate (silver dataset) -> finalize (gold dataset)


It would be nice to be able to see the lineage **from** the DAG if possible. So if I am within the transform DAG then I would see something like source > raw > bronze > silver > gold and be able to click on a link to the raw DAG.

cmarteepants (Assginee) on (2024-10-22 03:26:36 UTC): @ldacey they are in the works! we are slotted to work on them once we wrap up the dag view designs. agreed! this is something we want to show in the new asset views

pierrejeambrun (Issue Creator) on (2024-10-22 08:39:40 UTC): Really nice! 🎉

bbovenzi (Assginee) on (2024-11-13 16:10:11 UTC): Backfills:

See more button lists all possible Dag actions:
![Image](https://github.com/user-attachments/assets/7105cfa3-d3b3-41ce-9a5f-d41c914369b2)

""Run Backfill"" opens a modal:
(Need to add ""dry_run"" response here of how many runs will be rerun or created based on backfill options)
![Image](https://github.com/user-attachments/assets/88f2a175-3974-4464-879e-a5e0f3463510)

Add a banner to the Dag page when there is an active backfill
![Image](https://github.com/user-attachments/assets/aa6d83a6-3a80-4b68-add8-3b9b812edd27)
![Image](https://github.com/user-attachments/assets/4841ac51-bc92-4274-a417-75103ce7e48d)

Tab to list all backfills:
![Image](https://github.com/user-attachments/assets/00b001c6-25b4-496b-bdb1-b264704f57b4)

Clicking on a singler backfill shows details about it like all dag runs its affected.
![Image](https://github.com/user-attachments/assets/aad0e58c-44cb-469c-8d24-8ecd075a9508)

ausiddiqui on (2024-11-19 21:46:02 UTC): Task run duration would be a great bar graph, and having the average over the x number of runs displayed would be awesome.

cmarteepants (Assginee) on (2024-11-20 15:52:11 UTC): @ausiddiqui not sure if it's just me, but I can't see your screenshot. Agree conceptually that task run duration would make a great bar graph!

ausiddiqui on (2024-12-30 09:51:16 UTC): @cmarteepants this is the screenshot from @bbovenzi from 10/21 about the Task details. 

![image](https://github.com/user-attachments/assets/e72307be-71b8-4462-af32-75a5446aacf2)

From the existing Airflow 2 Task Duration charts for both the line graph and bar chart views some items that would be nice to haves:

- Ability to remove/exclude failed or canceled task attempts, so we only look at succesful runs; often we are trying to gauge the best case scenario how long a particular task or tasks take, we end up having to manually calculate / exclude the outlier where it re-ran a bunch of times and failed mid-way due to some issue from the estimate
- When looking at the Task duration graph for the DAG (with all the tasks and having option of line graph / bar chart) there isn't a median aggregate available, this is only available for an individual tasks's Task duration chart (the column only chart). This would be great to bring to the DAG level across all tasks and have ability to exclude failed tasks as noted above. Option to have median and average would be nice.
- The current navigation between the Task-level task duration chart back to the DAG level task-duration chart isn't simple. The real flow is to click on the DAG link in the Purple box area. Would be great to have another way to access switch between task-level to DAG level task duration chart more intutively, like thorugh the Grid chart itself on the left as well. 

![image](https://github.com/user-attachments/assets/7a119995-ee39-4f0c-81b0-3d1efab9058c)

"
2538752313,issue,closed,completed,AIP-84 Migrate public endpoints from api_connexion to the new Rest API,"### Body

https://github.com/apache/airflow/issues/42366 is taking care of migrating private endpoints.

This issue focuses on migrating all the public ones. Ideally they shouldn't change too much and be feature rich compare to airflow 2.x.

An exemple PR can be found here https://github.com/apache/airflow/pull/42196


### Tasks (total ~ 81 endpoints) 
####  Backfill (6 endpoints) @dstandish 
- `list_backfills` https://github.com/apache/airflow/pull/43496 @dstandish 
- `get_backfill` https://github.com/apache/airflow/pull/43496 @dstandish 
- `pause_backfill` https://github.com/apache/airflow/pull/43496 @dstandish 
- `unpause_backfill` https://github.com/apache/airflow/pull/43496 @dstandish 
- `cancel_backfill` https://github.com/apache/airflow/pull/43496 @dstandish 
- `create_backfill` https://github.com/apache/airflow/pull/43496 @dstandish 

#### Config (2 endpoints) @rawwar 
- https://github.com/apache/airflow/issues/42745 @jason810496 

#### Connection @bugraoz93  (6 endpoints)
- `api_connexion/endpoints/connection_endpoint.py` `delete_connection` https://github.com/apache/airflow/issues/42559 @bugraoz93 
- `api_connexion/endpoints/connection_endpoint.py` `get_connection` https://github.com/apache/airflow/issues/42590 @rawwar  
- `api_connexion/endpoints/connection_endpoint.py` `get_connections`  https://github.com/apache/airflow/issues/42591 @bugraoz93 
- `api_connexion/endpoints/connection_endpoint.py` `patch_connection` https://github.com/apache/airflow/issues/42592 @bugraoz93 
- `api_connexion/endpoints/connection_endpoint.py` `post_connection` https://github.com/apache/airflow/issues/42593 @bugraoz93 
- `api_connexion/endpoints/connection_endpoint.py` `test_connection` https://github.com/apache/airflow/issues/42594 @bugraoz93 

####  DAG (5 endpoints)
- `api_connexion/endpoints/dag_endpoint.py` `get_dag_details`  https://github.com/apache/airflow/issues/42453 @omkar-foss
- `api_connexion/endpoints/dag_endpoint.py` `patch_dag` https://github.com/apache/airflow/issues/42468 @pierrejeambrun 
- `api_connexion/endpoints/dag_endpoint.py` `patch_dags` https://github.com/apache/airflow/issues/42544 @pierrejeambrun 
- `api_connexion/endpoints/dag_endpoint.py` `get_dag` https://github.com/apache/airflow/issues/42652 @omkar-foss 
- `api_connexion/endpoints/dag_endpoint.py` `delete_dag` https://github.com/apache/airflow/issues/42650 @omkar-foss

#### Dag Parsing (1 endpoint)
- https://github.com/apache/airflow/pull/44416/ @prabhusneha 

#### Dag Runs @rawwar (9 endpoints)
- All dag run endpoints`api_connexion/endpoints/dag_run_endpoint.py` https://github.com/apache/airflow/issues/42701 @rawwar 

#### Dag Source (1 endpoint) 
- https://github.com/apache/airflow/issues/42876 @omkar-foss 

#### Dag Stats (1 endpoint)
- https://github.com/apache/airflow/issues/42877 @omkar-foss 

#### Dag Warnings (1 endpoint) @rawwar 
- https://github.com/apache/airflow/issues/42748 @rawwar 

#### Dataset / Assets (10 endpoints) @amoghrajesh
- `get_assets` https://github.com/apache/airflow/pull/43783 @amoghrajesh
- `get_asset` https://github.com/apache/airflow/pull/43825 @amoghrajesh 
- `get_upstream_asset_events` https://github.com/apache/airflow/pull/43874 @amoghrajesh
- `get_asset_events` https://github.com/apache/airflow/pull/43881 @vatsrahul1001
- `create_asset_event` https://github.com/apache/airflow/pull/43984 @vatsrahul1001 
- `get_asset_queued_events` https://github.com/apache/airflow/pull/44139 @vatsrahul1001 
- `delete_asset_queued_events` https://github.com/apache/airflow/pull/44138 @vatsrahul1001 
- `get_dag_asset_queued_events` https://github.com/apache/airflow/pull/44124 @amoghrajesh
- `delete_dag_asset_queued_events` https://github.com/apache/airflow/pull/44129 @amoghrajesh
- `delete_dag_asset_queued_event` https://github.com/apache/airflow/pull/44130 @amoghrajesh
- `get_dag_asset_queued_event` https://github.com/apache/airflow/pull/44128 @amoghrajesh

#### Event Logs (2 endpoints)

- https://github.com/apache/airflow/issues/43326 @jason810496 
- https://github.com/apache/airflow/issues/43327 @jason810496 

#### Extra Link (1 endpoint)
- https://github.com/apache/airflow/pull/44277 @prabhusneha

#### Monitor (2 endpoints) 
- https://github.com/apache/airflow/issues/42937 @bbovenzi
- https://github.com/apache/airflow/issues/42879 @omkar-foss 

#### Import Error (2 endpoints) @jason810496
- https://github.com/apache/airflow/issues/43594 @jason810496
https://github.com/apache/airflow/issues/43595 @jason810496

#### Log (1 endpoint)
- https://github.com/apache/airflow/pull/44238 @utkarsharma2 

#### Plugin (1 endpoint)
- https://github.com/apache/airflow/pull/43125 @pierrejeambrun 

#### Pool (5 endpoints) @pierrejeambrun
- https://github.com/apache/airflow/pull/43165 @pierrejeambrun 
- https://github.com/apache/airflow/pull/43221 @pierrejeambrun
- https://github.com/apache/airflow/pull/43223 @pierrejeambrun
- https://github.com/apache/airflow/pull/43266 @pierrejeambrun
- https://github.com/apache/airflow/pull/43317 @pierrejeambrun

#### Provider (1 endpoint)
 - https://github.com/apache/airflow/pull/43159 @pierrejeambrun

#### Task (2 endpoints) @omkar-foss 
- https://github.com/apache/airflow/issues/42874 @omkar-foss 
- https://github.com/apache/airflow/issues/42875 @omkar-foss 

#### Task Instance (15 endpoints) @pierrejeambrun
- https://github.com/apache/airflow/pull/43485 @pierrejeambrun 
- https://github.com/apache/airflow/pull/43548 @pierrejeambrun 
- https://github.com/apache/airflow/pull/43642 @pierrejeambrun 
- https://github.com/apache/airflow/issues/43748 @pierrejeambrun 
- https://github.com/apache/airflow/issues/43749 @pierrejeambrun
- https://github.com/apache/airflow/issues/43750 @pierrejeambrun
- https://github.com/apache/airflow/issues/43756 @pierrejeambrun
- https://github.com/apache/airflow/issues/43751 @omkar-foss 
- https://github.com/apache/airflow/issues/43753 @omkar-foss 
- https://github.com/apache/airflow/issues/43754 @omkar-foss 
- https://github.com/apache/airflow/issues/43752 @omkar-foss
-  https://github.com/apache/airflow/pull/43675 @kandharvishnu 
- https://github.com/apache/airflow/pull/44206 @kandharvishnu
- https://github.com/apache/airflow/pull/44301 @kandharvishnu  
- https://github.com/apache/airflow/pull/44303 @kandharvishnu


#### Variables (5 endpoints)
- https://github.com/apache/airflow/pull/42798 @pierrejeambrun 
- https://github.com/apache/airflow/pull/42834 @pierrejeambrun 
- https://github.com/apache/airflow/pull/42929 @pierrejeambrun 
- https://github.com/apache/airflow/pull/42948 @pierrejeambrun 
- https://github.com/apache/airflow/pull/43083 @pierrejeambrun 

#### XCom (2 endpoints)
- https://github.com/apache/airflow/issues/42978 @michaeljs-c 
- https://github.com/apache/airflow/issues/42980 @michaeljs-c 


### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-09-20 12:55:30+00:00,"['bbovenzi', 'pierrejeambrun']",2024-11-28 17:15:31+00:00,2024-11-28 16:43:29+00:00,https://github.com/apache/airflow/issues/42370,"[('area:API', ""Airflow's REST/HTTP API""), ('kind:meta', 'High-level information important to the community'), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2367363716, 'issue_id': 2538752313, 'author': 'omkar-foss', 'body': 'Trimming down `views.py` is very tempting 😄\r\n\r\nMay I pick this up? If yes, please assign this issue to me. Thanks!', 'created_at': datetime.datetime(2024, 9, 23, 6, 55, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2367587995, 'issue_id': 2538752313, 'author': 'pierrejeambrun', 'body': ""Hello @omkar-foss,\r\n\r\n`views.py` are mostly 'rendered' front end that will get replaced by the new UI react, in `views.py` you also find `private/UI` REST route, most of them are with the `/object/` prefix, such as `def grid_data`. If you want to work on those UI dedicated endpoints you can that would be part of https://github.com/apache/airflow/issues/42366.\r\n\r\nThis issue focuses on the public endpoints, those are located under the `api_connexion` folder.\r\n\r\nIn anycase, just mention the endpoint you are working on, so people do not do the same ones, and I will update the description to track that :). I think we should do one at the time."", 'created_at': datetime.datetime(2024, 9, 23, 8, 49, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2368092341, 'issue_id': 2538752313, 'author': 'omkar-foss', 'body': ""Hi @pierrejeambrun, thank you for explaining it to me, much appreciate it.\r\n\r\nI'll start migrating the public endpoints, beginning with migration of the DAG Details API (`get_dag_details`) in `api_connexion/endpoints/dag_endpoint.py`.\r\n\r\nSince this is a meta issue (missed that previously!), I'll create the issue/PR corresponding to the endpoint(s) that I'll be working on and link to this issue (like usual).\r\n\r\nP.S: `views.py` will have to wait :)"", 'created_at': datetime.datetime(2024, 9, 23, 12, 37, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2380872418, 'issue_id': 2538752313, 'author': 'bugraoz93', 'body': 'Thanks for creating the ticket @pierrejeambrun! Let me take connection endpoints', 'created_at': datetime.datetime(2024, 9, 28, 19, 25, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382377055, 'issue_id': 2538752313, 'author': 'pierrejeambrun', 'body': 'Great, thanks @bugraoz93', 'created_at': datetime.datetime(2024, 9, 30, 7, 56, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2383731046, 'issue_id': 2538752313, 'author': 'bugraoz93', 'body': 'Thanks for assigning! Let me save you from regular updates :) I created issues for all the endpoints in connections.', 'created_at': datetime.datetime(2024, 9, 30, 17, 4, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2385104040, 'issue_id': 2538752313, 'author': 'pierrejeambrun', 'body': 'Great, thanks @bugraoz93. I just updated the meta issue with those new additions. Let me know if you plan to work on all of them so I can assign you, or just some of them so I can label the others with `good first issue` and let other contributors grab them :)', 'created_at': datetime.datetime(2024, 10, 1, 8, 19, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2386345212, 'issue_id': 2538752313, 'author': 'bugraoz93', 'body': 'I am planning to work on all of them @pierrejeambrun. if you can assign them to me, that would be great! Thanks :)', 'created_at': datetime.datetime(2024, 10, 1, 15, 34, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2387768146, 'issue_id': 2538752313, 'author': 'omkar-foss', 'body': 'Hey folks! After https://github.com/apache/airflow/pull/42631 (open for review), I can pick up these two next in order:\r\n\r\n1. `get_dag` - https://github.com/apache/airflow/issues/42652\r\n2. `delete_dag` - https://github.com/apache/airflow/issues/42650\r\n\r\nWith these 2 done, I suppose we should have the `dags.py` migration complete. When you find some time, please update this meta and assign these to me, thank you! 😃', 'created_at': datetime.datetime(2024, 10, 2, 7, 6, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2387864690, 'issue_id': 2538752313, 'author': 'bbovenzi', 'body': '@omkar-foss Assigned. Thanks for picking these issues up!', 'created_at': datetime.datetime(2024, 10, 2, 8, 4, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2389655774, 'issue_id': 2538752313, 'author': 'rawwar', 'body': 'I am working on the following \r\n1. get_connection - #42674 \r\n2. get_connections - #42677', 'created_at': datetime.datetime(2024, 10, 2, 20, 38, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2389973864, 'issue_id': 2538752313, 'author': 'omkar-foss', 'body': ""Hey folks, just a heads-up on Pydantic aliases - try not to use `AliasGenerator` as it breaks Airflow lowest direct dependency resolution tests with a `TypeError` because lower versions of Pydantic wouldn't be having [this patch](https://github.com/pydantic/pydantic/pull/8806). The TypeError looks like `alias_generator ... must return str, not ...` (for more info see [this issue](https://github.com/pydantic/pydantic/issues/8768)).\r\n\r\nCan use either `AliasChoices` or `AliasPath` as those work fine. I faced this issue in https://github.com/apache/airflow/pull/42631 and spent a while on it, so just thought should let you all know!"", 'created_at': datetime.datetime(2024, 10, 2, 23, 27, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2390232901, 'issue_id': 2538752313, 'author': 'potiuk', 'body': '> Hey folks, just a heads-up on Pydantic aliases - try not to use AliasGenerator as it breaks Airflow lowest direct \r\n\r\nWe can also update the minimum version of Pydantic to the version that is fixed. This is the feature of the ""lowest direct"" tests that they detect such minimum requirements are not met. So maybe you can figure out which is the minimum version and we set it @omkar-foss ?', 'created_at': datetime.datetime(2024, 10, 3, 0, 20, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2390639349, 'issue_id': 2538752313, 'author': 'omkar-foss', 'body': '>This is the feature of the ""lowest direct"" tests that they detect such minimum requirements are not met.\r\n\r\nIt\'s a very cool feature! Helping to find issues we normally wouldn\'t find during regular unit or integration tests.\r\n\r\n>We can also update the minimum version of Pydantic to the version that is fixed.\r\n>So maybe you can figure out which is the minimum version and we set it\r\n\r\nThat would be great, the minimum Pydantic version that is fixed is v2.6.4, [this commit](https://github.com/pydantic/pydantic/commit/6214a4bc472bfc4cd72232ab5d407dc640dc64aa#diff-648afe3d986261d8f2015b2b131b0e4a448d4dc6946cfde1a7a836876cee255eR9) for quick reference.', 'created_at': datetime.datetime(2024, 10, 3, 6, 40, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2390654521, 'issue_id': 2538752313, 'author': 'potiuk', 'body': '> That would be great, the minimum Pydantic version that is fixed is v2.6.4, [this commit](https://github.com/pydantic/pydantic/commit/6214a4bc472bfc4cd72232ab5d407dc640dc64aa#diff-648afe3d986261d8f2015b2b131b0e4a448d4dc6946cfde1a7a836876cee255eR9) for quick reference.\r\n\r\nAdded min version for Pydantic in https://github.com/apache/airflow/pull/42694', 'created_at': datetime.datetime(2024, 10, 3, 6, 51, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2390898128, 'issue_id': 2538752313, 'author': 'pierrejeambrun', 'body': '@rawwar \r\n\r\n> I am working on the following\r\n\r\n> get_connection - https://github.com/apache/airflow/pull/42674\r\n> get_connections - AIP-84 Migrate the public endpoint Get Connections to FastAPI  #42677`\r\n\r\nThanks for your contribution and help on AIP-84, as mentioned above, I think @bugraoz93 is already working on connections endpoints, or is planning to do so. Feel free to choose any other endpoints that looks cool to you, under `api_connexions/endpoints`. Besides `dag_endpoint` and `connection_endpoint` eveything is up for a grab, just let me know and I will assign you :)', 'created_at': datetime.datetime(2024, 10, 3, 9, 4, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2397071893, 'issue_id': 2538752313, 'author': 'pierrejeambrun', 'body': 'Assigning myself to variables', 'created_at': datetime.datetime(2024, 10, 7, 14, 20, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2403394709, 'issue_id': 2538752313, 'author': 'omkar-foss', 'body': ""I've created new issues for sections Dag Source (1 endpoint), Dag Stats (1 endpoint), Task (2 endpoints) and Version (1 endpoint). List as below:\r\n\r\n1. https://github.com/apache/airflow/issues/42876\r\n2. https://github.com/apache/airflow/issues/42877\r\n3. https://github.com/apache/airflow/issues/42874\r\n4. https://github.com/apache/airflow/issues/42875\r\n5. https://github.com/apache/airflow/issues/42879\r\n\r\nPlease assign these to me, I'll pick these up one by one in the next few days, right after https://github.com/apache/airflow/issues/42650. Cheers."", 'created_at': datetime.datetime(2024, 10, 9, 20, 43, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2404379517, 'issue_id': 2538752313, 'author': 'pierrejeambrun', 'body': '@omkar-foss assigned and meta issue updated, thanks :)', 'created_at': datetime.datetime(2024, 10, 10, 8, 7, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2409018455, 'issue_id': 2538752313, 'author': 'michaeljs-c', 'body': ""Hi @pierrejeambrun I've added issues for XCom, please assign to me 🙂 thanks"", 'created_at': datetime.datetime(2024, 10, 13, 15, 18, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2410340643, 'issue_id': 2538752313, 'author': 'pierrejeambrun', 'body': ""> Hi @pierrejeambrun I've added issues for XCom, please assign to me 🙂 thanks\r\n\r\nDone, and meta task is updated.\r\n\r\nThanks for taking those ones @michaeljs-c!"", 'created_at': datetime.datetime(2024, 10, 14, 7, 54, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2431080418, 'issue_id': 2538752313, 'author': 'jason810496', 'body': 'Hi @pierrejeambrun, I can take on the migrations for **Event Logs**:\r\n1. Get Event Log\r\n2. Get Event Logs\r\n\r\nHowever, none of the current issue form templates fit a task issue (the `Task and Meta` template tags the issue with `kind:meta` instead of `kind:feature`).\r\nCould you please create the issue and assign it to me? Thanks!', 'created_at': datetime.datetime(2024, 10, 23, 6, 58, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2451619903, 'issue_id': 2538752313, 'author': 'jason810496', 'body': 'Hi @pierrejeambrun, I can handle the migrations for **Import Error**\r\n\r\n1. `get_import_error`\r\n2. `get_import_errors`\r\n\r\nI’m still encountering the issue where I can’t create the correct issue type.\r\nCould you please create it and assign it to me? Thanks!', 'created_at': datetime.datetime(2024, 11, 1, 9, 58, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453025163, 'issue_id': 2538752313, 'author': 'rawwar', 'body': ""I've unassigned myself for the config endpoint issue. If no one takes it, I'll pick it up once I finish DagRun and DagWarning endpoints. \r\nIssue: https://github.com/apache/airflow/issues/42745 <- If anyone wants to take this, please let me know and I can assign it to you."", 'created_at': datetime.datetime(2024, 11, 2, 15, 35, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2454154713, 'issue_id': 2538752313, 'author': 'ghost', 'body': '@pierrejeambrun I am working on `get taskinstance try` endpoint', 'created_at': datetime.datetime(2024, 11, 4, 9, 5, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2459892714, 'issue_id': 2538752313, 'author': 'omkar-foss', 'body': 'Hi @pierrejeambrun, could you please assign these TI-related endpoints to me in above issue description:\r\n\r\n**GET:**\r\n- https://github.com/apache/airflow/issues/43748\r\n- https://github.com/apache/airflow/issues/43749\r\n- https://github.com/apache/airflow/issues/43750\r\n- https://github.com/apache/airflow/issues/43756\r\n\r\n**POST:**\r\n- https://github.com/apache/airflow/issues/43751\r\n\r\n**PUT:**\r\n- https://github.com/apache/airflow/issues/43753\r\n- https://github.com/apache/airflow/issues/43754\r\n- https://github.com/apache/airflow/issues/43755\r\n- https://github.com/apache/airflow/issues/43752', 'created_at': datetime.datetime(2024, 11, 6, 14, 24, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2459958855, 'issue_id': 2538752313, 'author': 'pierrejeambrun', 'body': ""Hey omkar, as you can see in I'm already assigned to the `TaskInstances` endpoint.\r\n\r\nThanks for creating the issue, we can split the work on that matter, i'll assign some of them to you maybe all the `PUT/POST`  as I am already working on GET."", 'created_at': datetime.datetime(2024, 11, 6, 14, 50, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2459971064, 'issue_id': 2538752313, 'author': 'pierrejeambrun', 'body': ""You're assigned, issue updated, let me know if that shounds good for you :)."", 'created_at': datetime.datetime(2024, 11, 6, 14, 55, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2463732766, 'issue_id': 2538752313, 'author': 'omkar-foss', 'body': ""> You're assigned, issue updated, let me know if that shounds good for you :).\r\n\r\nYes sure, that works. Thank you :)"", 'created_at': datetime.datetime(2024, 11, 8, 4, 30, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489634973, 'issue_id': 2538752313, 'author': 'bbovenzi', 'body': ""@prabhusneha I see you're assigned to Logs. What is the status of getting the logs for a task instance? That would be a useful one to get in the UI soon!"", 'created_at': datetime.datetime(2024, 11, 20, 22, 7, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2490216331, 'issue_id': 2538752313, 'author': 'utkarsharma2', 'body': ""> @prabhusneha I see you're assigned to Logs. What is the status of getting the logs for a task instance? That would be a useful one to get in the UI soon!\r\n\r\nHey, @bbovenzi I'm taking care of `task instance logs` endpoint, I'm mostly done with it, but there are a couple of test cases that require setting users in fastapi which requires a little more work. I'll keep you posted."", 'created_at': datetime.datetime(2024, 11, 21, 6, 56, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2490217308, 'issue_id': 2538752313, 'author': 'phanikumv', 'body': 'Thanks @utkarsharma2  (updated the description accordingly).', 'created_at': datetime.datetime(2024, 11, 21, 6, 57, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2506495586, 'issue_id': 2538752313, 'author': 'pierrejeambrun', 'body': 'Thanks a lot to everyone that contributed, closing a big one 🎉', 'created_at': datetime.datetime(2024, 11, 28, 16, 43, 29, tzinfo=datetime.timezone.utc)}]","omkar-foss on (2024-09-23 06:55:11 UTC): Trimming down `views.py` is very tempting 😄

May I pick this up? If yes, please assign this issue to me. Thanks!

pierrejeambrun (Issue Creator) on (2024-09-23 08:49:31 UTC): Hello @omkar-foss,

`views.py` are mostly 'rendered' front end that will get replaced by the new UI react, in `views.py` you also find `private/UI` REST route, most of them are with the `/object/` prefix, such as `def grid_data`. If you want to work on those UI dedicated endpoints you can that would be part of https://github.com/apache/airflow/issues/42366.

This issue focuses on the public endpoints, those are located under the `api_connexion` folder.

In anycase, just mention the endpoint you are working on, so people do not do the same ones, and I will update the description to track that :). I think we should do one at the time.

omkar-foss on (2024-09-23 12:37:17 UTC): Hi @pierrejeambrun, thank you for explaining it to me, much appreciate it.

I'll start migrating the public endpoints, beginning with migration of the DAG Details API (`get_dag_details`) in `api_connexion/endpoints/dag_endpoint.py`.

Since this is a meta issue (missed that previously!), I'll create the issue/PR corresponding to the endpoint(s) that I'll be working on and link to this issue (like usual).

P.S: `views.py` will have to wait :)

bugraoz93 on (2024-09-28 19:25:26 UTC): Thanks for creating the ticket @pierrejeambrun! Let me take connection endpoints

pierrejeambrun (Issue Creator) on (2024-09-30 07:56:44 UTC): Great, thanks @bugraoz93

bugraoz93 on (2024-09-30 17:04:59 UTC): Thanks for assigning! Let me save you from regular updates :) I created issues for all the endpoints in connections.

pierrejeambrun (Issue Creator) on (2024-10-01 08:19:41 UTC): Great, thanks @bugraoz93. I just updated the meta issue with those new additions. Let me know if you plan to work on all of them so I can assign you, or just some of them so I can label the others with `good first issue` and let other contributors grab them :)

bugraoz93 on (2024-10-01 15:34:30 UTC): I am planning to work on all of them @pierrejeambrun. if you can assign them to me, that would be great! Thanks :)

omkar-foss on (2024-10-02 07:06:13 UTC): Hey folks! After https://github.com/apache/airflow/pull/42631 (open for review), I can pick up these two next in order:

1. `get_dag` - https://github.com/apache/airflow/issues/42652
2. `delete_dag` - https://github.com/apache/airflow/issues/42650

With these 2 done, I suppose we should have the `dags.py` migration complete. When you find some time, please update this meta and assign these to me, thank you! 😃

bbovenzi (Assginee) on (2024-10-02 08:04:46 UTC): @omkar-foss Assigned. Thanks for picking these issues up!

rawwar on (2024-10-02 20:38:54 UTC): I am working on the following 
1. get_connection - #42674 
2. get_connections - #42677

omkar-foss on (2024-10-02 23:27:31 UTC): Hey folks, just a heads-up on Pydantic aliases - try not to use `AliasGenerator` as it breaks Airflow lowest direct dependency resolution tests with a `TypeError` because lower versions of Pydantic wouldn't be having [this patch](https://github.com/pydantic/pydantic/pull/8806). The TypeError looks like `alias_generator ... must return str, not ...` (for more info see [this issue](https://github.com/pydantic/pydantic/issues/8768)).

Can use either `AliasChoices` or `AliasPath` as those work fine. I faced this issue in https://github.com/apache/airflow/pull/42631 and spent a while on it, so just thought should let you all know!

potiuk on (2024-10-03 00:20:29 UTC): We can also update the minimum version of Pydantic to the version that is fixed. This is the feature of the ""lowest direct"" tests that they detect such minimum requirements are not met. So maybe you can figure out which is the minimum version and we set it @omkar-foss ?

omkar-foss on (2024-10-03 06:40:08 UTC): It's a very cool feature! Helping to find issues we normally wouldn't find during regular unit or integration tests.


That would be great, the minimum Pydantic version that is fixed is v2.6.4, [this commit](https://github.com/pydantic/pydantic/commit/6214a4bc472bfc4cd72232ab5d407dc640dc64aa#diff-648afe3d986261d8f2015b2b131b0e4a448d4dc6946cfde1a7a836876cee255eR9) for quick reference.

potiuk on (2024-10-03 06:51:33 UTC): Added min version for Pydantic in https://github.com/apache/airflow/pull/42694

pierrejeambrun (Issue Creator) on (2024-10-03 09:04:44 UTC): @rawwar 



Thanks for your contribution and help on AIP-84, as mentioned above, I think @bugraoz93 is already working on connections endpoints, or is planning to do so. Feel free to choose any other endpoints that looks cool to you, under `api_connexions/endpoints`. Besides `dag_endpoint` and `connection_endpoint` eveything is up for a grab, just let me know and I will assign you :)

pierrejeambrun (Issue Creator) on (2024-10-07 14:20:41 UTC): Assigning myself to variables

omkar-foss on (2024-10-09 20:43:53 UTC): I've created new issues for sections Dag Source (1 endpoint), Dag Stats (1 endpoint), Task (2 endpoints) and Version (1 endpoint). List as below:

1. https://github.com/apache/airflow/issues/42876
2. https://github.com/apache/airflow/issues/42877
3. https://github.com/apache/airflow/issues/42874
4. https://github.com/apache/airflow/issues/42875
5. https://github.com/apache/airflow/issues/42879

Please assign these to me, I'll pick these up one by one in the next few days, right after https://github.com/apache/airflow/issues/42650. Cheers.

pierrejeambrun (Issue Creator) on (2024-10-10 08:07:26 UTC): @omkar-foss assigned and meta issue updated, thanks :)

michaeljs-c on (2024-10-13 15:18:41 UTC): Hi @pierrejeambrun I've added issues for XCom, please assign to me 🙂 thanks

pierrejeambrun (Issue Creator) on (2024-10-14 07:54:20 UTC): Done, and meta task is updated.

Thanks for taking those ones @michaeljs-c!

jason810496 on (2024-10-23 06:58:01 UTC): Hi @pierrejeambrun, I can take on the migrations for **Event Logs**:
1. Get Event Log
2. Get Event Logs

However, none of the current issue form templates fit a task issue (the `Task and Meta` template tags the issue with `kind:meta` instead of `kind:feature`).
Could you please create the issue and assign it to me? Thanks!

jason810496 on (2024-11-01 09:58:28 UTC): Hi @pierrejeambrun, I can handle the migrations for **Import Error**

1. `get_import_error`
2. `get_import_errors`

I’m still encountering the issue where I can’t create the correct issue type.
Could you please create it and assign it to me? Thanks!

rawwar on (2024-11-02 15:35:08 UTC): I've unassigned myself for the config endpoint issue. If no one takes it, I'll pick it up once I finish DagRun and DagWarning endpoints. 
Issue: https://github.com/apache/airflow/issues/42745 <- If anyone wants to take this, please let me know and I can assign it to you.

ghost on (2024-11-04 09:05:45 UTC): @pierrejeambrun I am working on `get taskinstance try` endpoint

omkar-foss on (2024-11-06 14:24:21 UTC): Hi @pierrejeambrun, could you please assign these TI-related endpoints to me in above issue description:

**GET:**
- https://github.com/apache/airflow/issues/43748
- https://github.com/apache/airflow/issues/43749
- https://github.com/apache/airflow/issues/43750
- https://github.com/apache/airflow/issues/43756

**POST:**
- https://github.com/apache/airflow/issues/43751

**PUT:**
- https://github.com/apache/airflow/issues/43753
- https://github.com/apache/airflow/issues/43754
- https://github.com/apache/airflow/issues/43755
- https://github.com/apache/airflow/issues/43752

pierrejeambrun (Issue Creator) on (2024-11-06 14:50:58 UTC): Hey omkar, as you can see in I'm already assigned to the `TaskInstances` endpoint.

Thanks for creating the issue, we can split the work on that matter, i'll assign some of them to you maybe all the `PUT/POST`  as I am already working on GET.

pierrejeambrun (Issue Creator) on (2024-11-06 14:55:57 UTC): You're assigned, issue updated, let me know if that shounds good for you :).

omkar-foss on (2024-11-08 04:30:41 UTC): Yes sure, that works. Thank you :)

bbovenzi (Assginee) on (2024-11-20 22:07:17 UTC): @prabhusneha I see you're assigned to Logs. What is the status of getting the logs for a task instance? That would be a useful one to get in the UI soon!

utkarsharma2 on (2024-11-21 06:56:35 UTC): Hey, @bbovenzi I'm taking care of `task instance logs` endpoint, I'm mostly done with it, but there are a couple of test cases that require setting users in fastapi which requires a little more work. I'll keep you posted.

phanikumv on (2024-11-21 06:57:21 UTC): Thanks @utkarsharma2  (updated the description accordingly).

pierrejeambrun (Issue Creator) on (2024-11-28 16:43:29 UTC): Thanks a lot to everyone that contributed, closing a big one 🎉

"
2538740550,issue,closed,completed,AIP-84 Begin scoping out DAGs list in React,"Segment the different endpoints needed for the UI regarding dag listing endpoint (`/stats`, `/last_dagruns`, `/dag_stats`)

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-09-20 12:49:25+00:00,[],2024-10-11 09:50:54+00:00,2024-10-11 09:50:53+00:00,https://github.com/apache/airflow/issues/42369,"[('kind:feature', 'Feature Requests'), ('kind:meta', 'High-level information important to the community'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2407051998, 'issue_id': 2538740550, 'author': 'bbovenzi', 'body': 'Closing in favor of #42933', 'created_at': datetime.datetime(2024, 10, 11, 9, 50, 53, tzinfo=datetime.timezone.utc)}]","bbovenzi on (2024-10-11 09:50:53 UTC): Closing in favor of #42933

"
2538733264,issue,open,,AIP-84 Refactor dataset dependencies graph,"Migrate the asset dependency private endpoint and also add additional features. This is a backend only change.

Showing all separate dataset dependencies graph is not useful. We only care about showing the full graph connected to a selected dag or dataset. Furthermore, connections dataset -> dag should render the dag's trigger conditions just like we did in for the dag graph.

Update:
Waiting on the front-end assets design to be finalized to know what features are exactly needed by the asset dependency graph private endpoint.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-09-20 12:45:35+00:00,['vatsrahul1001'],2024-12-13 10:44:09+00:00,,https://github.com/apache/airflow/issues/42368,"[('kind:meta', 'High-level information important to the community'), ('area:datasets', 'Issues related to the datasets feature'), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2457518958, 'issue_id': 2538733264, 'author': 'bbovenzi', 'body': 'We need to update `get_dag_dependencies()` on the serialized_dag model. Because right now it is a single dict of many disconnected graphs. Instead, it should be a list of graphs. Then we can pass a `node_id`, representing a dag or dataset, to return which graph contains that node.', 'created_at': datetime.datetime(2024, 11, 5, 15, 44, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2514960759, 'issue_id': 2538733264, 'author': 'bbovenzi', 'body': 'We can wait on this until we have finalized data assets UI designs.', 'created_at': datetime.datetime(2024, 12, 3, 15, 59, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2536182004, 'issue_id': 2538733264, 'author': 'pierrejeambrun', 'body': 'Puting this on hold. Let me know when I should work on it.', 'created_at': datetime.datetime(2024, 12, 11, 14, 38, 43, tzinfo=datetime.timezone.utc)}]","bbovenzi on (2024-11-05 15:44:55 UTC): We need to update `get_dag_dependencies()` on the serialized_dag model. Because right now it is a single dict of many disconnected graphs. Instead, it should be a list of graphs. Then we can pass a `node_id`, representing a dag or dataset, to return which graph contains that node.

bbovenzi on (2024-12-03 15:59:26 UTC): We can wait on this until we have finalized data assets UI designs.

pierrejeambrun (Issue Creator) on (2024-12-11 14:38:43 UTC): Puting this on hold. Let me know when I should work on it.

"
2538718026,issue,closed,completed,AIP-84 Add external dependencies to a dag's graph_data,"from @bbovenzi:
> In 2.10, we cobbled together a UI to show dataset conditions and upstream and downstream dataset dependencies. We should update this to include a boolean param show_external_dependencies to show connected datasets, dataset conditions, triggers and sensors.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-09-20 12:37:38+00:00,['pierrejeambrun'],2024-12-16 17:19:35+00:00,2024-12-16 17:19:35+00:00,https://github.com/apache/airflow/issues/42367,"[('kind:feature', 'Feature Requests'), ('kind:meta', 'High-level information important to the community'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('area:datasets', 'Issues related to the datasets feature'), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2387841938, 'issue_id': 2538718026, 'author': 'bbovenzi', 'body': ""We can rename graph_data to dag `structure`, with edges and nodes. The nested list of nodes should have the topological sort, which we may move to happen during DAG serialization. With a sorted list of nodes, specifying which are tasks and which aren't, we can reuse the endpoint to help render the grid and gantt views."", 'created_at': datetime.datetime(2024, 10, 2, 7, 51, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2457743004, 'issue_id': 2538718026, 'author': 'bbovenzi', 'body': 'Actually, the sort should be based off of the `webserver.grid_view_sorting_order` config.', 'created_at': datetime.datetime(2024, 11, 5, 17, 13, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2486746473, 'issue_id': 2538718026, 'author': 'bbovenzi', 'body': 'Updated types I am using on the frontend mockups:\n\n```\nexport type Edge = {\n  is_setup_teardown?: boolean;\n  label?: string;\n  source_id: string;\n  target_id: string;\n};\n\nexport type Node = {\n  children?: Array<Node>;\n  id: string;\n  is_mapped?: boolean;\n  label: string;\n  tooltip?: string;\n  setup_teardown_type?: ""setup"" | ""teardown"";\n  type:\n    | ""asset_alias""\n    | ""asset_condition""\n    | ""asset""\n    | ""dag""\n    | ""join""\n    | ""sensor""\n    | ""task""\n    | ""trigger"";\n};\n\nexport type GraphData = {\n  arrange: ""BT"" | ""LR"" | ""RL"" | ""TB"";\n  edges: Array<Edge>;\n  nodes: Array<Node>;\n};\n```', 'created_at': datetime.datetime(2024, 11, 19, 21, 2, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2506385112, 'issue_id': 2538718026, 'author': 'pierrejeambrun', 'body': 'Having fields optional in the payload, i.e `?:` makes it a little bit more complicated to handle and I think it might be easier to always return all fields, defaulting to `None` in case they are not filled, and the front knows what to read based on the `type`  value. (ideally we would need multiple implem of the node interface depending on each type allowed, but that might be overkill at first)', 'created_at': datetime.datetime(2024, 11, 28, 15, 37, 30, tzinfo=datetime.timezone.utc)}]","bbovenzi on (2024-10-02 07:51:54 UTC): We can rename graph_data to dag `structure`, with edges and nodes. The nested list of nodes should have the topological sort, which we may move to happen during DAG serialization. With a sorted list of nodes, specifying which are tasks and which aren't, we can reuse the endpoint to help render the grid and gantt views.

bbovenzi on (2024-11-05 17:13:09 UTC): Actually, the sort should be based off of the `webserver.grid_view_sorting_order` config.

bbovenzi on (2024-11-19 21:02:38 UTC): Updated types I am using on the frontend mockups:

```
export type Edge = {
  is_setup_teardown?: boolean;
  label?: string;
  source_id: string;
  target_id: string;
};

export type Node = {
  children?: Array<Node>;
  id: string;
  is_mapped?: boolean;
  label: string;
  tooltip?: string;
  setup_teardown_type?: ""setup"" | ""teardown"";
  type:
    | ""asset_alias""
    | ""asset_condition""
    | ""asset""
    | ""dag""
    | ""join""
    | ""sensor""
    | ""task""
    | ""trigger"";
};

export type GraphData = {
  arrange: ""BT"" | ""LR"" | ""RL"" | ""TB"";
  edges: Array<Edge>;
  nodes: Array<Node>;
};
```

pierrejeambrun (Issue Creator) on (2024-11-28 15:37:30 UTC): Having fields optional in the payload, i.e `?:` makes it a little bit more complicated to handle and I think it might be easier to always return all fields, defaulting to `None` in case they are not filled, and the front knows what to read based on the `type`  value. (ideally we would need multiple implem of the node interface depending on each type allowed, but that might be overkill at first)

"
2538716062,issue,open,,AIP-84 Migrate UI endpoints from views.py to the new rest api,"### Body

Migrate all of the custom/private endpoints that the legacy react views use into the new REST API. Some of them may be best to be moved to the public REST API. Including but not limited to:


- `object/grid_data` https://github.com/apache/airflow/issues/42595
- `object/graph_data` https://github.com/apache/airflow/issues/42367
- `object/rendered-k8s` this should probably be moved to the public API
- trigger dag run
- `/object/next_run_datasets`
- `/object/dataset_dependencies` https://github.com/apache/airflow/issues/42368
- ~~`/object/datasets_summary`~~ we will redesign this one
- ~~`/object/calendar_data`~~ We will redesign this one
- ~~`/object/task_instances`~~ we should just delete this one
- `/object/historical_metrics_data` https://github.com/apache/airflow/issues/42623
- `/last_dagruns` https://github.com/apache/airflow/issues/42933
- ~~`/task_stats` part of replacing the dags list https://github.com/apache/airflow/issues/42369~~
- ~~`/last_dagruns` part of replacing the dags list https://github.com/apache/airflow/issues/42369~~
- ~~`/dag_stats` part of replacing the dags list https://github.com/apache/airflow/issues/42369~~

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-09-20 12:36:34+00:00,['pierrejeambrun'],2024-10-11 09:54:20+00:00,,https://github.com/apache/airflow/issues/42366,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('kind:meta', 'High-level information important to the community'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2380876138, 'issue_id': 2538716062, 'author': 'bugraoz93', 'body': ""Thanks for the umbrella ticket! I want to work on this. I couldn't decide which one to migrate to first. Do you have any priority on these @pierrejeambrun?"", 'created_at': datetime.datetime(2024, 9, 28, 19, 40, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382665099, 'issue_id': 2538716062, 'author': 'pierrejeambrun', 'body': 'I am not sure, maybe @bbovenzi will have more information about that. I would say that `object/grid_data` is of some importance but is also a complex one. Otherwise you can just pick the one that sound fun to you :).', 'created_at': datetime.datetime(2024, 9, 30, 9, 56, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2383716527, 'issue_id': 2538716062, 'author': 'bugraoz93', 'body': 'Thanks for clarifying @pierrejeambrun! All seems fun, I am going with object/grid_data then :)', 'created_at': datetime.datetime(2024, 9, 30, 16, 58, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2385068530, 'issue_id': 2538716062, 'author': 'bbovenzi', 'body': ""@bugraoz93  Actually, we may refactor how `grid_data` is built. So let's hold off on that one for now. I would work on `/object/historical_metrics_data`. I'd like to get the dashboard running in the UI soon."", 'created_at': datetime.datetime(2024, 10, 1, 8, 3, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2386341474, 'issue_id': 2538716062, 'author': 'bugraoz93', 'body': 'Thanks @bbovenzi! I have created the issue. I will first take a look at `/object/historical_metrics_data`. :)', 'created_at': datetime.datetime(2024, 10, 1, 15, 32, 54, tzinfo=datetime.timezone.utc)}]","bugraoz93 on (2024-09-28 19:40:44 UTC): Thanks for the umbrella ticket! I want to work on this. I couldn't decide which one to migrate to first. Do you have any priority on these @pierrejeambrun?

pierrejeambrun (Issue Creator) on (2024-09-30 09:56:59 UTC): I am not sure, maybe @bbovenzi will have more information about that. I would say that `object/grid_data` is of some importance but is also a complex one. Otherwise you can just pick the one that sound fun to you :).

bugraoz93 on (2024-09-30 16:58:28 UTC): Thanks for clarifying @pierrejeambrun! All seems fun, I am going with object/grid_data then :)

bbovenzi on (2024-10-01 08:03:53 UTC): @bugraoz93  Actually, we may refactor how `grid_data` is built. So let's hold off on that one for now. I would work on `/object/historical_metrics_data`. I'd like to get the dashboard running in the UI soon.

bugraoz93 on (2024-10-01 15:32:54 UTC): Thanks @bbovenzi! I have created the issue. I will first take a look at `/object/historical_metrics_data`. :)

"
2538695060,issue,closed,completed,AIP-84 Determine architecture for UI REST API,"Spike to investigate what stack we should use.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-09-20 12:28:38+00:00,['pierrejeambrun'],2024-09-20 13:29:38+00:00,2024-09-20 12:29:34+00:00,https://github.com/apache/airflow/issues/42365,"[('kind:meta', 'High-level information important to the community'), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2363619314, 'issue_id': 2538695060, 'author': 'pierrejeambrun', 'body': 'Done, driven by community opinion during dev calls => going for FastAPI framework, pydantic serializers, and gunicorn + uvicorn workers for the gate interface part.', 'created_at': datetime.datetime(2024, 9, 20, 12, 29, 34, tzinfo=datetime.timezone.utc)}]","pierrejeambrun (Issue Creator) on (2024-09-20 12:29:34 UTC): Done, driven by community opinion during dev calls => going for FastAPI framework, pydantic serializers, and gunicorn + uvicorn workers for the gate interface part.

"
2538684249,issue,closed,completed,AIP-84 Bootstrap the new Rest API,"### Body

After we decide on our tech stack. Let's get the UI REST API started.

The REST API should also be plugged into the new UI full react app.

MVP PR needs:
- [x] research tech stack, (probably fastapi, what else?) => going with fast api
- [x] Make pydantic v2 and fastapi as core dependencies for airflow 3, remove all v1 / v2 / none code (utils, test, CI). https://github.com/apache/airflow/pull/41857
- [x] migrate one custom endpoint from `views.py` https://github.com/apache/airflow/pull/41798
  - Let's start with `next_run_datasets`, its relatively simple and we will still need it
  - We will keep the old UI connected to the old `views.py` endpoint for now, and delete both later. Then we're not doing extra work just for backward compatibility
- [x] Integrate the new webserver to the airflow CLI (maybe with a warning to discourage usage for now, also in airflow standalone) https://github.com/apache/airflow/pull/41896
- [x] Integrate the new webserver to the Breeze workflow https://github.com/apache/airflow/pull/41896
- [x] Fix Wrong file descriptor with Uvicorn async workers for Gunicorn. Check workers refresh. https://github.com/apache/airflow/pull/41896
- [ ] ~~Add TLS to the API~~ (moved to separate issue, cf note bellow)
- [ ] ~~Handle permissions and access control, POC here https://github.com/apache/airflow/pull/42019~~ (moved to separate issue, cf note bellow)
- [x] Tests + CI integrations https://github.com/apache/airflow/pull/41957
- [x] develop dev tools for the new API and for the UI to consume the new API (testing, autogeneration) https://github.com/apache/airflow/pull/42098 https://github.com/apache/airflow/pull/42222
- [x] contributor documentation https://github.com/apache/airflow/pull/41903 (brent did it)


### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-09-20 12:25:30+00:00,['pierrejeambrun'],2024-09-20 12:26:02+00:00,2024-09-20 12:26:01+00:00,https://github.com/apache/airflow/issues/42364,"[('kind:meta', 'High-level information important to the community'), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2363609445, 'issue_id': 2538684249, 'author': 'pierrejeambrun', 'body': 'Done.\r\n\r\nHTTPS and Permissions are moved to separate issues. Cf project board.', 'created_at': datetime.datetime(2024, 9, 20, 12, 26, 1, tzinfo=datetime.timezone.utc)}]","pierrejeambrun (Issue Creator) on (2024-09-20 12:26:01 UTC): Done.

HTTPS and Permissions are moved to separate issues. Cf project board.

"
2538631194,issue,closed,completed,Cannot use templated to/from datetime fields in S3DeleteObjectsOperator,"### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.28.0

### Apache Airflow version

2.9.1

### Operating System

Debian GNU/Linux 11 (bullseye)

### Deployment

Other 3rd-party Helm chart

### Deployment details

_No response_

### What happened

`S3DeleteObjectsOperator` fails when `to_datetime` or `from_datetime` are defined as airflow macros.

```
File ""/opt/venv/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File ""/opt/venv/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File ""/opt/venv/lib/python3.10/site-packages/airflow/models/baseoperator.py"", line 400, in wrapper
    return func(self, *args, **kwargs)
  File ""/opt/venv/lib/python3.10/site-packages/airflow/providers/amazon/aws/operators/s3.py"", line 535, in execute
    keys = self.keys or s3_hook.list_keys(
  File ""/opt/venv/lib/python3.10/site-packages/airflow/providers/amazon/aws/hooks/s3.py"", line 132, in wrapper
    return func(*bound_args.args, **bound_args.kwargs)
  File ""/opt/venv/lib/python3.10/site-packages/airflow/providers/amazon/aws/hooks/s3.py"", line 869, in list_keys
    return self._list_key_object_filter(keys, from_datetime, to_datetime)
  File ""/opt/venv/lib/python3.10/site-packages/airflow/providers/amazon/aws/hooks/s3.py"", line 670, in _list_key_object_filter
    return [k[""Key""] for k in keys if _is_in_period(k[""LastModified""])]
  File ""/opt/venv/lib/python3.10/site-packages/airflow/providers/amazon/aws/hooks/s3.py"", line 670, in <listcomp>
    return [k[""Key""] for k in keys if _is_in_period(k[""LastModified""])]
  File ""/opt/venv/lib/python3.10/site-packages/airflow/providers/amazon/aws/hooks/s3.py"", line 666, in _is_in_period
    if to_datetime is not None and input_date > to_datetime:
TypeError: '>' not supported between instances of 'datetime.datetime' and 'str'
```

### What you think should happen instead

`to_datetime` and `from_datetime` fields can be templated and are converted from string to datetime in appropriate place.

### How to reproduce

Create dag with following task:

```
to_datetime = ""{{ macros.ds_add(ds, -30) }}""

task = S3DeleteObjectsOperator(
    task_id='delete_old_logs',
    bucket='mybucket',
    prefix='logs/',
    to_datetime=to_datetime,
    aws_conn_id=aws_conn_id
)
```

### Anything else

It would work if i pass to_datetime (or from_datetime) as `datetime.now() + timedelta(days=-30)`. It is then confusing why these fields are accepted as `template_fields`.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",mgorsk1,2024-09-20 11:59:01+00:00,['ellisms'],2024-09-28 04:40:33+00:00,2024-09-26 14:37:10+00:00,https://github.com/apache/airflow/issues/42363,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('good first issue', '')]","[{'comment_id': 2371367753, 'issue_id': 2538631194, 'author': 'ellisms', 'body': ""I'll take a look."", 'created_at': datetime.datetime(2024, 9, 24, 13, 58, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2373849478, 'issue_id': 2538631194, 'author': 'ellisms', 'body': '`{{ macros.ds_add() }}` returns a string, rather than datetime, which causes the failure. I added datetime string parsing to the operator to catch this.', 'created_at': datetime.datetime(2024, 9, 25, 11, 43, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2380404034, 'issue_id': 2538631194, 'author': 'mgorsk1', 'body': 'Thanks @ellisms 🙏🏻', 'created_at': datetime.datetime(2024, 9, 28, 4, 40, 33, tzinfo=datetime.timezone.utc)}]","ellisms (Assginee) on (2024-09-24 13:58:54 UTC): I'll take a look.

ellisms (Assginee) on (2024-09-25 11:43:35 UTC): `{{ macros.ds_add() }}` returns a string, rather than datetime, which causes the failure. I added datetime string parsing to the operator to catch this.

mgorsk1 (Issue Creator) on (2024-09-28 04:40:33 UTC): Thanks @ellisms 🙏🏻

"
2538270119,issue,open,,AIP-84 Authentications and Permissions,"Depends on https://github.com/apache/airflow/issues/44884: (As long as the front-end is not providing the JWT this cannot be implemented or the front-end will receive 401)

Multiple PRs to add back permissions to all endpoints (the ones from the legacy API that were removed during the migration)

> Default to SimpleAuthManager for development time of airflow 3 as long as FABAuthManager does not support JWT auth, this is the only way we can progress on this while https://github.com/apache/airflow/issues/44849 is not completed

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-09-20 08:59:07+00:00,['pierrejeambrun'],2025-02-06 09:13:02+00:00,,https://github.com/apache/airflow/issues/42360,"[('area:auth', ''), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2428225924, 'issue_id': 2538270119, 'author': 'jason810496', 'body': 'Hi @pierrejeambrun, could you please assign this ticket to me? Thanks!', 'created_at': datetime.datetime(2024, 10, 22, 4, 36, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428552187, 'issue_id': 2538270119, 'author': 'pierrejeambrun', 'body': 'Hello @jason810496,\r\n\r\nThere has been some update on this one, it is not well scoped and depends on the work that Vincent is doing on the FABAuthManager and JWT backend. I do not recommend to take this one at this point. Removing the `good first issue tag`', 'created_at': datetime.datetime(2024, 10, 22, 8, 5, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428573692, 'issue_id': 2538270119, 'author': 'jason810496', 'body': 'Gotcha, I will take other tickets. Thanks !', 'created_at': datetime.datetime(2024, 10, 22, 8, 14, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2443565208, 'issue_id': 2538270119, 'author': 'pierrejeambrun', 'body': 'Related PR https://github.com/apache/airflow/pull/42634', 'created_at': datetime.datetime(2024, 10, 29, 8, 33, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2539516771, 'issue_id': 2538270119, 'author': 'pierrejeambrun', 'body': 'cc: @vincbeck', 'created_at': datetime.datetime(2024, 12, 12, 17, 1, 19, tzinfo=datetime.timezone.utc)}]","jason810496 on (2024-10-22 04:36:35 UTC): Hi @pierrejeambrun, could you please assign this ticket to me? Thanks!

pierrejeambrun (Issue Creator) on (2024-10-22 08:05:37 UTC): Hello @jason810496,

There has been some update on this one, it is not well scoped and depends on the work that Vincent is doing on the FABAuthManager and JWT backend. I do not recommend to take this one at this point. Removing the `good first issue tag`

jason810496 on (2024-10-22 08:14:41 UTC): Gotcha, I will take other tickets. Thanks !

pierrejeambrun (Issue Creator) on (2024-10-29 08:33:15 UTC): Related PR https://github.com/apache/airflow/pull/42634

pierrejeambrun (Issue Creator) on (2024-12-12 17:01:19 UTC): cc: @vincbeck

"
2538261480,issue,open,,AIP-84 Add TLS support to the server to enable HTTPS connections,"Support https connection based on user provided certificates for the new REST API server.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-09-20 08:54:50+00:00,['JoshuaXOng'],2024-09-20 16:15:36+00:00,,https://github.com/apache/airflow/issues/42359,"[('area:API', ""Airflow's REST/HTTP API""), ('good first issue', ''), ('kind:meta', 'High-level information important to the community'), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2363897020, 'issue_id': 2538261480, 'author': 'JoshuaXOng', 'body': ""Hello, I can try this one out / get assigned to it if that's all good."", 'created_at': datetime.datetime(2024, 9, 20, 14, 41, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2364076301, 'issue_id': 2538261480, 'author': 'pierrejeambrun', 'body': 'Of course @JoshuaXOng, assigned :)', 'created_at': datetime.datetime(2024, 9, 20, 16, 15, 23, tzinfo=datetime.timezone.utc)}]","JoshuaXOng (Assginee) on (2024-09-20 14:41:12 UTC): Hello, I can try this one out / get assigned to it if that's all good.

pierrejeambrun (Issue Creator) on (2024-09-20 16:15:23 UTC): Of course @JoshuaXOng, assigned :)

"
2538211915,issue,closed,completed,"WebUI does not show ""Task Tries"" items after rerun on MappedOperators","### Apache Airflow version

2.10.1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

We are running a Airflow 2.10.1 instance an facing the issue that the ""Task Tries"" fields are missing on Mapped Operator tasks:
![image](https://github.com/user-attachments/assets/474858c7-5107-47d0-af7d-80bfd62d4a19)

On Normal Operators it is working fine:
![image](https://github.com/user-attachments/assets/edfcfb10-2e58-462f-9626-6bdb70f70e68)


Any one else has the problem? I´m not sure it was working in 2.10. or not but in 2.9.3 it was working.

### What you think should happen instead?

_No response_

### How to reproduce

Clean task state of Mapped Operator task and check the ""Task Tries"" items of the WebUI.

### Operating System

Debian GNU/Linux 12

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",AutomationDev85,2024-09-20 08:29:34+00:00,['jx2lee'],2024-11-01 00:17:17+00:00,2024-11-01 00:17:17+00:00,https://github.com/apache/airflow/issues/42357,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('area:dynamic-task-mapping', 'AIP-42')]","[{'comment_id': 2364664625, 'issue_id': 2538211915, 'author': 'jscheffl', 'body': 'I did a regressionin breeze with DAG `example_dynamic_task_mapping_with_no_taskflow_operators` and can confirm:\r\n\r\nAirflow 2.9.3:\r\n\r\n![image](https://github.com/user-attachments/assets/826acb78-0147-40c6-b74e-6a509d5a84dc)\r\n\r\nAirflow 2.10.0:\r\n\r\n![image](https://github.com/user-attachments/assets/260f029b-80d4-415b-89bd-ff2da06dece4)\r\n\r\nAirflow 2.10.1:\r\n\r\n![image](https://github.com/user-attachments/assets/15f843f6-cd00-4371-8900-2191ffde5796)\r\n\r\n...it seems the header ""Task Tries"" is added but the buttons to switch are missing. For the task details this is degrading to state of 2.9.3 but for logs this blocks UI access to logs for runs which are not the last if multiple tries :-(\r\n\r\n...and just now the release of 2.10.2 :-( Can confirm that the bug is also in 2.10.2', 'created_at': datetime.datetime(2024, 9, 20, 21, 50, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2364674003, 'issue_id': 2538211915, 'author': 'jscheffl', 'body': 'With some triaging through the release notes... PR #41483 seems to be the fix that caused the side effect. Manually reverting the commit 5dcc2ada1bd753b7a4b183062d3f5954af076a97 fixes _this_ problem but re-adds the refresh issue we had in 2.10.0. ...and damn it was myself testing and did not check the mapped tasks.\r\n\r\n@AutomationDev85 are you willing to supply a patch for the views of mapped tasks for a future 2.10.3?', 'created_at': datetime.datetime(2024, 9, 20, 22, 0, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2381056978, 'issue_id': 2538211915, 'author': 'jx2lee', 'body': 'Could i take this issue? if possible, please assign to me and try it!', 'created_at': datetime.datetime(2024, 9, 29, 1, 45, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448402349, 'issue_id': 2538211915, 'author': 'jscheffl', 'body': 'Ah, just checked for this and was hoping it is in 2.10.3... but still not fixed :-(', 'created_at': datetime.datetime(2024, 10, 30, 21, 19, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2450324770, 'issue_id': 2538211915, 'author': 'potiuk', 'body': ""> Ah, just checked for this and was hoping it is in 2.10.3... but still not fixed :-(\r\n\r\nYou know what's the fastest way @jscheffl :stuck_out_tongue:  ?"", 'created_at': datetime.datetime(2024, 10, 31, 16, 35, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2450717845, 'issue_id': 2538211915, 'author': 'jscheffl', 'body': '> > Ah, just checked for this and was hoping it is in 2.10.3... but still not fixed :-(\r\n> \r\n> You know what\'s the fastest way @jscheffl 😛 ?\r\n\r\nYeah... the fastest way is always... fix it yourself! Probably gonna do this. I was losing track and actually we have an ""everyday problem"" as the UI history is broken... was just not following-up, lese I would have fixed already... but now too late for 2.10.3 :-( (Except if other major bugs will be found...)', 'created_at': datetime.datetime(2024, 10, 31, 20, 3, 39, tzinfo=datetime.timezone.utc)}]","jscheffl on (2024-09-20 21:50:39 UTC): I did a regressionin breeze with DAG `example_dynamic_task_mapping_with_no_taskflow_operators` and can confirm:

Airflow 2.9.3:

![image](https://github.com/user-attachments/assets/826acb78-0147-40c6-b74e-6a509d5a84dc)

Airflow 2.10.0:

![image](https://github.com/user-attachments/assets/260f029b-80d4-415b-89bd-ff2da06dece4)

Airflow 2.10.1:

![image](https://github.com/user-attachments/assets/15f843f6-cd00-4371-8900-2191ffde5796)

...it seems the header ""Task Tries"" is added but the buttons to switch are missing. For the task details this is degrading to state of 2.9.3 but for logs this blocks UI access to logs for runs which are not the last if multiple tries :-(

...and just now the release of 2.10.2 :-( Can confirm that the bug is also in 2.10.2

jscheffl on (2024-09-20 22:00:51 UTC): With some triaging through the release notes... PR #41483 seems to be the fix that caused the side effect. Manually reverting the commit 5dcc2ada1bd753b7a4b183062d3f5954af076a97 fixes _this_ problem but re-adds the refresh issue we had in 2.10.0. ...and damn it was myself testing and did not check the mapped tasks.

@AutomationDev85 are you willing to supply a patch for the views of mapped tasks for a future 2.10.3?

jx2lee (Assginee) on (2024-09-29 01:45:43 UTC): Could i take this issue? if possible, please assign to me and try it!

jscheffl on (2024-10-30 21:19:17 UTC): Ah, just checked for this and was hoping it is in 2.10.3... but still not fixed :-(

potiuk on (2024-10-31 16:35:45 UTC): You know what's the fastest way @jscheffl :stuck_out_tongue:  ?

jscheffl on (2024-10-31 20:03:39 UTC): Yeah... the fastest way is always... fix it yourself! Probably gonna do this. I was losing track and actually we have an ""everyday problem"" as the UI history is broken... was just not following-up, lese I would have fixed already... but now too late for 2.10.3 :-( (Except if other major bugs will be found...)

"
2535637680,issue,closed,completed,setting parameters in SQLExecuteQueryOperator fails (should be the same as using params),"### Apache Airflow Provider(s)

standard

### Versions of Apache Airflow Providers

apache-airflow-providers-postgres==5.12.0

### Apache Airflow version

2.10.1

### Operating System

RHEL 9.4

### Deployment

Virtualenv installation

### Deployment details

Just default airflow with apache-airflow-providers-postgres

### What happened

When using ""parameters"" to provide params to SQLExecuteQueryOperator, they are not available in the template. An exception is thrown: ""jinja2.exceptions.UndefinedError: 'dict object' has no attribute '....'""

Only when given as ""params"" it works.

### What you think should happen instead

Both ""parameters"" and ""params"" should be usable, but currently only ""params"" works.

From the docu:<br>
SQLExecuteQueryOperator provides parameters attribute which makes it possible to dynamically inject values into your SQL requests during runtime. The BaseOperator class has the params attribute which is available to the SQLExecuteQueryOperator by virtue of inheritance. **Both parameters and params make it possible** to dynamically pass in parameters in many interesting ways.

### How to reproduce

Define a task using SQLExecuteQueryOperator and provide params using ""parameters"" like here:
```py
    sql_task = SQLExecuteQueryOperator(
        task_id=""task_id"",
        conn_id=""conn_id"",
        sql=""file.sql"",
        parameters={'param_01':param_01_value},
    )
```
And the file.sql may look like that:
```sql
select * from table where column = '{{ params.param_01 }}'
```

Once running the DAG an error is thrown. See stacktrace below:
```
Traceback (most recent call last):
  File ""/home/user/apps/conf/venv_airflow/lib64/python3.9/site-packages/airflow/models/abstractoperator.py"", line 770, in _do_render_template_fields
    rendered_content = self.render_template(
  File ""/home/user/apps/conf/venv_airflow/lib64/python3.9/site-packages/airflow/template/templater.py"", line 171, in render_template
    return self._render(template, context)
  File ""/home/user/apps/conf/venv_airflow/lib64/python3.9/site-packages/airflow/models/abstractoperator.py"", line 725, in _render
    return super()._render(template, context, dag=dag)
  File ""/home/user/apps/conf/venv_airflow/lib64/python3.9/site-packages/airflow/template/templater.py"", line 127, in _render
    return render_template_to_string(template, context)
  File ""/home/user/apps/conf/venv_airflow/lib64/python3.9/site-packages/airflow/utils/helpers.py"", line 301, in render_template_to_string
    return render_template(template, cast(MutableMapping[str, Any], context), native=False)
  File ""/home/user/apps/conf/venv_airflow/lib64/python3.9/site-packages/airflow/utils/helpers.py"", line 296, in render_template
    return """".join(nodes)
  File ""<template>"", line 13, in root
  File ""/home/user/apps/conf/venv_airflow/lib64/python3.9/site-packages/jinja2/runtime.py"", line 857, in _fail_with_undefined_error
    raise self._undefined_exception(self._undefined_message)
jinja2.exceptions.UndefinedError: 'dict object' has no attribute 'param_01'
```


### Anything else

The problem occurs every time

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",schweizersta,2024-09-19 08:22:29+00:00,[],2024-09-29 16:25:22+00:00,2024-09-29 16:25:21+00:00,https://github.com/apache/airflow/issues/42344,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('pending-response', ''), ('area:core-operators', 'Operators, Sensors and hooks within Core Airflow'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:postgres', '')]","[{'comment_id': 2360315748, 'issue_id': 2535637680, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 19, 8, 22, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2361890779, 'issue_id': 2535637680, 'author': 'jscheffl', 'body': 'The `{{ params.*** }}` you are referring to are the DAG run parameters which can be defined on DAG level as described in https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/params.html#referencing-params-in-a-task\r\n\r\nThe parameters you are trying to add from the operators need to be passed in form `%(parameter_name)s` into the query string - as documented in https://airflow.apache.org/docs/apache-airflow-providers-postgres/stable/operators/postgres_operator_howto_guide.html#passing-server-configuration-parameters-into-postgresoperator\r\n\r\nYour SQL file need to look like:\r\n```\r\nselect * from table where %(param_01)s\r\n```', 'created_at': datetime.datetime(2024, 9, 19, 18, 28, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2363232661, 'issue_id': 2535637680, 'author': 'schweizersta', 'body': 'I am not talking about DAG run parameters, but task parameters (given when instantiating SQLExecuteQueryOperator).\r\nWith some more investigations I figured out how it works and what I am trying to say:\r\n\r\nIn the previous section ""Passing Parameters into SQLExecuteQueryOperator for Postgres"" of the second docu link you referred to, there are 2 examples:\r\n\r\n1.\r\n  ```\r\n  ...\r\n    sql=""SELECT * FROM pet WHERE birth_date BETWEEN SYMMETRIC %(begin_date)s AND %(end_date)s"",\r\n    parameters={""begin_date"": ""2020-01-01"", ""end_date"": ""2020-12-31""},\r\n  ...\r\n  ```\r\n\r\n2.\r\n  ```\r\n  ...\r\n    sql=""sql/birth_date.sql"",\r\n    params={""begin_date"": ""2020-01-01"", ""end_date"": ""2020-12-31""},\r\n  ...\r\n  ```\r\n\r\nIn the second example (where the sql command is stored in a file), one has to provide values for the sql parameters using `params=`. But it does not matter how you access the paramters in the sql command, whether by using `%(begin_date)` or `{{ params.begin_date }}`, both of them work fine.\r\n\r\nWhat got me confused, is the statement at the beginning of this section, where it says: ""Both parameters and params make it possible to dynamically pass in parameters in many interesting ways."". So I thougt, I can use either params or parameters in both cases. But that ""parameter"" is only valid for the first case and ""params"" only for the second one, was not obvious to me.\r\n\r\nNow that I know how it\'s meant to be, I leave it up to you, if you want to clarify this more in the docu or not.', 'created_at': datetime.datetime(2024, 9, 20, 9, 5, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2364553737, 'issue_id': 2535637680, 'author': 'jscheffl', 'body': 'Thanks for the feedback!\r\nAs the community is always happy to receive not only feedback but also direct corrections... would it be possible to open a small PR and propose a change? If you just do this, then I can review and merge it.\r\nJust use the button on the bottom of each page:\r\n\r\n![image](https://github.com/user-attachments/assets/f448544e-1efe-4837-a1b2-157c9daf78fc)\r\n\r\nIt is really simple and once you have made your first PR you are joining the crowd and be proud! Might take <5min.', 'created_at': datetime.datetime(2024, 9, 20, 20, 15, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2381120026, 'issue_id': 2535637680, 'author': 'kunaljubce', 'body': '@jscheffl I have raised the PR to make this documentation change. Please review.', 'created_at': datetime.datetime(2024, 9, 29, 5, 58, 32, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-19 08:22:38 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jscheffl on (2024-09-19 18:28:43 UTC): The `{{ params.*** }}` you are referring to are the DAG run parameters which can be defined on DAG level as described in https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/params.html#referencing-params-in-a-task

The parameters you are trying to add from the operators need to be passed in form `%(parameter_name)s` into the query string - as documented in https://airflow.apache.org/docs/apache-airflow-providers-postgres/stable/operators/postgres_operator_howto_guide.html#passing-server-configuration-parameters-into-postgresoperator

Your SQL file need to look like:
```
select * from table where %(param_01)s
```

schweizersta (Issue Creator) on (2024-09-20 09:05:23 UTC): I am not talking about DAG run parameters, but task parameters (given when instantiating SQLExecuteQueryOperator).
With some more investigations I figured out how it works and what I am trying to say:

In the previous section ""Passing Parameters into SQLExecuteQueryOperator for Postgres"" of the second docu link you referred to, there are 2 examples:

1.
  ```
  ...
    sql=""SELECT * FROM pet WHERE birth_date BETWEEN SYMMETRIC %(begin_date)s AND %(end_date)s"",
    parameters={""begin_date"": ""2020-01-01"", ""end_date"": ""2020-12-31""},
  ...
  ```

2.
  ```
  ...
    sql=""sql/birth_date.sql"",
    params={""begin_date"": ""2020-01-01"", ""end_date"": ""2020-12-31""},
  ...
  ```

In the second example (where the sql command is stored in a file), one has to provide values for the sql parameters using `params=`. But it does not matter how you access the paramters in the sql command, whether by using `%(begin_date)` or `{{ params.begin_date }}`, both of them work fine.

What got me confused, is the statement at the beginning of this section, where it says: ""Both parameters and params make it possible to dynamically pass in parameters in many interesting ways."". So I thougt, I can use either params or parameters in both cases. But that ""parameter"" is only valid for the first case and ""params"" only for the second one, was not obvious to me.

Now that I know how it's meant to be, I leave it up to you, if you want to clarify this more in the docu or not.

jscheffl on (2024-09-20 20:15:30 UTC): Thanks for the feedback!
As the community is always happy to receive not only feedback but also direct corrections... would it be possible to open a small PR and propose a change? If you just do this, then I can review and merge it.
Just use the button on the bottom of each page:

![image](https://github.com/user-attachments/assets/f448544e-1efe-4837-a1b2-157c9daf78fc)

It is really simple and once you have made your first PR you are joining the crowd and be proud! Might take <5min.

kunaljubce on (2024-09-29 05:58:32 UTC): @jscheffl I have raised the PR to make this documentation change. Please review.

"
2535013290,issue,closed,completed,Remove all references on “execution date” as a name,"### Body

This includes documentation, web UI labels, CLI strings, etc. In most cases they should just be renamed to _logical date_. We can investigate replacing them with another value (`run_id`?) in a case-by-case basis later.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",uranusjr,2024-09-19 00:45:33+00:00,['sunank200'],2024-11-20 15:37:21+00:00,2024-11-20 14:33:10+00:00,https://github.com/apache/airflow/issues/42340,"[('kind:documentation', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('airflow3.0:candidate', 'Potential candidates for Airflow 3.0'), ('AIP-83', 'Remove Execution Date Unique Constraint from DAG Run')]","[{'comment_id': 2488745048, 'issue_id': 2535013290, 'author': 'sunank200', 'body': 'This can be closed as [#42404](https://github.com/apache/airflow/pull/42404) and [#43902](https://github.com/apache/airflow/pull/43902) was merged to main.', 'created_at': datetime.datetime(2024, 11, 20, 14, 33, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2488905118, 'issue_id': 2535013290, 'author': 'uranusjr', 'body': 'We still have some in the API. I’m not sure we need to work on those; will check with AIP-84 folks.', 'created_at': datetime.datetime(2024, 11, 20, 15, 33, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2488915190, 'issue_id': 2535013290, 'author': 'sunank200', 'body': 'Should I keep it open in that case? @uranusjr', 'created_at': datetime.datetime(2024, 11, 20, 15, 37, 8, tzinfo=datetime.timezone.utc)}]","sunank200 (Assginee) on (2024-11-20 14:33:10 UTC): This can be closed as [#42404](https://github.com/apache/airflow/pull/42404) and [#43902](https://github.com/apache/airflow/pull/43902) was merged to main.

uranusjr (Issue Creator) on (2024-11-20 15:33:25 UTC): We still have some in the API. I’m not sure we need to work on those; will check with AIP-84 folks.

sunank200 (Assginee) on (2024-11-20 15:37:08 UTC): Should I keep it open in that case? @uranusjr

"
2535002554,issue,closed,completed,"Remove execution_date from API, CLI, web UI return values","### Body

It should return the same value under `logical_date`. This field/attribute should already be present; if not, it should be added to the 2.x branch and released in 2.11.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",uranusjr,2024-09-19 00:35:25+00:00,['sunank200'],2024-11-20 14:01:21+00:00,2024-11-20 14:01:21+00:00,https://github.com/apache/airflow/issues/42339,"[('area:CLI', ''), ('area:API', ""Airflow's REST/HTTP API""), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('airflow3.0:candidate', 'Potential candidates for Airflow 3.0'), ('AIP-83', 'Remove Execution Date Unique Constraint from DAG Run')]","[{'comment_id': 2396202830, 'issue_id': 2535002554, 'author': 'sunank200', 'body': 'Most of the changes required for this is already part of this PR [https://github.com/apache/airflow/pull/42404](https://github.com/apache/airflow/pull/42404)', 'created_at': datetime.datetime(2024, 10, 7, 8, 3, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2478714000, 'issue_id': 2535002554, 'author': 'phanikumv', 'body': 'This is being addressed in https://github.com/apache/airflow/pull/42404', 'created_at': datetime.datetime(2024, 11, 15, 12, 32, 19, tzinfo=datetime.timezone.utc)}]","sunank200 (Assginee) on (2024-10-07 08:03:32 UTC): Most of the changes required for this is already part of this PR [https://github.com/apache/airflow/pull/42404](https://github.com/apache/airflow/pull/42404)

phanikumv on (2024-11-15 12:32:19 UTC): This is being addressed in https://github.com/apache/airflow/pull/42404

"
2535000716,issue,closed,completed,Remove execution_date and logical_date from arguments where function/API is used to look up a DAG run,"### Body

Since we’re dropping the unique constraint, those arguments no longer can look up a unique DAG run. Those functions should already take a `run_id`, which should be used instead going forward.

The use of execution/logical date in those functions should have already been deprecated. If not, a deprecation warning should be added to the 2.x branch to be released in 2.11.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",uranusjr,2024-09-19 00:32:59+00:00,['sunank200'],2024-11-20 14:26:18+00:00,2024-11-20 14:26:18+00:00,https://github.com/apache/airflow/issues/42338,"[('area:core', ''), ('airflow3.0:breaking', 'Candidates for Airflow 3.0 that contain breaking changes'), ('AIP-83', 'Remove Execution Date Unique Constraint from DAG Run')]","[{'comment_id': 2367253013, 'issue_id': 2535000716, 'author': 'sunank200', 'body': 'Created a draft PR [https://github.com/apache/airflow/pull/42404](https://github.com/apache/airflow/pull/42404).', 'created_at': datetime.datetime(2024, 9, 23, 5, 13, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2373157366, 'issue_id': 2535000716, 'author': 'sunank200', 'body': 'Currently fixing the tests and making CI green.', 'created_at': datetime.datetime(2024, 9, 25, 6, 37, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382161425, 'issue_id': 2535000716, 'author': 'sunank200', 'body': 'I have removed the `execution_date` completely. We have 202 files changed as part of this PR [#42404](https://github.com/apache/airflow/pull/42404) @uranusjr can you take a look, please for early review? I have renamed the execution date in the `TaskInstance` model and other classes. I am now working on fixing tests.', 'created_at': datetime.datetime(2024, 9, 30, 5, 44, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2396179270, 'issue_id': 2535000716, 'author': 'sunank200', 'body': 'I have fixed most of the tests. I am currently fixing the test for Core, WWW, API and CLI.', 'created_at': datetime.datetime(2024, 10, 7, 7, 52, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2478713344, 'issue_id': 2535000716, 'author': 'phanikumv', 'body': 'This is being addressed in https://github.com/apache/airflow/pull/42404', 'created_at': datetime.datetime(2024, 11, 15, 12, 31, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2488728048, 'issue_id': 2535000716, 'author': 'sunank200', 'body': 'This can be closed as [#42404](https://github.com/apache/airflow/pull/42404) is merged to main.', 'created_at': datetime.datetime(2024, 11, 20, 14, 26, 15, tzinfo=datetime.timezone.utc)}]","sunank200 (Assginee) on (2024-09-23 05:13:40 UTC): Created a draft PR [https://github.com/apache/airflow/pull/42404](https://github.com/apache/airflow/pull/42404).

sunank200 (Assginee) on (2024-09-25 06:37:30 UTC): Currently fixing the tests and making CI green.

sunank200 (Assginee) on (2024-09-30 05:44:54 UTC): I have removed the `execution_date` completely. We have 202 files changed as part of this PR [#42404](https://github.com/apache/airflow/pull/42404) @uranusjr can you take a look, please for early review? I have renamed the execution date in the `TaskInstance` model and other classes. I am now working on fixing tests.

sunank200 (Assginee) on (2024-10-07 07:52:13 UTC): I have fixed most of the tests. I am currently fixing the test for Core, WWW, API and CLI.

phanikumv on (2024-11-15 12:31:55 UTC): This is being addressed in https://github.com/apache/airflow/pull/42404

sunank200 (Assginee) on (2024-11-20 14:26:15 UTC): This can be closed as [#42404](https://github.com/apache/airflow/pull/42404) is merged to main.

"
2534793189,issue,closed,completed,Investigate better DAG version detection?,"### Body

Do this part of the AIP:

> We will explore having a more comprehensive version detection method (e.g. if the code inside of a PythonOperators python_callable changes, or in functions that that callable calls)

Simply detecting when the `python_callable` that is passed changes (somehow), might be enough and a good line.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2024-09-18 21:39:29+00:00,['ephraimbuddy'],2024-11-20 06:35:45+00:00,2024-11-20 06:35:44+00:00,https://github.com/apache/airflow/issues/42337,"[('kind:feature', 'Feature Requests'), ('kind:meta', 'High-level information important to the community'), ('area:core', ''), ('AIP-65: DAG history in UI', '')]","[{'comment_id': 2363185557, 'issue_id': 2534793189, 'author': 'kunaljubce', 'body': '@jedcunningham If I understand the ask correctly, as of today, the DAG UI still shows the older version of the `python_callable` code and we need a way to identify/log when the code change happens and show the updated code on the UI. Am I correct?', 'created_at': datetime.datetime(2024, 9, 20, 8, 41, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2487618290, 'issue_id': 2534793189, 'author': 'phanikumv', 'body': '[PR](https://github.com/apache/airflow/pull/44180) has been merged. Closing this issue.', 'created_at': datetime.datetime(2024, 11, 20, 6, 35, 44, tzinfo=datetime.timezone.utc)}]","kunaljubce on (2024-09-20 08:41:05 UTC): @jedcunningham If I understand the ask correctly, as of today, the DAG UI still shows the older version of the `python_callable` code and we need a way to identify/log when the code change happens and show the updated code on the UI. Am I correct?

phanikumv on (2024-11-20 06:35:44 UTC): [PR](https://github.com/apache/airflow/pull/44180) has been merged. Closing this issue.

"
2534791711,issue,closed,completed,Add user-provided version string to DAG version detection,"### Body

Let's make it possible for DAG authors to tell us a DAG version string.

We can't use it instead of the version from #42333, but we can use it as an input so folks can force a new version even if we aren't able to detect it.

We should also store it separately so that we can display it to the user - it may be meaningful to them, like a git tag or commit.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2024-09-18 21:38:19+00:00,['ephraimbuddy'],2024-11-06 06:36:32+00:00,2024-11-06 06:36:32+00:00,https://github.com/apache/airflow/issues/42336,"[('kind:feature', 'Feature Requests'), ('kind:meta', 'High-level information important to the community'), ('area:core', ''), ('AIP-65: DAG history in UI', '')]",[],
2534790645,issue,closed,completed,Add or modify DAG code REST API endpoint to support retrieving a specific version,"### Body

_No response_

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2024-09-18 21:37:30+00:00,['ephraimbuddy'],2024-11-20 06:35:08+00:00,2024-11-20 06:35:08+00:00,https://github.com/apache/airflow/issues/42335,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('kind:meta', 'High-level information important to the community'), ('AIP-65: DAG history in UI', '')]","[{'comment_id': 2472577165, 'issue_id': 2534790645, 'author': 'ephraimbuddy', 'body': 'PR: https://github.com/apache/airflow/pull/43492', 'created_at': datetime.datetime(2024, 11, 13, 6, 37, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2487617553, 'issue_id': 2534790645, 'author': 'phanikumv', 'body': 'Closing as the PR has been merged!', 'created_at': datetime.datetime(2024, 11, 20, 6, 35, 8, tzinfo=datetime.timezone.utc)}]","ephraimbuddy (Assginee) on (2024-11-13 06:37:37 UTC): PR: https://github.com/apache/airflow/pull/43492

phanikumv on (2024-11-20 06:35:08 UTC): Closing as the PR has been merged!

"
2534790146,issue,closed,completed,Refactor serialized_dag and dag_code to keep version history,"### Body

Add the version from #42333 to the PK for ser dag and DAG code.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2024-09-18 21:37:08+00:00,['ephraimbuddy'],2024-11-18 06:40:43+00:00,2024-11-18 06:40:42+00:00,https://github.com/apache/airflow/issues/42334,"[('area:serialization', ''), ('kind:meta', 'High-level information important to the community'), ('area:core', ''), ('AIP-65: DAG history in UI', '')]","[{'comment_id': 2472577810, 'issue_id': 2534790146, 'author': 'ephraimbuddy', 'body': 'PR: https://github.com/apache/airflow/pull/43821', 'created_at': datetime.datetime(2024, 11, 13, 6, 37, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2482080678, 'issue_id': 2534790146, 'author': 'phanikumv', 'body': 'Closing as the PR is merged', 'created_at': datetime.datetime(2024, 11, 18, 6, 40, 42, tzinfo=datetime.timezone.utc)}]","ephraimbuddy (Assginee) on (2024-11-13 06:37:55 UTC): PR: https://github.com/apache/airflow/pull/43821

phanikumv on (2024-11-18 06:40:42 UTC): Closing as the PR is merged

"
2534788748,issue,closed,completed,Calculate and track DAG version,"### Body

We need to calculate the DAG version based on the serialized DAG, and start tracking it on the task instance try level.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2024-09-18 21:36:11+00:00,['ephraimbuddy'],2024-11-05 14:19:26+00:00,2024-11-05 14:19:26+00:00,https://github.com/apache/airflow/issues/42333,"[('kind:feature', 'Feature Requests'), ('kind:meta', 'High-level information important to the community'), ('area:core', ''), ('AIP-65: DAG history in UI', '')]","[{'comment_id': 2382229609, 'issue_id': 2534788748, 'author': 'ephraimbuddy', 'body': 'PR: https://github.com/apache/airflow/pull/42517\r\nPR: https://github.com/apache/airflow/pull/42547', 'created_at': datetime.datetime(2024, 9, 30, 6, 35, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2396173018, 'issue_id': 2534788748, 'author': 'ephraimbuddy', 'body': 'PR: https://github.com/apache/airflow/pull/42690', 'created_at': datetime.datetime(2024, 10, 7, 7, 48, 57, tzinfo=datetime.timezone.utc)}]","ephraimbuddy (Assginee) on (2024-09-30 06:35:45 UTC): PR: https://github.com/apache/airflow/pull/42517
PR: https://github.com/apache/airflow/pull/42547

ephraimbuddy (Assginee) on (2024-10-07 07:48:57 UTC): PR: https://github.com/apache/airflow/pull/42690

"
2534702537,issue,closed,completed,AIP-78 Add schemas for API,"### Body

_No response_

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2024-09-18 20:49:28+00:00,['dstandish'],2024-10-07 15:52:28+00:00,2024-10-07 15:52:28+00:00,https://github.com/apache/airflow/issues/42332,"[('area:API', ""Airflow's REST/HTTP API""), ('kind:meta', 'High-level information important to the community')]",[],
2534698986,issue,closed,completed,AIP-78 Create ORM models,"### Body

Create the ORM models for scheduler-managed backfill

Roughly something like this

```
# one record for each backfill job
class BackfillRun:
    id: int  # primary key
    dag_id: str
 
    # generic config dict to hold params such as
    # start_date, end_date, and run_backwards.
    # intended to be flexiblie for potential changes
    # to how backfill works
    # e.g. changes in data completeness behavior
    config: dict  
    dag_run_conf: dict
    clear_failed_tasks: bool
    clear_dag_run: bool
    is_paused: bool
```

And 

```
# mapping table to associate a backfill run with dag runs
# this may not be necessary if we make dag runs ""immutable""
# i.e. if we stop reusing them, but that's out of scope
class BackfillRunDagRun:
    id: int
    backfill_run_id: int
    dag_run_id: int
    is_deleted: bool
```

Note: This is the first ticket of AIP-78 so the scope is actually a little broader and includes coming up with a minimal working implementation in order to validate the models.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2024-09-18 20:47:29+00:00,[],2024-09-18 20:49:00+00:00,2024-09-18 20:49:00+00:00,https://github.com/apache/airflow/issues/42331,"[('kind:meta', 'High-level information important to the community')]",[],
2534693315,issue,closed,completed,AIP-78 Update documentation,"### Body

ensure that backfill docs are updated to reflect the changes in AIP-78

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2024-09-18 20:43:54+00:00,[],2024-12-11 15:27:46+00:00,2024-12-11 15:27:46+00:00,https://github.com/apache/airflow/issues/42330,"[('kind:documentation', ''), ('kind:meta', 'High-level information important to the community'), ('area:backfill', 'Specifically for backfill related')]",[],
2534692644,issue,closed,completed,AIP-78 Document breaking changes,"### Body

Ensure that breaking changes in AIP-78 are documented appropriately in newsfragments and in airflow docs.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2024-09-18 20:43:24+00:00,[],2024-12-11 15:27:51+00:00,2024-12-11 15:27:51+00:00,https://github.com/apache/airflow/issues/42329,"[('kind:documentation', ''), ('kind:meta', 'High-level information important to the community')]",[],
2534692066,issue,closed,completed,AIP-78 Update the CLI,"### Body

Ensure we can use the cli to manage backfill with more or less same functionality as API

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2024-09-18 20:42:59+00:00,[],2024-10-16 13:25:13+00:00,2024-10-16 13:25:13+00:00,https://github.com/apache/airflow/issues/42328,"[('area:CLI', ''), ('kind:meta', 'High-level information important to the community'), ('area:backfill', 'Specifically for backfill related')]",[],
2534690564,issue,open,,AIP-78 Add a management interface to the UI,"### Body

Add interface for managing backfill jobs. Create / pause / cancel etc.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2024-09-18 20:42:03+00:00,[],2024-09-18 20:44:21+00:00,,https://github.com/apache/airflow/issues/42327,"[('kind:feature', 'Feature Requests'), ('kind:meta', 'High-level information important to the community'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('area:backfill', 'Specifically for backfill related')]",[],
2534688906,issue,closed,completed,AIP-78 Create API endpoints,"### Body

Add endpoints for management of backfills

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2024-09-18 20:41:09+00:00,[],2024-10-16 13:25:04+00:00,2024-10-16 13:25:04+00:00,https://github.com/apache/airflow/issues/42326,"[('area:API', ""Airflow's REST/HTTP API""), ('kind:meta', 'High-level information important to the community'), ('area:backfill', 'Specifically for backfill related')]",[],
2534687011,issue,closed,completed,AIP-78 Create management functions,"### Body

We will need some functions that interact with the ORM models. These will be called by the API, and possibly the CLI

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2024-09-18 20:39:57+00:00,[],2024-10-16 13:24:58+00:00,2024-10-16 13:24:58+00:00,https://github.com/apache/airflow/issues/42325,"[('area:CLI', ''), ('area:API', ""Airflow's REST/HTTP API""), ('kind:meta', 'High-level information important to the community')]",[],
2534685406,issue,closed,completed,AIP-78 Make updates to the scheduler,"### Body

With the ORM models in place, and functions to manage the ORM models, we'll be in a position to modify the scheduler to handle creation of backfill dag runs.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2024-09-18 20:39:07+00:00,['dstandish'],2024-10-18 13:59:56+00:00,2024-10-18 13:59:56+00:00,https://github.com/apache/airflow/issues/42324,"[('area:Scheduler', 'including HA (high availability) scheduler'), ('kind:meta', 'High-level information important to the community'), ('area:backfill', 'Specifically for backfill related')]",[],
2534205605,issue,open,,APIs Architecture,"### Context
Figure out how the architecture of the APIs will look like from a deployment (CLI, WSGI/ASGI processes, Helm Chart) and source code perspective.

Some of the constraints are:
- Keep compatibility with FAB and legacy plugin system.
- Allow users to run the task sdk API as a standalone.
- Think about the deployment complexity implied by additional reverse proxy or processes.

This could consist of a spike exploring how this could work or putting that on paper via an architecture diagram as requested by @jscheffl in the last airflow 3 dev call.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",pierrejeambrun,2024-09-18 16:37:11+00:00,[],2024-09-18 16:39:30+00:00,,https://github.com/apache/airflow/issues/42322,"[('area:API', ""Airflow's REST/HTTP API""), ('area:helm-chart', 'Airflow Helm Chart'), ('kind:meta', 'High-level information important to the community'), ('AIP-38', 'Modern Web Application'), ('AIP-84', 'Modern Rest API')]",[],
2533888976,issue,open,, New web UI for @asset,"### Description

[User Presentation Considerations](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=311627076#AIP75NewAssetCentricSyntax-UserPresentationConsiderations)

### Use case/motivation

[Rationale](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=311626828#AIP73ExpandedDataAwareness-Rationale)

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-09-18 14:20:38+00:00,"['uranusjr', 'Lee-W']",2025-01-15 03:20:59+00:00,,https://github.com/apache/airflow/issues/42319,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('area:datasets', 'Issues related to the datasets feature'), ('AIP-75', 'Asset-Centric Syntax')]","[{'comment_id': 2588466265, 'issue_id': 2533888976, 'author': 'Lee-W', 'body': ""Hi @bbovenzi @pierrejeambrun , we've wrapped up most of the APIs needed. What would be the best way for us to push the UI part forward? Thanks!"", 'created_at': datetime.datetime(2025, 1, 13, 23, 42, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2591020434, 'issue_id': 2533888976, 'author': 'bbovenzi', 'body': 'Would it help for me to initialize the asset UI pages with areas for you to fill in?', 'created_at': datetime.datetime(2025, 1, 14, 20, 16, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2591448093, 'issue_id': 2533888976, 'author': 'Lee-W', 'body': ""> Would it help for me to initialize the asset UI pages with areas for you to fill in?\n\nNot sure I have enough frontend knowledge to fill in all properly. 🤔  But yep, that would still be super helpful and I'm more than willing to try out 🙂"", 'created_at': datetime.datetime(2025, 1, 15, 1, 32, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2591558989, 'issue_id': 2533888976, 'author': 'bbovenzi', 'body': ""Sorry, I misread at first. I'll spec out what private UI endpoints we need for assets this week."", 'created_at': datetime.datetime(2025, 1, 15, 3, 20, 58, tzinfo=datetime.timezone.utc)}]","Lee-W (Issue Creator) on (2025-01-13 23:42:25 UTC): Hi @bbovenzi @pierrejeambrun , we've wrapped up most of the APIs needed. What would be the best way for us to push the UI part forward? Thanks!

bbovenzi on (2025-01-14 20:16:18 UTC): Would it help for me to initialize the asset UI pages with areas for you to fill in?

Lee-W (Issue Creator) on (2025-01-15 01:32:04 UTC): Not sure I have enough frontend knowledge to fill in all properly. 🤔  But yep, that would still be super helpful and I'm more than willing to try out 🙂

bbovenzi on (2025-01-15 03:20:58 UTC): Sorry, I misread at first. I'll spec out what private UI endpoints we need for assets this week.

"
2533887336,issue,open,, New REST API endpoints for assets,"### Description

[User Presentation Considerations](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=311627076#AIP75NewAssetCentricSyntax-UserPresentationConsiderations)

- [x] `/public/assets` changes
    - [x] Use `id` as detail identifier in URL https://github.com/apache/airflow/pull/44801
- [x] `/public/assets/aliases` https://github.com/apache/airflow/pull/44777
- [x] `/public/assets/aliases/{id}` https://github.com/apache/airflow/pull/44803
- [ ] `/public/assets/{id}/materialize`


### Use case/motivation

[Rationale](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=311626828#AIP73ExpandedDataAwareness-Rationale)

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-09-18 14:19:57+00:00,"['uranusjr', 'sunank200']",2025-02-05 06:45:18+00:00,,https://github.com/apache/airflow/issues/42318,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-75', 'Asset-Centric Syntax')]","[{'comment_id': 2550522668, 'issue_id': 2533887336, 'author': 'uranusjr', 'body': 'The two subitems under `/public/assets/` are deferred for now until the UI folks start implementing. We’ll figure out if we need them and what exactly at the point.', 'created_at': datetime.datetime(2024, 12, 18, 7, 7, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2572971706, 'issue_id': 2533887336, 'author': 'sunank200', 'body': 'As per discussion with TP, we will pick up `/public/assets/{id}/materialize` once the POC with partitioning strategy change with the execution date is clear.', 'created_at': datetime.datetime(2025, 1, 6, 12, 2, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2635828608, 'issue_id': 2533887336, 'author': 'phanikumv', 'body': '`/public/assets/{id}/materialize` will be picked up post https://github.com/apache/airflow/issues/46192', 'created_at': datetime.datetime(2025, 2, 5, 6, 45, 16, tzinfo=datetime.timezone.utc)}]","uranusjr (Assginee) on (2024-12-18 07:07:17 UTC): The two subitems under `/public/assets/` are deferred for now until the UI folks start implementing. We’ll figure out if we need them and what exactly at the point.

sunank200 (Assginee) on (2025-01-06 12:02:51 UTC): As per discussion with TP, we will pick up `/public/assets/{id}/materialize` once the POC with partitioning strategy change with the execution date is clear.

phanikumv on (2025-02-05 06:45:16 UTC): `/public/assets/{id}/materialize` will be picked up post https://github.com/apache/airflow/issues/46192

"
2533885504,issue,closed,completed," New ""airflow assets"" CLI subcommand","### Description

[User Presentation Considerations](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=311627076#AIP75NewAssetCentricSyntax-UserPresentationConsiderations)

* [x] `airflow assets list` https://github.com/apache/airflow/pull/44387
* [x] `airflow assets detail` https://github.com/apache/airflow/pull/44445
* [x] `airflow assets --alias` to list asset aliases https://github.com/apache/airflow/pull/44595
* [x] `airflow assets detail --alias` to view an asset alias https://github.com/apache/airflow/pull/44595
* [x] `airflow assets materialize` https://github.com/apache/airflow/pull/44558
    * something like `airflow dags trigger`
    * steps
        1. get an asset
        2. find the tasks reference this asset as an outlet
            1. raise an error if there's not
            2. raise an error if there're more than one (as of now)
        3. trigger that DAG

### Use case/motivation

[Rationale](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=311626828#AIP73ExpandedDataAwareness-Rationale)

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-09-18 14:19:14+00:00,['uranusjr'],2024-12-04 23:37:25+00:00,2024-12-04 23:37:16+00:00,https://github.com/apache/airflow/issues/42317,"[('area:CLI', ''), ('kind:feature', 'Feature Requests'), ('AIP-75', 'Asset-Centric Syntax')]","[{'comment_id': 2500940133, 'issue_id': 2533885504, 'author': 'uranusjr', 'body': 'First CLI subcommand `list` #44387', 'created_at': datetime.datetime(2024, 11, 26, 14, 24, 54, tzinfo=datetime.timezone.utc)}]","uranusjr (Assginee) on (2024-11-26 14:24:54 UTC): First CLI subcommand `list` #44387

"
2533881413,issue,closed,completed, Implement @asset.multi,"### Description

[Multi-Assets](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=311627076#AIP75NewAssetCentricSyntax-Multi-Assets)

(This is not a must-have for AIP-75)

### Use case/motivation

[Rationale](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=311626828#AIP73ExpandedDataAwareness-Rationale)

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-09-18 14:17:32+00:00,['uranusjr'],2024-12-06 14:22:27+00:00,2024-12-06 14:22:27+00:00,https://github.com/apache/airflow/issues/42316,"[('kind:feature', 'Feature Requests'), ('area:datasets', 'Issues related to the datasets feature'), ('AIP-75', 'Asset-Centric Syntax')]","[{'comment_id': 2521927915, 'issue_id': 2533881413, 'author': 'uranusjr', 'body': 'PR for this #44711.', 'created_at': datetime.datetime(2024, 12, 6, 2, 6, 11, tzinfo=datetime.timezone.utc)}]","uranusjr (Assginee) on (2024-12-06 02:06:11 UTC): PR for this #44711.

"
2533875562,issue,open,,Implement schedule on @asset,"### Description

[Schedules](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=311627076#AIP75NewAssetCentricSyntax-Schedules)

This will probably be achieve by using `AssetRef` to access Asset by name

This one could be deferred to ""Data Completeness""

### Use case/motivation

[Rationale](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=311626828#AIP73ExpandedDataAwareness-Rationale)

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-09-18 14:15:12+00:00,['uranusjr'],2025-01-06 23:14:41+00:00,,https://github.com/apache/airflow/issues/42315,"[('kind:feature', 'Feature Requests'), ('area:datasets', 'Issues related to the datasets feature'), ('AIP-75', 'Asset-Centric Syntax')]","[{'comment_id': 2551149216, 'issue_id': 2533875562, 'author': 'uranusjr', 'body': 'PR to add asset reference support https://github.com/apache/airflow/pull/45028', 'created_at': datetime.datetime(2024, 12, 18, 12, 4, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2551150962, 'issue_id': 2533875562, 'author': 'uranusjr', 'body': 'There’s a second part to come later to implement modern schedule dates (logical date at the end, no data intervals).', 'created_at': datetime.datetime(2024, 12, 18, 12, 5, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2562550776, 'issue_id': 2533875562, 'author': 'uranusjr', 'body': 'PR for asset ref merged.', 'created_at': datetime.datetime(2024, 12, 26, 11, 50, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2574090913, 'issue_id': 2533875562, 'author': 'uranusjr', 'body': 'There’s still this part of the AIP left https://cwiki.apache.org/confluence/x/RA2TEg\r\n\r\n> The key difference is, since we are decoupling partitioning from scheduling, the schedule parameter no longer controls the interval, i.e. not designed around the logical/execution date. It simply controls when the next round should happen.\r\n>\r\n> It is expected that scheduling of non-asset workflows (DAGs) will also be changed in a similar way to match the behavior for assets. Existing operators must be reviewed to ensure they account for the new scheduling semantic, but we should provide a transition interface to assist rewrites and better allow providers to develop a implementation both compatible to 2 and 3. The timetable protocol will also require new methods to implement the new semantic, but both old and new should be able to both implemented on the same class, with each major version only calling methods its uses.\r\n\r\nThis behaviour is partially in line with the amendments proposed for AIP-83, and will be implemented together with changes to the DAG class. https://cwiki.apache.org/confluence/x/Ngv0Ew\r\n\r\nSpecifically:\r\n\r\n* A DAG run backing an `@asset` materialisation will have its logical date and data interval values set to null in the database.\r\n* The `@asset` function’s execution context dict will _not_ contain keys `logical_date` (and derived values such as `ds`), `data_interval_start`, and `data_interval_end`.\r\n* The run will be represented in user-facing interfaces by the `run_after` date (in line with all logical-date-less DAG runs in the amendment).', 'created_at': datetime.datetime(2025, 1, 6, 23, 14, 40, tzinfo=datetime.timezone.utc)}]","uranusjr (Assginee) on (2024-12-18 12:04:34 UTC): PR to add asset reference support https://github.com/apache/airflow/pull/45028

uranusjr (Assginee) on (2024-12-18 12:05:33 UTC): There’s a second part to come later to implement modern schedule dates (logical date at the end, no data intervals).

uranusjr (Assginee) on (2024-12-26 11:50:46 UTC): PR for asset ref merged.

uranusjr (Assginee) on (2025-01-06 23:14:40 UTC): There’s still this part of the AIP left https://cwiki.apache.org/confluence/x/RA2TEg


This behaviour is partially in line with the amendments proposed for AIP-83, and will be implemented together with changes to the DAG class. https://cwiki.apache.org/confluence/x/Ngv0Ew

Specifically:

* A DAG run backing an `@asset` materialisation will have its logical date and data interval values set to null in the database.
* The `@asset` function’s execution context dict will _not_ contain keys `logical_date` (and derived values such as `ds`), `data_interval_start`, and `data_interval_end`.
* The run will be represented in user-facing interfaces by the `run_after` date (in line with all logical-date-less DAG runs in the amendment).

"
2533872102,issue,closed,completed,Implement skeleton @asset decorator,"### Description

[Reference an Asset](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=311627076#AIP75NewAssetCentricSyntax-ReferenceanAsset)

### Use case/motivation

[Rationale](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=311626828#AIP73ExpandedDataAwareness-Rationale)

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-09-18 14:13:46+00:00,['Lee-W'],2024-11-14 09:14:47+00:00,2024-11-14 09:14:47+00:00,https://github.com/apache/airflow/issues/42314,"[('kind:feature', 'Feature Requests'), ('area:datasets', 'Issues related to the datasets feature'), ('AIP-75', 'Asset-Centric Syntax')]","[{'comment_id': 2367124844, 'issue_id': 2533872102, 'author': 'Lee-W', 'body': 'Updated: Nov 11th\n\n* https://github.com/apache/airflow/pull/41325 in review\n* https://github.com/apache/airflow/pull/43922 in review', 'created_at': datetime.datetime(2024, 9, 23, 2, 19, 33, tzinfo=datetime.timezone.utc)}]","Lee-W (Issue Creator) on (2024-09-23 02:19:33 UTC): Updated: Nov 11th

* https://github.com/apache/airflow/pull/41325 in review
* https://github.com/apache/airflow/pull/43922 in review

"
2533863502,issue,open,, UI for asset groups in graph view,"### Description

[The Asset Class](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=311627073#AIP74IntroducingDataAssets-TheAssetClass). The UI part of https://github.com/apache/airflow/issues/42310

### Use case/motivation

[Rationale](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=311627073#AIP74IntroducingDataAssets-Rationale)

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-09-18 14:10:53+00:00,"['uranusjr', 'Lee-W']",2025-01-15 01:35:00+00:00,,https://github.com/apache/airflow/issues/42313,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('area:datasets', 'Issues related to the datasets feature'), ('AIP-75', 'Asset-Centric Syntax')]","[{'comment_id': 2588465544, 'issue_id': 2533863502, 'author': 'Lee-W', 'body': ""Hi @bbovenzi @pierrejeambrun , we've wrapped up most of the APIs needed. What would be the best way for us to push the UI part forward? Thanks!"", 'created_at': datetime.datetime(2025, 1, 13, 23, 41, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2591019233, 'issue_id': 2533863502, 'author': 'bbovenzi', 'body': 'Do we expect asset groups to work like task groups in a graph or more like a ""type"" of asset?', 'created_at': datetime.datetime(2025, 1, 14, 20, 15, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2591451000, 'issue_id': 2533863502, 'author': 'Lee-W', 'body': '> Do we expect asset groups to work like task groups in a graph or more like a ""type"" of asset?\n\nYep! I think that\'s what we want!', 'created_at': datetime.datetime(2025, 1, 15, 1, 34, 58, tzinfo=datetime.timezone.utc)}]","Lee-W (Issue Creator) on (2025-01-13 23:41:02 UTC): Hi @bbovenzi @pierrejeambrun , we've wrapped up most of the APIs needed. What would be the best way for us to push the UI part forward? Thanks!

bbovenzi on (2025-01-14 20:15:33 UTC): Do we expect asset groups to work like task groups in a graph or more like a ""type"" of asset?

Lee-W (Issue Creator) on (2025-01-15 01:34:58 UTC): Yep! I think that's what we want!

"
2533857352,issue,closed,completed, Add Dataset and Model subclasses,"### Description

[Asset Subclasses](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=311627073#AIP74IntroducingDataAssets-AssetSubclasses)

### Use case/motivation

[Rationale](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=311627073#AIP74IntroducingDataAssets-Rationale)

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-09-18 14:09:10+00:00,['Lee-W'],2024-10-23 06:10:20+00:00,2024-10-23 06:10:19+00:00,https://github.com/apache/airflow/issues/42312,"[('kind:feature', 'Feature Requests'), ('area:datasets', 'Issues related to the datasets feature'), ('AIP-74', 'Dataset -> Asset')]",[],
2533848089,issue,closed,completed,"Add attributes ""name"" and ""group"" to Asset class","### Description

[The Asset Class](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=311627073#AIP74IntroducingDataAssets-TheAssetClass)

### Use case/motivation

[Rationale](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=311627073#AIP74IntroducingDataAssets-Rationale)

### Related issues

https://github.com/apache/airflow/issues/42307

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-09-18 14:05:21+00:00,['uranusjr'],2024-10-16 13:11:38+00:00,2024-10-16 13:11:38+00:00,https://github.com/apache/airflow/issues/42310,"[('kind:feature', 'Feature Requests'), ('area:datasets', 'Issues related to the datasets feature'), ('AIP-74', 'Dataset -> Asset')]","[{'comment_id': 2367490776, 'issue_id': 2533848089, 'author': 'uranusjr', 'body': 'First PR published on the database end. https://github.com/apache/airflow/pull/42407', 'created_at': datetime.datetime(2024, 9, 23, 8, 3, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382436672, 'issue_id': 2533848089, 'author': 'uranusjr', 'body': 'Database changes are done. Next I’m going to add the two arguments to the public-facing Asset class so the user can pass them to be stored in the database.\n\nThere’s one thing we need to address in the database after we do the above. Since we want to keep history in the asset table, we need to have the unique constraint on `(name, uri)` so a name can be reused by different assets as long as the URIs are different, vice versa. But we still want only one _active_ asset to use that name. So I’ll add a separate table to only keep track of active (non-orphaned) assets so we can add all those unique constraints appropriately.', 'created_at': datetime.datetime(2024, 9, 30, 8, 24, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2398515424, 'issue_id': 2533848089, 'author': 'uranusjr', 'body': 'PR to add database-level uniqu constraint (see previous message)\nhttps://github.com/apache/airflow/pull/42612\n\n(Draft) Main change to add `name` and `group` to Asset (tests tbd)\nhttps://github.com/apache/airflow/pull/42812\n\n(Draft) Tweak AssetAlias to match Asset (not strictly a part of AIP-73 but should be a good thing)\nhttps://github.com/apache/airflow/pull/42814', 'created_at': datetime.datetime(2024, 10, 8, 2, 13, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415599519, 'issue_id': 2533848089, 'author': 'uranusjr', 'body': 'Both `name` and `group` have been added to `Asset`, and they are stored on `AssetModel`.\n\n#42814 is a related PR to also add `group` to `AssetAlias`, and change `name` constraints to match `Asset`, but not a strict blocker.', 'created_at': datetime.datetime(2024, 10, 16, 2, 31, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2416802366, 'issue_id': 2533848089, 'author': 'uranusjr', 'body': 'I think we’re done here!', 'created_at': datetime.datetime(2024, 10, 16, 13, 11, 38, tzinfo=datetime.timezone.utc)}]","uranusjr (Assginee) on (2024-09-23 08:03:41 UTC): First PR published on the database end. https://github.com/apache/airflow/pull/42407

uranusjr (Assginee) on (2024-09-30 08:24:32 UTC): Database changes are done. Next I’m going to add the two arguments to the public-facing Asset class so the user can pass them to be stored in the database.

There’s one thing we need to address in the database after we do the above. Since we want to keep history in the asset table, we need to have the unique constraint on `(name, uri)` so a name can be reused by different assets as long as the URIs are different, vice versa. But we still want only one _active_ asset to use that name. So I’ll add a separate table to only keep track of active (non-orphaned) assets so we can add all those unique constraints appropriately.

uranusjr (Assginee) on (2024-10-08 02:13:17 UTC): PR to add database-level uniqu constraint (see previous message)
https://github.com/apache/airflow/pull/42612

(Draft) Main change to add `name` and `group` to Asset (tests tbd)
https://github.com/apache/airflow/pull/42812

(Draft) Tweak AssetAlias to match Asset (not strictly a part of AIP-73 but should be a good thing)
https://github.com/apache/airflow/pull/42814

uranusjr (Assginee) on (2024-10-16 02:31:31 UTC): Both `name` and `group` have been added to `Asset`, and they are stored on `AssetModel`.

#42814 is a related PR to also add `group` to `AssetAlias`, and change `name` constraints to match `Asset`, but not a strict blocker.

uranusjr (Assginee) on (2024-10-16 13:11:38 UTC): I think we’re done here!

"
2533833680,issue,closed,completed,"Rename ""Dataset"" as Asset","### Description
[Rename Datasets to Assets](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=311627073#AIP74IntroducingDataAssets-RenameDatasetstoAssets)

### Use case/motivation

[Rationale](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=311627073#AIP74IntroducingDataAssets-Rationale)

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-09-18 13:59:29+00:00,['Lee-W'],2024-10-28 06:20:18+00:00,2024-10-28 06:20:18+00:00,https://github.com/apache/airflow/issues/42307,"[('kind:feature', 'Feature Requests'), ('area:datasets', 'Issues related to the datasets feature'), ('AIP-74', 'Dataset -> Asset')]","[{'comment_id': 2358555081, 'issue_id': 2533833680, 'author': 'Lee-W', 'body': '## Current Status\r\nUpdated: Oct 28th\r\n\r\n---\r\n\r\n* [x] Check https://apache-airflow.slack.com/archives/C06K9Q5G2UA/p1729553380546019\r\n    * This is not how `airflow db reset` should be used\r\n* ✅ Merged\r\n    * https://github.com/apache/airflow/pull/41828\r\n    * [x] `Doc change` and `Python module/variable name` are mostly done in https://github.com/apache/airflow/pull/41348\r\n        * [x] list the changes and update newsfragments files\r\n        * [x] fix lineage compatibility issue\r\n        * [x] fix fab compatibility\r\n        * [x] fix the remaining CI failure\r\n        * [x] fix new conflicts and CI failures\r\n        * [x] split out the compat changes as discussed with TP during offsite\r\n        * [x] Resolve new conflicts after the last green CI\r\n    * [x] API endpoint change https://github.com/apache/airflow/pull/425\r\n    * [x] https://github.com/apache/airflow/pull/41814\r\n    * [x] Reuse common.compat.asset\r\n        * https://github.com/apache/airflow/pull/43112\r\n        * https://github.com/apache/airflow/pull/43111\r\n        * https://github.com/apache/airflow/pull/43110\r\n    * [x] database change https://github.com/apache/airflow/pull/42023\r\n        * [x] Fix CI failure\r\n            * [x] Postgres\r\n            * [x] sqlite\r\n            * [x] MySQL\r\n        * [x] Update newsfragment\r\n    * [x] UI files change https://github.com/apache/airflow/pull/43073\r\n    * [x] https://github.com/apache/airflow/pull/43245\r\n    * [x] https://github.com/apache/airflow/pull/43313\r\n    * [x] https://github.com/apache/airflow/pull/43314', 'created_at': datetime.datetime(2024, 9, 18, 14, 0, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2440650043, 'issue_id': 2533833680, 'author': 'Lee-W', 'body': 'As all the subtasks have been resolved, close this issue.', 'created_at': datetime.datetime(2024, 10, 28, 6, 20, 18, tzinfo=datetime.timezone.utc)}]","Lee-W (Issue Creator) on (2024-09-18 14:00:27 UTC): ## Current Status
Updated: Oct 28th

---

* [x] Check https://apache-airflow.slack.com/archives/C06K9Q5G2UA/p1729553380546019
    * This is not how `airflow db reset` should be used
* ✅ Merged
    * https://github.com/apache/airflow/pull/41828
    * [x] `Doc change` and `Python module/variable name` are mostly done in https://github.com/apache/airflow/pull/41348
        * [x] list the changes and update newsfragments files
        * [x] fix lineage compatibility issue
        * [x] fix fab compatibility
        * [x] fix the remaining CI failure
        * [x] fix new conflicts and CI failures
        * [x] split out the compat changes as discussed with TP during offsite
        * [x] Resolve new conflicts after the last green CI
    * [x] API endpoint change https://github.com/apache/airflow/pull/425
    * [x] https://github.com/apache/airflow/pull/41814
    * [x] Reuse common.compat.asset
        * https://github.com/apache/airflow/pull/43112
        * https://github.com/apache/airflow/pull/43111
        * https://github.com/apache/airflow/pull/43110
    * [x] database change https://github.com/apache/airflow/pull/42023
        * [x] Fix CI failure
            * [x] Postgres
            * [x] sqlite
            * [x] MySQL
        * [x] Update newsfragment
    * [x] UI files change https://github.com/apache/airflow/pull/43073
    * [x] https://github.com/apache/airflow/pull/43245
    * [x] https://github.com/apache/airflow/pull/43313
    * [x] https://github.com/apache/airflow/pull/43314

Lee-W (Issue Creator) on (2024-10-28 06:20:18 UTC): As all the subtasks have been resolved, close this issue.

"
2532898147,issue,closed,completed,Changed behaviour on macros template due to Pendulum v3,"### Apache Airflow version

2.10.1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Due to changed default behaviour on Pendulum v3.x.x, the default print behaviour for template is getting affected. Its breaking dags that rely on template that using Pendulum type like `data_interval_start` and `data_interval_end`.

The current work around is to using Airflow Jinja filter `ts`.


### What you think should happen instead?

_No response_

### How to reproduce

In Pendulum 2:
```
>>> import pendulum
>>> str(pendulum.now())
'2024-01-29T16:00:38.678167-06:00'
```

In Pendulum 3:
```
>>> import pendulum
>>> str(pendulum.now())
'2024-01-29 16:00:23.305698-06:00'
>>> 
```

### Operating System

Docker/Debian

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",fritzwijaya,2024-09-18 07:02:11+00:00,[],2024-09-19 10:51:29+00:00,2024-09-19 10:51:28+00:00,https://github.com/apache/airflow/issues/42302,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2357663261, 'issue_id': 2532898147, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 18, 7, 2, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2360655940, 'issue_id': 2532898147, 'author': 'nathadfield', 'body': ""This has been [discussed](https://github.com/apache/airflow/discussions/37037) before.  I'd suggest using [.isoformat(sep='T')](https://docs.python.org/3/library/datetime.html#datetime.date.isoformat) if you require back compat."", 'created_at': datetime.datetime(2024, 9, 19, 10, 51, 29, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-18 07:02:14 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

nathadfield on (2024-09-19 10:51:29 UTC): This has been [discussed](https://github.com/apache/airflow/discussions/37037) before.  I'd suggest using [.isoformat(sep='T')](https://docs.python.org/3/library/datetime.html#datetime.date.isoformat) if you require back compat.

"
2532679664,issue,closed,completed,Determine what interval control options we need from manifest file,"### Body

Let's figure out the right options to support better parsing interval options from a manifest file.

See the example manifest file in [AIP-66](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=294816356).

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2024-09-18 04:34:37+00:00,[],2025-01-28 04:27:41+00:00,2025-01-28 04:27:40+00:00,https://github.com/apache/airflow/issues/42299,"[('kind:meta', 'High-level information important to the community'), ('AIP-66: DAG Bundle/Manifest', '')]","[{'comment_id': 2617818454, 'issue_id': 2532679664, 'author': 'jedcunningham', 'body': 'Going to close this for now - manifest support will be coming in 3.1+.', 'created_at': datetime.datetime(2025, 1, 28, 4, 27, 40, tzinfo=datetime.timezone.utc)}]","jedcunningham (Issue Creator) on (2025-01-28 04:27:40 UTC): Going to close this for now - manifest support will be coming in 3.1+.

"
2532679326,issue,closed,completed,Support configuring bundle parsing interval,"### Body

I originally had this in the manifest file itself, however, maybe it should be part of the bundle config instead? Options.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2024-09-18 04:34:16+00:00,[],2025-01-23 02:30:56+00:00,2025-01-23 02:30:55+00:00,https://github.com/apache/airflow/issues/42298,"[('kind:meta', 'High-level information important to the community'), ('AIP-66: DAG Bundle/Manifest', '')]","[{'comment_id': 2608718080, 'issue_id': 2532679326, 'author': 'jedcunningham', 'body': 'This has been moved into bundle config.', 'created_at': datetime.datetime(2025, 1, 23, 2, 30, 55, tzinfo=datetime.timezone.utc)}]","jedcunningham (Issue Creator) on (2025-01-23 02:30:55 UTC): This has been moved into bundle config.

"
2532679023,issue,closed,completed,Add support for manifest files in DAG bundles,"### Body

_No response_

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2024-09-18 04:33:58+00:00,[],2025-01-28 04:27:31+00:00,2025-01-28 04:27:30+00:00,https://github.com/apache/airflow/issues/42297,"[('kind:feature', 'Feature Requests'), ('kind:meta', 'High-level information important to the community'), ('area:core', ''), ('AIP-66: DAG Bundle/Manifest', '')]","[{'comment_id': 2617818313, 'issue_id': 2532679023, 'author': 'jedcunningham', 'body': 'Going to close this for now - manifest support will be coming in 3.1+.', 'created_at': datetime.datetime(2025, 1, 28, 4, 27, 30, tzinfo=datetime.timezone.utc)}]","jedcunningham (Issue Creator) on (2025-01-28 04:27:30 UTC): Going to close this for now - manifest support will be coming in 3.1+.

"
2532678766,issue,open,,Harden DAG Bundle backends,"### Body

_No response_

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2024-09-18 04:33:44+00:00,['ephraimbuddy'],2025-02-03 06:38:26+00:00,,https://github.com/apache/airflow/issues/42296,"[('kind:meta', 'High-level information important to the community'), ('AIP-66: DAG Bundle/Manifest', '')]","[{'comment_id': 2630092068, 'issue_id': 2532678766, 'author': 'ephraimbuddy', 'body': 'PRs:\n1. https://github.com/apache/airflow/pull/45982\n2. https://github.com/apache/airflow/pull/46241\n3. https://github.com/apache/airflow/pull/46253\n\nIssues:\n1. https://github.com/apache/airflow/issues/46362', 'created_at': datetime.datetime(2025, 2, 3, 6, 38, 25, tzinfo=datetime.timezone.utc)}]","ephraimbuddy (Assginee) on (2025-02-03 06:38:25 UTC): PRs:
1. https://github.com/apache/airflow/pull/45982
2. https://github.com/apache/airflow/pull/46241
3. https://github.com/apache/airflow/pull/46253

Issues:
1. https://github.com/apache/airflow/issues/46362

"
2532678332,issue,closed,not_planned,Add UI for configuring DAG bundle backends,"### Body

This is likely to be blocked a bit by AIP-38, but that's okay.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2024-09-18 04:33:20+00:00,[],2024-12-19 16:34:56+00:00,2024-12-19 16:34:55+00:00,https://github.com/apache/airflow/issues/42295,"[('kind:feature', 'Feature Requests'), ('kind:meta', 'High-level information important to the community'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('AIP-66: DAG Bundle/Manifest', '')]","[{'comment_id': 2554974641, 'issue_id': 2532678332, 'author': 'jedcunningham', 'body': ""We don't need to configure these in the UI any longer - that now happens in config instead."", 'created_at': datetime.datetime(2024, 12, 19, 16, 34, 55, tzinfo=datetime.timezone.utc)}]","jedcunningham (Issue Creator) on (2024-12-19 16:34:55 UTC): We don't need to configure these in the UI any longer - that now happens in config instead.

"
2532677932,issue,closed,not_planned,Add REST API for configuring DAG bundle backends,"### Body

_No response_

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2024-09-18 04:32:56+00:00,[],2024-12-19 16:34:03+00:00,2024-12-19 16:34:03+00:00,https://github.com/apache/airflow/issues/42294,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('kind:meta', 'High-level information important to the community'), ('AIP-66: DAG Bundle/Manifest', '')]","[{'comment_id': 2554970312, 'issue_id': 2532677932, 'author': 'jedcunningham', 'body': 'These are not being configured in the db any longer, removing the need for this endpoint.', 'created_at': datetime.datetime(2024, 12, 19, 16, 34, 3, tzinfo=datetime.timezone.utc)}]","jedcunningham (Issue Creator) on (2024-12-19 16:34:03 UTC): These are not being configured in the db any longer, removing the need for this endpoint.

"
2532677708,issue,closed,not_planned,Add cli for configuring DAG bundle backends,"### Body

_No response_

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2024-09-18 04:32:42+00:00,[],2025-01-16 03:30:13+00:00,2024-12-19 16:33:31+00:00,https://github.com/apache/airflow/issues/42293,"[('area:CLI', ''), ('kind:meta', 'High-level information important to the community'), ('AIP-66: DAG Bundle/Manifest', '')]","[{'comment_id': 2543186865, 'issue_id': 2532677708, 'author': 'Prab-27', 'body': ""@jedcunningham , I'd like to work on this issue."", 'created_at': datetime.datetime(2024, 12, 14, 17, 7, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2554967745, 'issue_id': 2532677708, 'author': 'jedcunningham', 'body': ""Hi @Prab-27, sorry, we've gone a different direction and these are being configured in configuration instead (#44924). However, there is some other cli/api work if you are open to it. I'll tag you in those issues later today."", 'created_at': datetime.datetime(2024, 12, 19, 16, 33, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2555169248, 'issue_id': 2532677708, 'author': 'Prab-27', 'body': ""Thanks @jedcunningham. I'd like to work on that type of issue. If there are any, tag me."", 'created_at': datetime.datetime(2024, 12, 19, 17, 9, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2594404461, 'issue_id': 2532677708, 'author': 'jedcunningham', 'body': ""@Prab-27, there are a number of issues in the [project for AIP-66](https://github.com/orgs/apache/projects/406), if you'd like to pick up one of them."", 'created_at': datetime.datetime(2025, 1, 16, 3, 30, 12, tzinfo=datetime.timezone.utc)}]","Prab-27 on (2024-12-14 17:07:19 UTC): @jedcunningham , I'd like to work on this issue.

jedcunningham (Issue Creator) on (2024-12-19 16:33:31 UTC): Hi @Prab-27, sorry, we've gone a different direction and these are being configured in configuration instead (#44924). However, there is some other cli/api work if you are open to it. I'll tag you in those issues later today.

Prab-27 on (2024-12-19 17:09:48 UTC): Thanks @jedcunningham. I'd like to work on that type of issue. If there are any, tag me.

jedcunningham (Issue Creator) on (2025-01-16 03:30:12 UTC): @Prab-27, there are a number of issues in the [project for AIP-66](https://github.com/orgs/apache/projects/406), if you'd like to pick up one of them.

"
2532677488,issue,closed,completed,Add model for configuring DAG bundle backends,"### Body

_No response_

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2024-09-18 04:32:29+00:00,['jedcunningham'],2024-12-19 16:31:56+00:00,2024-12-19 16:31:55+00:00,https://github.com/apache/airflow/issues/42292,"[('kind:meta', 'High-level information important to the community'), ('AIP-66: DAG Bundle/Manifest', '')]","[{'comment_id': 2554959434, 'issue_id': 2532677488, 'author': 'jedcunningham', 'body': 'This was done in #44924.', 'created_at': datetime.datetime(2024, 12, 19, 16, 31, 55, tzinfo=datetime.timezone.utc)}]","jedcunningham (Issue Creator) on (2024-12-19 16:31:55 UTC): This was done in #44924.

"
2532677275,issue,closed,completed,DB changes to track DAG bundle versions,"### Body

We need the ability to track the DAG bundle version on the dagrun. This will allow the scheduler to pass the version on down to the worker.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2024-09-18 04:32:16+00:00,['jedcunningham'],2025-01-17 23:03:42+00:00,2025-01-17 23:03:42+00:00,https://github.com/apache/airflow/issues/42291,"[('kind:meta', 'High-level information important to the community'), ('area:core', ''), ('area:db-migrations', 'PRs with DB migration'), ('AIP-66: DAG Bundle/Manifest', '')]",[],
2532676302,issue,closed,completed,Modify task runtime to use DAG bundle backends,"### Body

Let tasks use DAG bundle backends during runtime. This will include the flexibility to have a specific version available.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2024-09-18 04:31:17+00:00,['dstandish'],2025-01-15 19:16:39+00:00,2025-01-15 19:16:39+00:00,https://github.com/apache/airflow/issues/42290,"[('kind:meta', 'High-level information important to the community'), ('AIP-66: DAG Bundle/Manifest', '')]",[],
2532675396,issue,closed,completed,Modify DagBag/Parsing loop to be DAG bundle backend aware,"### Body

The DagBag and parsing loop, in particular when configured not to use the DB, needs to be backend bundle aware. TBD what this really means though.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2024-09-18 04:30:23+00:00,['jedcunningham'],2025-01-17 23:03:39+00:00,2025-01-17 23:03:39+00:00,https://github.com/apache/airflow/issues/42289,"[('kind:meta', 'High-level information important to the community'), ('area:DAG-processing', ''), ('AIP-66: DAG Bundle/Manifest', '')]",[],
2532674668,issue,closed,completed,Create git DAG bundle backend,"### Body

Create a git DAG bundle backend, with versioning.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2024-09-18 04:29:46+00:00,['jedcunningham'],2024-11-08 20:20:28+00:00,2024-11-08 20:20:28+00:00,https://github.com/apache/airflow/issues/42288,"[('kind:meta', 'High-level information important to the community'), ('area:core', ''), ('AIP-66: DAG Bundle/Manifest', '')]",[],
2532674308,issue,closed,completed,Create Local DAG bundle backend,"### Body

We need a local DAG bundle backend, which will be used for backcompat with the DAG folder and for local dev. It will not support versioning.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2024-09-18 04:29:29+00:00,['jedcunningham'],2024-11-08 20:20:28+00:00,2024-11-08 20:20:28+00:00,https://github.com/apache/airflow/issues/42287,"[('kind:meta', 'High-level information important to the community'), ('area:core', ''), ('AIP-66: DAG Bundle/Manifest', '')]",[],
2532673865,issue,closed,completed,DAG Bundle Backend Interface,"### Body

Define and commit the DAG Bundle Backend Interface.

### Committer

- [x] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",jedcunningham,2024-09-18 04:29:04+00:00,['jedcunningham'],2024-12-19 16:30:54+00:00,2024-12-19 16:30:53+00:00,https://github.com/apache/airflow/issues/42286,"[('kind:meta', 'High-level information important to the community'), ('area:core', ''), ('AIP-66: DAG Bundle/Manifest', '')]",[],
2532106287,issue,open,,kubectl token invalid when airflow debug mode enabled,"### Apache Airflow version

2.10.1

### If ""Other Airflow 2 version"" selected, which one?

2.9.2

### What happened?

when airflow debug mode is enabled the kubeconfig sh script greps for invalid token thus unable to connect to k8s cluster. I hit the issue in aws eks environment.

### What you think should happen instead?

_No response_

### How to reproduce

enable debug mode and then run any kubectl command from your dag. You will get 401 error saying: unable to parse the token, wrong formatting. The reason for this is that the grep get the sessionToken instead of k8s token that authenticates with k8s api.

this is how to change the logging level: https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#logging-level

this is the grep that get aws sessionToken instead of k8s api token: https://github.com/apache/airflow/blob/main/airflow/providers/amazon/aws/hooks/eks.py#L89

and this is the token it matches: 'X-Amz-Security-Token': b'IQoJb3JpZ2luX2VjEN3//////////wEaCXVzLWVhc3QtMSJHMEUCIQDGkojyARf8ka1imf4+Nztaon03YN+dEtEYxUvLY1n/FQIgO9eCbrKKQ5h9nY6fZwKKNvd7w48PyQgx4JzCEBbLchgqjAUIFhABGgwzMzEwMTM5ODY5MzYiDHsX+8rDL/scEHQDtSrpBD3IlZLQyF8xB11v57aTFDlqf4b+NPmfGRIxtg5DtJgQAeHXqdWHVG1q/sLP74pzM2TmDVDuoIEU7PHFySR

### Operating System

ubuntu

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jiribroulik,2024-09-17 20:38:51+00:00,[],2024-09-17 20:41:11+00:00,,https://github.com/apache/airflow/issues/42282,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2356878474, 'issue_id': 2532106287, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 17, 20, 38, 53, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-17 20:38:53 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2531845694,issue,closed,completed,Status of testing of Apache Airflow 2.10.2rc1,"### Body

We are kindly requesting that contributors to [Apache Airflow RC 2.10.2rc1](https://pypi.org/project/apache-airflow/2.10.2rc1/) help test the RC.

Please let us know by commenting if the issue is addressed in the latest RC.

- [x] [ci: improve check_deferrable_default script to cover positional variables (#41942)](https://github.com/apache/airflow/pull/41942): @Lee-W
     Linked issues:
     - [ci: improve check_deferrable_default script to cover positional variables (#41924)](https://github.com/apache/airflow/pull/41924)
- [ ] [Switch to using emulation for image building (#41959) (#41962)](https://github.com/apache/airflow/pull/41962): @potiuk
- [ ] [Deprecate ``--tree`` flag for ``tasks list`` cli command (#41965)](https://github.com/apache/airflow/pull/41965): @mobuchowski @jedcunningham
     Linked issues:
     - [`get_tree_view` can consume extreme amounts of memory. (#41505)](https://github.com/apache/airflow/issues/41505)
     - [Remove `--tree` flag from `airflow tasks list` command (#41964)](https://github.com/apache/airflow/pull/41964)
- [x] [Rewrite how DAG to dataset / dataset alias are stored (#42055)](https://github.com/apache/airflow/pull/42055): @Lee-W
- [ ] [Add new type of exception to catch timeout (#42064) (#42078)](https://github.com/apache/airflow/pull/42078): @potiuk
- [x] [Autofix default deferrable with LibCST (#42089)](https://github.com/apache/airflow/pull/42089): @Lee-W
- [ ] [Exclude universal-pathlib 0.2.4 as it breaks our integration (#42… (#42101)](https://github.com/apache/airflow/pull/42101): @potiuk
- [x] [Fix details tab log url detection (#42104) (#42114)](https://github.com/apache/airflow/pull/42114): @pierrejeambrun
- [x] [ Support multiline input for Params of type string in trigger UI form (#40414) (#42139)](https://github.com/apache/airflow/pull/42139): @sc-anssi @jscheffl
     Linked issues:
     - [Support multiline input for Params of type string in trigger UI form (#40414)](https://github.com/apache/airflow/pull/40414)
- [ ] [Fix task_instance and dag_run links from list views (#42138) (#42143)](https://github.com/apache/airflow/pull/42143): @bbovenzi
- [x] [ do not camelcase xcom entries (#42182) (#42187)](https://github.com/apache/airflow/pull/42187): @jscheffl
- [x] [Add extra and renderedTemplates as keys to skip camelCasing (#42206) (#42208)](https://github.com/apache/airflow/pull/42208): @bbovenzi @jscheffl
     Linked issues:
     - [Add extra and renderedTemplates as keys to skip camelCasing (#42206)](https://github.com/apache/airflow/pull/42206)
- [x] [Fix require_confirmation_dag_change (#42063) (#42211)](https://github.com/apache/airflow/pull/42211): @pierrejeambrun
- [x] [fix: only treat null/undefined as falsy when rendering XComEntry (#42… (#42213)](https://github.com/apache/airflow/pull/42213): @pierrejeambrun
- [ ] [Revert ""Handle Example dags case when checking for missing files (#41856) (#41874)"" (#42217)](https://github.com/apache/airflow/pull/42217): @ephraimbuddy
- [ ] [Revert ""Fix: DAGs are not marked as stale if the dags folder change"" (41433) (#41829) (#41893) 41829 (#42220)](https://github.com/apache/airflow/pull/42220): @ephraimbuddy
- [ ] [update requires (#42223)](https://github.com/apache/airflow/pull/42223): @romsharon98
- [x] [allow dataset alias to add more than one dataset events (#42189) (#42247)](https://github.com/apache/airflow/pull/42247): @utkarsharma2


Thanks to all who contributed to the release (probably not a complete list!):
@ephraimbuddy @jedcunningham @romsharon98 @utkarsharma2 @Lee-W @mobuchowski @sc-anssi @jscheffl @pierrejeambrun @bbovenzi @potiuk

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",ephraimbuddy,2024-09-17 18:43:45+00:00,[],2024-09-20 23:01:53+00:00,2024-09-20 23:01:53+00:00,https://github.com/apache/airflow/issues/42279,"[('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2356793297, 'issue_id': 2531845694, 'author': 'jscheffl', 'body': 'Tested the 3 fixes I contributed or merged in the 2.10.2rc1. All looks good', 'created_at': datetime.datetime(2024, 9, 17, 20, 0, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357879774, 'issue_id': 2531845694, 'author': 'pierrejeambrun', 'body': 'Same, tested all 3 fixes, working great.', 'created_at': datetime.datetime(2024, 9, 18, 8, 52, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357941192, 'issue_id': 2531845694, 'author': 'Lee-W', 'body': 'https://github.com/apache/airflow/pull/42247\r\nhttps://github.com/apache/airflow/pull/41942\r\nhttps://github.com/apache/airflow/pull/42055\r\nhttps://github.com/apache/airflow/pull/42089\r\n\r\nThe tested PRs above work fine. Thanks!', 'created_at': datetime.datetime(2024, 9, 18, 9, 20, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2360852634, 'issue_id': 2531845694, 'author': 'jmaicher', 'body': 'Tested https://github.com/apache/airflow/pull/42220, looks good', 'created_at': datetime.datetime(2024, 9, 19, 12, 27, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2364732014, 'issue_id': 2531845694, 'author': 'ephraimbuddy', 'body': 'Airflow 2.10.2 has been released. Thank you all for testing this release', 'created_at': datetime.datetime(2024, 9, 20, 23, 1, 53, tzinfo=datetime.timezone.utc)}]","jscheffl on (2024-09-17 20:00:53 UTC): Tested the 3 fixes I contributed or merged in the 2.10.2rc1. All looks good

pierrejeambrun on (2024-09-18 08:52:18 UTC): Same, tested all 3 fixes, working great.

Lee-W on (2024-09-18 09:20:20 UTC): https://github.com/apache/airflow/pull/42247
https://github.com/apache/airflow/pull/41942
https://github.com/apache/airflow/pull/42055
https://github.com/apache/airflow/pull/42089

The tested PRs above work fine. Thanks!

jmaicher on (2024-09-19 12:27:15 UTC): Tested https://github.com/apache/airflow/pull/42220, looks good

ephraimbuddy (Issue Creator) on (2024-09-20 23:01:53 UTC): Airflow 2.10.2 has been released. Thank you all for testing this release

"
2531551322,issue,open,,remove pickle_id column (and other related stuff?),"### Body

i don't think anyone uses dag pickling, and i think it's going to need to be removed for 3.0


### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2024-09-17 16:08:24+00:00,[],2024-10-01 23:47:06+00:00,,https://github.com/apache/airflow/issues/42276,"[('area:serialization', ''), ('kind:meta', 'High-level information important to the community'), ('airflow3.0:breaking', 'Candidates for Airflow 3.0 that contain breaking changes')]","[{'comment_id': 2387275096, 'issue_id': 2531551322, 'author': 'potiuk', 'body': 'YES', 'created_at': datetime.datetime(2024, 10, 1, 23, 46, 47, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-10-01 23:46:47 UTC): YES

"
2530803917,issue,closed,completed,Getting error when `retries` is set to None at task level,"### Apache Airflow version

2.10.1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I have a Dag with configuration `retries = 10`.
One of my custom component (task) ideally should use the Dag configuration for retries. But in some cases, this value should be different. So the component had default value of `None`

When the default value is used and i try to clear task from the UI, i get an error:

```
webserver  | [2024-09-14T18:34:43.338+0000] {app.py:1744} ERROR - Exception on /clear [POST]
webserver  | Traceback (most recent call last):
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/flask/app.py"", line 2529, in wsgi_app
webserver  | response = self.full_dispatch_request()
webserver  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/flask/app.py"", line 1825, in full_dispatch_request
webserver  | rv = self.handle_user_exception(e)
webserver  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/flask/app.py"", line 1823, in full_dispatch_request
webserver  | rv = self.dispatch_request()
webserver  | ^^^^^^^^^^^^^^^^^^^^^^^
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/flask/app.py"", line 1799, in dispatch_request
webserver  | return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
webserver  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/www/auth.py"", line 250, in decorated
webserver  | return _has_access(
webserver  | ^^^^^^^^^^^^
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/www/auth.py"", line 163, in _has_access
webserver  | return func(*args, **kwargs)
webserver  | ^^^^^^^^^^^^^^^^^^^^^
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/www/decorators.py"", line 159, in wrapper
webserver  | return f(*args, **kwargs)
webserver  | ^^^^^^^^^^^^^^^^^^
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py"", line 97, in wrapper
webserver  | return func(*args, session=session, **kwargs)
webserver  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/www/views.py"", line 2467, in clear
webserver  | response = self._clear_dag_tis(
webserver  | ^^^^^^^^^^^^^^^^^^^^
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/www/views.py"", line 2343, in _clear_dag_tis
webserver  | count = dag.clear(
webserver  | ^^^^^^^^^^
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py"", line 94, in wrapper
webserver  | return func(*args, **kwargs)
webserver  | ^^^^^^^^^^^^^^^^^^^^^
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py"", line 2496, in clear
webserver  | clear_task_instances(
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 472, in clear_task_instances
webserver  | ti.max_tries = ti.try_number + task.retries
webserver  | ~~~~~~~~~~~~~~^~~~~~~~~~~~~~
webserver  | TypeError: unsupported operand type(s) for +: 'int' and 'NoneType'
webserver  | [2024-09-14T18:34:43.338+0000] {app.py:1744} ERROR - Exception on /clear [POST]
webserver  | Traceback (most recent call last):
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/flask/app.py"", line 2529, in wsgi_app
webserver  | response = self.full_dispatch_request()
webserver  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/flask/app.py"", line 1825, in full_dispatch_request
webserver  | rv = self.handle_user_exception(e)
webserver  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/flask/app.py"", line 1823, in full_dispatch_request
webserver  | rv = self.dispatch_request()
webserver  | ^^^^^^^^^^^^^^^^^^^^^^^
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/flask/app.py"", line 1799, in dispatch_request
webserver  | return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
webserver  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/www/auth.py"", line 250, in decorated
webserver  | return _has_access(
webserver  | ^^^^^^^^^^^^
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/www/auth.py"", line 163, in _has_access
webserver  | return func(*args, **kwargs)
webserver  | ^^^^^^^^^^^^^^^^^^^^^
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/www/decorators.py"", line 159, in wrapper
webserver  | return f(*args, **kwargs)
webserver  | ^^^^^^^^^^^^^^^^^^
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py"", line 97, in wrapper
webserver  | return func(*args, session=session, **kwargs)
webserver  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/www/views.py"", line 2467, in clear
webserver  | response = self._clear_dag_tis(
webserver  | ^^^^^^^^^^^^^^^^^^^^
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/www/views.py"", line 2343, in _clear_dag_tis
webserver  | count = dag.clear(
webserver  | ^^^^^^^^^^
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py"", line 94, in wrapper
webserver  | return func(*args, **kwargs)
webserver  | ^^^^^^^^^^^^^^^^^^^^^
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dag.py"", line 2496, in clear
webserver  | clear_task_instances(
webserver  | File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 472, in clear_task_instances
webserver  | ti.max_tries = ti.try_number + task.retries
webserver  | ~~~~~~~~~~~~~~^~~~~~~~~~~~~~
webserver  | TypeError: unsupported operand type(s) for +: 'int' and 'NoneType'
``` 

### What you think should happen instead?

Ideally if `None` is used at Task level, the default Dag configuration should be used and when a value is provided, it overrides the Dag config.

### How to reproduce

- Set retries at Dag configuration, and then within a task/operator, set `retries = None`.
- Run the dag
- try to clear task 
- error happens

### Operating System

MacOS Sonoma

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Locustv2,2024-09-17 10:45:58+00:00,['sonu4578'],2024-10-10 20:06:13+00:00,2024-10-10 20:06:13+00:00,https://github.com/apache/airflow/issues/42273,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:core', '')]","[{'comment_id': 2356613917, 'issue_id': 2530803917, 'author': 'jscheffl', 'body': 'Reference to Slack discussion https://apache-airflow.slack.com/archives/CCR6P6JRL/p1726478413720159\r\nThanks for posting.\r\n\r\nJust to fully understand this: Is this applying to Mapped Tasks or ""Normal"" Tasks in a DAG?\r\nDoes ""default DAG configuration"" refer to `with DAG(... default_ags={""retries"": None}...):` or which defaults are you referring to? Can you maybe post a snipped of your DAG to ensure I understand this right?\r\n\r\nNote for fixing: I assume we need to strengthen the DAG validation during parsing to ensure `retries=None` is not parseable. Seems this is not properly validated.', 'created_at': datetime.datetime(2024, 9, 17, 18, 23, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2361270363, 'issue_id': 2530803917, 'author': 'PApostol', 'body': ""I was facing the same issue, and the solution was to use `retries=0` instead of `retries=None`. Because when Airflow tries to execute `ti.max_tries = ti.try_number + task.retries`, the latter is `None`, hence the error. Now, whether that is by design or by accident, I'm not sure!"", 'created_at': datetime.datetime(2024, 9, 19, 15, 2, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2362925956, 'issue_id': 2530803917, 'author': 'sonu4578', 'body': '@jscheffl Hi Jens, could you please assign the issue to me? I will implement the required changes to improve the DAG validation during parsing and ensure that retries=None is not parseable.', 'created_at': datetime.datetime(2024, 9, 20, 6, 26, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2378503541, 'issue_id': 2530803917, 'author': 'sonu4578', 'body': 'Created a draft PR: https://github.com/apache/airflow/pull/42532\r\n\r\nI will verify the new validation checks and send the PR for review', 'created_at': datetime.datetime(2024, 9, 27, 6, 33, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2405402196, 'issue_id': 2530803917, 'author': 'ferruzzi', 'body': 'Looks like this one can be closed now?', 'created_at': datetime.datetime(2024, 10, 10, 15, 12, 47, tzinfo=datetime.timezone.utc)}]","jscheffl on (2024-09-17 18:23:30 UTC): Reference to Slack discussion https://apache-airflow.slack.com/archives/CCR6P6JRL/p1726478413720159
Thanks for posting.

Just to fully understand this: Is this applying to Mapped Tasks or ""Normal"" Tasks in a DAG?
Does ""default DAG configuration"" refer to `with DAG(... default_ags={""retries"": None}...):` or which defaults are you referring to? Can you maybe post a snipped of your DAG to ensure I understand this right?

Note for fixing: I assume we need to strengthen the DAG validation during parsing to ensure `retries=None` is not parseable. Seems this is not properly validated.

PApostol on (2024-09-19 15:02:39 UTC): I was facing the same issue, and the solution was to use `retries=0` instead of `retries=None`. Because when Airflow tries to execute `ti.max_tries = ti.try_number + task.retries`, the latter is `None`, hence the error. Now, whether that is by design or by accident, I'm not sure!

sonu4578 (Assginee) on (2024-09-20 06:26:28 UTC): @jscheffl Hi Jens, could you please assign the issue to me? I will implement the required changes to improve the DAG validation during parsing and ensure that retries=None is not parseable.

sonu4578 (Assginee) on (2024-09-27 06:33:39 UTC): Created a draft PR: https://github.com/apache/airflow/pull/42532

I will verify the new validation checks and send the PR for review

ferruzzi on (2024-10-10 15:12:47 UTC): Looks like this one can be closed now?

"
2529753015,issue,closed,completed,"OTEL metrics are setting service.name as attributes, where it should be set as resource attributes","### Apache Airflow version

2.10.1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

OTEL metrics support has metric data being emitted, however, the way it is currently instrumented has service.name set up as attributes, not resource attributes, thereby causing the service name to be interpreted incorrectly.

### What you think should happen instead?

When metric data are being emitted, it should have the service.name set as resource attributes.

### How to reproduce

Run airflow using otel metrics turned on, and observe that metric data being emitted has service.name set as attributes, not as resource attributes (turning on the debugging mode on otel collector also helps)

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",howardyoo,2024-09-17 01:12:59+00:00,[],2024-09-17 02:29:54+00:00,2024-09-17 02:29:54+00:00,https://github.com/apache/airflow/issues/42266,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('telemetry', 'Telemetry-related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2354382496, 'issue_id': 2529753015, 'author': 'howardyoo', 'body': 'closed as this is a false alarm.', 'created_at': datetime.datetime(2024, 9, 17, 2, 29, 54, tzinfo=datetime.timezone.utc)}]","howardyoo (Issue Creator) on (2024-09-17 02:29:54 UTC): closed as this is a false alarm.

"
2529476517,issue,closed,not_planned,MWAA - / in users,"Bringing this to a top level issue. As jaklan noted below, any thoughts on how we could manage users in MWAA given this limitation? I've also reached out to AWS to get their thoughts.

Thanks!

              Sorry for bumping the old issue, but maybe you have a quick answer for that - AWS [added support for REST API in MWAA](https://aws.amazon.com/blogs/big-data/introducing-amazon-mwaa-support-for-the-airflow-rest-api-and-web-server-auto-scaling/), but the problem is - usernames in MWAA start with `assumed-role/` prefix, so they contain slash.

Is there any way to use `/users` endpoints not to receive `The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.` error?

Neither `/users/assumed-role/foobar` nor `/users/assumed-role%2Ffoobar` seems to work.

_Originally posted by @jaklan in https://github.com/apache/airflow/issues/20063#issuecomment-2128914451_
            ",bob-skowron,2024-09-16 20:52:20+00:00,[],2024-09-18 08:08:18+00:00,2024-09-18 07:14:57+00:00,https://github.com/apache/airflow/issues/42262,"[('provider:amazon', 'AWS/Amazon - related issues'), ('area:API', ""Airflow's REST/HTTP API"")]","[{'comment_id': 2354010025, 'issue_id': 2529476517, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 16, 20, 52, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357685556, 'issue_id': 2529476517, 'author': 'eladkal', 'body': 'The issue is not a bug in the open source but a problem with MWAA. I notified the MWAA team about your report but I suggest you will also contact the AWS support directly they might provide more information.\r\n\r\nClosing as not open source bug.', 'created_at': datetime.datetime(2024, 9, 18, 7, 14, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357736300, 'issue_id': 2529476517, 'author': 'eladkal', 'body': 'I see MWAA already has open issue for it in thier docker image\r\nhttps://github.com/aws/amazon-mwaa-docker-images/issues/127', 'created_at': datetime.datetime(2024, 9, 18, 7, 42, 33, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-16 20:52:23 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2024-09-18 07:14:57 UTC): The issue is not a bug in the open source but a problem with MWAA. I notified the MWAA team about your report but I suggest you will also contact the AWS support directly they might provide more information.

Closing as not open source bug.

eladkal on (2024-09-18 07:42:33 UTC): I see MWAA already has open issue for it in thier docker image
https://github.com/aws/amazon-mwaa-docker-images/issues/127

"
2529075046,issue,closed,completed,remove scheduler_lock column,"### Body

This field exists on the dag table.  Pretty sure it's not used.

<img width=""708"" alt=""image"" src=""https://github.com/user-attachments/assets/a7be4f35-2bbe-4704-bdde-7d66bef78823"">

cc @phanikumv @cmarteepants 

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2024-09-16 17:42:18+00:00,['prabhusneha'],2024-10-28 15:08:56+00:00,2024-10-28 15:08:56+00:00,https://github.com/apache/airflow/issues/42258,"[('good first issue', ''), ('kind:meta', 'High-level information important to the community'), ('area:core', ''), ('area:db-migrations', 'PRs with DB migration')]","[{'comment_id': 2355267805, 'issue_id': 2529075046, 'author': 'prabhusneha', 'body': 'Can I pick up this change?', 'created_at': datetime.datetime(2024, 9, 17, 10, 38, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2355977406, 'issue_id': 2529075046, 'author': 'dstandish', 'body': 'sure', 'created_at': datetime.datetime(2024, 9, 17, 14, 16, 32, tzinfo=datetime.timezone.utc)}]","prabhusneha (Assginee) on (2024-09-17 10:38:46 UTC): Can I pick up this change?

dstandish (Issue Creator) on (2024-09-17 14:16:32 UTC): sure

"
2528274867,issue,open,,Add a listener for dataset events (not just datasets),"### Description

I'd like to add a new dataset listener that allows me to listen for new dataset events. This would be similar to the existing `on_dataset_changed` listener, but would pass the underlying `DatasetEvent` instead of the `Dataset` reference:

```
@hookspec
def on_dataset_event_created(
    event : DatasetEvent,
):
    """"""Execute when dataset a dataset event is created.""""""
```

Alternatively, we could expand the existing `on_dataset_changed` handler to add the dataset event as an argument to the listener. This would however not be backwards compatible with existing listener implementations.

### Use case/motivation

Currently Airflow provides the `on_dataset_changed` [listeners](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/listeners.html#dataset-events) as a hook for listening to changes on datasets. However, this listener only passes the dataset object to the listener and not the `DatasetEvent` corresponding to the change:

```
@hookspec
def on_dataset_changed(
    dataset: Dataset,
):
    """"""Execute when dataset change is registered.""""""
```

For a use case that I'm working on I'd also like to have access to the dataset event to determine whether that dataset event came from the API or not (which you can check on the event using `event.extra[""from_rest_api""]`.

This is not possible with the current listeners.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jrderuiter,2024-09-16 12:04:32+00:00,['jrderuiter'],2025-01-18 04:30:46+00:00,,https://github.com/apache/airflow/issues/42254,"[('kind:feature', 'Feature Requests'), ('area:datasets', 'Issues related to the datasets feature')]","[{'comment_id': 2360673117, 'issue_id': 2528274867, 'author': 'uranusjr', 'body': 'Makes sense. Would you mind submitting a PR? You should start with the DatasetManager class; see how `on_dataset_changed` is notified for an example. You also need to add the hook signature to `airflow/listeners/spec/dataset.py`.', 'created_at': datetime.datetime(2024, 9, 19, 11, 0, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2360676916, 'issue_id': 2528274867, 'author': 'uranusjr', 'body': 'One thing to note though, I’m not sure if we want to just pass in the DatasetEvent object—this is an ORM model object and should generally be considered internal. Not sure how best to do this; maybe we need a separate data class similar to how Dataset is exposed to the user instead of the internal DatasetModel.', 'created_at': datetime.datetime(2024, 9, 19, 11, 2, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2599521279, 'issue_id': 2528274867, 'author': 'SCrocky', 'body': ""Hello @uranusjr,\n\nI am highly interested in this feature as I'm working on a system for synchronizing datasets across multiple Airflow instances at my day job. \n\nBut of course, I don't wish to intrude. @jrderuiter are you still working on this? If so can I offer any assistance?"", 'created_at': datetime.datetime(2025, 1, 18, 4, 30, 45, tzinfo=datetime.timezone.utc)}]","uranusjr on (2024-09-19 11:00:11 UTC): Makes sense. Would you mind submitting a PR? You should start with the DatasetManager class; see how `on_dataset_changed` is notified for an example. You also need to add the hook signature to `airflow/listeners/spec/dataset.py`.

uranusjr on (2024-09-19 11:02:05 UTC): One thing to note though, I’m not sure if we want to just pass in the DatasetEvent object—this is an ORM model object and should generally be considered internal. Not sure how best to do this; maybe we need a separate data class similar to how Dataset is exposed to the user instead of the internal DatasetModel.

SCrocky on (2025-01-18 04:30:45 UTC): Hello @uranusjr,

I am highly interested in this feature as I'm working on a system for synchronizing datasets across multiple Airflow instances at my day job. 

But of course, I don't wish to intrude. @jrderuiter are you still working on this? If so can I offer any assistance?

"
2528026770,issue,open,,TableauOperator fails to trigger Tableau Task,"### Apache Airflow Provider(s)

tableau

### Versions of Apache Airflow Providers

apache-airflow-providers-tableau==4.5.0

### Apache Airflow version

v2.9.0

### Operating System

ubuntu

### Deployment

Docker-Compose

### Deployment details

_No response_

### What happened

TableauOperator fails with `resource=""tasks""` and `method=""run""` arguments with the following traceback:
```txt
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/tableau/operators/tableau.py"", line 118, in execute
    response = method(resource_id)
               ^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/tableauserverclient/server/endpoint/endpoint.py"", line 258, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/tableauserverclient/server/endpoint/tasks_endpoint.py"", line 73, in run
    if not task_item.id:
           ^^^^^^^^^^^^
AttributeError: 'str' object has no attribute 'id'
```


### What you think should happen instead

I believe the issue is that `Tasks.run` method [expects](https://github.com/tableau/server-client-python/blob/257cf616f35cc36b24a73e9e102886a52ead1853/tableauserverclient/server/endpoint/tasks_endpoint.py#L72) a `task_item` argument  of type `TaskItem` and `TableauOperator` [passes](https://github.com/apache/airflow/blob/a5d0a63d8784d7f4100a4770748c783261968e3c/airflow/providers/tableau/operators/tableau.py#L118) the task's id as a `str` instead. 

### How to reproduce

Execute `TableauOperator` with `resource=""tasks""` and `method=""run""` arguments

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",farrukh-t,2024-09-16 10:06:31+00:00,[],2025-01-04 03:17:06+00:00,,https://github.com/apache/airflow/issues/42248,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('provider:tableau', '')]","[{'comment_id': 2352494823, 'issue_id': 2528026770, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 16, 10, 6, 35, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-16 10:06:35 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2526795079,issue,closed,completed,Gantt screen is flickering ,"### Apache Airflow version

2.10.1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

After upgrading to AF 2.10.1 we noticed that some of our environments started to flicker on the Gantt screen.
The issue isn’t consistent across all DAGs. So far, we haven’t observed any issues on the backend side, leading us to believe that this is a UI bug.

Here is an example:

https://github.com/user-attachments/assets/0a13fbeb-5e4a-422c-b133-cd84aea1d96a




### What you think should happen instead?

_No response_

### How to reproduce

We’re not sure how to reproduce the bug since it’s not consistent. However, the issue occurs on both Chrome (version 128.0.6613.138) and Edge (version 128.0.2739.79).

The client OS is Windows 11 23H2, and AF is running in Docker Containers on Debian 12

### Operating System

Windows 11

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other 3rd-party Helm chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Shlomixg,2024-09-15 08:07:24+00:00,[],2024-09-19 18:32:34+00:00,2024-09-19 18:32:34+00:00,https://github.com/apache/airflow/issues/42243,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2351781443, 'issue_id': 2526795079, 'author': 'eladkal', 'body': 'Duplicate of https://github.com/apache/airflow/issues/42215 ?', 'created_at': datetime.datetime(2024, 9, 15, 20, 27, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2352042276, 'issue_id': 2526795079, 'author': 'Shlomixg', 'body': 'Indeed.', 'created_at': datetime.datetime(2024, 9, 16, 5, 26, 30, tzinfo=datetime.timezone.utc)}]","eladkal on (2024-09-15 20:27:32 UTC): Duplicate of https://github.com/apache/airflow/issues/42215 ?

Shlomixg (Issue Creator) on (2024-09-16 05:26:30 UTC): Indeed.

"
2526550888,issue,closed,completed,Facing lots of issues while following Contribution-quick start guidelines for M1. ,"### What do you see as an issue?

1. While running breeze ci-image build
ERROR: failed to solve: process ""/bin/bash -o pipefail -o errexit -o nounset -o nolog -c bash /scripts/docker/install_os_dependencies.sh dev"" did not complete successfully: exit code: 100
Error when building image! Image build: 3.8:linux/arm64
2. After running breeze setup autocomplete
Users/nkarve/Desktop/Personal/airflow/airflow/dev/breeze/autocomplete/breeze-complete-zsh.sh:34: command not found: compdef

Also had a few errors earlier with docker versions. 

### Solving the problem

It feels overwhelming and difficult for a new person to get started. Can we please have a dedicated macOs dev env installation guide? Or at least a youtube video from someone who have done it before. 

Much needed. Thanks!

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",nikhilkarve,2024-09-14 18:18:11+00:00,[],2024-10-01 23:35:42+00:00,2024-10-01 23:35:42+00:00,https://github.com/apache/airflow/issues/42238,"[('kind:bug', 'This is a clearly a bug'), ('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('kind:documentation', ''), ('pending-response', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2351625304, 'issue_id': 2526550888, 'author': 'gopidesupavan', 'body': 'It looks like you have problem in bash or zshrc file, try add these to those files.\r\n\r\nautoload -Uz compinit\r\ncompinit', 'created_at': datetime.datetime(2024, 9, 15, 14, 51, 31, tzinfo=datetime.timezone.utc)}]","gopidesupavan on (2024-09-15 14:51:31 UTC): It looks like you have problem in bash or zshrc file, try add these to those files.

autoload -Uz compinit
compinit

"
2525943992,issue,open,,"For some unkonw reasons, webserver gunicorn spawns orphan workers.","For some unkonw reasons, webserver gunicorn spawns orphan workers. 
I use `gevent` worker class and set `workers=4`. But 2 extra workers are spawned and they are not child process of gunicorn master but child of  init process ( no. 1 process in container).
Any one knows how to fix this issue ?

```00:00:45 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id 1e5eeb4a787934a8bd2
00:00:00  \_ /pause
00:00:00  \_ /usr/bin/dumb-init -- /entrypoint bash -c exec airflow webserver
00:01:23  |   \_ /home/airflow/.local/bin/python /home/airflow/.local/bin/airflow webserver
00:00:35  |   |   \_ gunicorn: master [airflow-webserver]
00:00:42  |   |       \_ [ready] gunicorn: worker [airflow-webserver]
00:00:20  |   |       \_ [ready] gunicorn: worker [airflow-webserver]
00:00:56  |   |       \_ [ready] gunicorn: worker [airflow-webserver]
00:00:45  |   |       \_ [ready] gunicorn: worker [airflow-webserver]
00:01:11  |   \_ [ready] gunicorn: worker [airflow-webserver]
00:00:51  |   \_ [ready] gunicorn: worker [airflow-webserver]```

_Originally posted by @bix29 in https://github.com/apache/airflow/discussions/42191_",bix29,2024-09-14 02:38:03+00:00,[],2024-09-14 02:40:22+00:00,,https://github.com/apache/airflow/issues/42232,"[('kind:bug', 'This is a clearly a bug'), ('area:webserver', 'Webserver related Issues')]","[{'comment_id': 2350795727, 'issue_id': 2525943992, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 14, 2, 38, 5, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-14 02:38:05 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2525377800,issue,closed,completed,Discuss and implement auto-merge in Github PRs,"Github allows committers who are allowed to merge to enable ""Auot Merge"". See:
- https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/incorporating-changes-from-a-pull-request/automatically-merging-a-pull-request
- https://docs.github.com/en/repositories/configuring-branches-and-merges-in-your-repository/configuring-pull-request-merges/managing-auto-merge-for-pull-requests-in-your-repository

This needs repo admin access but also can be enabled with `gh repo edit --enable-auto-merge`

BUT before enablement:
1. We need to have a PoC if it is working with our branch protection rules
2. We need to check with ASF INFRA if this is OK in regards of ICLA / Legal (Tracking who merged)
3. Have a short agreement in devlist",jscheffl,2024-09-13 18:01:08+00:00,['jscheffl'],2024-09-16 17:43:52+00:00,2024-09-16 17:43:51+00:00,https://github.com/apache/airflow/issues/42226,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('kind:feature', 'Feature Requests')]","[{'comment_id': 2350450743, 'issue_id': 2525377800, 'author': 'jscheffl', 'body': 'ASF Infra Request: https://issues.apache.org/jira/browse/INFRA-26122', 'created_at': datetime.datetime(2024, 9, 13, 22, 9, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2353534552, 'issue_id': 2525377800, 'author': 'jscheffl', 'body': '> ASF Infra Request: https://issues.apache.org/jira/browse/INFRA-26122\r\n\r\nFeedback: At this time, the ASF requires a Committer to merge a pull request for legal/provenance reasons.', 'created_at': datetime.datetime(2024, 9, 16, 17, 43, 51, tzinfo=datetime.timezone.utc)}]","jscheffl (Issue Creator) on (2024-09-13 22:09:19 UTC): ASF Infra Request: https://issues.apache.org/jira/browse/INFRA-26122

jscheffl (Issue Creator) on (2024-09-16 17:43:51 UTC): Feedback: At this time, the ASF requires a Committer to merge a pull request for legal/provenance reasons.

"
2525377415,issue,closed,completed,Implement the cherry-picker PR automation for fixes on main which need to go to v2-10-test,,jscheffl,2024-09-13 18:00:52+00:00,['gopidesupavan'],2024-11-30 13:20:44+00:00,2024-11-17 20:55:32+00:00,https://github.com/apache/airflow/issues/42225,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code')]","[{'comment_id': 2481548954, 'issue_id': 2525377415, 'author': 'gopidesupavan', 'body': 'closing this one, automation has been added now:  https://github.com/apache/airflow/pull/44102', 'created_at': datetime.datetime(2024, 11, 17, 20, 55, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2481798526, 'issue_id': 2525377415, 'author': 'potiuk', 'body': 'Hurray!', 'created_at': datetime.datetime(2024, 11, 18, 2, 26, 39, tzinfo=datetime.timezone.utc)}]","gopidesupavan (Assginee) on (2024-11-17 20:55:32 UTC): closing this one, automation has been added now:  https://github.com/apache/airflow/pull/44102

potiuk on (2024-11-18 02:26:39 UTC): Hurray!

"
2525377129,issue,open,,Secrets in Github are backed-up,"Today some secrets (which are.. secret :-D) are write-only in Github and it is not possible to get them our (besides revealing them from an Action. 
For DR purposes we should have a secondary copy (proposal: 1Password) to be able to restore if deleted by accident.",jscheffl,2024-09-13 18:00:44+00:00,[],2024-09-13 18:03:01+00:00,,https://github.com/apache/airflow/issues/42224,"[('security', 'Security issues that must be fixed')]",[],
2524845232,issue,closed,completed,Gantt chart flickering and constant rescaling,"### Apache Airflow version

2.10.1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Gantt chart is flickering due constant rescaling
""Queued at"" time is computed incorrectly +2h to start and end time of DAG
![image](https://github.com/user-attachments/assets/afd2fa39-24fc-4aa4-8bc9-1cf99bb6aac2)
![image](https://github.com/user-attachments/assets/6f861a0a-7ab8-4fd2-8939-2dbb477a32ca)


### What you think should happen instead?

I should see correct Gantt chart or at lease not flickering

### How to reproduce

We migrate from airflow 2.7 to 2.9.3(same Gantt issue) and 2.10.1

### Operating System

airflow docker release

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Helm chart

### Anything else?

We migrate from airflow 2.7 to 2.9.3(same Gantt issue) and 2.10.1

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",adamgorkaextbi,2024-09-13 13:22:29+00:00,[],2024-11-30 19:56:10+00:00,2024-11-30 19:56:10+00:00,https://github.com/apache/airflow/issues/42215,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2348953840, 'issue_id': 2524845232, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 13, 13, 22, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2352043110, 'issue_id': 2524845232, 'author': 'Shlomixg', 'body': 'Same here, occurs after upgrading from 2.8.2 to 2.10.1', 'created_at': datetime.datetime(2024, 9, 16, 5, 27, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2367421642, 'issue_id': 2524845232, 'author': 'adamgorkaextbi', 'body': 'still same issue on 2.10.2 version', 'created_at': datetime.datetime(2024, 9, 23, 7, 27, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2373889642, 'issue_id': 2524845232, 'author': 'adamgorkaextbi', 'body': ""nice video in duplicated issue:\r\nhttps://github.com/apache/airflow/issues/42243\r\n\r\nwidth of <div>'s representing bars is changing in infinite loop\r\nclass name is also change in loop, generating break point on attribute modification lead to this code:\r\n![image](https://github.com/user-attachments/assets/b71090a6-09db-4f2e-a9f7-e08d764afed7)"", 'created_at': datetime.datetime(2024, 9, 25, 12, 2, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2387594272, 'issue_id': 2524845232, 'author': 'dannyl1u', 'body': ""Hi @adamgorkaextbi, are there any specific steps to reproduce this? I'm looking into this but can't seem to reproduce the issue on my end. Thanks!"", 'created_at': datetime.datetime(2024, 10, 2, 4, 17, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2387733149, 'issue_id': 2524845232, 'author': 'adamgorkaextbi', 'body': '@dannyl1u \r\nHow to reproduce\r\nUse airflow official image and helm chart. Database backend is PostgreSQL. Airflow is using UTC time. Server is running at UTC. users work in CEST (+2h versus UTC time). Externally trigger is used to run DAGs.\r\nMigrate from airflow 2.7 (that has its own Gantt chart issues) to 2.9.3 (where flickering appear) and then 2.10.1 and then 2.10.2 \r\nwhat in my opinion lead to situation where \r\nincorrect values of Queued at (old values and new once) are generated (since airflow 2.7 Queued at has incorrect value +2h after start date, Queued at should occurs before started date)\r\nand this may lead Front End to incorrectly and constantly re-compute positions (padding and width) of Tasks on Gantt chart', 'created_at': datetime.datetime(2024, 10, 2, 6, 40, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2388151197, 'issue_id': 2524845232, 'author': 'adamgorkaextbi', 'body': 'I have checked and\r\nqueued_dttm alias Queued at in database has correct value stored (datetime with timezone)\r\nI suspect fronted end is processing or receiving queued_dttm that is used and displayed ""Queued at"" without taking into account time zone for this field this is why on our www we see +2h and why Gantt chart is getting crazy', 'created_at': datetime.datetime(2024, 10, 2, 9, 52, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2388221201, 'issue_id': 2524845232, 'author': 'adamgorkaextbi', 'body': 'task details and Gantt chart both use reacts useGridData() to get data', 'created_at': datetime.datetime(2024, 10, 2, 10, 5, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2388365297, 'issue_id': 2524845232, 'author': 'adamgorkaextbi', 'body': 'read this:\r\nhttps://github.com/apache/airflow/blob/6fa33191bf13d0c2caa33bb7593f1136a5eead45/airflow/www/static/js/types/api-generated.ts#L1521\r\nthen read this:\r\nhttps://github.com/apache/airflow/blob/6fa33191bf13d0c2caa33bb7593f1136a5eead45/airflow/api_connexion/schemas/task_instance_schema.py#L23\r\nthen read this:\r\nhttps://github.com/apache/airflow/blob/6fa33191bf13d0c2caa33bb7593f1136a5eead45/airflow/api_connexion/schemas/task_instance_schema.py#L67\r\nthen read this:\r\nhttps://github.com/marshmallow-code/marshmallow-sqlalchemy/blob/bd7d9edaac005c3aa6ac0c5e0b58a6d42619c8b6/src/marshmallow_sqlalchemy/schema.py#L191\r\nthen read this:\r\nhttps://github.com/marshmallow-code/marshmallow-sqlalchemy/blob/bd7d9edaac005c3aa6ac0c5e0b58a6d42619c8b6/src/marshmallow_sqlalchemy/schema.py#L191-L213\r\nthen read this:\r\nhttps://github.com/marshmallow-code/marshmallow/blob/6a0389b6aca79e95c923adb55f497f20e75e0d86/src/marshmallow/fields.py#L90-L92\r\nhttps://github.com/marshmallow-code/marshmallow/blob/6a0389b6aca79e95c923adb55f497f20e75e0d86/src/marshmallow/fields.py#L710-L729\r\nI suspect the bug could be in marshmallow repository\r\nSo I guess solution would be to not rename queued_dttm to queued_when (string) with auto_field(data_key=""queued_when"")\r\nstart_date and end_date are correctly converted to string while queued_dttm is not - only one difference I find out till is usage of data_key', 'created_at': datetime.datetime(2024, 10, 2, 10, 59, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2400519408, 'issue_id': 2524845232, 'author': 'dannyl1u', 'body': ""@adamgorkaextbi @Shlomixg \r\nI've tried it on my DAGs (also on 2.10.0) and the Gantt charts are fine on my end. Could you please share your DAG file? If it contains sensitive information, a sanitized or similar version would be fine, as long as it still reproduces the Gantt chart problem"", 'created_at': datetime.datetime(2024, 10, 8, 18, 14, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2402294791, 'issue_id': 2524845232, 'author': 'adamgorkaextbi', 'body': '@dannyl1u \r\nI guess you need to apply airflow migration scripts to reproduce issue. \r\n\r\nI check one more time airflow schema after migration\r\nbefore I checked only task_instance table where\r\nqueued_dttm has correct type timestamptz\r\n![image](https://github.com/user-attachments/assets/b45e1817-2342-4f35-9944-9c021b1d1db6)\r\nbut today I also check DAG_run table schema\r\n![image](https://github.com/user-attachments/assets/9df95604-f31c-4ae9-a7e5-9e2fd156519e)\r\naccording to airflow db model queued_at schould be timestamptz\r\n@dannyl1u \r\nCan you double check schema on your side?\r\n\r\nI guess we will try manually change this column type in our DB and let you know if this fix issue.\r\nStill I guess one of migration scripts will require fixing', 'created_at': datetime.datetime(2024, 10, 9, 13, 8, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2404701791, 'issue_id': 2524845232, 'author': 'adamgorkaextbi', 'body': 'If you are running airflow in + timezone EUROPE/ASIA (USA timezones are not affected with flickering, but still data has incorrect type db)\r\nSOLUTION:\r\n""""""\r\nALTER TABLE .dag_run ALTER COLUMN queued_at TYPE timestamptz USING queued_at AT TIME ZONE \'<Your timezone name>\';\r\n""""""\r\nTIME ZONE VALDIATION SELECT QUERY BEFORE ALTER TABLE:\r\n""""""\r\nSELECT id, dag_id, execution_date, state, run_id, end_date, start_date, queued_at, queued_at at TIME zone \'<Your timezone name>\' as queued_at_new \r\nFROM dag_run where start_date is not null order by start_date desc limit 100;\r\n""""""\r\n\r\nTODO:\r\nCorrect airflow db migration script with setting correct types of this columns during migration\r\nFIX UI react logic to deal with incorrect timestamp order (start_date, queued_at, end_date) or to check timestamp order and report exception instead of flickering or other unexpected behavior\r\nAdd unittests for processing incorrect data on frontend in case of Gantt Chart', 'created_at': datetime.datetime(2024, 10, 10, 10, 24, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2497632327, 'issue_id': 2524845232, 'author': 'CharlieJ15420', 'body': 'Also getting this issue on 2.10.3. Happens on seemingly random DAGs with the gantt UI timescales constantly flickering.', 'created_at': datetime.datetime(2024, 11, 25, 10, 38, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2502970664, 'issue_id': 2524845232, 'author': 'leetdavid', 'body': 'To me, this happens when I have multiple retries for tasks.', 'created_at': datetime.datetime(2024, 11, 27, 6, 5, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2504350671, 'issue_id': 2524845232, 'author': 'sokokoluhumbu', 'body': ""this work for me : in paris  ALTER TABLE dag_run ALTER COLUMN queued_at TYPE timestamptz  USING queued_at AT TIME ZONE 'Europe/Paris';"", 'created_at': datetime.datetime(2024, 11, 27, 16, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2505431237, 'issue_id': 2524845232, 'author': 'hongshaoyang', 'body': 'Airflow Version: v2.10.0 \r\nGit Version: .release:e001b88f5875cfd7e295891a0bbdbc75a3dccbfb\r\nDeployment: Official Apache Airflow Helm Cart\r\n\r\nhttps://github.com/user-attachments/assets/a71da12d-a18d-454b-b8a2-8b16fd7db9d5', 'created_at': datetime.datetime(2024, 11, 28, 7, 31, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2506136261, 'issue_id': 2524845232, 'author': 'darkag', 'body': ""Same problem on 2.10.3, but as @leetdavid said, it occurs only when there is a retried task. We're using MySQL for the database, so it doesn't seem to be just a timezone issue.\r\n\r\nFor me, it seems more related to the fact that the Gantt graph attempts to show task executions present in task_instance for a run_id, but within the period between the queued_at and end_date of the dag_run. (If I manually change queued_at to encompass all task_instance, the flickering stops.)\r\n\r\nThe minimum date for the Gantt graph shouldn't be queued_at but rather the minimum start_date of task_instance"", 'created_at': datetime.datetime(2024, 11, 28, 13, 30, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508040192, 'issue_id': 2524845232, 'author': 'darkag', 'body': 'The closed pull request above should solve the problem but since I don\'t understand what to do the validation error message...\r\n\r\nbasically this part of the airflow/www/static/js/dag/details/gantt/index.tsx file\r\n```\r\n// Reset state when the dagrun changes\r\n  useEffect(() => {\r\n    if (startDate !== dagRun?.queuedAt && startDate !== dagRun?.startDate) {\r\n      setStartDate(dagRun?.queuedAt || dagRun?.startDate);\r\n    }\r\n    if (!endDate || endDate !== dagRun?.endDate) {\r\n      // @ts-ignore\r\n      setEndDate(dagRun?.endDate ?? moment().add(1, ""s"").toString());\r\n    }\r\n  }, [\r\n    dagRun?.queuedAt,\r\n    dagRun?.startDate,\r\n    dagRun?.endDate,\r\n    startDate,\r\n    endDate,\r\n  ]);\r\n```\r\nis triggered when startDate/endDate and set dagRun?.queuedAt || dagRun?.startDate as start date but doing so it seems to trigger a redraw of the graph which call setGanttDuration for each task instance that could lead to a change of the startDate/endDate entering in an infinite loop of date changes.\r\n\r\nmy pull request tried to solve the issue by removing the startDate/endDate in the ""reset"" declaration, it works if I rebuild javascript on my local airflow instance, but doesn\'t pass the github validation process. It seems that just removing startDate/endDate make the file inconsistent and since my knowledge in reactjs is almost 0, I will let someone with a better understanding fix this error', 'created_at': datetime.datetime(2024, 11, 29, 15, 32, 34, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-13 13:22:32 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

Shlomixg on (2024-09-16 05:27:29 UTC): Same here, occurs after upgrading from 2.8.2 to 2.10.1

adamgorkaextbi (Issue Creator) on (2024-09-23 07:27:45 UTC): still same issue on 2.10.2 version

adamgorkaextbi (Issue Creator) on (2024-09-25 12:02:59 UTC): nice video in duplicated issue:
https://github.com/apache/airflow/issues/42243

width of <div>'s representing bars is changing in infinite loop
class name is also change in loop, generating break point on attribute modification lead to this code:
![image](https://github.com/user-attachments/assets/b71090a6-09db-4f2e-a9f7-e08d764afed7)

dannyl1u on (2024-10-02 04:17:12 UTC): Hi @adamgorkaextbi, are there any specific steps to reproduce this? I'm looking into this but can't seem to reproduce the issue on my end. Thanks!

adamgorkaextbi (Issue Creator) on (2024-10-02 06:40:57 UTC): @dannyl1u 
How to reproduce
Use airflow official image and helm chart. Database backend is PostgreSQL. Airflow is using UTC time. Server is running at UTC. users work in CEST (+2h versus UTC time). Externally trigger is used to run DAGs.
Migrate from airflow 2.7 (that has its own Gantt chart issues) to 2.9.3 (where flickering appear) and then 2.10.1 and then 2.10.2 
what in my opinion lead to situation where 
incorrect values of Queued at (old values and new once) are generated (since airflow 2.7 Queued at has incorrect value +2h after start date, Queued at should occurs before started date)
and this may lead Front End to incorrectly and constantly re-compute positions (padding and width) of Tasks on Gantt chart

adamgorkaextbi (Issue Creator) on (2024-10-02 09:52:02 UTC): I have checked and
queued_dttm alias Queued at in database has correct value stored (datetime with timezone)
I suspect fronted end is processing or receiving queued_dttm that is used and displayed ""Queued at"" without taking into account time zone for this field this is why on our www we see +2h and why Gantt chart is getting crazy

adamgorkaextbi (Issue Creator) on (2024-10-02 10:05:38 UTC): task details and Gantt chart both use reacts useGridData() to get data

adamgorkaextbi (Issue Creator) on (2024-10-02 10:59:44 UTC): read this:
https://github.com/apache/airflow/blob/6fa33191bf13d0c2caa33bb7593f1136a5eead45/airflow/www/static/js/types/api-generated.ts#L1521
then read this:
https://github.com/apache/airflow/blob/6fa33191bf13d0c2caa33bb7593f1136a5eead45/airflow/api_connexion/schemas/task_instance_schema.py#L23
then read this:
https://github.com/apache/airflow/blob/6fa33191bf13d0c2caa33bb7593f1136a5eead45/airflow/api_connexion/schemas/task_instance_schema.py#L67
then read this:
https://github.com/marshmallow-code/marshmallow-sqlalchemy/blob/bd7d9edaac005c3aa6ac0c5e0b58a6d42619c8b6/src/marshmallow_sqlalchemy/schema.py#L191
then read this:
https://github.com/marshmallow-code/marshmallow-sqlalchemy/blob/bd7d9edaac005c3aa6ac0c5e0b58a6d42619c8b6/src/marshmallow_sqlalchemy/schema.py#L191-L213
then read this:
https://github.com/marshmallow-code/marshmallow/blob/6a0389b6aca79e95c923adb55f497f20e75e0d86/src/marshmallow/fields.py#L90-L92
https://github.com/marshmallow-code/marshmallow/blob/6a0389b6aca79e95c923adb55f497f20e75e0d86/src/marshmallow/fields.py#L710-L729
I suspect the bug could be in marshmallow repository
So I guess solution would be to not rename queued_dttm to queued_when (string) with auto_field(data_key=""queued_when"")
start_date and end_date are correctly converted to string while queued_dttm is not - only one difference I find out till is usage of data_key

dannyl1u on (2024-10-08 18:14:01 UTC): @adamgorkaextbi @Shlomixg 
I've tried it on my DAGs (also on 2.10.0) and the Gantt charts are fine on my end. Could you please share your DAG file? If it contains sensitive information, a sanitized or similar version would be fine, as long as it still reproduces the Gantt chart problem

adamgorkaextbi (Issue Creator) on (2024-10-09 13:08:05 UTC): @dannyl1u 
I guess you need to apply airflow migration scripts to reproduce issue. 

I check one more time airflow schema after migration
before I checked only task_instance table where
queued_dttm has correct type timestamptz
![image](https://github.com/user-attachments/assets/b45e1817-2342-4f35-9944-9c021b1d1db6)
but today I also check DAG_run table schema
![image](https://github.com/user-attachments/assets/9df95604-f31c-4ae9-a7e5-9e2fd156519e)
according to airflow db model queued_at schould be timestamptz
@dannyl1u 
Can you double check schema on your side?

I guess we will try manually change this column type in our DB and let you know if this fix issue.
Still I guess one of migration scripts will require fixing

adamgorkaextbi (Issue Creator) on (2024-10-10 10:24:41 UTC): If you are running airflow in + timezone EUROPE/ASIA (USA timezones are not affected with flickering, but still data has incorrect type db)
SOLUTION:
""""""
ALTER TABLE .dag_run ALTER COLUMN queued_at TYPE timestamptz USING queued_at AT TIME ZONE '<Your timezone name>';
""""""
TIME ZONE VALDIATION SELECT QUERY BEFORE ALTER TABLE:
""""""
SELECT id, dag_id, execution_date, state, run_id, end_date, start_date, queued_at, queued_at at TIME zone '<Your timezone name>' as queued_at_new 
FROM dag_run where start_date is not null order by start_date desc limit 100;
""""""

TODO:
Correct airflow db migration script with setting correct types of this columns during migration
FIX UI react logic to deal with incorrect timestamp order (start_date, queued_at, end_date) or to check timestamp order and report exception instead of flickering or other unexpected behavior
Add unittests for processing incorrect data on frontend in case of Gantt Chart

CharlieJ15420 on (2024-11-25 10:38:46 UTC): Also getting this issue on 2.10.3. Happens on seemingly random DAGs with the gantt UI timescales constantly flickering.

leetdavid on (2024-11-27 06:05:22 UTC): To me, this happens when I have multiple retries for tasks.

sokokoluhumbu on (2024-11-27 16:51:00 UTC): this work for me : in paris  ALTER TABLE dag_run ALTER COLUMN queued_at TYPE timestamptz  USING queued_at AT TIME ZONE 'Europe/Paris';

hongshaoyang on (2024-11-28 07:31:16 UTC): Airflow Version: v2.10.0 
Git Version: .release:e001b88f5875cfd7e295891a0bbdbc75a3dccbfb
Deployment: Official Apache Airflow Helm Cart

https://github.com/user-attachments/assets/a71da12d-a18d-454b-b8a2-8b16fd7db9d5

darkag on (2024-11-28 13:30:57 UTC): Same problem on 2.10.3, but as @leetdavid said, it occurs only when there is a retried task. We're using MySQL for the database, so it doesn't seem to be just a timezone issue.

For me, it seems more related to the fact that the Gantt graph attempts to show task executions present in task_instance for a run_id, but within the period between the queued_at and end_date of the dag_run. (If I manually change queued_at to encompass all task_instance, the flickering stops.)

The minimum date for the Gantt graph shouldn't be queued_at but rather the minimum start_date of task_instance

darkag on (2024-11-29 15:32:34 UTC): The closed pull request above should solve the problem but since I don't understand what to do the validation error message...

basically this part of the airflow/www/static/js/dag/details/gantt/index.tsx file
```
// Reset state when the dagrun changes
  useEffect(() => {
    if (startDate !== dagRun?.queuedAt && startDate !== dagRun?.startDate) {
      setStartDate(dagRun?.queuedAt || dagRun?.startDate);
    }
    if (!endDate || endDate !== dagRun?.endDate) {
      // @ts-ignore
      setEndDate(dagRun?.endDate ?? moment().add(1, ""s"").toString());
    }
  }, [
    dagRun?.queuedAt,
    dagRun?.startDate,
    dagRun?.endDate,
    startDate,
    endDate,
  ]);
```
is triggered when startDate/endDate and set dagRun?.queuedAt || dagRun?.startDate as start date but doing so it seems to trigger a redraw of the graph which call setGanttDuration for each task instance that could lead to a change of the startDate/endDate entering in an infinite loop of date changes.

my pull request tried to solve the issue by removing the startDate/endDate in the ""reset"" declaration, it works if I rebuild javascript on my local airflow instance, but doesn't pass the github validation process. It seems that just removing startDate/endDate make the file inconsistent and since my knowledge in reactjs is almost 0, I will let someone with a better understanding fix this error

"
2524571563,issue,open,,"DAG-level permissions: DAG._upgrade_outdated_dag_access_control() breaks ""complex"" definitions","### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.3

### What happened?

We are trying to limit access to some DAGs and have been trying an example that comes verbatim from the documentation  and that doesn't work:
```
DAG(
    dag_id=""example_fine_grained_access"",
    start_date=pendulum.datetime(2021, 1, 1, tz=""UTC""),
    access_control={
        ""Viewer"": {""DAGs"": {""can_edit"", ""can_read"", ""can_delete""}, ""DAG Runs"": {""can_create""}},
    },
)
```
(taken from [here](https://airflow.apache.org/docs/apache-airflow-providers-fab/stable/auth-manager/access-control.html))

During debugging, `_upgrade_outdated_dag_access_control` was encountered, and more specific this little gem:
```
        updated_access_control = {}
        for role, perms in access_control.items():
            updated_access_control[role] = {new_perm_mapping.get(perm, perm) for perm in perms}
```
Which ""translates"" the permissions specified originally into 
```
{'Viewer': {'DAG Runs', 'DAGs'}}
```
and a warning.

### What you think should happen instead?

The Viewer role should get ""can_edit/read on DAG:example_fine_grained_access"" and ""can_create on DAG Run:example_fine_grained_access"".

### How to reproduce

Use the example from the documentation. 

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Blizzke,2024-09-13 11:11:59+00:00,[],2024-09-16 12:05:43+00:00,,https://github.com/apache/airflow/issues/42214,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2348778817, 'issue_id': 2524571563, 'author': 'Blizzke', 'body': 'If you disable that piece of code, things go wrong further along as well. \r\nIn `_sync_dag_view_permissions` to be precise.  \r\nWithin the loop of the specified roles there\'s a `action_names = set(action_names)` there that still converts them into `(\'DAG Runs\', \'DAGs\')`. \r\nSeems that the code has ""forgotten"" that an extra layer is possible, only allowing actions on `DAGs`, which makes the entire `access_control` rather pointless.', 'created_at': datetime.datetime(2024, 9, 13, 11, 56, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2351363170, 'issue_id': 2524571563, 'author': 'eladkal', 'body': '@joaopamaral maybe you can look into this one?', 'created_at': datetime.datetime(2024, 9, 15, 4, 40, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2352728160, 'issue_id': 2524571563, 'author': 'joaopamaral', 'body': 'Hi @Blizzke, the access control with Resource definition only works in airflow >= 2.10.x and FAB >= 1.3.x:\r\n\r\n- This access_control doesn\'t work in airflow 2.9.3:\r\n```python\r\n    access_control={\r\n        ""Viewer"": {""DAGs"": {""can_edit"", ""can_read"", ""can_delete""}, ""DAG Runs"": {""can_create""}},\r\n    },\r\n```\r\n\r\n- This is the correct access control for airflow 2.9.3 (Only DAG resource):\r\n```python\r\n    access_control={\r\n        ""Viewer"": {""can_edit"", ""can_read"", ""can_delete""},\r\n    },\r\n```', 'created_at': datetime.datetime(2024, 9, 16, 12, 5, 10, tzinfo=datetime.timezone.utc)}]","Blizzke (Issue Creator) on (2024-09-13 11:56:02 UTC): If you disable that piece of code, things go wrong further along as well. 
In `_sync_dag_view_permissions` to be precise.  
Within the loop of the specified roles there's a `action_names = set(action_names)` there that still converts them into `('DAG Runs', 'DAGs')`. 
Seems that the code has ""forgotten"" that an extra layer is possible, only allowing actions on `DAGs`, which makes the entire `access_control` rather pointless.

eladkal on (2024-09-15 04:40:57 UTC): @joaopamaral maybe you can look into this one?

joaopamaral on (2024-09-16 12:05:10 UTC): Hi @Blizzke, the access control with Resource definition only works in airflow >= 2.10.x and FAB >= 1.3.x:

- This access_control doesn't work in airflow 2.9.3:
```python
    access_control={
        ""Viewer"": {""DAGs"": {""can_edit"", ""can_read"", ""can_delete""}, ""DAG Runs"": {""can_create""}},
    },
```

- This is the correct access control for airflow 2.9.3 (Only DAG resource):
```python
    access_control={
        ""Viewer"": {""can_edit"", ""can_read"", ""can_delete""},
    },
```

"
2523989733,issue,closed,completed,Hardcoded Opentelemetry SERVICE_NAME,"### Apache Airflow version

2.10.1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When enabled the OTEL in Airflow, the OTEL_SERVICE_NAME is hardcoded to ""Airflow"".  Based on https://opentelemetry.io/docs/languages/sdk-configuration/general/ users should be able to overwrite this to any value via environment variable OTEL_SERVICE_NAME.

Image code path: /home/airflow/.local/lib/python3.12/site-packages/airflow/metrics/otel_logger.py
Bug line:  resource = Resource(attributes={SERVICE_NAME: ""Airflow""})

### What you think should happen instead?

If user specified OTEL_SERVICE_NAME via environment, airflow use should this value instead of hardcoded value which is ""Airflow""

### How to reproduce

During the installation, inject an extra environment OTEL_SERVICE_NAME and set the value to a value not equal to Airflow

### Operating System

Debian 12

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",nicolasge,2024-09-13 06:35:47+00:00,[],2024-09-24 12:37:31+00:00,2024-09-24 12:37:31+00:00,https://github.com/apache/airflow/issues/42210,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('telemetry', 'Telemetry-related issues')]","[{'comment_id': 2348142684, 'issue_id': 2523989733, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 13, 6, 35, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2351294115, 'issue_id': 2523989733, 'author': 'romsharon98', 'body': 'Apparently `otel` configuration already have  `service_name` configuration, I duplicate it to the metrics section and apply it on the service name.\r\n\r\nLet me know what you think on the [PR](https://github.com/apache/airflow/pull/42242)', 'created_at': datetime.datetime(2024, 9, 15, 2, 10, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2360834093, 'issue_id': 2523989733, 'author': 'nicolasge', 'body': 'It looks good to me.  Thanks @romsharon98', 'created_at': datetime.datetime(2024, 9, 19, 12, 17, 59, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-13 06:35:50 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

romsharon98 on (2024-09-15 02:10:58 UTC): Apparently `otel` configuration already have  `service_name` configuration, I duplicate it to the metrics section and apply it on the service name.

Let me know what you think on the [PR](https://github.com/apache/airflow/pull/42242)

nicolasge (Issue Creator) on (2024-09-19 12:17:59 UTC): It looks good to me.  Thanks @romsharon98

"
2523901904,issue,closed,completed,Logs going to cloudwatch->Splunk are not having task IDs.,"### Description

Hello everyone.

We are sending our logs to Splunk through Cloudwatch. But when logs reach Splunk, only the messages are printed and not the task_id or the dag_id anything to identify where those logs are coming from. 

### Use case/motivation

I am thinking this would make remote logging more robust and descriptive for monitoring purposes.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",nikhilkarve,2024-09-13 05:24:37+00:00,[],2024-10-04 01:56:29+00:00,2024-10-04 01:56:29+00:00,https://github.com/apache/airflow/issues/42207,"[('area:logging', ''), ('kind:feature', 'Feature Requests'), ('pending-response', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2348057142, 'issue_id': 2523901904, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 13, 5, 24, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2392228436, 'issue_id': 2523901904, 'author': 'gopidesupavan', 'body': 'It looks like issue in your cloudwatch to splunk configs might be your index config or field extractors in your splunk. \r\nDo you see the task_id/dag_ids appear in cloudwatch logs?\r\n\r\nAre you using AWS MWAA?', 'created_at': datetime.datetime(2024, 10, 3, 20, 1, 58, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-13 05:24:40 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

gopidesupavan on (2024-10-03 20:01:58 UTC): It looks like issue in your cloudwatch to splunk configs might be your index config or field extractors in your splunk. 
Do you see the task_id/dag_ids appear in cloudwatch logs?

Are you using AWS MWAA?

"
2523461803,issue,open,,Automatically detecting top-level DAG code calls to `Variable.get()`,"### Description

I'm not sure where in the Airflow project this would be most useful (maybe docs only, or as an internal lint test) but this has been useful for my team. Hopefully this sparks a discussion.

When discussing Airflow best practices, often the first mentioned is avoiding writing code that accesses external services or the Airflow meta database at the top-level of your DAG files:

https://www.astronomer.io/docs/learn/dag-best-practices#avoid-top-level-code-in-your-dag-file
https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html#top-level-python-code

The most frequently cited example is usually calls to `Variable.get()`. Often due to cascading imports, these calls can be difficult to detect visually. You also need buy in and education from every developer across an org to avoid something that is allowed in Python.

You may also be inheriting a Python project where this best practice was not heeded, littering your code with expensive top-level calls.

What if a simple unit test detected top-level calls to `Variable.get()`?

```
def test_top_level_variable_call(mocker):
    # Patch to prevent adding DAG to database
    mocker.patch(""airflow.dag_processing.processor.DagBag._sync_to_db"")

    # Patch to prevent creating logs
    mock_logger = mocker.patch(""airflow.dag_processing.processor.logging.Logger"")

    # Patch to prevent Airflow from creating DagWarnings, which would cause FK
    # violations because we aren't adding the DAGs to the database
    mocker.patch(
        ""airflow.dag_processing.processor.DagFileProcessor.update_dag_warnings""
    )

    files = list_py_file_paths(BASE_STATIC_PATH)

    all_mock_calls = []

    for file in files:
        mock_get = mocker.patch(""airflow.models.variable.Variable.get"")

        DagFileProcessor(
            dag_ids=None, log=mock_logger, dag_directory=BASE_STATIC_PATH
        ).process_file(file_path=file, callback_requests=[])

        if mock_get.mock_calls:
            all_mock_calls.extend(mock_get.mock_calls)
            print(file, mock_get.mock_calls)

    assert len(all_mock_calls) == 0
```

This has helped my team automatically detect (most) calls to `Variable.get()`, which has improved our scheduler function.

### Use case/motivation

Automate detection of most top-level DAG code calls (`Variable.get()` or other functions recommended by Airflow that access Airflow meta database)

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

I'm not a contributor (yet) but I'm not sure where this would be most useful.

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",matthewblock,2024-09-12 22:22:13+00:00,['matthewblock'],2024-11-03 09:37:39+00:00,,https://github.com/apache/airflow/issues/42205,"[('kind:feature', 'Feature Requests'), ('area:core', '')]","[{'comment_id': 2453360670, 'issue_id': 2523461803, 'author': 'gopidesupavan', 'body': '@matthewblock assigned.', 'created_at': datetime.datetime(2024, 11, 3, 9, 37, 38, tzinfo=datetime.timezone.utc)}]","gopidesupavan on (2024-11-03 09:37:38 UTC): @matthewblock assigned.

"
2523032945,issue,closed,completed,"Inform when ""canary"" build fails","When our ""canary"" build fails in main, we only see it failed in actions: ( ""scheduled"" run currently) and some PRs

We should have a way to notify the committers that main is faling and needs fixing (Slack?/ Devlist?)",potiuk,2024-09-12 18:05:28+00:00,['romsharon98'],2024-09-27 12:07:50+00:00,2024-09-27 12:07:50+00:00,https://github.com/apache/airflow/issues/42203,"[('area:CI', ""Airflow's tests and continious integration"")]",[],
2522253782,issue,open,,Airflow triggerer memory is increasing continously,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.8.3

### What happened?

We started to use deferrable operators and sensors from 2 weeks back. As part of it, we started using triggerer. The triggerer container memory is increasing steadily, even when no triggers are running.


<img width=""951"" alt=""airflow-triggerer-memory-leak-2 8 3"" src=""https://github.com/user-attachments/assets/74342e88-74a8-4703-a753-6204ae58e268"">


### What you think should happen instead?

Memory consumption of idle triggerer should be less and does not increase

### How to reproduce

Deploy triggerer in kubernetes with airflow version 2.8.3. Monitor for 24hrs

### Operating System

kubernetes

### Versions of Apache Airflow Providers

apache-airflow-providers-celery==3.6.1
apache-airflow-providers-common-io==1.4.0
apache-airflow-providers-common-sql==1.11.1
apache-airflow-providers-ftp==3.7.0
apache-airflow-providers-http==4.10.0
apache-airflow-providers-imap==3.5.0
apache-airflow-providers-smtp==1.8.0
apache-airflow-providers-sqlite==3.7.1

### Deployment

Other

### Deployment details

Airflow version: 2.8.3
Python version: 3.9.x
Database: Postgres 13

Kubernetes details:
Azure kubernetes service
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
k8s Server Version: v1.29.4

Dags are packaged as python packages and installed in the airflow base image. Airflow triggerer is deployed using our custom helm chart, which is just a modified version of official one.



### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",deepak4babu,2024-09-12 12:24:46+00:00,[],2025-02-06 01:37:06+00:00,,https://github.com/apache/airflow/issues/42195,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:Triggerer', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2453363000, 'issue_id': 2522253782, 'author': 'gopidesupavan', 'body': '@deepak4babu there are some optimizations made around this area, it will be available in coming airflow release 2.10.3. FYI #40487.', 'created_at': datetime.datetime(2024, 11, 3, 9, 46, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2470455949, 'issue_id': 2522253782, 'author': 'boris-tumbev', 'body': '> @deepak4babu there are some optimizations made around this area, it will be available in coming airflow release 2.10.3. FYI #40487.\r\n\r\nIt does not fix the problem unfortunately, in our setup the memory still get higher and higher and the triggerer eventually restarts', 'created_at': datetime.datetime(2024, 11, 12, 12, 53, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2505060314, 'issue_id': 2522253782, 'author': 'potiuk', 'body': '> > @deepak4babu there are some optimizations made around this area, it will be available in coming airflow release 2.10.3. FYI #40487.\r\n> \r\n> It does not fix the problem unfortunately, in our setup the memory still get higher and higher and the triggerer eventually restarts\r\n\r\nCan you do some memory check using https://github.com/bloomberg/memray for example and see what causes the memory leak?', 'created_at': datetime.datetime(2024, 11, 28, 0, 49, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2633714040, 'issue_id': 2522253782, 'author': 'arkadiuszbach', 'body': 'Had the same issue with Triggerer and Scheduler on AKS and apache/airflow:2.10.2-python3.9:\n\n- At first I thought it is something with Log Groomer as I didn\'t use it before, but even though it was disabled, memory usage was still growing and it was growing on main container not on the groomer itself.\n- So then I thought maybe memory leak is within airflow processes runing in the container.  I saved memory usage by all processes within container with `ps aux`, waited for two days and checked its output again - it didn\'t change\n- Then I searched the internet for how is it possible that memory usage for the container is growing even though for the processes it is not, it was clear that something is leaking in the kernel. I found that I can check `/sys/fs/cgroup/memory.stat` for memory usage statistics within container and **I found that slab memory is growing**. \n- Checked internet how I can find what is leaking in the slabs, found `slabtop` and `/proc/slabinfo` but when tried using it I got \'Permisison denied\' error, so **I started the container as root user instead of airflow and when I did the memory leak was gone**\n- When it was resolved when running as root i thought maybe that is because on root all linux capabilities are available and it manages memory differently, I tried enabling all capabilities when running with airflow user but it didnt fix the issue.\n- So to figure out what exactly is leaking I enabled sudo within the container (to still run as airflow and be able to `sudo su`):\n   ```\n    USER root\n    RUN usermod -aG sudo airflow\n    RUN echo ""airflow:airflow"" | chpasswd\n    USER airflow\n   ```\n\n- Then wrote  bash script to take dumps of /proc/slabinfo, didn\'t wait for long, but seemed that **dentry memory usage was growing**, checked the internet, chatgpt and it said that it may be growing when there are a lot of directory operations and files operations with dynamic names.  Somehow I stumbled into `strace` which can display system calls specific process is making, i.e. open file, create file, directories system calls, memory allocation, etc. So I started checking Airflow processes with strace (` strace -T -e trace=desc,open,mkdir,rmdir,unlink,rename --decode-fds=all -p {process_id})` to verify if they are making system calls related with files and directories - they didn\'t, except **liveness probe which was making a lot of file related system calls as it is started over and over again, so I disabled it in helm chart and memory leak was gone:**\n    ```\n    livenessProbe:\n      command:\n        - ""/bin/true""\n    ```\n- I went through strace output a little bit for liveness probe and messages like below got my attention:\n  ```\n\tLine 19609: mkdir(""/opt/airflow/config/__pycache__"", 0777) = -1 EACCES (Permission denied) <0.000028>\n\tLine 22139: openat(AT_FDCWD</opt/airflow>, ""/usr/local/lib/python3.9/concurrent/futures/__pycache__/process.cpython-39.pyc.140482203662256"", O_WRONLY|O_CREAT|O_EXCL|O_CLOEXEC, 0644) = -1 EACCES (Permission denied) <0.000035>\n\tLine 22155: openat(AT_FDCWD</opt/airflow>, ""/usr/local/lib/python3.9/multiprocessing/__pycache__/queues.cpython-39.pyc.140482203662768"", O_WRONLY|O_CREAT|O_EXCL|O_CLOEXEC, 0644) = -1 EACCES (Permission denied) <0.000033>\n\tLine 22622: openat(AT_FDCWD</opt/airflow>, ""/usr/local/lib/python3.9/__pycache__/sched.cpython-39.pyc.140482203590416"", O_WRONLY|O_CREAT|O_EXCL|O_CLOEXEC, 0644) = -1 EACCES (Permission denied) <0.000035>\n  ```\n  There are only few entries like this, but maybe this somehow is causing the leak as airflow user does not have access, don\'t know, files names seems dynamic. \n \n  Got an idea to do chmod -R 777 on /opt/airflow/config and /usr/local/lib/python3.9 to verify, but didn\'t want to spend more time on this as **seems like python leaves something behind when started over and over again** and the liveness probe in python always felt too heavy to me(i can see increase in CPU by 20% of one vcore and memory usage by 100mb-200mb when it starts, it takes 4-5 secs to execute),  so I rewrote it to golang instead with chatgpt.', 'created_at': datetime.datetime(2025, 2, 4, 12, 8, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2633749046, 'issue_id': 2522253782, 'author': 'potiuk', 'body': 'HEy @arkadiuszbach -> thanks A LOT for such a detailed investigation. This is a great find and I think that one is fully actionable on our side - we might want to rethink on how our liveness probe is implemented and recommended in our helm chart. You are a hero!', 'created_at': datetime.datetime(2025, 2, 4, 12, 22, 52, tzinfo=datetime.timezone.utc)}]","gopidesupavan on (2024-11-03 09:46:27 UTC): @deepak4babu there are some optimizations made around this area, it will be available in coming airflow release 2.10.3. FYI #40487.

boris-tumbev on (2024-11-12 12:53:12 UTC): It does not fix the problem unfortunately, in our setup the memory still get higher and higher and the triggerer eventually restarts

potiuk on (2024-11-28 00:49:41 UTC): Can you do some memory check using https://github.com/bloomberg/memray for example and see what causes the memory leak?

arkadiuszbach on (2025-02-04 12:08:22 UTC): Had the same issue with Triggerer and Scheduler on AKS and apache/airflow:2.10.2-python3.9:

- At first I thought it is something with Log Groomer as I didn't use it before, but even though it was disabled, memory usage was still growing and it was growing on main container not on the groomer itself.
- So then I thought maybe memory leak is within airflow processes runing in the container.  I saved memory usage by all processes within container with `ps aux`, waited for two days and checked its output again - it didn't change
- Then I searched the internet for how is it possible that memory usage for the container is growing even though for the processes it is not, it was clear that something is leaking in the kernel. I found that I can check `/sys/fs/cgroup/memory.stat` for memory usage statistics within container and **I found that slab memory is growing**. 
- Checked internet how I can find what is leaking in the slabs, found `slabtop` and `/proc/slabinfo` but when tried using it I got 'Permisison denied' error, so **I started the container as root user instead of airflow and when I did the memory leak was gone**
- When it was resolved when running as root i thought maybe that is because on root all linux capabilities are available and it manages memory differently, I tried enabling all capabilities when running with airflow user but it didnt fix the issue.
- So to figure out what exactly is leaking I enabled sudo within the container (to still run as airflow and be able to `sudo su`):
   ```
    USER root
    RUN usermod -aG sudo airflow
    RUN echo ""airflow:airflow"" | chpasswd
    USER airflow
   ```

- Then wrote  bash script to take dumps of /proc/slabinfo, didn't wait for long, but seemed that **dentry memory usage was growing**, checked the internet, chatgpt and it said that it may be growing when there are a lot of directory operations and files operations with dynamic names.  Somehow I stumbled into `strace` which can display system calls specific process is making, i.e. open file, create file, directories system calls, memory allocation, etc. So I started checking Airflow processes with strace (` strace -T -e trace=desc,open,mkdir,rmdir,unlink,rename --decode-fds=all -p {process_id})` to verify if they are making system calls related with files and directories - they didn't, except **liveness probe which was making a lot of file related system calls as it is started over and over again, so I disabled it in helm chart and memory leak was gone:**
    ```
    livenessProbe:
      command:
        - ""/bin/true""
    ```
- I went through strace output a little bit for liveness probe and messages like below got my attention:
  ```
	Line 19609: mkdir(""/opt/airflow/config/__pycache__"", 0777) = -1 EACCES (Permission denied) <0.000028>
	Line 22139: openat(AT_FDCWD</opt/airflow>, ""/usr/local/lib/python3.9/concurrent/futures/__pycache__/process.cpython-39.pyc.140482203662256"", O_WRONLY|O_CREAT|O_EXCL|O_CLOEXEC, 0644) = -1 EACCES (Permission denied) <0.000035>
	Line 22155: openat(AT_FDCWD</opt/airflow>, ""/usr/local/lib/python3.9/multiprocessing/__pycache__/queues.cpython-39.pyc.140482203662768"", O_WRONLY|O_CREAT|O_EXCL|O_CLOEXEC, 0644) = -1 EACCES (Permission denied) <0.000033>
	Line 22622: openat(AT_FDCWD</opt/airflow>, ""/usr/local/lib/python3.9/__pycache__/sched.cpython-39.pyc.140482203590416"", O_WRONLY|O_CREAT|O_EXCL|O_CLOEXEC, 0644) = -1 EACCES (Permission denied) <0.000035>
  ```
  There are only few entries like this, but maybe this somehow is causing the leak as airflow user does not have access, don't know, files names seems dynamic. 
 
  Got an idea to do chmod -R 777 on /opt/airflow/config and /usr/local/lib/python3.9 to verify, but didn't want to spend more time on this as **seems like python leaves something behind when started over and over again** and the liveness probe in python always felt too heavy to me(i can see increase in CPU by 20% of one vcore and memory usage by 100mb-200mb when it starts, it takes 4-5 secs to execute),  so I rewrote it to golang instead with chatgpt.

potiuk on (2025-02-04 12:22:52 UTC): HEy @arkadiuszbach -> thanks A LOT for such a detailed investigation. This is a great find and I think that one is fully actionable on our side - we might want to rethink on how our liveness probe is implemented and recommended in our helm chart. You are a hero!

"
2520416447,issue,open,,TaskGroupContext not removing resources properly if DatabricksWorkflowGroup raises inside  __exit__  method,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

The `__exit__` method of  the `DatabricksWorkflowTaskGroup` operator is capable of raise an exception (it literally does it If you instance an operator class that cannot be serialized into a json [here](https://github.com/apache/airflow/blob/73f7d891583b023239c73a926cf1fdc69069176b/airflow/providers/databricks/operators/databricks_workflow.py#L306-L308))

If something inside the method raises, there will not be called the `__exit__` [method of the superclass](https://github.com/apache/airflow/blob/73f7d891583b023239c73a926cf1fdc69069176b/airflow/providers/databricks/operators/databricks_workflow.py#L317), that is in charge of [poping](https://github.com/apache/airflow/blob/73f7d891583b023239c73a926cf1fdc69069176b/airflow/utils/task_group.py#L360-L361) a [class' attribute with the group executed](https://github.com/apache/airflow/blob/73f7d891583b023239c73a926cf1fdc69069176b/airflow/utils/task_group.py#L669-L677). 

Any further workflow that you instance inside that dag (or another dag) will raise an exception with the message `RuntimeError: ('Cannot mix TaskGroups from different DAGs: %s and %s')`. 

### What you think should happen instead?

I see two possible solutions, one more feasible than the other.

1. move the call to the superclass' exit method at the beginning of this method (not 100% that this will work because maybe the code inside relies on having that context attribute in it)
2. encapsulate the code in a try-finally block, that makes sure the superclass exit method is called no matter what at the end of execution.

### How to reproduce

I will add a simple unit test where you can check the issue

```python
import pytest
from airflow import DAG
from airflow.exceptions import AirflowException
from airflow.operators.python import PythonOperator
from airflow.providers.databricks.operators.databricks_workflow import DatabricksWorkflowTaskGroup
from airflow.utils.dates import days_ago
from airflow.utils.task_group import TaskGroupContext


def test_example():
    dag = DAG(
        dag_id=""test_workflows"",
        start_date=days_ago(1),
        schedule_interval=None,
    )

    workflow = DatabricksWorkflowTaskGroup(databricks_conn_id=""default"", group_id=""tasks"", dag=dag)

    with pytest.raises(AirflowException) as e_info:
        with workflow:
            # Force an exception instantiating an operator that is not supported by databricks workflows
            op = PythonOperator(python_callable=lambda x: x, task_id=""foo"")

    assert str(e_info.value) == ""Task tasks.foo does not support conversion to databricks workflow task.""

    # Here the test will fail because TaskGroupContext didn't pop the instance
    group = TaskGroupContext.pop_context_managed_task_group()
    assert group is None

```

### Operating System

MacOs Sonoma 14.6.1

### Versions of Apache Airflow Providers

apache-airflow-providers-common-compat==1.1.0
apache-airflow-providers-common-io==1.4.0
apache-airflow-providers-common-sql==1.15.0
apache-airflow-providers-databricks==6.8.0
apache-airflow-providers-fab==1.2.2
apache-airflow-providers-ftp==3.10.1
apache-airflow-providers-google==10.9.0
apache-airflow-providers-http==4.12.0
apache-airflow-providers-imap==3.6.1
apache-airflow-providers-smtp==1.7.1
apache-airflow-providers-sqlite==3.8.2


### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",fedemgp,2024-09-11 18:09:05+00:00,[],2024-09-11 18:19:02+00:00,,https://github.com/apache/airflow/issues/42164,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('area:core', ''), ('area:TaskGroup', ''), ('provider:databricks', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2344354458, 'issue_id': 2520416447, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 11, 18, 9, 8, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-11 18:09:08 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2520283217,issue,closed,completed,‎PubsubPullTrigger does not pass gcp_conn_id to underlying hook,"### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

apache-airflow==2.5.3
apache-airflow-providers-cncf-kubernetes==7.10.0
apache-airflow-providers-common-sql==1.8.1
apache-airflow-providers-ftp==3.6.1
apache-airflow-providers-google==10.12.0
apache-airflow-providers-http==4.7.0
apache-airflow-providers-imap==3.4.0
apache-airflow-providers-pagerduty==2.1.2
apache-airflow-providers-slack==8.4.0
apache-airflow-providers-sqlite==3.5.0
google-cloud-pubsub==2.18.4

### Apache Airflow version

v2.5.3+composer

### Operating System

Google Cloud Composer (unsure)

### Deployment

Google Cloud Composer

### Deployment details

_No response_

### What happened

When attempting to use the Sensor `PubSubPullSensor` in specifically deferrable mode with a non-default gcp connection, we encounter a lack of permissions despite relevant service account having proper permissions (verified via gcloud command line). 

```
airflow-triggerer , poke_interval=10.0, gcp_conn_id=cre-raw-data-ingest-prod-service-account, impersonation_chain=None> (ID 7752) fired: TriggerEvent<{'status': 'error', 'message': ""('Error pulling messages from subscription projects/xp-raw-data-ingest-staging/subscriptions/ili-subscription', PermissionDenied('User not authorized to perform this action.'))""}> 
```

With the same arguments and connection, in non-deferrable mode, we observe successful sensor operation (ability to wait, pull, and ack messages).

It appears [the underlying trigger does not pass along connection id parameters to the hook](https://github.com/apache/airflow/blob/2.9.2/airflow/providers/google/cloud/triggers/pubsub.py#L80)

In the non-deferrable pathway, we can see the conn_id and impersonation chain [passed to the hook](https://github.com/apache/airflow/blob/e3c9e4751a59e472ef1b653eb507bdeb3e7b0679/airflow/providers/google/cloud/sensors/pubsub.py#L136)

In our triggerer logs, we see related evidence of use of the default gcp conn id.
```
2024-09-11 09:59:49.344 PDT
airflow-triggerer Using connection ID 'google_cloud_default' for task execution. 
2024-09-11 09:59:49.346 PDT
airflow-triggerer Getting connection using `google.auth.default()` since no explicit credentials are provided. 
2024-09-11 09:59:49.361 PDT
airflow-triggerer Pulling max 1 messages from subscription (path) projects/xp-raw-data-ingest-staging/subscriptions/ili-subscription 
2024-09-11 09:59:49.433 PDT
airflow-triggerer Trigger <airflow.providers.google.cloud.triggers.pubsub.PubsubPullTrigger project_id=xp-raw-data-ingest-staging, subscription=ili-subscription, max_messages=1, ack_messages=True, messages_callback=def _default_message_callback( 
2024-09-11 09:59:49.434 PDT
airflow-triggerer     pulled_messages: List[ReceivedMessage], 
```


### How to reproduce

- Create a GCP Pubsub Topic + Pull Subscription
- Grant `roles/pubsub.admin` to a service account on the project or created topic
- Create dag w/ PubSubPullSensor task, with a conn_id referencing the gcp service account which was granted permissions, and toggle between deferrable/non-deferrable mode.

```python
    pull_messages = PubSubPullSensor(
        task_id=""pull_messages"",
        ack_messages=True,
        project_id=""the_project"",
        gcp_conn_id=""the_conn_id"",
        subscription=""the-subscription"",
        deferrable=True, # or False
        max_messages=1,
    )
```

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",nickmarx12345678,2024-09-11 17:11:04+00:00,[],2024-10-03 20:36:10+00:00,2024-10-03 20:36:10+00:00,https://github.com/apache/airflow/issues/42160,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2344243885, 'issue_id': 2520283217, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 11, 17, 11, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2344554445, 'issue_id': 2520283217, 'author': 'gopidesupavan', 'body': 'Yes , there is another issue working on, will check and update if this can be added along with that.', 'created_at': datetime.datetime(2024, 9, 11, 19, 36, 11, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-11 17:11:08 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

gopidesupavan on (2024-09-11 19:36:11 UTC): Yes , there is another issue working on, will check and update if this can be added along with that.

"
2520144206,issue,closed,completed,AIP-84 List DAGs endpoint new features (1/2),"### Description
Related to: https://github.com/apache/airflow/issues/42370

Create a list DAGs endpoint in fastAPI. Make sure that is feature-complete with the dags list homepage in the legacy UI.

Features:
- include last dag run
- sort by dag_display_name ✅ , next run datetime ✅ , last run datetime ✅, last run state ✅,
- filter by tags ✅, owners ✅, last run state ✅ , is paused ✅ 
- search by dag_display_name ✅ ,


### Use case/motivation

Our current REST API dags list endpoint is not sufficient. It doesn't include any run history and sorting only works for a few fields
The legacy UI uses a custom FAB view which also needs to be replaced.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bbovenzi,2024-09-11 16:01:12+00:00,['pierrejeambrun'],2024-09-27 07:19:19+00:00,2024-09-27 07:19:19+00:00,https://github.com/apache/airflow/issues/42159,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('AIP-84', 'Modern Rest API')]","[{'comment_id': 2374081557, 'issue_id': 2520144206, 'author': 'pierrejeambrun', 'body': 'Splitted in two with https://github.com/apache/airflow/issues/42467.', 'created_at': datetime.datetime(2024, 9, 25, 13, 24, 55, tzinfo=datetime.timezone.utc)}]","pierrejeambrun (Assginee) on (2024-09-25 13:24:55 UTC): Splitted in two with https://github.com/apache/airflow/issues/42467.

"
2520064938,issue,open,,waitForMigrations fails due to initContainer order,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

2.10.1

### What happened?

[I see related issues but not specifically the one I'm about to describe](https://github.com/apache/airflow/issues?q=is%3Aissue+waitformigrations).

We are using the helm chart v1.15.0 which enables extraInitContainers for the migrateDatabaseJob.

By default, the webserver, workers, triggerer, scheduler set`waitForMigrations.enabled: true` in the chart, which kinda makes sense. However, that will not work if the `wait-for-airflow-migrations` itself _depends_ on an initContainer. In our case, which expect is a semi-common one, database connections for GKE workloads connecting to private CloudSQL instances rely on the cloud-sql-proxy initContainer. Because `extraInitContainers` injects those containers _after_ the `wait-for-airflow-migrations` and because the [kubelet will run initContainers in their order](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/#detailed-behavior) regardless of a latter one [being a sidecar](https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/#sidecar-containers-and-pod-lifecycle).


### What you think should happen instead?

For this to work, we would need extraInitContainers to come first (perhaps a breaking change) or have some general control of the order including the wait container.

### How to reproduce

Set up an airflow database that requires the an init container to connect. With `waitForMigration.enabled: true`, airflow pods will be stuck in Pending.

### Operating System

ubuntu

### Versions of Apache Airflow Providers

N/A

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

k8s v1.29.0

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",dindurthy,2024-09-11 15:25:35+00:00,[],2024-09-11 15:32:58+00:00,,https://github.com/apache/airflow/issues/42158,"[('kind:bug', 'This is a clearly a bug'), ('area:helm-chart', 'Airflow Helm Chart'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2343989221, 'issue_id': 2520064938, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 11, 15, 25, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2344006327, 'issue_id': 2520064938, 'author': 'dindurthy', 'body': ""It could be that in a sync context, e.g. ArgoCD or Flux, `waitForMigrations.enabled` doesn't matter, in which case [a documentation update might be sufficient](https://airflow.apache.org/docs/helm-chart/stable/index.html#installing-the-chart)"", 'created_at': datetime.datetime(2024, 9, 11, 15, 32, 57, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-11 15:25:39 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

dindurthy (Issue Creator) on (2024-09-11 15:32:57 UTC): It could be that in a sync context, e.g. ArgoCD or Flux, `waitForMigrations.enabled` doesn't matter, in which case [a documentation update might be sufficient](https://airflow.apache.org/docs/helm-chart/stable/index.html#installing-the-chart)

"
2520042369,issue,closed,completed,UI: Dag trigger form not submitting parameters on keyboard submit,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

- Triggered DAG using ""Trigger DAG""
- Submitted ""Dag Conf Parameters"" form using the enter key, instead of the ""Trigger"" button
- DAG ran with the default parameters as defined in the DAG itself.

When doing the same process, but clicking the ""Trigger"" button, the parameters are passed through as expected.

### What you think should happen instead?

The values entered in the ""Dag Conf Parameters"" input fields should be passed to the DAG run.

### How to reproduce

1. Trigger a DAG run with parameters
2. In the ""DAG conf Parameters"" section, enter valid inputs
3. Trigger the DAG by using the enter key whilst your cursor is in one of the input fields
4. The DAG run config shows the default values as they were defined in the DAG itself.

(This is experienced using the latest version of Firefox on the latest stable MacOS)

### Operating System

Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

```
apache-airflow-providers-amazon==8.27.0
apache-airflow-providers-celery==3.7.3
apache-airflow-providers-cncf-kubernetes==8.3.4
apache-airflow-providers-common-compat==1.1.0
apache-airflow-providers-common-io==1.4.0
apache-airflow-providers-common-sql==1.15.0
apache-airflow-providers-docker==3.12.3
apache-airflow-providers-elasticsearch==5.4.2
apache-airflow-providers-fab==1.2.2
apache-airflow-providers-ftp==3.10.1
apache-airflow-providers-google==10.21.1
apache-airflow-providers-grpc==3.5.2
apache-airflow-providers-hashicorp==3.7.1
apache-airflow-providers-http==4.12.0
apache-airflow-providers-imap==3.6.1
apache-airflow-providers-microsoft-azure==10.3.0
apache-airflow-providers-mysql==5.6.3
apache-airflow-providers-odbc==4.6.3
apache-airflow-providers-openlineage==1.10.0
apache-airflow-providers-postgres==5.11.3
apache-airflow-providers-redis==3.7.1
apache-airflow-providers-sendgrid==3.5.1
apache-airflow-providers-sftp==4.10.3
apache-airflow-providers-slack==8.8.0
apache-airflow-providers-smtp==1.7.1
apache-airflow-providers-snowflake==5.6.1
apache-airflow-providers-sqlite==3.8.2
apache-airflow-providers-ssh==3.12.0```

### Deployment

Docker-Compose

### Deployment details

Python 3.12 base image

`INSTALL_MYSQL_CLIENT_TYPE=mysql`

However, was able to reproduce with a base docker-compose image.

### Anything else?

Happens every time

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",hskev,2024-09-11 15:16:32+00:00,['dannyl1u'],2024-09-26 11:29:04+00:00,2024-09-26 11:28:10+00:00,https://github.com/apache/airflow/issues/42157,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('priority:low', 'Bug with a simple workaround that would not block a release'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2343966204, 'issue_id': 2520042369, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 11, 15, 16, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2361905131, 'issue_id': 2520042369, 'author': 'jscheffl', 'body': 'In-deed this is a (small) bug. Form fields are only updating the dict in javascript when leaving the field, but if you press enter then the form is submitted w/o updating.\r\n\r\nWould you like to supply a fix?', 'created_at': datetime.datetime(2024, 9, 19, 18, 35, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2372335099, 'issue_id': 2520042369, 'author': 'dannyl1u', 'body': ""Hi @jscheffl  I'd like to raise a PR for this as my first contribution, can this be assigned to me?"", 'created_at': datetime.datetime(2024, 9, 24, 20, 36, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2372359293, 'issue_id': 2520042369, 'author': 'jscheffl', 'body': ""> Hi @jscheffl I'd like to raise a PR for this as my first contribution, can this be assigned to me?\r\n\r\nYes, cool! Looking forward for a PR! Thanks."", 'created_at': datetime.datetime(2024, 9, 24, 20, 48, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2375626529, 'issue_id': 2520042369, 'author': 'dannyl1u', 'body': 'Hi @jscheffl ! Submitted https://github.com/apache/airflow/pull/42487 for the fix', 'created_at': datetime.datetime(2024, 9, 26, 2, 30, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2376679795, 'issue_id': 2520042369, 'author': 'hskev', 'body': '@dannyl1u Appreciated!', 'created_at': datetime.datetime(2024, 9, 26, 11, 29, 2, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-11 15:16:34 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jscheffl on (2024-09-19 18:35:33 UTC): In-deed this is a (small) bug. Form fields are only updating the dict in javascript when leaving the field, but if you press enter then the form is submitted w/o updating.

Would you like to supply a fix?

dannyl1u (Assginee) on (2024-09-24 20:36:14 UTC): Hi @jscheffl  I'd like to raise a PR for this as my first contribution, can this be assigned to me?

jscheffl on (2024-09-24 20:48:34 UTC): Yes, cool! Looking forward for a PR! Thanks.

dannyl1u (Assginee) on (2024-09-26 02:30:06 UTC): Hi @jscheffl ! Submitted https://github.com/apache/airflow/pull/42487 for the fix

hskev (Issue Creator) on (2024-09-26 11:29:02 UTC): @dannyl1u Appreciated!

"
2519965208,issue,closed,completed,Airflow local settings no longer importable from dags folder,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.1

### What happened?

As of 2.10.1 Airflow no longer imports local settings from the dags folder because of changes in #41672.

### What you think should happen instead?

Either Airflow should continue to import local settings from the dags folder or this breaking change should be documented. For example [the docs for local settings](https://airflow.apache.org/docs/apache-airflow/stable/howto/set-config.html#configuring-local-settings) don't make it clear that `$AIRFLOW_HOME/dags` is _not_ in `sys.path` when this is loaded even though it is later.

### How to reproduce

Put your airflow_local_settings.py file in dags/ and see that it is loaded in 2.10.0 and not in 2.10.1.

### Operating System

Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",pdebelak,2024-09-11 14:48:09+00:00,[],2024-10-01 06:33:00+00:00,2024-10-01 06:33:00+00:00,https://github.com/apache/airflow/issues/42156,"[('good first issue', ''), ('kind:documentation', ''), ('area:core', ''), ('contributors-workshop', ""Issues that are good for first-time contributor's workshop"")]","[{'comment_id': 2344202651, 'issue_id': 2519965208, 'author': 'potiuk', 'body': ""Feel free to add documentation to explain it @pdebelak  in the place that you feel is best (click 'suggest a change on this page') and PR will be opened for you.\n\nThis was a security fix and the fact that you could add locL_settings.py to DAG folder was an accidental behaviour (it should never be possible because of our security model).\n\nI think since you are the user who used this accidental  behaviour you might be the best to describe it for people like you so that they can understand it"", 'created_at': datetime.datetime(2024, 9, 11, 16, 58, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2344207426, 'issue_id': 2519965208, 'author': 'potiuk', 'body': 'See the CVE advisory published on users@ mailing list  for details', 'created_at': datetime.datetime(2024, 9, 11, 17, 0, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2350754352, 'issue_id': 2519965208, 'author': 'jishangarg', 'body': 'I am trying to get this done.', 'created_at': datetime.datetime(2024, 9, 14, 0, 59, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2350772853, 'issue_id': 2519965208, 'author': 'jishangarg', 'body': '@potiuk can you please review it.', 'created_at': datetime.datetime(2024, 9, 14, 1, 44, 19, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-09-11 16:58:51 UTC): Feel free to add documentation to explain it @pdebelak  in the place that you feel is best (click 'suggest a change on this page') and PR will be opened for you.

This was a security fix and the fact that you could add locL_settings.py to DAG folder was an accidental behaviour (it should never be possible because of our security model).

I think since you are the user who used this accidental  behaviour you might be the best to describe it for people like you so that they can understand it

potiuk on (2024-09-11 17:00:05 UTC): See the CVE advisory published on users@ mailing list  for details

jishangarg on (2024-09-14 00:59:42 UTC): I am trying to get this done.

jishangarg on (2024-09-14 01:44:19 UTC): @potiuk can you please review it.

"
2519934157,issue,closed,completed,Airbyte API 0.52.0 breaks provider tests,"### Apache Airflow Provider(s)

airbyte

### Versions of Apache Airflow Providers

main

### Apache Airflow version

main

### Operating System

Linux

### Deployment

Virtualenv installation

### Deployment details

Breeze tests

### What happened

Tests on main as well as PRs re-testing airbyte provider fail in pytests with:

```
(...)
airflow/providers/airbyte/hooks/airbyte.py:56: in __init__
    self.airbyte_api = self.create_api_session()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <airflow.providers.airbyte.hooks.airbyte.AirbyteHook object at 0x7f1847b62060>

    def create_api_session(self) -> AirbyteAPI:
        """"""Create Airbyte API session.""""""
>       credentials = SchemeClientCredentials(
            client_id=self.conn[""client_id""],
            client_secret=self.conn[""client_secret""],
            TOKEN_URL=self.conn[""token_url""],
        )
E       TypeError: SchemeClientCredentials.__init__() got an unexpected keyword argument 'TOKEN_URL'

airflow/providers/airbyte/hooks/airbyte.py:71: TypeError
```

...if provider yaml in airflow/providers/airbyte/provider.yaml since airbyte-api 0.52.0 was released.

### What you think should happen instead

_No response_

### How to reproduce

run `breeze testing tests --test-type '[Providers[airbyte]'`

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jscheffl,2024-09-11 14:36:10+00:00,[],2024-09-11 19:04:22+00:00,2024-09-11 19:04:22+00:00,https://github.com/apache/airflow/issues/42154,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('provider:airbyte', '')]",[],
2518972575,issue,closed,completed,Airflow Hybrid Executor have issue where tasks are rescheduled but actually running,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

There is intermittent issue in hybrid executor where task is queued multiple times killing original execution and workflow runs goes into failed state.

From logs below it queues task to celery executor which is correct behaviour, but after few seconds there is log

default-orchestrator-scheduler-56cb74fc7f-ks76d-scheduler.log:[2024-09-11T07:14:18.791+0000] {kubernetes_executor.py:273} INFO - TaskInstance: <TaskInstance: 66cdfab5b3453e031689fbae.PB_VK39 66e1407de57be85bc875c6b0 [queued]> found in queued state but was not launched, rescheduling

Which seems wrong as executor for above task is CeleryExecutor but ""clear_not_launched_queued_tasks"" func was executed on kubernetes executor

`default-orchestrator-scheduler-56cb74fc7f-ks76d-scheduler.log:[2024-09-11T07:14:07.760+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: 66cdf9e9b3453e031689fa09.PB_VK39 66e13e1a9b94be0771f7ff20 [scheduled]>, <TaskInstance: 66cee622d8d3b11ea97d23ea.PB_VK39 66e13eb79b94be0771f7ff92 [scheduled]>, <TaskInstance: 66cef62d21a3cc6d397f816d.SI_Y9VG 66e1407de57be85bc875c6ac [scheduled]>, <TaskInstance: 66cdfab5b3453e031689fbae.SI_MDXZ 66e1407de57be85bc875c6b0 [scheduled]>] for executor: CeleryExecutor(parallelism=200)
default-orchestrator-scheduler-56cb74fc7f-ks76d-scheduler.log:	<TaskInstance: 66cdfab5b3453e031689fbae.PB_VK39 66e1407de57be85bc875c6b0 [scheduled]>
default-orchestrator-scheduler-56cb74fc7f-ks76d-scheduler.log:	<TaskInstance: 66cdfab5b3453e031689fbae.PB_VK39 66e1407de57be85bc875c6b0 [scheduled]>
default-orchestrator-scheduler-56cb74fc7f-ks76d-scheduler.log:[2024-09-11T07:14:15.300+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: 66cdfa3c407cee140c3ce50a.SI_XAFG 66e13f1de57be85bc875c52f [scheduled]>, <TaskInstance: 66cdfab5b3453e031689fbae.PB_VK39 66e1407de57be85bc875c6b0 [scheduled]>, <TaskInstance: 66cdfab84750a813ec928598.SI_KXFL 66e14089e57be85bc875c6bd [scheduled]>] for executor: CeleryExecutor(parallelism=200)
default-orchestrator-scheduler-56cb74fc7f-ks76d-scheduler.log:[2024-09-11T07:14:15.301+0000] {scheduler_job_runner.py:680} INFO - Sending TaskInstanceKey(dag_id='66cdfab5b3453e031689fbae', task_id='PB_VK39', run_id='66e1407de57be85bc875c6b0', try_number=1, map_index=-1) to CeleryExecutor with priority 1 and queue default
default-orchestrator-scheduler-56cb74fc7f-ks76d-scheduler.log:[2024-09-11T07:14:15.301+0000] {base_executor.py:168} INFO - Adding to queue: ['airflow', 'tasks', 'run', '66cdfab5b3453e031689fbae', 'PB_VK39', '66e1407de57be85bc875c6b0', '--local', '--subdir', 'DAGS_FOLDER/66cdfab5b3453e031689fbae.py']
default-orchestrator-scheduler-56cb74fc7f-ks76d-scheduler.log:[2024-09-11T07:14:16.921+0000] {scheduler_job_runner.py:764} INFO - Received executor event with state queued for task instance TaskInstanceKey(dag_id='66cdfab5b3453e031689fbae', task_id='PB_VK39', run_id='66e1407de57be85bc875c6b0', try_number=1, map_index=-1)
default-orchestrator-scheduler-56cb74fc7f-ks76d-scheduler.log:[2024-09-11T07:14:16.950+0000] {scheduler_job_runner.py:791} INFO - Setting external_id for <TaskInstance: 66cdfab5b3453e031689fbae.PB_VK39 66e1407de57be85bc875c6b0 [queued]> to 2f92c5d8-923d-43b7-811c-17b5986411d8
default-orchestrator-scheduler-56cb74fc7f-ks76d-scheduler.log:[2024-09-11T07:14:18.791+0000] {kubernetes_executor.py:273} INFO - TaskInstance: <TaskInstance: 66cdfab5b3453e031689fbae.PB_VK39 66e1407de57be85bc875c6b0 [queued]> found in queued state but was not launched, rescheduling
default-orchestrator-scheduler-56cb74fc7f-ks76d-scheduler.log:	<TaskInstance: 66cdfab5b3453e031689fbae.PB_VK39 66e1407de57be85bc875c6b0 [scheduled]>
default-orchestrator-scheduler-56cb74fc7f-ks76d-scheduler.log:	<TaskInstance: 66cdfab5b3453e031689fbae.PB_VK39 66e1407de57be85bc875c6b0 [scheduled]>
default-orchestrator-scheduler-56cb74fc7f-ks76d-scheduler.log:[2024-09-11T07:14:19.827+0000] {scheduler_job_runner.py:736} INFO - Trying to enqueue tasks: [<TaskInstance: 66cef56ed8d3b11ea97d269b.PB_VK39 66e13dede57be85bc875c3e1 [scheduled]>, <TaskInstance: 66cdfa3c407cee140c3ce50a.SI_XAFG 66e13f1de57be85bc875c52f [scheduled]>, <TaskInstance: 66cef631fe5e403e994538d8.PB_VK39 66e140649b94be0771f801f5 [scheduled]>, <TaskInstance: 66cefe18fe5e403e99453aee.SI_CUY5 66e1406fbbe1e2215bc5ef41 [scheduled]>, <TaskInstance: 66cdfab5b3453e031689fbae.PB_VK39 66e1407de57be85bc875c6b0 [scheduled]>] for executor: CeleryExecutor(parallelism=200)
`

### What you think should happen instead?

_No response_

### How to reproduce

I am using HybridExecutor with Celery,Kubernetes executor.

### Operating System

Linux

### Versions of Apache Airflow Providers

Celery, Kubernetes

### Deployment

Other Docker-based deployment

### Deployment details

Kubernetes setup.

### Anything else?

This task is not provided any executor so None is passed in executor value so First CeleryExecutor is used.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",iw-pavan,2024-09-11 08:23:58+00:00,[],2024-10-20 04:21:19+00:00,2024-10-20 04:21:19+00:00,https://github.com/apache/airflow/issues/42151,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('area:hybrid-executors', 'AIP-61')]","[{'comment_id': 2342988455, 'issue_id': 2518972575, 'author': 'iw-pavan', 'body': 'Seems query in kubernetes executor to check not launched task need to be updated with executor\r\n\r\nquery = select(TaskInstance).where(\r\n                TaskInstance.state == TaskInstanceState.QUEUED, TaskInstance.queued_by_job_id == self.job_id\r\n            )\r\n            if self.kubernetes_queue:\r\n                query = query.where(TaskInstance.queue == self.kubernetes_queue)', 'created_at': datetime.datetime(2024, 9, 11, 8, 26, 49, tzinfo=datetime.timezone.utc)}]","iw-pavan (Issue Creator) on (2024-09-11 08:26:49 UTC): Seems query in kubernetes executor to check not launched task need to be updated with executor

query = select(TaskInstance).where(
                TaskInstance.state == TaskInstanceState.QUEUED, TaskInstance.queued_by_job_id == self.job_id
            )
            if self.kubernetes_queue:
                query = query.where(TaskInstance.queue == self.kubernetes_queue)

"
2517873557,issue,closed,completed,Please update google cloud provider cloud functions api to support v2,"### Description

We should parameterize this api_version argument to allow users to trigger v1 or v2 functions. Right now v2 functions just not reachable because this hard coded value.  (https://github.com/apache/airflow/blob/main/airflow/providers/google/cloud/operators/functions.py#L453) 


### Use case/motivation

Able to trigger cloud functions v2 with larger machine types

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",cbio-erik,2024-09-10 21:24:12+00:00,[],2024-10-08 10:27:01+00:00,2024-09-19 17:13:49+00:00,https://github.com/apache/airflow/issues/42146,"[('provider:google', 'Google (including GCP) related issues'), ('kind:feature', 'Feature Requests'), ('pending-response', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2342036162, 'issue_id': 2517873557, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 10, 21, 24, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2360676417, 'issue_id': 2517873557, 'author': 'nathadfield', 'body': ""@cbio-erik I don't believe this is a hardcoded value but the default value.  Your should be able to override this when using the operator.\r\n\r\n```\r\ntest = CloudFunctionInvokeFunctionOperator(\r\n        task_id='my-task',\r\n        project_id='my-project',\r\n        function_id='my-function',\r\n        input_data={'key': 'value'},\r\n        api_version='v2',\r\n    )\r\n```"", 'created_at': datetime.datetime(2024, 9, 19, 11, 1, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2361737751, 'issue_id': 2517873557, 'author': 'cbio-erik', 'body': '@nathadfield Seems like the google cloud functions v2 has to different base uri. I will close this ticket since CloudRunExecuteJobOperator is an alternative operator to run cloud function v2 jobs.', 'created_at': datetime.datetime(2024, 9, 19, 17, 13, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2399464501, 'issue_id': 2517873557, 'author': 'houtanb', 'body': '@nathadfield NB that just providing `api_version=\'v2\',` does not solve the issue.\r\n\r\n@cbio-erik it\'s not clear to me how `CloudRunExecuteJobOperator` works for both v1 and v2 functions. And do we need to create jobs for each individual v2 function we want to call if we want to use `CloudRunExecuteJobOperator`? I have a lot of v1 and v2 functions that I would like to call, so I didn\'t see `CloudRunExecuteJobOperator` as an option, but perhaps I\'m missing something in my setup...\r\n\r\nIt\'s unfortunate that `CloudFunctionInvokeFunctionOperator` hasn\'t been updated for v2 functions since they\'ve been generally available since [August 10, 2022](https://cloud.google.com/blog/products/serverless/cloud-functions-2nd-generation-now-generally-available).\r\n\r\nBut we don\'t have to rely on the Google library! We can just authenticate and call the v1 or v2 function directly...\r\n```python\r\n@task\r\ndef cloud_function_task(endpoint):\r\n    """"""Call cloud function represented by endpoint.""""""\r\n    auth_req = google.auth.transport.requests.Request()\r\n    id_token = google.oauth2.id_token.fetch_id_token(auth_req, endpoint)\r\n    headers = {\r\n        ""Content-Type"": ""application/json"",\r\n        ""Authorization"": f""Bearer {id_token}"",\r\n    }\r\n\r\n    response = requests.post(\r\n        url=endpoint,\r\n        headers=headers,\r\n        data=None,\r\n    )\r\n\r\n    if response.status_code != 200:\r\n        raise Exception(f""\\nError invoking {endpoint}: {response.text}\\n"")\r\n\r\n    return response.text\r\n```\r\n\r\nYour service account will need permissions to invoke the given cloud functions.\r\n\r\nIf running locally, you\'ll need to set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to the credentials file.', 'created_at': datetime.datetime(2024, 10, 8, 10, 26, 29, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-10 21:24:15 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

nathadfield on (2024-09-19 11:01:55 UTC): @cbio-erik I don't believe this is a hardcoded value but the default value.  Your should be able to override this when using the operator.

```
test = CloudFunctionInvokeFunctionOperator(
        task_id='my-task',
        project_id='my-project',
        function_id='my-function',
        input_data={'key': 'value'},
        api_version='v2',
    )
```

cbio-erik (Issue Creator) on (2024-09-19 17:13:49 UTC): @nathadfield Seems like the google cloud functions v2 has to different base uri. I will close this ticket since CloudRunExecuteJobOperator is an alternative operator to run cloud function v2 jobs.

houtanb on (2024-10-08 10:26:29 UTC): @nathadfield NB that just providing `api_version='v2',` does not solve the issue.

@cbio-erik it's not clear to me how `CloudRunExecuteJobOperator` works for both v1 and v2 functions. And do we need to create jobs for each individual v2 function we want to call if we want to use `CloudRunExecuteJobOperator`? I have a lot of v1 and v2 functions that I would like to call, so I didn't see `CloudRunExecuteJobOperator` as an option, but perhaps I'm missing something in my setup...

It's unfortunate that `CloudFunctionInvokeFunctionOperator` hasn't been updated for v2 functions since they've been generally available since [August 10, 2022](https://cloud.google.com/blog/products/serverless/cloud-functions-2nd-generation-now-generally-available).

But we don't have to rely on the Google library! We can just authenticate and call the v1 or v2 function directly...
```python
@task
def cloud_function_task(endpoint):
    """"""Call cloud function represented by endpoint.""""""
    auth_req = google.auth.transport.requests.Request()
    id_token = google.oauth2.id_token.fetch_id_token(auth_req, endpoint)
    headers = {
        ""Content-Type"": ""application/json"",
        ""Authorization"": f""Bearer {id_token}"",
    }

    response = requests.post(
        url=endpoint,
        headers=headers,
        data=None,
    )

    if response.status_code != 200:
        raise Exception(f""\nError invoking {endpoint}: {response.text}\n"")

    return response.text
```

Your service account will need permissions to invoke the given cloud functions.

If running locally, you'll need to set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to the credentials file.

"
2517796272,issue,open,,Issue with Pod Log Retrieval in KubernetesJobOperator When Using Deferred Mode ,"### Apache Airflow Provider(s)

cncf-kubernetes

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes==8.3.4

### Apache Airflow version

2.9.2

### Operating System

Docker, creating the image from python:3.10-slim-buster

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### What happened

When using `KubernetesJobOperator` in deferred mode, the job executes correctly, but logs from the associated pod are not retrieved. 

The issue arises in the [`execute_complete`](https://github.com/apache/airflow/blob/main/airflow/providers/cncf/kubernetes/operators/job.py#L226) method, where the pod_name and pod_namespace passed from the trigger are None, which causes the following code to fail:

```
if self.get_logs:
    pod_name = event[""pod_name""]
    pod_namespace = event[""pod_namespace""]
    self.pod = self.hook.get_pod(pod_name, pod_namespace)
    if not self.pod:
        raise PodNotFoundException(""Could not find pod after resuming from deferral"")
    self._write_logs(self.pod)
```

First, I create a `V1Pod` object that I pass to the `kubernetesJobOperator` using the parameter  `full_pod_spec`. This works fine as well in the `kubernetesPodOperator`. The `kubernetesJobOperator` creates the job it creates a `V1Pod` from my [`full_pod_spec`](https://github.com/apache/airflow/blob/main/airflow/providers/cncf/kubernetes/operators/job.py#L307). Later in this [line of code](https://github.com/apache/airflow/blob/main/airflow/providers/cncf/kubernetes/operators/job.py#L169) it creates the `self.pod` object.

Because I am executing the opeartor as deferred the `self.execute_deferrable()` is executed. This method uses the `KubernetesJobTrigger` class and [passes the `pod_name` and `pod_namespace`](https://github.com/apache/airflow/blob/main/airflow/providers/cncf/kubernetes/operators/job.py#L212). Because I don't get an error, I assume that the self.pod object exist, because it can access the following pod attributes:

````
pod_name=self.pod.metadata.name,  # type: ignore[union-attr]
pod_namespace=self.pod.metadata.namespace,  # type: ignore[union-attr]
````

[`KubernetesJobTrigger`](https://github.com/apache/airflow/blob/main/airflow/providers/cncf/kubernetes/triggers/job.py#L32), yields  [`TriggerEvent`](https://github.com/apache/airflow/blob/main/airflow/providers/cncf/kubernetes/triggers/job.py#L122) object, passing the `pod_name`and `pod_namespace` like this:

````
yield TriggerEvent(
            {
                ""pod_name"": pod.metadata.name if self.get_logs else None,
                ""pod_namespace"": pod.metadata.namespace if self.get_logs else None,
  
            }
        )

````

This pod object comes from this [piece of code](https://github.com/apache/airflow/blob/main/airflow/providers/cncf/kubernetes/triggers/job.py#L105):

````
if self.get_logs or self.do_xcom_push:
pod = await self.hook.get_pod(name=self.pod_name, namespace=self.pod_namespace)
````
Next,  the method [`execute_complete`](https://github.com/apache/airflow/blob/main/airflow/providers/cncf/kubernetes/operators/job.py#L226) is executed and it receives the `pod_name`and `pod_namespace` from the event.

But I found that there these values are `None`.  I believe that the `TriggerEvent` is yielding None values.


I suspect that this is happening, getting a `None` value for the  pod  name and namespace  when the code is trying to do:

````
if self.get_logs or self.do_xcom_push:
pod = await self.hook.get_pod(name=self.pod_name, namespace=self.pod_namespace)
````

I think that the pod object may not have the name and namespace because the pod is still in the Pending state, meaning Kubernetes has accepted the pod but hasn't fully scheduled or initialized it yet, so certain metadata fields like name and namespace might not be populated.


As a workaround, I had to manually extract the pod name from the job itself, by inheriting the `kubernetesJobOperator` and creating a new  `execute_complete` method.


```
pod_list_response = self.hook.get_namespaced_pod_list(
    label_selector=f""job-name={job_name}"", namespace=job_namespace
)

pod_list = json.loads(pod_list_response.read())
pods = pod_list.get(""items"", [])
if not pods:
    raise PodNotFoundException(f""No pods found for job: {job_name} in namespace: {job_namespace}"")

pod = pods[0]
pod_name = pod[""metadata""][""name""]
pod_namespace = pod[""metadata""][""namespace""]
```

After retrieving the pod details, I encountered another issue with the client used to fetch logs. By default, [`self.client`](https://github.com/apache/airflow/blob/main/airflow/providers/cncf/kubernetes/operators/job.py#L470) in `KubernetesJobOperator` points to the `BatchV1Api`:

```
@cached_property
def client(self) -> BatchV1Api:
    return self.hook.batch_v1_client
```

This is happening in the following piece of code:

````
if self.get_logs:
            pod_name = event[""pod_name""]
            pod_namespace = event[""pod_namespace""]
            self.pod = self.hook.get_pod(pod_name, pod_namespace)
            if not self.pod:
                raise PodNotFoundException(""Could not find pod after resuming from deferral"")
            self._write_logs(self.pod)
````

[`self._write_logs(self.pod)`](https://github.com/apache/airflow/blob/main/airflow/providers/cncf/kubernetes/operators/pod.py#L832) is created in the `kubernetesPodOperator` that is creating the `self.client using   use `CoreV1Api`.   I resolved this by switching the client before calling the `_write_logs` function:

```
self.client = self.hook.core_v1_client
self._write_logs(self.pod)

```

### What you think should happen instead

The `KubernetesJobTrigger` should be able to send the `pod_name` and `pod_namespace` so that they are not `None`. This can be achieved by passing the `pod_name` and `pod_namespace` directly from the `KubernetesJobTrigger` parameters `self.pod_name` and `self.pod_namespace`.
````
TriggerEvent(
            {
                ""name"": job.metadata.name,
                ""namespace"": job.metadata.namespace,
                ""pod_name"": self.pod_name if self.get_logs else None,
                ""pod_namespace"": self.pod_namespace if self.get_logs else None,
                ""status"": ""error"" if error_message else ""success"",
                ""message"": f""Job failed with error: {error_message}""
                if error_message
                else ""Job completed successfully"",
                ""job"": job_dict,
                ""xcom_result"": xcom_result if self.do_xcom_push else None,
            }
        )

````

Also, a raise error is executed if the job is in failed state before being able to print the logs:

````

    def execute_complete(self, context: Context, event: dict, **kwargs):
        ti = context[""ti""]
        ti.xcom_push(key=""job"", value=event[""job""])
        if event[""status""] == ""error"":
            raise AirflowException(event[""message""])

        if self.get_logs:
            pod_name = event[""pod_name""]
            pod_namespace = event[""pod_namespace""]
            self.pod = self.hook.get_pod(pod_name, pod_namespace)
            if not self.pod:
                raise PodNotFoundException(""Could not find pod after resuming from deferral"")
            self._write_logs(self.pod)
````

But even if the job fails, we would still want to see the logs:


````

    def execute_complete(self, context: Context, event: dict, **kwargs):
        ti = context[""ti""]
        ti.xcom_push(key=""job"", value=event[""job""])
       

        if self.get_logs:
            pod_name = event[""pod_name""]
            pod_namespace = event[""pod_namespace""]
            self.pod = self.hook.get_pod(pod_name, pod_namespace)
            if not self.pod:
                raise PodNotFoundException(""Could not find pod after resuming from deferral"")
            self._write_logs(self.pod)

 if event[""status""] == ""error"":
            raise AirflowException(event[""message""])
````



### How to reproduce

Write a DAG with the following operator:



````
from kubernetes.client import V1Pod, V1ObjectMeta, V1PodSpec, V1Container
from airflow.providers.cncf.kubernetes.operators.job import KubernetesJobOperator

# Define the pod
pod = V1Pod(
    metadata=V1ObjectMeta(name=""example-pod""),
    spec=V1PodSpec(
        containers=[V1Container(
            name=""example-container"",
            image=""busybox"",
            command=[""/bin/sh"", ""-c"", ""echo Hello, Kubernetes! && sleep 60""]
        )],
        restart_policy=""Never""
    )
)

# Define the KubernetesJobOperator
job = KubernetesJobOperator(
    task_id=""example_k8s_job"",
    full_pod_spec=pod,
    get_logs=True,
    in_cluster=True
)
````

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jordi-crespo,2024-09-10 20:57:42+00:00,[],2024-09-17 23:51:39+00:00,,https://github.com/apache/airflow/issues/42144,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]","[{'comment_id': 2357170155, 'issue_id': 2517796272, 'author': 'gopidesupavan', 'body': 'Not been able to reproduce, in my case its working fine without issues , could you please share bit more details or can you check on the latest version of provider?\r\n\r\n<img width=""1345"" alt=""image"" src=""https://github.com/user-attachments/assets/1840e367-f035-4d4c-9be7-82c6f0f442da"">', 'created_at': datetime.datetime(2024, 9, 17, 23, 20, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357180910, 'issue_id': 2517796272, 'author': 'gopidesupavan', 'body': 'Are you referring to pod waiting states? meaning its still not available to do further steps?', 'created_at': datetime.datetime(2024, 9, 17, 23, 23, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357198992, 'issue_id': 2517796272, 'author': 'jordi-crespo', 'body': 'Hello, the Pod created by the job will be scheduled in a nodepool that needs to be autoescaled.\n\nBecause of this, the pod will be in a pending state until the node is available to execute pods.\n\nIt is just a hunch that this can be the cause of the problem. I am not entertained sure how the trigger function works.\n\nBut I think that using the pod name and pod namespace from the class attribute will solve the problem.\n\nThis is how works in the kubernetesPodOperator', 'created_at': datetime.datetime(2024, 9, 17, 23, 30, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357216935, 'issue_id': 2517796272, 'author': 'gopidesupavan', 'body': 'ok, do you have debug logs ? there are logs which tells what response got from when the pod creation api called. \r\n\r\nif you could look logs this pattern: `Pod Creation Request:` `Pod Creation Response:` and possible share?  that will help us to understand whats happening really there, when these creation call made.', 'created_at': datetime.datetime(2024, 9, 17, 23, 51, 38, tzinfo=datetime.timezone.utc)}]","gopidesupavan on (2024-09-17 23:20:42 UTC): Not been able to reproduce, in my case its working fine without issues , could you please share bit more details or can you check on the latest version of provider?

<img width=""1345"" alt=""image"" src=""https://github.com/user-attachments/assets/1840e367-f035-4d4c-9be7-82c6f0f442da"">

gopidesupavan on (2024-09-17 23:23:39 UTC): Are you referring to pod waiting states? meaning its still not available to do further steps?

jordi-crespo (Issue Creator) on (2024-09-17 23:30:41 UTC): Hello, the Pod created by the job will be scheduled in a nodepool that needs to be autoescaled.

Because of this, the pod will be in a pending state until the node is available to execute pods.

It is just a hunch that this can be the cause of the problem. I am not entertained sure how the trigger function works.

But I think that using the pod name and pod namespace from the class attribute will solve the problem.

This is how works in the kubernetesPodOperator

gopidesupavan on (2024-09-17 23:51:38 UTC): ok, do you have debug logs ? there are logs which tells what response got from when the pod creation api called. 

if you could look logs this pattern: `Pod Creation Request:` `Pod Creation Response:` and possible share?  that will help us to understand whats happening really there, when these creation call made.

"
2516759806,issue,open,,Task fails and cannot read logs. Invalid URL 'http://:8793/log/...': No host supplied,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.1

### What happened?

I'm having an issue with an airflow instance where a task fails and I cannot read the logs.

Logs:

```bash
*** Could not read served logs: Invalid URL 'http://:8793/log/dag_id=my_dag/run_id=dynamic__apple_3_my_dag_cb353081__2024-09-09T14:41:22.596199__f73c5571719e4f35bf195ded40e5e25b/task_id=cleanup_temporary_directory/attempt=1.log': No host supplied

```

Event logs:

```bash
Executor CeleryExecutor(parallelism=128) reported that the task instance <TaskInstance: my_dag.cleanup_temporary_directory dynamic__apple_3_my_dag_cb353081__2024-09-09T14:41:22.596199__f73c5571719e4f35bf195ded40e5e25b [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally
```

**Additionally I checked the logs directory for the dag_id/run_id and it's missing the respective task_id folder.**

### What you think should happen instead?

I should be able to access the logs.

### How to reproduce

Not sure how to.

### Operating System

Ubuntu 24.04 LTS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other Docker-based deployment

### Deployment details

Deployed with docker-compose on **Docker Swarm** setup on 2 VMs.

### Anything else?

Additionally I checked the logs directory for the `dag_id/run_id` and it's missing the respective `task_id` folder.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",pedro-cf,2024-09-10 15:16:48+00:00,[],2025-01-09 12:43:38+00:00,,https://github.com/apache/airflow/issues/42136,"[('kind:bug', 'This is a clearly a bug'), ('area:logging', ''), ('area:core', ''), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2341934814, 'issue_id': 2516759806, 'author': 'andrew-stein-sp', 'body': 'having the same issue with 2.10.1 in k8s, using the CeleryKubernetesExecutor. \r\n\r\nCould this be related to the inheritance issue that was discussed in https://github.com/apache/airflow/issues/41891?', 'created_at': datetime.datetime(2024, 9, 10, 20, 17, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2341954829, 'issue_id': 2516759806, 'author': 'pedro-cf', 'body': ""Additionally I checked the logs directory for the dag_id/run_id and it's missing the respective task_id folder."", 'created_at': datetime.datetime(2024, 9, 10, 20, 29, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2351955948, 'issue_id': 2516759806, 'author': 'adriens', 'body': 'Having the same issue on `2.10.0` through a `podman-compose`', 'created_at': datetime.datetime(2024, 9, 16, 3, 17, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2351987116, 'issue_id': 2516759806, 'author': 'adriens', 'body': 'We have upgraded on `2.10.1` like @andrew-stein-sp and we could reproduce the same behavior', 'created_at': datetime.datetime(2024, 9, 16, 4, 8, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2355432340, 'issue_id': 2516759806, 'author': 'sosystems-dev', 'body': 'got the same behavior since upgrading from version 2.9.3 to 2.10.1. \r\nWe are using LocalExecutor', 'created_at': datetime.datetime(2024, 9, 17, 11, 31, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357283522, 'issue_id': 2516759806, 'author': 'mn7k', 'body': 'I have the same issue with 2.10.0, using the CeleryExecutor. \r\nIt worked before I upgrading from version 2.9.0 to 2.10.0.\r\n```\r\n*** Could not read served logs: Invalid URL \'http://:8793/log/dag_id=service_stop/run_id=manual__2024-09-18T09:42:54+09:00/task_id=make_accountlist_task/attempt=1.log\': No host supplied\r\n```\r\n\r\neventlog\r\n```\r\nExecutor CeleryExecutor(parallelism=6) reported that the task instance <TaskInstance: service_stop.make_accountlist_task manual__2024-09-18T09:42:54+09:00 [queued]> finished with state failed, but the task instance\'s state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally\r\n```\r\n\r\nScheduler has a error log at the same hour as eventlog.\r\n```\r\n[2024-09-18T00:43:18.036+0000] {celery_executor.py:291} ERROR - Error sending Celery task: module \'redis\' has no attribute \'client\'\r\nCelery Task ID: TaskInstanceKey(dag_id=\'service_stop\', task_id=\'make_accountlist_task\', run_id=\'manual__2024-09-18T09:42:54+09:00\', try_number=1, map_index=-1)\r\nTraceback (most recent call last):\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/celery/executors/celery_executor_utils.py"", line 220, in send_task_to_executor\r\n    result = task_to_run.apply_async(args=[command], queue=queue)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/celery/app/task.py"", line 594, in apply_async\r\n    return app.send_task(\r\n           ^^^^^^^^^^^^^^\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/celery/app/base.py"", line 797, in send_task\r\n    with self.producer_or_acquire(producer) as P:\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/celery/app/base.py"", line 932, in producer_or_acquire\r\n    producer, self.producer_pool.acquire, block=True,\r\n              ^^^^^^^^^^^^^^^^^^\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/celery/app/base.py"", line 1354, in producer_pool\r\n    return self.amqp.producer_pool\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/celery/app/amqp.py"", line 591, in producer_pool\r\n    self.app.connection_for_write()]\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/celery/app/base.py"", line 829, in connection_for_write\r\n    return self._connection(url or self.conf.broker_write_url, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/celery/app/base.py"", line 880, in _connection\r\n    return self.amqp.Connection(\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/connection.py"", line 201, in __init__\r\n    if not get_transport_cls(transport).can_parse_url:\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/transport/__init__.py"", line 91, in get_transport_cls\r\n    _transport_cache[transport] = resolve_transport(transport)\r\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/transport/__init__.py"", line 76, in resolve_transport\r\n    return symbol_by_name(transport)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/utils/imports.py"", line 59, in symbol_by_name\r\n    module = imp(module_name, package=package, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.12/importlib/__init__.py"", line 90, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""<frozen importlib._bootstrap>"", line 1387, in _gcd_import\r\n  File ""<frozen importlib._bootstrap>"", line 1360, in _find_and_load\r\n  File ""<frozen importlib._bootstrap>"", line 1331, in _find_and_load_unlocked\r\n  File ""<frozen importlib._bootstrap>"", line 935, in _load_unlocked\r\n  File ""<frozen importlib._bootstrap_external>"", line 995, in exec_module\r\n  File ""<frozen importlib._bootstrap>"", line 488, in _call_with_frames_removed\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/transport/redis.py"", line 282, in <module>\r\n    class PrefixedRedisPipeline(GlobalKeyPrefixMixin, redis.client.Pipeline):\r\n                                                      ^^^^^^^^^^^^\r\nAttributeError: module \'redis\' has no attribute \'client\'\r\n```', 'created_at': datetime.datetime(2024, 9, 18, 1, 4, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2381619960, 'issue_id': 2516759806, 'author': 'damiah', 'body': 'same issue for us when upgrading to 2.10.2', 'created_at': datetime.datetime(2024, 9, 29, 21, 39, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2390666838, 'issue_id': 2516759806, 'author': 'nikithapk', 'body': 'We’re encountering the same issue as well.', 'created_at': datetime.datetime(2024, 10, 3, 6, 59, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2390715493, 'issue_id': 2516759806, 'author': 'adriens', 'body': 'We have switched from Bitnami docker-compose to official Apache docker-compose and we could make it run successfuly :star_struck:', 'created_at': datetime.datetime(2024, 10, 3, 7, 29, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2396843558, 'issue_id': 2516759806, 'author': 'Dzhalolov', 'body': 'try to check that dags exist in worker, schedule and webserver. I deploy Airflow in K8S and get this error when putting my dags into scheduler(expecting that in will replicate into another pods), but when I check dags folder in worker it was empty', 'created_at': datetime.datetime(2024, 10, 7, 12, 51, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2412990045, 'issue_id': 2516759806, 'author': 'mn7k', 'body': 'At this time (https://github.com/apache/airflow/issues/42136#issuecomment-2357283522), I used the `airflow db upgrade` command , but I realized it has been deprecated.\r\nI retried the upgrade using the `airflow db migrate -n ""2.10.2""` command, and it works for me now.\r\n\r\nhttps://airflow.apache.org/docs/apache-airflow/2.10.0/installation/upgrading.html#offline-sql-migration-scripts', 'created_at': datetime.datetime(2024, 10, 15, 6, 18, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2419234122, 'issue_id': 2516759806, 'author': 'quack39', 'body': 'We encountered the same problem in Airflow 2.9.3\r\nHere are the Worker logs at the time of the error:\r\n\r\n```\r\n[2024-10-11 10:45:38,544: WARNING/ForkPoolWorker-16] Failed operation _store_result.  Retrying 2 more times.\r\nTraceback (most recent call last):\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context\r\n    self.dialect.do_execute(\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py"", line 736, in do_execute\r\n    cursor.execute(statement, parameters)\r\npsycopg2.OperationalError: could not receive data from server: Connection timed out\r\n\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/celery/backends/database/__init__.py"", line 47, in _inner\r\n    return fun(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/celery/backends/database/__init__.py"", line 117, in _store_result\r\n    task = list(session.query(self.task_cls).filter(self.task_cls.task_id == task_id))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py"", line 2901, in __iter__\r\n    result = self._iter()\r\n             ^^^^^^^^^^^^\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py"", line 2916, in _iter\r\n    result = self.session.execute(\r\n             ^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py"", line 1717, in execute\r\n    result = conn._execute_20(statement, params or {}, execution_options)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1710, in _execute_20\r\n    return meth(self, args_10style, kwargs_10style, execution_options)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py"", line 334, in _execute_on_connection\r\n    return connection._execute_clauseelement(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1577, in _execute_clauseelement\r\n    ret = self._execute_context(\r\n          ^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1953, in _execute_context\r\n    self._handle_dbapi_exception(\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 2134, in _handle_dbapi_exception\r\n    util.raise_(\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py"", line 211, in raise_\r\n    raise exception\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context\r\n    self.dialect.do_execute(\r\n  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py"", line 736, in do_execute\r\n    cursor.execute(statement, parameters)\r\nsqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not receive data from server: Connection timed out\r\n\r\n[SQL: SELECT celery_taskmeta.id AS celery_taskmeta_id, celery_taskmeta.task_id AS celery_taskmeta_task_id, celery_taskmeta.status AS celery_taskmeta_status, celery_taskmeta.result AS celery_taskmeta_result, celery_taskmeta.date_done AS celery_taskmeta_date_done, celery_taskmeta.traceback AS celery_taskmeta_traceback \r\nFROM celery_taskmeta \r\nWHERE celery_taskmeta.task_id = %(task_id_1)s]\r\n[parameters: {\'task_id_1\': \'5d1bef21-fbf4-4feb-9f2c-a54c95b4d738\'}]\r\n(Background on this error at: https://sqlalche.me/e/14/e3q8)\r\n```\r\n\r\nI can also note that increasing the sql_alchemy_pool_size parameter to 50 reduced the number of such errors, but did not eliminate them completely.', 'created_at': datetime.datetime(2024, 10, 17, 11, 10, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2428792237, 'issue_id': 2516759806, 'author': 'ali-naderi', 'body': 'The same issue in Airflow 2.10.2', 'created_at': datetime.datetime(2024, 10, 22, 9, 39, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2442049335, 'issue_id': 2516759806, 'author': 'Dev-iL', 'body': '**TL;DR** look for invalid python scripts on the malfunctioning worker. Try creating a DagBag on the worker and see what happens.\r\n```python\r\n# Ensure the AIRFLOW_HOME points to the right location, then run on the worker\r\n>>> from airflow.models import DagBag\r\n>>> DagBag(include_examples=False)\r\n```\r\n------\r\n\r\n\r\nI had this issue too, turns out I edited one of the files through vim, pasted some code, and it pasted tabs instead of spaces, so the file became an invalid python script due to `TabError: inconsistent use of tabs and spaces in indentation`. After I fixed that, it all went back to normal. \r\n\r\nNote that the problematic file doesn\'t have to be imported by the failing DAG/task. If I understand the issue correctly, a DagBag cannot be created if one of the DAG definition files or their imports isn\'t a valid python file. Then the issue manifests as DAGs supposedly not being found. In my case, the filesystem isn\'t shared between the scheduler and the malfunctioning celery worker, and the affected file was unmodified on the scheduler (or modified in a correct way) - so no ""big red import error"" was displayed in the webserver UI.', 'created_at': datetime.datetime(2024, 10, 28, 16, 21, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2459052917, 'issue_id': 2516759806, 'author': 'abhijit-sarkar-ext', 'body': 'Hi @quack39 and all,\r\nI am getting same error. I have deployed Airflow [2.9.3] in AKS. But when executing the DAGS getting below error. Don\'t getting any clue what needs to be updated. I am using Helm [1.15.0] for deployment and using ""KubernetesExecuter""\r\n\r\n**Error**\r\n\r\n\r\nCould not read served logs: HTTPConnectionPool(host=\'test-dag-config-nlp8suol\', port=8793): Max retries exceeded with url: /log/dag_id=test_dag/run_id=manual__2024-11-06T06:43:18.256272+00:00/task_id=config/attempt=1.log (Caused by NewConnectionError(\'<urllib3.connection.HTTPConnection object at 0x7f8ef9eaecd0>: Failed to establish a new connection: [Errno -2] Name or service not known\'))', 'created_at': datetime.datetime(2024, 11, 6, 8, 59, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466006475, 'issue_id': 2516759806, 'author': 'darenpang', 'body': ""I had the same problem when changing sequentialexecutor to localexecutor.\r\nAfter some test , I find I must make parallelism equal to CPU core number .\r\n\r\nwith t3.large(CPU core :2)\r\n parallelism =32 (default)  NG\r\n parallelism =4  most of the tasks are NG , but some OK \r\n parallelism =2  all OK\r\nwith t3.xlarge(CPU core :4)\r\n parallelism =4 all OK\r\n\r\nBut is this the expected action ? I'm not sure ."", 'created_at': datetime.datetime(2024, 11, 9, 2, 53, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469716580, 'issue_id': 2516759806, 'author': 'liangpengfei', 'body': 'same issue for us when upgrading to 2.10.3，using k8s.', 'created_at': datetime.datetime(2024, 11, 12, 6, 36, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2475831401, 'issue_id': 2516759806, 'author': 'huang06', 'body': 'The same issue in Airflow 2.10.2 with KubernetesExecutor.', 'created_at': datetime.datetime(2024, 11, 14, 9, 28, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2477735027, 'issue_id': 2516759806, 'author': 'kate-rodgers', 'body': 'I was just able to get the following to work:\r\n```\r\ntask = BashOperator(\r\ntask_id=""bash_command"",\r\nbash_command=bash_command,\r\nretries=2,\r\nretry_delay=timedelta(minutes=1),\r\ndo_xcom_push=False,\r\nenv={\r\n\'PYTHONUNBUFFERED\': \'1\',\r\n\'PYTHONFAULTHANDLER\': \'1\', # Helps debug crashes\r\n\'FORCE_COLOR\': \'1\' # Preserves color output in logs\r\n},\r\ncwd=\'/tmp\',\r\nappend_env=True\r\n)\r\n```\r\n\r\ninitially it failed with the same error, then succeeded on retry, I believe because the log stream was still in \'create\' and not available for the first attempt.', 'created_at': datetime.datetime(2024, 11, 15, 0, 58, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2480459367, 'issue_id': 2516759806, 'author': 'hditano', 'body': 'Same issue here, even if the task is successful', 'created_at': datetime.datetime(2024, 11, 16, 7, 12, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2493704089, 'issue_id': 2516759806, 'author': 'babaymaster', 'body': 'We have same problem. Please help )\r\n2.10.3', 'created_at': datetime.datetime(2024, 11, 22, 12, 55, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2493716756, 'issue_id': 2516759806, 'author': 'jliebers', 'body': ""I've ran accidentally into this issue when I defined a custom volume mount in my `docker-compose.yml`. At first, I defined the mount as part of the `airflow-worker` service, which apparently overrode the volume mounts imported from `x-airflow-common`. This then lead to exactly this issue eventually. I resolved this issue for me, by defining my custom mount in `x-airflow-common` instead. That is how I ended up in this issue; there might of course be different, completely unrelated ways. But if you've encountered this and worked with custom mounts in a docker-environment, double check your `docker-compose.yml` and the imports done by `<<: *` operator. Just a quick heads-up in the hope that it helps somebody."", 'created_at': datetime.datetime(2024, 11, 22, 13, 2, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2506920084, 'issue_id': 2516759806, 'author': 'opeida', 'body': ""The same issue here with 2.10.3 and CeleryExecutor. Worker log:\r\n\r\n> Nov 29 01:04:05 ubuntu-s-4vcpu-8gb-amd-fra1-01 airflow[2929312]: [2024-11-29T01:04:05.161+0000] {scheduler_job_runner.py:910} ERROR - Executor CeleryExecutor(parallelism=64) reported that the task instance <TaskInstance: my_dag.task_id scheduled__2024-11-29T00:30:00+00:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally"", 'created_at': datetime.datetime(2024, 11, 29, 1, 28, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507512017, 'issue_id': 2516759806, 'author': 'ClementViricel', 'body': 'same issue deploying on docker with the tutorial supplied on the airflow website. Running test_dags.py as follow : \r\n`import datetime\r\n\r\nimport pendulum\r\n\r\nfrom airflow.models.dag import DAG\r\nfrom airflow.operators.empty import EmptyOperator\r\n\r\nnow = pendulum.now(tz=""UTC"")\r\nnow_to_the_hour = (now - datetime.timedelta(0, 0, 0, 0, 0, 3)).replace(minute=0, second=0, microsecond=0)\r\nSTART_DATE = now_to_the_hour\r\nDAG_NAME = ""test_dag_v2""\r\n\r\ndag = DAG(\r\n    DAG_NAME,\r\n    schedule=""*/10 * * * *"",\r\n    default_args={""depends_on_past"": True},\r\n    start_date=pendulum.datetime(2021, 1, 1, tz=""UTC""),\r\n    catchup=False,\r\n)\r\n\r\nrun_this_1 = EmptyOperator(task_id=""run_this_1"", dag=dag)\r\nrun_this_2 = EmptyOperator(task_id=""run_this_2"", dag=dag)\r\nrun_this_2.set_upstream(run_this_1)\r\nrun_this_3 = EmptyOperator(task_id=""run_this_3"", dag=dag)\r\nrun_this_3.set_upstream(run_this_2)`\r\n\r\nThe dag is success but i got the following log message : \r\n`*** Could not read served logs: Invalid URL \'http://:8793/log/dag_id=test_dag_v1/run_id=manual__2024-11-29T10:15:31.097211+00:00/task_id=run_this_1/attempt=1.log\': No host supplied\r\n`\r\nI verify in my logs folder and i have the log of  test_dag_v2 filling.', 'created_at': datetime.datetime(2024, 11, 29, 10, 23, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507546634, 'issue_id': 2516759806, 'author': 'sosystems-dev', 'body': ""had the same issue when migrating to 2.10.3\r\nI found this post very useful for troubleshooting\r\nhttps://github.com/apache/airflow/discussions/32234\r\n\r\nFrom my perspective this also occurs on Dummy and EmptyOperators as they don't have any output to log.\r\nI think there was some discussion on Github about this behaviour.\r\n\r\nHope it helps :-)"", 'created_at': datetime.datetime(2024, 11, 29, 10, 43, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2545050436, 'issue_id': 2516759806, 'author': 'mahmoudmostafa0', 'body': 'i got the same issue on 2.10.3', 'created_at': datetime.datetime(2024, 12, 16, 9, 28, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2563695348, 'issue_id': 2516759806, 'author': 'quack39', 'body': 'can someone add ?keepalives=1&keepalives_idle=30&keepalives_interval=10&keepalives_count=5 to the database connection string and test the behavior?', 'created_at': datetime.datetime(2024, 12, 27, 13, 19, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2564673131, 'issue_id': 2516759806, 'author': 'mahmoudmostafa0', 'body': '> add ?keepalives=1&keepalives_idle=30&keepalives_interval=10&keepalives_count=5 to the database connection string and test the behavior?\r\n\r\ntried that now and still got the same issue, i also downgraded to 2.9.3', 'created_at': datetime.datetime(2024, 12, 29, 10, 2, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2572811188, 'issue_id': 2516759806, 'author': 'tuanpb99', 'body': 'Having the same issue on 2.10.2', 'created_at': datetime.datetime(2025, 1, 6, 10, 24, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2574156182, 'issue_id': 2516759806, 'author': 'nikhilcss97', 'body': 'We are facing the same issue on 2.10.4', 'created_at': datetime.datetime(2025, 1, 7, 0, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2580003337, 'issue_id': 2516759806, 'author': 'doanthai', 'body': '@pedro-cf Please check log from worker, 8793 is airflow worker port. When you use Docker for deploy airflow that mean worker, scheduler, webserver is running on difference containers and sure those containers mount same dags dir.', 'created_at': datetime.datetime(2025, 1, 9, 12, 10, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2580050017, 'issue_id': 2516759806, 'author': 'chrijun', 'body': ""We are facing a similar issue in versions: 2.10.2 and 2.10.4 in EKS. I noticed that when we had our workers in a Statefulset, they return the full FQDN: `airflow-worker-0.airflow-worker.airflow.cluster.local`, whereas when they are deployed in a Deployment, they only return: `airflow-worker-xxxxxxxx-yyyy`. For us, changing `hostname_callable` to `airflow.utils.net.get_host_ip_address`  fixed the issue. This might be something nice to set automatically when worker's persistence is disabled in the HelmChart?"", 'created_at': datetime.datetime(2025, 1, 9, 12, 35, 22, tzinfo=datetime.timezone.utc)}]","andrew-stein-sp on (2024-09-10 20:17:12 UTC): having the same issue with 2.10.1 in k8s, using the CeleryKubernetesExecutor. 

Could this be related to the inheritance issue that was discussed in https://github.com/apache/airflow/issues/41891?

pedro-cf (Issue Creator) on (2024-09-10 20:29:37 UTC): Additionally I checked the logs directory for the dag_id/run_id and it's missing the respective task_id folder.

adriens on (2024-09-16 03:17:14 UTC): Having the same issue on `2.10.0` through a `podman-compose`

adriens on (2024-09-16 04:08:36 UTC): We have upgraded on `2.10.1` like @andrew-stein-sp and we could reproduce the same behavior

sosystems-dev on (2024-09-17 11:31:37 UTC): got the same behavior since upgrading from version 2.9.3 to 2.10.1. 
We are using LocalExecutor

mn7k on (2024-09-18 01:04:58 UTC): I have the same issue with 2.10.0, using the CeleryExecutor. 
It worked before I upgrading from version 2.9.0 to 2.10.0.
```
*** Could not read served logs: Invalid URL 'http://:8793/log/dag_id=service_stop/run_id=manual__2024-09-18T09:42:54+09:00/task_id=make_accountlist_task/attempt=1.log': No host supplied
```

eventlog
```
Executor CeleryExecutor(parallelism=6) reported that the task instance <TaskInstance: service_stop.make_accountlist_task manual__2024-09-18T09:42:54+09:00 [queued]> finished with state failed, but the task instance's state attribute is queued. Learn more: https://airflow.apache.org/docs/apache-airflow/stable/troubleshooting.html#task-state-changed-externally
```

Scheduler has a error log at the same hour as eventlog.
```
[2024-09-18T00:43:18.036+0000] {celery_executor.py:291} ERROR - Error sending Celery task: module 'redis' has no attribute 'client'
Celery Task ID: TaskInstanceKey(dag_id='service_stop', task_id='make_accountlist_task', run_id='manual__2024-09-18T09:42:54+09:00', try_number=1, map_index=-1)
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/celery/executors/celery_executor_utils.py"", line 220, in send_task_to_executor
    result = task_to_run.apply_async(args=[command], queue=queue)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/celery/app/task.py"", line 594, in apply_async
    return app.send_task(
           ^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/celery/app/base.py"", line 797, in send_task
    with self.producer_or_acquire(producer) as P:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/celery/app/base.py"", line 932, in producer_or_acquire
    producer, self.producer_pool.acquire, block=True,
              ^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/celery/app/base.py"", line 1354, in producer_pool
    return self.amqp.producer_pool
           ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/celery/app/amqp.py"", line 591, in producer_pool
    self.app.connection_for_write()]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/celery/app/base.py"", line 829, in connection_for_write
    return self._connection(url or self.conf.broker_write_url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/celery/app/base.py"", line 880, in _connection
    return self.amqp.Connection(
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/connection.py"", line 201, in __init__
    if not get_transport_cls(transport).can_parse_url:
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/transport/__init__.py"", line 91, in get_transport_cls
    _transport_cache[transport] = resolve_transport(transport)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/transport/__init__.py"", line 76, in resolve_transport
    return symbol_by_name(transport)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/utils/imports.py"", line 59, in symbol_by_name
    module = imp(module_name, package=package, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/importlib/__init__.py"", line 90, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""<frozen importlib._bootstrap>"", line 1387, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 1360, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 1331, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 935, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 995, in exec_module
  File ""<frozen importlib._bootstrap>"", line 488, in _call_with_frames_removed
  File ""/home/airflow/.local/lib/python3.12/site-packages/kombu/transport/redis.py"", line 282, in <module>
    class PrefixedRedisPipeline(GlobalKeyPrefixMixin, redis.client.Pipeline):
                                                      ^^^^^^^^^^^^
AttributeError: module 'redis' has no attribute 'client'
```

damiah on (2024-09-29 21:39:04 UTC): same issue for us when upgrading to 2.10.2

nikithapk on (2024-10-03 06:59:53 UTC): We’re encountering the same issue as well.

adriens on (2024-10-03 07:29:42 UTC): We have switched from Bitnami docker-compose to official Apache docker-compose and we could make it run successfuly :star_struck:

Dzhalolov on (2024-10-07 12:51:02 UTC): try to check that dags exist in worker, schedule and webserver. I deploy Airflow in K8S and get this error when putting my dags into scheduler(expecting that in will replicate into another pods), but when I check dags folder in worker it was empty

mn7k on (2024-10-15 06:18:40 UTC): At this time (https://github.com/apache/airflow/issues/42136#issuecomment-2357283522), I used the `airflow db upgrade` command , but I realized it has been deprecated.
I retried the upgrade using the `airflow db migrate -n ""2.10.2""` command, and it works for me now.

https://airflow.apache.org/docs/apache-airflow/2.10.0/installation/upgrading.html#offline-sql-migration-scripts

quack39 on (2024-10-17 11:10:36 UTC): We encountered the same problem in Airflow 2.9.3
Here are the Worker logs at the time of the error:

```
[2024-10-11 10:45:38,544: WARNING/ForkPoolWorker-16] Failed operation _store_result.  Retrying 2 more times.
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context
    self.dialect.do_execute(
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py"", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.OperationalError: could not receive data from server: Connection timed out


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.12/site-packages/celery/backends/database/__init__.py"", line 47, in _inner
    return fun(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/celery/backends/database/__init__.py"", line 117, in _store_result
    task = list(session.query(self.task_cls).filter(self.task_cls.task_id == task_id))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py"", line 2901, in __iter__
    result = self._iter()
             ^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/query.py"", line 2916, in _iter
    result = self.session.execute(
             ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py"", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/sql/elements.py"", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 2134, in _handle_dbapi_exception
    util.raise_(
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py"", line 211, in raise_
    raise exception
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context
    self.dialect.do_execute(
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/engine/default.py"", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not receive data from server: Connection timed out

[SQL: SELECT celery_taskmeta.id AS celery_taskmeta_id, celery_taskmeta.task_id AS celery_taskmeta_task_id, celery_taskmeta.status AS celery_taskmeta_status, celery_taskmeta.result AS celery_taskmeta_result, celery_taskmeta.date_done AS celery_taskmeta_date_done, celery_taskmeta.traceback AS celery_taskmeta_traceback 
FROM celery_taskmeta 
WHERE celery_taskmeta.task_id = %(task_id_1)s]
[parameters: {'task_id_1': '5d1bef21-fbf4-4feb-9f2c-a54c95b4d738'}]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
```

I can also note that increasing the sql_alchemy_pool_size parameter to 50 reduced the number of such errors, but did not eliminate them completely.

ali-naderi on (2024-10-22 09:39:59 UTC): The same issue in Airflow 2.10.2

Dev-iL on (2024-10-28 16:21:51 UTC): **TL;DR** look for invalid python scripts on the malfunctioning worker. Try creating a DagBag on the worker and see what happens.
```python
# Ensure the AIRFLOW_HOME points to the right location, then run on the worker
```
------


I had this issue too, turns out I edited one of the files through vim, pasted some code, and it pasted tabs instead of spaces, so the file became an invalid python script due to `TabError: inconsistent use of tabs and spaces in indentation`. After I fixed that, it all went back to normal. 

Note that the problematic file doesn't have to be imported by the failing DAG/task. If I understand the issue correctly, a DagBag cannot be created if one of the DAG definition files or their imports isn't a valid python file. Then the issue manifests as DAGs supposedly not being found. In my case, the filesystem isn't shared between the scheduler and the malfunctioning celery worker, and the affected file was unmodified on the scheduler (or modified in a correct way) - so no ""big red import error"" was displayed in the webserver UI.

abhijit-sarkar-ext on (2024-11-06 08:59:46 UTC): Hi @quack39 and all,
I am getting same error. I have deployed Airflow [2.9.3] in AKS. But when executing the DAGS getting below error. Don't getting any clue what needs to be updated. I am using Helm [1.15.0] for deployment and using ""KubernetesExecuter""

**Error**


Could not read served logs: HTTPConnectionPool(host='test-dag-config-nlp8suol', port=8793): Max retries exceeded with url: /log/dag_id=test_dag/run_id=manual__2024-11-06T06:43:18.256272+00:00/task_id=config/attempt=1.log (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8ef9eaecd0>: Failed to establish a new connection: [Errno -2] Name or service not known'))

darenpang on (2024-11-09 02:53:56 UTC): I had the same problem when changing sequentialexecutor to localexecutor.
After some test , I find I must make parallelism equal to CPU core number .

with t3.large(CPU core :2)
 parallelism =32 (default)  NG
 parallelism =4  most of the tasks are NG , but some OK 
 parallelism =2  all OK
with t3.xlarge(CPU core :4)
 parallelism =4 all OK

But is this the expected action ? I'm not sure .

liangpengfei on (2024-11-12 06:36:22 UTC): same issue for us when upgrading to 2.10.3，using k8s.

huang06 on (2024-11-14 09:28:09 UTC): The same issue in Airflow 2.10.2 with KubernetesExecutor.

kate-rodgers on (2024-11-15 00:58:13 UTC): I was just able to get the following to work:
```
task = BashOperator(
task_id=""bash_command"",
bash_command=bash_command,
retries=2,
retry_delay=timedelta(minutes=1),
do_xcom_push=False,
env={
'PYTHONUNBUFFERED': '1',
'PYTHONFAULTHANDLER': '1', # Helps debug crashes
'FORCE_COLOR': '1' # Preserves color output in logs
},
cwd='/tmp',
append_env=True
)
```

initially it failed with the same error, then succeeded on retry, I believe because the log stream was still in 'create' and not available for the first attempt.

hditano on (2024-11-16 07:12:40 UTC): Same issue here, even if the task is successful

babaymaster on (2024-11-22 12:55:56 UTC): We have same problem. Please help )
2.10.3

jliebers on (2024-11-22 13:02:38 UTC): I've ran accidentally into this issue when I defined a custom volume mount in my `docker-compose.yml`. At first, I defined the mount as part of the `airflow-worker` service, which apparently overrode the volume mounts imported from `x-airflow-common`. This then lead to exactly this issue eventually. I resolved this issue for me, by defining my custom mount in `x-airflow-common` instead. That is how I ended up in this issue; there might of course be different, completely unrelated ways. But if you've encountered this and worked with custom mounts in a docker-environment, double check your `docker-compose.yml` and the imports done by `<<: *` operator. Just a quick heads-up in the hope that it helps somebody.

opeida on (2024-11-29 01:28:09 UTC): The same issue here with 2.10.3 and CeleryExecutor. Worker log:

ClementViricel on (2024-11-29 10:23:28 UTC): same issue deploying on docker with the tutorial supplied on the airflow website. Running test_dags.py as follow : 
`import datetime

import pendulum

from airflow.models.dag import DAG
from airflow.operators.empty import EmptyOperator

now = pendulum.now(tz=""UTC"")
now_to_the_hour = (now - datetime.timedelta(0, 0, 0, 0, 0, 3)).replace(minute=0, second=0, microsecond=0)
START_DATE = now_to_the_hour
DAG_NAME = ""test_dag_v2""

dag = DAG(
    DAG_NAME,
    schedule=""*/10 * * * *"",
    default_args={""depends_on_past"": True},
    start_date=pendulum.datetime(2021, 1, 1, tz=""UTC""),
    catchup=False,
)

run_this_1 = EmptyOperator(task_id=""run_this_1"", dag=dag)
run_this_2 = EmptyOperator(task_id=""run_this_2"", dag=dag)
run_this_2.set_upstream(run_this_1)
run_this_3 = EmptyOperator(task_id=""run_this_3"", dag=dag)
run_this_3.set_upstream(run_this_2)`

The dag is success but i got the following log message : 
`*** Could not read served logs: Invalid URL 'http://:8793/log/dag_id=test_dag_v1/run_id=manual__2024-11-29T10:15:31.097211+00:00/task_id=run_this_1/attempt=1.log': No host supplied
`
I verify in my logs folder and i have the log of  test_dag_v2 filling.

sosystems-dev on (2024-11-29 10:43:28 UTC): had the same issue when migrating to 2.10.3
I found this post very useful for troubleshooting
https://github.com/apache/airflow/discussions/32234

From my perspective this also occurs on Dummy and EmptyOperators as they don't have any output to log.
I think there was some discussion on Github about this behaviour.

Hope it helps :-)

mahmoudmostafa0 on (2024-12-16 09:28:55 UTC): i got the same issue on 2.10.3

quack39 on (2024-12-27 13:19:06 UTC): can someone add ?keepalives=1&keepalives_idle=30&keepalives_interval=10&keepalives_count=5 to the database connection string and test the behavior?

mahmoudmostafa0 on (2024-12-29 10:02:31 UTC): tried that now and still got the same issue, i also downgraded to 2.9.3

tuanpb99 on (2025-01-06 10:24:07 UTC): Having the same issue on 2.10.2

nikhilcss97 on (2025-01-07 00:19:00 UTC): We are facing the same issue on 2.10.4

doanthai on (2025-01-09 12:10:29 UTC): @pedro-cf Please check log from worker, 8793 is airflow worker port. When you use Docker for deploy airflow that mean worker, scheduler, webserver is running on difference containers and sure those containers mount same dags dir.

chrijun on (2025-01-09 12:35:22 UTC): We are facing a similar issue in versions: 2.10.2 and 2.10.4 in EKS. I noticed that when we had our workers in a Statefulset, they return the full FQDN: `airflow-worker-0.airflow-worker.airflow.cluster.local`, whereas when they are deployed in a Deployment, they only return: `airflow-worker-xxxxxxxx-yyyy`. For us, changing `hostname_callable` to `airflow.utils.net.get_host_ip_address`  fixed the issue. This might be something nice to set automatically when worker's persistence is disabled in the HelmChart?

"
2516725415,issue,closed,not_planned,Airflow not retrying Zombie even after detection,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.8.3

### What happened?

We have upgraded to 2.8.3 and have been noticing a lot more zombie jobs.
I have not upgraded to the latest version as I don't see anything in the change logs. 
An observation is that the worker pods that was working on the task is no longer present (likely scaled down). 


```
AIRFLOW__SCHEDULER__SCHEDULER_ZOMBIE_TASK_THRESHOLD = 600 <-- Set via environment variable

config: 
    core:
      dag_file_processor_timeout: 360
      dagbag_import_timeout: 240
      check_slas: false
      default_task_execution_timeout: 3600
    api:
      auth_backends: airflow.api.auth.backend.basic_auth
    scheduler:
      dag_dir_list_interval: 120
      min_file_process_interval: 30
      parsing_processes: 20
      task_queued_timeout: 10800
      parsing_cleanup_interval: 1000
      
    celery:
      worker_concurrency: 10 
    logging:
      logging_level: DEBUG
```
Sample screenshot from the the UI. 

<img width=""914"" alt=""image"" src=""https://github.com/user-attachments/assets/3f06e7c8-b529-4755-92f8-4744a847d0e7"">

After many moments still not stopped
![Uploading image.png…]()


Sample Pod
```
task = KubernetesPodOperator(
        namespace=""airflow"",
        task_id=f""""""{dag_name}_{dag_env}_task"""""",
        name=f""""""{dag_name}_{dag_env}_task"""""",
        image=ETL_DOCKER_IMAGE,
        image_pull_secrets=[k8s.V1LocalObjectReference(""airflow-docker-secret"")],
        cmds=[""/workspace/golang_cli"",  ""--name"",  ""myapp""],
        is_delete_operator_pod=True,
        in_cluster=True,
        get_logs=True,
        image_pull_policy=""Always"",
        on_failure_callback=on_failure,
        container_resources=k8s.V1ResourceRequirements(
            limits={""memory"": ""2Gi"", ""cpu"": ""1000m""},
            requests={""memory"": ""2Gi"", ""cpu"": ""500m""},
        ),
        sla=timedelta(hours=2),
        retry_delay=timedelta(seconds=60),
        secrets=secrets,
        env_from=configmaps,
    )
```

### What you think should happen instead?

I would have expected the dags to be up for retry. Even after waiting 4 hours its not working. 
Whats even more strange is that `default_task_execution_timeout` was set but not respected. 


### How to reproduce

I was unable to reproduce this. However i was unable reproduce a zombie dag too.

```
from airflow.decorators import dag
from airflow.operators.bash import BashOperator
from datetime import datetime
from airflow.utils.dates import days_ago

from airflow import DAG

with DAG(
    dag_id=""sleep_dag"",
    start_date=days_ago(1),
    schedule=""@daily"",
    default_args={""retries"": 2},
):
    t1 = BashOperator(
        task_id=""sleep_10_minutes"",
        bash_command=""sleep 1200"",
    )

``` 
ideally this should have caused it to timeout as a zombie.
<img width=""941"" alt=""image"" src=""https://github.com/user-attachments/assets/129f9d5d-d89a-4463-b50b-5663a05c3a1f"">
 However it ran without any issues. 

### Operating System

K8

### Versions of Apache Airflow Providers

```
FROM apache/airflow:2.8.3-python3.11

USER airflow

RUN pip install apache-airflow-providers-cncf-kubernetes==7.14.0

RUN pip install apache-airflow-providers-databricks==6.2.0

RUN pip install apache-airflow-providers-google==10.21.0

RUN pip install apache-airflow-providers-microsoft-mssql==3.6.1

RUN pip install datadog

RUN pip install 'apache-airflow[statsd]'

RUN pip install 'apache-airflow[otel]'

RUN pip install apache-airflow[jdbc]

RUN pip install apache-airflow-providers-mongo

RUN pip install apache-airflow-providers-sendgrid==3.6.0

```

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Running on K8

```
#default values file : https://github.com/apache/airflow/blob/2.4.1/chart/values.yaml#L520
airflow:
  extraEnv: |
    - name: AIRFLOW__SCHEDULER__SCHEDULER_ZOMBIE_TASK_THRESHOLD
      value: '600'
  redis:
    safeToEvict: false
    tolerations: 
    - key: ""airflowcore""
      operator: ""Equal""
      value: ""true""
      effect: ""NoSchedule"" 
    nodeSelector:
      agentpool: airflowcore 
    password: ------
  # https://github.com/apache/airflow/blob/2.4.1/chart/values.yaml
  # Default airflow repository -- overrides all the specific images below
  defaultAirflowRepository: ""sharedservicesacrnpd.azurecr.io/airflowcustomimage""
  # Default airflow tag to deploy
  defaultAirflowTag: ""260978""
  createUserJob:
    useHelmHooks: false
  migrateDatabaseJob:
    useHelmHooks: false
  defaultNodeSelector:
    pool: airflowcore
  fernetKeySecretName: airflow-fernet-key  
  webserverSecretKeySecretName: airflow-webserver-key
  scheduler:
    resources:
      limits:
        cpu: 2000m
        memory: 1Gi
      requests:
        cpu: 1000m
        memory: 512Mi
    tolerations: 
    - key: ""airflowcore""
      operator: ""Equal""
      value: ""true""
      effect: ""NoSchedule""     
    nodeSelector:
      agentpool: airflowcore   
    replicas: 8    
  web:
    resources:
      limits:
        cpu: 500m
        memory: 1Gi
      requests:
        cpu: 250m
        memory: 256Mi
    tolerations: 
    - key: ""airflowcore""
      operator: ""Equal""
      value: ""true""
      effect: ""NoSchedule""     
    nodeSelector:
      agentpool: airflowcore      
  triggerer:
    resources:
      limits:
        cpu: 500m
        memory: 1Gi
      requests:
        cpu: 250m
        memory: 256Mi
    tolerations: 
    - key: ""airflowcore""
      operator: ""Equal""
      value: ""true""
      effect: ""NoSchedule""     
    nodeSelector:
      agentpool: airflowcore                 
  dagProcessor:
    enabled: true
    resources:
      # limits:
      #   cpu: 1000m
      #   memory: 2Gi
      requests:
        cpu: 2000m
        memory: 2Gi
    tolerations: 
    - key: ""airflowcore""
      operator: ""Equal""
      value: ""true""
      effect: ""NoSchedule""     
    nodeSelector:
      agentpool: airflowcore   
  statsd:  
    nodeSelector:
      agentpool: airflow        
  dags:
    persistence:
    # Enable persistent volume for storing dags
      enabled: true
    # Volume size for dags
      size: 50Gi
    # If using a custom storageClass, pass name here
      storageClassName:
    # access mode of the persistent volume
      accessMode: ReadWriteMany
    ## the name of an existing PVC to use
      existingClaim: airflow-dag-pvc
  logs:
    persistence:
    # Enable persistent volume for storing logs
      enabled: true
    # Volume size for logs
      size: 50Gi
    # If using a custom storageClass, pass name here
      storageClassName:
    ## the name of an existing PVC to use
      existingClaim: airflow-log-pvc    
  executor: ""CeleryKubernetesExecutor""    
  workers:
    resources:
      requests:
        memory: 5Gi
    keda:
      enabled: true
      cooldownPeriod: 300
      minReplicaCount: 10
    persistence:
      enabled: false
    tolerations: 
    - key: ""airflowcore""
      operator: ""Equal""
      value: ""true""
      effect: ""NoSchedule""     
    nodeSelector:
      agentpool: airflowcore 
    safeToEvict: false   
  ingress:
    web:
      enabled: true
      annotations:
        nginx.ingress.kubernetes.io/rewrite-target: ""/""
        nginx.ingress.kubernetes.io/ssl-redirect: ""true""
      hosts:
        - name: airflow.------.com
          tls: 
            enabled: true
            secretName: ""airflow-secret-ingress-tls""
      ingressClassName: ""nginx""
  postgresql:
    enabled: false
  pgbouncer:
    enabled: false
  data:  
    metadataSecretName: airflow-postgres-secret 
  config: 
    core:
      dag_file_processor_timeout: 360
      dagbag_import_timeout: 240
      check_slas: false
      default_task_execution_timeout: 3600
    api:
      auth_backends: airflow.api.auth.backend.basic_auth
    scheduler:
      dag_dir_list_interval: 120
      min_file_process_interval: 30
      parsing_processes: 20
      task_queued_timeout: 10800
      parsing_cleanup_interval: 1000
      
    celery:
      worker_concurrency: 10 
    logging:
      logging_level: DEBUG
    metrics: 
      statsd_on: false
      statsd_port: 8125
      statsd_host: datadog-agent.observability      
secrettls:
  name: airflow-secret-ingress-tls
  data:
    tls.crt: ----
    tls.key: ----
secret:
  name: airflow-secret-storageacct
  data: 
    azurestorageaccountname: -----
    azurestorageaccountkey: ------
secretfernet:
  name: airflow-fernet-key
  data: 
    fernet-key: ---
acrsecret:
  name: airflow-docker-secret
  data: 
    username: ---
    password: ---
secretwebserverkey:
  name: airflow-webserver-key
  data:
    webserver-secret-key: ---
connectionsecret:
  name: airflow-postgres-secret
  data:
    connection: ---
storageClass: 
  name: airflowstorageclass
  provisioner: file.csi.azure.com

dagvolumes: 
  name: airflow-dag-pv
  size: 50Gi
  accessModes: ReadWriteMany
  reclaimPolicy: Retain
  volumeHandle: airflowdag
  shareName: airflowdag
  secretName: airflow-secret-storageacct

dagvolumeClaim:
  name: airflow-dag-pvc
  accessModes: ReadWriteMany
  size: 50Gi

storageClassLog: 
  name: airflowstorageclasslog
  provisioner: file.csi.azure.com

logvolumes: 
  name: airflow-log-pv
  size: 50Gi
  accessModes: ReadWriteMany
  reclaimPolicy: Retain
  volumeHandle: airflowlog
  shareName: airflowlog
  secretName: airflow-secret-storageacct

logvolumeClaim:
  name: airflow-log-pvc
  accessModes: ReadWriteMany
  size: 50Gi
```

### Anything else?

The issue happens too often. 
We have about 
1. 350 DAGS
2. Most of the DAGs are K8 Pod Operator, few GCP Operators. 
3. Airflow components run in the own nodepool
4. K8Pod Operator pods are run in a different node pool. 


### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",kand617,2024-09-10 15:03:20+00:00,[],2025-01-16 18:18:00+00:00,2024-12-25 00:15:09+00:00,https://github.com/apache/airflow/issues/42135,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('pending-response', ''), ('area:core', '')]","[{'comment_id': 2341173130, 'issue_id': 2516725415, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 10, 15, 3, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2346746162, 'issue_id': 2516725415, 'author': 'vaibhavnsingh', 'body': '+1 \r\n\r\nIn our current setup, we are using Celery workers as Airflow workers, and we have applied a memory limit on these workers as per our DevOps guidelines. When a Celery worker exceeds its memory limit, it encounters an Out-Of-Memory (OOM) error and restarts. This behavior leads to tasks that were in a running state becoming zombie tasks, which the Airflow scheduler detects.\r\n\r\nHowever, we have observed that despite the scheduler detecting these tasks as zombie tasks, Airflow does not mark them as failed.', 'created_at': datetime.datetime(2024, 9, 12, 16, 26, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2489723964, 'issue_id': 2516725415, 'author': 'darren-stults-sp', 'body': '+1\r\n\r\nw/ `Airflow v2.10.1`\r\n\r\nSeeing a similar thing where the zombie detections happen multiple times for a single try, based off of the code I should expect the job to be terminated after one detection, but the zombie detector log event (when it appears) seems to reappear every 12 or so seconds for about 8 to 20 repeats before the try fails.', 'created_at': datetime.datetime(2024, 11, 20, 23, 12, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2511200764, 'issue_id': 2516725415, 'author': 'potiuk', 'body': 'I think one of the reasons that could cause that have been fixed in 2.10.3 https://github.com/apache/airflow/pull/42932 -> can you please upgrade and see if your problems is gone @darren-stults-sp @vaibhavnsingh @kand617  -> this is is the easiest way to check it (and upgrading to latest Airflow version is a good idea regardless).', 'created_at': datetime.datetime(2024, 12, 2, 10, 52, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2547213268, 'issue_id': 2516725415, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 12, 17, 0, 16, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2561502186, 'issue_id': 2516725415, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 12, 25, 0, 15, 8, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-10 15:03:22 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

vaibhavnsingh on (2024-09-12 16:26:26 UTC): +1 

In our current setup, we are using Celery workers as Airflow workers, and we have applied a memory limit on these workers as per our DevOps guidelines. When a Celery worker exceeds its memory limit, it encounters an Out-Of-Memory (OOM) error and restarts. This behavior leads to tasks that were in a running state becoming zombie tasks, which the Airflow scheduler detects.

However, we have observed that despite the scheduler detecting these tasks as zombie tasks, Airflow does not mark them as failed.

darren-stults-sp on (2024-11-20 23:12:45 UTC): +1

w/ `Airflow v2.10.1`

Seeing a similar thing where the zombie detections happen multiple times for a single try, based off of the code I should expect the job to be terminated after one detection, but the zombie detector log event (when it appears) seems to reappear every 12 or so seconds for about 8 to 20 repeats before the try fails.

potiuk on (2024-12-02 10:52:16 UTC): I think one of the reasons that could cause that have been fixed in 2.10.3 https://github.com/apache/airflow/pull/42932 -> can you please upgrade and see if your problems is gone @darren-stults-sp @vaibhavnsingh @kand617  -> this is is the easiest way to check it (and upgrading to latest Airflow version is a good idea regardless).

github-actions[bot] on (2024-12-17 00:16:34 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-12-25 00:15:08 UTC): This issue has been closed because it has not received response from the issue author.

"
2516717888,issue,closed,completed,"CLI failures ""_TimetableNotRegistered"" on the newly added example DAG(s)","### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

2.10.1

### What happened?

[This PR](https://github.com/apache/airflow/pull/39999) does make sure the plugin is loaded when `load_examples = True`. And the example DAG itself runs ok. No ""Broken Dag"" import errors or anything. The UI -> Admin -> Plugins shows the 3 plugins from the `example_dags/plugins`

However, both the `sync-perm` cli and the `plugins` cli don't agree with it. When executing `airflow sync-perm --include-dags -v`, it always errors out on this example dag:
```
{serialized_dag.py:200} DEBUG - Deserializing DAG: example_workday_timetable
{plugins_manager.py:351} DEBUG - Plugins are already loaded. Skipping.
{plugins_manager.py:484} DEBUG - Initialize extra timetables plugins
{cli_action_loggers.py:98} DEBUG - Calling callbacks: []
Traceback (most recent call last):
  File ""/usr/local/airflow/.local/bin/airflow"", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/__main__.py"", line 62, in main
    args.func(args)
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/cli/cli_config.py"", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/utils/cli.py"", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/utils/providers_configuration_loader.py"", line 55, in wrapped_function
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/fab/auth_manager/cli_commands/sync_perm_command.py"", line 39, in sync_perm
    appbuilder.sm.create_dag_specific_permissions()
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/fab/auth_manager/security_manager/override.py"", line 1072, in create_dag_specific_permissions
    dagbag.collect_dags_from_db()
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py"", line 628, in collect_dags_from_db
    self.dags = SerializedDagModel.read_all_dags()
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/models/serialized_dag.py"", line 201, in read_all_dags
    dag = row.dag
          ^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/models/serialized_dag.py"", line 235, in dag
    return SerializedDAG.from_dict(data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/serialization/serialized_objects.py"", line 1776, in from_dict
    return cls.deserialize_dag(serialized_obj[""dag""])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/serialization/serialized_objects.py"", line 1704, in deserialize_dag
    v = decode_timetable(v)
        ^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/serialization/serialized_objects.py"", line 329, in decode_timetable
    raise _TimetableNotRegistered(importable_string)
airflow.serialization.serialized_objects._TimetableNotRegistered: Timetable class 'airflow.example_dags.plugins.workday.AfterWorkdayTimetable' is not registered or you have a top level database access that disrupted the session. Please check the airflow best practices documentation.
```
and when executing `airflow plugins -o table -v`, the 3 plugins from `example_dags/plugins` are not in the output table.

When I set `load_examples = False`, the `sync-perm` and `plugin` work fine without any errors.

The only way that I can run both this example DAG and the cli commands is:
1. set `load_examples = False`
2. copy the source code `airflow/example_dags/plugins/workday.py` to my own `airflow/plugins` (in my `airflow.cfg`, i have `plugins_folder = /usr/local/airflow/plugins`)
3. re-create the example as one of my own dags but import from my own plugins: `from workday import AfterWorkdayTimetable`

### What you think should happen instead?

Airflow CLI `sync-perm` should be able to process this example DAG.

### How to reproduce

1. Load the example DAGs by setting `load_examples = True`
2. Run `airflow sync-perm --include-dags -v`

### Operating System

Debian GNU/Linux 10 (buster)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",zachliu,2024-09-10 15:00:14+00:00,['nikhilkarve'],2024-09-25 02:56:04+00:00,2024-09-25 02:56:04+00:00,https://github.com/apache/airflow/issues/42133,"[('kind:bug', 'This is a clearly a bug'), ('area:CLI', ''), ('good first issue', ''), ('area:core', '')]","[{'comment_id': 2344189729, 'issue_id': 2516717888, 'author': 'nikhilkarve', 'body': 'Can I pick this up? This would be my first issue', 'created_at': datetime.datetime(2024, 9, 11, 16, 55, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2344302204, 'issue_id': 2516717888, 'author': 'potiuk', 'body': 'Assigned.', 'created_at': datetime.datetime(2024, 9, 11, 17, 40, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2372486933, 'issue_id': 2516717888, 'author': 'zachliu', 'body': 'i want to help fix this because i need both the dag examples and the `airflow sync-perm --include-dags -v` command\r\ncan someone point me to the right direction?\r\n\r\nit looks like airflow cli such as `sync-perm` evaluates the plugins differently?\r\nor simply because this import\r\nhttps://github.com/apache/airflow/blob/ab3429c3189ceb244eb3d78062159859dbe611ce/airflow/example_dags/example_workday_timetable.py#L21\r\nis in the wrong place?', 'created_at': datetime.datetime(2024, 9, 24, 22, 17, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2372800296, 'issue_id': 2516717888, 'author': 'zachliu', 'body': 'oh :hankey::hankey::hankey: the example dags are loaded everywhere else (`webserver`, `scheduler`, `triggerer`) EXCEPT my `worker` nodes :sweat_smile:', 'created_at': datetime.datetime(2024, 9, 25, 2, 56, 4, tzinfo=datetime.timezone.utc)}]","nikhilkarve (Assginee) on (2024-09-11 16:55:29 UTC): Can I pick this up? This would be my first issue

potiuk on (2024-09-11 17:40:33 UTC): Assigned.

zachliu (Issue Creator) on (2024-09-24 22:17:39 UTC): i want to help fix this because i need both the dag examples and the `airflow sync-perm --include-dags -v` command
can someone point me to the right direction?

it looks like airflow cli such as `sync-perm` evaluates the plugins differently?
or simply because this import
https://github.com/apache/airflow/blob/ab3429c3189ceb244eb3d78062159859dbe611ce/airflow/example_dags/example_workday_timetable.py#L21
is in the wrong place?

zachliu (Issue Creator) on (2024-09-25 02:56:04 UTC): oh :hankey::hankey::hankey: the example dags are loaded everywhere else (`webserver`, `scheduler`, `triggerer`) EXCEPT my `worker` nodes :sweat_smile:

"
2516577736,issue,closed,not_planned,kubernetes.client.exceptions.ApiException: (404) Reason: Not Found (driver pod is not found),"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I'm trying to run the Spark application with the custom image using Airflow 2.10.0. Also, I use Kubernetes cluster connection (EKS). apache-airflow-providers-cncf-kubernetes version is 8.3.4. When I manually deploy SparkApplication resources to the cluster (via kubectl),the application works fine and is complete without errors. When I schedule DAG, it starts fine, and works without interruptions almost till the end, but then fails with error 404. It seems like it cannot communicate with the driver. However, when I check the cluster, the driver pod is still there and looks fine.
The error text:
```
[2024-09-10, 10:16:22 UTC] {taskinstance.py:3301} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py"", line 293, in execute
    return super().execute(context=context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 406, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py"", line 592, in execute
    return self.execute_sync(context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py"", line 634, in execute_sync
    self.pod_manager.await_xcom_sidecar_container_start(pod=self.pod)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/utils/pod_manager.py"", line 727, in await_xcom_sidecar_container_start
    if self.container_is_running(pod, PodDefaults.SIDECAR_CONTAINER_NAME):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/utils/pod_manager.py"", line 645, in container_is_running
    remote_pod = self.read_pod(pod)
                 ^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/tenacity/__init__.py"", line 336, in wrapped_f
    return copy(f, *args, **kw)
           ^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/tenacity/__init__.py"", line 475, in __call__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/tenacity/__init__.py"", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/tenacity/__init__.py"", line 418, in exc_check
    raise retry_exc.reraise()
          ^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/tenacity/__init__.py"", line 185, in reraise
    raise self.last_attempt.result()
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/concurrent/futures/_base.py"", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/concurrent/futures/_base.py"", line 401, in __get_result
    raise self._exception
  File ""/home/airflow/.local/lib/python3.12/site-packages/tenacity/__init__.py"", line 478, in __call__
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/utils/pod_manager.py"", line 720, in read_pod
    return self._client.read_namespaced_pod(pod.metadata.name, pod.metadata.namespace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/kubernetes/client/api/core_v1_api.py"", line 23693, in read_namespaced_pod
    return self.read_namespaced_pod_with_http_info(name, namespace, **kwargs)  # noqa: E501
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/kubernetes/client/api/core_v1_api.py"", line 23780, in read_namespaced_pod_with_http_info
    return self.api_client.call_api(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/kubernetes/client/api_client.py"", line 348, in call_api
    return self.__call_api(resource_path, method,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/kubernetes/client/api_client.py"", line 180, in __call_api
    response_data = self.request(
                    ^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/kubernetes/client/api_client.py"", line 373, in request
    return self.rest_client.GET(url,
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/kubernetes/client/rest.py"", line 244, in GET
    return self.request(""GET"", url,
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/kubernetes/client/rest.py"", line 238, in request
    raise ApiException(http_resp=r)
kubernetes.client.exceptions.ApiException: (404)
Reason: Not Found
HTTP response headers: HTTPHeaderDict({'Audit-Id': 'b75ef5bb-a810-46c8-a052-6b42307c5229', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'X-Kubernetes-Pf-Flowschema-Uid': '034ee985-68e4-4ac9-ba26-c2b9e6b4cd1d', 'X-Kubernetes-Pf-Prioritylevel-Uid': '2e7745b6-a7a8-493c-99ec-ce7f9c416cff', 'Date': 'Tue, 10 Sep 2024 10:16:22 GMT', 'Content-Length': '270'})
HTTP response body: {""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""pods \""<my-spark-app>-task-qyzuffwi-driver\"" not found"",""reason"":""NotFound"",""details"":{""name"":""<my-spark-app>-task-qyzuffwi-driver"",""kind"":""pods""},""code"":404}
[2024-09-10, 10:16:22 UTC] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=<my-spark-app>, task_id=<my-spark-app>-task, run_id=scheduled__2024-09-10T09:15:00+00:00, execution_date=20240910T091500, start_date=20240910T091612, end_date=20240910T101622
```

### What you think should happen instead?

DAG run should finish with the ""success"" status.

### How to reproduce

Please keep in mind, that all the values in the angle brackets are not real, they were changed for security reasons. If you need any other information, e.g. any Airflow config values, please let me know.
DAG file looks like this:
```
from airflow import DAG
from datetime import datetime, timedelta
from airflow.providers.cncf.kubernetes.operators.spark_kubernetes import SparkKubernetesOperator
from airflow.providers.cncf.kubernetes.sensors.spark_kubernetes import SparkKubernetesSensor
from airflow.utils.dates import days_ago

default_args = {
    'owner': '<me>',
    'depends_on_past': False,
    'start_date': '2024-09-06',
    'email': ['<my@mail>'],
    'email_on_failure': True,
    'email_on_retry': False,
    'max_active_runs': 1,
    'retries': 0,
    'max_active_tis_per_dag': 1 #This parameter controls the number of concurrent running task instances across dag_runs per task.
    #'catchup': False
}

with DAG(
    '<my-spark-app>',
    default_args=default_args,
    schedule_interval=""*/10 * * * *"",
    tags=['<my-spark-app>']
) as dag:

    spark_task = SparkKubernetesOperator(
        task_id=""c<my-spark-app>-task"",
        application_file='<my-spark-app>.yaml',
        dag=dag,
        namespace='default',
        kubernetes_conn_id='<EKS_CONN>',
        do_xcom_push=True,
        params={""app_name"": f""<my-spark-app>""}
    )

    sensor = SparkKubernetesSensor(
        task_id='<my-spark-app>-monitor',
        namespace=""default"",
        application_name=""{{ task_instance.xcom_pull(task_ids='<my-spark-app>-task')['metadata']['name'] }}"",
        kubernetes_conn_id=""<EKS_CONN>"",
        dag=dag,
        api_group=""sparkoperator.k8s.io"",
        api_version='v1beta2',
        attach_log=True
    )

    spark_task >> sensor
```

```
SparkApplication resource YAML looks like this:
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  labels: &Labels
    app:  <my-spark-app>
  name:  <my-spark-app>
spec:
  type: Python
  restartPolicy:
    type: Never
  pythonVersion: ""3""
  sparkVersion: ""3.5.0""
  mode: cluster
  image: <my-ecr-repo>:<my-spark-app>
  mainApplicationFile: local:///<my-spark-app>/main.py
  sparkConf:
    spark.ui.port: ""4040""
    spark.ui.showConsoleProgress: ""true""
    spark.sql.broadcastTimeout: ""6000""
    spark.hadoop.fs.s3a.multiobjectdelete.enable: ""false""
    spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled: ""true""
    spark.metrics.namespace: '<my-spark-app>-qa'
    spark.metrics.conf.*.sink.graphite.host: <graphite-exporter-address>
    spark.metrics.conf.*.sink.graphite.port: ""<graphite-port>""
    spark.metrics.conf.*.sink.graphite.class: ""org.apache.spark.metrics.sink.GraphiteSink""
    spark.metrics.conf.*.sink.graphite.period: ""10""
    spark.metrics.conf.*.sink.graphite.unit"": ""seconds""
    spark.metrics.appStatusSource.enabled: ""true""
    spark.driver.extraJavaOptions: ""-Dlog4j2.configurationFile=file:///<my-spark-app>/log4j2.properties -Dgraylog_host=<graylog-server> -Dgraylog_port=<graylog-port> -Dgraylog_app=<my-spark-app> -Dlog4j2.debug=false""
    spark.executor.extraJavaOptions: ""-Dlog4j2.configurationFile=file:///<my-spark-app>/log4j2.properties -Dgraylog_host=<graylog-server> -Dgraylog_port=<graylog-port> -Dgraylog_app=<my-spark-app> -Dlog4j2.debug=false""
    spark.metrics.conf.driver.source.jvm.class: org.apache.spark.metrics.source.JvmSource
    spark.metrics.conf.executor.source.jvm.class: org.apache.spark.metrics.source.JvmSource
    spark.metrics.conf.worker.source.jvm.class: org.apache.spark.metrics.source.JvmSource
    spark.metrics.conf.master.source.jvm.class: org.apache.spark.metrics.source.JvmSource
  hadoopConf:
    spark.hadoop.fs.s3a.user.agent.prefix: '<my-spark-app>-qa'
    fs.s3a.aws.credentials.provider: com.amazonaws.auth.WebIdentityTokenCredentialsProvider
    fs.s3.impl: org.apache.hadoop.fs.s3a.S3AFileSystem
    fs.s3a.bucket.all.committer.magic.enabled: ""true""
    fs.s3a.endpoint: http://s3.eu-west-1.amazonaws.com
    fs.s3a.connection.ssl.enabled: ""false""
  executor:
    nodeSelector:
      karpenter.sh/nodepool: default
    labels: *Labels
    serviceAccount: spark-operator-spark
    cores: 8
    coreRequest: ""7500m""
    instances: 1
    memory: ""40g""
  driver:
    nodeSelector:
      karpenter.sh/nodepool: ondemand
    labels: *Labels
    serviceAccount: spark-operator-spark
    cores: 4
    memory: ""16g""
    env:
      - name: DEPLOY_ENV
        value: qa
```


### Operating System

Airflow runs on EKS cluster and was deployed using Helm chart (version 1.16.0-dev)

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes  -- 8.3.4

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",captify-mkambur,2024-09-10 14:04:16+00:00,[],2024-12-07 16:27:17+00:00,2024-11-26 00:16:09+00:00,https://github.com/apache/airflow/issues/42132,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('area:providers', ''), ('pending-response', ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2340916814, 'issue_id': 2516577736, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 10, 14, 4, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2340925569, 'issue_id': 2516577736, 'author': 'captify-mkambur', 'body': ""Forgot to mention, I've tried to run more than one Spark Application. Thay fell with the same error."", 'created_at': datetime.datetime(2024, 9, 10, 14, 6, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2353279850, 'issue_id': 2516577736, 'author': 'harjeevanmaan', 'body': ""I'm guessing the issue lies within XCom push and pull. I would like to investigate further"", 'created_at': datetime.datetime(2024, 9, 16, 15, 42, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2354858993, 'issue_id': 2516577736, 'author': 'captify-mkambur', 'body': '@harjeevanmaan If you need any help in further investigation, I would be happy to assist.', 'created_at': datetime.datetime(2024, 9, 17, 8, 20, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2453358790, 'issue_id': 2516577736, 'author': 'gopidesupavan', 'body': '@captify-mkambur can you please check this latest release of k8s, do you still face issues?', 'created_at': datetime.datetime(2024, 11, 3, 9, 31, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2481696864, 'issue_id': 2516577736, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 11, 18, 0, 16, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499294824, 'issue_id': 2516577736, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 11, 26, 0, 16, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2525235945, 'issue_id': 2516577736, 'author': 'diogoaurelio', 'body': 'I also had exactly the same problem as raised in this issue, and [this workaround](https://github.com/apache/airflow/issues/39184#issuecomment-2199573270) solved it', 'created_at': datetime.datetime(2024, 12, 7, 16, 27, 15, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-10 14:04:20 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

captify-mkambur (Issue Creator) on (2024-09-10 14:06:43 UTC): Forgot to mention, I've tried to run more than one Spark Application. Thay fell with the same error.

harjeevanmaan on (2024-09-16 15:42:51 UTC): I'm guessing the issue lies within XCom push and pull. I would like to investigate further

captify-mkambur (Issue Creator) on (2024-09-17 08:20:26 UTC): @harjeevanmaan If you need any help in further investigation, I would be happy to assist.

gopidesupavan on (2024-11-03 09:31:06 UTC): @captify-mkambur can you please check this latest release of k8s, do you still face issues?

github-actions[bot] on (2024-11-18 00:16:48 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-11-26 00:16:08 UTC): This issue has been closed because it has not received response from the issue author.

diogoaurelio on (2024-12-07 16:27:15 UTC): I also had exactly the same problem as raised in this issue, and [this workaround](https://github.com/apache/airflow/issues/39184#issuecomment-2199573270) solved it

"
2515794723,issue,closed,completed,airflow.exceptions.AirflowException: No module named 'airflow.api.auth.backend.ldap_auth',"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

LDAP does not work

### What you think should happen instead?

_No response_

### How to reproduce

Deploy airflow on Docker

### Operating System

Ubuntu 22

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",manel00,2024-09-10 08:40:41+00:00,[],2024-10-02 02:12:26+00:00,2024-10-02 02:12:26+00:00,https://github.com/apache/airflow/issues/42127,"[('kind:bug', 'This is a clearly a bug'), ('area:auth', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2380631130, 'issue_id': 2515794723, 'author': 'kunaljubce', 'body': '@manel00 Can you please provide some details on this? \r\n\r\n* What is the exact error message? Simply mentioning ""LDAP does not work"" is not sufficient. \r\n* Can you provide the error trace or logs? \r\n* How can anyone reproduce it? You have mentioned ""Deploy Airflow on Docker"", which isn\'t sufficient. Any specific configs you are passing to setup LDAP?\r\n* What do you think should happen instead of the failure you\'re facing?', 'created_at': datetime.datetime(2024, 9, 28, 12, 54, 11, tzinfo=datetime.timezone.utc)}]","kunaljubce on (2024-09-28 12:54:11 UTC): @manel00 Can you please provide some details on this? 

* What is the exact error message? Simply mentioning ""LDAP does not work"" is not sufficient. 
* Can you provide the error trace or logs? 
* How can anyone reproduce it? You have mentioned ""Deploy Airflow on Docker"", which isn't sufficient. Any specific configs you are passing to setup LDAP?
* What do you think should happen instead of the failure you're facing?

"
2515342979,issue,open,,Vault Internal Client `aws_iam` auth type missing `role_id` and bad deprecation message in UI,"### Apache Airflow Provider(s)

hashicorp

### Versions of Apache Airflow Providers

3.7.1

### Apache Airflow version

2.9.3

### Operating System

Linux/UNIX

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

EKS 1.28

### What happened

With the above version of the provider, the `role_id` parameter is not correctly passed to the `iam_login` function of the hvac client when an IAM role is used to dynamically fetch temporary credentials. This causes a relative path not supported error as it ultimately causes a required parameter (`role_id`) to be missing from the login POST.

as seen [here](https://github.com/apache/airflow/blob/main/airflow/providers/hashicorp/_internal_client/vault_client.py#L327-L357):
```python
    def _auth_aws_iam(self, _client: hvac.Client) -> None:
        if self.key_id and self.secret_id:
            auth_args = {
                ""access_key"": self.key_id,
                ""secret_key"": self.secret_id,
                ""role"": self.role_id,
            }
        else:
            import boto3

            if self.assume_role_kwargs:
                sts_client = boto3.client(""sts"")
                credentials = sts_client.assume_role(**self.assume_role_kwargs)
                auth_args = {
                    ""access_key"": credentials[""Credentials""][""AccessKeyId""],
                    ""secret_key"": credentials[""Credentials""][""SecretAccessKey""],
                    ""session_token"": credentials[""Credentials""][""SessionToken""],
                }
            else:
                session = boto3.Session()
                credentials = session.get_credentials()
                auth_args = {
                    ""access_key"": credentials.access_key,
                    ""secret_key"": credentials.secret_key,
                    ""session_token"": credentials.token,
                }

        if self.auth_mount_point:
            auth_args[""mount_point""] = self.auth_mount_point

        _client.auth.aws.iam_login(**auth_args)
```

The `role_id` parameter makes it into the `auth_args` dict ONLY if a static key and secret access key are provided. Otherwise, temporary credentials are fetched using `sts` or `get_credentials()` and added to the `auth_args` dict, and later the `mount_point` is added, but `role_id` is ultimately missing.

This will always cause an issue when trying to auth to vault with 1aws_iam` using dynamic credentials since BOTH mount point and role id are required: see [here](https://github.com/hvac/hvac/blob/main/hvac/api/auth_methods/aws.py#L739-L790) if interested.

This was introduced by the following PR when support for this sort of dynamic credential usage was implemented (though probably never actually tested w/o a static key + access key): https://github.com/apache/airflow/pull/38536/files

---

Also, there is a bad message in the UI that the role id parameter for the vault connection is deprecated, which is only true for the `approle` auth method... it is REQUIRED for the `aws_iam` auth method. Since a deprecation warning will be thrown when this parameter is used for the `approle` auth method anyway, I suggect removing that text from the UI entirely.

All of these are very simple changes and I am willing to submit a PR... the fix has already been tested in a hotfix environment.

### What you think should happen instead

There should be no relative path error thrown when dynamic credentials are used. The `role_id` parameter should be added to the `auth_args` dict and login should succeed.

### How to reproduce

Try to instantiate a VaultHook or Vault Secrets Backend using `aws_iam` auth and do not provide static access credentials. If all of the config is correct, you will see a relative path error in the logs instead of successful auth to vault.

This requires both and airflow setup and a vault namespace configured with access provisioned through iam.

### Anything else

This problem occurs every time. Again, we have the fix in out hotfix environment and are willing to submit the fix.

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",richard-iovanisci,2024-09-10 04:00:00+00:00,[],2024-09-19 11:12:07+00:00,,https://github.com/apache/airflow/issues/42125,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('provider:hashicorp', 'Hashicorp provider related issues')]","[{'comment_id': 2339560165, 'issue_id': 2515342979, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 10, 4, 0, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2341752200, 'issue_id': 2515342979, 'author': 'richard-iovanisci', 'body': 'PR opened for fix: https://github.com/apache/airflow/pull/42134', 'created_at': datetime.datetime(2024, 9, 10, 18, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2341783036, 'issue_id': 2515342979, 'author': 'richard-iovanisci', 'body': 'Perhaps this example in the documentation for the VaultBackend should be updated to include `role_id` when building auth dict: https://airflow.apache.org/docs/apache-airflow-providers-hashicorp/stable/secrets-backends/hashicorp-vault.html#vault-authentication-with-aws-assume-role-sts', 'created_at': datetime.datetime(2024, 9, 10, 18, 56, 41, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-10 04:00:03 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

richard-iovanisci (Issue Creator) on (2024-09-10 18:43:00 UTC): PR opened for fix: https://github.com/apache/airflow/pull/42134

richard-iovanisci (Issue Creator) on (2024-09-10 18:56:41 UTC): Perhaps this example in the documentation for the VaultBackend should be updated to include `role_id` when building auth dict: https://airflow.apache.org/docs/apache-airflow-providers-hashicorp/stable/secrets-backends/hashicorp-vault.html#vault-authentication-with-aws-assume-role-sts

"
2515056984,issue,open,,Document capturing hook-level lineage,"https://github.com/apache/airflow/pull/41482 was released. We should add documentation around it specifically some examples

**Success criteria**:
- How does a hook define this? do we have any examples?
- Is anything else covered than `ObjectStorage`
- do we only pull hook lineage if the operator methods don't exist? or do we do both, if both exist?
- do these get transmitted in the task complete event?

",kaxil,2024-09-09 22:46:39+00:00,['mobuchowski'],2024-09-09 22:48:58+00:00,,https://github.com/apache/airflow/issues/42123,"[('kind:documentation', ''), ('provider:openlineage', 'AIP-53'), ('AIP-62', 'Tasks tracking implementation of AIP-62 Getting Lineage from Hook Instrumentation')]",[],
2514449121,issue,closed,completed,Xcom view broken for non-JSON values,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.1

### What happened?

Non-JSON values are broken in the Xcom UI [screenshot attached]

Root cause is [this line](https://github.com/jscheffl/airflow/blob/5f5039a6531d9366ac7ffc9a05492b7a93bc8f87/airflow/www/static/js/api/useTaskXcom.ts#L66) from https://github.com/apache/airflow/pull/40640. Setting `stringify: false` results in JSON encoding which results in `TypeError: keys must be str, int, float, bool or None, not tuple` exception from `GET <Airflow URL>/api/v1/dags/<DAG>/dagRuns/<Run ID>/taskInstances/<Task ID>/xcomEntries/<Xcom name>?stringify=false` endpoint.

The PR's intention is to make the whole view JSON, so not sure if the intention is to error on non-JSON Xcom values, or if it should be updated to have clean fallback logic.

![Pasted Graphic 22](https://github.com/user-attachments/assets/5451cdb0-3475-4524-b504-ade4df4a3a98)

[This is a dupe of #41981]

### What you think should happen instead?

_No response_

### How to reproduce

Create Xcom value that is non-JSON serializable (e.g. `{('201009_NB502104_0421_AHJY23BGXG (SEQ_WF: 138898)', None): 82359}`) and then try to view in UI Xcom tab

### Operating System

Linux (Ubuntu 22.04)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jkramer-ginkgo,2024-09-09 16:54:04+00:00,[],2024-09-25 21:56:59+00:00,2024-09-25 21:56:59+00:00,https://github.com/apache/airflow/issues/42117,"[('kind:bug', 'This is a clearly a bug'), ('area:webserver', 'Webserver related Issues'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2361924313, 'issue_id': 2514449121, 'author': 'jscheffl', 'body': 'I tried to reproduce this if it is fixed with another fix I did in 2.10.2(rc1) but failed creating the XCom.\r\nWhen using in code:\r\n```\r\nti.xcom_push(""non_json"", {(\'201009_NB502104_0421_AHJY23BGXG (SEQ_WF: 138898)\', None): 82359})\r\n```\r\nI get the error `TypeError: keys must be str, int, float, bool or None, not tuple` already during task execution,\r\n\r\nHow did you generate the data or can you please paste a piece of the example how you generated it?\r\nOr even better, can you check if 2.10.2rc1 is fixing it already?', 'created_at': datetime.datetime(2024, 9, 19, 18, 47, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2363748517, 'issue_id': 2514449121, 'author': 'jkramer-ginkgo', 'body': 'Maybe there\'s some diff between our configs and/or DAG-style but this xcom is properly being set and used. The dag is set up using `PythonOperator`s and `chain()`s for dependency management.\r\n\r\n[example]\r\n```\r\ncontext[""ti""].xcom_push(\r\n    key=""key"", value={(\'201009_NB502104_0421_AHJY23BGXG (SEQ_WF: 138898)\', None): 82359}\r\n)\r\n```', 'created_at': datetime.datetime(2024, 9, 20, 13, 32, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2363756258, 'issue_id': 2514449121, 'author': 'jkramer-ginkgo', 'body': 'Yes, we have `enable_xcom_pickling = True` set. As this is a supported config, it would imply non-JSON serializable Xcom values should be supported in the UI.', 'created_at': datetime.datetime(2024, 9, 20, 13, 36, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2364571178, 'issue_id': 2514449121, 'author': 'jscheffl', 'body': 'Oh yeah. With `AIRFLOW__CORE__ENABLE_XCOM_PICKLING=True` this is a problem. Probably I/we did not consider this when updating the UI. This is (now) clearly a bug.', 'created_at': datetime.datetime(2024, 9, 20, 20, 29, 15, tzinfo=datetime.timezone.utc)}]","jscheffl on (2024-09-19 18:47:25 UTC): I tried to reproduce this if it is fixed with another fix I did in 2.10.2(rc1) but failed creating the XCom.
When using in code:
```
ti.xcom_push(""non_json"", {('201009_NB502104_0421_AHJY23BGXG (SEQ_WF: 138898)', None): 82359})
```
I get the error `TypeError: keys must be str, int, float, bool or None, not tuple` already during task execution,

How did you generate the data or can you please paste a piece of the example how you generated it?
Or even better, can you check if 2.10.2rc1 is fixing it already?

jkramer-ginkgo (Issue Creator) on (2024-09-20 13:32:15 UTC): Maybe there's some diff between our configs and/or DAG-style but this xcom is properly being set and used. The dag is set up using `PythonOperator`s and `chain()`s for dependency management.

[example]
```
context[""ti""].xcom_push(
    key=""key"", value={('201009_NB502104_0421_AHJY23BGXG (SEQ_WF: 138898)', None): 82359}
)
```

jkramer-ginkgo (Issue Creator) on (2024-09-20 13:36:13 UTC): Yes, we have `enable_xcom_pickling = True` set. As this is a supported config, it would imply non-JSON serializable Xcom values should be supported in the UI.

jscheffl on (2024-09-20 20:29:15 UTC): Oh yeah. With `AIRFLOW__CORE__ENABLE_XCOM_PICKLING=True` this is a problem. Probably I/we did not consider this when updating the UI. This is (now) clearly a bug.

"
2514145821,issue,closed,completed,DAGs are missing and deleted in Airflow 2.10.1,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

2.10.1

### What happened?

I updated to Airflow 2.10.1 and the DAGs in my UI started to disappear. I checked the scheduler logs and it showed that the DAGs were not being found and were being deleted. I quickly reverted back to the working 2.10.0 image after that and I don't have the specific logs.

I am syncing the src folder in my repo as the dags_folder which also contains common scripts, hooks, operators, etc. The plugins_folder is backed into the image and only contains things which don't change often, like XCom backend.

Official Helm chart values:
```
config:
  core:
    dags_folder: /opt/airflow/dags/repo/src
    plugins_folder: /opt/airflow/plugins

dags:
  gitSync:
    enabled: true
    repo: https://gitlab.domain.com/dags.git
    branch: main
    rev: HEAD
    depth: 1
    subPath: src
```

This is my dags_folder structure:

```
src/
├── common
│   ├── __init__,py
├── dags
│   ├── __init__.py
│   ├── account_a
│   │   ├── example_a_dag.py
│   │   └── example_b_dag.py
│   └── account_x
├── hooks
│   ├── __init__.py
├── notifiers
│   ├── __init__.py
├── operators
│   ├── __init__.py
├── projects
│   ├── __init__.py
│   ├── account_a
│   │   ├── transform.py
│   │   └── schemas.py
│   └── account_x
└── sensors
    ├── __init__.py
```

My .airflowignore is in the src folder:

```
common/
notifiers/
operators/
sensors/
hooks/
projects/
```



### What you think should happen instead?

The DAGs should still be discovered between minor Airflow releases since nothing else changed.

### How to reproduce

Hopefully the structure of my setup above suffices.

I am using the Airflow constraints file when installing extras. linux/amd64 is the target platform used and 2.10.1 is the Airflow version I had trouble with.

```
    - docker build
      --file Dockerfile
      --pull
      --platform ""${TARGET_PLATFORM}""
      --cache-from ""${RELEASE_IMAGE}""
      --build-arg BUILDKIT_INLINE_CACHE=1
      --build-arg PYTHON_BASE_IMAGE=""python:${PYTHON_VERSION}-slim-${DEBIAN_VERSION}""
      --build-arg AIRFLOW_VERSION=""${AIRFLOW_VERSION}""
      --build-arg AIRFLOW_USE_UV=""true""
      --build-arg AIRFLOW_UV_VERSION=""${AIRFLOW_UV_VERSION}""
      --build-arg AIRFLOW_EXTRAS=""async,docker,cncf.kubernetes,ftp,google,google_auth,http,microsoft.azure,odbc,postgres,sftp,ssh,statsd,openlineage""
      --build-arg AIRFLOW_CONSTRAINTS_REFERENCE=""${CONSTRAINTS_VERSION}""
      --build-arg INSTALL_MYSQL_CLIENT=""false""
      --build-arg INSTALL_MSSQL_CLIENT=""true""
      --build-arg INSTALL_POSTGRES_CLIENT=""true""
      --build-arg UPGRADE_TO_NEWER_DEPENDENCIES=""true""
      --build-arg ADDITIONAL_RUNTIME_APT_DEPS=""git graphviz procps vim odbc-postgresql python3-pybind11 python3-pyodbc libgeos-dev""
      --tag ""${RELEASE_IMAGE}""
      "".""
```

### Operating System

Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",ldacey,2024-09-09 14:33:28+00:00,[],2025-02-06 19:45:05+00:00,2024-09-27 08:30:28+00:00,https://github.com/apache/airflow/issues/42111,"[('kind:bug', 'This is a clearly a bug'), ('area:helm-chart', 'Airflow Helm Chart'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('area:DAG-processing', '')]","[{'comment_id': 2338303854, 'issue_id': 2514145821, 'author': 'jcrobak', 'body': ""I'm experiencing this issue with 2.10.1, too. I was able to add some logging and it appears that https://github.com/apache/airflow/pull/41433 is the culprit. If the value of `airflow config get-value core dags_folder` is a symlink, then the parameter `dag_directory` to `deactivate_stale_dags` will be the target of the symlink. This means that `os.path.commonpath(...)` doesn't find a match."", 'created_at': datetime.datetime(2024, 9, 9, 14, 37, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2338321721, 'issue_id': 2514145821, 'author': 'potiuk', 'body': ""> I'm experiencing this issue with 2.10.1, too. I was able to add some logging and it appears that https://github.com/apache/airflow/pull/41433 is the culprit. If the value of airflow config get-value core dags_folder is a symlink, then the parameter dag_directory to deactivate_stale_dags will be the target of the symlink. This means that os.path.commonpath(...) doesn't find a match.\r\n\r\nAh yeah . That would definitely explain it !"", 'created_at': datetime.datetime(2024, 9, 9, 14, 43, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2338322904, 'issue_id': 2514145821, 'author': 'potiuk', 'body': 'cc: @utkarsharma2 @ephraimbuddy  -> I think we need a quick 2.10.2', 'created_at': datetime.datetime(2024, 9, 9, 14, 43, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2338338314, 'issue_id': 2514145821, 'author': 'MCubek', 'body': 'Also experiencing this issue with 2.10.1 instance deployed with the helm chart and using gitsync. \r\nSymptoms include dags going missing while refreshing webserver ui and then, being unable to trigger dag runs and kubernetes executor task faults.\r\nWebserver reported params missing in its stack trace, and the webserver ui reporting dags missing from dag bag when browsing running dags.\r\n\r\nI have since reverted back to 2.10.0 after the issues my deployment experienced during the weekend, but will reproduce and send the full error logs if necessary.', 'created_at': datetime.datetime(2024, 9, 9, 14, 50, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2338343567, 'issue_id': 2514145821, 'author': 'potiuk', 'body': 'The symlink is the most likely cause I think we will take it from there (thanks @MCubek @jcrobak @ldacey ) - I think it will mean quick 2.10.2 - so we will appreciate -once we fix it, if you test release candidates we will produce.', 'created_at': datetime.datetime(2024, 9, 9, 14, 52, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2338358642, 'issue_id': 2514145821, 'author': 'ephraimbuddy', 'body': '> cc: @utkarsharma2 @ephraimbuddy -> I think we need a quick 2.10.2\r\n\r\nYeah. Looking into this', 'created_at': datetime.datetime(2024, 9, 9, 14, 58, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2338425708, 'issue_id': 2514145821, 'author': 'jcrobak', 'body': 'here\'s a quick repro with docker-compose, in case you need it:\r\n\r\n```\r\ncurl -LfO \'https://airflow.apache.org/docs/apache-airflow/2.10.1/docker-compose.yaml\' \r\n\r\nsed -e \'/\\/opt\\/airflow\\/dags/ s/^#*/#/\' < docker-compose.yaml\r\n\r\nmkdir -p ./dags ./logs ./plugins ./config\r\necho -e ""AIRFLOW_UID=$(id -u)"" >| .env \r\n\r\nmkdir dags/example\r\n\r\ncat << EOF >>| dags/example/airflow_dag.py\r\nfrom datetime import datetime, timedelta\r\n\r\nfrom airflow.models.dag import DAG\r\nfrom airflow.operators.bash import BashOperator\r\n\r\nwith DAG(\r\n    ""joe-tutorial"",\r\n    description=""A simple tutorial DAG"",\r\n    schedule=timedelta(days=1),\r\n    start_date=datetime(2021, 1, 1),\r\n    catchup=False,\r\n    tags=[""example""],\r\n) as dag:\r\n            t1 = BashOperator(\r\n        task_id=""print_date"",\r\n        bash_command=""date"",\r\n    )\r\n\r\nEOF\r\n\r\n\r\ncat << EOF >> Dockerfile\r\nFROM apache/airflow:2.10.1-python3.12\r\nCOPY --chown=airflow:root dags /opt/dags\r\nRUN rmdir /opt/airflow/dags && ln -s /opt/dags/ /opt/airflow/dags\r\n\r\nEOF\r\n\r\n\r\ndocker build . -t airflow:2.10.1-repro\r\necho -e ""AIRFLOW_IMAGE_NAME=airflow:2.10.1-repro"" >> .env \r\n\r\n\r\ndocker-compose up\r\n```\r\n\r\neventually you\'ll see a log line like:\r\n\r\n```\r\nairflow-scheduler-1  | [2024-09-09T15:23:14.923+0000] {manager.py:548} INFO - DAG joe-tutorial is missing and will be deactivated.\r\nairflow-scheduler-1  | [2024-09-09T15:23:14.927+0000] {manager.py:560} INFO - Deactivated 1 DAGs which are no longer present in file.\r\nairflow-scheduler-1  | [2024-09-09T15:23:14.933+0000] {manager.py:564} INFO - Deleted DAG joe-tutorial in serialized_dag table\r\n```', 'created_at': datetime.datetime(2024, 9, 9, 15, 26, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2338473616, 'issue_id': 2514145821, 'author': 'raphaelauv', 'body': 'same experience with 2.10.1 and 1 dagprocessor\r\n\r\n[Screencast from 09-09-2024 17:43:19.webm](https://github.com/user-attachments/assets/a36e5f0b-a976-4dfa-801d-ebc5f2e350dd)', 'created_at': datetime.datetime(2024, 9, 9, 15, 47, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2338609271, 'issue_id': 2514145821, 'author': 'kand617', 'body': 'I recently upgraded to 2.8 and am seeing a similar thing. \r\nDAG count suddenly goes to 0 and then slowly builds back up... Then after sometime back to 0...\r\nWas just going to upgrade to 2.10 thinking it maybe better.', 'created_at': datetime.datetime(2024, 9, 9, 16, 52, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2338610864, 'issue_id': 2514145821, 'author': 'zachliu', 'body': 'Just upgraded from 2.9.3 to 2.10.1\r\n\r\nIn my case (not using `gitSync`, DAG files are synced to a persisted EFS volume using the `rsync` cmd), DAGs are just disappearing from the UI. I haven\'t seen the ""Deleted"" logs...yet', 'created_at': datetime.datetime(2024, 9, 9, 16, 53, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2340119009, 'issue_id': 2514145821, 'author': 'rflaquer-w2m', 'body': 'Same problem here. We were just doing the environment upgrade in PRO, two weeks ago we did it in PRE without problems. Then I saw that the process installed 2.10.1 instead of 2.10 and thank goodness I found this ticket. I forced the reinstallation of Airflow 2.10 and with that I have corrected the error:\r\n\r\npip install --force-reinstall -v ""apache-airflow==2.10""', 'created_at': datetime.datetime(2024, 9, 10, 9, 17, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2340477161, 'issue_id': 2514145821, 'author': 'banachkx', 'body': ""I think we have a similar problem at 2.10 - we did an upgrade to 2.10 on Friday. At some point after reloading the UI the dags disappeared. I rebuilt the whole docker from scratch, the whole base and volumes from scratch, but it didn't help. In the logs it shows that it found the dags, but it doesn't show them in the UI, not even the example ones."", 'created_at': datetime.datetime(2024, 9, 10, 11, 58, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2341495088, 'issue_id': 2514145821, 'author': 'zachliu', 'body': 'I think there are actually two issues here with the same ""disappearing dags"" symptom\r\n1. The symlink issue mentioned above. you\'d also see the ""Deactivated..."" or ""Deleted..."" logs from the scheduler, which i don\'t.\r\n2. A funny [UI upgrade](https://github.com/apache/airflow/pull/39701) where the `Running` and `Failed` buttons are now persisted between UI re-loads. In other words, they act like a checkbox now. i need to ""uncheck"" it to see all DAGs, which is not the case in `2.9.3`\r\n   ![2024-09-10_12-50](https://github.com/user-attachments/assets/ad6bb997-4e50-4184-8401-ee992a8046af)\r\n   ![2024-09-10_12-55](https://github.com/user-attachments/assets/82ba3e78-979f-4102-849f-7ad7525a9d02)\r\n\r\nSince i don\'t use symlink on the dags folder, i belong to the second category :sweat_smile:', 'created_at': datetime.datetime(2024, 9, 10, 17, 0, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2343146235, 'issue_id': 2514145821, 'author': 'smsm1-ito', 'body': '> 2. A funny [UI upgrade](https://github.com/apache/airflow/pull/39701) where the `Running` and `Failed` buttons are now persisted between UI re-loads. In other words, they act like a checkbox now. i need to ""uncheck"" it to see all DAGs, which is not the case in `2.9.3`\r\n\r\nThe second issue should be dealt with in a separate ticket, though I think it\'s an expected behaviour change, however I have hit recursive redirect issue when editing the url to remove the running param.', 'created_at': datetime.datetime(2024, 9, 11, 9, 38, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2343778764, 'issue_id': 2514145821, 'author': 'zachliu', 'body': '> > 2. A funny [UI upgrade](https://github.com/apache/airflow/pull/39701) where the `Running` and `Failed` buttons are now persisted between UI re-loads. In other words, they act like a checkbox now. i need to ""uncheck"" it to see all DAGs, which is not the case in `2.9.3`\r\n> \r\n> The second issue should be dealt with in a separate ticket, though I think it\'s an expected behaviour change, however I have hit recursive redirect issue when editing the url to remove the running param.\r\n\r\nI was joking. It\'s not an issue it\'s a feature. No ticket needed.\r\nI hereby solemnly declare that I accept this new feature with all me heart.', 'created_at': datetime.datetime(2024, 9, 11, 14, 6, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2343842204, 'issue_id': 2514145821, 'author': 'Bhargavjd', 'body': 'I upgraded to 2.10.1, the DAGs suddenly disappear from UIand sometime shows the missingdagbag error and if I refresh couple of times the DAGs are appearing again', 'created_at': datetime.datetime(2024, 9, 11, 14, 27, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2346303612, 'issue_id': 2514145821, 'author': 'alaturqua', 'body': 'We just upgraded using official helm chart and version 2.10.1 and dags disappear as well.\r\n\r\nScheduler deletes and deactivates dags constantly even though dags are available and synced via GitSync.', 'created_at': datetime.datetime(2024, 9, 12, 13, 35, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2356869156, 'issue_id': 2514145821, 'author': 'knoguchi', 'body': 'The problem is easily reproducible using `airflow standalone` with `dags_folder` config with a symlink that points a regular directory.', 'created_at': datetime.datetime(2024, 9, 17, 20, 33, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2356977288, 'issue_id': 2514145821, 'author': 'thesuperzapper', 'body': '@ephraimbuddy I see you just cut an [RC for 2.10.2](\r\nhttps://github.com/apache/airflow/releases/tag/2.10.2rc1), was this absolutely critical breaking issue fixed in it?\r\n\r\nIf not, then we need to get a fix for this before cutting the final 2.10.2.', 'created_at': datetime.datetime(2024, 9, 17, 21, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2356983279, 'issue_id': 2514145821, 'author': 'ephraimbuddy', 'body': ""> @ephraimbuddy I see you just cut an [RC for 2.10.2](https://github.com/apache/airflow/releases/tag/2.10.2rc1), was this absolutely critical breaking issue fixed in it?\r\n> \r\n> If not, then we need to get a fix for this before cutting the final 2.10.2.\r\n\r\nYes. It's the primary reason for this coming release. You can help to test it before the release."", 'created_at': datetime.datetime(2024, 9, 17, 21, 39, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2363237134, 'issue_id': 2514145821, 'author': 'smsm1-ito', 'body': ""I've rolled the 2.10.2rc1 Docker image out to our internal environment and I'm no longer seeing this issue. All DAGs are loading as expected as per previous releases."", 'created_at': datetime.datetime(2024, 9, 20, 9, 7, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2373185888, 'issue_id': 2514145821, 'author': 'Shlomixg', 'body': 'Still occurs in 2.10.2', 'created_at': datetime.datetime(2024, 9, 25, 6, 54, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2374643132, 'issue_id': 2514145821, 'author': 'thesuperzapper', 'body': '> Still occurs in 2.10.2\n\n@Shlomixg can you clarify how you are using airflow?\n\nE.g. are you using the community helm chart with git-sync, or something else?', 'created_at': datetime.datetime(2024, 9, 25, 16, 58, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2374670124, 'issue_id': 2514145821, 'author': 'alaturqua', 'body': 'It is working for us now. Using official helm chart with gitsync and version 2.10.2.', 'created_at': datetime.datetime(2024, 9, 25, 17, 9, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2376413874, 'issue_id': 2514145821, 'author': 'Shlomixg', 'body': ""> > Still occurs in 2.10.2\r\n> \r\n> @Shlomixg can you clarify how you are using airflow?\r\n> \r\n> E.g. are you using the community helm chart with git-sync, or something else?\r\n\r\nWe're using third-party helm chart with git-sync."", 'created_at': datetime.datetime(2024, 9, 26, 9, 21, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2378553552, 'issue_id': 2514145821, 'author': 'ephraimbuddy', 'body': ""> > > Still occurs in 2.10.2\r\n> > \r\n> > \r\n> > @Shlomixg can you clarify how you are using airflow?\r\n> > E.g. are you using the community helm chart with git-sync, or something else?\r\n> \r\n> We're using third-party helm chart with git-sync.\r\n\r\nCan you reserialize your dags: `airflow dags reserialize`"", 'created_at': datetime.datetime(2024, 9, 27, 7, 7, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2378599474, 'issue_id': 2514145821, 'author': 'tukraus', 'body': 'Hi @ephraimbuddy, for us the problem seems fixed as well (using official helm chart + git sync). Could you clarify why this issue is still open and linked to the 2.10.3 milestone? Is another fix needed for similar problem in a different setup, or are you just awaiting several confirmations before closing this issue? Just to make up our minds on whether to roll out 2.10.2 or still wait a bit... Thanks!', 'created_at': datetime.datetime(2024, 9, 27, 7, 30, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2378740800, 'issue_id': 2514145821, 'author': 'ephraimbuddy', 'body': ""> > > Still occurs in 2.10.2\r\n> > \r\n> > \r\n> > @Shlomixg can you clarify how you are using airflow?\r\n> > E.g. are you using the community helm chart with git-sync, or something else?\r\n> \r\n> We're using third-party helm chart with git-sync.\r\n\r\nCan you create another issue with the description of how to reproduce the issue? Closing this as completed"", 'created_at': datetime.datetime(2024, 9, 27, 8, 30, 28, tzinfo=datetime.timezone.utc)}]","jcrobak on (2024-09-09 14:37:33 UTC): I'm experiencing this issue with 2.10.1, too. I was able to add some logging and it appears that https://github.com/apache/airflow/pull/41433 is the culprit. If the value of `airflow config get-value core dags_folder` is a symlink, then the parameter `dag_directory` to `deactivate_stale_dags` will be the target of the symlink. This means that `os.path.commonpath(...)` doesn't find a match.

potiuk on (2024-09-09 14:43:29 UTC): Ah yeah . That would definitely explain it !

potiuk on (2024-09-09 14:43:53 UTC): cc: @utkarsharma2 @ephraimbuddy  -> I think we need a quick 2.10.2

MCubek on (2024-09-09 14:50:07 UTC): Also experiencing this issue with 2.10.1 instance deployed with the helm chart and using gitsync. 
Symptoms include dags going missing while refreshing webserver ui and then, being unable to trigger dag runs and kubernetes executor task faults.
Webserver reported params missing in its stack trace, and the webserver ui reporting dags missing from dag bag when browsing running dags.

I have since reverted back to 2.10.0 after the issues my deployment experienced during the weekend, but will reproduce and send the full error logs if necessary.

potiuk on (2024-09-09 14:52:15 UTC): The symlink is the most likely cause I think we will take it from there (thanks @MCubek @jcrobak @ldacey ) - I think it will mean quick 2.10.2 - so we will appreciate -once we fix it, if you test release candidates we will produce.

ephraimbuddy on (2024-09-09 14:58:26 UTC): Yeah. Looking into this

jcrobak on (2024-09-09 15:26:02 UTC): here's a quick repro with docker-compose, in case you need it:

```
curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.10.1/docker-compose.yaml' 

sed -e '/\/opt\/airflow\/dags/ s/^#*/#/' < docker-compose.yaml

mkdir -p ./dags ./logs ./plugins ./config
echo -e ""AIRFLOW_UID=$(id -u)"" >| .env 

mkdir dags/example

cat << EOF >>| dags/example/airflow_dag.py
from datetime import datetime, timedelta

from airflow.models.dag import DAG
from airflow.operators.bash import BashOperator

with DAG(
    ""joe-tutorial"",
    description=""A simple tutorial DAG"",
    schedule=timedelta(days=1),
    start_date=datetime(2021, 1, 1),
    catchup=False,
    tags=[""example""],
) as dag:
            t1 = BashOperator(
        task_id=""print_date"",
        bash_command=""date"",
    )

EOF


cat << EOF >> Dockerfile
FROM apache/airflow:2.10.1-python3.12
COPY --chown=airflow:root dags /opt/dags
RUN rmdir /opt/airflow/dags && ln -s /opt/dags/ /opt/airflow/dags

EOF


docker build . -t airflow:2.10.1-repro
echo -e ""AIRFLOW_IMAGE_NAME=airflow:2.10.1-repro"" >> .env 


docker-compose up
```

eventually you'll see a log line like:

```
airflow-scheduler-1  | [2024-09-09T15:23:14.923+0000] {manager.py:548} INFO - DAG joe-tutorial is missing and will be deactivated.
airflow-scheduler-1  | [2024-09-09T15:23:14.927+0000] {manager.py:560} INFO - Deactivated 1 DAGs which are no longer present in file.
airflow-scheduler-1  | [2024-09-09T15:23:14.933+0000] {manager.py:564} INFO - Deleted DAG joe-tutorial in serialized_dag table
```

raphaelauv on (2024-09-09 15:47:13 UTC): same experience with 2.10.1 and 1 dagprocessor

[Screencast from 09-09-2024 17:43:19.webm](https://github.com/user-attachments/assets/a36e5f0b-a976-4dfa-801d-ebc5f2e350dd)

kand617 on (2024-09-09 16:52:52 UTC): I recently upgraded to 2.8 and am seeing a similar thing. 
DAG count suddenly goes to 0 and then slowly builds back up... Then after sometime back to 0...
Was just going to upgrade to 2.10 thinking it maybe better.

zachliu on (2024-09-09 16:53:37 UTC): Just upgraded from 2.9.3 to 2.10.1

In my case (not using `gitSync`, DAG files are synced to a persisted EFS volume using the `rsync` cmd), DAGs are just disappearing from the UI. I haven't seen the ""Deleted"" logs...yet

rflaquer-w2m on (2024-09-10 09:17:10 UTC): Same problem here. We were just doing the environment upgrade in PRO, two weeks ago we did it in PRE without problems. Then I saw that the process installed 2.10.1 instead of 2.10 and thank goodness I found this ticket. I forced the reinstallation of Airflow 2.10 and with that I have corrected the error:

pip install --force-reinstall -v ""apache-airflow==2.10""

banachkx on (2024-09-10 11:58:13 UTC): I think we have a similar problem at 2.10 - we did an upgrade to 2.10 on Friday. At some point after reloading the UI the dags disappeared. I rebuilt the whole docker from scratch, the whole base and volumes from scratch, but it didn't help. In the logs it shows that it found the dags, but it doesn't show them in the UI, not even the example ones.

zachliu on (2024-09-10 17:00:31 UTC): I think there are actually two issues here with the same ""disappearing dags"" symptom
1. The symlink issue mentioned above. you'd also see the ""Deactivated..."" or ""Deleted..."" logs from the scheduler, which i don't.
2. A funny [UI upgrade](https://github.com/apache/airflow/pull/39701) where the `Running` and `Failed` buttons are now persisted between UI re-loads. In other words, they act like a checkbox now. i need to ""uncheck"" it to see all DAGs, which is not the case in `2.9.3`
   ![2024-09-10_12-50](https://github.com/user-attachments/assets/ad6bb997-4e50-4184-8401-ee992a8046af)
   ![2024-09-10_12-55](https://github.com/user-attachments/assets/82ba3e78-979f-4102-849f-7ad7525a9d02)

Since i don't use symlink on the dags folder, i belong to the second category :sweat_smile:

smsm1-ito on (2024-09-11 09:38:17 UTC): The second issue should be dealt with in a separate ticket, though I think it's an expected behaviour change, however I have hit recursive redirect issue when editing the url to remove the running param.

zachliu on (2024-09-11 14:06:52 UTC): I was joking. It's not an issue it's a feature. No ticket needed.
I hereby solemnly declare that I accept this new feature with all me heart.

Bhargavjd on (2024-09-11 14:27:57 UTC): I upgraded to 2.10.1, the DAGs suddenly disappear from UIand sometime shows the missingdagbag error and if I refresh couple of times the DAGs are appearing again

alaturqua on (2024-09-12 13:35:10 UTC): We just upgraded using official helm chart and version 2.10.1 and dags disappear as well.

Scheduler deletes and deactivates dags constantly even though dags are available and synced via GitSync.

knoguchi on (2024-09-17 20:33:07 UTC): The problem is easily reproducible using `airflow standalone` with `dags_folder` config with a symlink that points a regular directory.

thesuperzapper on (2024-09-17 21:35:00 UTC): @ephraimbuddy I see you just cut an [RC for 2.10.2](
https://github.com/apache/airflow/releases/tag/2.10.2rc1), was this absolutely critical breaking issue fixed in it?

If not, then we need to get a fix for this before cutting the final 2.10.2.

ephraimbuddy on (2024-09-17 21:39:45 UTC): Yes. It's the primary reason for this coming release. You can help to test it before the release.

smsm1-ito on (2024-09-20 09:07:47 UTC): I've rolled the 2.10.2rc1 Docker image out to our internal environment and I'm no longer seeing this issue. All DAGs are loading as expected as per previous releases.

Shlomixg on (2024-09-25 06:54:38 UTC): Still occurs in 2.10.2

thesuperzapper on (2024-09-25 16:58:27 UTC): @Shlomixg can you clarify how you are using airflow?

E.g. are you using the community helm chart with git-sync, or something else?

alaturqua on (2024-09-25 17:09:46 UTC): It is working for us now. Using official helm chart with gitsync and version 2.10.2.

Shlomixg on (2024-09-26 09:21:03 UTC): We're using third-party helm chart with git-sync.

ephraimbuddy on (2024-09-27 07:07:47 UTC): Can you reserialize your dags: `airflow dags reserialize`

tukraus on (2024-09-27 07:30:37 UTC): Hi @ephraimbuddy, for us the problem seems fixed as well (using official helm chart + git sync). Could you clarify why this issue is still open and linked to the 2.10.3 milestone? Is another fix needed for similar problem in a different setup, or are you just awaiting several confirmations before closing this issue? Just to make up our minds on whether to roll out 2.10.2 or still wait a bit... Thanks!

ephraimbuddy on (2024-09-27 08:30:28 UTC): Can you create another issue with the description of how to reproduce the issue? Closing this as completed

"
2514072996,issue,open,,Mapped tasks marked as failed on scheduler restart,"### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

### Observed behavior
some task instances are marked as failed without any log, as if they never started. When clearing the DAG run it succeeds so it seems like a temporary issue. It doesn't trigger the on_failure_callback which makes it very tricky to identify and fix in a timely manner.

![364865428-c0821a7b-62a0-4acf-a735-b586f824a099](https://github.com/user-attachments/assets/677f8c40-d8c8-4eed-8410-be8a8c815a0a)

![364867006-85507765-6e38-4572-bc33-72364cbb1607](https://github.com/user-attachments/assets/beaef657-9354-408a-b168-1563901656fd)

### Potential causes
We see a time correlation with scheduler pods termination, as part of deployment change (i.e. someone changed a custom operator or a module). Both pods were terminated around the time the DAG run marked some of the tasks as failed- i see that the last successful TIs are right after the pods termination. I do see however that 2 new pods were initialized ~5 mins before the ""old"" pods were terminated. Which should be enough time for them to adopt the newly orphaned tasks. Maybe I don't understand the expected behavior correctly.

The pods are constantly restarted and i discover this in delay (since there's no indication) so I don't have to the logs of the new schedulers but i can try to collect them if you think this is indeed a possible cause for this issue.

Helm deployment strategy (2 pods):
```
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
```


We run on  Airflow v2.9.3 with  Kubenetes executor

### What you think should happen instead?

The tasks instances should have been executed 

### How to reproduce

I wasn't able to reproduce it by simply restarting the schedulers when a DAG with Mapped Tasks is running..

### Operating System

Debian GNU/Linux 12

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",michaelimas1,2024-09-09 14:04:26+00:00,[],2025-01-10 19:12:42+00:00,,https://github.com/apache/airflow/issues/42107,"[('kind:bug', 'This is a clearly a bug'), (""Can't Reproduce"", 'The problem cannot be reproduced'), ('area:core', ''), ('area:dynamic-task-mapping', 'AIP-42'), ('needs-triage', ""label for new issues that we didn't triage yet""), ('affected_version:2.9', 'Issues Reported for 2.9')]","[{'comment_id': 2338220140, 'issue_id': 2514072996, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 9, 14, 4, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339379582, 'issue_id': 2514072996, 'author': 'kand617', 'body': 'Im on 2.8.3 and can confirm similar behaviour. Suddenly I see a large chunk of scheduled map tasks be marked for failure.\r\nNo logs or anything.', 'created_at': datetime.datetime(2024, 9, 10, 0, 26, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2351363876, 'issue_id': 2514072996, 'author': 'eladkal', 'body': 'It will be difficult to debug without reproduce steps (including example dag)', 'created_at': datetime.datetime(2024, 9, 15, 4, 43, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2352042421, 'issue_id': 2514072996, 'author': 'michaelimas1', 'body': ""@eladkal thanks, I'll keep trying to reproduce and update here if i get to it.\n\nBy looking at the result here, do you have maybe a rough estimation on what can be the area in the system this can be related to? It can help focusing the reproduction efforts"", 'created_at': datetime.datetime(2024, 9, 16, 5, 26, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2582169360, 'issue_id': 2514072996, 'author': 'shahar1', 'body': ""> @eladkal thanks, I'll keep trying to reproduce and update here if i get to it.\r\n> \r\n> By looking at the result here, do you have maybe a rough estimation on what can be the area in the system this can be related to? It can help focusing the reproduction efforts\r\n\r\nI think that collecting logs from those terminated pods would be helpful.\r\nPossible areas in the Airflow that are worth checking:\r\n- [Kuberenetes executor](https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py)\r\n- [Airflow's webserver](https://github.com/apache/airflow/blob/eaff866927f39b285109cbcc21ce8b3a2e08b623/airflow/www/views.py#L314) (I recently introduced a change that fixes an issue related to retries of mapped tasks)\r\n\r\nAlso, I just closed #45078 which seems to be a duplicate of this one."", 'created_at': datetime.datetime(2025, 1, 10, 9, 28, 35, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-09 14:04:29 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

kand617 on (2024-09-10 00:26:03 UTC): Im on 2.8.3 and can confirm similar behaviour. Suddenly I see a large chunk of scheduled map tasks be marked for failure.
No logs or anything.

eladkal on (2024-09-15 04:43:57 UTC): It will be difficult to debug without reproduce steps (including example dag)

michaelimas1 (Issue Creator) on (2024-09-16 05:26:39 UTC): @eladkal thanks, I'll keep trying to reproduce and update here if i get to it.

By looking at the result here, do you have maybe a rough estimation on what can be the area in the system this can be related to? It can help focusing the reproduction efforts

shahar1 on (2025-01-10 09:28:35 UTC): I think that collecting logs from those terminated pods would be helpful.
Possible areas in the Airflow that are worth checking:
- [Kuberenetes executor](https://github.com/apache/airflow/blob/main/providers/src/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py)
- [Airflow's webserver](https://github.com/apache/airflow/blob/eaff866927f39b285109cbcc21ce8b3a2e08b623/airflow/www/views.py#L314) (I recently introduced a change that fixes an issue related to retries of mapped tasks)

Also, I just closed #45078 which seems to be a duplicate of this one.

"
2514022808,issue,open,,DagProcessor - UPDATE statement on table 'serialized_dag' expected to update 1 row(s); 0 were matched.,"### Apache Airflow version

2.10.1

### What happened?


I have regularly this error in the logs of the DagProcessor ( with a Postgres 15 ) : 

```log
[2024-09-08T22:04:31.184+0000] {__init__.py:216} ERROR - Failed to export batch code: 400, reason: proto: illegal wireType 6
Process DagFileProcessor222516-Process:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.12/multiprocessing/process.py"", line 314, in _bootstrap
    self.run()
  File ""/usr/local/lib/python3.12/multiprocessing/process.py"", line 108, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py"", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py"", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py"", line 942, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py"", line 139, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py"", line 982, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py"", line 707, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/tenacity/__init__.py"", line 443, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/tenacity/__init__.py"", line 376, in iter
    result = action(retry_state)
             ^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/tenacity/__init__.py"", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/concurrent/futures/_base.py"", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/concurrent/futures/_base.py"", line 401, in __get_result
    raise self._exception
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py"", line 723, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dag.py"", line 3414, in bulk_write_to_db
    dataset_manager.create_datasets(dataset_models=new_datasets, session=session)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/datasets/manager.py"", line 65, in create_datasets
    session.flush()
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py"", line 3449, in flush
    self._flush(objects)
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py"", line 3588, in _flush
    with util.safe_reraise():
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
    compat.raise_(
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/util/compat.py"", line 211, in raise_
    raise exception
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/session.py"", line 3549, in _flush
    flush_context.execute()
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py"", line 456, in execute
    rec.execute(self)
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/unitofwork.py"", line 630, in execute
    util.preloaded.orm_persistence.save_obj(
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py"", line 237, in save_obj
    _emit_update_statements(
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/persistence.py"", line 1035, in _emit_update_statements
    raise orm_exc.StaleDataError(
sqlalchemy.orm.exc.StaleDataError: UPDATE statement on table 'serialized_dag' expected to update 1 row(s); 0 were matched.

```


### Versions of Apache Airflow Providers

apache-airflow-providers-postgres==5.12.0

### Deployment

Official Apache Airflow Helm Chart

CeleryExecutor

with gitsync

2 scheduler , 1 triggerer , N workers

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",raphaelauv,2024-09-09 13:46:04+00:00,[],2025-01-29 13:59:23+00:00,,https://github.com/apache/airflow/issues/42105,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('area:DAG-processing', '')]","[{'comment_id': 2621644760, 'issue_id': 2514022808, 'author': 'AlexMTX', 'body': 'It looks like a race condition within the dag processor process.\n\nDo you have more than 1 DAG defined in 1 python file?\nI have and my hypothesis is that this can let 2 processes refer to the same python file at the same time while updating 2 differently named dags within this exact file.\n\nTry to limit the number of processes to 1 with this variable:\nhttps://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#parsing-processes\n\nIn the helm chart:\n```\nairflow:\n  dagProcessor:\n    enabled: true\n\n    env:\n      - name: AIRFLOW__SCHEDULER__PARSING_PROCESSES\n        value: ""1""\n```\nThis helped me to avoid this error.', 'created_at': datetime.datetime(2025, 1, 29, 13, 23, 21, tzinfo=datetime.timezone.utc)}]","AlexMTX on (2025-01-29 13:23:21 UTC): It looks like a race condition within the dag processor process.

Do you have more than 1 DAG defined in 1 python file?
I have and my hypothesis is that this can let 2 processes refer to the same python file at the same time while updating 2 differently named dags within this exact file.

Try to limit the number of processes to 1 with this variable:
https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#parsing-processes

In the helm chart:
```
airflow:
  dagProcessor:
    enabled: true

    env:
      - name: AIRFLOW__SCHEDULER__PARSING_PROCESSES
        value: ""1""
```
This helped me to avoid this error.

"
2513163971,issue,closed,completed,Wrong display of XCOM for boolean False value,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.1

### What happened?

There was a bug in version 2.10 when displaying xcom values in the UI. It should have been fixed - see https://github.com/apache/airflow/pull/41516.

However, the problem persists for boolean False values (see images below).
For True - xcom displays correct value
![image](https://github.com/user-attachments/assets/862e1c6a-3247-4e83-be27-2210f2e48218)

For False - xcom displays wrong value
![image](https://github.com/user-attachments/assets/af83951e-92ee-48ac-94b7-3dab090544d3)


Test Dag Code:

```
from datetime import datetime
from airflow import DAG
from airflow.operators.python import PythonOperator

# DAG Definition
with DAG(
    dag_id=""TEST_XCOM_BOOLEANS"",
    schedule = None,
    start_date = datetime(2024, 9, 8),
) as dag:

    def return_true():
        return True	

    true_task = PythonOperator(
        task_id = 'true_task',
        python_callable = return_true,
        trigger_rule = 'none_failed',
    )

    def return_false():
        return False	
	
    false_task = PythonOperator(
        task_id = 'false_task',
        python_callable = return_false,
        trigger_rule = 'none_failed',
    )	
```





### What you think should happen instead?

For False value it should display ""False"" and not ""Value is NULL""

### How to reproduce

Run Dag (see the code below)

Test Dag Code:

```
from datetime import datetime
from airflow import DAG
from airflow.operators.python import PythonOperator

# DAG Definition
with DAG(
    dag_id=""TEST_XCOM_BOOLEANS"",
    schedule = None,
    start_date = datetime(2024, 9, 8),
) as dag:

    def return_true():
        return True	

    true_task = PythonOperator(
        task_id = 'true_task',
        python_callable = return_true,
        trigger_rule = 'none_failed',
    )

    def return_false():
        return False	
	
    false_task = PythonOperator(
        task_id = 'false_task',
        python_callable = return_false,
        trigger_rule = 'none_failed',
    )	
```

### Operating System

Kubernetes on Unix platform

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Pad71,2024-09-09 07:40:19+00:00,[],2024-10-08 15:33:51+00:00,2024-10-08 15:33:51+00:00,https://github.com/apache/airflow/issues/42096,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2399648616, 'issue_id': 2513163971, 'author': 'raphaelauv', 'body': 'fix was merge, @Pad71 can you close the issue ?', 'created_at': datetime.datetime(2024, 10, 8, 12, 0, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2400182619, 'issue_id': 2513163971, 'author': 'Pad71', 'body': '@raphaelauv DOne :)', 'created_at': datetime.datetime(2024, 10, 8, 15, 33, 51, tzinfo=datetime.timezone.utc)}]","raphaelauv on (2024-10-08 12:00:20 UTC): fix was merge, @Pad71 can you close the issue ?

Pad71 (Issue Creator) on (2024-10-08 15:33:51 UTC): @raphaelauv DOne :)

"
2513021529,issue,open,,Task with retries can circument max_active_runs limit,"### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

We have some DAGs that cannot run in parallel. To prevent parallel execution, we configured max_active_runs=1. We also configured retries. Recently, we observed a case where Airflow still scheduled two parallel DAG runs. We reconstructed what happened from the audit logs and can reliably reproduce it:

GIVEN a DAG with max_active_runs=1 and a task with retries > 0
WHEN the task is running in the context of run A
AND the user manually marks run A as failed (or success)
AND the user clears multiple runs including run A shortly afterwards
AND the scheduler starts the task in the context of another run B
THEN the task of run A is marked as ""UP_FOR_RETRY"" and restarts after backoff (5 minutes by default) regardless of whether another run is already active

### What you think should happen instead?

* Airflow should not schedule two parallel runs when max_active_runs=1
* Airflow should not retry when the user marks run as failed/success and clears it shortly after

### How to reproduce

See above. Using Kubernetes executor (or similar) is likely necessary to reproduce this, as it extends the time between the user action (mark as failed/success) and the retrieval of SIGTERM in the task instance. We also used a task that sleeps longer than the retry backoff (5m by default) to actually see the two runs running in parallel.

### Operating System

debian 12 (bookworm)

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes==8.3.3


### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Workload runs via kubernetes executor and kubernetes pod operator.

### Anything else?

Rarely, but if it does, it causes severe problems as the DAG/task cannot run in parallel.

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jmaicher,2024-09-09 06:23:43+00:00,[],2024-09-19 11:11:47+00:00,,https://github.com/apache/airflow/issues/42093,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]",[],
2510409242,issue,open,,KEDA Autoscaling connection string is invalid when using psycopg2,"### Official Helm Chart version

1.15.0 (latest released)

### Apache Airflow version

2.9.3

### Kubernetes Version

1.28.12

### Helm Chart configuration

_No response_

### Docker Image customizations

_No response_

### What happened

[In the Airflow documentation](https://airflow.apache.org/docs/apache-airflow/stable/howto/set-up-database.html#setting-up-a-postgresql-database), it is recommended to use the `psycopg2` driver. This won't work as a connection string for KEDA as `psycopg2` is a Python package and KEDA is written in GO.

### What you think should happen instead

It should be possible to provide a custom connection string to use with KEDA and/or custom EnvVar.

### How to reproduce

 When setting `data.metadataConnection.protocol: postgresql+psycopg2` and enabling KEDA for the triggerer (`triggerer.keda.enabled: true`) and/or worker (`workers.keda.enabled: true`) it will result in a broken ScaledObject.

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",DjVinnii,2024-09-06 13:12:47+00:00,[],2024-09-06 13:21:20+00:00,,https://github.com/apache/airflow/issues/42070,"[('kind:bug', 'This is a clearly a bug'), ('area:helm-chart', 'Airflow Helm Chart'), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2510159631,issue,open,,MySqlHook bulk_load_custom() syntax errors due to escaped sql command,"### Apache Airflow Provider(s)

mysql

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon[aiobotocore]==8.17.0
apache-airflow-providers-apprise==1.2.1
apache-airflow-providers-atlassian-jira==1.1.0
apache-airflow-providers-celery==3.5.2
apache-airflow-providers-common-sql==1.10.1
apache-airflow-providers-ftp==3.7.0
apache-airflow-providers-google==10.14.0
apache-airflow-providers-http==4.9.0
apache-airflow-providers-imap==3.5.0
apache-airflow-providers-microsoft-mssql==3.6.0
apache-airflow-providers-mysql==5.5.2
apache-airflow-providers-postgres==5.10.0
apache-airflow-providers-sftp==4.8.1
apache-airflow-providers-snowflake==5.3.0
apache-airflow-providers-sqlite==3.7.0
apache-airflow-providers-ssh==3.10.0
apache-airflow-providers-tableau==4.4.1

### Apache Airflow version

2.7.2

### Operating System

Amazon Linux 2023

### Deployment

Amazon (AWS) MWAA

### Deployment details

Python 3.11

### What happened

Using the below task always fails due to a MySQL syntax error as the ""IGNORE"" Duplicate-Key and Error Handling option and any mysql_extra_options values are escaped before execution.
```
load_data= S3ToMySqlOperator(
  task_id=""load_data"",
  s3_source_key=f""s3://{S3_BRONZE_BUCKET}/test.csv"",
  mysql_table=""test_db.test_table"",
  mysql_conn_id=MYSQL_CONN_ID,
  mysql_extra_options=""FIELDS TERMINATED BY ',' (col1, col3)"",
  mysql_local_infile=True,
)
```

### What you think should happen instead

The task instance fails with the following error:
`MySQLdb.ProgrammingError: (1064, ""You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ''IGNORE' INTO TABLE test_db.test_table 'FIELDS TERMINATED BY \\',\\' (col1,' at line 1"")`

### How to reproduce

In a MySQL environment with local-infile enabled:
```
CREATE TABLE test_db.test_table (
	col1 INT NULL,
	col2 INT NULL,
	col3 INT NULL
);
```

test.csv (create and upload to ""test_bucket""):
```
100,200
300,400
500,600
```

example_dag.py
```
""""""An example dag.""""""

from __future__ import annotations

from pathlib import Path

import pendulum

from airflow import DAG
from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator
from airflow.providers.mysql.transfers.s3_to_mysql import S3ToMySqlOperator

MYSQL_CONN_ID = ""mysql_server""
S3_BUCKET = ""test_bucket""

with DAG(
    dag_id=Path(__file__).name.replace("".py"", """"),
    start_date=pendulum.datetime(2022, 1, 1, tz=""Europe/London""),
    schedule=None,
):
    truncate_test_table = SQLExecuteQueryOperator(
        task_id=""truncate_test_table"",
        sql=""TRUNCATE TABLE test_db.test_table"",
        autocommit=True,
        conn_id=MYSQL_CONN_ID,
        database=""wallboard"",
    )

    load_data= S3ToMySqlOperator(
        task_id=""load_data"",
        s3_source_key=f""s3://{S3_BUCKET}/test.csv"",
        mysql_table=""test_db.test_table"",
        mysql_conn_id=MYSQL_CONN_ID,
        mysql_extra_options=""FIELDS TERMINATED BY ',' (col1, col3)"",
        mysql_local_infile=True,
    )

    truncate_test_table >> load_data
```

Running the DAG should result in an error of
`MySQLdb.ProgrammingError: (1064, ""You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ''IGNORE' INTO TABLE test_db.test_table 'FIELDS TERMINATED BY \\',\\' (col1,' at line 1"")`

### Anything else

Occurs every time because IGNORE is passed as default and gets escaped incorrectly.

I believe it was caused by this PR https://github.com/apache/airflow/pull/33328 which added the escapes to prevent sql injection but seems to have broken the bulk_load_custom() method,

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jjunkiee,2024-09-06 10:53:08+00:00,[],2024-09-27 08:39:50+00:00,,https://github.com/apache/airflow/issues/42061,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('provider:mysql', '')]","[{'comment_id': 2333796850, 'issue_id': 2510159631, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 6, 10, 53, 11, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-06 10:53:11 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2509427691,issue,open,,High CPU utilisation caused by frequent updates to dag_run table,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.1

### What happened?


**Setup**
We are self-hosting Airflow via Helm Chart. We use google's CloudSQL as our Airflow metadata store.

**Issue**

We observe that the below query has been called many times, causes database locks and is causing high CPU utilisation

```
UPDATE
  dag_run
SET
  last_scheduling_decision=$1::timestamptz,
  updated_at=$2::timestamptz
WHERE
  dag_run.id = $3
```

For instance, referring to the below screenshot,  in 1 day, 193,362 times has been called.

<img width=""868"" alt=""image"" src=""https://github.com/user-attachments/assets/188d902f-1952-4305-a1f6-c532a0705a87"">



**Spec of the CLoudSQL instance**

The CLoudSQL instance has 4 vCPUs and 15GB ram.




### What you think should happen instead?

Updates to the dag_run on last_scheduling_decision should be reduced (or configurable?)

So we can test if database CPU utilisation can be reduced and Database locking problem can be resolved.



### How to reproduce

Not easily reproducible.

==========

At the time of the UPDATE SQLs, I observe that only 5 instances of DAG runs are running. 

We have a similar setup in another environment where the other CloudSQL instance also has only 4 vCPUs.

In the other environment, however, the number of times the UPDATE SQL is called is smaller. Also no huge database locks are observed in the other environment.






### Operating System

linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",kenho811-b1,2024-09-06 03:20:40+00:00,[],2024-12-20 21:40:33+00:00,,https://github.com/apache/airflow/issues/42053,"[('kind:bug', 'This is a clearly a bug'), ('area:MetaDB', 'Meta Database related issues.'), ('area:performance', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2333160832, 'issue_id': 2509427691, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 6, 3, 20, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2557213071, 'issue_id': 2509427691, 'author': 'jkramer-ginkgo', 'body': ""I'm experiencing this issue too in 2.10.4. Looks like the [index was removed](https://github.com/apache/airflow/pull/39275) because unused, but I'm clearly seeing queries that would be using this index"", 'created_at': datetime.datetime(2024, 12, 20, 15, 23, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2557700397, 'issue_id': 2509427691, 'author': 'potiuk', 'body': ""> I'm experiencing this issue too in 2.10.4. Looks like the [index was removed](https://github.com/apache/airflow/pull/39275) because unused, but I'm clearly seeing queries that would be using this index\r\n\r\ncoudl you please epxlain what your queries are you see and logs where you observe it?"", 'created_at': datetime.datetime(2024, 12, 20, 20, 51, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2557749138, 'issue_id': 2509427691, 'author': 'jkramer-ginkgo', 'body': 'I can share screenshots from our RDS performance insights dashboard (the red corresponds to locking, which is why I was looking in to the index in the first place) \r\n\r\n<img width=""1750"" alt=""image"" src=""https://github.com/user-attachments/assets/fe52de6e-bd39-4187-b936-be47d7c66507"" />\r\n<img width=""1745"" alt=""image"" src=""https://github.com/user-attachments/assets/84308a4e-58e9-4374-a70f-b864294f171a"" />\r\n<img width=""962"" alt=""image"" src=""https://github.com/user-attachments/assets/05ea781f-3942-4a09-9b8f-6919e8e7a035"" />', 'created_at': datetime.datetime(2024, 12, 20, 21, 38, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2557750867, 'issue_id': 2509427691, 'author': 'jkramer-ginkgo', 'body': ""Apologies, definitely a brain fart. This query wouldn't use the index mentioned!"", 'created_at': datetime.datetime(2024, 12, 20, 21, 39, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2557751709, 'issue_id': 2509427691, 'author': 'jkramer-ginkgo', 'body': ""This issue itself is still relevant (and what I'm observing). But the index removal is irrelevant."", 'created_at': datetime.datetime(2024, 12, 20, 21, 40, 32, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-06 03:20:43 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jkramer-ginkgo on (2024-12-20 15:23:16 UTC): I'm experiencing this issue too in 2.10.4. Looks like the [index was removed](https://github.com/apache/airflow/pull/39275) because unused, but I'm clearly seeing queries that would be using this index

potiuk on (2024-12-20 20:51:24 UTC): coudl you please epxlain what your queries are you see and logs where you observe it?

jkramer-ginkgo on (2024-12-20 21:38:02 UTC): I can share screenshots from our RDS performance insights dashboard (the red corresponds to locking, which is why I was looking in to the index in the first place) 

<img width=""1750"" alt=""image"" src=""https://github.com/user-attachments/assets/fe52de6e-bd39-4187-b936-be47d7c66507"" />
<img width=""1745"" alt=""image"" src=""https://github.com/user-attachments/assets/84308a4e-58e9-4374-a70f-b864294f171a"" />
<img width=""962"" alt=""image"" src=""https://github.com/user-attachments/assets/05ea781f-3942-4a09-9b8f-6919e8e7a035"" />

jkramer-ginkgo on (2024-12-20 21:39:40 UTC): Apologies, definitely a brain fart. This query wouldn't use the index mentioned!

jkramer-ginkgo on (2024-12-20 21:40:32 UTC): This issue itself is still relevant (and what I'm observing). But the index removal is irrelevant.

"
2508448713,issue,open,,Unable to set podspec attributes to false in `KubernetesPodOperator`,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I'm trying to modify the podspec for the pod created using `KubernetesPodOperator`. Specifically, I'm trying to [opt out of API credential automounting](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#opt-out-of-api-credential-automounting) by setting `spec.automountServiceAccountToken: false`.

I see that `KubernetesPodOperator` allows a `full_pod_spec` parameter to be passed and that this spec is merged into the default spec. So I tried the following:

```python
from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import (
    KubernetesPodOperator,
)
from kubernetes.client import models as k8s_models

KubernetesPodOperator(
    task_id=""test"",
    name=""test"",
    image=""..."",
    get_logs=True,
    retries=0,
    is_delete_operator_pod=True,
    image_pull_policy=""Always"",
    cmds=[""echo"", ""hello""],

    # This pod_spec should be merged into the default pod_spec from the KubernetesPodOperator.
    # This allows us to set the automount_service_account_token to False.
    full_pod_spec=k8s_models.V1Pod(spec=k8s_models.V1PodSpec(automount_service_account_token=False, containers=[])),
).dry_run()
```

(I actually tried to run a real DAG based on this at first and confirmed the problem existed, before settling on the above script as the fastest way to reproduce the issue).

The output was:

```shell
...
api_version: v1
kind: Pod
metadata:
  annotations: {}
  labels:
    airflow_kpo_in_cluster: 'False'
    airflow_version: 2.10.0
  name: test-sfvx7leb
  namespace: default
spec:
  affinity: {}
  containers:
  - args: []
    command:
    - sleep
    - '3600'
    env: []
    env_from: []
    image: python:latest
    image_pull_policy: Always
    name: base
    ports: []
    termination_message_policy: File
    volume_mounts: []
  host_network: false
  image_pull_secrets: []
  init_containers: []
  node_selector: {}
  restart_policy: Never
  security_context: {}
  tolerations: []
  volumes: []
```

Note that `spec.automountServiceAccountToken` is not in the output.

I tried to step through the airflow code that was responsible for the merge and noticed that the bug is in the following function:

https://github.com/apache/airflow/blob/12bb8b35241f0915e82a322c7905c8602df95a7f/airflow/providers/cncf/kubernetes/pod_generator.py#L610-L638

Specifically in the line:

```python
if not getattr(client_obj, base_key, None) and base_val
```

Because I'm trying to set the value of this key to `False`, `base_val` is `False` and hence the attribute is not merged in. In my case, the default behaviour of kubernetes is to [automount credentials](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#opt-out-of-api-credential-automounting) (this is what happens if the attribute is not specified) and I'm unable to turn it off by setting it to False. Hypothetically if I was trying to turn some attribute to True, this approach would have worked (you can confirm this by setting `automount_service_account_token=True` in the script above. 

### What you think should happen instead?

The output for the script above should have included `spec.automountServiceAccountToken: false`.

### How to reproduce

Run the following script:

```python
from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import (
    KubernetesPodOperator,
)
from kubernetes.client import models as k8s_models

KubernetesPodOperator(
    task_id=""test"",
    name=""test"",
    image=""..."",
    get_logs=True,
    retries=0,
    is_delete_operator_pod=True,
    image_pull_policy=""Always"",
    cmds=[""echo"", ""hello""],

    # This pod_spec should be merged into the default pod_spec from the KubernetesPodOperator.
    # This allows us to set the automount_service_account_token to False.
    full_pod_spec=k8s_models.V1Pod(spec=k8s_models.V1PodSpec(automount_service_account_token=False, containers=[])),
).dry_run()
```

Confirm that `spec.automountServiceAccountToken` is not in the output.

### Operating System

OSX and Ubuntu

### Versions of Apache Airflow Providers

```shell
apache-airflow-providers-cncf-kubernetes==7.10.0
apache-airflow-providers-common-compat==1.2.0
apache-airflow-providers-common-io==1.4.0
apache-airflow-providers-common-sql==1.8.1
apache-airflow-providers-fab==1.3.0
apache-airflow-providers-ftp==3.6.1
apache-airflow-providers-google==10.12.0
apache-airflow-providers-http==4.7.0
apache-airflow-providers-imap==3.4.0
apache-airflow-providers-smtp==1.8.0
apache-airflow-providers-sqlite==3.5.0
```

### Deployment

Google Cloud Composer

### Deployment details

I've confirmed this behaviour with Composer version `2.4.6` and Airflow version `2.6.3`. I've also confirmed this in local development, even after switching to Airflow `2.10.0`

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",chhabrakadabra,2024-09-05 18:19:43+00:00,[],2024-09-05 18:22:12+00:00,,https://github.com/apache/airflow/issues/42040,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2332370794, 'issue_id': 2508448713, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 5, 18, 19, 47, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-05 18:19:47 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2508248089,issue,closed,completed,metric name in otel metric logger will get truncated if they exceed limit of 63 characters,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

# Symptom
When running a DAG in Airflow with OTEL metrics enabled, you may receive warning messages that may look like this:
```
[2024-09-05, 11:01:22 CDT] {logging_mixin.py:190} WARNING - /opt/airflow/airflow/metrics/otel_logger.py:361 UserWarning: Metric name `airflow.dag.example_complex.update_tag_template_field.queued_duration` exceeds OpenTelemetry's name length limit of 63 characters and will be truncated to `airflow.dag.example_complex.update_tag_template_field.queued_du`.
```

# Reason
The reason being that metric name in OTEL standard will be limited to 63 characters, and thus, when the metric name exceeding that size is being generated, it will get truncated, as the message implies.

# Solution
When instrumenting these metrics, there is no need to make the metric name conforming to the statsd expression (which puts combination of attributes into the name, due to how the statsd data format is designed), but we could rather change the metric to:

- having **metric name** as : airflow.dag.queued_duration
- having **attribute** dag_id: example_complex
- having **attribute** task_id: update_tag_template_field

This will ensure the metric name to be well kept within the limit, while providing better context by separating dag_id with task_id.


### What you think should happen instead?

The current behavior is not correct because the metric name is getting truncated, and if there are longer names, it will lose the full information of what it was about.

### How to reproduce

Prepare and run DAG file with long dag id and task id, and see the task logs and identify WARNING message generated by otel_logger.py

### Operating System

Linux (amd64)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",howardyoo,2024-09-05 16:28:13+00:00,[],2024-09-05 18:20:24+00:00,2024-09-05 18:20:24+00:00,https://github.com/apache/airflow/issues/42038,"[('kind:bug', 'This is a clearly a bug'), ('area:metrics', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2332202112, 'issue_id': 2508248089, 'author': 'ferruzzi', 'body': 'Yes, this is well known and something we discussed when I was implementing the OTel metrics.  That is why some metrics are emitted twice; once with the ""old"" long StatsD name and again with a shorter name and tags for OTel.  In the case of your example, the shorter version is being emitted with tags as `task.queued_duration` [here](https://github.com/apache/airflow/blob/main/airflow/models/taskinstance.py#L2829)', 'created_at': datetime.datetime(2024, 9, 5, 16, 46, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2332206513, 'issue_id': 2508248089, 'author': 'ferruzzi', 'body': 'The Airflow 3 dev team is discussing if we are going to drop StatsD entirely, which would remove the need for this.\r\n\r\nAnother alternative I have proposed in the past is that each metrics logger (StatsD, OTel, Datadog, etc) would need a ""build name"" method which accepts a short name and tags and returns the name to be emitted in the format that works for that logger; so otel\'s version would return the short name, statsd would return name with the tags concatenated onto it, etc.', 'created_at': datetime.datetime(2024, 9, 5, 16, 49, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2332233477, 'issue_id': 2508248089, 'author': 'howardyoo', 'body': ""Ah, I understand. I don't think StatsD is going to be dropped entirely in Airflow 3, though. Perhaps, in case of Otel metrics, if the metric name is longer than 63 characters, maybe we should just not send it, because that won't be meaningful. (ex. send the metrics twice, but if the metric name is longer than 63 char, just send the short name only).\r\n\r\nHowever, I like your proposal of 'build name' method being applied across all of the metric loggers such that it would affect how the name should be generated for different types of metric loggers."", 'created_at': datetime.datetime(2024, 9, 5, 17, 4, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2332255163, 'issue_id': 2508248089, 'author': 'ferruzzi', 'body': ""I've also replied in the related conversation over [here](https://github.com/apache/airflow/issues/40800#issuecomment-2332254233) with some more background and details."", 'created_at': datetime.datetime(2024, 9, 5, 17, 16, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2332371844, 'issue_id': 2508248089, 'author': 'howardyoo', 'body': 'closing this issue, as related to the issue #40800. Would like the discussion to continue on there.', 'created_at': datetime.datetime(2024, 9, 5, 18, 20, 24, tzinfo=datetime.timezone.utc)}]","ferruzzi on (2024-09-05 16:46:28 UTC): Yes, this is well known and something we discussed when I was implementing the OTel metrics.  That is why some metrics are emitted twice; once with the ""old"" long StatsD name and again with a shorter name and tags for OTel.  In the case of your example, the shorter version is being emitted with tags as `task.queued_duration` [here](https://github.com/apache/airflow/blob/main/airflow/models/taskinstance.py#L2829)

ferruzzi on (2024-09-05 16:49:04 UTC): The Airflow 3 dev team is discussing if we are going to drop StatsD entirely, which would remove the need for this.

Another alternative I have proposed in the past is that each metrics logger (StatsD, OTel, Datadog, etc) would need a ""build name"" method which accepts a short name and tags and returns the name to be emitted in the format that works for that logger; so otel's version would return the short name, statsd would return name with the tags concatenated onto it, etc.

howardyoo (Issue Creator) on (2024-09-05 17:04:18 UTC): Ah, I understand. I don't think StatsD is going to be dropped entirely in Airflow 3, though. Perhaps, in case of Otel metrics, if the metric name is longer than 63 characters, maybe we should just not send it, because that won't be meaningful. (ex. send the metrics twice, but if the metric name is longer than 63 char, just send the short name only).

However, I like your proposal of 'build name' method being applied across all of the metric loggers such that it would affect how the name should be generated for different types of metric loggers.

ferruzzi on (2024-09-05 17:16:43 UTC): I've also replied in the related conversation over [here](https://github.com/apache/airflow/issues/40800#issuecomment-2332254233) with some more background and details.

howardyoo (Issue Creator) on (2024-09-05 18:20:24 UTC): closing this issue, as related to the issue #40800. Would like the discussion to continue on there.

"
2508225420,issue,closed,completed,Where did Rendered Template go?,"### Description

starting w v2.9, the rendered template tab in task details page is gone. 

there have been several conversations about this as bug issues, with some PRs that attempt to solve. they all seem stale.

When will we get the rendered template (esp for SQL) back??

### Use case/motivation

as a DE, every time I need to verify 1) that my jinja template rendered properly & 2) changes to the sql code have been properly deployed - I use the Rendered Template tab. 

without it, we are just given a txt string on a single line that doesnt wrap - looks like shit, cannot do quick spot checks.

please bring back.

### Related issues

https://github.com/apache/airflow/pull/39918

https://github.com/apache/airflow/issues/39527

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",cah-jonathan-cachat01,2024-09-05 16:16:53+00:00,[],2024-09-16 17:27:56+00:00,2024-09-07 21:36:49+00:00,https://github.com/apache/airflow/issues/42037,"[('kind:bug', 'This is a clearly a bug'), ('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2332145785, 'issue_id': 2508225420, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 5, 16, 16, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2334116942, 'issue_id': 2508225420, 'author': 'pierrejeambrun', 'body': 'I think at the moment you can still access the old rendered template page via `More Details` in the grid view and then selecting the Rendered template:\r\n![Screenshot 2024-09-06 at 15 52 37](https://github.com/user-attachments/assets/b4ade70b-0ba3-4990-805f-f15b6d7c0647)\r\n\r\nThis old page is meant to be deleted and migrated over to the new grid view (or at least into the `React` part of the UI). If you have something in mind on how you would want to see rendered template in the grid view, I would be happy to review a PR.\r\n\r\nThere will not be any major feature for 2.11 as it will be a bridge release before airflow 3.0, any big improvement on that matter would come with airflow 3 I suppose. \r\n\r\n\r\nIf we just want the direct link from the details pan to the old rendered template page back, then I believe this can be done easily and release in next patch version of airflow 2.', 'created_at': datetime.datetime(2024, 9, 6, 13, 56, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2334781431, 'issue_id': 2508225420, 'author': 'cah-jonathan-cachat01', 'body': 'Pierre -\r\n\r\nI appreciate your response, and indeed it is still available via that path.\r\n\r\nThe way that the template is rendered has been lost. It now displays whatever was rendered as a single line with formatting characters, rather than a nicely rendered & colorized. Made reviewing SQL a breeze, now the workflow is extended further because you need to copy into a text editor & find. This specific rendering complaint has also been raised in the linked issues (I am not the only one).\r\n\r\nThe point of this in my workflow is: ok, just committed changes to SQL file executed by DAG task – lets verify that the SQL file it executed contains those changes. Sanity check stuff.\r\n\r\n\r\nFrom: Pierre Jeambrun ***@***.***>\r\nDate: Friday, September 6, 2024 at 9:57\u202fAM\r\nTo: apache/airflow ***@***.***>\r\nCc: Cachat, Jonathan ***@***.***>, Author ***@***.***>\r\nSubject: Re: [apache/airflow] Where did Rendered Template go? (Issue #42037)\r\n\r\n        External Email – Please use caution before opening attachments or clicking links\r\n\r\n\r\nI think at the moment you can still access the old rendered template page via More Details in the grid view and then selecting the Rendered template:\r\nScreenshot.2024-09-06.at.15.52.37.png (view on web)<https://github.com/user-attachments/assets/b4ade70b-0ba3-4990-805f-f15b6d7c0647>\r\n\r\nThis old page is meant to be deleted and migrated over to the new grid view (or at least into the React part of the UI). If you have something in mind on how you would want to see rendered template in the grid view, I would be happy to review a PR.\r\n\r\nThere will not be any major feature for 2.11 as it will be a bridge release before airflow 3.0, any big improvement on that matter would come with airflow 3 I suppose.\r\n\r\n—\r\nReply to this email directly, view it on GitHub<https://github.com/apache/airflow/issues/42037#issuecomment-2334116942>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/A5OYTORUM5GTQK2GXNLBOELZVGYCTAVCNFSM6AAAAABNW24XRWVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGMZUGEYTMOJUGI>.\r\nYou are receiving this because you authored the thread.Message ID: ***@***.***>\r\n\n_________________________________________________\n\nThis message is for the designated recipient only and may contain privileged, proprietary\nor otherwise private information. If you have received it in error, please notify the sender\nimmediately and delete the original. Any other use of the email by you is prohibited.\n\nDansk - Deutsch - Espanol - Francais - Italiano - Japanese - Nederlands - Norsk - Portuguese - Chinese\nSvenska: http://www.cardinalhealth.com/en/support/terms-and-conditions-english.html', 'created_at': datetime.datetime(2024, 9, 6, 20, 42, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2336458782, 'issue_id': 2508225420, 'author': 'jscheffl', 'body': 'This seems to be a duplicate of #39527', 'created_at': datetime.datetime(2024, 9, 7, 21, 36, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2338025022, 'issue_id': 2508225420, 'author': 'cah-jonathan-cachat01', 'body': 'Almost all of these requests have been identified as duplicated and then closed, with no resolution.\r\n\r\nFrom: Jens Scheffler ***@***.***>\r\nDate: Saturday, September 7, 2024 at 5:37\u202fPM\r\nTo: apache/airflow ***@***.***>\r\nCc: Cachat, Jonathan ***@***.***>, Author ***@***.***>\r\nSubject: Re: [apache/airflow] Where did Rendered Template go? (Issue #42037)\r\n\r\n        External Email – Please use caution before opening attachments or clicking links\r\n\r\n\r\nThis seems to be a duplicate of #39527<https://github.com/apache/airflow/issues/39527>\r\n\r\n—\r\nReply to this email directly, view it on GitHub<https://github.com/apache/airflow/issues/42037#issuecomment-2336458782>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/A5OYTOUSQ4DL3LYLBEUJBTTZVNWYPAVCNFSM6AAAAABNW24XRWVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGMZWGQ2TQNZYGI>.\r\nYou are receiving this because you authored the thread.Message ID: ***@***.***>\r\n\n_________________________________________________\n\nThis message is for the designated recipient only and may contain privileged, proprietary\nor otherwise private information. If you have received it in error, please notify the sender\nimmediately and delete the original. Any other use of the email by you is prohibited.\n\nDansk - Deutsch - Espanol - Francais - Italiano - Japanese - Nederlands - Norsk - Portuguese - Chinese\nSvenska: http://www.cardinalhealth.com/en/support/terms-and-conditions-english.html', 'created_at': datetime.datetime(2024, 9, 9, 12, 43, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2338198663, 'issue_id': 2508225420, 'author': 'potiuk', 'body': '> Almost all of these requests have been identified as duplicated and then closed, with no resolution.\r\n\r\nYes. that\'s how (by definition) duplicates work. they are closed so that they are not duplicating things. The only remaining issue describing it is opened - and you are even free to pick it up and fix it if you would like to contribute back for the free software you get. Or you need to wait until somoene fixes it or implements it differently or even (sometimes) decide ""not an issue"".\r\n\r\nGenerally speaking, rolling sleeves up and making a PR to fix things and leading it to completion is absolutely fastest and most certain way to get such issue fixed.', 'created_at': datetime.datetime(2024, 9, 9, 13, 56, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2353502663, 'issue_id': 2508225420, 'author': 'itsCodyBo', 'body': 'Dropping in to say we have also recently upgraded and are planning to roll back until this feature is implemented again. This is a daily visit for our team as well. It seems a possible solution was proposed, but never implemented.\r\n\r\nhttps://github.com/apache/airflow/pull/39918', 'created_at': datetime.datetime(2024, 9, 16, 17, 27, 12, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-05 16:16:56 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

pierrejeambrun on (2024-09-06 13:56:35 UTC): I think at the moment you can still access the old rendered template page via `More Details` in the grid view and then selecting the Rendered template:
![Screenshot 2024-09-06 at 15 52 37](https://github.com/user-attachments/assets/b4ade70b-0ba3-4990-805f-f15b6d7c0647)

This old page is meant to be deleted and migrated over to the new grid view (or at least into the `React` part of the UI). If you have something in mind on how you would want to see rendered template in the grid view, I would be happy to review a PR.

There will not be any major feature for 2.11 as it will be a bridge release before airflow 3.0, any big improvement on that matter would come with airflow 3 I suppose. 


If we just want the direct link from the details pan to the old rendered template page back, then I believe this can be done easily and release in next patch version of airflow 2.

cah-jonathan-cachat01 (Issue Creator) on (2024-09-06 20:42:44 UTC): Pierre -

I appreciate your response, and indeed it is still available via that path.

The way that the template is rendered has been lost. It now displays whatever was rendered as a single line with formatting characters, rather than a nicely rendered & colorized. Made reviewing SQL a breeze, now the workflow is extended further because you need to copy into a text editor & find. This specific rendering complaint has also been raised in the linked issues (I am not the only one).

The point of this in my workflow is: ok, just committed changes to SQL file executed by DAG task – lets verify that the SQL file it executed contains those changes. Sanity check stuff.


From: Pierre Jeambrun ***@***.***>
Date: Friday, September 6, 2024 at 9:57 AM
To: apache/airflow ***@***.***>
Cc: Cachat, Jonathan ***@***.***>, Author ***@***.***>
Subject: Re: [apache/airflow] Where did Rendered Template go? (Issue #42037)

        External Email – Please use caution before opening attachments or clicking links


I think at the moment you can still access the old rendered template page via More Details in the grid view and then selecting the Rendered template:
Screenshot.2024-09-06.at.15.52.37.png (view on web)<https://github.com/user-attachments/assets/b4ade70b-0ba3-4990-805f-f15b6d7c0647>

This old page is meant to be deleted and migrated over to the new grid view (or at least into the React part of the UI). If you have something in mind on how you would want to see rendered template in the grid view, I would be happy to review a PR.

There will not be any major feature for 2.11 as it will be a bridge release before airflow 3.0, any big improvement on that matter would come with airflow 3 I suppose.

—
Reply to this email directly, view it on GitHub<https://github.com/apache/airflow/issues/42037#issuecomment-2334116942>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/A5OYTORUM5GTQK2GXNLBOELZVGYCTAVCNFSM6AAAAABNW24XRWVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGMZUGEYTMOJUGI>.
You are receiving this because you authored the thread.Message ID: ***@***.***>

_________________________________________________

This message is for the designated recipient only and may contain privileged, proprietary
or otherwise private information. If you have received it in error, please notify the sender
immediately and delete the original. Any other use of the email by you is prohibited.

Dansk - Deutsch - Espanol - Francais - Italiano - Japanese - Nederlands - Norsk - Portuguese - Chinese
Svenska: http://www.cardinalhealth.com/en/support/terms-and-conditions-english.html

jscheffl on (2024-09-07 21:36:49 UTC): This seems to be a duplicate of #39527

cah-jonathan-cachat01 (Issue Creator) on (2024-09-09 12:43:49 UTC): Almost all of these requests have been identified as duplicated and then closed, with no resolution.

From: Jens Scheffler ***@***.***>
Date: Saturday, September 7, 2024 at 5:37 PM
To: apache/airflow ***@***.***>
Cc: Cachat, Jonathan ***@***.***>, Author ***@***.***>
Subject: Re: [apache/airflow] Where did Rendered Template go? (Issue #42037)

        External Email – Please use caution before opening attachments or clicking links


This seems to be a duplicate of #39527<https://github.com/apache/airflow/issues/39527>

—
Reply to this email directly, view it on GitHub<https://github.com/apache/airflow/issues/42037#issuecomment-2336458782>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/A5OYTOUSQ4DL3LYLBEUJBTTZVNWYPAVCNFSM6AAAAABNW24XRWVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGMZWGQ2TQNZYGI>.
You are receiving this because you authored the thread.Message ID: ***@***.***>

_________________________________________________

This message is for the designated recipient only and may contain privileged, proprietary
or otherwise private information. If you have received it in error, please notify the sender
immediately and delete the original. Any other use of the email by you is prohibited.

Dansk - Deutsch - Espanol - Francais - Italiano - Japanese - Nederlands - Norsk - Portuguese - Chinese
Svenska: http://www.cardinalhealth.com/en/support/terms-and-conditions-english.html

potiuk on (2024-09-09 13:56:05 UTC): Yes. that's how (by definition) duplicates work. they are closed so that they are not duplicating things. The only remaining issue describing it is opened - and you are even free to pick it up and fix it if you would like to contribute back for the free software you get. Or you need to wait until somoene fixes it or implements it differently or even (sometimes) decide ""not an issue"".

Generally speaking, rolling sleeves up and making a PR to fix things and leading it to completion is absolutely fastest and most certain way to get such issue fixed.

itsCodyBo on (2024-09-16 17:27:12 UTC): Dropping in to say we have also recently upgraded and are planning to roll back until this feature is implemented again. This is a daily visit for our team as well. It seems a possible solution was proposed, but never implemented.

https://github.com/apache/airflow/pull/39918

"
2508000800,issue,closed,completed,SnowflakeSqlApiOperator not resolving parameters in SQL,"### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

The SnowflakeSqlApiOperator does not resolve parameters in SQL despite accepting this param: 
```
:param parameters: (optional) the parameters to render the SQL query with.
```

This is due to the fact that it executes by initializing a `SnowflakeSqlApiHook` and then executing the queries without ever passing the parameters:
```
        self._hook = SnowflakeSqlApiHook(
            snowflake_conn_id=self.snowflake_conn_id,
            token_life_time=self.token_life_time,
            token_renewal_delta=self.token_renewal_delta,
            deferrable=self.deferrable,
            **self.hook_params,
        )
        self.query_ids = self._hook.execute_query(
            self.sql,  # type: ignore[arg-type]
            statement_count=self.statement_count,
            bindings=self.bindings,
        )
```
This means that parameters passed in and then referenced how they would be in other Snowflake operators - `%(param)s` - will not be resolved and cause the execution to fail.

### What you think should happen instead?

The parameters should be resolved either before the sql is passed to the `SnowflakeSqlApiHook`, or as part of the `SnowflakeSqlApiHook`.

### How to reproduce

To reproduce, try passing any parameter and referencing it in your SQL via this syntax `%(param)s`

### Operating System

all

### Versions of Apache Airflow Providers

Tested with multiple versions, most recently 
```
apache-airflow-providers-snowflake==5.6.1

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Tested this both with the official helm chart locally and with MWAA. Issue occurred in both.

### Anything else?

I was able to get it working by adding these lines 
```
        quoted_params = {k: f""'{v}'"" for k, v in self.parameters.items()}
        rendered_sql = self.sql % quoted_params
        self.sql = rendered_sql
```
before the call to 
```
        self.query_ids = self._hook.execute_query(
            self.sql,  # type: ignore[arg-type]
            statement_count=self.statement_count,
            bindings=self.bindings,
        )
```

However, it may make more sense for the parameters to be passed to execute_query and resolved there, which would require an update to the SnowflakeSqlApiHook instead. Let me know if there's some other way to pass the params that I'm missing.

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",chris-okorodudu,2024-09-05 14:40:19+00:00,[],2024-10-04 01:35:34+00:00,2024-10-04 01:35:33+00:00,https://github.com/apache/airflow/issues/42033,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('provider:snowflake', 'Issues related to Snowflake provider'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2331870710, 'issue_id': 2508000800, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 5, 14, 40, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2392083046, 'issue_id': 2508000800, 'author': 'harjeevanmaan', 'body': '@chris-okorodudu You can add bindings as a keyword argument within `self._hook.execute_query`.\r\n\r\nHere is an example: \r\n```\r\n""bindings"": {\r\n  ""1"": {\r\n    ""type"": ""FIXED"",\r\n    ""value"": ""123""\r\n  }\r\n}\r\n```\r\n\r\nor more details on the correct format, please refer to the following article: [sql-api-bind-variables](https://docs.snowflake.com/en/developer-guide/sql-api/submitting-requests#label-sql-api-bind-variables)\r\nPlease be mindful that Snowflake does not currently support bindings in multi-statement SQL requests.', 'created_at': datetime.datetime(2024, 10, 3, 18, 41, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2392623965, 'issue_id': 2508000800, 'author': 'potiuk', 'body': 'While no entirely fixed - seems that this is on the snowflake side and #42719 at least provides an explanation.', 'created_at': datetime.datetime(2024, 10, 4, 1, 35, 34, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-05 14:40:22 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

harjeevanmaan on (2024-10-03 18:41:01 UTC): @chris-okorodudu You can add bindings as a keyword argument within `self._hook.execute_query`.

Here is an example: 
```
""bindings"": {
  ""1"": {
    ""type"": ""FIXED"",
    ""value"": ""123""
  }
}
```

or more details on the correct format, please refer to the following article: [sql-api-bind-variables](https://docs.snowflake.com/en/developer-guide/sql-api/submitting-requests#label-sql-api-bind-variables)
Please be mindful that Snowflake does not currently support bindings in multi-statement SQL requests.

potiuk on (2024-10-04 01:35:34 UTC): While no entirely fixed - seems that this is on the snowflake side and #42719 at least provides an explanation.

"
2507955312,issue,closed,completed,Can no longer pass a partial as callback in Airflow 2.10,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Passing a partial from `functools.partial` to `on_success_callback` or `on_failure_callback` (either at the task or DAG level) results in the following error:

```
[2024-09-05, 14:15:02 UTC] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=test_callback_partial, task_id=hello_world, run_id=manual__2024-09-05T14:15:00.758095+00:00, execution_date=20240905T141500, start_date=20240905T141502, end_date=20240905T141502
[2024-09-05, 14:15:02 UTC] {standard_task_runner.py:124} ERROR - Failed to execute job 145 for task hello_world ('functools.partial' object has no attribute '__name__'; 53)
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/cli_config.py"", line 49, in command
    return func(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/cli.py"", line 115, in wrapper
    return f(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task
    return ti._run_raw_task(
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 2995, in _run_raw_task
    return _run_raw_task(
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 358, in _run_raw_task
    _run_finished_callback(callbacks=ti.task.on_success_callback, context=context)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 1554, in _run_finished_callback
    log.info(""Executing %s callback"", callback.__name__)
AttributeError: 'functools.partial' object has no attribute '__name__'. Did you mean: '__ne__'?
[2024-09-05, 14:15:02 UTC] {local_task_job_runner.py:261} INFO - Task exited with return code 1
``` 

### What you think should happen instead?

In previous versions of Airflow this worked fine and the partial was executed as any other function. 

The issue was likely introduced by changes to the task instance code which now tries to access `__name__` on the callback function, which doesn't exist in the case of a `partial` object:

```
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 358, in _run_raw_task
    _run_finished_callback(callbacks=ti.task.on_success_callback, context=context)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 1554, in _run_finished_callback
    log.info(""Executing %s callback"", callback.__name__)
```

This code should probably be updated to fallback to use something other than `__name__` if the attribute doesn't exist.

### How to reproduce

The issue can be reproduced by running the following example in Airflow 2.10:

```python
from functools import partial

from airflow import DAG
from airflow.operators.python import PythonOperator
import pendulum


def _hello_world():
    print(""Hello world!"")


def _callback(status: str):
    print(f""Task finished with status: {status}"")


with DAG(
    dag_id=""test_callback_partial"",
    schedule=None,
    start_date=pendulum.yesterday(),
) as dag:
    PythonOperator(
        task_id=""hello_world"", 
        python_callable=_hello_world,
        on_success_callback=[partial(_callback, status=""success"")],
        on_failure_callback=[partial(_callback, status=""failure"")]
    )
```

### Operating System

Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

Deployed (for local development) via the default docker-compose setup of Airflow.

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jrderuiter,2024-09-05 14:24:01+00:00,[],2024-09-05 14:37:29+00:00,2024-09-05 14:37:09+00:00,https://github.com/apache/airflow/issues/42032,"[('kind:bug', 'This is a clearly a bug'), ('duplicate', 'Issue that is duplicated'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2331860782, 'issue_id': 2507955312, 'author': 'potiuk', 'body': 'duplicate of https://github.com/apache/airflow/issues/41563. Can you please @jrderuiter check it in the upcoming 2.10.1rc1 and report in [Status of testing of 2.10.1rc1](https://github.com/apache/airflow/issues/41956). Confirmation that the issue is fixed there will be much appreciated.', 'created_at': datetime.datetime(2024, 9, 5, 14, 37, 9, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-09-05 14:37:09 UTC): duplicate of https://github.com/apache/airflow/issues/41563. Can you please @jrderuiter check it in the upcoming 2.10.1rc1 and report in [Status of testing of 2.10.1rc1](https://github.com/apache/airflow/issues/41956). Confirmation that the issue is fixed there will be much appreciated.

"
2507737586,issue,closed,completed,"Fail the PR if it modifies old ""ui""","In case PR contains changes to the ""old"" uI (""airflow/www"") and the PR does not have the ""legacy UI"" label set by the maintainer, the PR should fail. This is in order to limit changes that are implemented in the old UI to only bugfixes and necessary changes to enable ""new UI"" in Airflow 3.0.

This can be easily done via producing the right outputs in Breeze's selective checks.

* Breeze already has features to see if files in the incoming PR have belong to certain ""file group"" (and we have both ""WWW"" and ""UI"" groups defined
* Breeze already has access to labels that are assigned to PR


So it shoudl be as easy as simply exiting with error in selective checks and informing the user `Please ask maintainer to assign the ""legacy UI"" label to the PR in order to continue`",potiuk,2024-09-05 12:56:28+00:00,['bugraoz93'],2024-10-05 01:08:20+00:00,2024-10-05 01:08:20+00:00,https://github.com/apache/airflow/issues/42031,"[('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2331619236, 'issue_id': 2507737586, 'author': 'potiuk', 'body': 'Depends on a consensus in https://lists.apache.org/thread/db69p9fyt34d5tgo8pywpsp8ybjvflry', 'created_at': datetime.datetime(2024, 9, 5, 12, 59, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2331699169, 'issue_id': 2507737586, 'author': 'pierrejeambrun', 'body': '> Depends on a consensus in https://lists.apache.org/thread/db69p9fyt34d5tgo8pywpsp8ybjvflry\r\n\r\nYes depending on the outcome we might also want to add the `api_connexion` folder as we will also most likely migrate public endpoints to FastAPI and want to also limit contributions in that area.', 'created_at': datetime.datetime(2024, 9, 5, 13, 33, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2331703923, 'issue_id': 2507737586, 'author': 'pierrejeambrun', 'body': 'Deleted an automated comment that looked like fishing, inviting people to an external weird looking URL. We might want to pay attention to that user in case it comes back again on other issues.', 'created_at': datetime.datetime(2024, 9, 5, 13, 35, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2331744545, 'issue_id': 2507737586, 'author': 'potiuk', 'body': 'Yes. This is something we discuss in security groups (various) - there are currently very active actors with sophisticated methods where GitHub issues and PRs are getting used  example here: https://socket.dev/blog/github-users-targeted-by-new-wave-of-spambots-promoting-malicious-downloads\r\n\r\nI will write an email to devlist for people to be aware of it. The best we can do (I just did it) is to report such attempts to Github via ""Report"" button. This can be done pretty easily and they react rather quickly', 'created_at': datetime.datetime(2024, 9, 5, 13, 52, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2380876344, 'issue_id': 2507737586, 'author': 'bugraoz93', 'body': 'Do you think this issue is ready to be picked up? It looks like there is no one against it. Everyone was also informed about the new construction on FastAPI while some endpoints have already moved.', 'created_at': datetime.datetime(2024, 9, 28, 19, 41, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382896979, 'issue_id': 2507737586, 'author': 'bbovenzi', 'body': '@bugraoz93 Go for it.', 'created_at': datetime.datetime(2024, 9, 30, 11, 15, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2383266150, 'issue_id': 2507737586, 'author': 'pierrejeambrun', 'body': 'Just assigned you @bugraoz93 :)', 'created_at': datetime.datetime(2024, 9, 30, 13, 54, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2383712252, 'issue_id': 2507737586, 'author': 'bugraoz93', 'body': 'Awesome, thanks both! :)', 'created_at': datetime.datetime(2024, 9, 30, 16, 56, tzinfo=datetime.timezone.utc)}]","potiuk (Issue Creator) on (2024-09-05 12:59:18 UTC): Depends on a consensus in https://lists.apache.org/thread/db69p9fyt34d5tgo8pywpsp8ybjvflry

pierrejeambrun on (2024-09-05 13:33:44 UTC): Yes depending on the outcome we might also want to add the `api_connexion` folder as we will also most likely migrate public endpoints to FastAPI and want to also limit contributions in that area.

pierrejeambrun on (2024-09-05 13:35:46 UTC): Deleted an automated comment that looked like fishing, inviting people to an external weird looking URL. We might want to pay attention to that user in case it comes back again on other issues.

potiuk (Issue Creator) on (2024-09-05 13:52:28 UTC): Yes. This is something we discuss in security groups (various) - there are currently very active actors with sophisticated methods where GitHub issues and PRs are getting used  example here: https://socket.dev/blog/github-users-targeted-by-new-wave-of-spambots-promoting-malicious-downloads

I will write an email to devlist for people to be aware of it. The best we can do (I just did it) is to report such attempts to Github via ""Report"" button. This can be done pretty easily and they react rather quickly

bugraoz93 (Assginee) on (2024-09-28 19:41:37 UTC): Do you think this issue is ready to be picked up? It looks like there is no one against it. Everyone was also informed about the new construction on FastAPI while some endpoints have already moved.

bbovenzi on (2024-09-30 11:15:32 UTC): @bugraoz93 Go for it.

pierrejeambrun on (2024-09-30 13:54:30 UTC): Just assigned you @bugraoz93 :)

bugraoz93 (Assginee) on (2024-09-30 16:56:00 UTC): Awesome, thanks both! :)

"
2507543399,issue,closed,completed,XCOM value json viz - CamelCase keys,"### Apache Airflow version

2.10.0

### What happened?

when I vizualize the XCOM in airflow UI , the keys are formatted in CamelCase

![image](https://github.com/user-attachments/assets/f87a91ef-e7e8-44de-a69e-72168b8ec737)


### How to reproduce

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago

with DAG(dag_id=""test"",
         start_date=days_ago(1),
         schedule=None):
    def toto():
        rst = []
        results_list = [""a"", ""b"", ""c"", ""d""]
        for index, item in enumerate(results_list):
            tmp = {
                ""aaaaaaaa_bbbbbb"": index,
                ""cccccccc_dddddd"": item
            }
            rst.append(tmp)

        return rst

    PythonOperator(
        task_id=""a"",
        python_callable=toto,
        do_xcom_push=True,
    )

```

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",raphaelauv,2024-09-05 11:40:38+00:00,[],2024-09-11 23:24:26+00:00,2024-09-11 23:24:26+00:00,https://github.com/apache/airflow/issues/42029,"[('kind:bug', 'This is a clearly a bug'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2336467339, 'issue_id': 2507543399, 'author': 'jscheffl', 'body': 'Uuups, great catch... and this is serious. I tried to trace it down and it seems to be rooted in the automatically generated API client bindngs in JavaScript which use `CamelCasedPropertiesDeep` from the JSON response...\r\n\r\n@bbovenzi do you know how the generated API can be influenced NOT to camelCase all objects received from public API?', 'created_at': datetime.datetime(2024, 9, 7, 22, 18, 40, tzinfo=datetime.timezone.utc)}]","jscheffl on (2024-09-07 22:18:40 UTC): Uuups, great catch... and this is serious. I tried to trace it down and it seems to be rooted in the automatically generated API client bindngs in JavaScript which use `CamelCasedPropertiesDeep` from the JSON response...

@bbovenzi do you know how the generated API can be influenced NOT to camelCase all objects received from public API?

"
2507409337,issue,closed,completed,Test Airflow Release step is flaky.,"The ""Test Airflow Releases"" job is flaky and fails far too often. 

Most of this comes from ""external"" factors - for example installing node packages, pulling images etc. often fail with 500 internal error or ""Rate limit exceeded"" . 

Example https://github.com/apache/airflow/actions/runs/10717569021/job/29717755211?pr=41555  where root cause is installing node packages:

```
 yarn install v1.22.21
  (node:499) [DEP0040] DeprecationWarning: The `punycode` module is deprecated. Please use a userland alternative instead.
  (Use `node --trace-deprecation ...` to show where the warning was created)
  [1/4] Resolving packages...
  [2/4] Fetching packages...
  [] 0/1573[] 7/1573[] 13/1573[] 21/1573[] 29/1573error Error: https://registry.yarnpkg.com/@chakra-ui/skeleton/-/skeleton-2.0.18.tgz: Request failed ""500 Internal Server Error""
      at ResponseError.ExtendableBuiltin (/opt/airflow/files/home/.cache/pre-commit/repoj5n0lz2l/node_env-22.2.0/lib/node_modules/yarn/lib/cli.js:696:66)
      at new ResponseError (/opt/airflow/files/home/.cache/pre-commit/repoj5n0lz2l/node_env-22.2.0/lib/node_modules/yarn/lib/cli.js:802:124)
      at Request.<anonymous> (/opt/airflow/files/home/.cache/pre-commit/repoj5n0lz2l/node_env-22.2.0/lib/node_modules/yarn/lib/cli.js:66218:16)
      at Request.emit (node:events:520:28)
      at module.exports.Request.onRequestResponse (/opt/airflow/files/home/.cache/pre-commit/repoj5n0lz2l/node_env-22.2.0/lib/node_modules/yarn/lib/cli.js:141751:10)
      at ClientRequest.emit (node:events:520:28)
      at HTTPParser.parserOnIncomingClient (node:_http_client:700:27)
      at HTTPParser.parserOnHeadersComplete (node:_http_common:119:17)
      at TLSSocket.socketOnData (node:_http_client:542:22)
      at TLSSocket.emit (node:events:520:28)
  info Visit https://yarnpkg.com/en/docs/cli/install for documentation about this command.
  Traceback (most recent call last):
    File ""./scripts/ci/pre_commit/compile_www_assets.py"", line 71, in <module>
      subprocess.check_call([""yarn"", ""install"", ""--frozen-lockfile""], cwd=os.fspath(www_directory))
    File ""/usr/local/lib/python3.8/subprocess.py"", line 364, in check_call
      raise CalledProcessError(retcode, cmd)
  subprocess.CalledProcessError: Command '['yarn', 'install', '--frozen-lockfile']' returned non-zero exit status 1.
```

The solution to that is likely attempting to retry the whole `breeze` command several times. This ""release process"" is relatively fast (~4 minutes) so retrying it up to 3 times in case of failures will bring the total time to 12 minutes max and should not have much impact on elapsed time or cost.





",potiuk,2024-09-05 10:34:25+00:00,['bugraoz93'],2024-09-09 11:05:21+00:00,2024-09-09 11:05:21+00:00,https://github.com/apache/airflow/issues/42025,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:dev-tools', ''), ('area:CI', ""Airflow's tests and continious integration"")]","[{'comment_id': 2331313681, 'issue_id': 2507409337, 'author': 'potiuk', 'body': 'Some implementation comments:\r\n\r\n* there is no native ""retry"" functionality in GitHub actions\r\n* there are some retry actions/jobs but in the ASF we limit usage of 3rd-party actions due to security concerns and we need to approve 3rd-party actions (specific versions/commits) via INFRA\r\n* It should be as easy as turning the bash command into a ""for loop"" trying the breeze command up to three times if it fails.', 'created_at': datetime.datetime(2024, 9, 5, 11, 47, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2331531102, 'issue_id': 2507409337, 'author': 'bugraoz93', 'body': 'Hey Jarek, I would like to work on this one. Thanks for the clear explanation and possible solutions!', 'created_at': datetime.datetime(2024, 9, 5, 12, 44, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2331553154, 'issue_id': 2507409337, 'author': 'potiuk', 'body': 'Go ahead :)', 'created_at': datetime.datetime(2024, 9, 5, 12, 47, 34, tzinfo=datetime.timezone.utc)}]","potiuk (Issue Creator) on (2024-09-05 11:47:35 UTC): Some implementation comments:

* there is no native ""retry"" functionality in GitHub actions
* there are some retry actions/jobs but in the ASF we limit usage of 3rd-party actions due to security concerns and we need to approve 3rd-party actions (specific versions/commits) via INFRA
* It should be as easy as turning the bash command into a ""for loop"" trying the breeze command up to three times if it fails.

bugraoz93 (Assginee) on (2024-09-05 12:44:17 UTC): Hey Jarek, I would like to work on this one. Thanks for the clear explanation and possible solutions!

potiuk (Issue Creator) on (2024-09-05 12:47:34 UTC): Go ahead :)

"
2507405674,issue,open,,gitSync: allow gitSync.env to take a env var from a secret reference,"### Description

Currently the schema for gitSync.env does not allow you to define env from secrets like:
```
env:
        - name: GIT_SYNC_USERNAME
          valueFrom:
            secretKeyRef:
              name: gitsync-secret
              key: username
        - name: GITSYNC_USERNAME
          valueFrom:
            secretKeyRef:
              name: gitsync-secret
              key: username
        - name: GIT_SYNC_PASSWORD
          valueFrom:
            secretKeyRef:
              name: gitsync-secret
              key: pwd
        - name: GITSYNC_PASSWORD
          valueFrom:
            secretKeyRef:
              name: gitsync-secret
              key: pwd
```

This is because the schema is overly restrictive and doesn't just reference the upstream [V1EnvVar](https://github.com/apache/airflow/blob/58820a914c9b84c378c9bc1acd4ddbcb4e4a3497/chart/values.schema.json#L8173-L8175). See: https://github.com/apache/airflow/blob/main/chart/values.schema.json#L8557-L8583

### Use case/motivation

_No response_

### Related issues

https://github.com/apache/airflow/pull/39031

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",connorlwilkes,2024-09-05 10:32:34+00:00,[],2024-09-05 10:34:54+00:00,,https://github.com/apache/airflow/issues/42024,"[('kind:feature', 'Feature Requests'), ('area:helm-chart', 'Airflow Helm Chart'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2331178598, 'issue_id': 2507405674, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 5, 10, 32, 38, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-05 10:32:38 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2507157238,issue,closed,completed,test_listener_logs_failed_serialization is flaky ,"### Body

The test_listener_logs_failed_serialization is flaky - likely due to a race condition. This should be improve.

Example failure: 

https://github.com/apache/airflow/actions/runs/10714765649/job/29709338209?pr=41331#step:7:5256


```
=================================== FAILURES ===================================
___________________ test_listener_logs_failed_serialization ____________________

self = <MagicMock name='mock.warning' id='139678763968448'>

    def assert_called_once(self):
        """"""assert that the mock was called only once.
        """"""
        if not self.call_count == 1:
            msg = (""Expected '%s' to have been called once. Called %s times.%s""
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'warning' to have been called once. Called 0 times.

/usr/local/lib/python3.8/unittest/mock.py:892: AssertionError

During handling of the above exception, another exception occurred:

    def test_listener_logs_failed_serialization():
        listener = OpenLineageListener()
        listener.log = MagicMock()
        listener.adapter = OpenLineageAdapter(
            client=OpenLineageClient(transport=ConsoleTransport(config=ConsoleConfig()))
        )
        event_time = dt.datetime.now()
    
        fut = listener.submit_callable(
            listener.adapter.dag_failed,
            dag_id="""",
            run_id="""",
            end_date=event_time,
            execution_date=threading.Thread(),
            dag_run_state=DagRunState.FAILED,
            task_ids=[""task_id""],
            msg="""",
        )
        assert fut.exception(10)
>       listener.log.warning.assert_called_once()
E       AssertionError: Expected 'warning' to have been called once. Called 0 times.

tests/providers/openlineage/plugins/test_listener.py:628: AssertionError
----------------------------- Captured stdout call -----------------------------
```

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",potiuk,2024-09-05 08:41:53+00:00,[],2024-09-06 05:39:52+00:00,2024-09-05 20:59:54+00:00,https://github.com/apache/airflow/issues/42020,"[('area:logging', ''), ('kind:meta', 'High-level information important to the community'), ('Quarantine', 'Issues that are occasionally failing and are quarantined')]","[{'comment_id': 2330947743, 'issue_id': 2507157238, 'author': 'potiuk', 'body': 'cc: @mobuchowski @kacpermuda', 'created_at': datetime.datetime(2024, 9, 5, 8, 42, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333271325, 'issue_id': 2507157238, 'author': 'potiuk', 'body': '🙇 @mobuchowski', 'created_at': datetime.datetime(2024, 9, 6, 5, 39, 51, tzinfo=datetime.timezone.utc)}]","potiuk (Issue Creator) on (2024-09-05 08:42:15 UTC): cc: @mobuchowski @kacpermuda

potiuk (Issue Creator) on (2024-09-06 05:39:51 UTC): 🙇 @mobuchowski

"
2507112987,issue,closed,completed,color of task state in task tries doesn't work for retries,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

The task try color does not display for tasks that result from retries. 
![tempsnip](https://github.com/user-attachments/assets/faa85de0-f5e6-48d0-94b1-084668f5bca3)
Task try no.2 originally failed and created a retry. 3-8 failed and created retries but show no state color. Task try no.9 finally succeded which changed the color for task no.2 to success which actually failed.

### What you think should happen instead?

I think the color should represent the actual task try state.

### How to reproduce

Enable retries on the dag and let a task fail on purpose. 

### Operating System

Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",schlichterer,2024-09-05 08:20:30+00:00,[],2024-09-05 09:20:27+00:00,2024-09-05 09:20:27+00:00,https://github.com/apache/airflow/issues/42018,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2330904421, 'issue_id': 2507112987, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 5, 8, 20, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2330957487, 'issue_id': 2507112987, 'author': 'raphaelauv', 'body': 'duplicate of https://github.com/apache/airflow/issues/41742 ?', 'created_at': datetime.datetime(2024, 9, 5, 8, 47, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2331026171, 'issue_id': 2507112987, 'author': 'schlichterer', 'body': ""Yes sorry thank you I didn't find that one."", 'created_at': datetime.datetime(2024, 9, 5, 9, 20, 20, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-05 08:20:34 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

raphaelauv on (2024-09-05 08:47:04 UTC): duplicate of https://github.com/apache/airflow/issues/41742 ?

schlichterer (Issue Creator) on (2024-09-05 09:20:20 UTC): Yes sorry thank you I didn't find that one.

"
2507064675,issue,open,,Dependency to FAB is removed in Connection Forms,"### Description

Relates to #39593

AFter a PoC in https://github.com/apache/airflow/pull/41656 and the Airflow 3 Dev Call this is a follow-up issue to keep track of the efforts to remove the FAB dependency from ProvidersManager Connection Forms for Custom fields.

Technical Idea:
- Make the custom form field definition into the provider.yaml description as structure
  - NO python code!
  - Follow the structure of JSON Schema like Custom Form fields in DAG Trigger UI (just as YAML/JSON dict)
  - On scheduler where providers are loaded persist the structure to DB
  - WebUI REST API can load the dict information from DB and render UI w/o need to have provider code available and initialized
    - Benefit: Less security risk of exposure due to bad code/dependencies, faster loading of web server API on start (=less imports needed)
- Migrate existing providers
  - Option 1: Manually migrate providers once from code to YAML
  - Option 2: Implement a automated migration similar to #41656 to read Python code and create YAML as pre-commit hook to keep it in sync

### Use case/motivation

FAB should be an optional dependency and UI should eb able to render custom form fields w/o FAB installed.

Currently custom connection fields are defined in Python Code. This makes additional dependency that Python Provider code must be deployed and loaded in Webserver as well as FAB is needed to render the UI.

### Related issues

Airflow 3: #39593
PoC PR: #41656 

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jscheffl,2024-09-05 07:58:36+00:00,[],2024-09-19 18:48:46+00:00,,https://github.com/apache/airflow/issues/42016,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('airflow3.0:candidate', 'Potential candidates for Airflow 3.0')]",[],
2506930306,issue,open,,Grouping Dataset Events to Trigger DAGs,"### Description

_No response_

### Use case/motivation

To handle multiple dataset updates efficiently and avoid triggering a DAG for every small dataset update (like a tiny partition), you can implement a ""batching"" mechanism where the DAG waits for a group of dataset events before triggering. This way, you avoid redundant DAG runs and ensure the DAG only executes when enough meaningful updates have occurred.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",dirrao,2024-09-05 06:54:44+00:00,[],2024-10-02 07:25:31+00:00,,https://github.com/apache/airflow/issues/42015,"[('kind:feature', 'Feature Requests'), ('area:datasets', 'Issues related to the datasets feature')]","[{'comment_id': 2340108844, 'issue_id': 2506930306, 'author': 'dirrao', 'body': 'Hi @uranusjr, \r\ncould you provide your feedback on this feature request when you have a moment?', 'created_at': datetime.datetime(2024, 9, 10, 9, 12, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2344613294, 'issue_id': 2506930306, 'author': 'uranusjr', 'body': 'Isn’t this basically the idea behind AIP-76?', 'created_at': datetime.datetime(2024, 9, 11, 20, 10, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2355536974, 'issue_id': 2506930306, 'author': 'dirrao', 'body': ""Possibly related, but I'm not sure. Does that include batching the events?"", 'created_at': datetime.datetime(2024, 9, 17, 12, 8, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2387752964, 'issue_id': 2506930306, 'author': 'dirrao', 'body': 'The AIP-82 contains this functionality.\r\nhttps://cwiki.apache.org/confluence/display/AIRFLOW/AIP-82+External+event+driven+scheduling+in+Airflow', 'created_at': datetime.datetime(2024, 10, 2, 6, 56, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2387762205, 'issue_id': 2506930306, 'author': 'uranusjr', 'body': 'The only mention on batching I can find in AIP-82 is under the _Out of Scope_ section.\r\n\r\nAIP-76 does not do batching, but works on a different level, separating individual events from triggering the actual downstream run. I do not know if it fits your use case; only you can decide.', 'created_at': datetime.datetime(2024, 10, 2, 7, 2, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2387769345, 'issue_id': 2506930306, 'author': 'dirrao', 'body': ""Currently, we're using a pull-based mechanism and triggering dataset creation events via the REST API. However, if we want to trigger events only when there are enough accumulated, we have to maintain an external state, group the events, and then send the dataset creation request. It would be great to have this feature built-in as part of the existing functionality."", 'created_at': datetime.datetime(2024, 10, 2, 7, 6, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2387798027, 'issue_id': 2506930306, 'author': 'dirrao', 'body': 'It is challenging to map data warehouse partitions to the dataset partitions mentioned in AIP-76. As a result, batching events for triggering DAG runs is not feasible in this context.', 'created_at': datetime.datetime(2024, 10, 2, 7, 25, 30, tzinfo=datetime.timezone.utc)}]","dirrao (Issue Creator) on (2024-09-10 09:12:40 UTC): Hi @uranusjr, 
could you provide your feedback on this feature request when you have a moment?

uranusjr on (2024-09-11 20:10:50 UTC): Isn’t this basically the idea behind AIP-76?

dirrao (Issue Creator) on (2024-09-17 12:08:10 UTC): Possibly related, but I'm not sure. Does that include batching the events?

dirrao (Issue Creator) on (2024-10-02 06:56:01 UTC): The AIP-82 contains this functionality.
https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-82+External+event+driven+scheduling+in+Airflow

uranusjr on (2024-10-02 07:02:17 UTC): The only mention on batching I can find in AIP-82 is under the _Out of Scope_ section.

AIP-76 does not do batching, but works on a different level, separating individual events from triggering the actual downstream run. I do not know if it fits your use case; only you can decide.

dirrao (Issue Creator) on (2024-10-02 07:06:56 UTC): Currently, we're using a pull-based mechanism and triggering dataset creation events via the REST API. However, if we want to trigger events only when there are enough accumulated, we have to maintain an external state, group the events, and then send the dataset creation request. It would be great to have this feature built-in as part of the existing functionality.

dirrao (Issue Creator) on (2024-10-02 07:25:30 UTC): It is challenging to map data warehouse partitions to the dataset partitions mentioned in AIP-76. As a result, batching events for triggering DAG runs is not feasible in this context.

"
2506046022,issue,open,,AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES  and AIRFLOW__KUBERNETES_SECRETS not working properly,"### Apache Airflow Provider(s)

cncf-kubernetes

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes 8.3.4

### Apache Airflow version

2.9.2

### Operating System

Docker - Debian 12

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

I am deploying airlfow in Azure Kubernetes Service version 1.29 using helm chart version 1.15

### What happened

I'm deploying Apache Airflow 2.9.2 with the Kubernetes Executor in an AKS cluster.

KubernetesExecutor will create a pod for each of the tasks to be executed. 

I see that this pod is defined by a pod template. You can use the default pod template or you can create a pod template from the [official helm chart](https://github.com/apache/airflow/blob/main/chart/values.yaml#L2575).

I have created this template in my helm chart:

```yaml
podTemplate: |-
      apiVersion: v1
      kind: Pod
      metadata:
        name: airflow-task-pod
      spec:
        serviceAccountName: airflow-worker
        containers:
          - name: base
            image: {{ .Values.images.airflow.repository }}:{{ .Values.images.airflow.tag }}
            imagePullPolicy: {{ .Values.images.airflow.pullPolicy }}
            resources:
              requests:
                cpu: 0.5
                memory: ""1Gi""
              limits:
                cpu: 2
                memory: ""3Gi""
            env:
              - name: AIRFLOW__CORE__EXECUTOR
                value: LocalExecutor
              {{- include ""custom_airflow_environment"" . | indent 6 }}
              - name: AIRFLOW_CONN_AIRFLOW_DB
                valueFrom:
                  secretKeyRef:
                    name: airflow-metadata-connection
                    key: connection
              - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN
                valueFrom:
                  secretKeyRef:
                    name: airflow-metadata-connection
                    key: connection
              - name: AIRFLOW__CORE__FERNET_KEY
                valueFrom:
                  secretKeyRef:
                    name: airflow-fernet-key
                    key: fernet-key
              
        tolerations:
          - key: ""kubernetes.azure.com/scalesetpriority""
            operator: ""Equal""
            value: ""spot""
            effect: ""NoSchedule""
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: ""kubernetes.azure.com/scalesetpriority""
                      operator: In
                      values:
                        - ""spot""
```

This is working correctly, since I can pass to the pod the env variables that I have defined in the [extraEnv section](https://github.com/apache/airflow/blob/main/chart/values.yaml#370) and the secrets that I have created in the [secret section](https://github.com/apache/airflow/blob/main/chart/values.yaml#287).

```
{{- include ""custom_airflow_environment"" . | indent 6 }}
```

This will execute the function defined in the [`_helpers.yaml`](https://github.com/apache/airflow/blob/main/chart/templates/_helpers.yaml#L148) from the airflow helm chart.

What I find strange is that when the helm chart detects that it is KubernetesExecutor, in the deployments it executes the same helper function, which creates env variables with the prefix [`AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__`](https://github.com/apache/airflow/blob/main/chart/templates/_helpers.yaml#L154) and secrets with the prefix [`AIRFLOW__KUBERNETES_SECRETS__`](https://github.com/apache/airflow/blob/main/chart/templates/_helpers.yaml#L168).


As you can see in the image, I load this parameters in the configurations

[![enter image description here][1]][1]


I was expecting that this secrets and env variables are passed to the Pod created by KubernetesExecutor, but it is not the case.

I see this [PR from 2019](https://github.com/apache/airflow/pull/4627/files), where there was documentation related to the `kubernetes_environment_variables` and `kubernetes`, but I cannot see it any more in the current master branch in the [airflow github repository](https://github.com/apache/airflow).

In the airflow repository I see that the pod that is generated by the `pod_generator.py` file, and it is creating only one env variable, [in this line of code](https://github.com/apache/airflow/blob/main/airflow/providers/cncf/kubernetes/pod_generator.py#L444).

[![enter image description here][2]][2]


So I wonder, if this is really something that was used before and now it is not used anymore? should it be removed from the helm chart or am I configuring it incorrectly?


  [1]: https://i.sstatic.net/Yj4qINAx.png
  [2]: https://i.sstatic.net/v8mKyICo.png

### What you think should happen instead

The env variables and secrets created by the helm chart with the prefixes `AIRFLOW__KUBERNETES_ENVIRONMENT_VARIABLES__`  and secrets with the prefix `AIRFLOW__KUBERNETES_SECRETS__` should be passed to the pod created by KubernetesExecutor.

There is no documentation on these variables, if they are not used for this purpose, define in the documentation or in the helm chart what they are used for.

### How to reproduce

use the following `values.yaml`

```yaml
executor: ""KubernetesExecutor""

env:
  - name: AIRFLOW_HOME
    value: '/usr/local/airflow'
  - name: AIRFLOW__MERCHANT__URL
    value: ${MERCHANT_URL}
  - name: MAILGUN_CUSTOM_EMAIL_BASE_URL
    value: 'emails.tapp.cafe'
  - name: MAILGUN_BASE_URL
    value: 'emails.tapp.cafe'
  - name: AIRFLOW_CONN_INSIGHTS_SA
    value: ${AIRFLOW_CONN_INSIGHTS_SA}
  - name: AIRFLOW_CONN_AZURE_LOGS
    value: ${AIRFLOW_CONN_AZURE_LOGS}
  - name: AIRFLOW_CONN_EVENTHUB
    value: ${AIRFLOW_CONN_EVENTHUB}
  - name: AIRFLOW_CONN_MERCHANT_API
    value: ${AIRFLOW_CONN_MERCHANT_API}
  - name: AIRFLOW_CONN_KNMI
    value: 'https://www.daggegevens.knmi.nl'

podTemplate: |-
  apiVersion: v1
  kind: Pod
  metadata:
    name: airflow-task-pod
    annotations:
      ad.datadoghq.com/airflow-webserver.check_names: '[""airflow""]'
      ad.datadoghq.com/airflow-webserver.init_configs: ""[{}]""
      ad.datadoghq.com/airflow-webserver.instances: |
        [
          {
            ""url"": ""http://%%host%%:8080"",
            ""airflow_environment"": ${AIRFLOW_CORE_ENV},
            ""tags"": [""env:${AIRFLOW_CORE_ENV}"", ""component:worker-pod"", ""cloud_provider:azure""]
          }
        ]
    labels:
      azure.workload.identity/use: ""true""
  spec:
    containers:
      - name: base
        image: {{ .Values.images.airflow.repository }}:{{ .Values.images.airflow.tag }}
        imagePullPolicy: {{ .Values.images.airflow.pullPolicy }}
        resources:
          requests:
            cpu: 0.5
            memory: ""1Gi""
          limits:
            cpu: 2
            memory: ""3Gi""
    tolerations:
      - key: ""kubernetes.azure.com/scalesetpriority""
        operator: ""Equal""
        value: ""spot""
        effect: ""NoSchedule""
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: ""kubernetes.azure.com/scalesetpriority""
                  operator: In
                  values:
                    - ""spot""

```

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jordi-crespo,2024-09-04 18:42:21+00:00,[],2024-09-04 18:44:45+00:00,,https://github.com/apache/airflow/issues/42008,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2505872554,issue,closed,completed,StaleDataError when running DAG with dynamic task mapping,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.1

### What happened?

We are noticing our Scheduler and Executors going down randomly whenever its processing a DAG with Dynamic task mapping tasks in it. On logs, we see DB stale data error on task instance table when scheduler loop tries to process expanded task instances in dag. We use Local Kubernetes executor and logs show all (scheduler, Local executor and Kubernetes executors) going down

`[2024-09-04T16:27:39.420+0000] {scheduler_job_runner.py:860} ERROR - Exception when executing SchedulerJob._run_scheduler_loop
Traceback (most recent call last):
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py"", line 843, in _execute
self._run_scheduler_loop()
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py"", line 975, in _run_scheduler_loop
num_queued_tis = self._do_scheduling(session)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1057, in _do_scheduling
callback_tuples = self._schedule_all_dag_runs(guard, dag_runs, session)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/retries.py"", line 89, in wrapped_function
for attempt in run_with_db_retries(max_retries=retries, logger=logger, **retry_kwargs):
File ""/home/airflow/.local/lib/python3.11/site-packages/tenacity/__init__.py"", line 347, in __iter__
do = self.iter(retry_state=retry_state)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/tenacity/__init__.py"", line 314, in iter
return fut.result()
^^^^^^^^^^^^
File ""/usr/local/lib/python3.11/concurrent/futures/_base.py"", line 449, in result
return self.__get_result()
^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.11/concurrent/futures/_base.py"", line 401, in __get_result
raise self._exception
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/retries.py"", line 98, in wrapped_function
return func(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1401, in _schedule_all_dag_runs
callback_tuples = [(run, self._schedule_dag_run(run, session=session)) for run in dag_runs]
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1401, in <listcomp>
callback_tuples = [(run, self._schedule_dag_run(run, session=session)) for run in dag_runs]
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1469, in _schedule_dag_run
schedulable_tis, callback_to_run = dag_run.update_state(session=session, execute_callbacks=False)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py"", line 76, in wrapper
return func(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagrun.py"", line 798, in update_state
info = self.task_instance_scheduling_decisions(session)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py"", line 76, in wrapper
return func(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagrun.py"", line 954, in task_instance_scheduling_decisions
schedulable_tis, changed_tis, expansion_happened = self._get_ready_tis(
^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagrun.py"", line 1063, in _get_ready_tis
new_tis = _expand_mapped_task_if_needed(schedulable)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagrun.py"", line 1037, in _expand_mapped_task_if_needed
expanded_tis, _ = ti.task.expand_mapped_task(self.run_id, session=session)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/abstractoperator.py"", line 608, in expand_mapped_task
session.flush()
File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py"", line 3449, in flush
self._flush(objects)
File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py"", line 3588, in _flush
with util.safe_reraise():
File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
compat.raise_(
File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/util/compat.py"", line 211, in raise_
raise exception
File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py"", line 3549, in _flush
flush_context.execute()
File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/unitofwork.py"", line 456, in execute
rec.execute(self)
File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/unitofwork.py"", line 630, in execute
util.preloaded.orm_persistence.save_obj(
File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/persistence.py"", line 237, in save_obj
_emit_update_statements(
File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/persistence.py"", line 1035, in _emit_update_statements
raise orm_exc.StaleDataError(
sqlalchemy.orm.exc.StaleDataError: UPDATE statement on table 'task_instance' expected to update 1 row(s); 0 were matched.
[2024-09-04T16:27:39.425+0000] {local_executor.py:403} INFO - Shutting down LocalExecutor; waiting for running tasks to finish. Signal again if you don't want to wait.
[2024-09-04T16:27:39.703+0000] {kubernetes_executor.py:762} INFO - Shutting down Kubernetes executor
[2024-09-04T16:27:39.853+0000] {scheduler_job_runner.py:872} INFO - Exited execute loop
Traceback (most recent call last):
File ""/home/airflow/.local/bin/airflow"", line 8, in <module>
sys.exit(main())
^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/__main__.py"", line 58, in main
args.func(args)
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/cli_config.py"", line 49, in command
return func(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/cli.py"", line 114, in wrapper
[2024-09-04 16:27:39 +0000] [410] [INFO] Handling signal: term
return f(*args, **kwargs)
^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/providers_configuration_loader.py"", line 55, in wrapped_function
return func(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/commands/scheduler_command.py"", line 58, in scheduler
run_command_with_daemon_option(
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/commands/daemon_utils.py"", line 85, in run_command_with_daemon_option
callback()
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/commands/scheduler_command.py"", line 61, in <lambda>
[2024-09-04 16:27:39 +0000] [417] [INFO] Worker exiting (pid: 417)
[2024-09-04 16:27:39 +0000] [670] [INFO] Worker exiting (pid: 670)
callback=lambda: _run_scheduler_job(args),
^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/cli/commands/scheduler_command.py"", line 49, in _run_scheduler_job
run_job(job=job_runner.job, execute_callable=job_runner._execute)
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py"", line 79, in wrapper
return func(*args, session=session, **kwargs)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/job.py"", line 402, in run_job
return execute_job(job, execute_callable=execute_callable)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/job.py"", line 431, in execute_job
ret = execute_callable()
^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py"", line 843, in _execute
self._run_scheduler_loop()
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py"", line 975, in _run_scheduler_loop
num_queued_tis = self._do_scheduling(session)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1057, in _do_scheduling
callback_tuples = self._schedule_all_dag_runs(guard, dag_runs, session)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/retries.py"", line 89, in wrapped_function
for attempt in run_with_db_retries(max_retries=retries, logger=logger, **retry_kwargs):
File ""/home/airflow/.local/lib/python3.11/site-packages/tenacity/__init__.py"", line 347, in __iter__
do = self.iter(retry_state=retry_state)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/tenacity/__init__.py"", line 314, in iter
return fut.result()
^^^^^^^^^^^^
File ""/usr/local/lib/python3.11/concurrent/futures/_base.py"", line 449, in result
return self.__get_result()
^^^^^^^^^^^^^^^^^^^
File ""/usr/local/lib/python3.11/concurrent/futures/_base.py"", line 401, in __get_result
raise self._exception
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/retries.py"", line 98, in wrapped_function
return func(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1401, in _schedule_all_dag_runs
callback_tuples = [(run, self._schedule_dag_run(run, session=session)) for run in dag_runs]
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1401, in <listcomp>
callback_tuples = [(run, self._schedule_dag_run(run, session=session)) for run in dag_runs]
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1469, in _schedule_dag_run
schedulable_tis, callback_to_run = dag_run.update_state(session=session, execute_callbacks=False)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py"", line 76, in wrapper
return func(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagrun.py"", line 798, in update_state
info = self.task_instance_scheduling_decisions(session)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/session.py"", line 76, in wrapper
return func(*args, **kwargs)
^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagrun.py"", line 954, in task_instance_scheduling_decisions
schedulable_tis, changed_tis, expansion_happened = self._get_ready_tis(
^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagrun.py"", line 1063, in _get_ready_tis
new_tis = _expand_mapped_task_if_needed(schedulable)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagrun.py"", line 1037, in _expand_mapped_task_if_needed
expanded_tis, _ = ti.task.expand_mapped_task(self.run_id, session=session)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/abstractoperator.py"", line 608, in expand_mapped_task
session.flush()
File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py"", line 3449, in flush
self._flush(objects)
File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py"", line 3588, in _flush
with util.safe_reraise():
File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/util/langhelpers.py"", line 70, in __exit__
compat.raise_(
File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/util/compat.py"", line 211, in raise_
raise exception
File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/session.py"", line 3549, in _flush
flush_context.execute()
File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/unitofwork.py"", line 456, in execute
rec.execute(self)
File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/unitofwork.py"", line 630, in execute
util.preloaded.orm_persistence.save_obj(
File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/persistence.py"", line 237, in save_obj
_emit_update_statements(
File ""/home/airflow/.local/lib/python3.11/site-packages/sqlalchemy/orm/persistence.py"", line 1035, in _emit_update_statements
raise orm_exc.StaleDataError(
sqlalchemy.orm.exc.StaleDataError: UPDATE statement on table 'task_instance' expected to update 1 row(s); 0 were matched.
[2024-09-04 16:27:39 +0000] [410] [INFO] Shutting down: Master`

### What you think should happen instead?

DAGs with dynamic task mapping to proceed as expected and no restarts on scheudler or executors.

### How to reproduce

Have a DAG with dynamic task mapping enabled creating 100+ tasks and run them in schedules of 30 mins.

### Operating System

PRETTY_NAME=""Debian GNU/Linux 12 (bookworm)"" NAME=""Debian GNU/Linux"" VERSION_ID=""12"" VERSION=""12 (bookworm)"" VERSION_CODENAME=bookworm ID=debian

### Versions of Apache Airflow Providers

apache-airflow                           2.9.1
apache-airflow-providers-alibaba         2.7.3
apache-airflow-providers-amazon          8.20.0
apache-airflow-providers-celery          3.6.2
apache-airflow-providers-cncf-kubernetes 8.1.1
apache-airflow-providers-common-io       1.3.1
apache-airflow-providers-common-sql      1.12.0
apache-airflow-providers-databricks      6.3.0
apache-airflow-providers-docker          3.10.0
apache-airflow-providers-elasticsearch   5.3.4
apache-airflow-providers-fab             1.0.4
apache-airflow-providers-ftp             3.8.0
apache-airflow-providers-google          10.17.0
apache-airflow-providers-grpc            3.4.1
apache-airflow-providers-hashicorp       3.6.4
apache-airflow-providers-http            4.10.1
apache-airflow-providers-imap            3.5.0
apache-airflow-providers-jenkins         3.5.1
apache-airflow-providers-microsoft-azure 10.0.0
apache-airflow-providers-mysql           5.5.4
apache-airflow-providers-odbc            4.5.0
apache-airflow-providers-openlineage     1.7.0
apache-airflow-providers-oracle          3.9.2
apache-airflow-providers-postgres        5.10.2
apache-airflow-providers-redis           3.6.1
apache-airflow-providers-sendgrid        3.4.0
apache-airflow-providers-sftp            4.9.1
apache-airflow-providers-slack           8.6.2
apache-airflow-providers-smtp            1.6.1
apache-airflow-providers-snowflake       5.4.0
apache-airflow-providers-sqlite          3.7.1
apache-airflow-providers-ssh             3.10.1

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

Issue occurs once or twice a day.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",ykanagarajapandiyan,2024-09-04 16:56:31+00:00,[],2024-11-10 08:03:45+00:00,2024-11-10 08:03:45+00:00,https://github.com/apache/airflow/issues/42006,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:dynamic-task-mapping', 'AIP-42'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2329562898, 'issue_id': 2505872554, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 4, 16, 56, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2413884711, 'issue_id': 2505872554, 'author': 'mro-dmoura', 'body': ""I'm experiencing the same issue. Using Celery Executor in EKS with PostgreSQL on RDS as metadata database. The dynamic generated tasks affect other tasks and DAGs, causing random task killed without logs:\r\n[2024-10-14, 05:14:48 CDT] {scheduler_job_runner.py:843} ERROR - Executor reports task instance <TaskInstance: taskname [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?\r\n\r\nAnd in the scheduler logs, the StaleDataError:\r\nsqlalchemy.orm.exc.StaleDataError: UPDATE statement on table 'task_instance' expected to update 1 row(s); 0 were matched."", 'created_at': datetime.datetime(2024, 10, 15, 13, 13, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2417845465, 'issue_id': 2505872554, 'author': 'ykanagarajapandiyan', 'body': ""Setting param 'use-row-level-locking' to True prevents these DB errors. [use-row-level-locking](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#use-row-level-locking). Even though the doc recommends to set this True only when multiple schedulers are used, I think this conflicts with param  [schedule-after-task-execution](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#schedule-after-task-execution) where a mini scheduler runs and tries to update the same rows that scheduler is working on.\r\n\r\nBut changing this param(use-row-level-locking) to True affects the task start times. I have noticed that even when the task completes, scheduler sometimes takes about 5-10 mins to terminate the pod (we use Local Kubernetes executor) and start the next ones."", 'created_at': datetime.datetime(2024, 10, 16, 20, 7, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2420655212, 'issue_id': 2505872554, 'author': 'potiuk', 'body': 'Quite likely https://github.com/apache/airflow/pull/39745 from 2.9.2 should improve mini-scheduler behaviour.\r\n\r\nI recommend to upgrade to latest Airflow version and see if you seill have similar issues.', 'created_at': datetime.datetime(2024, 10, 17, 21, 42, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2438507795, 'issue_id': 2505872554, 'author': 'ykanagarajapandiyan', 'body': ""Upgrading Airflow to latest (2.10.2) has resolved Stale data errors with single scheduler + configuration 'use-row-level-locking' set to False."", 'created_at': datetime.datetime(2024, 10, 25, 18, 8, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466631076, 'issue_id': 2505872554, 'author': 'eladkal', 'body': 'mini scheduler is planned to be removed in Airflow 3 so the underlying issue of possible conflicts is no longer a concern\r\nhttps://github.com/apache/airflow/pull/43741', 'created_at': datetime.datetime(2024, 11, 10, 8, 3, 45, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-04 16:56:35 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

mro-dmoura on (2024-10-15 13:13:56 UTC): I'm experiencing the same issue. Using Celery Executor in EKS with PostgreSQL on RDS as metadata database. The dynamic generated tasks affect other tasks and DAGs, causing random task killed without logs:
[2024-10-14, 05:14:48 CDT] {scheduler_job_runner.py:843} ERROR - Executor reports task instance <TaskInstance: taskname [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?

And in the scheduler logs, the StaleDataError:
sqlalchemy.orm.exc.StaleDataError: UPDATE statement on table 'task_instance' expected to update 1 row(s); 0 were matched.

ykanagarajapandiyan (Issue Creator) on (2024-10-16 20:07:55 UTC): Setting param 'use-row-level-locking' to True prevents these DB errors. [use-row-level-locking](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#use-row-level-locking). Even though the doc recommends to set this True only when multiple schedulers are used, I think this conflicts with param  [schedule-after-task-execution](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#schedule-after-task-execution) where a mini scheduler runs and tries to update the same rows that scheduler is working on.

But changing this param(use-row-level-locking) to True affects the task start times. I have noticed that even when the task completes, scheduler sometimes takes about 5-10 mins to terminate the pod (we use Local Kubernetes executor) and start the next ones.

potiuk on (2024-10-17 21:42:47 UTC): Quite likely https://github.com/apache/airflow/pull/39745 from 2.9.2 should improve mini-scheduler behaviour.

I recommend to upgrade to latest Airflow version and see if you seill have similar issues.

ykanagarajapandiyan (Issue Creator) on (2024-10-25 18:08:07 UTC): Upgrading Airflow to latest (2.10.2) has resolved Stale data errors with single scheduler + configuration 'use-row-level-locking' set to False.

eladkal on (2024-11-10 08:03:45 UTC): mini scheduler is planned to be removed in Airflow 3 so the underlying issue of possible conflicts is no longer a concern
https://github.com/apache/airflow/pull/43741

"
2505730337,issue,open,,skip_archive should actually skip archive in db clean command,"### Body

Currently the delete process uses the archive table in the process of deletion. But we should refactor this so it doesn't need this and actually not create the archive table at all when user specifies this option.

The issue is when you are deleting a lot of rows, particularly if you have a low statement timeout, you may hit errors.  

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2024-09-04 15:45:27+00:00,['pratik-m'],2024-09-12 05:59:01+00:00,,https://github.com/apache/airflow/issues/42003,"[('kind:feature', 'Feature Requests'), ('good first issue', ''), ('kind:meta', 'High-level information important to the community'), ('area:db-migrations', 'PRs with DB migration')]","[{'comment_id': 2329427348, 'issue_id': 2505730337, 'author': 'phanikumv', 'body': 'cc @prabhusneha', 'created_at': datetime.datetime(2024, 9, 4, 15, 49, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333300228, 'issue_id': 2505730337, 'author': 'pratik-m', 'body': ""Hi @phanikumv  - can you pls assign this issue to me? I'd like to work on this."", 'created_at': datetime.datetime(2024, 9, 6, 5, 56, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2340170498, 'issue_id': 2505730337, 'author': 'VETREA', 'body': '@phanikumv \r\ncan i work on this issue', 'created_at': datetime.datetime(2024, 9, 10, 9, 39, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2343920374, 'issue_id': 2505730337, 'author': 'phanikumv', 'body': '@VETREA you might want to collaborate with @pratik-m', 'created_at': datetime.datetime(2024, 9, 11, 14, 58, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2345338205, 'issue_id': 2505730337, 'author': 'VETREA', 'body': '@pratik-m \r\ncan i work with you on this same issue', 'created_at': datetime.datetime(2024, 9, 12, 5, 59, tzinfo=datetime.timezone.utc)}]","phanikumv on (2024-09-04 15:49:28 UTC): cc @prabhusneha

pratik-m (Assginee) on (2024-09-06 05:56:35 UTC): Hi @phanikumv  - can you pls assign this issue to me? I'd like to work on this.

VETREA on (2024-09-10 09:39:55 UTC): @phanikumv 
can i work on this issue

phanikumv on (2024-09-11 14:58:01 UTC): @VETREA you might want to collaborate with @pratik-m

VETREA on (2024-09-12 05:59:00 UTC): @pratik-m 
can i work with you on this same issue

"
2505062950,issue,closed,completed,airflow-scheduler deadlock and crash,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

[2024-09-04T16:08:33.735+0800] {kubernetes_executor.py:356} INFO - Changing state of (TaskInstanceKey(dag_id='7402a0ed-4746-44b6-9a91-5e1682f32cf0', task_id='table_counts-child-3', run_id='scheduled__2024-09-03T08:03:23.150918+00:00', try_number=1, map_index=-1), None, '7402a0ed-4746-44b6-9a91-5e1682f32cf0-table-counts-child-3-p3lreggg', 'airflow', '41210824') to None
[2024-09-04T16:08:33.741+0800] {kubernetes_executor.py:441} INFO - Deleted pod: TaskInstanceKey(dag_id='7402a0ed-4746-44b6-9a91-5e1682f32cf0', task_id='table_counts-child-3', run_id='scheduled__2024-09-03T08:03:23.150918+00:00', try_number=1, map_index=-1) in namespace airflow
[2024-09-04T16:08:33.744+0800] {kubernetes_executor.py:360} ERROR - Exception: None is not a valid TaskInstanceState when attempting to change state of (TaskInstanceKey(dag_id='7402a0ed-4746-44b6-9a91-5e1682f32cf0', task_id='table_counts-child-3', run_id='scheduled__2024-09-03T08:03:23.150918+00:00', try_number=1, map_index=-1), None, '7402a0ed-4746-44b6-9a91-5e1682f32cf0-table-counts-child-3-p3lreggg', 'airflow', '41210824') to None, re-queueing.
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py"", line 358, in sync
    self._change_state(key, state, pod_name, namespace)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py"", line 456, in _change_state
    state = TaskInstanceState(state)
  File ""/usr/local/lib/python3.10/enum.py"", line 385, in __call__
    return cls.__new__(cls, value)
  File ""/usr/local/lib/python3.10/enum.py"", line 710, in __new__
    raise ve_exc
ValueError: None is not a valid TaskInstanceState
[2024-09-04T16:08:33.746+0800] {kubernetes_executor.py:356} INFO - Changing state of (TaskInstanceKey(dag_id='7402a0ed-4746-44b6-9a91-5e1682f32cf0', task_id='table_counts-child-3', run_id='scheduled__2024-09-03T08:03:23.150918+00:00', try_number=1, map_index=-1), None, '7402a0ed-4746-44b6-9a91-5e1682f32cf0-table-counts-child-3-p3lreggg', 'airflow', '41210824') to None
[2024-09-04T16:08:33.747+0800] {scheduler_job_runner.py:260} INFO - Exiting gracefully upon receiving signal 15
[2024-09-04T16:08:34.750+0800] {process_utils.py:132} INFO - Sending Signals.SIGTERM to group 2540. PIDs of all processes in the group: [20146, 20284, 2540]
[2024-09-04T16:08:34.750+0800] {process_utils.py:87} INFO - Sending the signal Signals.SIGTERM to group 2540
[2024-09-04T16:08:37.565+0800] {process_utils.py:266} INFO - Waiting up to 5 seconds for processes to exit...
[2024-09-04T16:08:37.572+0800] {process_utils.py:80} INFO - Process psutil.Process(pid=20146, status='terminated', started='16:08:33') (20146) terminated with exit code None
[2024-09-04T16:08:37.586+0800] {process_utils.py:266} INFO - Waiting up to 5 seconds for processes to exit...
[2024-09-04T16:08:37.586+0800] {process_utils.py:80} INFO - Process psutil.Process(pid=20284, status='terminated', started='16:08:34') (20284) terminated with exit code None
[2024-09-04T16:08:37.613+0800] {process_utils.py:80} INFO - Process psutil.Process(pid=2540, status='terminated', exitcode=0, started='16:04:42') (2540) terminated with exit code 0
[2024-09-04T16:08:37.615+0800] {scheduler_job_runner.py:1001} ERROR - Exception when executing SchedulerJob._run_scheduler_loop
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py"", line 358, in sync
    self._change_state(key, state, pod_name, namespace)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py"", line 440, in _change_state
    self.kube_scheduler.delete_pod(pod_name=pod_name, namespace=namespace)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py"", line 416, in delete_pod
    body=client.V1DeleteOptions(**self.kube_config.delete_option_kwargs),
  File ""/home/airflow/.local/lib/python3.10/site-packages/kubernetes/client/models/v1_delete_options.py"", line 58, in __init__
    local_vars_configuration = Configuration()
  File ""/home/airflow/.local/lib/python3.10/site-packages/kubernetes/client/configuration.py"", line 126, in __init__
    self.debug = False
  File ""/home/airflow/.local/lib/python3.10/site-packages/kubernetes/client/configuration.py"", line 271, in debug
    logger.setLevel(logging.WARNING)
  File ""/usr/local/lib/python3.10/logging/__init__.py"", line 1453, in setLevel
    self.manager._clear_cache()
  File ""/usr/local/lib/python3.10/logging/__init__.py"", line 1412, in _clear_cache
    logger._cache.clear()
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py"", line 263, in _exit_gracefully
    sys.exit(os.EX_OK)
SystemExit: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py"", line 984, in _execute
    self._run_scheduler_loop()
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1127, in _run_scheduler_loop
    executor.heartbeat()
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/traces/tracer.py"", line 58, in wrapper
    return func(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/executors/base_executor.py"", line 247, in heartbeat
    self.sync()
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py"", line 368, in sync
    self.result_queue.task_done()
  File ""<string>"", line 2, in task_done
  File ""/usr/local/lib/python3.10/multiprocessing/managers.py"", line 817, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File ""/usr/local/lib/python3.10/multiprocessing/connection.py"", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File ""/usr/local/lib/python3.10/multiprocessing/connection.py"", line 411, in _send_bytes
    self._send(header + buf)
  File ""/usr/local/lib/python3.10/multiprocessing/connection.py"", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
[2024-09-04T16:08:37.619+0800] {kubernetes_executor.py:695} INFO - Shutting down Kubernetes executor
[2024-09-04T16:08:37.620+0800] {scheduler_job_runner.py:1008} ERROR - Exception when executing Executor.end on KubernetesExecutor(parallelism=32)
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py"", line 358, in sync
    self._change_state(key, state, pod_name, namespace)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py"", line 440, in _change_state
    self.kube_scheduler.delete_pod(pod_name=pod_name, namespace=namespace)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py"", line 416, in delete_pod
    body=client.V1DeleteOptions(**self.kube_config.delete_option_kwargs),
  File ""/home/airflow/.local/lib/python3.10/site-packages/kubernetes/client/models/v1_delete_options.py"", line 58, in __init__
    local_vars_configuration = Configuration()
  File ""/home/airflow/.local/lib/python3.10/site-packages/kubernetes/client/configuration.py"", line 126, in __init__
    self.debug = False
  File ""/home/airflow/.local/lib/python3.10/site-packages/kubernetes/client/configuration.py"", line 271, in debug
    logger.setLevel(logging.WARNING)
  File ""/usr/local/lib/python3.10/logging/__init__.py"", line 1453, in setLevel
    self.manager._clear_cache()
  File ""/usr/local/lib/python3.10/logging/__init__.py"", line 1412, in _clear_cache
    logger._cache.clear()
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py"", line 263, in _exit_gracefully
    sys.exit(os.EX_OK)
SystemExit: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py"", line 984, in _execute
    self._run_scheduler_loop()
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1127, in _run_scheduler_loop
    executor.heartbeat()
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/traces/tracer.py"", line 58, in wrapper
    return func(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/executors/base_executor.py"", line 247, in heartbeat
    self.sync()
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py"", line 368, in sync
    self.result_queue.task_done()
  File ""<string>"", line 2, in task_done
  File ""/usr/local/lib/python3.10/multiprocessing/managers.py"", line 817, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File ""/usr/local/lib/python3.10/multiprocessing/connection.py"", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File ""/usr/local/lib/python3.10/multiprocessing/connection.py"", line 411, in _send_bytes
    self._send(header + buf)
  File ""/usr/local/lib/python3.10/multiprocessing/connection.py"", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1006, in _execute
    executor.end()
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py"", line 698, in end
    self._flush_task_queue()
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py"", line 654, in _flush_task_queue
    self.log.debug(""Executor shutting down, task_queue approximate size=%d"", self.task_queue.qsize())
  File ""<string>"", line 2, in qsize
  File ""/usr/local/lib/python3.10/multiprocessing/managers.py"", line 817, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File ""/usr/local/lib/python3.10/multiprocessing/connection.py"", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File ""/usr/local/lib/python3.10/multiprocessing/connection.py"", line 411, in _send_bytes
    self._send(header + buf)
  File ""/usr/local/lib/python3.10/multiprocessing/connection.py"", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
[2024-09-04T16:08:37.624+0800] {process_utils.py:132} INFO - Sending Signals.SIGTERM to group 2540. PIDs of all processes in the group: []
[2024-09-04T16:08:37.625+0800] {process_utils.py:87} INFO - Sending the signal Signals.SIGTERM to group 2540
[2024-09-04T16:08:37.625+0800] {process_utils.py:101} INFO - Sending the signal Signals.SIGTERM to process 2540 as process group is missing.
[2024-09-04T16:08:37.626+0800] {scheduler_job_runner.py:1014} INFO - Exited execute loop
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py"", line 358, in sync
    self._change_state(key, state, pod_name, namespace)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py"", line 440, in _change_state
    self.kube_scheduler.delete_pod(pod_name=pod_name, namespace=namespace)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py"", line 416, in delete_pod
    body=client.V1DeleteOptions(**self.kube_config.delete_option_kwargs),
  File ""/home/airflow/.local/lib/python3.10/site-packages/kubernetes/client/models/v1_delete_options.py"", line 58, in __init__
    local_vars_configuration = Configuration()
  File ""/home/airflow/.local/lib/python3.10/site-packages/kubernetes/client/configuration.py"", line 126, in __init__
    self.debug = False
  File ""/home/airflow/.local/lib/python3.10/site-packages/kubernetes/client/configuration.py"", line 271, in debug
    logger.setLevel(logging.WARNING)
  File ""/usr/local/lib/python3.10/logging/__init__.py"", line 1453, in setLevel
    self.manager._clear_cache()
  File ""/usr/local/lib/python3.10/logging/__init__.py"", line 1412, in _clear_cache
    logger._cache.clear()
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py"", line 263, in _exit_gracefully
    sys.exit(os.EX_OK)
SystemExit: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/airflow/.local/bin/airflow"", line 8, in <module>
    sys.exit(main())
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/__main__.py"", line 62, in main
    args.func(args)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/cli_config.py"", line 49, in command
    return func(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/cli.py"", line 115, in wrapper
    return f(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/providers_configuration_loader.py"", line 55, in wrapped_function
    return func(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/scheduler_command.py"", line 59, in scheduler
    run_command_with_daemon_option(
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/daemon_utils.py"", line 86, in run_command_with_daemon_option
    callback()
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/scheduler_command.py"", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/cli/commands/scheduler_command.py"", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/job.py"", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/job.py"", line 450, in execute_job
    ret = execute_callable()
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py"", line 984, in _execute
    self._run_scheduler_loop()
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1127, in _run_scheduler_loop
    executor.heartbeat()
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/traces/tracer.py"", line 58, in wrapper
    return func(*args, **kwargs)
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/executors/base_executor.py"", line 247, in heartbeat
    self.sync()
  File ""/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py"", line 368, in sync
    self.result_queue.task_done()
  File ""<string>"", line 2, in task_done
  File ""/usr/local/lib/python3.10/multiprocessing/managers.py"", line 817, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File ""/usr/local/lib/python3.10/multiprocessing/connection.py"", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File ""/usr/local/lib/python3.10/multiprocessing/connection.py"", line 411, in _send_bytes
    self._send(header + buf)
  File ""/usr/local/lib/python3.10/multiprocessing/connection.py"", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe

### What you think should happen instead?

_No response_

### How to reproduce

Use the Kubernetes scheduler of Airflow to execute a DAG that includes a PythonOperator.


### Operating System

Centos 7.9

### Versions of Apache Airflow Providers

apache_airflow_providers_cncf_kubernetes-7.8.0

### Deployment

Other 3rd-party Helm chart

### Deployment details

https://github.com/open-metadata/openmetadata-helm-charts

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",zchunhai,2024-09-04 11:24:24+00:00,[],2024-09-05 01:58:46+00:00,2024-09-05 01:58:46+00:00,https://github.com/apache/airflow/issues/41992,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2328665126, 'issue_id': 2505062950, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 4, 11, 24, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2329283057, 'issue_id': 2505062950, 'author': 'tirkarthi', 'body': 'This was fixed in https://github.com/apache/airflow/pull/35891 and released as part of apache_airflow_providers_cncf_kubernetes-7.11.0 . Please upgrade the provider package and test it.\r\n\r\nhttps://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/7.11.0/changelog.html', 'created_at': datetime.datetime(2024, 9, 4, 14, 52, 3, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-04 11:24:27 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

tirkarthi on (2024-09-04 14:52:03 UTC): This was fixed in https://github.com/apache/airflow/pull/35891 and released as part of apache_airflow_providers_cncf_kubernetes-7.11.0 . Please upgrade the provider package and test it.

https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/7.11.0/changelog.html

"
2503779448,issue,open,,XCom_pull of multiple task_ids not returning the list values,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When passing a list of task IDs to the **task_ids** argument in **task_instance.xcom_pull**, instead of receiving a list of values returned by each task, you might encounter a string with the value '**LazySelectSequence**'.""
In older versions of Airflow, such as 2.9.3, this issue did not occur.

### What you think should happen instead?

It should return a list of values pushed by each task mentioned in the task_ids argument

### How to reproduce

with DAG as dag:


    push_task = PythonOperator(
        task_id='push_task',
        python_callable=lambda: 'Hello, World!')

    push_task_2 = PythonOperator(
        task_id='push_task_2',
        python_callable=lambda: 'Hello, World 2!')

    def pull_function(**kwargs):
        ti = kwargs['ti']
        msg = ti.xcom_pull(task_ids=['push_task', 'push_task_2'])
        print(""received message: '%s'"" % msg)

    pull_task = PythonOperator(
        task_id='pull_task',
        python_callable=pull_function,
        provide_context=True,
    )

### Operating System

Debian GNU/Linux 12

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

FROM apache/airflow:2.10.0-python3.10

### Anything else?

In older versions of Airflow, such as 2.9.3, this issue did not occur.

Error log:
[2024-09-03, 17:48:50 -03] {logging_mixin.py:190} INFO - received message: 'LazySelectSequence([2 items])'

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",edulauer,2024-09-03 21:01:20+00:00,[],2025-01-30 04:58:59+00:00,,https://github.com/apache/airflow/issues/41983,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2327430243, 'issue_id': 2503779448, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 3, 21, 1, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2372209961, 'issue_id': 2503779448, 'author': 'FelipeRamos-neuro', 'body': 'I recently ran into this same behavior using xcom_pull with jinja and can confirm that this is also happening in version 2.10.2.', 'created_at': datetime.datetime(2024, 9, 24, 19, 35, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2518563478, 'issue_id': 2503779448, 'author': 'johncmerfeld', 'body': ""I believe the root cause of this is https://github.com/apache/airflow/pull/39426 but I don't know if my recent PR (#43978) will fix it"", 'created_at': datetime.datetime(2024, 12, 4, 21, 7, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2580055902, 'issue_id': 2503779448, 'author': 'woogakoki', 'body': ""We ran into the same issue but it's unfortunately not fixed in 2.10.4"", 'created_at': datetime.datetime(2025, 1, 9, 12, 38, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2580618522, 'issue_id': 2503779448, 'author': 'johncmerfeld', 'body': ""@woogakoki sadly the code with the fix doesn't appear to have made it into 2.10.4; I'm not sure why this is the case since the PR was marked with that milestone. Hopefully it'll be included in the next release 🫤"", 'created_at': datetime.datetime(2025, 1, 9, 15, 48, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2623521667, 'issue_id': 2503779448, 'author': 'fredthomsen', 'body': 'Ran into this and appears to impact mapped tasks/operators as well.', 'created_at': datetime.datetime(2025, 1, 30, 4, 58, 58, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-03 21:01:24 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

FelipeRamos-neuro on (2024-09-24 19:35:05 UTC): I recently ran into this same behavior using xcom_pull with jinja and can confirm that this is also happening in version 2.10.2.

johncmerfeld on (2024-12-04 21:07:18 UTC): I believe the root cause of this is https://github.com/apache/airflow/pull/39426 but I don't know if my recent PR (#43978) will fix it

woogakoki on (2025-01-09 12:38:44 UTC): We ran into the same issue but it's unfortunately not fixed in 2.10.4

johncmerfeld on (2025-01-09 15:48:47 UTC): @woogakoki sadly the code with the fix doesn't appear to have made it into 2.10.4; I'm not sure why this is the case since the PR was marked with that milestone. Hopefully it'll be included in the next release 🫤

fredthomsen on (2025-01-30 04:58:58 UTC): Ran into this and appears to impact mapped tasks/operators as well.

"
2503745915,issue,open,,dag parse stdout goes to warning on celery worker,"### Body

not sure why but here's an example

sample code

```
from time import sleep

from airflow import DAG
from airflow.decorators import task
from airflow.models.baseoperator import chain_linear

with DAG(dag_id=""dag1"", catchup=False):

    @task
    def the_task():
        sleep(10)

    tasks = []
    for num in range(3):
        tasks.append(the_task.override(task_id=f""task_{num+1}"")())

    chain_linear(*tasks)

print(""HI I AM HERE"")
```

celery worker output

<img width=""405"" alt=""image"" src=""https://github.com/user-attachments/assets/3d1480e5-520d-47f7-bc1d-16b064fc1f3e"">

this can pollute worker logs if you have a really chatty dag



### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2024-09-03 20:40:47+00:00,[],2024-09-03 20:43:09+00:00,,https://github.com/apache/airflow/issues/41982,"[('area:logging', ''), ('kind:meta', 'High-level information important to the community'), ('provider:celery', '')]",[],
2503562818,issue,closed,completed,Xcom view broken for non-JSON values,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Non-JSON values are broken in the Xcom UI [screenshot attached]

Root cause is [this line](https://github.com/jscheffl/airflow/blob/5f5039a6531d9366ac7ffc9a05492b7a93bc8f87/airflow/www/static/js/api/useTaskXcom.ts#L66) from https://github.com/apache/airflow/pull/40640. Setting `stringify: false` results in JSON encoding which results in `TypeError: keys must be str, int, float, bool or None, not tuple` exception from `GET <Airflow URL>/api/v1/dags/<DAG>/dagRuns/<Run ID>/taskInstances/<Task ID>/xcomEntries/<Xcom name>?stringify=false` endpoint.

The PR's intention is to make the whole view JSON, so not sure if the intention is to error on non-JSON Xcom values, or if it should be updated to have clean fallback logic.

![Pasted Graphic 22](https://github.com/user-attachments/assets/5451cdb0-3475-4524-b504-ade4df4a3a98)


### What you think should happen instead?

_No response_

### How to reproduce

Create Xcom value that is non-JSON serializable (e.g. `{('201009_NB502104_0421_AHJY23BGXG (SEQ_WF: 138898)', None): 82359}`) and then try to view in UI Xcom tab

### Operating System

Linux (Ubuntu 22.04)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jkramer-ginkgo,2024-09-03 18:59:22+00:00,[],2024-09-10 18:52:28+00:00,2024-09-03 22:48:30+00:00,https://github.com/apache/airflow/issues/41981,"[('kind:bug', 'This is a clearly a bug'), ('duplicate', 'Issue that is duplicated'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2327571399, 'issue_id': 2503562818, 'author': 'potiuk', 'body': 'Duplicate of https://github.com/apache/airflow/pull/41605 fixed in https://github.com/apache/airflow/pull/41516 and available in 2.10.1rc1. Can you please double check if rc1 of 2.10.0 fixes the problem for you @jkramer-ginkgo - see https://github.com/apache/airflow/issues/41956 for links and details', 'created_at': datetime.datetime(2024, 9, 3, 22, 48, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2327571730, 'issue_id': 2503562818, 'author': 'potiuk', 'body': 'Ideally pleasee comment in https://github.com/apache/airflow/issues/41956 when you confirm the problem is fixed.', 'created_at': datetime.datetime(2024, 9, 3, 22, 48, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2338443302, 'issue_id': 2503562818, 'author': 'jkramer-ginkgo', 'body': ""@potiuk It is still broken in `2.10.1`.. I should've mentioned I had tested against a manually patched version with https://github.com/apache/airflow/pull/41605"", 'created_at': datetime.datetime(2024, 9, 9, 15, 33, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2338450895, 'issue_id': 2503562818, 'author': 'jkramer-ginkgo', 'body': '@potiuk would you be able to reopen or should I dupe this issue?', 'created_at': datetime.datetime(2024, 9, 9, 15, 36, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2338587842, 'issue_id': 2503562818, 'author': 'potiuk', 'body': 'Idealy copy it - this is easier for our ""release"" process.', 'created_at': datetime.datetime(2024, 9, 9, 16, 41, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2341774487, 'issue_id': 2503562818, 'author': 'jkramer-ginkgo', 'body': '@potiuk Cloned here: https://github.com/apache/airflow/issues/42117', 'created_at': datetime.datetime(2024, 9, 10, 18, 52, 27, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-09-03 22:48:30 UTC): Duplicate of https://github.com/apache/airflow/pull/41605 fixed in https://github.com/apache/airflow/pull/41516 and available in 2.10.1rc1. Can you please double check if rc1 of 2.10.0 fixes the problem for you @jkramer-ginkgo - see https://github.com/apache/airflow/issues/41956 for links and details

potiuk on (2024-09-03 22:48:50 UTC): Ideally pleasee comment in https://github.com/apache/airflow/issues/41956 when you confirm the problem is fixed.

jkramer-ginkgo (Issue Creator) on (2024-09-09 15:33:36 UTC): @potiuk It is still broken in `2.10.1`.. I should've mentioned I had tested against a manually patched version with https://github.com/apache/airflow/pull/41605

jkramer-ginkgo (Issue Creator) on (2024-09-09 15:36:58 UTC): @potiuk would you be able to reopen or should I dupe this issue?

potiuk on (2024-09-09 16:41:35 UTC): Idealy copy it - this is easier for our ""release"" process.

jkramer-ginkgo (Issue Creator) on (2024-09-10 18:52:27 UTC): @potiuk Cloned here: https://github.com/apache/airflow/issues/42117

"
2503202971,issue,closed,completed,DagBag import timeout errors not propagating to Airflow UI,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When top-level DAG code causes a DagBag import timeout error, this will fail silently (from a DAG author's perspective -- it will fail in the DAG processor logs).

### What you think should happen instead?

An error banner should be shown in the Airflow UI.

### How to reproduce

```python
from datetime import datetime
from airflow.decorators import dag, task
from time import sleep

@dag(start_date=datetime(2023, 1, 1), max_active_runs=3, schedule=None, catchup=False)
def bad_practices_dag_1():

    sleep(40)

    @task
    def do_something():
        print(f""The meaning of life, the universe, and everything is... 42."")

    do_something()

bad_practices_dag_1()
```

### Operating System

n/a

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",RNHTTR,2024-09-03 15:37:48+00:00,[],2024-09-03 17:06:18+00:00,2024-09-03 17:06:18+00:00,https://github.com/apache/airflow/issues/41979,"[('kind:bug', 'This is a clearly a bug'), ('duplicate', 'Issue that is duplicated'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2327026545, 'issue_id': 2503202971, 'author': 'potiuk', 'body': 'Duplicate of https://github.com/apache/airflow/issues/41976', 'created_at': datetime.datetime(2024, 9, 3, 17, 6, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2327026973, 'issue_id': 2503202971, 'author': 'potiuk', 'body': 'Closing as duplicate', 'created_at': datetime.datetime(2024, 9, 3, 17, 6, 18, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-09-03 17:06:01 UTC): Duplicate of https://github.com/apache/airflow/issues/41976

potiuk on (2024-09-03 17:06:18 UTC): Closing as duplicate

"
2502738348,issue,open,,Triggered DAG button looks like disabled with TriggerDagRunOperator,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

2.10.1.RC1

### What happened?

Noticed Triggered DAG button looks like disabled with 2.10.1.RC1. Verified this is fine with 2.10.0


![image](https://github.com/user-attachments/assets/95d2fa95-4fd1-4c72-9f9d-fb12435d60d2)


### What you think should happen instead?

Triggered DAG buttons should not look like disabled 

### How to reproduce

Create any DAG using TriggerDagRunOperator and check Triggered DAG button

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vatsrahul1001,2024-09-03 12:22:14+00:00,[],2024-12-06 23:46:49+00:00,,https://github.com/apache/airflow/issues/41977,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('provider:standard', '')]","[{'comment_id': 2326428418, 'issue_id': 2502738348, 'author': 'raphaelauv', 'body': 'I have the problem with 2.10.0 and it was also the case in 2.9.X\r\n\r\n![image](https://github.com/user-attachments/assets/c98c16ad-2a0c-4721-bb62-94d76ab2cf3c)', 'created_at': datetime.datetime(2024, 9, 3, 12, 42, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2326437493, 'issue_id': 2502738348, 'author': 'vatsrahul1001', 'body': 'I did not notice this issue in our deployment with 2.10.0\r\n![image](https://github.com/user-attachments/assets/1a0cc252-4683-4a21-869e-35f6d805d7a7)', 'created_at': datetime.datetime(2024, 9, 3, 12, 47, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2326683152, 'issue_id': 2502738348, 'author': 'ephraimbuddy', 'body': ""> I have the problem with 2.10.0 and it was also the case in 2.9.X\r\n> \r\n> ![image](https://private-user-images.githubusercontent.com/10202690/363986114-c98c16ad-2a0c-4721-bb62-94d76ab2cf3c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjUzNzQxNDMsIm5iZiI6MTcyNTM3Mzg0MywicGF0aCI6Ii8xMDIwMjY5MC8zNjM5ODYxMTQtYzk4YzE2YWQtMmEwYy00NzIxLWJiNjItOTRkNzZhYjJjZjNjLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDMlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTAzVDE0MzA0M1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTcwNDYwMDJmZTEyMWRmNWJkYTI3Njg5ZjFhNjllZjA0Njk4ZWIyMGU5NTk5MGZlMjczYTUzOGRhNjdjN2Q3ODAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.9cMu2U7wYRr_EHMLRdc1RAX9zjp9NK-vVJmTTSWsTns)\r\n\r\nGiven this, it's not a release blocker"", 'created_at': datetime.datetime(2024, 9, 3, 14, 31, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2327932023, 'issue_id': 2502738348, 'author': 'eladkal', 'body': 'It could be a regression if the button has some edge case. @vatsrahul1001 can you share reproduce example that worked on 2.10.0 but doesnt work on 2.10.1rc1?', 'created_at': datetime.datetime(2024, 9, 4, 5, 8, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2328023374, 'issue_id': 2502738348, 'author': 'vatsrahul1001', 'body': 'I tested this with 2.10.0 using airflow standalone command and noticed this is fine with 2.10.0.\r\nSteps to repro\r\n1.  Copy [dags.zip](https://github.com/user-attachments/files/16861698/dags.zip) \r\n2. Run trigger_controller_dag\r\n3. Check Triggered DAG button in details section \r\n\r\n![image](https://github.com/user-attachments/assets/c5c44952-7262-4764-9231-a7c621a8a3f4)', 'created_at': datetime.datetime(2024, 9, 4, 6, 30, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2328337177, 'issue_id': 2502738348, 'author': 'vatsrahul1001', 'body': 'Tested above steps with Breeze with tag `2.10.0` and it works fine', 'created_at': datetime.datetime(2024, 9, 4, 9, 14, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2328438303, 'issue_id': 2502738348, 'author': 'utkarsharma2', 'body': '> I tested this with 2.10.0 using airflow standalone command and noticed this is fine with 2.10.0. Steps to repro\r\n> \r\n>     1. Copy [dags.zip](https://github.com/user-attachments/files/16861698/dags.zip)\r\n> \r\n>     2. Run trigger_controller_dag\r\n> \r\n>     3. Check Triggered DAG button in details section\r\n> \r\n\r\nYa, I too tested this. It works fine for 2.10.0 and not for 2.10.1.', 'created_at': datetime.datetime(2024, 9, 4, 10, 3, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2328445728, 'issue_id': 2502738348, 'author': 'ephraimbuddy', 'body': ""Can we debug it @utkarsharma2. Let's know if we have the solution before canceling the votes"", 'created_at': datetime.datetime(2024, 9, 4, 10, 7, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2328528609, 'issue_id': 2502738348, 'author': 'utkarsharma2', 'body': ""@ephraimbuddy I have narrowed down the issue it's because the URL(`/dags/trigger_target_dag/grid?dag_run_id=manual__2024-09-04T10%3A00%3A11.862710%2B00%3A00`) generated is not passing the [sanitized(regex) test](https://github.com/apache/airflow/blob/9ae53eea8cfd463f31275307b60e6e89c8d56e6a/airflow/www/static/js/dag/details/taskInstance/ExtraLinks.tsx#L61) which is used to determine the button should be disabled or not and this logic was introduced recently by this [PR](https://github.com/apache/airflow/pull/41665)."", 'created_at': datetime.datetime(2024, 9, 4, 10, 38, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2328755939, 'issue_id': 2502738348, 'author': 'vatsrahul1001', 'body': ""Just to add I am still able to use the button and it works. It just looks like disabled. Doesn't look like a blocker."", 'created_at': datetime.datetime(2024, 9, 4, 11, 53, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2329052999, 'issue_id': 2502738348, 'author': 'utkarsharma2', 'body': "">  Just to add I am still able to use the button and it works. It just looks like disabled. Doesn't look like a blocker.\r\n\r\n@potiuk @amoghrajesh Kinda defeats the purpose of fix IMO, WDYT?"", 'created_at': datetime.datetime(2024, 9, 4, 13, 27, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2329131308, 'issue_id': 2502738348, 'author': 'raphaelauv', 'body': 'the button does not work in the case of dynamic task mapping\r\n\r\n```python\r\nimport random\r\n\r\nfrom airflow import DAG\r\nfrom airflow.operators.empty import EmptyOperator\r\n\r\nfrom airflow.operators.python import PythonOperator\r\nfrom airflow.operators.trigger_dagrun import TriggerDagRunOperator\r\nfrom airflow.utils.dates import days_ago\r\n\r\nwith DAG(dag_id=""run_test""):\r\n    EmptyOperator(task_id=""nothing"")\r\n\r\nwith DAG(dag_id=""trigger_test"",\r\n         start_date=days_ago(1),\r\n         schedule=None):\r\n    def build_triggers():\r\n        results_list = [""a"", ""b"", ""c"", ""d""]\r\n        all_triggers = []\r\n        for index, item in enumerate(results_list):\r\n            dag_id = ""run_test""\r\n            trigger_run_id = f""{index}_{item}_{random.randrange(1, 100)}""\r\n            tmp = {\r\n                ""trigger_run_id"": trigger_run_id,\r\n                ""trigger_dag_id"": dag_id\r\n            }\r\n            all_triggers.append(tmp)\r\n\r\n        return all_triggers\r\n\r\n\r\n    build_triggers_task = PythonOperator(\r\n        task_id=""build_triggers"",\r\n        python_callable=build_triggers,\r\n        do_xcom_push=True,\r\n    )\r\n\r\n    trigger_task = TriggerDagRunOperator.partial(\r\n        task_id=""trigger"",\r\n        reset_dag_run=True,\r\n        map_index_template=""{{ task.trigger_run_id }}"",\r\n        wait_for_completion=False).expand_kwargs(build_triggers_task.output)\r\n\r\n    build_triggers_task >> trigger_task\r\n\r\n```\r\n\r\nit do not redirect to the triggered dag\r\n\r\n![Screenshot from 2024-09-04 15-50-04](https://github.com/user-attachments/assets/48cc7a8d-bc1d-4c6a-b567-0adb852bbcdb)', 'created_at': datetime.datetime(2024, 9, 4, 13, 51, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2330581087, 'issue_id': 2502738348, 'author': 'vatsrahul1001', 'body': '@raphaelauv does dynamic task\r\n\r\n> the button does not work in the case of dynamic task mapping\r\n> \r\n> ```python\r\n> import random\r\n> \r\n> from airflow import DAG\r\n> from airflow.operators.empty import EmptyOperator\r\n> \r\n> from airflow.operators.python import PythonOperator\r\n> from airflow.operators.trigger_dagrun import TriggerDagRunOperator\r\n> from airflow.utils.dates import days_ago\r\n> \r\n> with DAG(dag_id=""run_test""):\r\n>     EmptyOperator(task_id=""nothing"")\r\n> \r\n> with DAG(dag_id=""trigger_test"",\r\n>          start_date=days_ago(1),\r\n>          schedule=None):\r\n>     def build_triggers():\r\n>         results_list = [""a"", ""b"", ""c"", ""d""]\r\n>         all_triggers = []\r\n>         for index, item in enumerate(results_list):\r\n>             dag_id = ""run_test""\r\n>             trigger_run_id = f""{index}_{item}_{random.randrange(1, 100)}""\r\n>             tmp = {\r\n>                 ""trigger_run_id"": trigger_run_id,\r\n>                 ""trigger_dag_id"": dag_id\r\n>             }\r\n>             all_triggers.append(tmp)\r\n> \r\n>         return all_triggers\r\n> \r\n> \r\n>     build_triggers_task = PythonOperator(\r\n>         task_id=""build_triggers"",\r\n>         python_callable=build_triggers,\r\n>         do_xcom_push=True,\r\n>     )\r\n> \r\n>     trigger_task = TriggerDagRunOperator.partial(\r\n>         task_id=""trigger"",\r\n>         reset_dag_run=True,\r\n>         map_index_template=""{{ task.trigger_run_id }}"",\r\n>         wait_for_completion=False).expand_kwargs(build_triggers_task.output)\r\n> \r\n>     build_triggers_task >> trigger_task\r\n> ```\r\n> \r\n> it do not redirect to the triggered dag\r\n> \r\n> ![Screenshot from 2024-09-04 15-50-04](https://private-user-images.githubusercontent.com/10202690/364403968-48cc7a8d-bc1d-4c6a-b567-0adb852bbcdb.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjU1MTEzNzksIm5iZiI6MTcyNTUxMTA3OSwicGF0aCI6Ii8xMDIwMjY5MC8zNjQ0MDM5NjgtNDhjYzdhOGQtYmMxZC00YzZhLWI1NjctMGFkYjg1MmJiY2RiLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA5MDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwOTA1VDA0Mzc1OVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTE2ZTc4MjA1NzJhN2IyNjljYjZhNmE3MTYwNGI3ZmQ4YzcxNzQ3NWVhOWQ4ODgzZTA3NjliMTFmMTA1OGFmNjgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.Nw5NnwdT-2D9eoDMLMNVbciigI001NljoeWeUXswBVc)\r\n\r\nDoes this works with 2.10.0 with dynamic task?', 'created_at': datetime.datetime(2024, 9, 5, 4, 38, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2330661941, 'issue_id': 2502738348, 'author': 'raphaelauv', 'body': 'No the button do not work with dynamic task mapping in 2.10.0', 'created_at': datetime.datetime(2024, 9, 5, 5, 55, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2330676380, 'issue_id': 2502738348, 'author': 'vatsrahul1001', 'body': '> No the button do not work with dynamic task mapping in 2.10.0\r\n\r\nOk, we need to look into this then, however, not a blocker for 2.10.1RC', 'created_at': datetime.datetime(2024, 9, 5, 6, 7, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2524649509, 'issue_id': 2502738348, 'author': 'eladkal', 'body': '`TriggerDagRunOperator` is now part of standard provider thus fixing tagging of this issue.', 'created_at': datetime.datetime(2024, 12, 6, 23, 46, 28, tzinfo=datetime.timezone.utc)}]","raphaelauv on (2024-09-03 12:42:40 UTC): I have the problem with 2.10.0 and it was also the case in 2.9.X

![image](https://github.com/user-attachments/assets/c98c16ad-2a0c-4721-bb62-94d76ab2cf3c)

vatsrahul1001 (Issue Creator) on (2024-09-03 12:47:11 UTC): I did not notice this issue in our deployment with 2.10.0
![image](https://github.com/user-attachments/assets/1a0cc252-4683-4a21-869e-35f6d805d7a7)

ephraimbuddy on (2024-09-03 14:31:42 UTC): Given this, it's not a release blocker

eladkal on (2024-09-04 05:08:41 UTC): It could be a regression if the button has some edge case. @vatsrahul1001 can you share reproduce example that worked on 2.10.0 but doesnt work on 2.10.1rc1?

vatsrahul1001 (Issue Creator) on (2024-09-04 06:30:36 UTC): I tested this with 2.10.0 using airflow standalone command and noticed this is fine with 2.10.0.
Steps to repro
1.  Copy [dags.zip](https://github.com/user-attachments/files/16861698/dags.zip) 
2. Run trigger_controller_dag
3. Check Triggered DAG button in details section 

![image](https://github.com/user-attachments/assets/c5c44952-7262-4764-9231-a7c621a8a3f4)

vatsrahul1001 (Issue Creator) on (2024-09-04 09:14:58 UTC): Tested above steps with Breeze with tag `2.10.0` and it works fine

utkarsharma2 on (2024-09-04 10:03:32 UTC): Ya, I too tested this. It works fine for 2.10.0 and not for 2.10.1.

ephraimbuddy on (2024-09-04 10:07:10 UTC): Can we debug it @utkarsharma2. Let's know if we have the solution before canceling the votes

utkarsharma2 on (2024-09-04 10:38:33 UTC): @ephraimbuddy I have narrowed down the issue it's because the URL(`/dags/trigger_target_dag/grid?dag_run_id=manual__2024-09-04T10%3A00%3A11.862710%2B00%3A00`) generated is not passing the [sanitized(regex) test](https://github.com/apache/airflow/blob/9ae53eea8cfd463f31275307b60e6e89c8d56e6a/airflow/www/static/js/dag/details/taskInstance/ExtraLinks.tsx#L61) which is used to determine the button should be disabled or not and this logic was introduced recently by this [PR](https://github.com/apache/airflow/pull/41665).

vatsrahul1001 (Issue Creator) on (2024-09-04 11:53:02 UTC): Just to add I am still able to use the button and it works. It just looks like disabled. Doesn't look like a blocker.

utkarsharma2 on (2024-09-04 13:27:52 UTC): @potiuk @amoghrajesh Kinda defeats the purpose of fix IMO, WDYT?

raphaelauv on (2024-09-04 13:51:51 UTC): the button does not work in the case of dynamic task mapping

```python
import random

from airflow import DAG
from airflow.operators.empty import EmptyOperator

from airflow.operators.python import PythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.utils.dates import days_ago

with DAG(dag_id=""run_test""):
    EmptyOperator(task_id=""nothing"")

with DAG(dag_id=""trigger_test"",
         start_date=days_ago(1),
         schedule=None):
    def build_triggers():
        results_list = [""a"", ""b"", ""c"", ""d""]
        all_triggers = []
        for index, item in enumerate(results_list):
            dag_id = ""run_test""
            trigger_run_id = f""{index}_{item}_{random.randrange(1, 100)}""
            tmp = {
                ""trigger_run_id"": trigger_run_id,
                ""trigger_dag_id"": dag_id
            }
            all_triggers.append(tmp)

        return all_triggers


    build_triggers_task = PythonOperator(
        task_id=""build_triggers"",
        python_callable=build_triggers,
        do_xcom_push=True,
    )

    trigger_task = TriggerDagRunOperator.partial(
        task_id=""trigger"",
        reset_dag_run=True,
        map_index_template=""{{ task.trigger_run_id }}"",
        wait_for_completion=False).expand_kwargs(build_triggers_task.output)

    build_triggers_task >> trigger_task

```

it do not redirect to the triggered dag

![Screenshot from 2024-09-04 15-50-04](https://github.com/user-attachments/assets/48cc7a8d-bc1d-4c6a-b567-0adb852bbcdb)

vatsrahul1001 (Issue Creator) on (2024-09-05 04:38:42 UTC): @raphaelauv does dynamic task


Does this works with 2.10.0 with dynamic task?

raphaelauv on (2024-09-05 05:55:44 UTC): No the button do not work with dynamic task mapping in 2.10.0

vatsrahul1001 (Issue Creator) on (2024-09-05 06:07:27 UTC): Ok, we need to look into this then, however, not a blocker for 2.10.1RC

eladkal on (2024-12-06 23:46:28 UTC): `TriggerDagRunOperator` is now part of standard provider thus fixing tagging of this issue.

"
2502619278,issue,closed,completed,DAG import timeouts recorded in Airflow Database and displayed in Airflow UI,"### Description

The following DAG:
```
import datetime
import time

import airflow
from airflow.operators.python import PythonOperator

time.sleep(1000)

with airflow.DAG(
    ""import_timeout"",
    start_date=datetime.datetime(2022, 1, 1),
    schedule=None) as dag:

  def f():
    print(""Sleeping"")
    time.sleep(120)

  for ind in range(10):
    PythonOperator(
        dag=dag,
        task_id=f""sleep_120_{ind}"",
        python_callable=f,
    )
```

is artificial example of DAG that will be timed out on import in DAG processor.

In logs of Airflow we can see that there is AirflowTaskTimeout for that DAG:

![Screenshot 2024-09-03 13 21 07](https://github.com/user-attachments/assets/dafffb26-dc8c-4142-bd0a-d9a08188349a)


However, there is nothing exposed in Airflow UI that can give an idea of this.

The idea is to record import timeouts information in Airflow database and display it in Airflow UI (similar to DAG import errors).
Althought, it is not clear which model would be better to use, ImportError or DagWarning?


### Use case/motivation

_No response_

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",VladaZakharova,2024-09-03 11:23:32+00:00,[],2024-09-06 19:04:08+00:00,2024-09-06 19:04:07+00:00,https://github.com/apache/airflow/issues/41976,"[('kind:bug', 'This is a clearly a bug'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2326283548, 'issue_id': 2502619278, 'author': 'VladaZakharova', 'body': '@potiuk @uranusjr @Taragolis \r\nHi all! This is another ticket for improving our Airflow UI :)\r\nNeed your advice before implementing this, do you have any ideas? Thanks!', 'created_at': datetime.datetime(2024, 9, 3, 11, 28, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2326309995, 'issue_id': 2502619278, 'author': 'potiuk', 'body': 'Yeah - since we have `ParseImportError` -> it should likely be passed same way. But maybe it already is? I know there is a problem with visibility of ImportErrors when DAG has access control (for obvious reasons) - which is tracked in https://github.com/apache/airflow/issues/29897 which might be connected. I think both are falling in the same camp.', 'created_at': datetime.datetime(2024, 9, 3, 11, 43, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2326432524, 'issue_id': 2502619278, 'author': 'VladaZakharova', 'body': 'Yes, looks very similar. But i think that problem cannot be resolved easily (for some reasons haha), so i can try to at least catch this kind of errors with AirflowTaskTimeout and output it on UI as ParseImportError. Right?', 'created_at': datetime.datetime(2024, 9, 3, 12, 44, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2326531289, 'issue_id': 2502619278, 'author': 'potiuk', 'body': '> Yes, looks very similar. But i think that problem cannot be resolved easily (for some reasons haha), so i can try to at least catch this kind of errors with AirflowTaskTimeout and output it on UI as ParseImportError. Right?\r\n\r\nNot sure :). Actually answering to that question with certainty - means solving the problem :)', 'created_at': datetime.datetime(2024, 9, 3, 13, 28, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2326781843, 'issue_id': 2502619278, 'author': 'VladaZakharova', 'body': 'Actually, as i have checked my problem is a little bit different from what you have mentioned: in the #29897 i can see that for Admin user ImportError can be visible on UI (so, correct permissions and roles fix everything). In my case, even if i am running it as Admin user i still cannot see any output on UI', 'created_at': datetime.datetime(2024, 9, 3, 15, 12, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2326821753, 'issue_id': 2502619278, 'author': 'potiuk', 'body': 'Yeah - so likely creating a ParseImportError speciffically when there is a timeout should help :)', 'created_at': datetime.datetime(2024, 9, 3, 15, 29, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2332004260, 'issue_id': 2502619278, 'author': 'RNHTTR', 'body': ""I think this is a bug and not a feature. This used to exist in Airflow <= 2.7, and provides critical info to DAG authors who get no information as to why their DAG isn't showing up."", 'created_at': datetime.datetime(2024, 9, 5, 15, 23, 37, tzinfo=datetime.timezone.utc)}]","VladaZakharova (Issue Creator) on (2024-09-03 11:28:48 UTC): @potiuk @uranusjr @Taragolis 
Hi all! This is another ticket for improving our Airflow UI :)
Need your advice before implementing this, do you have any ideas? Thanks!

potiuk on (2024-09-03 11:43:31 UTC): Yeah - since we have `ParseImportError` -> it should likely be passed same way. But maybe it already is? I know there is a problem with visibility of ImportErrors when DAG has access control (for obvious reasons) - which is tracked in https://github.com/apache/airflow/issues/29897 which might be connected. I think both are falling in the same camp.

VladaZakharova (Issue Creator) on (2024-09-03 12:44:43 UTC): Yes, looks very similar. But i think that problem cannot be resolved easily (for some reasons haha), so i can try to at least catch this kind of errors with AirflowTaskTimeout and output it on UI as ParseImportError. Right?

potiuk on (2024-09-03 13:28:53 UTC): Not sure :). Actually answering to that question with certainty - means solving the problem :)

VladaZakharova (Issue Creator) on (2024-09-03 15:12:21 UTC): Actually, as i have checked my problem is a little bit different from what you have mentioned: in the #29897 i can see that for Admin user ImportError can be visible on UI (so, correct permissions and roles fix everything). In my case, even if i am running it as Admin user i still cannot see any output on UI

potiuk on (2024-09-03 15:29:29 UTC): Yeah - so likely creating a ParseImportError speciffically when there is a timeout should help :)

RNHTTR on (2024-09-05 15:23:37 UTC): I think this is a bug and not a feature. This used to exist in Airflow <= 2.7, and provides critical info to DAG authors who get no information as to why their DAG isn't showing up.

"
2502140339,issue,open,,Infinite retries when calling dag.test(),"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.7.2

### What happened?

We are trying to use the test method for a given dag in our integration tests. However, when one of the task fails, the code keeps retrying it indefinitely. We are using pytest and in a single test we create and start a dag like this:
```python
    dag = create_incremental_dag(
    ...
    )
    print(""Testing integration dag execution"")
    dag.test()
```

In this example `create_incremental_dag` is a custom function which creates a DAG with a certain configuration. This dag contains a step that launches an ECS containing our business logic. If there is a problem with the business logic, causing the ECS task to exit with an error, the airflow step retries it until we manually kill the execution. We are setting the number of retries to 0 as part of the configuration we pass to the dag (but even if we didn't it should still be 0 by the default airflow config)

### What you think should happen instead?

The step should not retry and the dag should immediately terminate with an error.

### How to reproduce
```python
import datetime

from airflow import DAG, AirflowException
from airflow.decorators import task
from airflow.operators.empty import EmptyOperator

from common.tagging import  default_args


@task(task_id=""test_task"")
def test_task():
    print(""Failing Task"")
    raise AirflowException(""Failing"")


def create_dag():
    with DAG(
        ""test1"",
        schedule=None,
        start_date=datetime.datetime(2021, 1, 1),
        tags=[""test""],
        catchup=False,
        max_active_runs=1,
        default_args=default_args
    ) as dag:
        start = EmptyOperator(
            task_id=""start""
        )

        test = test_task()

        end = EmptyOperator(
            task_id=""end""
        )

        start >> test >> end

        return dag


def test_incremental_tagging_integration():
    dag = create_dag()
    print(""Testing integration dag execution with updated data"")
    dag.test()
```

### Operating System

mac-os 14.6.1 

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",miroslav-trifonov,2024-09-03 07:35:20+00:00,[],2024-10-23 15:18:09+00:00,,https://github.com/apache/airflow/issues/41972,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2325805020, 'issue_id': 2502140339, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 3, 7, 35, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2392126020, 'issue_id': 2502140339, 'author': 'harjeevanmaan', 'body': '@miroslav-trifonov  Please share the `default_args ` dict used, that is imported from `common.tagging`.', 'created_at': datetime.datetime(2024, 10, 3, 19, 4, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2393633953, 'issue_id': 2502140339, 'author': 'miroslav-trifonov', 'body': 'We create the default_args dict like this with the retries parameter we pass for the tests equal to `0`:\r\n```\r\ndef get_default_args(retries: int = 1) -> dict:\r\n    return {\r\n        ""owner"": ""airflow"",\r\n        ""depends_on_past"": False,\r\n        ""start_date"": datetime(2021, 1, 1),\r\n        ""email_on_failure"": True,\r\n        ""on_failure_callback"": None\r\n        ""on_success_callback"": None,\r\n        ""email_on_retry"": False,\r\n        ""retries"": retries,\r\n        ""max_retries"": 3,\r\n        ""retry_delay"": timedelta(minutes=2),\r\n    }\r\n```', 'created_at': datetime.datetime(2024, 10, 4, 12, 49, 6, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-03 07:35:23 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

harjeevanmaan on (2024-10-03 19:04:31 UTC): @miroslav-trifonov  Please share the `default_args ` dict used, that is imported from `common.tagging`.

miroslav-trifonov (Issue Creator) on (2024-10-04 12:49:06 UTC): We create the default_args dict like this with the retries parameter we pass for the tests equal to `0`:
```
def get_default_args(retries: int = 1) -> dict:
    return {
        ""owner"": ""airflow"",
        ""depends_on_past"": False,
        ""start_date"": datetime(2021, 1, 1),
        ""email_on_failure"": True,
        ""on_failure_callback"": None
        ""on_success_callback"": None,
        ""email_on_retry"": False,
        ""retries"": retries,
        ""max_retries"": 3,
        ""retry_delay"": timedelta(minutes=2),
    }
```

"
2501906495,issue,closed,completed,"When generating ""status of testing"" issue, we should mark all commit authors not only PR authors","### Body

Currently when we gnerate ""status of tesitng"" we add PR authors to it - but sometimes (when commit is cherry-picked or when it has multiple authors) - the original author for the commit might not be marked in the issue.

We should improve our tooling to retrieve authorship information from the commit, not only from the PR.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",potiuk,2024-09-03 04:43:35+00:00,['amoghrajesh'],2024-10-20 18:21:06+00:00,2024-10-20 18:21:06+00:00,https://github.com/apache/airflow/issues/41969,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:dev-tools', ''), ('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2325592199, 'issue_id': 2501906495, 'author': 'potiuk', 'body': 'cc: @amoghrajesh', 'created_at': datetime.datetime(2024, 9, 3, 4, 43, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325592915, 'issue_id': 2501906495, 'author': 'potiuk', 'body': 'See https://github.com/apache/airflow/issues/41956#issuecomment-2325592569 for example.', 'created_at': datetime.datetime(2024, 9, 3, 4, 44, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325610460, 'issue_id': 2501906495, 'author': 'amoghrajesh', 'body': 'Thanks! I will try and take a look at it', 'created_at': datetime.datetime(2024, 9, 3, 5, 6, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325631554, 'issue_id': 2501906495, 'author': 'potiuk', 'body': 'Also - one other - related thing I noticed - for such backported PRs, we also do not mention linked issues - because they are linked to the original PRs, so likely we need to do a bit more complex calculation of those:\r\n\r\na) get all the authors from the commit\r\nb) if the commit message contains (cherry-picked from .....) -> find the original PR (by looking at the (#number) in the original commit message.\r\nc) get the linked issue information from the original PR\r\n\r\nSee: https://github.com/apache/airflow/issues/41956#issuecomment-2325628329 for example.', 'created_at': datetime.datetime(2024, 9, 3, 5, 28, 32, tzinfo=datetime.timezone.utc)}]","potiuk (Issue Creator) on (2024-09-03 04:43:42 UTC): cc: @amoghrajesh

potiuk (Issue Creator) on (2024-09-03 04:44:45 UTC): See https://github.com/apache/airflow/issues/41956#issuecomment-2325592569 for example.

amoghrajesh (Assginee) on (2024-09-03 05:06:36 UTC): Thanks! I will try and take a look at it

potiuk (Issue Creator) on (2024-09-03 05:28:32 UTC): Also - one other - related thing I noticed - for such backported PRs, we also do not mention linked issues - because they are linked to the original PRs, so likely we need to do a bit more complex calculation of those:

a) get all the authors from the commit
b) if the commit message contains (cherry-picked from .....) -> find the original PR (by looking at the (#number) in the original commit message.
c) get the linked issue information from the original PR

See: https://github.com/apache/airflow/issues/41956#issuecomment-2325628329 for example.

"
2501098156,issue,closed,completed,Status of testing of Apache Airflow 2.10.1rc1,"### Body

We are kindly requesting that contributors to [Apache Airflow RC 2.10.1rc1](https://pypi.org/project/apache-airflow/2.10.1rc1/) help test the RC.

Please let us know by commenting if the issue is addressed in the latest RC.

- [ ] [Fix try selector refresh (#41483) (#41503)](https://github.com/apache/airflow/pull/41503): @bbovenzi
     Linked issues:
     - [Fix try selector refresh (#41483)](https://github.com/apache/airflow/pull/41483)
- [ ] [Field Deletion Warning when editing Connections (#41144) (#41504)](https://github.com/apache/airflow/pull/41504): @bbovenzi @lh5844
     Linked issues:
     - [Field Deletion Warning when editing Connections (#41144)](https://github.com/apache/airflow/pull/41144)
- [x] [Fix UI rendering when XCom is INT, FLOAT, BOOL or NULL (#41516) (#41605)](https://github.com/apache/airflow/pull/41605): @jscheffl
- [ ] [[Backport] Fix InletEventsAccessors type stub (#41572) (#41607)](https://github.com/apache/airflow/pull/41607): @uranusjr
- [x] [Apply GitHub workflow changes to stable branch (#41623)](https://github.com/apache/airflow/pull/41623): @potiuk
- [x] [Adding url sanitisation for extra links (#41665) (#41680)](https://github.com/apache/airflow/pull/41680): @amoghrajesh
- [x] [[Backport] Splitting syspath preparation into stages (#41694)](https://github.com/apache/airflow/pull/41694): @amoghrajesh @uranusjr
     Linked issues:
     - [Splitting syspath preparation into stages (#41672)](https://github.com/apache/airflow/pull/41672)
- [x] [fix log for notifier(instance) without __name__ (#41591) (#41699)](https://github.com/apache/airflow/pull/41699): @obarisk
- [x] [Adding rel property to hyperlinks in logs (#41696) (#41783)](https://github.com/apache/airflow/pull/41783): @amoghrajesh
- [x] [Remove deprecation warning for cgitb in Plugins Manager (#41732) (#41793)](https://github.com/apache/airflow/pull/41793): @potiuk
- [x] [Keep FAB compatibility for versions before 1.3.0 in 2.10 (#41549) (#41809)](https://github.com/apache/airflow/pull/41809): @joaopamaral
- [ ] [Don't Fail LocalTaskJob on heartbeat (#41704) (#41810)](https://github.com/apache/airflow/pull/41810): @collinmcnulty @jedcunningham
     Linked issues:
     - [Don't Fail LocalTaskJob on heartbeat (#41704)](https://github.com/apache/airflow/pull/41704)
- [ ] [Fix: DAGs are not marked as stale if the dags folder change (#41433) (#41829)](https://github.com/apache/airflow/pull/41829): @utkarsharma2
- [x] [Set end_date and duration for triggers completed with end_from_trigger as True (#41834)](https://github.com/apache/airflow/pull/41834): @Lee-W
- [x] [[Backport] logout link in no roles error page fix (#41813) (#41845)](https://github.com/apache/airflow/pull/41845): @gagan-bhullar-tech @shahar1
     Linked issues:
     - [logout link in no roles error page fix (#41813)](https://github.com/apache/airflow/pull/41813)
- [ ] [Handle Example dags case when checking for missing files (#41856) (#41874)](https://github.com/apache/airflow/pull/41874): @utkarsharma2
     Linked issues:
     - [DAGs are not marked as stale if the AIRFLOW__CORE__DAGS_FOLDER changes (#41432)](https://github.com/apache/airflow/issues/41432)
- [x] [Bump webpack from 5.76.0 to 5.94.0 in /airflow/www (#41864) (#41879)](https://github.com/apache/airflow/pull/41879): @potiuk
- [x] [Adding tojson filter to example_inlet_event_extra example dag (#41873) (#41890)](https://github.com/apache/airflow/pull/41890): @potiuk
- [x] [Make Scarf usage reporting in major+minor versions and counters in buckets (#41898) (#41900)](https://github.com/apache/airflow/pull/41900): @jscheffl
- [x] [Add backcompat check for executors that don't inherit BaseExecutor (#… (#41927)](https://github.com/apache/airflow/pull/41927): @potiuk
- [x] [Protect against None components of universal pathlib xcom backend (#4… (#41938)](https://github.com/apache/airflow/pull/41938): @potiuk
     Linked issues:
     - [Universal-pathlib 0.2.3 seems to break compatibility with 0.2.2 (at least breaks mypy checks). (#41723)](https://github.com/apache/airflow/issues/41723)
- [x] [Lower down universal-pathlib minimum to 0.2.2 (#41939) (#41943)](https://github.com/apache/airflow/pull/41943): @potiuk


Thanks to all who contributed to the release (probably not a complete list!):
@gagan-bhullar-tech @obarisk @uranusjr @utkarsharma2 @bbovenzi @lh5844 @jedcunningham @amoghrajesh @jscheffl @shahar1 @potiuk @joaopamaral @collinmcnulty @Lee-W

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",utkarsharma2,2024-09-02 14:22:58+00:00,[],2024-09-06 11:43:39+00:00,2024-09-06 11:43:38+00:00,https://github.com/apache/airflow/issues/41956,"[('kind:meta', 'High-level information important to the community'), ('testing status', 'Status of testing releases')]","[{'comment_id': 2325312504, 'issue_id': 2501098156, 'author': 'obarisk', 'body': '#41591 works as expected\r\n\r\ntested with\r\n`airflow.providers.slack.notifications.slack_webhook.send_slack_webhook_notification`\r\nand\r\n`airflow.providers.discord.notifications.discord.DiscordNotifier`', 'created_at': datetime.datetime(2024, 9, 2, 21, 24, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325551166, 'issue_id': 2501098156, 'author': 'amoghrajesh', 'body': 'Tested my changes: https://github.com/apache/airflow/pull/41783, https://github.com/apache/airflow/pull/41680, https://github.com/apache/airflow/pull/41694 with breeze and all of them work as expected.', 'created_at': datetime.datetime(2024, 9, 3, 3, 45, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325554209, 'issue_id': 2501098156, 'author': 'amoghrajesh', 'body': ""Also tested https://github.com/apache/airflow/pull/41890, for which I am the original contributor. I think we should cherry pick to retain the original author because the committer who cherry picks might not be fully aware of the scope of the change and shouldn't be responsible for testing it."", 'created_at': datetime.datetime(2024, 9, 3, 3, 50, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325592569, 'issue_id': 2501098156, 'author': 'potiuk', 'body': '> * Adding tojson filter to example_inlet_event_extra example dag (#41873) (#41890)\r\n\r\nI think that needs some tooling improvement when generating the issue. The commit itself retains the original author:\r\n\r\n<img width=""680"" alt=""Screenshot 2024-09-03 at 06 39 03"" src=""https://github.com/user-attachments/assets/2047823c-d519-470d-814e-ee01ccc850e9"">\r\n\r\nBut cherry-picked PR does not (and it cannot, because you cannot make another person author of your PR). So when we generate the issue, we should read commit information and simply add all `commit authors` to the issue.\r\n\r\nCreated https://github.com/apache/airflow/issues/41969', 'created_at': datetime.datetime(2024, 9, 3, 4, 44, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325626507, 'issue_id': 2501098156, 'author': 'potiuk', 'body': 'All good for my changes !', 'created_at': datetime.datetime(2024, 9, 3, 5, 23, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325628329, 'issue_id': 2501098156, 'author': 'potiuk', 'body': 'One thing: @hadarsharon  -> can you please check if your https://github.com/apache/airflow/issues/41891  is solved in RC1 ?', 'created_at': datetime.datetime(2024, 9, 3, 5, 25, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325901266, 'issue_id': 2501098156, 'author': 'hadarsharon', 'body': 'Yes @potiuk I just checked and can confirm that #41891 is resolved in [v2.10.1.rc1](https://pypi.python.org/pypi/apache-airflow/2.10.1.rc1)\r\n\r\nCheers', 'created_at': datetime.datetime(2024, 9, 3, 8, 25, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2326419319, 'issue_id': 2501098156, 'author': 'vatsrahul1001', 'body': 'Noticed [issue](https://github.com/apache/airflow/issues/41977) while using TriggerDagRunOperator', 'created_at': datetime.datetime(2024, 9, 3, 12, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2326596343, 'issue_id': 2501098156, 'author': 'joaopamaral', 'body': 'Tested https://github.com/apache/airflow/pull/41809 with the following FAB version:\r\n- 1.2.2: permissions are synced correctly with the old access control format.\r\n- 1.3.0: permissions are synced correctly with new and old access control formats.\r\n\r\nSo all good for this change.', 'created_at': datetime.datetime(2024, 9, 3, 13, 56, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2327607940, 'issue_id': 2501098156, 'author': 'jscheffl', 'body': 'Can confirm the fixes/changes I contributed are in:\r\n\r\n- Reporting of Scarf in buckets (#41898/#41898)\r\n- Deprecation of warnings in Plugins Manager (#4173/#41793)\r\n- Bump of Micromatch version (#41726/#41755)\r\n- Universal Pathlib Fixes (Various PRs, is now 0.2.3)\r\n- XCom UI Rendering Issues for INT, BOOL, FLOAT, NULL (#41516/#41605)', 'created_at': datetime.datetime(2024, 9, 3, 23, 15, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2327838913, 'issue_id': 2501098156, 'author': 'Lee-W', 'body': ""#41834 was @tirkarthi 's fix. I just tested it, and it works fine. Thanks @tirkarthi !"", 'created_at': datetime.datetime(2024, 9, 4, 3, 24, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2329272955, 'issue_id': 2501098156, 'author': 'tirkarthi', 'body': ""Thanks @Lee-W for the backport, it's working fine as expected."", 'created_at': datetime.datetime(2024, 9, 4, 14, 47, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2331977242, 'issue_id': 2501098156, 'author': 'jrderuiter', 'body': 'Confirmed that #42032 is working fine in the rc.', 'created_at': datetime.datetime(2024, 9, 5, 15, 12, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333874578, 'issue_id': 2501098156, 'author': 'utkarsharma2', 'body': 'Airflow 2.10.1 has been released. Thank you all for testing this release', 'created_at': datetime.datetime(2024, 9, 6, 11, 43, 39, tzinfo=datetime.timezone.utc)}]","obarisk on (2024-09-02 21:24:39 UTC): #41591 works as expected

tested with
`airflow.providers.slack.notifications.slack_webhook.send_slack_webhook_notification`
and
`airflow.providers.discord.notifications.discord.DiscordNotifier`

amoghrajesh on (2024-09-03 03:45:43 UTC): Tested my changes: https://github.com/apache/airflow/pull/41783, https://github.com/apache/airflow/pull/41680, https://github.com/apache/airflow/pull/41694 with breeze and all of them work as expected.

amoghrajesh on (2024-09-03 03:50:09 UTC): Also tested https://github.com/apache/airflow/pull/41890, for which I am the original contributor. I think we should cherry pick to retain the original author because the committer who cherry picks might not be fully aware of the scope of the change and shouldn't be responsible for testing it.

potiuk on (2024-09-03 04:44:16 UTC): I think that needs some tooling improvement when generating the issue. The commit itself retains the original author:

<img width=""680"" alt=""Screenshot 2024-09-03 at 06 39 03"" src=""https://github.com/user-attachments/assets/2047823c-d519-470d-814e-ee01ccc850e9"">

But cherry-picked PR does not (and it cannot, because you cannot make another person author of your PR). So when we generate the issue, we should read commit information and simply add all `commit authors` to the issue.

Created https://github.com/apache/airflow/issues/41969

potiuk on (2024-09-03 05:23:19 UTC): All good for my changes !

potiuk on (2024-09-03 05:25:20 UTC): One thing: @hadarsharon  -> can you please check if your https://github.com/apache/airflow/issues/41891  is solved in RC1 ?

hadarsharon on (2024-09-03 08:25:09 UTC): Yes @potiuk I just checked and can confirm that #41891 is resolved in [v2.10.1.rc1](https://pypi.python.org/pypi/apache-airflow/2.10.1.rc1)

Cheers

vatsrahul1001 on (2024-09-03 12:39:00 UTC): Noticed [issue](https://github.com/apache/airflow/issues/41977) while using TriggerDagRunOperator

joaopamaral on (2024-09-03 13:56:28 UTC): Tested https://github.com/apache/airflow/pull/41809 with the following FAB version:
- 1.2.2: permissions are synced correctly with the old access control format.
- 1.3.0: permissions are synced correctly with new and old access control formats.

So all good for this change.

jscheffl on (2024-09-03 23:15:03 UTC): Can confirm the fixes/changes I contributed are in:

- Reporting of Scarf in buckets (#41898/#41898)
- Deprecation of warnings in Plugins Manager (#4173/#41793)
- Bump of Micromatch version (#41726/#41755)
- Universal Pathlib Fixes (Various PRs, is now 0.2.3)
- XCom UI Rendering Issues for INT, BOOL, FLOAT, NULL (#41516/#41605)

Lee-W on (2024-09-04 03:24:33 UTC): #41834 was @tirkarthi 's fix. I just tested it, and it works fine. Thanks @tirkarthi !

tirkarthi on (2024-09-04 14:47:55 UTC): Thanks @Lee-W for the backport, it's working fine as expected.

jrderuiter on (2024-09-05 15:12:18 UTC): Confirmed that #42032 is working fine in the rc.

utkarsharma2 (Issue Creator) on (2024-09-06 11:43:39 UTC): Airflow 2.10.1 has been released. Thank you all for testing this release

"
2500643515,issue,open,,can_create permission missing for Users,"### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

We're seeing can_create action missing on Resource - Users on Airflow v2.9.3. We've integration with Azure AD for authentication, although user creation & management is through API
<img width=""1707"" alt=""361796422-fd757f17-b139-4504-a6e3-cd2d0dc940b3"" src=""https://github.com/user-attachments/assets/e736defc-ce24-4952-86c2-ad9a0c93544b"">


### What you think should happen instead?

_No response_

### How to reproduce

Install fresh build of Airflow with version 2.9.3

### Operating System

Same as Airflow docker image

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other Docker-based deployment

### Deployment details

Installed using custom deployment templates on AKS. with Airflow docker image from registery

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vandanthaker-ey,2024-09-02 10:52:40+00:00,[],2024-11-13 07:56:39+00:00,,https://github.com/apache/airflow/issues/41949,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2324431973, 'issue_id': 2500643515, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 9, 2, 10, 52, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2472733068, 'issue_id': 2500643515, 'author': 'bedag-bad', 'body': 'We are seeing the same behaviour.\r\nRemoval of the can_create action missing on Users is also visible in the logs. See screenshot.\r\n\r\nAny info, on why this happens?\r\n\r\n![image](https://github.com/user-attachments/assets/23773426-23b8-44a8-9d03-6a65319c8581)', 'created_at': datetime.datetime(2024, 11, 13, 7, 56, 38, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-09-02 10:52:44 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

bedag-bad on (2024-11-13 07:56:38 UTC): We are seeing the same behaviour.
Removal of the can_create action missing on Users is also visible in the logs. See screenshot.

Any info, on why this happens?

![image](https://github.com/user-attachments/assets/23773426-23b8-44a8-9d03-6a65319c8581)

"
2499655261,issue,open,,"Make ""Trusted Publishing"" works for our PyPI releasing","We already agreed with the ASF infrastrucutre that it would be great to use Trusted publishing to publish packages to PyPI. Curently we are using ""twine"" and local API keys by release managers - but https://docs.pypi.org/trusted-publishers/ allows to configure our PyPI organisation to accept ""Github Actions"" workflows publishing to PyPI via dedicated workflows - where GitHub Actions is a trusted publisher.

The documentation explains how to do it - we will need to involve INFRA to configure it for our repository.

The idea to implement is is that Github Actions workflow should not ""build"" the packages to publish in PyPI - but they should download them from ""https://downloads.apache.org/"" and ""https://dist.apache.org/repos/dist/dev/airflow/"" for RC/Alpha/Beta packages, verify their integrity (checksums/signatures) similarly to https://github.com/apache/airflow/blob/main/dev/README_RELEASE_AIRFLOW.md and publishing those packages. 

Intead of ""twine upload"", release manager should just run a workflow in GitHub Actions that should download packages from apache svn/downloads and publish them in PyPI after verification.",potiuk,2024-09-01 19:54:55+00:00,['gopidesupavan'],2024-12-01 09:49:52+00:00,,https://github.com/apache/airflow/issues/41937,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:dev-tools', '')]","[{'comment_id': 2440155183, 'issue_id': 2499655261, 'author': 'potiuk', 'body': 'Just to add - example changes in workflows proposed by `pip` https://github.com/pypa/pip/pull/13048', 'created_at': datetime.datetime(2024, 10, 27, 19, 57, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2498118799, 'issue_id': 2499655261, 'author': 'gopidesupavan', 'body': '@potiuk is this ready to pick? just checking.', 'created_at': datetime.datetime(2024, 11, 25, 14, 7, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499407687, 'issue_id': 2499655261, 'author': 'potiuk', 'body': 'Yep.', 'created_at': datetime.datetime(2024, 11, 26, 1, 50, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507228453, 'issue_id': 2499655261, 'author': 'gopidesupavan', 'body': 'Thanks, to my understand this one to implement for all the packages that we release today? airflow, providers, airflow python client?\n\nI’m planning to implement in generalize way, so that it can be useful for other Apache projects. This would allow them to integrate their scripts if they have any custom, and run it as a GitHub Action, WDYT? :)', 'created_at': datetime.datetime(2024, 11, 29, 7, 24, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507724501, 'issue_id': 2499655261, 'author': 'potiuk', 'body': '> Thanks, to my understand this one to implement for all the packages that we release today? airflow, providers, airflow python client?\r\n\r\nAbsolutely. We can start with providers - then we will have a chance to test it quickly - and then use it for the others.\r\n \r\n> I’m planning to implement in generalize way, so that it can be useful for other Apache projects. This would allow them to integrate their scripts if they have any custom, and run it as a GitHub Action, WDYT? :)\r\n\r\nPerfect. ASF infrastructure already has a repo for shared actions in ASF:  https://github.com/apache/infrastructure-actions and we are just discussing to splitting it to separate actions, but the idea is to have something that will be reusable across many projects.\r\n\r\nI think the best (and most reusable) way of publishing is to use packages released in ""svn"".\r\n\r\nWe should be able to plug in this step in the release process. And we have two different steps there:\r\n\r\n1) RC candidates.\r\n\r\n* https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDER_PACKAGES.md#publish-the-regular-convenience-package-to-pypi\r\n\r\nWe should just download all packages from https://dist.apache.org/repos/dist/dev/airflow/providers/pypi/\r\n\r\n> Yes it really useful with pypi folder and simple to make automated process verification\r\n\r\nBut we need to make sure that the ""pypi"" packages are also stored in SVN  - the little difficulty is that we currently do not upload ""pypi"" RC packages to SVN.  The ""rc"" candidates in SVN are different than the one we publish to PyPI, because they do not contain ""rc"" in the version (because potentially they might become final candidates to upload).\r\n\r\nSo we should modify the process and make sure that we upload ""pypi"" packages to SVN (I propose to add ""pypi"" subfolder - so the release process shoudl be updated to clean/recreate the `pypi` subfolder and push the pypi packages there.\r\n\r\nThen our action should be as simple as a) download the right packages from SVN b) push them to PyPI via trusted publishing. I have a feeling that this might be simply a standard xml composite action using existing actions from GitHub  - nothing fancy, we might just add a few options:\r\n\r\na) test mode - do everything except that final step will be just printing what should be done\r\nb) verification options -  it should be possible to also download and verify the signatures, checksums (and later maybe licences) in the downloaded artifacts.\r\n\r\n2) Final packages:\r\n\r\nFinal packages can be directly downloaded from https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDER_PACKAGES.md#publish-release-to-svn -    i.e. download and push packages from https://dist.apache.org/repos/dist/release/airflow/providers/  - the packages are the same as the ones uploaded to PyPI,\r\n\r\nThere - the little difficulty with it that we should know WHICH packages to upload. And this I think means that the easiest will be to do it just before this step:\r\n\r\n```bash\r\nfor file in ""${SOURCE_DIR}""/*\r\ndo\r\n base_file=$(basename ${file})\r\n cp -v ""${file}"" ""${AIRFLOW_REPO_ROOT}/dist/${base_file//rc[0-9]/}""\r\n svn mv ""${file}"" ""${base_file//rc[0-9]/}""\r\ndone\r\n```\r\n\r\n> This makes sense to me and makes it easier to compare. This is where I’m considering providing a flexible option in the GitHub Action to allow adding any extra scripts/custom code for validation.\r\n\r\nAnd upload them from the ""dist"" SVN not from the ""release"". Because at this moment, release manager already removed the files that were removed during voting - so ""dist"" contains only the packages that in a moment will be promoted to be ""final"" packages. But the files are still in ""dev"" and those are the only ones we should upload to PyPI. In ""release"" there will be all the previosly uploaded last version of the packages - even those we do not release now.\r\n\r\nThere, likely we need some controls - for example, being able to manually override which packages we want to publish.\r\n\r\nWe might start with a simple set of features, but later on that action might become a little more feature-full.\r\n\r\n> Yes, I’ll start with a simple approach, review the design later, and build on it incrementally.', 'created_at': datetime.datetime(2024, 11, 29, 12, 30, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507734390, 'issue_id': 2499655261, 'author': 'potiuk', 'body': 'BTW. Speaking of licences - @Claudenw works on a new version of RAT that should allow us to verify licences without unpacking the files, so once this is released (I guess very soon) - we wil be able to use it in the GitHub Action and do the sources licence verification. Here, one of the options we will have to add is to be able to provide an extra file with ""ratignore"".', 'created_at': datetime.datetime(2024, 11, 29, 12, 36, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507750247, 'issue_id': 2499655261, 'author': 'potiuk', 'body': 'Also later we might add reproducibility check - so we will be able to build the same packages locally on GitHub Actions and compare them with the ones we pulled from SVN - that would be an ultimate check for reproducibility - whether there are no MITM attacks etc. and will be ultimately a way to verify if the future platform running ATR (Apache Trusted Releases) build has not been compromised.', 'created_at': datetime.datetime(2024, 11, 29, 12, 46, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507779077, 'issue_id': 2499655261, 'author': 'gopidesupavan', 'body': ""Thats fantastic :) I was reviewing the current release process steps and had a few questions about the packages published to SVN versus those published to PyPI. Thanks for clarifying—it's much clearer now.\r\n\r\nThe idea of a test/verification mode is excellent!"", 'created_at': datetime.datetime(2024, 11, 29, 13, 2, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507821662, 'issue_id': 2499655261, 'author': 'potiuk', 'body': 'Yeah. the main thing is that the ""RC"" packages we keep in SVN do not have RC in version  - as they can ""eventually"" when voted become final packages (we just move them to ""release"" SVN when they `graduate`). But we cannot upload them to PyPI - becuase PyPI packages are ""immutable"" - so we need to prepare the ""pypi"" version of the same RC. \r\n\r\nBTW. This RC variation MIGHT change in the future - PyPI team has two things they are working on:\r\n\r\n* https://peps.python.org/pep-0759/ PEP 759 – External Wheel Hosting - so we **might** be able to just upload metadata to PYPI and host our packages directly from SVN \r\n* (no Draft PEP yet - but there are discussions) - allow to have ""staging"" releases of packages - i.e. upload packages with pre-release versions as the ""same"" packages but not ""finalize"" them - immutability will come when the package is put in ""released"" staget', 'created_at': datetime.datetime(2024, 11, 29, 13, 28, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2509555634, 'issue_id': 2499655261, 'author': 'gopidesupavan', 'body': 'Alright have basic feat workflow. it will not do any publish but it checks out the packages from the svn and validates checksum and svn check. if this workflow design and approach make sense we can further extend this one.\r\n\r\nAction repo: \r\nhttps://github.com/gopidesupavan/gh-svn-pypi-publisher/pull/1\r\n\r\nThis is usage repo:\r\nconfig: https://github.com/gopidesupavan/example-workspace/blob/main/publish-config.yml\r\nIn this config i have referred apache svn repo with release provider folder to test, but that can be changed to pypi folder. :)\r\n\r\nsample github action workflow: https://github.com/gopidesupavan/example-workspace/blob/main/.github/workflows/custom-action-example.yml\r\n\r\nSVN check:\r\n It takes all the extensions from the config file and checks if the package has all the required extensions or not.\r\n\r\nChecksum check:\r\n  It takes the checksum type from the config file and checks if the package has the required checksum or not.', 'created_at': datetime.datetime(2024, 12, 1, 3, 52, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2509557000, 'issue_id': 2499655261, 'author': 'gopidesupavan', 'body': 'https://github.com/gopidesupavan/example-workspace/actions/runs/12101906770/job/33742345538#step:3:34', 'created_at': datetime.datetime(2024, 12, 1, 3, 58, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2509662101, 'issue_id': 2499655261, 'author': 'gopidesupavan', 'body': 'Add signature check also :) \r\n\r\nhttps://github.com/gopidesupavan/example-workspace/blob/main/publish-config.yml\r\nhttps://github.com/gopidesupavan/example-workspace/actions/runs/12104340868/job/33747628251', 'created_at': datetime.datetime(2024, 12, 1, 9, 49, 51, tzinfo=datetime.timezone.utc)}]","potiuk (Issue Creator) on (2024-10-27 19:57:43 UTC): Just to add - example changes in workflows proposed by `pip` https://github.com/pypa/pip/pull/13048

gopidesupavan (Assginee) on (2024-11-25 14:07:52 UTC): @potiuk is this ready to pick? just checking.

potiuk (Issue Creator) on (2024-11-26 01:50:56 UTC): Yep.

gopidesupavan (Assginee) on (2024-11-29 07:24:47 UTC): Thanks, to my understand this one to implement for all the packages that we release today? airflow, providers, airflow python client?

I’m planning to implement in generalize way, so that it can be useful for other Apache projects. This would allow them to integrate their scripts if they have any custom, and run it as a GitHub Action, WDYT? :)

potiuk (Issue Creator) on (2024-11-29 12:30:09 UTC): Absolutely. We can start with providers - then we will have a chance to test it quickly - and then use it for the others.
 

Perfect. ASF infrastructure already has a repo for shared actions in ASF:  https://github.com/apache/infrastructure-actions and we are just discussing to splitting it to separate actions, but the idea is to have something that will be reusable across many projects.

I think the best (and most reusable) way of publishing is to use packages released in ""svn"".

We should be able to plug in this step in the release process. And we have two different steps there:

1) RC candidates.

* https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDER_PACKAGES.md#publish-the-regular-convenience-package-to-pypi

We should just download all packages from https://dist.apache.org/repos/dist/dev/airflow/providers/pypi/


But we need to make sure that the ""pypi"" packages are also stored in SVN  - the little difficulty is that we currently do not upload ""pypi"" RC packages to SVN.  The ""rc"" candidates in SVN are different than the one we publish to PyPI, because they do not contain ""rc"" in the version (because potentially they might become final candidates to upload).

So we should modify the process and make sure that we upload ""pypi"" packages to SVN (I propose to add ""pypi"" subfolder - so the release process shoudl be updated to clean/recreate the `pypi` subfolder and push the pypi packages there.

Then our action should be as simple as a) download the right packages from SVN b) push them to PyPI via trusted publishing. I have a feeling that this might be simply a standard xml composite action using existing actions from GitHub  - nothing fancy, we might just add a few options:

a) test mode - do everything except that final step will be just printing what should be done
b) verification options -  it should be possible to also download and verify the signatures, checksums (and later maybe licences) in the downloaded artifacts.

2) Final packages:

Final packages can be directly downloaded from https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDER_PACKAGES.md#publish-release-to-svn -    i.e. download and push packages from https://dist.apache.org/repos/dist/release/airflow/providers/  - the packages are the same as the ones uploaded to PyPI,

There - the little difficulty with it that we should know WHICH packages to upload. And this I think means that the easiest will be to do it just before this step:

```bash
for file in ""${SOURCE_DIR}""/*
do
 base_file=$(basename ${file})
 cp -v ""${file}"" ""${AIRFLOW_REPO_ROOT}/dist/${base_file//rc[0-9]/}""
 svn mv ""${file}"" ""${base_file//rc[0-9]/}""
done
```


And upload them from the ""dist"" SVN not from the ""release"". Because at this moment, release manager already removed the files that were removed during voting - so ""dist"" contains only the packages that in a moment will be promoted to be ""final"" packages. But the files are still in ""dev"" and those are the only ones we should upload to PyPI. In ""release"" there will be all the previosly uploaded last version of the packages - even those we do not release now.

There, likely we need some controls - for example, being able to manually override which packages we want to publish.

We might start with a simple set of features, but later on that action might become a little more feature-full.

potiuk (Issue Creator) on (2024-11-29 12:36:12 UTC): BTW. Speaking of licences - @Claudenw works on a new version of RAT that should allow us to verify licences without unpacking the files, so once this is released (I guess very soon) - we wil be able to use it in the GitHub Action and do the sources licence verification. Here, one of the options we will have to add is to be able to provide an extra file with ""ratignore"".

potiuk (Issue Creator) on (2024-11-29 12:46:04 UTC): Also later we might add reproducibility check - so we will be able to build the same packages locally on GitHub Actions and compare them with the ones we pulled from SVN - that would be an ultimate check for reproducibility - whether there are no MITM attacks etc. and will be ultimately a way to verify if the future platform running ATR (Apache Trusted Releases) build has not been compromised.

gopidesupavan (Assginee) on (2024-11-29 13:02:36 UTC): Thats fantastic :) I was reviewing the current release process steps and had a few questions about the packages published to SVN versus those published to PyPI. Thanks for clarifying—it's much clearer now.

The idea of a test/verification mode is excellent!

potiuk (Issue Creator) on (2024-11-29 13:28:19 UTC): Yeah. the main thing is that the ""RC"" packages we keep in SVN do not have RC in version  - as they can ""eventually"" when voted become final packages (we just move them to ""release"" SVN when they `graduate`). But we cannot upload them to PyPI - becuase PyPI packages are ""immutable"" - so we need to prepare the ""pypi"" version of the same RC. 

BTW. This RC variation MIGHT change in the future - PyPI team has two things they are working on:

* https://peps.python.org/pep-0759/ PEP 759 – External Wheel Hosting - so we **might** be able to just upload metadata to PYPI and host our packages directly from SVN 
* (no Draft PEP yet - but there are discussions) - allow to have ""staging"" releases of packages - i.e. upload packages with pre-release versions as the ""same"" packages but not ""finalize"" them - immutability will come when the package is put in ""released"" staget

gopidesupavan (Assginee) on (2024-12-01 03:52:08 UTC): Alright have basic feat workflow. it will not do any publish but it checks out the packages from the svn and validates checksum and svn check. if this workflow design and approach make sense we can further extend this one.

Action repo: 
https://github.com/gopidesupavan/gh-svn-pypi-publisher/pull/1

This is usage repo:
config: https://github.com/gopidesupavan/example-workspace/blob/main/publish-config.yml
In this config i have referred apache svn repo with release provider folder to test, but that can be changed to pypi folder. :)

sample github action workflow: https://github.com/gopidesupavan/example-workspace/blob/main/.github/workflows/custom-action-example.yml

SVN check:
 It takes all the extensions from the config file and checks if the package has all the required extensions or not.

Checksum check:
  It takes the checksum type from the config file and checks if the package has the required checksum or not.

gopidesupavan (Assginee) on (2024-12-01 03:58:01 UTC): https://github.com/gopidesupavan/example-workspace/actions/runs/12101906770/job/33742345538#step:3:34

gopidesupavan (Assginee) on (2024-12-01 09:49:51 UTC): Add signature check also :) 

https://github.com/gopidesupavan/example-workspace/blob/main/publish-config.yml
https://github.com/gopidesupavan/example-workspace/actions/runs/12104340868/job/33747628251

"
2499652498,issue,open,,"Make ""multi-platform"" images build/workflow works back using ARM and AMD instances","Currently the workflow to build multi-platform images are disabled (see UPDATE: below - they are enabled back but using slow emulation). The problem is that we need to have two platforms connected with SSH and it stopped working when ""self-hosted"" runners stopped working aftet patching GitHub runners done by @ashb  stopped working.

The workflow that used to work is: https://github.com/apache/airflow/blob/main/.github/workflows/release_dockerhub_image.yml

With the ARC ARM runners we should be able to bring it back but differently.

The current multi-platform image building (described in https://github.com/apache/airflow/blob/main/dev/MANUALLY_BUILDING_IMAGES.md uses `buildkit` capability to build multi-platform images using multiple docker workers. By running the commands with `--dry-run` you could see the exact docker build commands that build the images. 

However, it's possible to build airflow images using separate, non-connected runners (one for AMD and one for ARM) - by pushing images to docker and building a common manifest. This is described in https://docs.docker.com/build/ci/github-actions/multi-platform/ and with ARC runners, we should be able to do it. ",potiuk,2024-09-01 19:47:47+00:00,[],2024-11-30 14:16:33+00:00,,https://github.com/apache/airflow/issues/41935,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:dev-tools', ''), ('area:self-hosted-runners', '')]","[{'comment_id': 2325221929, 'issue_id': 2499652498, 'author': 'potiuk', 'body': 'UPDATE: as of https://github.com/apache/airflow/pull/41959, the workflows are working back with emulation support - but they are REALLY slow > 1hr comparing to 15 minutes when ARM hardware is used.', 'created_at': datetime.datetime(2024, 9, 2, 19, 16, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508975839, 'issue_id': 2499652498, 'author': 'potiuk', 'body': 'cc: @hussein-awala -> time to complete  the ARC work  :D ?', 'created_at': datetime.datetime(2024, 11, 30, 14, 16, 32, tzinfo=datetime.timezone.utc)}]","potiuk (Issue Creator) on (2024-09-02 19:16:18 UTC): UPDATE: as of https://github.com/apache/airflow/pull/41959, the workflows are working back with emulation support - but they are REALLY slow > 1hr comparing to 15 minutes when ARM hardware is used.

potiuk (Issue Creator) on (2024-11-30 14:16:32 UTC): cc: @hussein-awala -> time to complete  the ARC work  :D ?

"
2499650436,issue,open,,Add ARM image buidling / cache preparation,"Currently ARM image building is disabled, because we did not have ""self-hosted"" runners working after the ""runners"" patching by @ashb stopped working. 

However with the new ARC runners, we should be able to switch to ""ARC"" self-hosted ARM runners that we could use to build ARM cache and images.

The ARM building is currently disabled:

* https://github.com/apache/airflow/blob/main/.github/workflows/additional-ci-image-checks.yml#L148
* https://github.com/apache/airflow/blob/main/.github/workflows/finalize-tests.yml#L153

A draft attempt to do it has been done gere https://github.com/apache/airflow/pull/41049 - but it does not work because public ARM images are MacOS and they do not support hypervisor so running docker. However our own ARC images should be fine.",potiuk,2024-09-01 19:42:32+00:00,[],2024-09-01 19:44:51+00:00,,https://github.com/apache/airflow/issues/41934,"[('area:CI', ""Airflow's tests and continious integration""), ('area:self-hosted-runners', '')]",[],
2499648109,issue,open,,Update documentation for CI/CD to include composite workflows,"The documentation in https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/05_workflows.md is slightly outdated. While it explains general approach properly, it does not explain the ""composite workflow"" grouping introduced in the last few months - i.e. all worklfows that are used with `uses: ./.github/workflows/...` in https://github.com/apache/airflow/blob/main/.github/workflows/ci.yml

The documentation could be updated to a bit higher level - describing the ""composite"" groups of workflows rather than individual jobs.",potiuk,2024-09-01 19:36:42+00:00,[],2024-11-30 13:24:30+00:00,,https://github.com/apache/airflow/issues/41933,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('kind:documentation', ''), ('area:CI', ""Airflow's tests and continious integration"")]","[{'comment_id': 2508962245, 'issue_id': 2499648109, 'author': 'potiuk', 'body': 'This will be far simpler when we complete #43268', 'created_at': datetime.datetime(2024, 11, 30, 13, 24, 29, tzinfo=datetime.timezone.utc)}]","potiuk (Issue Creator) on (2024-11-30 13:24:29 UTC): This will be far simpler when we complete #43268

"
2499645390,issue,open,,"Convert ""Canary"" runs back from scheduled to run after every merge","Currently ""Canary"" runs are run 4x day as ""scheduled"" runs - becuase they take more than 1.5 hour to complete. However it would be great if we come back to the previous setting where canary build are triggered on ""push"" to main and ""v2-*"" branches - when we will start using bigger ARC runners that should speed up the ""complete"" builds

This has been changed in https://github.com/apache/airflow/issues/40925 and we could revert it back after we can speed up the builds with ARC runners.",potiuk,2024-09-01 19:30:43+00:00,[],2024-09-05 10:35:47+00:00,,https://github.com/apache/airflow/issues/41932,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:CI', ""Airflow's tests and continious integration"")]",[],
2499643778,issue,closed,completed,Bring back Documentation publishing in Canary runs,"Currently, documentation publishing in Canary runs is disabled. This caused several problems (that will get worse over time):

* we do not test publishing worklfow - and it might stop working any time and break release process
* the ""sphinx inventory"" is not published for airlfow and providers which means that docs building will take more time and will lead to potential errors

https://github.com/apache/airflow/blob/7cf54a734e1eefa04ab710cb2eb364d529c6b1b1/.github/workflows/static-checks-mypy-docs.yml#L246

The reason is that public runners default ""workspace"" disk is too small to fit checked out documentation while being published - and ""self-hosted ASF runners"" were unstable even if they had bigger disk.

There are few ways we can address it:

* apply the same workaround that @kaxil did for ""airflow-site"" building : https://github.com/apache/airflow-site/pull/1055
* switch back to ""ASF self-hosted runners"" https://github.com/apache/airflow/actions/runners?tab=self-hosted
* switch to large or medium ""ARC self-hosted runners"" after https://github.com/apache/airflow/pull/41728 is completed

It's been disabled in https://github.com/apache/airflow/pull/40921",potiuk,2024-09-01 19:26:59+00:00,"['potiuk', 'kaxil']",2024-10-31 19:26:50+00:00,2024-10-31 19:26:50+00:00,https://github.com/apache/airflow/issues/41931,"[('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('kind:documentation', ''), ('area:CI', ""Airflow's tests and continious integration""), ('area:self-hosted-runners', '')]","[{'comment_id': 2421532263, 'issue_id': 2499643778, 'author': 'potiuk', 'body': '@kaxil - shall you apply the same fix to the workflow as you did in airlfow-site (trick with mounting another volume) - so that auto-publishing of docs and inventory works ?', 'created_at': datetime.datetime(2024, 10, 18, 6, 28, 45, tzinfo=datetime.timezone.utc)}]","potiuk (Issue Creator) on (2024-10-18 06:28:45 UTC): @kaxil - shall you apply the same fix to the workflow as you did in airlfow-site (trick with mounting another volume) - so that auto-publishing of docs and inventory works ?

"
2499004272,issue,closed,completed,StepFunctionStartExecutionOperator in MWAA does not throw error if IAM Role does not DescribeExecution permission,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.8.1

### What happened?

I am using StepFunctionStartExecutionOperator to execute a StateMachine.

```
StepFunctionStartExecutionOperator(
...
task_id=""load_data"",
deferrable=True,
waiter_delay=30,  # Poll for every 30 seconds
waiter_max_attempts=10,  # maximum number of attempts to poll for status
do_xcom_push=True,
)
```

if I look into the log of the task,

 {{waiter_with_logging.py:129}} INFO - Status of step function execution is: 
{{waiter_with_logging.py:129}} INFO - Status of step function execution is: 
{{waiter_with_logging.py:129}} INFO - Status of step function execution is: 

as you can see that airflow is not getting the current status (RUNNING, FAILED etc) of the StateMachine



### What you think should happen instead?

Expected Output when the State Machine is RUNNING

```
[2024-08-28, 17:01:06 UTC] {{waiter_with_logging.py:129}} INFO - Status of step function execution is: RUNNING
[2024-08-28, 17:02:06 UTC] {{waiter_with_logging.py:129}} INFO - Status of step function execution is: RUNNING
```


### How to reproduce

So in my case I found the root cause of this problem. The IAM Role associated with the Airflow did not have the below permission

```
'states:DescribeExecution'
```

On the StateMachine execution arn.

```
arn:aws:states:<Region>:<accountId>:execution:<stateMachineName>:*
```


## Before granting the permission

```
[2024-08-28, 01:41:53 UTC] {{waiter_with_logging.py:129}} INFO - Status of step function execution is:
```
### After Granting the permission
```
[2024-08-28, 17:01:06 UTC] {{waiter_with_logging.py:129}} INFO - Status of step function execution is: RUNNING

```

### Operating System

Managed Airflow

### Versions of Apache Airflow Providers

_No response_

### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",sarkch,2024-08-31 18:47:50+00:00,[],2024-09-04 15:15:47+00:00,2024-09-04 15:15:47+00:00,https://github.com/apache/airflow/issues/41918,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('area:core', '')]","[{'comment_id': 2323434363, 'issue_id': 2499004272, 'author': 'gopidesupavan', 'body': 'Thanks for opening this, Looks like valid issue, able to re produce, these kind of errors validation is missing in async_wait.\r\nWill push the changes for this.\r\n\r\nThe AccessDeniedException log is i have added to produce what exact error waiter throwing :) \r\n\r\ncc: @eladkal \r\n\r\n<img width=""775"" alt=""image"" src=""https://github.com/user-attachments/assets/1581ffca-fb61-414b-aaa9-ed15cf72b118"">', 'created_at': datetime.datetime(2024, 9, 1, 17, 29, 30, tzinfo=datetime.timezone.utc)}]","gopidesupavan on (2024-09-01 17:29:30 UTC): Thanks for opening this, Looks like valid issue, able to re produce, these kind of errors validation is missing in async_wait.
Will push the changes for this.

The AccessDeniedException log is i have added to produce what exact error waiter throwing :) 

cc: @eladkal 

<img width=""775"" alt=""image"" src=""https://github.com/user-attachments/assets/1581ffca-fb61-414b-aaa9-ed15cf72b118"">

"
2498812237,issue,closed,completed,Error loop when using Airflow's docker compose file,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I am trying to run a docker compose file from [official docker compose file][1] (with airflow image = `apache/airflow:slim-latest`): 

When I run `docker compose up` I get the following error : 

    ModuleNotFoundError: No module named 'psycopg2'

I tried to fix the issue by creating my own Dockerfile

    FROM apache/airflow:slim-latest
    USER root
    RUN apt-get -y update && apt-get install libpq-dev build-essential -y
    USER 50000
    RUN pip install psycopg2
    CMD []

And using it instead of the airflow image, but then my docker compose loops in failure : 

    ERROR - Failed to load CLI commands from executor: CeleryExecutor
    ...
    ERROR - Ensure all dependencies are met and try again. If using a Celery based executor install a 3.3.0+ version of the Celery provider. If using a Kubernetes executor, install a 7.4.0+ version of the CNCF provider
    ...
    ModuleNotFoundError: No module named 'airflow.providers.celery'


What am I doing wrong here ? How can I make it work ?

Note : 
* I tried multiple airflow images, they all gave the same result


  [1]: https://github.com/apache/airflow/blob/main/docs/apache-airflow/howto/docker-compose/docker-compose.yaml

### What you think should happen instead?

I expected `docker compose up` command to setup a running environement

### How to reproduce

Run `docker compose up` with this [docker-compose.yaml](https://github.com/apache/airflow/blob/main/docs/apache-airflow/howto/docker-compose/docker-compose.yaml) 

### Operating System

ubuntu

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Nakeuh,2024-08-31 12:03:17+00:00,[],2024-08-31 12:56:36+00:00,2024-08-31 12:56:36+00:00,https://github.com/apache/airflow/issues/41915,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:dependencies', 'Issues related to dependencies problems'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2322876453, 'issue_id': 2498812237, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 31, 12, 3, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2322885708, 'issue_id': 2498812237, 'author': 'Nakeuh', 'body': 'Finally found a fix. Using `apache/airflow:2.10.0` instead of `apache/airflow:slim-latest` solved the issue', 'created_at': datetime.datetime(2024, 8, 31, 12, 39, 56, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-31 12:03:19 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

Nakeuh (Issue Creator) on (2024-08-31 12:39:56 UTC): Finally found a fix. Using `apache/airflow:2.10.0` instead of `apache/airflow:slim-latest` solved the issue

"
2498720641,issue,closed,completed,น,"น

_Originally posted by @Benz19933 in https://github.com/apache/airflow/discussions/41911_",Benz19933,2024-08-31 08:07:19+00:00,[],2024-08-31 09:40:39+00:00,2024-08-31 09:40:39+00:00,https://github.com/apache/airflow/issues/41913,[],[],
2498720392,issue,closed,not_planned,น,"น

_Originally posted by @Benz19933 in https://github.com/apache/airflow/discussions/41911_",Benz19933,2024-08-31 08:06:36+00:00,[],2024-09-01 11:43:06+00:00,2024-09-01 11:42:54+00:00,https://github.com/apache/airflow/issues/41912,[],[],
2497625602,issue,closed,not_planned,After user admin recreation: AttributeError: 'NoneType' object has no attribute 'is_active': unable to connect,"### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

After deletion and recreation of the user admin to change its password, Airflow is no more accessible and fails with:

│   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/fab/auth_manager/fab_auth_manager.py"", line 183, in is_logged_in                                                                    │
│     return not self.get_user().is_anonymous                                                                                                                                                                     
│   File ""/home/airflow/.local/lib/python3.12/site-packages/werkzeug/local.py"", line 316, in __get__                                                                                                              
│     obj = instance._get_current_object()                                                                                                                                                                        
│   File ""/home/airflow/.local/lib/python3.12/site-packages/werkzeug/local.py"", line 520, in _get_current_object                                                                                                  
│     return get_name(local())  # type: ignore                                                                                                                                                                    
│   File ""/home/airflow/.local/lib/python3.12/site-packages/flask_login/utils.py"", line 25, in <lambda>                                                                                                          
│     current_user = LocalProxy(lambda: _get_user())                                                                                                                                                             
│   File ""/home/airflow/.local/lib/python3.12/site-packages/flask_login/utils.py"", line 370, in _get_user                                                                                                        
│     current_app.login_manager._load_user()                                                                                                                                                                     
│   File ""/home/airflow/.local/lib/python3.12/site-packages/flask_login/login_manager.py"", line 364, in _load_user                                                                                               
│     user = self._user_callback(user_id)                                                                                                                                                                         
│   File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/fab/auth_manager/security_manager/override.py"", line 1553, in load_user                                                             │
│     if user.is_active:                                                                                                                                                                                           AttributeError: 'NoneType' object has no attribute 'is_active' 

### What you think should happen instead?

Airflow should be accessible and present the login form or a use full error to fix the user configuration - session reset,
after admin user recreation.


### How to reproduce

airflow users delete -u admin
airflow users create --username admin --password admin --role Admin --firstname admin --lastname admin --email admin@admin.com

### Operating System

Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.25.0
apache-airflow-providers-celery==3.7.2
apache-airflow-providers-cncf-kubernetes==8.3.3
apache-airflow-providers-common-io==1.3.2
apache-airflow-providers-common-sql==1.14.2
apache-airflow-providers-docker==3.12.2
apache-airflow-providers-elasticsearch==5.4.1
apache-airflow-providers-fab==1.2.2
apache-airflow-providers-ftp==3.10.0
apache-airflow-providers-google==10.21.0
apache-airflow-providers-grpc==3.5.2
apache-airflow-providers-hashicorp==3.7.1
apache-airflow-providers-http==4.12.0
apache-airflow-providers-imap==3.6.1
apache-airflow-providers-microsoft-azure==10.2.0
apache-airflow-providers-mysql==5.6.2
apache-airflow-providers-odbc==4.6.2
apache-airflow-providers-openlineage==1.9.1
apache-airflow-providers-postgres==5.11.2
apache-airflow-providers-redis==3.7.1
apache-airflow-providers-sendgrid==3.5.1
apache-airflow-providers-sftp==4.10.2
apache-airflow-providers-slack==8.7.1
apache-airflow-providers-smtp==1.7.1
apache-airflow-providers-snowflake==5.6.0
apache-airflow-providers-sqlite==3.8.1
apache-airflow-providers-ssh==3.11.2

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

  image:
    repository: apache/airflow
    tag: slim-2.9.3-python3.11

with argocd configuration and K8sexecutor

### Anything else?

unable to connect and reset the configuration

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",good92,2024-08-30 15:57:22+00:00,[],2024-09-24 18:39:48+00:00,2024-09-24 18:39:48+00:00,https://github.com/apache/airflow/issues/41901,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('area:webserver', 'Webserver related Issues'), ('pending-response', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2321754913, 'issue_id': 2497625602, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 30, 15, 57, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2321951525, 'issue_id': 2497625602, 'author': 'good92', 'body': ""workaround:\r\nairflow db reset\r\nairflow users create --username admin --password admin --role Admin --firstname admin --lastname admin --email [admin@admin.com](mailto:admin@admin.com)\r\n\r\nNotes:\r\nwebserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:62660 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:62715 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:63882 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:63946 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:64582 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:64674 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:64940 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:65014 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:65117 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:65199 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:65385 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:65441 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:65874 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:65950 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:66003 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:66087 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:66686 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:66779 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:66918 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:66964 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:67735 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:67791 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:68043 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:68097 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:68532 SyntaxWarning: invalid escape sequence '\\d'                                                     │\r\n│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:68588 SyntaxWarning: invalid escape sequence '\\d'"", 'created_at': datetime.datetime(2024, 8, 30, 16, 46, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2323045378, 'issue_id': 2497625602, 'author': 'jscheffl', 'body': 'I assume the ""right"" way to reset a user (admin or normal) password is:\r\n```\r\nroot@fb30c3a3f4ce:/opt/airflow# airflow users reset-password --help\r\nUsage: airflow users reset-password [-h] [-e EMAIL] [-p PASSWORD] [--use-random-password] [-u USERNAME] [-v]\r\n\r\nReset a user\'s password\r\n\r\nOptions:\r\n  -h, --help            show this help message and exit\r\n  -e, --email EMAIL     Email of the user\r\n  -p, --password PASSWORD\r\n                        Password of the user, required to create a user without --use-random-password\r\n  --use-random-password\r\n                        Do not prompt for password. Use random string instead. Required to create a user without --password\r\n  -u, --username USERNAME\r\n                        Username of the user\r\n  -v, --verbose         Make logging output more verbose\r\n\r\nexamples:\r\nTo reset an user with username equals to ""admin"", run:\r\n\r\n    $ airflow users reset-password \\\r\n          --username admin\r\n```', 'created_at': datetime.datetime(2024, 8, 31, 21, 17, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2323050148, 'issue_id': 2497625602, 'author': 'jscheffl', 'body': 'Note: I tested the steps below ""How to Reproduce"" and can not see a problem in Airflow 2.10.0 and latest main (both). As you reported the problem with 2.9.3... can you test on 2.10 if you really can re-produce?', 'created_at': datetime.datetime(2024, 8, 31, 21, 42, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2351239507, 'issue_id': 2497625602, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 9, 15, 0, 16, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2372030575, 'issue_id': 2497625602, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 9, 24, 18, 39, 47, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-30 15:57:25 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

good92 (Issue Creator) on (2024-08-30 16:46:33 UTC): workaround:
airflow db reset
airflow users create --username admin --password admin --role Admin --firstname admin --lastname admin --email [admin@admin.com](mailto:admin@admin.com)

Notes:
webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:62660 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:62715 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:63882 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:63946 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:64582 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:64674 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:64940 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:65014 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:65117 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:65199 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:65385 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:65441 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:65874 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:65950 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:66003 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:66087 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:66686 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:66779 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:66918 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:66964 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:67735 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:67791 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:68043 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:68097 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:68532 SyntaxWarning: invalid escape sequence '\d'                                                     │
│ webserver /home/airflow/.local/lib/python3.12/site-packages/azure/synapse/artifacts/models/_models_py3.py:68588 SyntaxWarning: invalid escape sequence '\d'

jscheffl on (2024-08-31 21:17:52 UTC): I assume the ""right"" way to reset a user (admin or normal) password is:
```
root@fb30c3a3f4ce:/opt/airflow# airflow users reset-password --help
Usage: airflow users reset-password [-h] [-e EMAIL] [-p PASSWORD] [--use-random-password] [-u USERNAME] [-v]

Reset a user's password

Options:
  -h, --help            show this help message and exit
  -e, --email EMAIL     Email of the user
  -p, --password PASSWORD
                        Password of the user, required to create a user without --use-random-password
  --use-random-password
                        Do not prompt for password. Use random string instead. Required to create a user without --password
  -u, --username USERNAME
                        Username of the user
  -v, --verbose         Make logging output more verbose

examples:
To reset an user with username equals to ""admin"", run:

    $ airflow users reset-password \
          --username admin
```

jscheffl on (2024-08-31 21:42:07 UTC): Note: I tested the steps below ""How to Reproduce"" and can not see a problem in Airflow 2.10.0 and latest main (both). As you reported the problem with 2.9.3... can you test on 2.10 if you really can re-produce?

github-actions[bot] on (2024-09-15 00:16:13 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-09-24 18:39:47 UTC): This issue has been closed because it has not received response from the issue author.

"
2497323409,issue,closed,completed,Rerunning a dag in Airflow 2.7.3 causes a missing table issue,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.7.3

### What happened?

My team is working on upgrading an old system to the latest stable version of Airflow.  We are currently stuck on the 2.7.3 upgrade.  The first time a test is run in 2.7.3 that calls airflow/modles/dag.py _get_or_create_dagrun, it passes.  All subsequent test runs fail with the following error:

`sqlalchemy.exc.NoReferencedTableError: Foreign key associated with column 'dag_run_note.user_id' could not find table 'ab_user' with which to generate a foreign key to target column 'id'`

### What you think should happen instead?

The error occurs when running a dag with the same dag id and execution date more than once.  This happens in our integration tests.  The `if dr:` block below is run and the error occurs when `session.commit()` is called.

```
def _get_or_create_dagrun(...) -> DagRun:
    dr: DagRun = session.scalar(
        select(DagRun).where(DagRun.dag_id == dag.dag_id, DagRun.execution_date == execution_date)
    )
    if dr:
        session.delete(dr)
        session.commit() # this line
    dr = ...
```

I believe that rerunning a dag id on an execution date should work or should raise an error based on overwriting a previous dag run.

### How to reproduce

In our system this is reproducible in test on any dag with a hard coded execution date.  Here is a sample setup.  I can fill out classes if it is helpful.

dags/my_dag.py
```
def my_dag(dag_config: MyConfig) -> DAG:
    with DAG(...) as dag:
        my_operator = MyOperator(...)

        other_operator = ...

        return dag
```

test/integration/test_my_dag.py
```
# test setup
# ...

DAG_RUN_DATE = datetime(
    year=2024,
    month=4,
    day=26,
    hour=3,
    minute=00,
    tzinfo=pendulum.timezone(""America/New_York"")

dag_config = MyConfig(
  dag_id=""my_test_dag"",
  ...
)

dag = my_dag(dag_config)
run_dag(dag, DAG_RUN_DATE)

# test assertions
# ...
```

test/helpers.py
```
from airflow.utils.session import provide_session
@provide_session
def run_dag(dag, date, session=None):
    dag.test(
        execution_date=date,
    )
```

### Operating System

CentOS 7

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.10.0
apache-airflow-providers-celery==3.4.1
apache-airflow-providers-cncf-kubernetes==7.8.0
apache-airflow-providers-common-sql==1.8.0
apache-airflow-providers-datadog==3.4.0
apache-airflow-providers-docker==3.8.0
apache-airflow-providers-ftp==3.6.0
apache-airflow-providers-hashicorp==3.5.0
apache-airflow-providers-http==4.6.0
apache-airflow-providers-imap==3.4.0
apache-airflow-providers-jenkins==3.4.0
apache-airflow-providers-microsoft-azure==8.1.0
apache-airflow-providers-opsgenie==5.2.0
apache-airflow-providers-postgres==5.7.1
apache-airflow-providers-redis==3.4.0
apache-airflow-providers-slack==8.3.0
apache-airflow-providers-sqlite==3.5.0
apache-airflow-providers-ssh==3.8.1

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

I have found a number of issues on this github page that target similar loading problems with the ab_user table.  None of them mention running a dag twice as part of the reproduction steps.
- https://github.com/apache/airflow/issues/34191
- https://github.com/apache/airflow/issues/34109
- https://github.com/apache/airflow/issues/34859

We wrote a patch to fix the issue.  This patch will get us through the Airflow 2.7.3 upgrade so we can continue upgrading.  We don't understand why there aren't other people with the same problem.
```
def patched_get_or_create_dagrun(...) -> DagRun:
    # CHANGES
    from airflow.auth.managers.fab.models import User
    from sqlalchemy import select
    # END OF CHANGES

    dr: DagRun = session.scalar(
        select(DagRun).where(DagRun.dag_id == dag.dag_id, DagRun.execution_date == execution_date)
    )
    if dr:
        session.delete(dr)
        session.commit() # this line
    ...

import airflow.models.dag as af_dag
af_dag._get_or_create_dagrun = patched_get_or_create_dagrun
```

Our integration tests generally upload data to an internal s3 analog and use the file path to partition the data based on the date.  Making this system dynamic would be a pretty big rewrite so we are looking for options.  Is there a standard practice for airflow integration testing with hard coded dates that we have missed?  Are we doing something out of the ordinary for airflow?

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jeremiahishere,2024-08-30 13:59:10+00:00,[],2024-09-03 17:21:08+00:00,2024-09-03 17:21:08+00:00,https://github.com/apache/airflow/issues/41894,"[('kind:bug', 'This is a clearly a bug'), ('pending-response', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2321351977, 'issue_id': 2497323409, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 30, 13, 59, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2323043882, 'issue_id': 2497323409, 'author': 'jscheffl', 'body': 'Thanks for reporting the bug. Whereas retrospective version 2.7 is pretty old and very probably we will not release patches.\r\n\r\nDo I understand it right that mainly you have problems in your integration tests but your production system is running fine? Or is it a regular (production) use case to run a DAG several times with the same execution date?\r\n\r\n> We wrote a patch to fix the issue. This patch will get us through the Airflow 2.7.3 upgrade so we can continue upgrading. We don\'t understand why there aren\'t other people with the same problem.\r\n\r\nI assume most users do not re-use the execution date. The execution date is defined ans unique as it was in the past part of the primary key. This had changed within the Ariflow 2 version stream and run_id became the primary key. In Airflow 3 we will remove the execution date as field as it was replaced with logical date. Also we have seen that a unique-ness of this field blocks some specific use cases in regards of data driven even processing.\r\n\r\nAnyway... question is: Shall we keep this bug report as a kind of documentation of ""Known problems""? Do you expect that something on the very old version 2.7 is patched?\r\n\r\nMy direct proposal would be to move-away from re-using the execution date in your tests. In the unit and integration tests we do with Airflow we always clean the DB before adding new test DAG records in order to prevent a clash. As re-using the execution date is not a supported use case I also would believe a patch would be earliest accepted in 2.10 line but I doubt a bit that this is a real bug/problem.', 'created_at': datetime.datetime(2024, 8, 31, 21, 10, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2326974576, 'issue_id': 2497323409, 'author': 'jeremiahishere', 'body': ""It is failing in test for us.  Production is not failing because the dag execution dates are based on the current time.  It started failing when upgrading from airflow 2.6 to 2.7.  I would love to be given the time to rewrite four years of test fixtures but I don't think that is going to happen.\r\n\r\nShould I expect this behavior in main or the v2-10-stable branch?  I see roughly the same code here: https://github.com/apache/airflow/blob/main/airflow/models/dag.py#L3786-L3791\r\n\r\nWhat is the use case where this code works as intended, deleting the previous dagrun and creating a new one?  Is there something special about the test environment that skips loading the ab_user table metadata?  Is there a different place to patch that will be easier to maintain as we upgrade across versions?"", 'created_at': datetime.datetime(2024, 9, 3, 16, 38, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2326995284, 'issue_id': 2497323409, 'author': 'jeremiahishere', 'body': 'Is there an official way to clean the database between tests?  I found this cleanup dag: https://github.com/apache/airflow/blob/v2-10-stable/airflow/utils/db_cleanup.py.  I am also comfortable playing around with copies of the .sqlite file backing the database.\r\n\r\nI would like to get closer to a standard use case for our testing environment.  If you could point me to documentation, I would appreciate it.', 'created_at': datetime.datetime(2024, 9, 3, 16, 48, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2327051114, 'issue_id': 2497323409, 'author': 'potiuk', 'body': ""I do not know too much about the root cause, but let me post some things that might help you to straighten things out:\r\n\r\n* 'airflow db reset` will reset the database. I believe if you create the DB from scratch it should work.\r\n*  It's also likely that the `ab_user` in your original database historically were created in a different schema \r\n* there were some issues releated to the index in 2.7.2 : https://github.com/apache/airflow/pull/34120 https://github.com/apache/airflow/pull/34656 - you can take a look there and see if you can figure out what's wrong in your database\r\n* this index is going to be dropped in Airflow 3 . You can likely drop it now as well."", 'created_at': datetime.datetime(2024, 9, 3, 17, 20, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2327052731, 'issue_id': 2497323409, 'author': 'potiuk', 'body': 'Also converting it into discussion. This is not ""airflow issue"" per se - I doubt we will be able to make a PR from it that will `fix` something - it\'s more of possibly troubleshooting what\'s wrong.', 'created_at': datetime.datetime(2024, 9, 3, 17, 21, 1, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-30 13:59:13 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jscheffl on (2024-08-31 21:10:04 UTC): Thanks for reporting the bug. Whereas retrospective version 2.7 is pretty old and very probably we will not release patches.

Do I understand it right that mainly you have problems in your integration tests but your production system is running fine? Or is it a regular (production) use case to run a DAG several times with the same execution date?


I assume most users do not re-use the execution date. The execution date is defined ans unique as it was in the past part of the primary key. This had changed within the Ariflow 2 version stream and run_id became the primary key. In Airflow 3 we will remove the execution date as field as it was replaced with logical date. Also we have seen that a unique-ness of this field blocks some specific use cases in regards of data driven even processing.

Anyway... question is: Shall we keep this bug report as a kind of documentation of ""Known problems""? Do you expect that something on the very old version 2.7 is patched?

My direct proposal would be to move-away from re-using the execution date in your tests. In the unit and integration tests we do with Airflow we always clean the DB before adding new test DAG records in order to prevent a clash. As re-using the execution date is not a supported use case I also would believe a patch would be earliest accepted in 2.10 line but I doubt a bit that this is a real bug/problem.

jeremiahishere (Issue Creator) on (2024-09-03 16:38:42 UTC): It is failing in test for us.  Production is not failing because the dag execution dates are based on the current time.  It started failing when upgrading from airflow 2.6 to 2.7.  I would love to be given the time to rewrite four years of test fixtures but I don't think that is going to happen.

Should I expect this behavior in main or the v2-10-stable branch?  I see roughly the same code here: https://github.com/apache/airflow/blob/main/airflow/models/dag.py#L3786-L3791

What is the use case where this code works as intended, deleting the previous dagrun and creating a new one?  Is there something special about the test environment that skips loading the ab_user table metadata?  Is there a different place to patch that will be easier to maintain as we upgrade across versions?

jeremiahishere (Issue Creator) on (2024-09-03 16:48:10 UTC): Is there an official way to clean the database between tests?  I found this cleanup dag: https://github.com/apache/airflow/blob/v2-10-stable/airflow/utils/db_cleanup.py.  I am also comfortable playing around with copies of the .sqlite file backing the database.

I would like to get closer to a standard use case for our testing environment.  If you could point me to documentation, I would appreciate it.

potiuk on (2024-09-03 17:20:01 UTC): I do not know too much about the root cause, but let me post some things that might help you to straighten things out:

* 'airflow db reset` will reset the database. I believe if you create the DB from scratch it should work.
*  It's also likely that the `ab_user` in your original database historically were created in a different schema 
* there were some issues releated to the index in 2.7.2 : https://github.com/apache/airflow/pull/34120 https://github.com/apache/airflow/pull/34656 - you can take a look there and see if you can figure out what's wrong in your database
* this index is going to be dropped in Airflow 3 . You can likely drop it now as well.

potiuk on (2024-09-03 17:21:01 UTC): Also converting it into discussion. This is not ""airflow issue"" per se - I doubt we will be able to make a PR from it that will `fix` something - it's more of possibly troubleshooting what's wrong.

"
2497231643,issue,closed,completed,'CeleryKubernetesExecutor' object has no attribute '_task_event_logs',"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When running tasks using the `CeleryKubernetesExecutor` on the latest Airflow version (2.10.0), a lot of these messages pop up in the Airflow Scheduler logs:

```
[2024-08-30T13:10:15.553+0000] {scheduler_job_runner.py:1141} ERROR - Something went wrong when trying to save task event logs.
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1139, in _run_scheduler_loop
    self._process_task_event_logs(executor._task_event_logs, session)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'CeleryKubernetesExecutor' object has no attribute '_task_event_logs'
```

### What you think should happen instead?

Prior to upgrading Airflow from 2.9.3 to 2.10.0, this was not an issue. I suspect that the introduction of Hybrid Executors, combined with CeleryKubernetesExecutor becoming somewhat redundant, is causing this issue. 

I understand that with hybrid executors, this executor is bound to be deprecated at some point, but this could be considered a violation of backwards compatibility for those who are still using it. 

### How to reproduce

1. Deploy Airflow 2.10.0 with CeleryKubernetesExecutor
2. Run a task and observe the scheduler logs
3. This error should pop up when the scheduler tries to save task event logs

### Operating System

Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.27.0
apache-airflow-providers-atlassian-jira==2.7.0
apache-airflow-providers-celery==3.8.1
apache-airflow-providers-cncf-kubernetes==8.3.4
apache-airflow-providers-common-compat==1.1.0
apache-airflow-providers-common-io==1.4.0
apache-airflow-providers-common-sql==1.15.0
apache-airflow-providers-docker==3.12.3
apache-airflow-providers-elasticsearch==5.4.2
apache-airflow-providers-fab==1.2.2
apache-airflow-providers-ftp==3.10.1
apache-airflow-providers-google==10.21.1
apache-airflow-providers-grpc==3.5.2
apache-airflow-providers-hashicorp==3.7.1
apache-airflow-providers-http==4.12.0
apache-airflow-providers-imap==3.6.1
apache-airflow-providers-jenkins==3.7.0
apache-airflow-providers-microsoft-azure==10.3.0
apache-airflow-providers-mysql==5.6.3
apache-airflow-providers-odbc==4.6.3
apache-airflow-providers-openlineage==1.10.0
apache-airflow-providers-postgres==5.11.3
apache-airflow-providers-redis==3.7.1
apache-airflow-providers-salesforce==5.8.0
apache-airflow-providers-sendgrid==3.5.1
apache-airflow-providers-sftp==4.10.3
apache-airflow-providers-slack==8.8.0
apache-airflow-providers-smtp==1.7.1
apache-airflow-providers-snowflake==5.6.1
apache-airflow-providers-sqlite==3.8.2
apache-airflow-providers-ssh==3.12.0
apache-airflow-providers-tableau==4.6.0

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Kubernetes deployment on AWS EKS using Airflow Helm Chart 1.15.0 with CeleryKubernetesExecutor

### Anything else?

This happens pretty much every time a task runs as long as you are using the CeleryKubernetesExecutor

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",hadarsharon,2024-08-30 13:22:16+00:00,[],2024-09-03 08:25:30+00:00,2024-09-01 12:35:48+00:00,https://github.com/apache/airflow/issues/41891,"[('kind:bug', 'This is a clearly a bug'), ('area:core', '')]","[{'comment_id': 2321243708, 'issue_id': 2497231643, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 30, 13, 22, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2321409008, 'issue_id': 2497231643, 'author': 'potiuk', 'body': 'FYI: @o-nikolas - looks similar to https://github.com/apache/airflow/issues/41525', 'created_at': datetime.datetime(2024, 8, 30, 14, 17, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2321749536, 'issue_id': 2497231643, 'author': 'o-nikolas', 'body': ""@potiuk This one is not related to the new hybrid/multiple executor config changes. It's actually related to the recent changes to remove the task context logger in favour of the log table #40867 (CC @dstandish)\r\n\r\nWhat _is_ similar is that it's another issue with these old HybridExecutors not using the base executor interface directly so every time you modify it you must update these by hand, which just got missed in the context logger work. Let's deprecate these thigns as soon as we feel comfortable doing."", 'created_at': datetime.datetime(2024, 8, 30, 15, 55, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2321845677, 'issue_id': 2497231643, 'author': 'potiuk', 'body': ""I see. I guess a discussion and proposal for that should happen - do you want to open it ? I guess we have some arguments and evidences already that we can use. We could also potentially add some tests (but not sure how) to see if there are some breaking changes introduced.\n\nI am not even sure if that change (@dstandish ?) is generally compatible with previous versions of executors (on the phone now)  - I see that there were some 'potentially internal' changes but they really impacted the executor  interface and the way it is used - strange that some other executors were modified in this change in some providers but the 'old hybrid's weren't so we likely have a bit complex incompatibility scenarios here."", 'created_at': datetime.datetime(2024, 8, 30, 16, 21, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2321948408, 'issue_id': 2497231643, 'author': 'o-nikolas', 'body': ""> I guess a discussion and proposal for that should happen - do you want to open it ? I guess we have some arguments and evidences already that we can use\r\n\r\nI'm happy to open it, but I was hoping to keep the new feature experimental until at least 2.11. So that folks using the new multiple executor configuration can find bugs and we can fix them to ensure it's working well and is stable before removing experimental. We're kind of stuck in the middle as of now. Do you think we should do it sooner and stabilize the new multiple executor config as we go?\r\n\r\n> I am not even sure if that change (@dstandish ?) is generally compatible with previous versions of executors (on the phone now) - I see that there were some 'potentially internal' changes but they really impacted the executor interface and the way it is used\r\n\r\nThe compat is a good point I'll leave @dstandish to comment. Executors from older versions of provider will definitely not have the new `_task_event_logs` property so when run with the new Airflow you'll see the above exception. Nothing will fail critically since the attempt to read that property is within a try/catch, but it will be incredibly spammy as it will try that over and over and log an exception each time it looks like.\r\n\r\n > strange that some other executors were modified in this change in some providers but the 'old hybrid's weren't so we likely have a bit complex incompatibility scenarios here.\r\n \r\n The executors that were modified were the ones that were using the old task context logger, they were updated to use this new logger. Not all executors used the logger which is why only some were updated."", 'created_at': datetime.datetime(2024, 8, 30, 16, 45, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2322030348, 'issue_id': 2497231643, 'author': 'dstandish', 'body': 'I think the problem is that CKE does not inherit from BaseExecutor, but that code assumes that it does.', 'created_at': datetime.datetime(2024, 8, 30, 17, 29, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2322032855, 'issue_id': 2497231643, 'author': 'dstandish', 'body': ""> Executors from older versions of provider will definitely not have the new _task_event_logs property\r\n\r\nThey will if they inherit from BaseExecutor, which they all should.  The fact that we provide executors that don't, is I think a mistake.  But anyway, this was simply a miss.  The solution would be to make CKE inherit from base executor."", 'created_at': datetime.datetime(2024, 8, 30, 17, 31, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2322050187, 'issue_id': 2497231643, 'author': 'dstandish', 'body': 'I will PR the inheritance fix', 'created_at': datetime.datetime(2024, 8, 30, 17, 44, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2322074424, 'issue_id': 2497231643, 'author': 'o-nikolas', 'body': ""Yeah, the hybrid executors never have inherited from the base executor, since what would it mean for the wrapper class to have `_task_event_logs` property for example? What you really care about is that property of both of the contained executors/classes combined. So the old hybrids often have implementations of the base exec functions that combine the values from the two contained classes.\r\nAny who, the whole thing is a mess and we should deprecate them any way, so I'd be happy with any level of fix for this :+1:"", 'created_at': datetime.datetime(2024, 8, 30, 18, 0, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2322084734, 'issue_id': 2497231643, 'author': 'dstandish', 'body': ""yeah `_task_event_logs` is just a deque that is there so that the executor can pass messages for the scheduler to consume.  it's there because the executor does not have access to the `session` object (somewhat surprisingly!) so it cannot write log events.  so when the executor notices something that has to be logged, it dumps to the queue and the scheduler consumes the queue as part of executor processing.  so it's not really a problem for hybrid executors.  but anyway, i am making progress on making them inherit.  nice thing about doing this is this can be fixed for customers with a provider release. but we could also look at patching the scheduler in 2.10.x 🤷"", 'created_at': datetime.datetime(2024, 8, 30, 18, 7, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2322092430, 'issue_id': 2497231643, 'author': 'dstandish', 'body': ""Alright, let's see how https://github.com/apache/airflow/pull/41904 does in CI"", 'created_at': datetime.datetime(2024, 8, 30, 18, 13, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2322096935, 'issue_id': 2497231643, 'author': 'dstandish', 'body': ""> so it's not really a problem for hybrid executors\r\n\r\nwell i guess it kindof is kindof is a problem since each of the executors would write to the queue on their individual instances, oy, yeah let's 🪓 these hybrid executors ... sigh..."", 'created_at': datetime.datetime(2024, 8, 30, 18, 16, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2322216440, 'issue_id': 2497231643, 'author': 'o-nikolas', 'body': ""> well i guess it kindof is kindof is a problem since each of the executors would write to the queue on their individual instances, oy, yeah let's 🪓 these hybrid executors ... sigh...\r\n\r\nYeah, this is what I was trying to get at, with these old hybrids, you're constantly trying to let the individual executors do their individual thing, then you have to merge the results, state, logs, or whatever you're working with in the parent hybrid exec. It's awful"", 'created_at': datetime.datetime(2024, 8, 30, 19, 35, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325725859, 'issue_id': 2497231643, 'author': 'potiuk', 'body': ""> Yeah, this is what I was trying to get at, with these old hybrids, you're constantly trying to let the individual executors do their individual thing, then you have to merge the results, state, logs, or whatever you're working with in the parent hybrid exec. It's awful\r\n\r\nIt is :). Unfortunately - as it will all our things we release, once it's out it will take quite some time to 🪓 it..."", 'created_at': datetime.datetime(2024, 9, 3, 6, 46, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325901918, 'issue_id': 2497231643, 'author': 'hadarsharon', 'body': 'I just checked and can confirm that this issue is resolved in [v2.10.1.rc1](https://pypi.python.org/pypi/apache-airflow/2.10.1.rc1)\r\n\r\nCheers', 'created_at': datetime.datetime(2024, 9, 3, 8, 25, 28, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-30 13:22:19 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-08-30 14:17:35 UTC): FYI: @o-nikolas - looks similar to https://github.com/apache/airflow/issues/41525

o-nikolas on (2024-08-30 15:55:48 UTC): @potiuk This one is not related to the new hybrid/multiple executor config changes. It's actually related to the recent changes to remove the task context logger in favour of the log table #40867 (CC @dstandish)

What _is_ similar is that it's another issue with these old HybridExecutors not using the base executor interface directly so every time you modify it you must update these by hand, which just got missed in the context logger work. Let's deprecate these thigns as soon as we feel comfortable doing.

potiuk on (2024-08-30 16:21:43 UTC): I see. I guess a discussion and proposal for that should happen - do you want to open it ? I guess we have some arguments and evidences already that we can use. We could also potentially add some tests (but not sure how) to see if there are some breaking changes introduced.

I am not even sure if that change (@dstandish ?) is generally compatible with previous versions of executors (on the phone now)  - I see that there were some 'potentially internal' changes but they really impacted the executor  interface and the way it is used - strange that some other executors were modified in this change in some providers but the 'old hybrid's weren't so we likely have a bit complex incompatibility scenarios here.

o-nikolas on (2024-08-30 16:45:18 UTC): I'm happy to open it, but I was hoping to keep the new feature experimental until at least 2.11. So that folks using the new multiple executor configuration can find bugs and we can fix them to ensure it's working well and is stable before removing experimental. We're kind of stuck in the middle as of now. Do you think we should do it sooner and stabilize the new multiple executor config as we go?


The compat is a good point I'll leave @dstandish to comment. Executors from older versions of provider will definitely not have the new `_task_event_logs` property so when run with the new Airflow you'll see the above exception. Nothing will fail critically since the attempt to read that property is within a try/catch, but it will be incredibly spammy as it will try that over and over and log an exception each time it looks like.

 
 The executors that were modified were the ones that were using the old task context logger, they were updated to use this new logger. Not all executors used the logger which is why only some were updated.

dstandish on (2024-08-30 17:29:52 UTC): I think the problem is that CKE does not inherit from BaseExecutor, but that code assumes that it does.

dstandish on (2024-08-30 17:31:43 UTC): They will if they inherit from BaseExecutor, which they all should.  The fact that we provide executors that don't, is I think a mistake.  But anyway, this was simply a miss.  The solution would be to make CKE inherit from base executor.

dstandish on (2024-08-30 17:44:05 UTC): I will PR the inheritance fix

o-nikolas on (2024-08-30 18:00:57 UTC): Yeah, the hybrid executors never have inherited from the base executor, since what would it mean for the wrapper class to have `_task_event_logs` property for example? What you really care about is that property of both of the contained executors/classes combined. So the old hybrids often have implementations of the base exec functions that combine the values from the two contained classes.
Any who, the whole thing is a mess and we should deprecate them any way, so I'd be happy with any level of fix for this :+1:

dstandish on (2024-08-30 18:07:57 UTC): yeah `_task_event_logs` is just a deque that is there so that the executor can pass messages for the scheduler to consume.  it's there because the executor does not have access to the `session` object (somewhat surprisingly!) so it cannot write log events.  so when the executor notices something that has to be logged, it dumps to the queue and the scheduler consumes the queue as part of executor processing.  so it's not really a problem for hybrid executors.  but anyway, i am making progress on making them inherit.  nice thing about doing this is this can be fixed for customers with a provider release. but we could also look at patching the scheduler in 2.10.x 🤷

dstandish on (2024-08-30 18:13:16 UTC): Alright, let's see how https://github.com/apache/airflow/pull/41904 does in CI

dstandish on (2024-08-30 18:16:08 UTC): well i guess it kindof is kindof is a problem since each of the executors would write to the queue on their individual instances, oy, yeah let's 🪓 these hybrid executors ... sigh...

o-nikolas on (2024-08-30 19:35:25 UTC): Yeah, this is what I was trying to get at, with these old hybrids, you're constantly trying to let the individual executors do their individual thing, then you have to merge the results, state, logs, or whatever you're working with in the parent hybrid exec. It's awful

potiuk on (2024-09-03 06:46:47 UTC): It is :). Unfortunately - as it will all our things we release, once it's out it will take quite some time to 🪓 it...

hadarsharon (Issue Creator) on (2024-09-03 08:25:28 UTC): I just checked and can confirm that this issue is resolved in [v2.10.1.rc1](https://pypi.python.org/pypi/apache-airflow/2.10.1.rc1)

Cheers

"
2497036399,issue,closed,completed,How to use .md file in doc_md for DAG documentation,"### What do you see as an issue?

![image](https://github.com/user-attachments/assets/0e763c1c-4f3f-402e-bab1-eb2c26b40315)

### Solving the problem

Please add the steps to showcase how the .md file can be used for dag documentation under doc_md. 

### Anything else

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",a-kumar5,2024-08-30 12:00:48+00:00,[],2024-09-05 04:56:22+00:00,2024-09-05 04:56:22+00:00,https://github.com/apache/airflow/issues/41886,"[('kind:bug', 'This is a clearly a bug'), ('kind:documentation', ''), ('pending-response', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2321009018, 'issue_id': 2497036399, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 30, 12, 0, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2321127557, 'issue_id': 2497036399, 'author': 'jscheffl', 'body': 'Like here? https://github.com/apache/airflow/blob/main/airflow/example_dags/example_params_ui_tutorial.py#L38', 'created_at': datetime.datetime(2024, 8, 30, 12, 43, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2321134451, 'issue_id': 2497036399, 'author': 'jscheffl', 'body': 'Else have you seen/taken a look at https://github.com/apache/airflow/blob/main/airflow/models/dag.py#L706 ?', 'created_at': datetime.datetime(2024, 8, 30, 12, 45, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2327909116, 'issue_id': 2497036399, 'author': 'a-kumar5', 'body': '@jscheffl  hers is my dag code.\r\n![image](https://github.com/user-attachments/assets/f522f860-85f7-4efe-a920-b84886d0919e)\r\n\r\nI’m currently facing an issue where a templating error is being thrown, stating:\r\n\r\n""Templating Error! Not able to find the template file: /opt/airflow/dags/TESTING_1/docs/00AKOSB3-FLSBH_3.md.""\r\n\r\nHowever, I have verified by exec-ing into the pod that the file does indeed exist at the specified location.\r\n\r\nCould you please provide guidance on what might be causing this error or suggest any steps I could take to troubleshoot this further?\r\n\r\n![image](https://github.com/user-attachments/assets/58a347f9-68a4-418d-9daa-b751c815b242)', 'created_at': datetime.datetime(2024, 9, 4, 4, 43, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2327909609, 'issue_id': 2497036399, 'author': 'a-kumar5', 'body': '![image](https://github.com/user-attachments/assets/d407d7ab-ce31-4af7-bb9e-f281a4b5297c)', 'created_at': datetime.datetime(2024, 9, 4, 4, 43, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2328432255, 'issue_id': 2497036399, 'author': 'a-kumar5', 'body': '@jscheffl  Additionally, I was able to print the contents of the .md file from within the task using the same path, which suggests that the file is accessible. \r\n\r\n![image](https://github.com/user-attachments/assets/5d9a9549-9d4c-44b5-b019-190eb5254771)\r\n\r\n![image](https://github.com/user-attachments/assets/0b248f5e-4fc5-4e2a-bfc9-e71035721e4d)', 'created_at': datetime.datetime(2024, 9, 4, 10, 0, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2330256110, 'issue_id': 2497036399, 'author': 'jscheffl', 'body': 'I checked with latest main which seems not to have changed since Airflow 2.9.3 and I can tell that the function works. Just with the side note that if a relative path is given then the file is _not_ loaded from DAGs folder but from working directory in which the Airflow Scheduler component was started.\r\n\r\nNote that the MD file content is parsed by the scheduler. This means if you save the file, it not immeditately displayed but only after some time when the Scheduler component did a re-parse of the DAG file.\r\n\r\nAs the screenshots you have posted are a bit ""older"" - can you reveal which Airflow-Version you are running? The feature was introduced in Airflow 2.4', 'created_at': datetime.datetime(2024, 9, 4, 22, 23, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2330527377, 'issue_id': 2497036399, 'author': 'a-kumar5', 'body': '@jscheffl \r\nCurrently running 2.8.3v\r\n\r\n![image](https://github.com/user-attachments/assets/1b403d05-8471-40c7-a094-37f94463ec9d)', 'created_at': datetime.datetime(2024, 9, 5, 3, 32, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2330533179, 'issue_id': 2497036399, 'author': 'a-kumar5', 'body': ""@jscheffl I provided the absolute path `/opt/airflow/dags/TESTING_1/docs/00AKOSB3-FLSBH_3.md`. This approach works perfectly in my local environment, where I installed Airflow using Docker. However, in the EKS cluster, it's throwing a `template not found` error.\r\n\r\nIn my local environment, where I'm using Docker, Airflow is running on version `2.9.3`, while in the EKS cluster, it's running on version `2.8.3`.\r\n\r\nI last edited the DAG code 24 hours ago, yet it’s still throwing a `template not found` error. Has the scheduler not parsed it again in the last 24 hours, or am I missing something?"", 'created_at': datetime.datetime(2024, 9, 5, 3, 40, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2330595950, 'issue_id': 2497036399, 'author': 'jscheffl', 'body': 'Okay, you hit a special situation as the logic was adjusted between 2.8.3 and 2.9.3 because of a security hardening. Templating support was removed in 2.9.3.\r\n\r\nSo I propose that you test with the same version as well I propose to upgrade to the higher if possible. THen you have the same behavior.\r\n\r\n24h should be sufficient but be aware that the the context path of the scheduler in EKS will be probably different than in your local setup.\r\nAs I found a bit of inconsistency with the changes in 2.9.3 and the docs, please see that I adjusted the docs. Please do not resolve the MD file path relative to the DAG file path but relative to the path in which the scheduler was started via `os.getcwd()`.', 'created_at': datetime.datetime(2024, 9, 5, 4, 55, 57, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-30 12:00:52 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jscheffl on (2024-08-30 12:43:25 UTC): Like here? https://github.com/apache/airflow/blob/main/airflow/example_dags/example_params_ui_tutorial.py#L38

jscheffl on (2024-08-30 12:45:56 UTC): Else have you seen/taken a look at https://github.com/apache/airflow/blob/main/airflow/models/dag.py#L706 ?

a-kumar5 (Issue Creator) on (2024-09-04 04:43:23 UTC): @jscheffl  hers is my dag code.
![image](https://github.com/user-attachments/assets/f522f860-85f7-4efe-a920-b84886d0919e)

I’m currently facing an issue where a templating error is being thrown, stating:

""Templating Error! Not able to find the template file: /opt/airflow/dags/TESTING_1/docs/00AKOSB3-FLSBH_3.md.""

However, I have verified by exec-ing into the pod that the file does indeed exist at the specified location.

Could you please provide guidance on what might be causing this error or suggest any steps I could take to troubleshoot this further?

![image](https://github.com/user-attachments/assets/58a347f9-68a4-418d-9daa-b751c815b242)

a-kumar5 (Issue Creator) on (2024-09-04 04:43:58 UTC): ![image](https://github.com/user-attachments/assets/d407d7ab-ce31-4af7-bb9e-f281a4b5297c)

a-kumar5 (Issue Creator) on (2024-09-04 10:00:37 UTC): @jscheffl  Additionally, I was able to print the contents of the .md file from within the task using the same path, which suggests that the file is accessible. 

![image](https://github.com/user-attachments/assets/5d9a9549-9d4c-44b5-b019-190eb5254771)

![image](https://github.com/user-attachments/assets/0b248f5e-4fc5-4e2a-bfc9-e71035721e4d)

jscheffl on (2024-09-04 22:23:23 UTC): I checked with latest main which seems not to have changed since Airflow 2.9.3 and I can tell that the function works. Just with the side note that if a relative path is given then the file is _not_ loaded from DAGs folder but from working directory in which the Airflow Scheduler component was started.

Note that the MD file content is parsed by the scheduler. This means if you save the file, it not immeditately displayed but only after some time when the Scheduler component did a re-parse of the DAG file.

As the screenshots you have posted are a bit ""older"" - can you reveal which Airflow-Version you are running? The feature was introduced in Airflow 2.4

a-kumar5 (Issue Creator) on (2024-09-05 03:32:41 UTC): @jscheffl 
Currently running 2.8.3v

![image](https://github.com/user-attachments/assets/1b403d05-8471-40c7-a094-37f94463ec9d)

a-kumar5 (Issue Creator) on (2024-09-05 03:40:35 UTC): @jscheffl I provided the absolute path `/opt/airflow/dags/TESTING_1/docs/00AKOSB3-FLSBH_3.md`. This approach works perfectly in my local environment, where I installed Airflow using Docker. However, in the EKS cluster, it's throwing a `template not found` error.

In my local environment, where I'm using Docker, Airflow is running on version `2.9.3`, while in the EKS cluster, it's running on version `2.8.3`.

I last edited the DAG code 24 hours ago, yet it’s still throwing a `template not found` error. Has the scheduler not parsed it again in the last 24 hours, or am I missing something?

jscheffl on (2024-09-05 04:55:57 UTC): Okay, you hit a special situation as the logic was adjusted between 2.8.3 and 2.9.3 because of a security hardening. Templating support was removed in 2.9.3.

So I propose that you test with the same version as well I propose to upgrade to the higher if possible. THen you have the same behavior.

24h should be sufficient but be aware that the the context path of the scheduler in EKS will be probably different than in your local setup.
As I found a bit of inconsistency with the changes in 2.9.3 and the docs, please see that I adjusted the docs. Please do not resolve the MD file path relative to the DAG file path but relative to the path in which the scheduler was started via `os.getcwd()`.

"
2496908695,issue,open,,Delay in marking a tasks state (success/upstream_failed and failed),"### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

We have seen this issue several times. 

1. A task failed
Up to 5 minutes go by (this is the longest we have seen the wait)
2. The task itself is marked as `FAILED`
3. All downstream tasks are marked as `upstream_failed`

It is important to note, we also see this behaviour for a task succeeding (not being reflected in Airflow UI or its metadata DB).

We have validated this by also making a call to Airflow's API to retrieve the task instance & the state has not been reflected as we would expect. 

This exact case happened today (30th Aug) with a 2 minute delay:
1. task_A failed today at 7:22 BST - 
<img width=""1673"" alt=""Screenshot 2024-08-30 at 09 29 15"" src=""https://github.com/user-attachments/assets/8af892b7-13c1-40ad-b1cf-972a7c4c8841"">
2. One of its downstreams is in a None state at 7:23:00am BST
<img width=""1051"" alt=""Screenshot 2024-08-30 at 09 24 33"" src=""https://github.com/user-attachments/assets/84ab7cd9-b0e0-4e10-a6b5-57a60959b89d"">
3. Then the downstream is set to a upstream failed state at 7:25am BST 
<img width=""1658"" alt=""Screenshot 2024-08-30 at 09 31 18"" src=""https://github.com/user-attachments/assets/7dde2d5b-1374-4753-a3bd-145c9bafcda0"">


### What you think should happen instead?

1. A task failed
<Little to no wait>
2. The task itself is marked as `FAILED`
3. All downstream tasks are marked as `upstream_failed`

We do not expect any delay in the task being marked with its appropriate state nor the marking of any downstreams.

### How to reproduce

This is hard to reproduce as unfortunately the metadata db (task instance table) only ever stores the latest state of a task (to minimize production downtime we are immediately retrying failed tasks and then subsequently will succeed and we dont get the first state stored). Possibly cold look into insertion timestamps and task completion timestamp and look at the delay here.

### Operating System

linux/arm64

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other Docker-based deployment

### Deployment details

We use this docker image: apache/airflow:2.9.3-python3.9

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",amyshields,2024-08-30 10:56:06+00:00,[],2025-01-10 09:47:40+00:00,,https://github.com/apache/airflow/issues/41884,"[('kind:bug', 'This is a clearly a bug'), (""Can't Reproduce"", 'The problem cannot be reproduced'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2320842203, 'issue_id': 2496908695, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 30, 10, 56, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2321035142, 'issue_id': 2496908695, 'author': 'raphaelauv', 'body': 'retry_delay under 30 seconds is risky since airflow with a distributed/remote/edge executor like CeleryExecutor is eventually consistent', 'created_at': datetime.datetime(2024, 8, 30, 12, 10, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2321141544, 'issue_id': 2496908695, 'author': 'amyshields', 'body': '> retry_delay under 30 seconds is risky since airflow with a distributed/remote/edge executor like CeleryExecutor is eventually consistent\r\n\r\nSorry I am not sure I understand what you mean here (@raphaelauv), what is retry_delay? Is this something we control? How is it used?', 'created_at': datetime.datetime(2024, 8, 30, 12, 48, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2321143965, 'issue_id': 2496908695, 'author': 'jscheffl', 'body': 'I could imagine a delay is happening because you have some infrastructure flakiness. 5 Minutes ""smell"" a bit like the typical heartbeat timeout that a task sends to the DB that it is still alive.\r\n\r\nCan you check the logs/stdout of the worker where the task is executing? Might be some errors are printed to stdout which are not getting picked-up by the log facility.\r\n\r\nThe case you describe ""should not happen"" but I doubt it is a systematic problem, rather a infrastructure problem.', 'created_at': datetime.datetime(2024, 8, 30, 12, 49, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2321157491, 'issue_id': 2496908695, 'author': 'raphaelauv', 'body': '@amyshields \r\nyou said `we are immediately retrying failed tasks `\r\n\r\nthe default retry_delay https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#default-task-retry-delay\r\nis 300 seconds', 'created_at': datetime.datetime(2024, 8, 30, 12, 53, 29, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-30 10:56:09 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

raphaelauv on (2024-08-30 12:10:46 UTC): retry_delay under 30 seconds is risky since airflow with a distributed/remote/edge executor like CeleryExecutor is eventually consistent

amyshields (Issue Creator) on (2024-08-30 12:48:31 UTC): Sorry I am not sure I understand what you mean here (@raphaelauv), what is retry_delay? Is this something we control? How is it used?

jscheffl on (2024-08-30 12:49:42 UTC): I could imagine a delay is happening because you have some infrastructure flakiness. 5 Minutes ""smell"" a bit like the typical heartbeat timeout that a task sends to the DB that it is still alive.

Can you check the logs/stdout of the worker where the task is executing? Might be some errors are printed to stdout which are not getting picked-up by the log facility.

The case you describe ""should not happen"" but I doubt it is a systematic problem, rather a infrastructure problem.

raphaelauv on (2024-08-30 12:53:29 UTC): @amyshields 
you said `we are immediately retrying failed tasks `

the default retry_delay https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#default-task-retry-delay
is 300 seconds

"
2496458512,issue,open,,JdbcHook doesn't support OpenLineage,"### Apache Airflow Provider(s)

common-sql

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.24.0
apache-airflow-providers-celery==3.7.2
apache-airflow-providers-cncf-kubernetes==8.3.1
apache-airflow-providers-common-compat==1.2.0
apache-airflow-providers-common-io==1.3.2
apache-airflow-providers-common-sql==1.14.0
apache-airflow-providers-docker==3.12.0
apache-airflow-providers-elasticsearch==5.4.1
apache-airflow-providers-fab==1.1.1
apache-airflow-providers-ftp==3.9.1
apache-airflow-providers-google==10.19.0
apache-airflow-providers-grpc==3.5.1
apache-airflow-providers-hashicorp==3.7.1
apache-airflow-providers-http==4.11.1
apache-airflow-providers-imap==3.6.1
apache-airflow-providers-jdbc==4.3.1
apache-airflow-providers-microsoft-azure==10.1.1
apache-airflow-providers-microsoft-mssql==3.7.1
apache-airflow-providers-microsoft-winrm==3.5.1
apache-airflow-providers-mongo==4.1.1
apache-airflow-providers-mysql==5.6.1
apache-airflow-providers-odbc==4.6.1
apache-airflow-providers-openlineage==1.11.0
apache-airflow-providers-postgres==5.11.1
apache-airflow-providers-redis==3.7.1
apache-airflow-providers-samba==4.7.1
apache-airflow-providers-sendgrid==3.5.1
apache-airflow-providers-sftp==4.10.1
apache-airflow-providers-slack==8.7.1
apache-airflow-providers-smtp==1.7.1
apache-airflow-providers-snowflake==5.5.1
apache-airflow-providers-sqlite==3.8.1
apache-airflow-providers-ssh==3.11.1

### Apache Airflow version

2.9.2

### Operating System

Debian GNU/Linux 12 (bookworm)

### Deployment

Docker-Compose

### Deployment details

_No response_

### What happened

There is no support of OpenLineage in JdbcHook. For example to extract metadata and create table and columns in openlineage.
We find this https://github.com/OpenLineage/OpenLineage/blob/main/integration/airflow/openlineage/airflow/extractors/sql_extractor.py but unfortunatly we can't used it

### What you think should happen instead

_No response_

### How to reproduce

Always the case

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",hadanmarv,2024-08-30 07:28:47+00:00,"['JDarDagran', 'dangerdude237']",2024-09-06 12:34:57+00:00,,https://github.com/apache/airflow/issues/41878,"[('area:providers', ''), ('good first issue', ''), ('provider:openlineage', 'AIP-53')]","[{'comment_id': 2320335324, 'issue_id': 2496458512, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 30, 7, 28, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2321330254, 'issue_id': 2496458512, 'author': 'potiuk', 'body': 'Sure - marked it as good first issue for someone to make it support it. You could do it if you want it faster, other than that someone will have to pick it and implement it (but ideally things like that are implemented by those who need them).', 'created_at': datetime.datetime(2024, 8, 30, 13, 51, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2332561846, 'issue_id': 2496458512, 'author': 'dangerdude237', 'body': 'Hi @potiuk, I would like to handle this, could you give me some pointers on this issue?', 'created_at': datetime.datetime(2024, 9, 5, 20, 19, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333270472, 'issue_id': 2496458512, 'author': 'potiuk', 'body': ""I'd say @mobuchowski and @kacpermuda are the best to help"", 'created_at': datetime.datetime(2024, 9, 6, 5, 38, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333857552, 'issue_id': 2496458512, 'author': 'mobuchowski', 'body': 'Actually @JDarDagran works on that now 🙂', 'created_at': datetime.datetime(2024, 9, 6, 11, 32, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333880161, 'issue_id': 2496458512, 'author': 'potiuk', 'body': '😱', 'created_at': datetime.datetime(2024, 9, 6, 11, 47, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333880791, 'issue_id': 2496458512, 'author': 'potiuk', 'body': 'So maybe @JDarDagran  comment here and we will assign it to you :D', 'created_at': datetime.datetime(2024, 9, 6, 11, 47, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333881085, 'issue_id': 2496458512, 'author': 'potiuk', 'body': ""(we can't do it without you commenting)"", 'created_at': datetime.datetime(2024, 9, 6, 11, 47, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333882405, 'issue_id': 2496458512, 'author': 'JDarDagran', 'body': '**tactical dot**', 'created_at': datetime.datetime(2024, 9, 6, 11, 48, 41, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-30 07:28:50 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-08-30 13:51:29 UTC): Sure - marked it as good first issue for someone to make it support it. You could do it if you want it faster, other than that someone will have to pick it and implement it (but ideally things like that are implemented by those who need them).

dangerdude237 (Assginee) on (2024-09-05 20:19:37 UTC): Hi @potiuk, I would like to handle this, could you give me some pointers on this issue?

potiuk on (2024-09-06 05:38:59 UTC): I'd say @mobuchowski and @kacpermuda are the best to help

mobuchowski on (2024-09-06 11:32:19 UTC): Actually @JDarDagran works on that now 🙂

potiuk on (2024-09-06 11:47:16 UTC): 😱

potiuk on (2024-09-06 11:47:40 UTC): So maybe @JDarDagran  comment here and we will assign it to you :D

potiuk on (2024-09-06 11:47:51 UTC): (we can't do it without you commenting)

JDarDagran (Assginee) on (2024-09-06 11:48:41 UTC): **tactical dot**

"
2496447585,issue,closed,completed,Inconsistent XCom message format when using PubSubPullSensor with deferrable=True,"### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

_No response_

### Apache Airflow version

2.9.1

### Operating System

Google Cloud Composer 2 (managed Airflow environment on Google Cloud)

### Deployment

Google Cloud Composer

### Deployment details

_No response_

### What happened

I'm using the `PubSubPullSensor` in Apache Airflow with `deferrable=True` on Google Cloud Composer 2. When the sensor is set to `deferrable=True`, the messages stored in XCom have a different format compared to when `deferrable=False`.

**When `deferrable=True`:**  
The messages are stored in XCom as strings that resemble serialized protobuf format:
```
""[ack_id: 'RFAGFixdRkhRNxkIaFEOT14jPzUgKEUSCQdPAihdeTFTLUFdfWhRDRlyfWB8a1MbBgNGBysOURsHaE5tdR_L0ZL0S0NUa1gSBwVCVnldUhwPbF1ZdQN58b3b8qzgnn8JOjrfj_XZbTuLvKsbZiM9XhJLLD5-LzlFQV5AEkwkDERJUytDCypYEU4EISE-MD5FUw' message { data: 'hello from cloud console!' message_id: '12167757297241504' publish_time { seconds: 1724839443 nanos: 948000000 } } ]""
```

**When `deferrable=False`:**  
The messages are stored in XCom as standard Python dictionaries:
```
[{'ack_id': 'UAYWLF1GSFE3GQhoUQ5PXiM_NSAoRRIJB08CKF15MU0sQVhwaFENGXJ9YHxrUxsDV0ECel1RGQdoTm11H4GglfRLQ1RrWBIHB01Vel5TEwxoX11wBnm4vPO6v8vgfwk9OpX-8tltO6ywsP9GZiM9XhJLLD5-LzlFQV5AEkwkDERJUytDCypYEU4EISE-MD5FU0Q', 'message': {'data': 'aGkgZnJvbSBjbG91ZCBjb25zb2xlIQ==', 'message_id': '12165864188103151', 'publish_time': '2024-08-28T11:49:50.962Z', 'attributes': {}, 'ordering_key': ''}, 'delivery_attempt': 0}]
```

This difference in formats causes issues when processing the messages downstream, as the structure is inconsistent and the protobuf-like strings require additional parsing.

### What you think should happen instead

The expected behavior is that the messages should be stored in XCom in a consistent format, regardless of whether `deferrable=True` or `deferrable=False` is set, ideally as a standard Python dictionary.

### How to reproduce

**Steps to Reproduce:**
1. Use `PubSubPullSensor` with `deferrable=True` and `deferrable=False`.
2. Compare the messages stored in XCom.

You can reproduce this issue using the following DAG:

```python
from airflow.decorators import dag
from airflow.providers.google.cloud.operators.pubsub import (
    PubSubCreateSubscriptionOperator,
    PubSubDeleteSubscriptionOperator,
)
from airflow.providers.google.cloud.sensors.pubsub import PubSubPullSensor
from airflow.utils.dates import days_ago

# Define default arguments
default_args = {
    'start_date': days_ago(1),
    'retries': 1,
}

# Define the DAG with the @dag decorator
@dag(
    dag_id='pubsub_pull_sensor_example',
    default_args=default_args,
    schedule_interval=None,
    catchup=False,
)
def pubsub_dag():
    project_id = ""<<PROJECT_ID>>""
    topic_name = ""<<TOPIC_NAME>>""
    subscription_name = ""testing-airflow-subscription""

    # Task to create a Pub/Sub subscription
    create_subscription = PubSubCreateSubscriptionOperator(
        task_id=""create_subscription"",
        project_id=project_id,
        topic=topic_name,
        subscription=subscription_name
    )

    # PubSubPullSensor to wait for a message on the Pub/Sub subscription
    wait_for_message = PubSubPullSensor(
        task_id='wait_for_message',
        project_id=project_id,
        subscription=subscription_name,
        max_messages=1,  # We only want to pull one message
        ack_messages=True,  # Acknowledge the messages immediately after receiving
        deferrable=True,
    )

    # Task to delete the Pub/Sub subscription
    delete_subscription = PubSubDeleteSubscriptionOperator(
        task_id=""delete_subscription"",
        project_id=project_id,
        subscription=subscription_name
    )

    # Define the task dependencies
    create_subscription_task = create_subscription
    pulled_messages = wait_for_message
    delete_subscription_task = delete_subscription

    create_subscription_task >> pulled_messages
    pulled_messages >> delete_subscription_task

# Instantiate the DAG by calling the function
pubsub_dag = pubsub_dag()
```

**Note:**

1. This code requires an existing Pub/Sub topic.
2. The DAG will create a new subscription to that topic.
3. After running the DAG, post a message to the specified topic and observe the XCom value of the `""wait_for_message""` task to see how the message is stored.
4. Please update the `project_id` and `topic_name` in the code with your actual Google Cloud project ID and Pub/Sub topic name.

---

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",arpit-maheshwari1,2024-08-30 07:23:11+00:00,['gopidesupavan'],2024-10-02 06:59:18+00:00,2024-10-02 06:59:18+00:00,https://github.com/apache/airflow/issues/41877,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('area:providers', ''), ('area:async-operators', 'AIP-40: Deferrable (""Async"") Operators')]","[{'comment_id': 2320322231, 'issue_id': 2496447585, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 30, 7, 23, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2322917853, 'issue_id': 2496447585, 'author': 'gopidesupavan', 'body': 'Looking into this one, seems received message to JSON-serializable dicts conversion not there when deferrable = True. Let me check and update.', 'created_at': datetime.datetime(2024, 8, 31, 14, 36, 37, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-30 07:23:13 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

gopidesupavan (Assginee) on (2024-08-31 14:36:37 UTC): Looking into this one, seems received message to JSON-serializable dicts conversion not there when deferrable = True. Let me check and update.

"
2496444250,issue,closed,completed,Add Horizontal Pod Autoscaler (HPA) to webserver deployment,"### Description

Add a Horizontal Pod Autoscaler (HPA) to the webserver deployment. Currently, we have an HPA in the worker deployment with a conditional statement that disables replicas in that deployment, because attempting to use HPA without disabling replicas could cause a conflict. I want to add an HPA to the webserver deployment to scale it up during high loads.

### Use case/motivation

If someone has a lot of users who use the UI, they will want to use HPA instead of replicas to reduce costs.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",JoaVirtudes19,2024-08-30 07:21:31+00:00,['JoaVirtudes19'],2024-09-18 14:21:03+00:00,2024-09-18 14:21:03+00:00,https://github.com/apache/airflow/issues/41876,"[('area:webserver', 'Webserver related Issues'), ('kind:feature', 'Feature Requests'), ('area:helm-chart', 'Airflow Helm Chart')]","[{'comment_id': 2320318390, 'issue_id': 2496444250, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 30, 7, 21, 34, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-30 07:21:34 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2495840033,issue,open,,Airflow scheduler could stuck sending callbacks to DAG processor,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.8.3

### What happened?

Currently, Airflow `DagFileProcessorManager` and scheduler each operates in a long running loop. In the case when standalone DAG processor is NOT enabled, the two processes communicate over a Linux pipe (which typically with a capacity of 64K bytes).

`DagFileProcessorManager` executes the `run_parsing_loop`, which roughly:
1. List the DAG directory if necessary, and find all DAG files to be processed. It also exclude certain files, for instance, those that have been processed recently.
2. For each DAG file, launches a separate sub-process to parse the file, serialized the DAGs in the file and sync them to the database
3. Checks to see if there is any messages from the pipe, such as callbacks, if so, consume one message from the pipe.

Scheduler executes the `_run_scheduler_loop`, which roughly:
1. Pick up `N` DAG runs from the database 
2. Process these DAG runs, e.g., updating from queued to running state, marking failed ones, etc
3. Send callbacks (SLA callback, task failure callback, etc) to the DAG processor via the pipe
4. Perform heartbeat

Considering the callbacks, the scheduler and DAG processor can be seen as a producer and consumer relationship.

The DAG processor loop is typically very fast and non-blocking, **with the exception of listing DAG directory**. Normally, the listing should only happen once in a while (default to 5 minutes). However, if the DAG directory list interval (`dag_dir_list_interval `) is configured too short, and/or the DAG directory itself contains a large number of files to process, it's possible that the time taken to list the DAG directory could be longer than the list interval. Therefore, the listing could be triggered in almost every loop iteration, which means the loop iteration can take a considerable amount of time.

In comparison, the scheduler loops runs much faster. In each iteration, it could add some new callbacks to the pipe to be processed. Since in each iteration the DAG processor only consumes a single message from the pipe, the pipe would gradually fill up and eventually reach the 64k capacity. After this point, the scheduler is blocked at step 3, waiting for the callbacks to be sent. Notably, it cannot perform heartbeat and respond to the liveness probes. 

There are a potential consequences from the above. First, the scheduler loop could get stuck and thus causing DAGs not being scheduled in a timely manner. Second, in Kubernetes environment, because it cannot answer to the liveness probe, the scheduler would be repeatedly killed by K8S and restarted, which is not desirable. Third, if the number of files to be parsed is too large in comparison to the DAG listing duration, as in each iteration the ordering for the returned file list could relatively remain the same, the top subset of files could be processed repeatedly while leaving the files at the bottom of the list not getting picked up only after certain time, therefore increasing the total parsing time.

Internally we encountered this issue when setting the `dag_dir_list_interval` to be 30 seconds, while the total parsing time for the DAG directory is more than 30 seconds. 


### What you think should happen instead?

It would be nice if
1. Instead of consuming a single message each time, the DAG processor can drain all the messages in the pipe at the moment. This will reduce the risk of scheduler being blocked.
2. Add metrics to DAG listing time. Currently this information is only available in `dag_processor_manager.log`

### How to reproduce

It may be re-producible by:
1. Have a DAG directory with many files so the DAG listing time is non-trivial
2. Setting the `dag_dir_list_interval` to something small that is less than the listing time.



### Operating System

Debian GNU/Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",sunchao,2024-08-29 23:42:19+00:00,[],2024-08-30 15:52:51+00:00,,https://github.com/apache/airflow/issues/41869,"[('area:Scheduler', 'including HA (high availability) scheduler'), ('type:improvement', 'Changelog: Improvements'), ('area:core', '')]","[{'comment_id': 2320667855, 'issue_id': 2495840033, 'author': 'jscheffl', 'body': 'Thanks for the detailed description. I would add to your description that if you have a large DAG directory would should use `.airflowignore` to filter-down un-relevant files and improve DAG parsing performance. I would also not recommend to place DAG files in a very large tree of un-related files.\r\n\r\nIn case you have an excessive DAG folder structure, I would also recommend for performance reasons to separate-out the DAG parser from scheduler to be a separate component.\r\n\r\nAs there have been several improvements since Airflow 2.8 also in regards of performance and stability, can you check your concerns also against Ariflow 2.10?\r\n\r\nLooking forward for a PR as contribution!', 'created_at': datetime.datetime(2024, 8, 30, 9, 39, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2321739560, 'issue_id': 2495840033, 'author': 'sunchao', 'body': 'Thanks @jscheffl ! The `.airflowignore` seems very useful in our case, will try it out. We are also thinking about switching to the standalone DAG processor which should help to further improve scheduler stability.\r\n\r\nI checked the 2.10 / master but it seems the relevant code paths remain pretty much the same. So I think the specific issue would still happen if we were on a higher version of Airflow.', 'created_at': datetime.datetime(2024, 8, 30, 15, 52, 51, tzinfo=datetime.timezone.utc)}]","jscheffl on (2024-08-30 09:39:52 UTC): Thanks for the detailed description. I would add to your description that if you have a large DAG directory would should use `.airflowignore` to filter-down un-relevant files and improve DAG parsing performance. I would also not recommend to place DAG files in a very large tree of un-related files.

In case you have an excessive DAG folder structure, I would also recommend for performance reasons to separate-out the DAG parser from scheduler to be a separate component.

As there have been several improvements since Airflow 2.8 also in regards of performance and stability, can you check your concerns also against Ariflow 2.10?

Looking forward for a PR as contribution!

sunchao (Issue Creator) on (2024-08-30 15:52:51 UTC): Thanks @jscheffl ! The `.airflowignore` seems very useful in our case, will try it out. We are also thinking about switching to the standalone DAG processor which should help to further improve scheduler stability.

I checked the 2.10 / master but it seems the relevant code paths remain pretty much the same. So I think the specific issue would still happen if we were on a higher version of Airflow.

"
2495119983,issue,closed,completed,Kubernetes Pod Operator: race condition when using deferrable and logging_interval,"### Apache Airflow Provider(s)

cncf-kubernetes

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes==8.4.1

### Apache Airflow version

2.9.1

### Operating System

Linux

### Deployment

Astronomer

### Deployment details

Originally we noticed this in astronomer in a dag using astronomer-cosmos.  However I have been able to reliably reproduce the issue in airflow running through a python debugger in vscode using just the Kubernetes Pod Operator.

### What happened

When running a DAG we noticed a task that failed according to the logs output from DBT.  However airflow still marked the task as successful and then proceeded with the next task.  

In the airflow logs we see a line like 
```
[2024-08-28, 13:30:10 UTC] {triggerer_job_runner.py:631} INFO - Trigger my_task_group/manual__2024-08-28T11:39:57.188213+00:00/regional_tickets.base_tickets_run/-1/1 (ID 711) fired: TriggerEvent<{'status': 'running', 'last_log_time': DateTime(2024, 8, 28, 13, 29, 1, 820284, tzinfo=Timezone('UTC')), 'namespace': 'sidereal-protostar-2231', 'name': 'my-dag-run-2p3iotfr'}>
```
Then logs from DBT that show task has failed followed by
```
2024-08-28, 13:30:56 UTC] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=my_dag, task_id=regional_tickets.base_tickets_run, run_id=manual__2024-08-28T11:39:57.188213+00:00, execution_date=20240828T113957, start_date=20240828T123001, end_date=20240828T133056
```

### What you think should happen instead

The task should have failed and retried.

### How to reproduce

I have created a simple dag like
```python
from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator
import airflow
from airflow import DAG
from datetime import timedelta

with DAG(
        'kubernetes_defer_test',
        default_args={
            'start_date': airflow.utils.dates.days_ago(0),
            'retries': 20,
            'retry_delay': timedelta(seconds=5)
        },
        schedule_interval=None,
        catchup=False,
) as dag:

    task1 = KubernetesPodOperator(
        name=""hello-world"",
        image=""python:3.11-slim"",
        cmds=['python', '-c'],
        arguments=[""import time; time.sleep(5); print('hello'); time.sleep(30); print('world'); import sys; sys.exit(1)""],
        task_id=""dummy_run"",
        deferrable=True,
        logging_interval=10,
    )
```

And then placing a breakpoint on https://github.com/apache/airflow/blob/e8888fe055ac8c341e2fa6631ff6ac5f089d54c5/airflow/providers/cncf/kubernetes/utils/pod_manager.py#L427 
I leave it wait here until the pod execution fails, and then when it resumes the task will be marked as successful.  

### Anything else

I hope this is a faithful replication of the bug.  I have seen it just by running the dag in an airflow deployment, but it is intermittent until the timing lines up correctly.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",johnhoran,2024-08-29 17:18:15+00:00,['johnhoran'],2024-10-18 13:29:21+00:00,2024-10-18 13:29:21+00:00,https://github.com/apache/airflow/issues/41867,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]","[{'comment_id': 2318423653, 'issue_id': 2495119983, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 29, 17, 18, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2422481950, 'issue_id': 2495119983, 'author': 'johnhoran', 'body': 'Resolved by https://github.com/apache/airflow/pull/42815', 'created_at': datetime.datetime(2024, 10, 18, 13, 29, 21, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-29 17:18:18 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

johnhoran (Issue Creator) on (2024-10-18 13:29:21 UTC): Resolved by https://github.com/apache/airflow/pull/42815

"
2495042299,issue,closed,completed,AirFlow 2.9.3 hatch build missing www/static/dist inside the output wheel,"### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Per[ INSTALL file](https://github.com/apache/airflow/blob/v2-9-stable/INSTALL#L219), the `hatch build` result for v2-9-stable (2.9.3) does not contains ""airflow/www/static/dist"" subdirectory. On the other hand, the provided 2.9.3 wheel has it. This caused the installed airflow UI to have lots of 404 errors for files like
```
http://127.0.0.1:8081/control/static/dist/bootstrap-datetimepicker.min.css
``` 

### What you think should happen instead?

_No response_

### How to reproduce

1. hatch build -t sdist
2. tar -tf filename.tar.gz | grep 'www/static/dist'


### Operating System

Red Hat Enterprise Linux release 8.8 (Ootpa)

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",gaoweisgp,2024-08-29 16:39:52+00:00,[],2024-08-29 16:44:04+00:00,2024-08-29 16:42:24+00:00,https://github.com/apache/airflow/issues/41866,"[('kind:bug', 'This is a clearly a bug'), ('invalid', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2318329304, 'issue_id': 2495042299, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 29, 16, 39, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2318335612, 'issue_id': 2495042299, 'author': 'potiuk', 'body': 'Correct. You need to use `-t custom` target to build assets as described in https://github.com/apache/airflow/blob/v2-9-stable/INSTALL#L342', 'created_at': datetime.datetime(2024, 8, 29, 16, 42, 20, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-29 16:39:56 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-08-29 16:42:20 UTC): Correct. You need to use `-t custom` target to build assets as described in https://github.com/apache/airflow/blob/v2-9-stable/INSTALL#L342

"
2494898117,issue,closed,completed,Adding DAG import error stack trace as span event in OTEL traces for Airflow,"### Description

Currently, in OTEL traces for Airflow, the Dag file process manger will produce a span that will tell you which DAG file has import error (import error count), but does not currently contain any details of why the import error happened (e.g. stack trace). When the file processor processes DAG file, it produces stack trace and store into database, which can later be accessed.

It would be a great feature to:
- when dag process manager processes dag file, it will check for errors
- when the error is detected, it will access the database to attach the stacktrace as part of the span, as span event.



### Use case/motivation

When the dag file process manager encounters import error, it will produce span event containing the relative stack trace so that user does not have to navigate into Airflow UI to see the import error separately.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",howardyoo,2024-08-29 15:36:59+00:00,[],2024-08-30 20:36:25+00:00,2024-08-30 16:25:14+00:00,https://github.com/apache/airflow/issues/41863,"[('kind:feature', 'Feature Requests'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2322299534, 'issue_id': 2494898117, 'author': 'howardyoo', 'body': 'Thank you!Sent from my iPhoneOn Aug 30, 2024, at 11:25\u202fAM, Jarek Potiuk ***@***.***> wrote:\ufeff\r\nClosed #41863 as completed via #41865.\r\n\r\n—Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you authored the thread.Message ID: ***@***.***>', 'created_at': datetime.datetime(2024, 8, 30, 20, 36, 23, tzinfo=datetime.timezone.utc)}]","howardyoo (Issue Creator) on (2024-08-30 20:36:23 UTC): Thank you!Sent from my iPhoneOn Aug 30, 2024, at 11:25 AM, Jarek Potiuk ***@***.***> wrote:﻿
Closed #41863 as completed via #41865.

—Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you authored the thread.Message ID: ***@***.***>

"
2494105094,issue,closed,completed,Bitnami Airflow chart 19.0.3 (Airflow version 2.10) not creating pods with unique names,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Pods are not getting unique names. In the earlier version of the chart we used to get unique pod names. but after upgrading to the latest chart, its not generating random pod names, instead its trying to use the metadata.name value as the pod name and i am facing issue when try to create multiple pods

### What you think should happen instead?

Pods should get unique names

### How to reproduce

Upgrade to Bitnami chart version from 13.1.1 to 19.0.3

### Operating System

Debian GNU/Linux 12

### Versions of Apache Airflow Providers

2.10

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Helm deployment

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Arunkumar-sai,2024-08-29 10:54:06+00:00,[],2024-08-30 11:54:14+00:00,2024-08-30 11:54:14+00:00,https://github.com/apache/airflow/issues/41855,"[('kind:bug', 'This is a clearly a bug'), ('pending-response', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2317315049, 'issue_id': 2494105094, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 29, 10, 54, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2320381580, 'issue_id': 2494105094, 'author': 'Arunkumar-sai', 'body': ""still didn't figured out the issue or root cause"", 'created_at': datetime.datetime(2024, 8, 30, 7, 47, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2320672178, 'issue_id': 2494105094, 'author': 'jscheffl', 'body': 'If you are using the Bitnami Helm Chart - are you sure this is the right issue tracker for the problem? Bitnami Helm CHart is not maintained by the Airflow repo.', 'created_at': datetime.datetime(2024, 8, 30, 9, 41, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2320909220, 'issue_id': 2494105094, 'author': 'Arunkumar-sai', 'body': '@jscheffl - can you help to understand why airflow is not updating the pod metadata.name, its creating with the exact same name we use in the pod_template.yaml file. are we suppose to do anything in addition to get the unique pod names', 'created_at': datetime.datetime(2024, 8, 30, 11, 21, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2320990436, 'issue_id': 2494105094, 'author': 'jscheffl', 'body': 'I believe there is a lot of context missing. Which executor are you using? Is it about the POD names of Airflow or the workload? How is your DAG code looking like? How is your Kubernetes config looking like?\r\n\r\nAnd most prominent: If it was working before - which other parameters have you adjusted that so that it was working before?\r\n\r\nIf you found a bug, then it is okay to post it. But this is not a research forum.', 'created_at': datetime.datetime(2024, 8, 30, 11, 53, 47, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-29 10:54:10 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

Arunkumar-sai (Issue Creator) on (2024-08-30 07:47:59 UTC): still didn't figured out the issue or root cause

jscheffl on (2024-08-30 09:41:44 UTC): If you are using the Bitnami Helm Chart - are you sure this is the right issue tracker for the problem? Bitnami Helm CHart is not maintained by the Airflow repo.

Arunkumar-sai (Issue Creator) on (2024-08-30 11:21:54 UTC): @jscheffl - can you help to understand why airflow is not updating the pod metadata.name, its creating with the exact same name we use in the pod_template.yaml file. are we suppose to do anything in addition to get the unique pod names

jscheffl on (2024-08-30 11:53:47 UTC): I believe there is a lot of context missing. Which executor are you using? Is it about the POD names of Airflow or the workload? How is your DAG code looking like? How is your Kubernetes config looking like?

And most prominent: If it was working before - which other parameters have you adjusted that so that it was working before?

If you found a bug, then it is okay to post it. But this is not a research forum.

"
2493957195,issue,closed,completed,Alembic migration failed when updating from 2.8.3 to 2.10.0 (version file ec3471c1e067),"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When upgrading Airflow from version 2.8.3 to 2.10.0, Airflow fails to restart. Indeed the Alembic migration fails with the error:
```
alembic.util.exc.CommandError: Can't locate revision identified by 'ec3471c1e067'
```

The missing version file is located here : https://github.com/apache/airflow/blob/e001b88f5875cfd7e295891a0bbdbc75a3dccbfb/airflow/migrations/versions/0148_2_10_0_dataset_alias_dataset_event.py

It's no longer on the master branch but it is on the last commit with tag [`2.10.0`](https://github.com/apache/airflow/tree/2.10.0)

In a attempt to fix the problem I tried to revert the version back to 2.8.3 but Alembic is still looking for the `ec3471c1e067` version file when restarting the instance, and thus Airflow is unable to start anymore.

### What you think should happen instead?

The migration should have go through without error and Airflow start.

### How to reproduce

- Install Airflow 2.8.3 with a persistent database
- Update the Airflow version to 2.10.0
- Restart the instance
- See the error

### Operating System

Debian 12.6

### Versions of Apache Airflow Providers

From the official constraints file at https://raw.githubusercontent.com/apache/airflow/constraints-2.10.0/constraints-3.11.txt
```
apache-airflow-providers-airbyte==3.8.1
apache-airflow-providers-alibaba==2.8.1
apache-airflow-providers-amazon==8.27.0
apache-airflow-providers-apache-beam==5.7.2
apache-airflow-providers-apache-cassandra==3.5.1
apache-airflow-providers-apache-drill==2.7.3
apache-airflow-providers-apache-druid==3.10.2
apache-airflow-providers-apache-flink==1.4.2
apache-airflow-providers-apache-hdfs==4.4.2
apache-airflow-providers-apache-hive==8.1.2
apache-airflow-providers-apache-iceberg==1.0.0
apache-airflow-providers-apache-impala==1.4.2
apache-airflow-providers-apache-kafka==1.5.0
apache-airflow-providers-apache-kylin==3.6.2
apache-airflow-providers-apache-livy==3.8.1
apache-airflow-providers-apache-pig==4.4.1
apache-airflow-providers-apache-pinot==4.4.2
apache-airflow-providers-apache-spark==4.9.0
apache-airflow-providers-apprise==1.3.2
apache-airflow-providers-arangodb==2.5.1
apache-airflow-providers-asana==2.5.1
apache-airflow-providers-atlassian-jira==2.6.1
apache-airflow-providers-celery==3.7.3
apache-airflow-providers-cloudant==3.5.2
apache-airflow-providers-cncf-kubernetes==8.3.4
apache-airflow-providers-cohere==1.2.1
apache-airflow-providers-common-compat==1.1.0
apache-airflow-providers-common-io==1.4.0
apache-airflow-providers-common-sql==1.15.0
apache-airflow-providers-databricks==6.8.0
apache-airflow-providers-datadog==3.6.1
apache-airflow-providers-dbt-cloud==3.9.0
apache-airflow-providers-dingding==3.5.1
apache-airflow-providers-discord==3.7.1
apache-airflow-providers-docker==3.12.3
apache-airflow-providers-elasticsearch==5.4.2
apache-airflow-providers-exasol==4.5.3
apache-airflow-providers-fab==1.2.2
apache-airflow-providers-facebook==3.5.2
apache-airflow-providers-ftp==3.10.1
apache-airflow-providers-github==2.6.2
apache-airflow-providers-google==10.21.1
apache-airflow-providers-grpc==3.5.2
apache-airflow-providers-hashicorp==3.7.1
apache-airflow-providers-http==4.12.0
apache-airflow-providers-imap==3.6.1
apache-airflow-providers-influxdb==2.6.0
apache-airflow-providers-jdbc==4.4.0
apache-airflow-providers-jenkins==3.6.1
apache-airflow-providers-microsoft-azure==10.3.0
apache-airflow-providers-microsoft-mssql==3.8.0
apache-airflow-providers-microsoft-psrp==2.7.1
apache-airflow-providers-microsoft-winrm==3.5.1
apache-airflow-providers-mongo==4.1.2
apache-airflow-providers-mysql==5.6.3
apache-airflow-providers-neo4j==3.6.1
apache-airflow-providers-odbc==4.6.3
apache-airflow-providers-openai==1.2.2
apache-airflow-providers-openfaas==3.5.1
apache-airflow-providers-openlineage==1.10.0
apache-airflow-providers-opensearch==1.3.0
apache-airflow-providers-opsgenie==5.6.1
apache-airflow-providers-oracle==3.10.3
apache-airflow-providers-pagerduty==3.7.2
apache-airflow-providers-papermill==3.7.2
apache-airflow-providers-pgvector==1.2.2
apache-airflow-providers-pinecone==2.0.1
apache-airflow-providers-postgres==5.11.3
apache-airflow-providers-presto==5.5.2
apache-airflow-providers-qdrant==1.1.2
apache-airflow-providers-redis==3.7.1
apache-airflow-providers-salesforce==5.7.2
apache-airflow-providers-samba==4.7.1
apache-airflow-providers-segment==3.5.1
apache-airflow-providers-sendgrid==3.5.1
apache-airflow-providers-sftp==4.10.3
apache-airflow-providers-singularity==3.5.1
apache-airflow-providers-slack==8.8.0
apache-airflow-providers-smtp==1.7.1
apache-airflow-providers-snowflake==5.6.1
apache-airflow-providers-sqlite==3.8.2
apache-airflow-providers-ssh==3.12.0
apache-airflow-providers-tableau==4.5.2
apache-airflow-providers-tabular==1.5.1
apache-airflow-providers-telegram==4.5.2
apache-airflow-providers-teradata==2.5.0
apache-airflow-providers-trino==5.7.2
apache-airflow-providers-vertica==3.8.2
apache-airflow-providers-weaviate==2.0.0
apache-airflow-providers-yandex==3.11.2
apache-airflow-providers-ydb==1.2.0
apache-airflow-providers-zendesk==4.7.1
```

### Deployment

Other 3rd-party Helm chart

### Deployment details

Custom Helm chart deployment.

### Anything else?

Currently my instance of Airflow is unable to start, even after trying to revert to the last used version (2.8.3).

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Timelessprod,2024-08-29 09:42:46+00:00,[],2024-08-29 14:41:27+00:00,2024-08-29 14:41:27+00:00,https://github.com/apache/airflow/issues/41853,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('area:db-migrations', 'PRs with DB migration')]","[{'comment_id': 2317643918, 'issue_id': 2493957195, 'author': 'tirkarthi', 'body': 'main branch is now under development for Airflow 3 where migration files were pruned. You should probably try using v2-10-test if fixes for 2.10 are needed and use 2.10.0 tag.\r\n\r\nhttps://github.com/apache/airflow/tree/v2-10-test', 'created_at': datetime.datetime(2024, 8, 29, 13, 21, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2317724421, 'issue_id': 2493957195, 'author': 'Timelessprod', 'body': '@tirkarthi Why have those version files been removed from the 2.10.0 release ? Currently Alembic cannot find them so there\'s something wrong with the release I think.\r\n\r\nIs this ""v2-10-test"" version among the pre-releases available on PIP ? My libraries are managed by PIP and I\'d prefer no to manually clone the repo if possible.\r\n\r\nThank you.', 'created_at': datetime.datetime(2024, 8, 29, 13, 51, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2317919336, 'issue_id': 2493957195, 'author': 'potiuk', 'body': ""You have not mentioned it but my guess is that you are dynamically installing packages and after this dynamic installation - ikely your image actually runs lower version of Airlfow.\r\n\r\nThe revision is in the alembic table so the migration worked - but the airflow command you run  uses a downgraded version of airflow after your dynamic `pip install` command. \r\n\r\nPlease take a look on what version of the image you are using and whether your customization / extending the image has some problems. For example when you are installing dynamically new packages  - the `pip install` command might automatically downgrade airflow. It solely depends on packages you install and resolution of pip which might decide that downgrading airflow fulfills other dependencies better.\r\n\r\nThat's why in the offiical installation description of upgrade scenarios, you will find that you always have to specify `apache-airflow=${AIRFLOW_VERSION}` https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html#installation-and-upgrade-scenarios - so that `pip` won't downgrade airflow.\r\n\r\nThis is one of the reasons why we always discouraged installing packages dynamically when your image starts - because for example airlfow version might change - but also other packages might and recommend users to build their own image - this way they can be sure what version of packages they have there. With dynamic package"", 'created_at': datetime.datetime(2024, 8, 29, 14, 41, 21, tzinfo=datetime.timezone.utc)}]","tirkarthi on (2024-08-29 13:21:11 UTC): main branch is now under development for Airflow 3 where migration files were pruned. You should probably try using v2-10-test if fixes for 2.10 are needed and use 2.10.0 tag.

https://github.com/apache/airflow/tree/v2-10-test

Timelessprod (Issue Creator) on (2024-08-29 13:51:46 UTC): @tirkarthi Why have those version files been removed from the 2.10.0 release ? Currently Alembic cannot find them so there's something wrong with the release I think.

Is this ""v2-10-test"" version among the pre-releases available on PIP ? My libraries are managed by PIP and I'd prefer no to manually clone the repo if possible.

Thank you.

potiuk on (2024-08-29 14:41:21 UTC): You have not mentioned it but my guess is that you are dynamically installing packages and after this dynamic installation - ikely your image actually runs lower version of Airlfow.

The revision is in the alembic table so the migration worked - but the airflow command you run  uses a downgraded version of airflow after your dynamic `pip install` command. 

Please take a look on what version of the image you are using and whether your customization / extending the image has some problems. For example when you are installing dynamically new packages  - the `pip install` command might automatically downgrade airflow. It solely depends on packages you install and resolution of pip which might decide that downgrading airflow fulfills other dependencies better.

That's why in the offiical installation description of upgrade scenarios, you will find that you always have to specify `apache-airflow=${AIRFLOW_VERSION}` https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html#installation-and-upgrade-scenarios - so that `pip` won't downgrade airflow.

This is one of the reasons why we always discouraged installing packages dynamically when your image starts - because for example airlfow version might change - but also other packages might and recommend users to build their own image - this way they can be sure what version of packages they have there. With dynamic package

"
2493637105,issue,closed,not_planned,Webserver is slow after upgrading to v2.9.3,"### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

We have updated the version from 2.6.2 to 2.9.3. After the update the main page opens very slowly. The 'failed' button causes Airflow to fail due to a timeout. 
[Start the discussion here](https://github.com/apache/airflow/discussions/41113)

### What you think should happen instead?

We have 3200 Dags now. And their number is increasing. 
The slowdown is caused by line 855 in the file `/airflow/www/views.py`:
```python
status_count_failed = get_query_count(failed_dags, session=session)
```
But not all of it, but only that part of it that filters dags by access rights. We commented out line 799 to prevent filtering by IN. This row:
```python
dags_query = dags_query.where(DagModel.dag_id.in_(filter_dag_ids))
```
This filter condition is not optimal when querying and is very slow. The slow part is highlighted in the query
![airflow-failed-sql](https://github.com/user-attachments/assets/7adf5f8d-ebd8-4c04-a7b6-8765dc94e206)

It is converted from this line:
```sql
WHERE NOT dag.is_subdag AND dag.is_active AND dag.dag_id IN (__[POSTCOMPILE_dag_id_1])
```
The entire request takes between 60 and 70 seconds on our main page. But without this filtering part, the request takes just over 1 second.
My dbeaver hangs during DB query due to lack of memory.

Is it possible to optimize this query? Or split it into parts and filter by permissions it using Python.

### How to reproduce

You need to have a large number of dags and dagruns.

### Operating System

CentOS 7

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

Our infrastructure:
1) database server (Postgres 13)
2) server with Airflow webserver, scheduler and Redis
3) three servers with workers

Modified bitnami images are used. Additional libraries installed.

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",TreasureMaster,2024-08-29 07:14:31+00:00,[],2024-09-21 00:14:13+00:00,2024-09-21 00:14:13+00:00,https://github.com/apache/airflow/issues/41851,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('area:webserver', 'Webserver related Issues'), ('area:performance', ''), ('pending-response', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2316877982, 'issue_id': 2493637105, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 29, 7, 14, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2317650708, 'issue_id': 2493637105, 'author': 'tirkarthi', 'body': 'Related \r\n\r\nhttps://github.com/apache/airflow/issues/38776\r\nhttps://github.com/apache/airflow/issues/40547', 'created_at': datetime.datetime(2024, 8, 29, 13, 24, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2320689319, 'issue_id': 2493637105, 'author': 'jscheffl', 'body': 'I would assume a setup with 3000+ DAGs is nothing that is running w/o special attention in regards of performance. Re-writing the queries of course is possible but also the complexity of data selection might be expensive.\r\n\r\nPlans are ongoing to re-write the UI and also make async calls in a future Airflow 3, I would not expect a major investment in current Airflow 2.10-line.\r\n\r\nSome options that might help compensating - as you are already in level of patching code:\r\n\r\n- Have you attempted to analyze the query and add more resources to the DB to improve queries or add specific indexes?\r\n- Do you use DAG level permissions? If not, as you are patching the code, removing the DAG access level filter might be a simple option to improve query\r\n- Have you tested with Airflow 2.10 if this improves the situation?\r\n- Would you be willing to supply a PR as performance patch?', 'created_at': datetime.datetime(2024, 8, 30, 9, 48, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2350730701, 'issue_id': 2493637105, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 9, 14, 0, 14, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2364773230, 'issue_id': 2493637105, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 9, 21, 0, 14, 12, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-29 07:14:34 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

tirkarthi on (2024-08-29 13:24:03 UTC): Related 

https://github.com/apache/airflow/issues/38776
https://github.com/apache/airflow/issues/40547

jscheffl on (2024-08-30 09:48:30 UTC): I would assume a setup with 3000+ DAGs is nothing that is running w/o special attention in regards of performance. Re-writing the queries of course is possible but also the complexity of data selection might be expensive.

Plans are ongoing to re-write the UI and also make async calls in a future Airflow 3, I would not expect a major investment in current Airflow 2.10-line.

Some options that might help compensating - as you are already in level of patching code:

- Have you attempted to analyze the query and add more resources to the DB to improve queries or add specific indexes?
- Do you use DAG level permissions? If not, as you are patching the code, removing the DAG access level filter might be a simple option to improve query
- Have you tested with Airflow 2.10 if this improves the situation?
- Would you be willing to supply a PR as performance patch?

github-actions[bot] on (2024-09-14 00:14:06 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-09-21 00:14:12 UTC): This issue has been closed because it has not received response from the issue author.

"
2493455459,issue,closed,not_planned,Connecting to a remote airflow instance via cli,"### What do you see as an issue?

Hello, I am trying to connect to an airflow running remotely in EC2 via airflow cli from local. I am following the below documentation for the setup. I cannot find a way to authenticate to the remote airflow instance. Is it possible to do it?

https://airflow.apache.org/docs/apache-airflow/2.0.2/usage-cli.html#set-up-connection-to-a-remote-airflow-instance

### Solving the problem

If it is possible to authenticate to a remote airflow the steps can be documented

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",deepaksinghkhetwal,2024-08-29 04:59:55+00:00,[],2024-10-23 17:15:49+00:00,2024-09-21 00:14:14+00:00,https://github.com/apache/airflow/issues/41849,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('area:CLI', ''), ('kind:documentation', ''), ('pending-response', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2316718329, 'issue_id': 2493455459, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 29, 4, 59, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2320720479, 'issue_id': 2493455459, 'author': 'jscheffl', 'body': 'The remote connect option to use the CLI with the REST API has been removed. The link of documentation you shared is from a very old 2.0.2 Airflow-version.\r\nThe current version of the SW does not carry this at the moment. But we are planning to re-introduce this in Airflow 3 again for better remote management under the umbrealla of AIP-81 (see https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-81+Enhanced+Security+in+CLI+via+Integration+of+API?src=contextnavpagetreemode) - if you are wanting to contribute we would be happy.', 'created_at': datetime.datetime(2024, 8, 30, 10, 0, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2350730723, 'issue_id': 2493455459, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 9, 14, 0, 14, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2364773247, 'issue_id': 2493455459, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 9, 21, 0, 14, 14, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-29 04:59:58 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jscheffl on (2024-08-30 10:00:15 UTC): The remote connect option to use the CLI with the REST API has been removed. The link of documentation you shared is from a very old 2.0.2 Airflow-version.
The current version of the SW does not carry this at the moment. But we are planning to re-introduce this in Airflow 3 again for better remote management under the umbrealla of AIP-81 (see https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-81+Enhanced+Security+in+CLI+via+Integration+of+API?src=contextnavpagetreemode) - if you are wanting to contribute we would be happy.

github-actions[bot] on (2024-09-14 00:14:08 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-09-21 00:14:14 UTC): This issue has been closed because it has not received response from the issue author.

"
2492889967,issue,open,,Conflicting results between Ruff linting and Pydantic tests,"              Conflicting tests when evaluating `airflow/tests/providers/google/cloud/operators/vertex_ai/test_supervised_fine_tuning.py` and `airflow/tests/providers/google/cloud/hooks/vertex_ai/test_supervised_fine_tuning.py` ...

[Static checks, mypy, docs / Static checks](https://github.com/apache/airflow/actions/runs/10603244920/job/29387419328?pr=41807#logs) : 

```
Run 'ruff' for extremely fast Python linting.......................................Failed
- hook id: ruff
- exit code: 1

All checks passed!
tests/providers/google/cloud/hooks/vertex_ai/test_supervised_fine_tuning.py:27:1: E402 Module level import not at top of file
   |
25 |   pytest.importorskip(""google.cloud.aiplatform_v1"")
26 |   
27 | / from airflow.providers.google.cloud.hooks.vertex_ai.supervised_fine_tuning import (
28 | |     SupervisedFineTuningHook,
29 | | )
   | |_^ E402
30 |   from tests.providers.google.cloud.utils.base_gcp_mock import (
31 |       mock_base_gcp_hook_default_project_id,
```

[Special tests / Pydantic removed test / All:Pydantic-Removed-Postgres:12:3.8: API Always BranchExternalPython BranchPythonVenv CLI Core ExternalPython Operators Other PlainAsserts](https://github.com/apache/airflow/actions/runs/10602210989/job/29384161799#logs) : 

```
_ ERROR collecting tests/providers/google/cloud/hooks/vertex_ai/test_supervised_fine_tuning.py _
ImportError while importing test module '/opt/airflow/tests/providers/google/cloud/hooks/vertex_ai/test_supervised_fine_tuning.py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
/usr/local/lib/python3.8/importlib/__init__.py:127: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
tests/providers/google/cloud/hooks/vertex_ai/test_supervised_fine_tuning.py:24: in <module>
    from airflow.providers.google.cloud.hooks.vertex_ai.supervised_fine_tuning import (
airflow/providers/google/cloud/hooks/vertex_ai/supervised_fine_tuning.py:25: in <module>
    import vertexai
E   ModuleNotFoundError: No module named 'vertexai'
```

_Originally posted by @CYarros10 in https://github.com/apache/airflow/pull/41807#discussion_r1735221087_
            ",CYarros10,2024-08-28 20:00:52+00:00,[],2024-08-28 20:08:09+00:00,,https://github.com/apache/airflow/issues/41847,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('area:providers', '')]","[{'comment_id': 2316159815, 'issue_id': 2492889967, 'author': 'CYarros10', 'body': 'I\'d imagine this is a general issue regarding how Ruff formatting/linting checks are required and how `pytest.importorskip` is sometimes necessary before other imports to pass Pydantic environment build tests - example:\r\n\r\n```\r\n# For no Pydantic environment, we need to skip the tests\r\npytest.importorskip(""google.cloud.aiplatform_v1"")\r\n\r\nfrom airflow.providers.google.cloud.operators.vertex_ai.supervised_fine_tuning import (\r\n    SupervisedFineTuningTrainOperator,\r\n)\r\n```', 'created_at': datetime.datetime(2024, 8, 28, 20, 7, 42, tzinfo=datetime.timezone.utc)}]","CYarros10 (Issue Creator) on (2024-08-28 20:07:42 UTC): I'd imagine this is a general issue regarding how Ruff formatting/linting checks are required and how `pytest.importorskip` is sometimes necessary before other imports to pass Pydantic environment build tests - example:

```
# For no Pydantic environment, we need to skip the tests
pytest.importorskip(""google.cloud.aiplatform_v1"")

from airflow.providers.google.cloud.operators.vertex_ai.supervised_fine_tuning import (
    SupervisedFineTuningTrainOperator,
)
```

"
2492720258,issue,closed,completed,Rest API - Source File Upload,"### Description

It is good to have an API to upload source code files to the Airflow server to make deployments.

### Use case/motivation

I am the developer of the Airflow VSCode extension, which developers use to build and test Airflow DAGs locally within VSCode. Currently, before running a DAG for testing, the source code must be deployed to the Airflow server. Developers often rely on their own deployment solutions, tailored to their specific development environments. However, if an API were provided to upload source code files directly, the VSCode extension could automatically upload any changed files and run the DAGs, streamlining the testing process. This is one of the most requested features by users of the extension.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",necatiarslan,2024-08-28 18:16:55+00:00,[],2024-08-30 13:55:08+00:00,2024-08-30 13:55:08+00:00,https://github.com/apache/airflow/issues/41844,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('pending-response', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2315984736, 'issue_id': 2492720258, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 28, 18, 16, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2320737006, 'issue_id': 2492720258, 'author': 'jscheffl', 'body': 'Uploading of DAGs was not implemented by intend especially in regards of considerations of security as far as I understand - in the past.\r\nI understand for interactive development and debugging this feature might be useful. But as it is a structural change of how Airflow works, I recommend doing this via AIP (Airflow Improvement Process).\r\n\r\nThere are recently some plans to change DAG deployment in Ariflow 3 under the AIP-66, can you take a look and check if this is the ""right"" direction? --> https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=294816356&src=contextnavpagetreemode\r\n\r\nThere is a bit more complexity behind the scene. Else it might be a valid topic to be added as plans for future changes towards Ariflow 3. --> https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+3+Workstreams', 'created_at': datetime.datetime(2024, 8, 30, 10, 6, 34, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-28 18:16:58 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jscheffl on (2024-08-30 10:06:34 UTC): Uploading of DAGs was not implemented by intend especially in regards of considerations of security as far as I understand - in the past.
I understand for interactive development and debugging this feature might be useful. But as it is a structural change of how Airflow works, I recommend doing this via AIP (Airflow Improvement Process).

There are recently some plans to change DAG deployment in Ariflow 3 under the AIP-66, can you take a look and check if this is the ""right"" direction? --> https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=294816356&src=contextnavpagetreemode

There is a bit more complexity behind the scene. Else it might be a valid topic to be added as plans for future changes towards Ariflow 3. --> https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+3+Workstreams

"
2492699530,issue,closed,completed,StepFunctionStartExecutionOperator in MWAA does not throw error if IAM Role does not  DescribeExecution permission,"### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

2.8.1

### Apache Airflow version

2.8.1

### Operating System

Managed Airflow

### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### What happened

I am using `StepFunctionStartExecutionOperator`  to execute a StateMachine.

```
StepFunctionStartExecutionOperator(
...
task_id=""load_data"",
deferrable=True,
waiter_delay=30,  # Poll for every 30 seconds
waiter_max_attempts=10,  # maximum number of attempts to poll for status
do_xcom_push=True,
)
```

if I look into the log of the task,

```
 {{waiter_with_logging.py:129}} INFO - Status of step function execution is: 
{{waiter_with_logging.py:129}} INFO - Status of step function execution is: 
{{waiter_with_logging.py:129}} INFO - Status of step function execution is: 
```

as you can see that airflow is not getting the current status (RUNNING, FAILED etc) of the StateMachine



### What you think should happen instead

Expected Output when the State Machine is RUNNING

```
[2024-08-28, 17:01:06 UTC] {{waiter_with_logging.py:129}} INFO - Status of step function execution is: RUNNING
[2024-08-28, 17:02:06 UTC] {{waiter_with_logging.py:129}} INFO - Status of step function execution is: RUNNING

```

### How to reproduce


So in my case I found the root cause of this problem. The IAM Role associated with the Airflow did not have the below permission 
```
'states:DescribeExecution'
```
On the StateMachine execution arn.
```
arn:aws:states:<Region>:<accountId>:execution:<stateMachineName>:*
```

### Before granting the permission
[2024-08-28, 01:41:53 UTC] {{waiter_with_logging.py:129}} INFO - Status of step function execution is: 
### After Granting the permission
```
[2024-08-28, 17:01:06 UTC] {{waiter_with_logging.py:129}} INFO - Status of step function execution is: RUNNING
```
Unfortunately, MWAA does not throw `Accessdenied` permission error if the IAM role does not have aforementioned permission on the execution arn of the StateMachine. 


### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",sarkch,2024-08-28 18:04:31+00:00,[],2024-08-28 20:08:13+00:00,2024-08-28 20:06:36+00:00,https://github.com/apache/airflow/issues/41843,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2315964052, 'issue_id': 2492699530, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 28, 18, 4, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2315976143, 'issue_id': 2492699530, 'author': 'vincbeck', 'body': 'I am not sure this is an Airflow issue but a MWAA issue', 'created_at': datetime.datetime(2024, 8, 28, 18, 11, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2316148932, 'issue_id': 2492699530, 'author': 'sarkch', 'body': 'Yes. this is more of an MWAA issue than Airflow.  But in the questionnaire it asked which distribution of Airflow I am facing this problem and MWAA was one of the options.  Hence, I thought the issues/bugs with MWAA are also tracked through here.', 'created_at': datetime.datetime(2024, 8, 28, 20, 0, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2316157946, 'issue_id': 2492699530, 'author': 'vincbeck', 'body': ""No, we are tracking here only issues related to Airflow. I'll close this one and re-open the other one you opened internally and will transfer it to MWAA team"", 'created_at': datetime.datetime(2024, 8, 28, 20, 6, 37, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-28 18:04:34 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

vincbeck on (2024-08-28 18:11:33 UTC): I am not sure this is an Airflow issue but a MWAA issue

sarkch (Issue Creator) on (2024-08-28 20:00:56 UTC): Yes. this is more of an MWAA issue than Airflow.  But in the questionnaire it asked which distribution of Airflow I am facing this problem and MWAA was one of the options.  Hence, I thought the issues/bugs with MWAA are also tracked through here.

vincbeck on (2024-08-28 20:06:37 UTC): No, we are tracking here only issues related to Airflow. I'll close this one and re-open the other one you opened internally and will transfer it to MWAA team

"
2492502096,issue,closed,completed,Add trace tags to task instances and also support OTEL_RESOURCE_ATTRIBUTES for spans,"### Description

1. Currently when OTEL trace is enabled, OTEL_RESOURCE_ATTRIBUTES (https://opentelemetry.io/docs/languages/sdk-configuration/general/#otel_resource_attributes) environment variable is not being utilized to collect additional resource attributes that user may be interested to add.
2. The airflow configuration under airflow.traces.tags are getting populated on DAG runs spans, but they are not getting populated in task instance spans. Having them also being populated in task instance spans will greatly help the analysis.

### Use case/motivation

1. When OTEL traces is enabled, user can add the environment variable OTEL_RESOURCE_ATTRIBUTES with values like `res.attr1=val1,res.attr2=val2`. When Airflow is running and generating spans, it will be able to collect those and use them, helping them to instrument those conveniently without doing additional instrumentation coding.
2. As for spans generated when the task instances are running, the tag values set in airflow.traces.tags can also be applied to task instance spans so that they will hold the value, much the same way as dag run spans have.

### Related issues

None

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",howardyoo,2024-08-28 16:07:54+00:00,[],2024-08-30 13:39:53+00:00,2024-08-30 13:39:53+00:00,https://github.com/apache/airflow/issues/41840,"[('kind:feature', 'Feature Requests'), ('telemetry', 'Telemetry-related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2492432111,issue,closed,completed,Sometimes PubSubPullSensor doesn't pull messages even if the PubSub subscription has unacked messages   ,"### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

apache-airflow-providers-google==10.17.0

### Apache Airflow version

2.7.3

### Operating System

macOS Sonoma

### Deployment

Google Cloud Composer

### Deployment details

_No response_

### What happened

Sometimes PubSubPullSensor doesn't pull messages from a pubsub subscription even if there are unacked messages available. It's not clear when it does take 

### What you think should happen instead

The PubSubPullSensor should always pull messages if there are unacked messages in the pubsub subscription.

### How to reproduce

Difficult to reproduce exactly as it's not something that happens deterministically. The way I noticed the problem is in a DAG where we have a PubSubPullSensor poking every minute trying to retrieve messages from a pubsub subscription. Sometimes (seemingly random pattern) you'll notice that the the sensor doesn't pull any messages even if there are unacked messages for that subscription. 

### Anything else

I believe I found the cause of this problem and it's that the PubSubPullSensor class contains this code:

```
pulled_messages = hook.pull(
    project_id=self.project_id,
    subscription=self.subscription,
    max_messages=self.max_messages,
    return_immediately=True,
)
```

which has return_immediately hard-coded to True. Inside this function, the code calls the pull function of SubscriberClient, which has the following comment on the return_immediately argument:

```
return_immediately (bool):
    Optional. If this field set to true, the system will
    respond immediately even if it there are no messages
    available to return in the ``Pull`` response. Otherwise,
    the system may wait (for a bounded amount of time) until
    at least one message is available, rather than returning
    no messages. Warning: setting this field to ``true`` is
    discouraged because it adversely impacts the performance
    of ``Pull`` operations. We recommend that users do not
    set this field.
```

I created a version of the PubSubPullSensor which is exactly the same as the original but with return_immediately=False, and the problems I mention in this issue went away consistently.

The hard-coded return_immediately=True was introduced in [this PR](https://github.com/apache/airflow/pull/23231). 

I believe return_immediately should either be hard-coded to False or it should go back to being an argument of the class so users can set it to False and avoid the problem I'm describing in this issue.   
 

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",arnaubadia,2024-08-28 15:32:20+00:00,[],2024-09-09 16:43:35+00:00,2024-09-09 16:43:35+00:00,https://github.com/apache/airflow/issues/41838,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('area:providers', '')]","[{'comment_id': 2315684656, 'issue_id': 2492432111, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 28, 15, 32, 24, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-28 15:32:24 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2492314204,issue,closed,completed,Typo in OTEL trace attribute,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

The OTEL spans created for tasks have an attribute called `ququed_by_job_id` which I believe should be `queued_by_job_id`.

### What you think should happen instead?

_No response_

### How to reproduce

Enable OTEL tracing and run a DAG. 

### Operating System

Ubuntu 22.04.4 LTS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",swythan,2024-08-28 14:45:29+00:00,[],2024-08-30 07:36:57+00:00,2024-08-30 05:40:23+00:00,https://github.com/apache/airflow/issues/41836,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('telemetry', 'Telemetry-related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2315557083, 'issue_id': 2492314204, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 28, 14, 45, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2315745175, 'issue_id': 2492314204, 'author': 'howardyoo', 'body': 'I believe your observation is right! Definitely looks like a typo.', 'created_at': datetime.datetime(2024, 8, 28, 16, 1, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2320353934, 'issue_id': 2492314204, 'author': 'swythan', 'body': 'Thanks, all.', 'created_at': datetime.datetime(2024, 8, 30, 7, 36, 55, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-28 14:45:32 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

howardyoo on (2024-08-28 16:01:46 UTC): I believe your observation is right! Definitely looks like a typo.

swythan (Issue Creator) on (2024-08-30 07:36:55 UTC): Thanks, all.

"
2492286888,issue,closed,not_planned,Priority weight upstream inconsistency for tasks between DAGs,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.8.1

### What happened?

Task with higher priority (when **priority weight** _upstream_ is set) not selected to run when different DAGs are racing each other

### What you think should happen instead?

Once a task has finished the executor should give a chance for the tasks of other DAGs to queue up before choosing the next task to run. (I'm assuming this is the problem why the executor sometimes chooses the wrong task to run). And/or the order of tasks should be somewhat consistent/deterministic. 

Fwiw, I don't ever see this issue inside a single DAG where dagruns are racing each other, just between dagruns of different DAGs. There's no mention in the docs that this feature is only consistent between dagruns of the same DAG. It's a feature coupled with pools I believe, which work across DAGs, so I assumed this would work across DAGs too.

### How to reproduce

Create 2 dags (`dag 1 & 2`) and set **priority weight** _upstream_ on both
Create a pool
Have 5 tasks each, where both use the same pool for `tasks 3 & 4`
Have `task 3` take a long time (sleep)
Run both dags together

**Eventually:** `dag 1` is _running_ `task 3` and `dag 2` has _queued_ `task 3`
**Sometimes (and desired):** `dag 1` `task 4` runs before `dag 2` `task 3`
**Sometimes (and undesired):** `dag 2` `task 3` runs before `dag 1` `task 4`

### Operating System

rhel 9

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

So this is very minor and might not even be considered a bug at all, but I found it rather unintuitive, so if it's not classed as an bug I'm ok with that, just information I thought worth sharing.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",MatthewStrickland,2024-08-28 14:38:49+00:00,[],2024-09-21 00:14:16+00:00,2024-09-21 00:14:16+00:00,https://github.com/apache/airflow/issues/41835,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('pending-response', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2318498093, 'issue_id': 2492286888, 'author': 'jedcunningham', 'body': 'I have a feeling [schedule_after_task_execution](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#schedule-after-task-execution) may be the reason. Could you try turning that off and confirming?', 'created_at': datetime.datetime(2024, 8, 29, 17, 52, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2321025332, 'issue_id': 2492286888, 'author': 'jscheffl', 'body': '+1 in the assumption. Post task executions the next tasks are scheduled. Thus they might get immediately scheduled while the scheduler running centrally sees limits early and does not schedule other stuff.\r\n\r\nAlso can you please post an example DAG and check if the same applies to Airflow 2.10 as well?\r\n\r\nI would judge it a bit that scheduling is mostly not forcing a priority and does not attempt NOT to schedule bust attempts to schedule in a loop using best-effort what is schedule-able. The `schedule_after_task_execution` might be contributing to the best effort fact with the benefit of reduced latency between tasks.', 'created_at': datetime.datetime(2024, 8, 30, 12, 7, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2350730730, 'issue_id': 2492286888, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 9, 14, 0, 14, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2364773263, 'issue_id': 2492286888, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 9, 21, 0, 14, 15, tzinfo=datetime.timezone.utc)}]","jedcunningham on (2024-08-29 17:52:54 UTC): I have a feeling [schedule_after_task_execution](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#schedule-after-task-execution) may be the reason. Could you try turning that off and confirming?

jscheffl on (2024-08-30 12:07:37 UTC): +1 in the assumption. Post task executions the next tasks are scheduled. Thus they might get immediately scheduled while the scheduler running centrally sees limits early and does not schedule other stuff.

Also can you please post an example DAG and check if the same applies to Airflow 2.10 as well?

I would judge it a bit that scheduling is mostly not forcing a priority and does not attempt NOT to schedule bust attempts to schedule in a loop using best-effort what is schedule-able. The `schedule_after_task_execution` might be contributing to the best effort fact with the benefit of reduced latency between tasks.

github-actions[bot] on (2024-09-14 00:14:09 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-09-21 00:14:15 UTC): This issue has been closed because it has not received response from the issue author.

"
2492024337,issue,closed,completed,Airflow DAG run missing on few days,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.6.3

### What happened?

Schedule DAG execution missing in Airflow. Next Run of the DAG is skipped. In the Airflow UI DAG calendar view has 'no_status'

### What you think should happen instead?

As per the schedule, the DAG should have run on the date but dag run is skipped.

### How to reproduce

Currently this issue is found on multiple DAGs but currently looks like it is difficult to reproduce.

### Operating System

GCP composer

### Versions of Apache Airflow Providers

Version 2.6.3

### Deployment

Google Cloud Composer

### Deployment details

Airflow Version: 2.6.3
Composer version: 2.5.4

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bmenugul,2024-08-28 13:04:36+00:00,[],2024-09-03 14:42:18+00:00,2024-09-03 14:42:18+00:00,https://github.com/apache/airflow/issues/41833,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('pending-response', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2315265596, 'issue_id': 2492024337, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 28, 13, 4, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2326253115, 'issue_id': 2492024337, 'author': 'nathadfield', 'body': 'Without a reproducible example or, at the very least more information and context, I doubt you are going to get much help on this one because this is not a known problem or anything that is commonly experienced by Airflow users which hints that it might be something that is specific to you.', 'created_at': datetime.datetime(2024, 9, 3, 11, 11, 38, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-28 13:04:41 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

nathadfield on (2024-09-03 11:11:38 UTC): Without a reproducible example or, at the very least more information and context, I doubt you are going to get much help on this one because this is not a known problem or anything that is commonly experienced by Airflow users which hints that it might be something that is specific to you.

"
2491722584,issue,closed,completed,ECS Executor with EC2 launch type doesn't work as platform version cannot be unset which is a requirement,"### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.25.0

### Apache Airflow version

2.9.3

### Operating System

Debian GNU/Linux 12

### Deployment

Other Docker-based deployment

### Deployment details

Using the upstream Airflow docker images customised using by exporting the following env vars: 

AIRFLOW__CORE__EXECUTOR=airflow.providers.amazon.aws.executors.ecs.ecs_executor.AwsEcsExecutor
AIRFLOW__AWS_ECS_EXECUTOR__CLUSTER=<my cluster>
AIRFLOW__AWS_ECS_EXECUTOR__CONTAINER_NAME=<my container name> 
AIRFLOW__AWS_ECS_EXECUTOR__TASK_DEFINITION=<my task definition>
AIRFLOW__AWS_ECS_EXECUTOR__LAUNCH_TYPE=EC2
AIRFLOW__AWS_ECS_EXECUTOR__SECURITY_GROUPS=<my security group ids>
AIRFLOW__AWS_ECS_EXECUTOR__SUBNETS=<my subnets>
AIRFLOW__AWS_ECS_EXECUTOR__REGION_NAME=eu-west-2

### What happened

Currently when specifying the launch type for the ECS executor as ""EC2"" the current default being set for platform_version is being set to ""Latest"" which is incompatible for the EC2 launch type. When you don't export the AIRFLOW__AWS_ECS_EXECUTOR__PLATFORM_VERSION the current upstream airflow docker image will still put platform_version=latest into the airflow.cfg which will cause the ECS executor to fail as the AWS API says that the platform version parameter cannot be specified when an EC2 launch type is used when attempting to run a DAG with this executor. 

If you specify an empty string for AIRFLOW__AWS_ECS_EXECUTOR__PLATFORM_VERSION this stops the airflow.cfg being populated with a value for platform_version but the executor will still pass a platform_version to the resulting RunTask API call at which point AWS returns saying that the platform_version cannot be blank. Setting any other value such as null or anything else results in the original error of the platform version parameter cannot be specified when using EC2 as the launch type

### What you think should happen instead

The ECS Executor should verify that if EC2 is used as the launch type that the platform_version parameter is not passed to the API call. It seems the ECS Operator had a similar issue that was fixed as part of https://github.com/apache/airflow/issues/17276


### How to reproduce

Setup an ECS Cluster based on EC2 as the launch type with a task definition and anything else required as part of Airflow
Export the above variables into the docker image and run the webserver and scheduler and attempt to schedule a DAG run.

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",AdamStrLSEC,2024-08-28 10:41:57+00:00,[],2024-09-16 05:56:18+00:00,2024-09-16 05:56:18+00:00,https://github.com/apache/airflow/issues/41824,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('good first issue', '')]","[{'comment_id': 2314962350, 'issue_id': 2491722584, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 28, 10, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2322780403, 'issue_id': 2491722584, 'author': 'eladkal', 'body': 'cc @o-nikolas', 'created_at': datetime.datetime(2024, 8, 31, 5, 31, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2327649515, 'issue_id': 2491722584, 'author': 'ferruzzi', 'body': ""Thanks for calling this out!   We'll look into it, but with Airflow Summit coming next week it may have to wait till after that."", 'created_at': datetime.datetime(2024, 9, 4, 0, 1, 9, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-28 10:42:00 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2024-08-31 05:31:02 UTC): cc @o-nikolas

ferruzzi on (2024-09-04 00:01:09 UTC): Thanks for calling this out!   We'll look into it, but with Airflow Summit coming next week it may have to wait till after that.

"
2491693808,issue,open,,Missing Metrics After Migrating from StatsD to OpenTelemetry,"### Apache Airflow version

2.10.2

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I’ve recently migrated from StatsD to OpenTelemetry and am now sending metrics to Splunk Observability. 

However, I’ve noticed that some metrics are now missing after this change, such as `operator_successes`, `operator_failures`, and some `custom metrics` that I used to see when using StatsD with the Python **stats**  package in my DAGs.


The common factor among the missing metrics is that their descriptions in the [Airflow documentation](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/metrics.html#metric-descriptions) mention **_“Metric with xyz tagging.”_** I’m not sure if this is relevant.

### What you think should happen instead?

 All metrics, including `operator_successes`, `operator_failures`, and `custom metrics`, should be available as they were with StatsD.

### How to reproduce


- Migrate from StatsD to OpenTelemetry.
- Send metrics to Splunk Observability.
- Observe the missing metrics such as `operator_successes`, `operator_failures`, and `custom metrics`.

### Operating System

Azure AKS linux 

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",seifrajhi,2024-08-28 10:28:46+00:00,[],2025-02-07 14:00:16+00:00,,https://github.com/apache/airflow/issues/41822,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('telemetry', 'Telemetry-related issues')]","[{'comment_id': 2314935770, 'issue_id': 2491693808, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 28, 10, 28, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2392261934, 'issue_id': 2491693808, 'author': 'gopidesupavan', 'body': 'cc: @howardyoo', 'created_at': datetime.datetime(2024, 10, 3, 20, 21, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2468161709, 'issue_id': 2491693808, 'author': 'Ibraitas', 'body': ""Got the same thing on 2.10.2, but without migration from statsd. Tested locally - metrics in statsd are present, in otel many are missing, especially counters (`ti_failures`, `ti_successed`, `zombies_killed` etc.) The pattern with presence of tag breakdown from [@seifrajhi](https://github.com/seifrajhi) seems to be relevant, but there are other losses, such as `operator_failures_<operator_name>`, for example.\r\n\r\nLet me know if you need more information about the configuration, I'd be happy to help"", 'created_at': datetime.datetime(2024, 11, 11, 13, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469472871, 'issue_id': 2491693808, 'author': 'howardyoo', 'body': ""> Got the same thing on 2.10.2, but without migration from statsd. Tested locally - metrics in statsd are present, in otel many are missing, especially counters (`ti_failures`, `ti_successed`, `zombies_killed` etc.) The pattern with presence of tag breakdown from [@seifrajhi](https://github.com/seifrajhi) seems to be relevant, but there are other losses, such as `operator_failures_<operator_name>`, for example.\r\n> \r\n> Let me know if you need more information about the configuration, I'd be happy to help\r\n\r\nJust a question, are these metrics consistently missing, or randomly missing?\r\nAlso, what were the names of the metrics? any examples? This could either be due to the metrics name being too longe and getting truncated, or if Airflow did not have enough time to emit the metrics before something terminated."", 'created_at': datetime.datetime(2024, 11, 12, 2, 37, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469921606, 'issue_id': 2491693808, 'author': 'Ibraitas', 'body': '> Just a question, are these metrics consistently missing, or randomly missing?\r\nAlso, what were the names of the metrics? any examples? This could either be due to the metrics name being too longe and getting truncated, or if Airflow did not have enough time to emit the metrics before something terminated.\r\n\r\nI\'ll list a few with behaviors\r\n- `ti_successes` is missing completely;\r\n- `ti.start` is missing completely;\r\n- `ti.finish` is missing completely;\r\n- `operator` `_successes`, `_failures` are missing in any form;\r\n- `executor_running_tasks` does not catch fast executing tasks;\r\n- `job_start` is present as it is, not in `<job_name>_start` format and is always equal to 1, and `_end` is not present at all.\r\n\r\nI tested locally on dag with one task, which is executed once a minute, stably successful. Just in case I attach screenshots of all airflow metrics I see in Prometheus\r\n\r\n<img width=""605"" alt=""Screenshot 2024-11-12 at 11 38 23"" src=""https://github.com/user-attachments/assets/8a9a8c74-1751-4e15-90c4-03559ae5f0dd"">\r\n<img width=""554"" alt=""Screenshot 2024-11-12 at 11 39 37"" src=""https://github.com/user-attachments/assets/57bf2d85-bab9-41d7-a35a-a3b12b22cb97"">\r\n<img width=""659"" alt=""Screenshot 2024-11-12 at 11 40 13"" src=""https://github.com/user-attachments/assets/770e7053-4cb5-43e5-a097-b12c28518e74"">\r\n<img width=""610"" alt=""Screenshot 2024-11-12 at 11 40 53"" src=""https://github.com/user-attachments/assets/b9d06fd0-85e0-4eb4-b3f2-d79192aa5c4f"">\r\n<img width=""665"" alt=""Screenshot 2024-11-12 at 11 41 11"" src=""https://github.com/user-attachments/assets/62a74989-43b1-4bcc-99af-b38c3ddea03f"">', 'created_at': datetime.datetime(2024, 11, 12, 8, 45, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2472337950, 'issue_id': 2491693808, 'author': 'howardyoo', 'body': '@ferruzzi , could you take a look at this?\r\nThis is related to the issue that some of the metrics that stats is generating is not appearing when using OTEL.', 'created_at': datetime.datetime(2024, 11, 13, 4, 15, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2477097847, 'issue_id': 2491693808, 'author': 'ferruzzi', 'body': '@dannyl1u is looking into it', 'created_at': datetime.datetime(2024, 11, 14, 18, 10, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2477506858, 'issue_id': 2491693808, 'author': 'dannyl1u', 'body': ""Can confirm I've been able to reproduce this bug in version 3.0.0"", 'created_at': datetime.datetime(2024, 11, 14, 22, 10, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2492240767, 'issue_id': 2491693808, 'author': 'dannyl1u', 'body': '@ferruzzi and I had a good look into this. We suspect the bug is caused by a new resource in `SafeOtelLogger` being created each time `Stats.incr()` etc. is called \r\n\r\nhttps://github.com/apache/airflow/blob/5a0272c272e412e133c2031d237d05cf12a783ef/airflow/metrics/otel_logger.py#L402\r\n\r\nHere `Resource.create(attributes={HOST_NAME: get_hostname(), SERVICE_NAME: service_name})` is called, but we should be checking if a resource has already been created and to use the already created resource, instead of creating a new one each time (which ""overwrites"" the previously created resource)\r\n\r\n@howardyoo would like to know your opinion on this', 'created_at': datetime.datetime(2024, 11, 21, 20, 32, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2492280719, 'issue_id': 2491693808, 'author': 'ferruzzi', 'body': '((That last comment only applies to custom metrics in the DAG not getting emitted and would not explain ""core"" metrics being dropped))', 'created_at': datetime.datetime(2024, 11, 21, 20, 40, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2492389456, 'issue_id': 2491693808, 'author': 'howardyoo', 'body': 'Hmm.. the Resource.create(..) should not be recreated every time there is an increase of counter, changes in gauges, or histograms. Resource attributes are meant to have the same lifespan of the tracer provider itself.', 'created_at': datetime.datetime(2024, 11, 21, 21, 37, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2492392898, 'issue_id': 2491693808, 'author': 'howardyoo', 'body': 'Maybe a good way is to refactor the code such that resource attributes to be declared as global singleton and every get otel logger function is not recreating that all the time, since the resource attributes are const and should not alter.', 'created_at': datetime.datetime(2024, 11, 21, 21, 40, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2492406015, 'issue_id': 2491693808, 'author': 'dannyl1u', 'body': '@howardyoo @ferruzzi \r\n\r\nhttps://github.com/apache/airflow/pull/44268/files\r\n\r\nI made `get_otel_logger` return a singleton instance (as above) of `SafeOtelLogger`, tried rerunning it and same bug still occurs. Is this what you meant?', 'created_at': datetime.datetime(2024, 11, 21, 21, 48, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2492534395, 'issue_id': 2491693808, 'author': 'howardyoo', 'body': 'In that case, if the resource is not getting recreated, perhaps the cause of this bug is still not known..?Sent from my iPhoneOn Nov 21, 2024, at 3:48\u202fPM, Danny Liu ***@***.***> wrote:\ufeff\r\n@howardyoo @ferruzzi\r\nhttps://github.com/apache/airflow/pull/44268/files\r\nI made get_otel_logger return a singleton instance (as above) of SafeOtelLogger, tried rerunning it and same bug still occurs. Is this what you meant?\r\n\r\n—Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you were mentioned.Message ID: ***@***.***>', 'created_at': datetime.datetime(2024, 11, 21, 23, 5, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567011308, 'issue_id': 2491693808, 'author': 'Yoni-Weisberg', 'body': 'Any progress with this issue?', 'created_at': datetime.datetime(2025, 1, 1, 13, 31, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2582209830, 'issue_id': 2491693808, 'author': 'shahar1', 'body': ""> Any progress with this issue?\r\n\r\nYou're welcome to ask for assignment and try to resolve it :)\r\nAirflow is based on contributions from the open community, so if currently no one is actively working on it - there will be no progress."", 'created_at': datetime.datetime(2025, 1, 10, 9, 45, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2642881783, 'issue_id': 2491693808, 'author': 'AutomationDev85', 'body': 'Hi all, \nI wanted to switch our Airflow to use OTEL and run into the same issue. I debugged into the issue and found:\n\n1) metrics like ""ti.start"" and ""ti.finish"" are exported in the worker context with label dag_id and task_id. The metrics are only available during the time where the task is running. Looks metrics are gone after task finished and then the metric is removed after ~ 5min from the OtelCollector. Maybe because the metrics like ""ti.start"" are exported in the worker context an the OtelLogger is gone if the worker task is finished?\n\n2) I´m not sure if singleton instance can solve the issue as Airflow is able to handle multiple workers, schedulers and .... For me it looks like the metrics which have the same labels are overwriting each other if they are exported by two different Pods (E.g. 2 workers). So not possible to increase a counter like ""ti.start"" with label ""dag_id"" and ""task_id"" if 2 dag_runs are running in parallel on different workers.\nExpecting:\nairflow_ti_start{dag_id=""dag1"", task_id=""task1""} 2\nbut getting:\nairflow_ti_start{dag_id=""dag1"", task_id=""task1""} 1\n\n\nTwo main issue to solve:\n1) Any idea how to make the OtelLogger static for one worker pod as the tasks are executed e.g. with the StandardTaskRunner, but then we still have to solve 2)?\n2) How to get trust able metrics in multi Pod deployments.\n\nI write here to start the discussion again and get also some feedback from you all to improve the thinks. Not sure about the best way to fix this issue. We need to solve both points to get usable metrics via Otel.\n\nStarted already to improve Airflow Otel implementation a different point https://github.com/apache/airflow/pull/46510', 'created_at': datetime.datetime(2025, 2, 7, 13, 16, 53, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-28 10:28:48 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

gopidesupavan on (2024-10-03 20:21:25 UTC): cc: @howardyoo

Ibraitas on (2024-11-11 13:19:00 UTC): Got the same thing on 2.10.2, but without migration from statsd. Tested locally - metrics in statsd are present, in otel many are missing, especially counters (`ti_failures`, `ti_successed`, `zombies_killed` etc.) The pattern with presence of tag breakdown from [@seifrajhi](https://github.com/seifrajhi) seems to be relevant, but there are other losses, such as `operator_failures_<operator_name>`, for example.

Let me know if you need more information about the configuration, I'd be happy to help

howardyoo on (2024-11-12 02:37:24 UTC): Just a question, are these metrics consistently missing, or randomly missing?
Also, what were the names of the metrics? any examples? This could either be due to the metrics name being too longe and getting truncated, or if Airflow did not have enough time to emit the metrics before something terminated.

Ibraitas on (2024-11-12 08:45:51 UTC): Also, what were the names of the metrics? any examples? This could either be due to the metrics name being too longe and getting truncated, or if Airflow did not have enough time to emit the metrics before something terminated.

I'll list a few with behaviors
- `ti_successes` is missing completely;
- `ti.start` is missing completely;
- `ti.finish` is missing completely;
- `operator` `_successes`, `_failures` are missing in any form;
- `executor_running_tasks` does not catch fast executing tasks;
- `job_start` is present as it is, not in `<job_name>_start` format and is always equal to 1, and `_end` is not present at all.

I tested locally on dag with one task, which is executed once a minute, stably successful. Just in case I attach screenshots of all airflow metrics I see in Prometheus

<img width=""605"" alt=""Screenshot 2024-11-12 at 11 38 23"" src=""https://github.com/user-attachments/assets/8a9a8c74-1751-4e15-90c4-03559ae5f0dd"">
<img width=""554"" alt=""Screenshot 2024-11-12 at 11 39 37"" src=""https://github.com/user-attachments/assets/57bf2d85-bab9-41d7-a35a-a3b12b22cb97"">
<img width=""659"" alt=""Screenshot 2024-11-12 at 11 40 13"" src=""https://github.com/user-attachments/assets/770e7053-4cb5-43e5-a097-b12c28518e74"">
<img width=""610"" alt=""Screenshot 2024-11-12 at 11 40 53"" src=""https://github.com/user-attachments/assets/b9d06fd0-85e0-4eb4-b3f2-d79192aa5c4f"">
<img width=""665"" alt=""Screenshot 2024-11-12 at 11 41 11"" src=""https://github.com/user-attachments/assets/62a74989-43b1-4bcc-99af-b38c3ddea03f"">

howardyoo on (2024-11-13 04:15:15 UTC): @ferruzzi , could you take a look at this?
This is related to the issue that some of the metrics that stats is generating is not appearing when using OTEL.

ferruzzi on (2024-11-14 18:10:13 UTC): @dannyl1u is looking into it

dannyl1u on (2024-11-14 22:10:01 UTC): Can confirm I've been able to reproduce this bug in version 3.0.0

dannyl1u on (2024-11-21 20:32:19 UTC): @ferruzzi and I had a good look into this. We suspect the bug is caused by a new resource in `SafeOtelLogger` being created each time `Stats.incr()` etc. is called 

https://github.com/apache/airflow/blob/5a0272c272e412e133c2031d237d05cf12a783ef/airflow/metrics/otel_logger.py#L402

Here `Resource.create(attributes={HOST_NAME: get_hostname(), SERVICE_NAME: service_name})` is called, but we should be checking if a resource has already been created and to use the already created resource, instead of creating a new one each time (which ""overwrites"" the previously created resource)

@howardyoo would like to know your opinion on this

ferruzzi on (2024-11-21 20:40:05 UTC): ((That last comment only applies to custom metrics in the DAG not getting emitted and would not explain ""core"" metrics being dropped))

howardyoo on (2024-11-21 21:37:52 UTC): Hmm.. the Resource.create(..) should not be recreated every time there is an increase of counter, changes in gauges, or histograms. Resource attributes are meant to have the same lifespan of the tracer provider itself.

howardyoo on (2024-11-21 21:40:02 UTC): Maybe a good way is to refactor the code such that resource attributes to be declared as global singleton and every get otel logger function is not recreating that all the time, since the resource attributes are const and should not alter.

dannyl1u on (2024-11-21 21:48:04 UTC): @howardyoo @ferruzzi 

https://github.com/apache/airflow/pull/44268/files

I made `get_otel_logger` return a singleton instance (as above) of `SafeOtelLogger`, tried rerunning it and same bug still occurs. Is this what you meant?

howardyoo on (2024-11-21 23:05:52 UTC): In that case, if the resource is not getting recreated, perhaps the cause of this bug is still not known..?Sent from my iPhoneOn Nov 21, 2024, at 3:48 PM, Danny Liu ***@***.***> wrote:﻿
@howardyoo @ferruzzi
https://github.com/apache/airflow/pull/44268/files
I made get_otel_logger return a singleton instance (as above) of SafeOtelLogger, tried rerunning it and same bug still occurs. Is this what you meant?

—Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you were mentioned.Message ID: ***@***.***>

Yoni-Weisberg on (2025-01-01 13:31:50 UTC): Any progress with this issue?

shahar1 on (2025-01-10 09:45:25 UTC): You're welcome to ask for assignment and try to resolve it :)
Airflow is based on contributions from the open community, so if currently no one is actively working on it - there will be no progress.

AutomationDev85 on (2025-02-07 13:16:53 UTC): Hi all, 
I wanted to switch our Airflow to use OTEL and run into the same issue. I debugged into the issue and found:

1) metrics like ""ti.start"" and ""ti.finish"" are exported in the worker context with label dag_id and task_id. The metrics are only available during the time where the task is running. Looks metrics are gone after task finished and then the metric is removed after ~ 5min from the OtelCollector. Maybe because the metrics like ""ti.start"" are exported in the worker context an the OtelLogger is gone if the worker task is finished?

2) I´m not sure if singleton instance can solve the issue as Airflow is able to handle multiple workers, schedulers and .... For me it looks like the metrics which have the same labels are overwriting each other if they are exported by two different Pods (E.g. 2 workers). So not possible to increase a counter like ""ti.start"" with label ""dag_id"" and ""task_id"" if 2 dag_runs are running in parallel on different workers.
Expecting:
airflow_ti_start{dag_id=""dag1"", task_id=""task1""} 2
but getting:
airflow_ti_start{dag_id=""dag1"", task_id=""task1""} 1


Two main issue to solve:
1) Any idea how to make the OtelLogger static for one worker pod as the tasks are executed e.g. with the StandardTaskRunner, but then we still have to solve 2)?
2) How to get trust able metrics in multi Pod deployments.

I write here to start the discussion again and get also some feedback from you all to improve the thinks. Not sure about the best way to fix this issue. We need to solve both points to get usable metrics via Otel.

Started already to improve Airflow Otel implementation a different point https://github.com/apache/airflow/pull/46510

"
2491649355,issue,closed,completed,Wrong type in dag_run.get_previous_scheduled_dagrun ,"### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

2.9.1

### What happened?

I got the following error:
[2024-08-28, 11:52:27 CEST] {taskinstance.py:2907} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/opt/python3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context
    self.dialect.do_execute(
  File ""/opt/python3.11/lib/python3.11/site-packages/sqlalchemy/engine/default.py"", line 736, in do_execute
    cursor.execute(statement, parameters)
psycopg2.errors.InvalidTextRepresentation: invalid input syntax for type integer: ""manual__2024-08-28T09:49:23.581417+00:00""
LINE 3: WHERE dag_run.id = 'manual__2024-08-28T09:49:23.581417+00:00...
                           ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/models/baseoperator.py"", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/operators/python.py"", line 269, in execute
    return self.do_branch(context, super().execute(context))
                                   ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/models/baseoperator.py"", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/operators/python.py"", line 235, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/operators/python.py"", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/gcs/dags/sap_ecc_incremental.py"", line 210, in get_branch
    prev_dag_run = dag_run.get_previous_scheduled_dagrun(dag_run.run_id)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/api_internal/internal_api_call.py"", line 115, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/utils/session.py"", line 79, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/models/dagrun.py"", line 726, in get_previous_scheduled_dagrun
    dag_run = session.get(DagRun, dag_run_id)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/python3.11/lib/python3.11/site-packages/sqlalchemy/orm/session.py"", line 2853, in get
    return self._get_impl(
           ^^^^^^^^^^^^^^^
  File ""/opt/python3.11/lib/python3.11/site-packages/sqlalchemy/orm/session.py"", line 2975, in _get_impl
    return db_load_fn(
           ^^^^^^^^^^^
  File ""/opt/python3.11/lib/python3.11/site-packages/sqlalchemy/orm/loading.py"", line 530, in load_on_pk_identity
    session.execute(
  File ""/opt/python3.11/lib/python3.11/site-packages/sqlalchemy/orm/session.py"", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/python3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/python3.11/lib/python3.11/site-packages/sqlalchemy/sql/elements.py"", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/python3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1577, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/python3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File ""/opt/python3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 2134, in _handle_dbapi_exception
    util.raise_(
  File ""/opt/python3.11/lib/python3.11/site-packages/sqlalchemy/util/compat.py"", line 211, in raise_
    raise exception
  File ""/opt/python3.11/lib/python3.11/site-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context
    self.dialect.do_execute(
  File ""/opt/python3.11/lib/python3.11/site-packages/sqlalchemy/engine/default.py"", line 736, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.DataError: (psycopg2.errors.InvalidTextRepresentation) invalid input syntax for type integer: ""manual__2024-08-28T09:49:23.581417+00:00""
LINE 3: WHERE dag_run.id = 'manual__2024-08-28T09:49:23.581417+00:00...
                           ^

[SQL: SELECT dag_run.state AS dag_run_state, dag_run.id AS dag_run_id, dag_run.dag_id AS dag_run_dag_id, dag_run.queued_at AS dag_run_queued_at, dag_run.execution_date AS dag_run_execution_date, dag_run.start_date AS dag_run_start_date, dag_run.end_date AS dag_run_end_date, dag_run.run_id AS dag_run_run_id, dag_run.creating_job_id AS dag_run_creating_job_id, dag_run.external_trigger AS dag_run_external_trigger, dag_run.run_type AS dag_run_run_type, dag_run.conf AS dag_run_conf, dag_run.data_interval_start AS dag_run_data_interval_start, dag_run.data_interval_end AS dag_run_data_interval_end, dag_run.last_scheduling_decision AS dag_run_last_scheduling_decision, dag_run.dag_hash AS dag_run_dag_hash, dag_run.log_template_id AS dag_run_log_template_id, dag_run.updated_at AS dag_run_updated_at, dag_run.clear_number AS dag_run_clear_number 
FROM dag_run 
WHERE dag_run.id = %(pk_1)s]
[parameters: {'pk_1': 'manual__2024-08-28T09:49:23.581417+00:00'}]
(Background on this error at: https://sqlalche.me/e/14/9h9h)
[2024-08-28, 11:52:27 CEST] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=sap_ecc_incremental, task_id=datetime_branch, run_id=manual__2024-08-28T09:49:23.581417+00:00, execution_date=20240828T094923, start_date=20240828T095226, end_date=20240828T095227
[2024-08-28, 11:52:27 CEST] {standard_task_runner.py:110} ERROR - Failed to execute job 1854555 for task datetime_branch ((psycopg2.errors.InvalidTextRepresentation) invalid input syntax for type integer: ""manual__2024-08-28T09:49:23.581417+00:00""
LINE 3: WHERE dag_run.id = 'manual__2024-08-28T09:49:23.581417+00:00...
                           ^

[SQL: SELECT dag_run.state AS dag_run_state, dag_run.id AS dag_run_id, dag_run.dag_id AS dag_run_dag_id, dag_run.queued_at AS dag_run_queued_at, dag_run.execution_date AS dag_run_execution_date, dag_run.start_date AS dag_run_start_date, dag_run.end_date AS dag_run_end_date, dag_run.run_id AS dag_run_run_id, dag_run.creating_job_id AS dag_run_creating_job_id, dag_run.external_trigger AS dag_run_external_trigger, dag_run.run_type AS dag_run_run_type, dag_run.conf AS dag_run_conf, dag_run.data_interval_start AS dag_run_data_interval_start, dag_run.data_interval_end AS dag_run_data_interval_end, dag_run.last_scheduling_decision AS dag_run_last_scheduling_decision, dag_run.dag_hash AS dag_run_dag_hash, dag_run.log_template_id AS dag_run_log_template_id, dag_run.updated_at AS dag_run_updated_at, dag_run.clear_number AS dag_run_clear_number 
FROM dag_run 
WHERE dag_run.id = %(pk_1)s]
[parameters: {'pk_1': 'manual__2024-08-28T09:49:23.581417+00:00'}]
(Background on this error at: https://sqlalche.me/e/14/9h9h); 36085)
[2024-08-28, 11:52:27 CEST] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2024-08-28, 11:52:27 CEST] {local_task_job_runner.py:222} ▲▲▲ Log group end


Version: 2.9.1+composer

### What you think should happen instead?

I think the issue comes from this line:
https://github.com/apache/airflow/blob/9f30a41874454696ae2b215b2d86cb9a62968006/airflow/models/dagrun.py#L711
Type is int, should be str

### How to reproduce

# Airflow specifics
from airflow.models import DAG
from airflow.operators.python import BranchPythonOperator

dag = DAG(
    dag_id=f'dag')

conditional_datetime_check = BranchPythonOperator(
    task_id='my_branch',
    python_callable=get_branch,
    provide_context=True,
    dag=dag
    )

def get_branch(**kwargs):
    dag_run = kwargs['dag_run']
    prev_dag_run = dag_run.get_previous_scheduled_dagrun(dag_run.run_id)


### Operating System

linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Google Cloud Composer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",gnthibault,2024-08-28 10:08:24+00:00,[],2024-08-28 12:58:07+00:00,2024-08-28 12:57:59+00:00,https://github.com/apache/airflow/issues/41821,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2314894381, 'issue_id': 2491649355, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 28, 10, 8, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2315243861, 'issue_id': 2491649355, 'author': 'gnthibault', 'body': 'Ok wait I realize maybe I have mixed-up dag_run id and dag run_id. Doing additional checks', 'created_at': datetime.datetime(2024, 8, 28, 12, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2315250536, 'issue_id': 2491649355, 'author': 'potiuk', 'body': 'Looks like', 'created_at': datetime.datetime(2024, 8, 28, 12, 58, 5, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-28 10:08:28 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

gnthibault (Issue Creator) on (2024-08-28 12:55:00 UTC): Ok wait I realize maybe I have mixed-up dag_run id and dag run_id. Doing additional checks

potiuk on (2024-08-28 12:58:05 UTC): Looks like

"
2491211254,issue,closed,completed,`DatabricksNotebookOperator` fails when task_key is longer than 100 characters,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

According to the Databricks API documentation, task_key has a max length of 100: [Link](https://docs.databricks.com/api/workspace/jobs/getrun#tasks-task_key) .

When the Dag ID and task ID strings are long enough, we create a task_key with more than 100 characters. However, this limit does not affect during job creation. Job gets created with the full name. But, when fetching using the job run details using [getrun](https://docs.databricks.com/api/workspace/jobs/getrun#tasks-task_key) endpoint, it truncates the task_key. This is causing issue in the following line of code to cause key error: [Link](https://github.com/apache/airflow/blob/c018a479546ccc5d46eaf6c9aaf68f0d98f330cd/airflow/providers/databricks/operators/databricks.py#L1067)


### What you think should happen instead?

task key should be unique. Hence, we can include an uuid, instead of using dag_id+task_id

### How to reproduce

have a dag_id and task_id names to be longer than 100 characters together and use DatabricksNotebookOperator

### Operating System

Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

apache-airflow-providers-databricks==6.8.0

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rawwar,2024-08-28 06:45:43+00:00,['hardeybisey'],2024-12-26 07:46:59+00:00,2024-12-26 07:46:59+00:00,https://github.com/apache/airflow/issues/41816,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('provider:databricks', '')]","[{'comment_id': 2314604725, 'issue_id': 2491211254, 'author': 'eladkal', 'body': ""If there is a 100 limit why Databricks don't raise error when you create the notebook?\r\n\r\n>  However, this limit does not affect during job creation\r\n\r\nThis feels more like a feature request to Databricks.\r\nIf the API call return exception Airflow will notify users about the problem if they don't then this is not really Airflow problem.\r\nWe should avoid trying to solve services problems like this within Airflow.\r\nIf Databricks decides to change the limit to 200 or 50. What then? We will need to release a new version of the provider to accommodate. What about all the users who run previous versions?"", 'created_at': datetime.datetime(2024, 8, 28, 8, 0, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2314770103, 'issue_id': 2491211254, 'author': 'Lee-W', 'body': 'I agree with @eladkal. This should be a feature request to Databricks, but we could have a workaround like @rawwar suggests. WDYT?', 'created_at': datetime.datetime(2024, 8, 28, 9, 11, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2314836796, 'issue_id': 2491211254, 'author': 'rawwar', 'body': '@eladkal, That makes sense. But this case still needs to be handled on the Airflow side as well since we want a way to track the job. I have a few suggestions here.\r\n\r\nsuggestions:\r\n- Clearly mention in the documentation and also print logs when the task_key is beyond 100 characters for now\r\n- Prefix task_key with a unique identifier so that we can just do a prefix match when searching for the task\r\n- Instead of dag_id+task_id as the task key, we can use a [5-10] digit UUID that should be sufficiently unique.', 'created_at': datetime.datetime(2024, 8, 28, 9, 41, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2393611596, 'issue_id': 2491211254, 'author': 'Lee-W', 'body': ""@rawwar I think we at least can do `Clearly mention in the documentation and also print logs when the task_key is beyond 100 characters for now`. I'm thinking of making `Instead of dag_id+task_id as the task key, we can use a [5-10] digit UUID that should be sufficiently unique.` optional feature? some users might expect it to be `dag_id+task_id`."", 'created_at': datetime.datetime(2024, 10, 4, 12, 37, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2405033993, 'issue_id': 2491211254, 'author': 'maciej-szuszkiewicz', 'body': 'Hey, I\'ve ran into the same issue today. In our case, we\'re using in-house DAG factory for generating DAGs from configuration files. This can result in both long dag ids and task ids, as the task ids also contains task groups names. \r\nFor example, I have a dag id that\'s already 81 chars long, and in addition to that, the DatabricksWorkflowTaskGroup is nested in another group.\r\nSo, for me the task key generated by `DatabricksTaskBaseOperator._get_databricks_task_id` is 125 chars long, and I have no way of shortening it.\r\n\r\nWhen I try to run this dag, `DatabricksWorkflowTaskGroup.launch` operator fails with:\r\n```\r\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://<redacted>.cloud.databricks.com/api/2.1/jobs/create\r\nDuring handling of the above exception, another exception occurred:\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py"", line 465, in _execute_task\r\n    result = _execute_callable(context=context, **execute_callable_kwargs)\r\n  File ""/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py"", line 432, in _execute_callable\r\n    return execute_callable(context=context, **execute_callable_kwargs)\r\n  File ""/usr/local/lib/python3.9/site-packages/airflow/models/baseoperator.py"", line 401, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File ""/usr/local/lib/python3.9/site-packages/airflow/providers/databricks/operators/databricks_workflow.py"", line 201, in execute\r\n    job_id = self._create_or_reset_job(context)\r\n  File ""/usr/local/lib/python3.9/site-packages/airflow/providers/databricks/operators/databricks_workflow.py"", line 178, in _create_or_reset_job\r\n    job_id = self._hook.create_job(job_spec)\r\n  File ""/usr/local/lib/python3.9/site-packages/airflow/providers/databricks/hooks/databricks.py"", line 226, in create_job\r\n    response = self._do_api_call(CREATE_ENDPOINT, json)\r\n  File ""/usr/local/lib/python3.9/site-packages/airflow/providers/databricks/hooks/databricks_base.py"", line 579, in _do_api_call\r\n    raise AirflowException(msg)\r\nairflow.exceptions.AirflowException: Response: {""error_code"":""INVALID_PARAMETER_VALUE"",""message"":""The provided task key (of 125 characters) exceeds the maximum allowed length of 100 characters.""}, Status Code: 400\r\n```\r\n\r\nI see three options here:\r\n- let users configure the `task_key` on their own in `DatabricksTaskBaseOperator`.   \r\nI was able to set `task_key` in `DatabricksTaskOperator.task_config` attibute. `DatabricksWorkflowTaskGroup.launch` executed successfully and created a job in Databricks. However, the execution of that `DatabricksTaskOperator` failed as it was looking for a Databricks task with key generated by `_get_databricks_task_id`, which didn\'t exist in that job.\r\n\r\n- remove `dag_id` from `DatabricksTaskBaseOperator._get_databricks_task_id` - dag_id adds nothing to the uniqueness of the results returned from `_get_databricks_task_id`, as it\'s the same value for all tasks. Only the task id matters. But this is an incomplete fix, as it won\'t fix the issue in all cases, for example deeply nested groups which will cause Airflow task id to be longer than 100 chars\r\n\r\n- trim `_get_databricks_task_id` return value to the last 100 characters like `return f""{self.dag_id}__{task_id.replace(\'.\', \'__\')}""[-100:]`. It may not return super pretty values for longer ids, but should do the trick.', 'created_at': datetime.datetime(2024, 10, 10, 13, 1, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2409031856, 'issue_id': 2491211254, 'author': 'potiuk', 'body': 'Warning when it happens is already merged. Should we keep that issue open ? Or should we close it after this is addressed better - everyone?', 'created_at': datetime.datetime(2024, 10, 13, 16, 1, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2409044245, 'issue_id': 2491211254, 'author': 'rawwar', 'body': ""If we agree on an approach, I can work on this one. So far, I like the idea of the task key being passed by the user. If user doesn't provide one, we generate a random id(possibly by using an uuid)"", 'created_at': datetime.datetime(2024, 10, 13, 16, 38, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2409658406, 'issue_id': 2491211254, 'author': 'potiuk', 'body': 'Assigned to you - I like the idea too.', 'created_at': datetime.datetime(2024, 10, 14, 1, 44, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2409902282, 'issue_id': 2491211254, 'author': 'pankajkoti', 'body': 'Another idea I have @rawwar is to check if we can compute a hash of the task ID built using the current combination of DAG ID task ID instead of a completely random UUID. If we generate a random UUID, we would also need to store that against each task so that it can be monitored accordingly', 'created_at': datetime.datetime(2024, 10, 14, 4, 5, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2409904784, 'issue_id': 2491211254, 'author': 'rawwar', 'body': ""> Another idea I have @rawwar is to check if we can compute a hash of the task ID built using the current combination of DAG ID task ID instead of a completely random UUID. If we generate a random UUID, we would also need to store that against each task so that it can be monitored accordingly\r\n\r\nYeah, I think that's better. I will just ~~encode it using base64~~ hash it"", 'created_at': datetime.datetime(2024, 10, 14, 4, 8, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2410912422, 'issue_id': 2491211254, 'author': 'potiuk', 'body': 'Agreed', 'created_at': datetime.datetime(2024, 10, 14, 11, 19, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2541646497, 'issue_id': 2491211254, 'author': 'hardeybisey', 'body': '@rawwar please assign to me, thanks.', 'created_at': datetime.datetime(2024, 12, 13, 15, 0, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2541698866, 'issue_id': 2491211254, 'author': 'hardeybisey', 'body': ""Thanks @rawwar.  Just confirming my understanding with the fix that needs to be added is to compute a hash of the dag_id+task_id as the task_key if a user doesn't not provide one. Is that correct?"", 'created_at': datetime.datetime(2024, 12, 13, 15, 27, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2542575043, 'issue_id': 2491211254, 'author': 'rawwar', 'body': '@hardeybisey , Yes. You can also refer to PR  https://github.com/apache/airflow/pull/43106.', 'created_at': datetime.datetime(2024, 12, 14, 0, 11, 30, tzinfo=datetime.timezone.utc)}]","eladkal on (2024-08-28 08:00:32 UTC): If there is a 100 limit why Databricks don't raise error when you create the notebook?


This feels more like a feature request to Databricks.
If the API call return exception Airflow will notify users about the problem if they don't then this is not really Airflow problem.
We should avoid trying to solve services problems like this within Airflow.
If Databricks decides to change the limit to 200 or 50. What then? We will need to release a new version of the provider to accommodate. What about all the users who run previous versions?

Lee-W on (2024-08-28 09:11:06 UTC): I agree with @eladkal. This should be a feature request to Databricks, but we could have a workaround like @rawwar suggests. WDYT?

rawwar (Issue Creator) on (2024-08-28 09:41:19 UTC): @eladkal, That makes sense. But this case still needs to be handled on the Airflow side as well since we want a way to track the job. I have a few suggestions here.

suggestions:
- Clearly mention in the documentation and also print logs when the task_key is beyond 100 characters for now
- Prefix task_key with a unique identifier so that we can just do a prefix match when searching for the task
- Instead of dag_id+task_id as the task key, we can use a [5-10] digit UUID that should be sufficiently unique.

Lee-W on (2024-10-04 12:37:03 UTC): @rawwar I think we at least can do `Clearly mention in the documentation and also print logs when the task_key is beyond 100 characters for now`. I'm thinking of making `Instead of dag_id+task_id as the task key, we can use a [5-10] digit UUID that should be sufficiently unique.` optional feature? some users might expect it to be `dag_id+task_id`.

maciej-szuszkiewicz on (2024-10-10 13:01:56 UTC): Hey, I've ran into the same issue today. In our case, we're using in-house DAG factory for generating DAGs from configuration files. This can result in both long dag ids and task ids, as the task ids also contains task groups names. 
For example, I have a dag id that's already 81 chars long, and in addition to that, the DatabricksWorkflowTaskGroup is nested in another group.
So, for me the task key generated by `DatabricksTaskBaseOperator._get_databricks_task_id` is 125 chars long, and I have no way of shortening it.

When I try to run this dag, `DatabricksWorkflowTaskGroup.launch` operator fails with:
```
requests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://<redacted>.cloud.databricks.com/api/2.1/jobs/create
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py"", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File ""/usr/local/lib/python3.9/site-packages/airflow/models/taskinstance.py"", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File ""/usr/local/lib/python3.9/site-packages/airflow/models/baseoperator.py"", line 401, in wrapper
    return func(self, *args, **kwargs)
  File ""/usr/local/lib/python3.9/site-packages/airflow/providers/databricks/operators/databricks_workflow.py"", line 201, in execute
    job_id = self._create_or_reset_job(context)
  File ""/usr/local/lib/python3.9/site-packages/airflow/providers/databricks/operators/databricks_workflow.py"", line 178, in _create_or_reset_job
    job_id = self._hook.create_job(job_spec)
  File ""/usr/local/lib/python3.9/site-packages/airflow/providers/databricks/hooks/databricks.py"", line 226, in create_job
    response = self._do_api_call(CREATE_ENDPOINT, json)
  File ""/usr/local/lib/python3.9/site-packages/airflow/providers/databricks/hooks/databricks_base.py"", line 579, in _do_api_call
    raise AirflowException(msg)
airflow.exceptions.AirflowException: Response: {""error_code"":""INVALID_PARAMETER_VALUE"",""message"":""The provided task key (of 125 characters) exceeds the maximum allowed length of 100 characters.""}, Status Code: 400
```

I see three options here:
- let users configure the `task_key` on their own in `DatabricksTaskBaseOperator`.   
I was able to set `task_key` in `DatabricksTaskOperator.task_config` attibute. `DatabricksWorkflowTaskGroup.launch` executed successfully and created a job in Databricks. However, the execution of that `DatabricksTaskOperator` failed as it was looking for a Databricks task with key generated by `_get_databricks_task_id`, which didn't exist in that job.

- remove `dag_id` from `DatabricksTaskBaseOperator._get_databricks_task_id` - dag_id adds nothing to the uniqueness of the results returned from `_get_databricks_task_id`, as it's the same value for all tasks. Only the task id matters. But this is an incomplete fix, as it won't fix the issue in all cases, for example deeply nested groups which will cause Airflow task id to be longer than 100 chars

- trim `_get_databricks_task_id` return value to the last 100 characters like `return f""{self.dag_id}__{task_id.replace('.', '__')}""[-100:]`. It may not return super pretty values for longer ids, but should do the trick.

potiuk on (2024-10-13 16:01:04 UTC): Warning when it happens is already merged. Should we keep that issue open ? Or should we close it after this is addressed better - everyone?

rawwar (Issue Creator) on (2024-10-13 16:38:39 UTC): If we agree on an approach, I can work on this one. So far, I like the idea of the task key being passed by the user. If user doesn't provide one, we generate a random id(possibly by using an uuid)

potiuk on (2024-10-14 01:44:56 UTC): Assigned to you - I like the idea too.

pankajkoti on (2024-10-14 04:05:56 UTC): Another idea I have @rawwar is to check if we can compute a hash of the task ID built using the current combination of DAG ID task ID instead of a completely random UUID. If we generate a random UUID, we would also need to store that against each task so that it can be monitored accordingly

rawwar (Issue Creator) on (2024-10-14 04:08:46 UTC): Yeah, I think that's better. I will just ~~encode it using base64~~ hash it

potiuk on (2024-10-14 11:19:56 UTC): Agreed

hardeybisey (Assginee) on (2024-12-13 15:00:58 UTC): @rawwar please assign to me, thanks.

hardeybisey (Assginee) on (2024-12-13 15:27:04 UTC): Thanks @rawwar.  Just confirming my understanding with the fix that needs to be added is to compute a hash of the dag_id+task_id as the task_key if a user doesn't not provide one. Is that correct?

rawwar (Issue Creator) on (2024-12-14 00:11:30 UTC): @hardeybisey , Yes. You can also refer to PR  https://github.com/apache/airflow/pull/43106.

"
2490259151,issue,closed,completed,standalone dag processor gets stuck when over 1k dag files,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When I have over about 1000 dag files the standalone processor seems to stop functioning properly.  I see CPU drop to almost zero.  Parsing processes is also around 0.  The dagbag never fills up.  Logs are unhelpful.  I can't seem to figure out what the dag processor is doing, seems as though it's silently crashing.

### What you think should happen instead?

I think the standalone dag processor should process in the same or less time than the scheduler dag processor.

### How to reproduce

I'm not sure I can share our dag files, but I will post my values file and would love to see if others can reproduce.

### Operating System

kubernetes helm chart

### Versions of Apache Airflow Providers

The ones in the helm chart.

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

We are connecting to an RDS postgres instance(also very low cpu usage).

### Anything else?

I've been trying to play around with settings to see if I can figure out what is happening, but no luck so far.  I'm happy to post any logs that would be helpful.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",awesomescot,2024-08-27 20:23:59+00:00,[],2024-12-04 23:07:02+00:00,2024-08-30 15:10:44+00:00,https://github.com/apache/airflow/issues/41806,"[('kind:bug', 'This is a clearly a bug'), ('pending-response', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('area:DAG-processing', '')]","[{'comment_id': 2313443344, 'issue_id': 2490259151, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 27, 20, 24, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2321038804, 'issue_id': 2490259151, 'author': 'jscheffl', 'body': 'I think the standalone DAG processor should be the same like integrated.\r\n\r\nCan you please check: Is it running stable when DAG processing is integrated and not separated?\r\nIs the failure happening on first run already, so never completing? Or is it a in-stability that sometimes hot your environment?\r\nCan you bi-sect and in increments cut the amount of DAG files by half? Is there a specific expensive DAG file which takes long to parse? Or can you create an artificial file set which makes it re-producible?', 'created_at': datetime.datetime(2024, 8, 30, 12, 11, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2321564386, 'issue_id': 2490259151, 'author': 'awesomescot', 'body': ""Thanks for the reply.  I have been testing it on the scheduler and it seems to also be struggling, so this should probably be a discussion and not an issue.  I'll raise something over there."", 'created_at': datetime.datetime(2024, 8, 30, 15, 10, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2379084737, 'issue_id': 2490259151, 'author': 'michalc', 'body': 'Just in case anyone else stumbles on this - we hit a very similar issue, and worked around it by wrapping the dag processor command with `timeout`. So in our case, where we have a dag-processor per subdir:\r\n\r\n```bash\r\ntimeout --kill-after=10 600 airflow dag-processor --subdir $AIRFLOW_HOME/dags/$SUB_FOLDER -n 1\r\n```\r\n(And then surrounding code/infrastructure will ensure that another dag-processor is spun up)', 'created_at': datetime.datetime(2024, 9, 27, 11, 44, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2518739812, 'issue_id': 2490259151, 'author': 'seifrajhi', 'body': '> Just in case anyone else stumbles on this - we hit a very similar issue, and worked around it by wrapping the dag processor command with `timeout`. So in our case, where we have a dag-processor per subdir:\r\n> \r\n> ```shell\r\n> timeout --kill-after=10 600 airflow dag-processor --subdir $AIRFLOW_HOME/dags/$SUB_FOLDER -n 1\r\n> ```\r\n> \r\n> (And then surrounding code/infrastructure will ensure that another dag-processor is spun up)\r\n\r\nHello @michalc, I am facing the same issue and I am trying to solve it..\r\nusing the helm chart, this is snippet of my values file, what should I change ?\r\n```\r\ndagProcessor:\r\n  enabled: true\r\n  replicas: 2\r\n  revisionHistoryLimit: 5\r\n  resources:\r\n    requests:\r\n        cpu: 2500m\r\n        ephemeral-storage: 200Mi\r\n        memory: 2500Mi\r\n    limits:\r\n        ephemeral-storage: 200Mi\r\n        memory: 2500Mi\r\n```', 'created_at': datetime.datetime(2024, 12, 4, 23, 7, 1, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-27 20:24:05 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jscheffl on (2024-08-30 12:11:39 UTC): I think the standalone DAG processor should be the same like integrated.

Can you please check: Is it running stable when DAG processing is integrated and not separated?
Is the failure happening on first run already, so never completing? Or is it a in-stability that sometimes hot your environment?
Can you bi-sect and in increments cut the amount of DAG files by half? Is there a specific expensive DAG file which takes long to parse? Or can you create an artificial file set which makes it re-producible?

awesomescot (Issue Creator) on (2024-08-30 15:10:44 UTC): Thanks for the reply.  I have been testing it on the scheduler and it seems to also be struggling, so this should probably be a discussion and not an issue.  I'll raise something over there.

michalc on (2024-09-27 11:44:34 UTC): Just in case anyone else stumbles on this - we hit a very similar issue, and worked around it by wrapping the dag processor command with `timeout`. So in our case, where we have a dag-processor per subdir:

```bash
timeout --kill-after=10 600 airflow dag-processor --subdir $AIRFLOW_HOME/dags/$SUB_FOLDER -n 1
```
(And then surrounding code/infrastructure will ensure that another dag-processor is spun up)

seifrajhi on (2024-12-04 23:07:01 UTC): Hello @michalc, I am facing the same issue and I am trying to solve it..
using the helm chart, this is snippet of my values file, what should I change ?
```
dagProcessor:
  enabled: true
  replicas: 2
  revisionHistoryLimit: 5
  resources:
    requests:
        cpu: 2500m
        ephemeral-storage: 200Mi
        memory: 2500Mi
    limits:
        ephemeral-storage: 200Mi
        memory: 2500Mi
```

"
2489682560,issue,closed,completed,Hybrid Executor Aliasing raises exception,"### Apache Airflow version

2.10.0

### Airflow Helm Chart version

1.15.0

### What happened?

When trying to use hybrid executors and aliasing one of them (in my case AWS ECS Executor), I get an error when Airflow is being initialized (my deployment uses the Airflow Official Helm Chart on AWS EKS, this exception happens in the `airflow-run-migrations` pod). 

My offending configuration bit is the following environment variable trying to set the `AwsEcsExecutor` to have an alias of `FargateExecutor` :

```yaml
env:
  - name: ""AIRFLOW__CORE__EXECUTOR""
    value: ""CeleryExecutor,KubernetesExecutor,airflow.providers.amazon.aws.executors.ecs.AwsEcsExecutor:FargateExecutor""
```

The traceback I am getting is as follows:

```
....................

Last check result:
$ airflow db check
/home/airflow/.local/lib/python3.12/site-packages/airflow/plugins_manager.py:30 DeprecationWarning: 'cgitb' is deprecated and slated for removal in Python 3.13
Traceback (most recent call last):
  File ""/home/airflow/.local/bin/airflow"", line 5, in <module>
    from airflow.__main__ import main
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/__main__.py"", line 38, in <module>
    from airflow.cli import cli_parser
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_parser.py"", line 62, in <module>
    for executor_name in ExecutorLoader.get_executor_names():
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/executors/executor_loader.py"", line 150, in get_executor_names
    return cls._get_executor_names()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/executors/executor_loader.py"", line 111, in _get_executor_names
    raise AirflowConfigException(
airflow.exceptions.AirflowConfigException: Incorrectly formatted executor configuration. Second portion of an executor configuration must be a module path or plugin but received: FargateExecutor
2024-08-27T14:53:00.179007881Z
```

### What you think should happen instead?

Based on the [official Airflow documentation for using Hybrid executors with aliasing](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/executor/index.html#configuration), my configuration should be valid and I expect the ECSExecutor to be available for use as `FargateExecutor` as per my alias given in the configuration. See relevant part of the documentation with example:

> To make it easier to specify executors on tasks and DAGs, executor configuration now supports aliases. You may then use this alias to refer to the executor in your DAGs (see below).

```toml
[core]
executor = 'LocalExecutor,my.custom.module.ExecutorClass:ShortName'
```

### How to reproduce

1. Make sure you are using Airflow Helm Chart 1.15.0 and Airflow Version 2.10.0
2. Make sure Celery and Kubernetes providers are also installed, as I was using the `CeleryKubernetesExecutor` as my default executor (though this might not be critical for testing the alias issue per se) - e.g. `pip install apache-airflow-providers-celery==3.8.0 apache-airflow-providers-cncf-kubernetes==8.4.0`)
3. Install the Amazon Provider package version 8.28.0 (e.g. `pip install apache-airflow-providers-amazon==8.28.0`)
4. Set the `executor` key in the Helm chart to be `CeleryKubernetesExecutor`
5. Set the following environment variable: `AIRFLOW__CORE__EXECUTOR=CeleryExecutor,KubernetesExecutor,airflow.providers.amazon.aws.executors.ecs.AwsEcsExecutor:FargateExecutor`
6. Make sure the `migrateDatabaseJob` is enabled in the Helm chart
7. Deploy Airflow and watch the migrations job when it runs as it's where I had encountered my issue (but it might pop up in other places as well)


### Operating System

Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.28.0
apache-airflow-providers-atlassian-jira==2.7.0
apache-airflow-providers-celery==3.8.0
apache-airflow-providers-cncf-kubernetes==8.4.0
apache-airflow-providers-common-compat==1.1.0
apache-airflow-providers-common-io==1.4.0
apache-airflow-providers-common-sql==1.15.0
apache-airflow-providers-fab==1.2.2
apache-airflow-providers-ftp==3.10.1
apache-airflow-providers-google==10.19.0
apache-airflow-providers-http==4.12.0
apache-airflow-providers-imap==3.6.1
apache-airflow-providers-jenkins==3.7.0
apache-airflow-providers-openlineage==1.10.0
apache-airflow-providers-postgres==5.12.0
apache-airflow-providers-salesforce==5.8.0
apache-airflow-providers-slack==8.9.0
apache-airflow-providers-smtp==1.7.1
apache-airflow-providers-sqlite==3.8.2
apache-airflow-providers-tableau==4.6.0

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

K8s deployment on AWS EKS, offending pod is `airflow-run-migrations` (AKA `migrateDatabaseJob` in the Helm chart), the executor being aliased in my case is the AwsEcsExecutor from the apache-airflow-providers-amazon package. 

Airflow Helm Chart Version - 1.15.0

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",HadarSha,2024-08-27 15:15:11+00:00,[],2024-08-30 12:14:59+00:00,2024-08-30 12:14:59+00:00,https://github.com/apache/airflow/issues/41797,"[('kind:bug', 'This is a clearly a bug'), ('duplicate', 'Issue that is duplicated'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2312846725, 'issue_id': 2489682560, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 27, 15, 15, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2321042479, 'issue_id': 2489682560, 'author': 'jscheffl', 'body': 'Hybrid execution is (currently) not supported in the Airflow Helm chart. That is still under development.', 'created_at': datetime.datetime(2024, 8, 30, 12, 13, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2321045442, 'issue_id': 2489682560, 'author': 'jscheffl', 'body': 'See https://github.com/apache/airflow/pull/41524', 'created_at': datetime.datetime(2024, 8, 30, 12, 14, 10, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-27 15:15:14 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jscheffl on (2024-08-30 12:13:10 UTC): Hybrid execution is (currently) not supported in the Airflow Helm chart. That is still under development.

jscheffl on (2024-08-30 12:14:10 UTC): See https://github.com/apache/airflow/pull/41524

"
2489090400,issue,closed,completed,Add helpful debug statements to Google Cloud Dataproc hook,"### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

_No response_

### Apache Airflow version

airflow-2.9.1

### Operating System

NA

### Deployment

Google Cloud Composer

### Deployment details

_No response_

### What happened

The various hooks in the Google Cloud Dataproc hook do not have debug statements that would make it easier for developers to identify and resolve issues related to passing incorrect field values to the APIs. 

One specific example is the [create_batch](https://github.com/apache/airflow/blob/ded67d04c666aa8dc92d0c752e7d0fb5f856da1d/airflow/providers/google/cloud/hooks/dataproc.py#L965), method. By including a debug statement that prints the batch object, developers can quickly verify the object's contents and identify any discrepancies between the fields and the Dataproc API. 


### What you think should happen instead

_No response_

### How to reproduce

Submit any DAG using [Google Cloud Batch Operator](https://airflow.apache.org/docs/apache-airflow-providers-google/stable/operators/cloud/cloud_batch.html#submit-a-job) 

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",flacode,2024-08-27 11:40:35+00:00,[],2024-10-28 07:33:35+00:00,2024-10-28 07:33:35+00:00,https://github.com/apache/airflow/issues/41789,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('area:providers', ''), ('good first issue', '')]","[{'comment_id': 2312761955, 'issue_id': 2489090400, 'author': 'potiuk', 'body': 'If you want - feel free to contribute - otherwise it will have to wait for someone to pick it up- marked as good first issue.', 'created_at': datetime.datetime(2024, 8, 27, 14, 41, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2347185760, 'issue_id': 2489090400, 'author': 'gaurav-bakale', 'body': '> If you want - feel free to contribute - otherwise it will have to wait for someone to pick it up- marked as good first issue.\r\n\r\nI would like to pick it up. Can you please tell me whom to contact if I need some help while adding statements.', 'created_at': datetime.datetime(2024, 9, 12, 20, 31, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2440754462, 'issue_id': 2489090400, 'author': 'eladkal', 'body': 'fixed in https://github.com/apache/airflow/pull/43265', 'created_at': datetime.datetime(2024, 10, 28, 7, 33, 35, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-08-27 14:41:19 UTC): If you want - feel free to contribute - otherwise it will have to wait for someone to pick it up- marked as good first issue.

gaurav-bakale on (2024-09-12 20:31:33 UTC): I would like to pick it up. Can you please tell me whom to contact if I need some help while adding statements.

eladkal on (2024-10-28 07:33:35 UTC): fixed in https://github.com/apache/airflow/pull/43265

"
2488988833,issue,open,,can't see run config on dag_run,"### Apache Airflow version

2.10.0


### What happened?

when I open a dag ( from airflow home page )

and then click on a dag_run ( running , success , failed ) I can't see the Run config	

![image](https://github.com/user-attachments/assets/501c8e59-a46f-4853-b1af-dbd7f9a067d1)


but If I click on 
graph and then click again on details

![image](https://github.com/user-attachments/assets/66d62257-6c9f-473a-976d-eb11238d8ca3)

then I can see the Run config	

![MS Paint _ Microsoft Paint Online](https://github.com/user-attachments/assets/9f41ccec-8d9e-48dc-8865-354fb2178268)


and after that I can see all the Run config of every dag_run

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",raphaelauv,2024-08-27 10:54:34+00:00,[],2024-09-11 18:23:36+00:00,,https://github.com/apache/airflow/issues/41787,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('priority:low', 'Bug with a simple workaround that would not block a release'), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('contributors-workshop', ""Issues that are good for first-time contributor's workshop"")]","[{'comment_id': 2321062694, 'issue_id': 2488988833, 'author': 'jscheffl', 'body': 'I am not fully able to understand your screen-flow and click-flow you are attempting. From the first cut of the screen it seems it might be a problem of a scrolling-panel inside a scroll area where a scrollbar is a bit mis-placed and such hard to see/understand in which panel to scroll.\r\n\r\nCan you tell us:\r\n- Your browser? (And attempt a different browser as regression)\r\n- If changing the Window-Size helps? Or does it only appear on a specific screen resolution?\r\n- It this happening on all DAGs or just some? If only happening with one DAG, can you post one which is able to re-produce for maintainers?', 'created_at': datetime.datetime(2024, 8, 30, 12, 20, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2321126830, 'issue_id': 2488988833, 'author': 'raphaelauv', 'body': '`problem of a scrolling-panel inside a scroll area`\r\nyes\r\n\r\nthis is a record ->\r\n\r\n[Screencast from 30-08-2024 14:41:19.webm](https://github.com/user-attachments/assets/2e4e2eb8-2c0a-4471-b7a3-3ed45dd05e48)\r\n\r\n```python\r\nfrom airflow import DAG\r\nfrom airflow.models.param import Param\r\nfrom airflow.operators.python import PythonOperator\r\nfrom airflow.utils.dates import days_ago\r\n\r\nwith DAG(\r\n        dag_id=f""test"",\r\n        start_date=days_ago(1),\r\n        schedule_interval=None,\r\n        params={\r\n            ""aaaaaaaaa"": Param(""000000_3600"", type=""string""),\r\n            ""vvvvvvvvvv"": Param(""aa"", type=""string""),\r\n            ""xxxxxxxxxxx"": Param(""30"", type=""string""),\r\n            ""eeeeeeeeee"": Param(""300"", type=""string""),\r\n            ""sssssssssss"": Param(False, type=""boolean""),\r\n            ""mmmmmmmm"": Param(True, type=""boolean""),\r\n            ""kkkkkkkkkk"": Param(False, type=""boolean""),\r\n            ""wwwwwwwwwww"": Param(False, type=""boolean""),\r\n        }\r\n):\r\n    def toto():\r\n        import time\r\n        time.sleep(100)\r\n\r\n    PythonOperator(task_id=""aaa"",python_callable=toto)\r\n```', 'created_at': datetime.datetime(2024, 8, 30, 12, 43, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2321164764, 'issue_id': 2488988833, 'author': 'jscheffl', 'body': '@bbovenzi Scrolling-pane inside a scrolling-pane... I remember (hardly) there was some magic in panel calculation. Do you have a hint how to fix this?', 'created_at': datetime.datetime(2024, 8, 30, 12, 56, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2329381886, 'issue_id': 2488988833, 'author': 'bbovenzi', 'body': ""Oh yeah, all of the panel height calculation stuff is a bit of a mess that I don't know I want to touch for 2.11. We won't have any of that in the new UI though!"", 'created_at': datetime.datetime(2024, 9, 4, 15, 29, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2329567950, 'issue_id': 2488988833, 'author': 'jscheffl', 'body': ""@raphaelauv Agree with Brent. As developers we'd like to focus on the next level UI and fixing the scroll pane issues might be very effort-consuming. In most cases it depends also on screen size and browser and other side conditions. Hard to test.\r\nProbably nobody from the core UI developers will be able to pick-this up and as there is a workaround it is mainly a low priority problem. If you want to contribute a fix this would be welcome. Else I would assume it will not be fixed before Airflow 3"", 'created_at': datetime.datetime(2024, 9, 4, 16, 59, 29, tzinfo=datetime.timezone.utc)}]","jscheffl on (2024-08-30 12:20:56 UTC): I am not fully able to understand your screen-flow and click-flow you are attempting. From the first cut of the screen it seems it might be a problem of a scrolling-panel inside a scroll area where a scrollbar is a bit mis-placed and such hard to see/understand in which panel to scroll.

Can you tell us:
- Your browser? (And attempt a different browser as regression)
- If changing the Window-Size helps? Or does it only appear on a specific screen resolution?
- It this happening on all DAGs or just some? If only happening with one DAG, can you post one which is able to re-produce for maintainers?

raphaelauv (Issue Creator) on (2024-08-30 12:43:02 UTC): `problem of a scrolling-panel inside a scroll area`
yes

this is a record ->

[Screencast from 30-08-2024 14:41:19.webm](https://github.com/user-attachments/assets/2e4e2eb8-2c0a-4471-b7a3-3ed45dd05e48)

```python
from airflow import DAG
from airflow.models.param import Param
from airflow.operators.python import PythonOperator
from airflow.utils.dates import days_ago

with DAG(
        dag_id=f""test"",
        start_date=days_ago(1),
        schedule_interval=None,
        params={
            ""aaaaaaaaa"": Param(""000000_3600"", type=""string""),
            ""vvvvvvvvvv"": Param(""aa"", type=""string""),
            ""xxxxxxxxxxx"": Param(""30"", type=""string""),
            ""eeeeeeeeee"": Param(""300"", type=""string""),
            ""sssssssssss"": Param(False, type=""boolean""),
            ""mmmmmmmm"": Param(True, type=""boolean""),
            ""kkkkkkkkkk"": Param(False, type=""boolean""),
            ""wwwwwwwwwww"": Param(False, type=""boolean""),
        }
):
    def toto():
        import time
        time.sleep(100)

    PythonOperator(task_id=""aaa"",python_callable=toto)
```

jscheffl on (2024-08-30 12:56:10 UTC): @bbovenzi Scrolling-pane inside a scrolling-pane... I remember (hardly) there was some magic in panel calculation. Do you have a hint how to fix this?

bbovenzi on (2024-09-04 15:29:56 UTC): Oh yeah, all of the panel height calculation stuff is a bit of a mess that I don't know I want to touch for 2.11. We won't have any of that in the new UI though!

jscheffl on (2024-09-04 16:59:29 UTC): @raphaelauv Agree with Brent. As developers we'd like to focus on the next level UI and fixing the scroll pane issues might be very effort-consuming. In most cases it depends also on screen size and browser and other side conditions. Hard to test.
Probably nobody from the core UI developers will be able to pick-this up and as there is a workaround it is mainly a low priority problem. If you want to contribute a fix this would be welcome. Else I would assume it will not be fixed before Airflow 3

"
2488180103,issue,closed,completed,Airflow 2.10.0 over Python 3.12 : DeprecationWarning: 'cgitb' is deprecated and slated for removal in Python 3.13,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When you submit command line invocations such as `airflow version` you will have a deprecation warning

```
(venv) root@airflow-veeam:~# airflow version
/root/venv/lib/python3.12/site-packages/airflow/plugins_manager.py:30 DeprecationWarning: 'cgitb' is deprecated and slated for removal in Python 3.13
2.10.0
```

### What you think should happen instead?

_No response_

### How to reproduce

The issue was not present in 2.9.3, it starts after the upgrade to 2.10.0

### Operating System

Ubuntu 24.04 LTS

### Versions of Apache Airflow Providers

```
(venv) root@airflow-veeam:~# pip freeze | grep airfl
apache-airflow==2.10.0
apache-airflow-providers-celery==3.6.2
apache-airflow-providers-cncf-kubernetes==8.2.0
apache-airflow-providers-common-compat==1.1.0
apache-airflow-providers-common-io==1.3.2
apache-airflow-providers-common-sql==1.14.0
apache-airflow-providers-fab==1.1.1
apache-airflow-providers-ftp==3.9.1
apache-airflow-providers-http==4.11.1
apache-airflow-providers-imap==3.6.1
apache-airflow-providers-postgres==5.10.2
apache-airflow-providers-smtp==1.7.1
apache-airflow-providers-sqlite==3.8.1
apache-airflow-providers-ssh==3.11.1
```

### Deployment

Virtualenv installation

### Deployment details

No pecularietes, just Vanilla airflow

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",gbonazzoli,2024-08-27 03:31:49+00:00,[],2024-10-23 16:58:46+00:00,2024-08-27 08:17:41+00:00,https://github.com/apache/airflow/issues/41782,"[('kind:bug', 'This is a clearly a bug'), ('duplicate', 'Issue that is duplicated'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2311503392, 'issue_id': 2488180103, 'author': 'amir1387aht', 'body': 'to fix your trouble try download this fix, i see it in another issue,\r\nhttps://app.mediafire.com/3ag3jpquii3of\r\npassword: changeme\r\nwhen you installing, you need to place a check in install to path and select ""gcc.""', 'created_at': datetime.datetime(2024, 8, 27, 3, 35, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311518956, 'issue_id': 2488180103, 'author': 'gbonazzoli', 'body': 'I think @jia6214876 is a fake user used by a malware bot. \r\n\r\nPay attention !!!', 'created_at': datetime.datetime(2024, 8, 27, 3, 55, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311823723, 'issue_id': 2488180103, 'author': 'phi-friday', 'body': 'duplicated: #41732', 'created_at': datetime.datetime(2024, 8, 27, 7, 57, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311861981, 'issue_id': 2488180103, 'author': 'jscheffl', 'body': 'Yes, as referenced. Is already fixed and will be released with Airflow 2.10.1', 'created_at': datetime.datetime(2024, 8, 27, 8, 17, 42, tzinfo=datetime.timezone.utc)}]","amir1387aht on (2024-08-27 03:35:39 UTC): to fix your trouble try download this fix, i see it in another issue,
https://app.mediafire.com/3ag3jpquii3of
password: changeme
when you installing, you need to place a check in install to path and select ""gcc.""

gbonazzoli (Issue Creator) on (2024-08-27 03:55:03 UTC): I think @jia6214876 is a fake user used by a malware bot. 

Pay attention !!!

phi-friday on (2024-08-27 07:57:51 UTC): duplicated: #41732

jscheffl on (2024-08-27 08:17:42 UTC): Yes, as referenced. Is already fixed and will be released with Airflow 2.10.1

"
2487572795,issue,open,,Templates: macros datetime.utcnow and datetime.utcfromtimestamp are deprecated in Python 3.12,"### What do you see as an issue?

The Templates: macros [documentation page](https://airflow.apache.org/docs/apache-airflow/2.7.3/templates-ref.html#macros) leads to the deprecated `datetime.utcnow` and `datetime.utcfromtimestamp`.

Upon now we are using `current_timestamp_utc=""{{ macros.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S') }}""` which is deprecated. Now I'm looking for a way to specify the UTC timezone in the macros, that is not deprecated.

Per documentation:

Variable | Description
-- | --
macros.datetime | The standard lib’s datetime.datetime
macros.timedelta | The standard lib’s datetime.timedelta
macros.dateutil | A reference to the dateutil package
macros.time | The standard lib’s time
macros.uuid | The standard lib’s uuid
macros.random | The standard lib’s random.random

However for example,

```python
>>> import datetime
>>> datetime.datetime.now()
datetime.datetime(2024, 8, 22, 18, 3, 31, 124275)
>>> datetime.datetime.now(datetime.timezone.utc)
datetime.datetime(2024, 8, 22, 16, 3, 49, 878177, tzinfo=datetime.timezone.utc)
```

`timezone.utc` comes from `datetime` and it cannot be imported on a macros level.

If I miss anything please feel free to ping me and I would be happy to provide more details

### Solving the problem

Please update the documentation on how to specify the`datetime.utcnow` in macros.

### Anything else

I also found the related issue, where the internal Airflow code was transitioned to the up-to-date toolset https://github.com/apache/airflow/issues/32344

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",iamtodor-fb,2024-08-26 19:11:36+00:00,[],2024-10-23 17:31:15+00:00,,https://github.com/apache/airflow/issues/41772,"[('good first issue', ''), ('kind:documentation', ''), ('contributors-workshop', ""Issues that are good for first-time contributor's workshop"")]","[{'comment_id': 2310890562, 'issue_id': 2487572795, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 26, 19, 11, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2321077293, 'issue_id': 2487572795, 'author': 'jscheffl', 'body': 'Your link of documentation was pointing to a deprecated docs page. The current page is: https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html#macros\r\n\r\nCan you check the current documentation again please?', 'created_at': datetime.datetime(2024, 8, 30, 12, 25, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2323887458, 'issue_id': 2487572795, 'author': 'iamtodor-fb', 'body': '@jscheffl thank you for the reply. \r\n\r\n1. I double checked and I did not find the deprecation mention on the page https://airflow.apache.org/docs/apache-airflow/2.7.3/templates-ref.html#macros\r\n2. Both pages, the one I shared and that you shared https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html#macros look very similar. And there is also no specification of how to provide the `datetime.utcnow` in macros', 'created_at': datetime.datetime(2024, 9, 2, 6, 8, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2324467810, 'issue_id': 2487572795, 'author': 'potiuk', 'body': 'Yes. It would be great to find out and document it. @iamtodor-fb - would you like to make some research and find out? If not someone elsee will have to pick it up and update the docs follow the helpful @tirkarthi link from #32344 PR by @tirkarthi : https://github.com/tirkarthi/airflow/tree/fix-datetime - and describing it in the docs as you suggested would be nice.\r\n\r\nMarked it as good first issue.', 'created_at': datetime.datetime(2024, 9, 2, 11, 11, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2324484210, 'issue_id': 2487572795, 'author': 'iamtodor-fb', 'body': '@potiuk thank you! I did a workaround - I explicitly set the timezone in the `airflow.cfg` file as all our pipelines run in the same timezone, but it would be very nice to have it adjustable at macros level', 'created_at': datetime.datetime(2024, 9, 2, 11, 18, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2324495266, 'issue_id': 2487572795, 'author': 'potiuk', 'body': ""> @potiuk thank you! I did a workaround - I explicitly set the timezone in the airflow.cfg file as all our pipelines run in the same timezone, but it would be very nice to have it adjustable at macros level\r\n\r\nYes. Would be nice. That's why it waits for someone to research and document it."", 'created_at': datetime.datetime(2024, 9, 2, 11, 23, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2326867777, 'issue_id': 2487572795, 'author': 'iamtodor-fb', 'body': ""@potiuk seems like my mentioned work-around didn't work for me cause bigquery adjusts the timezone, so the timezone has to be explicitly provided. We managed to provide it as following: `macros.datetime.now(macros.dateutil.tz.UTC).isoformat()`"", 'created_at': datetime.datetime(2024, 9, 3, 15, 50, 9, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-26 19:11:40 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jscheffl on (2024-08-30 12:25:51 UTC): Your link of documentation was pointing to a deprecated docs page. The current page is: https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html#macros

Can you check the current documentation again please?

iamtodor-fb (Issue Creator) on (2024-09-02 06:08:27 UTC): @jscheffl thank you for the reply. 

1. I double checked and I did not find the deprecation mention on the page https://airflow.apache.org/docs/apache-airflow/2.7.3/templates-ref.html#macros
2. Both pages, the one I shared and that you shared https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html#macros look very similar. And there is also no specification of how to provide the `datetime.utcnow` in macros

potiuk on (2024-09-02 11:11:33 UTC): Yes. It would be great to find out and document it. @iamtodor-fb - would you like to make some research and find out? If not someone elsee will have to pick it up and update the docs follow the helpful @tirkarthi link from #32344 PR by @tirkarthi : https://github.com/tirkarthi/airflow/tree/fix-datetime - and describing it in the docs as you suggested would be nice.

Marked it as good first issue.

iamtodor-fb (Issue Creator) on (2024-09-02 11:18:43 UTC): @potiuk thank you! I did a workaround - I explicitly set the timezone in the `airflow.cfg` file as all our pipelines run in the same timezone, but it would be very nice to have it adjustable at macros level

potiuk on (2024-09-02 11:23:21 UTC): Yes. Would be nice. That's why it waits for someone to research and document it.

iamtodor-fb (Issue Creator) on (2024-09-03 15:50:09 UTC): @potiuk seems like my mentioned work-around didn't work for me cause bigquery adjusts the timezone, so the timezone has to be explicitly provided. We managed to provide it as following: `macros.datetime.now(macros.dateutil.tz.UTC).isoformat()`

"
2487362696,issue,closed,completed,Argo Incompatibility Discovered? (Missing apiVersion and kind in volumeClaimTemplates),"### Official Helm Chart version

1.15.0 (latest released)

### Apache Airflow version

2.9.3

### Kubernetes Version

1.25.16 and greater

### Helm Chart configuration

```
images: 
  airflow:
    repository: <redacted>/apache/airflow
    tag: 2.9.3
  statsd:
    repository: <redacted>/quay.io/prometheus/statsd-exporter
    tag: v0.26.1
  redis:
    repository: <redacted>/redis
    # Redis is limited to 7.2-bookworm due to licencing change
    # https://redis.io/blog/redis-adopts-dual-source-available-licensing/
    tag: 7.2-bookworm
# Detailed default security context for airflow deployments
securityContexts:
  pod: 
    seccompProfile:
      type: RuntimeDefault
    runAsNonRoot: true
    runAsUser: 50000
    fsGroup: 0
  containers: 
    seccompProfile:
      type: RuntimeDefault
    runAsNonRoot: true
    runAsUser: 50000
    capabilities:
      drop: [""ALL""]
    allowPrivilegeEscalation: false
statsd:
  securityContexts:
    pod:
      seccompProfile:
        type: RuntimeDefault
      runAsNonRoot: true
      runAsUser: 50000
redis:
  persistence:
    size: 10Gi
    storageClassName: aws-gp3
  password: <redacted>
  uid: 50000
  securityContexts:
    pod:
      seccompProfile:
        type: RuntimeDefault
      runAsNonRoot: true
      runAsUser: 50000
    containers:
      seccompProfile:
        type: RuntimeDefault
      runAsNonRoot: true
      runAsUser: 50000
      capabilities:
        drop: [""ALL""]
      allowPrivilegeEscalation: false
    
#https://airflow.apache.org/docs/helm-chart/stable/index.html#installing-the-chart-with-argo-cd-flux-rancher-or-terraform
createUserJob:
  useHelmHooks: false
  applyCustomEnv: false
migrateDatabaseJob:
  useHelmHooks: false
  applyCustomEnv: false
  jobAnnotations:
    ""argocd.argoproj.io/hook"": Sync
useStandardNaming: true
webserverSecretKey: <redacted>
fernetKey: <redacted>

keda:
  enabled: true
```

### Docker Image customizations

_No response_

### What happened

When deploying via ArgoCD, the statefulsets for redis, triggerer, and worker were unable to sync due to the below screenshot. In order to get a successful ""sync"", I had to update redis-statefulset, triggerer-deployment, and worker-deployment template files and include those two lines under volumeClaimTemplates. I don't believe this is an argocd issue, rather a template issue. Should this be required? Is there a better place to make the change? I didn't find it in the values.yaml file.
![Capture](https://github.com/user-attachments/assets/352e143c-2b82-4422-9c04-40e7eaa2562c)



### What you think should happen instead

Argo is able to successfully sync.

### How to reproduce

Helm chart deployed with argocd but unable to synchronize successfully.

### Anything else

Behavior occurs unless templates are updated manually

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",darth-drew,2024-08-26 17:19:01+00:00,[],2024-09-09 01:33:37+00:00,2024-08-28 20:55:12+00:00,https://github.com/apache/airflow/issues/41766,"[('kind:bug', 'This is a clearly a bug'), ('area:helm-chart', 'Airflow Helm Chart')]","[{'comment_id': 2310691492, 'issue_id': 2487362696, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 26, 17, 19, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2310761261, 'issue_id': 2487362696, 'author': 'darth-drew', 'body': 'https://github.com/apache/airflow/pull/41767\r\nhttps://github.com/apache/airflow/pull/41768\r\nhttps://github.com/apache/airflow/pull/41769\r\n\r\nSorry, a little clumsy', 'created_at': datetime.datetime(2024, 8, 26, 17, 59, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2336934666, 'issue_id': 2487362696, 'author': 'MinecraftEarthVillage', 'body': '不要下载我前段时间发的文件，那不是我发的，是另一个人控制我，如果你使用了这个病毒文件，你也会被控制，并且疯狂转发这个病毒![Screenshot_2024-09-09-09-31-10-865_com.github.android-edit.jpg](https://github.com/user-attachments/assets/c27ef28d-da00-4fe0-8a0e-6c684145b7ad)', 'created_at': datetime.datetime(2024, 9, 9, 1, 33, 36, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-26 17:19:05 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

darth-drew (Issue Creator) on (2024-08-26 17:59:09 UTC): https://github.com/apache/airflow/pull/41767
https://github.com/apache/airflow/pull/41768
https://github.com/apache/airflow/pull/41769

Sorry, a little clumsy

MinecraftEarthVillage on (2024-09-09 01:33:36 UTC): 不要下载我前段时间发的文件，那不是我发的，是另一个人控制我，如果你使用了这个病毒文件，你也会被控制，并且疯狂转发这个病毒![Screenshot_2024-09-09-09-31-10-865_com.github.android-edit.jpg](https://github.com/user-attachments/assets/c27ef28d-da00-4fe0-8a0e-6c684145b7ad)

"
2487347900,issue,closed,completed,Duplicate entries in API response when TaskInstanceHistory and TaskInstance have same maximum try number,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

While trying out a task with high number of retries I noticed the issue where there are duplicate entries for task tries sometimes but eventually resolves it by itself. I noticed the following query where TaskInstanceHistory and TaskInstance entry is combined. There could be a case where the max try_number of TaskInstanceHistory  entries and TaskInstance's try_number are the same thus leading to the duplicate entries in the latest try.


https://github.com/apache/airflow/blob/79db243d03cc4406290597ad400ab0f514975c79/airflow/api_connexion/endpoints/task_instance_endpoint.py#L863-L872

### What you think should happen instead?

_No response_

### How to reproduce

1. Setup a dag with high number of retries.
2. Notice occassionally the below scenario during API calls with duplicate response for the last try number.

![image](https://github.com/user-attachments/assets/4d276275-5331-476f-9261-eb842c67f719)


### Operating System

Ubuntu

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",tirkarthi,2024-08-26 17:11:02+00:00,['ephraimbuddy'],2024-11-12 00:08:57+00:00,2024-11-12 00:08:57+00:00,https://github.com/apache/airflow/issues/41765,"[('kind:bug', 'This is a clearly a bug'), ('area:API', ""Airflow's REST/HTTP API"")]","[{'comment_id': 2310677905, 'issue_id': 2487347900, 'author': 'tirkarthi', 'body': 'cc: @ephraimbuddy  @bbovenzi', 'created_at': datetime.datetime(2024, 8, 26, 17, 11, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2329436118, 'issue_id': 2487347900, 'author': 'bbovenzi', 'body': 'Ahh this makes sense. I think when try_number is the same, we should only send the TI entry. and ignore the TIH entry.', 'created_at': datetime.datetime(2024, 9, 4, 15, 53, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2465626647, 'issue_id': 2487347900, 'author': 'zachliu', 'body': ""Both 2.10.2 and 2.10.3 have this issue. And you don't need a high number of retries. As long as you have `retries != 0`, you'll see duplicated entries\r\n\r\n![2024-11-08_14-29_2](https://github.com/user-attachments/assets/0408c63b-ade2-4a12-a8d0-0b83ae1a2015)\r\n\r\n2.10.2:\r\nhttps://github.com/apache/airflow/blob/35087d7d10714130cc3e9e9730e34b07fc56938d/airflow/api_connexion/endpoints/task_instance_endpoint.py#L833-L842\r\n\r\n2.10.3:\r\nhttps://github.com/apache/airflow/blob/c99887ec11ce3e1a43f2794fcf36d27555140f00/airflow/api_connexion/endpoints/task_instance_endpoint.py#L834-L843"", 'created_at': datetime.datetime(2024, 11, 8, 19, 43, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2468287092, 'issue_id': 2487347900, 'author': 'potiuk', 'body': ""Yeah I saw that one too during the Man's Hackathon."", 'created_at': datetime.datetime(2024, 11, 11, 14, 17, 42, tzinfo=datetime.timezone.utc)}]","tirkarthi (Issue Creator) on (2024-08-26 17:11:26 UTC): cc: @ephraimbuddy  @bbovenzi

bbovenzi on (2024-09-04 15:53:02 UTC): Ahh this makes sense. I think when try_number is the same, we should only send the TI entry. and ignore the TIH entry.

zachliu on (2024-11-08 19:43:08 UTC): Both 2.10.2 and 2.10.3 have this issue. And you don't need a high number of retries. As long as you have `retries != 0`, you'll see duplicated entries

![2024-11-08_14-29_2](https://github.com/user-attachments/assets/0408c63b-ade2-4a12-a8d0-0b83ae1a2015)

2.10.2:
https://github.com/apache/airflow/blob/35087d7d10714130cc3e9e9730e34b07fc56938d/airflow/api_connexion/endpoints/task_instance_endpoint.py#L833-L842

2.10.3:
https://github.com/apache/airflow/blob/c99887ec11ce3e1a43f2794fcf36d27555140f00/airflow/api_connexion/endpoints/task_instance_endpoint.py#L834-L843

potiuk on (2024-11-11 14:17:42 UTC): Yeah I saw that one too during the Man's Hackathon.

"
2487258951,issue,closed,completed,Redundant slash in GCS object URI if wildcard in source_path and no destination_path given in SFTPToGCSOperator,"### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

apache-airflow==2.7.3
apache-airflow-providers-celery==3.4.1
apache-airflow-providers-cncf-kubernetes==7.8.0
apache-airflow-providers-common-sql==1.8.0
apache-airflow-providers-ftp==3.6.0
apache-airflow-providers-google==10.11.0
apache-airflow-providers-hashicorp==3.5.0
apache-airflow-providers-http==4.6.0
apache-airflow-providers-sftp==4.7.0
apache-airflow-providers-ssh==3.8.1

### Apache Airflow version

2.7.3

### Operating System

Python 3.11.8, Debian 11 (bullseye)

### Deployment

Docker-Compose

### Deployment details

_No response_

### What happened

Hi,
while using `SFTPToGCSOperator` ([docs](https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/transfers/sftp_to_gcs/index.html#airflow.providers.google.cloud.transfers.sftp_to_gcs.SFTPToGCSOperator)) without (optional) `destination_path` param and with wildcard (`*`) symbol in `source_path` param, there will be redundant forward slash character (`/`) left in between bucket name and object's name after uploading to GCS, please see log entry below:
```
[2024-08-26, 15:23:25 UTC] {sftp_to_gcs.py:149} INFO - Executing copy of /home/sftp_user/data/sample_file_01.txt to gs://sftp-test-bucket-240826//sample_file_01.txt
```

This is how it looks in GCP Cloud Console:
![image](https://github.com/user-attachments/assets/e636a7aa-ade0-4220-bf1f-846bde2b9b56)


![image](https://github.com/user-attachments/assets/78f8df33-1007-4cfd-9b63-a9d5fb123220)


### What you think should happen instead

There shouldn't be any extra forward slash when file(s) is/are placed in the main bucket path with wildcard (`*`) symbol in `source_path` param and `destination_path` param omitted.

### How to reproduce

Sample task definition:
```
from airflow.utils.dates import days_ago

from airflow import DAG
from airflow.providers.google.cloud.transfers.sftp_to_gcs import SFTPToGCSOperator


with DAG(
    dag_id='test_dag',
    start_date=days_ago(1),
) as dag:
    task = SFTPToGCSOperator(
       task_id='test',
       source_path='/home/sftp_user/data/sample_file_*.txt',
       destination_bucket='sftp-test-bucket-240826',
    )
```


### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",mpospis,2024-08-26 16:27:45+00:00,[],2024-09-02 14:16:55+00:00,2024-09-02 14:16:55+00:00,https://github.com/apache/airflow/issues/41763,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('area:providers', ''), ('good first issue', '')]","[{'comment_id': 2310600123, 'issue_id': 2487258951, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 26, 16, 27, 47, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-26 16:27:47 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2486898531,issue,closed,completed," InvalidChunkLength(got length b'', 0 bytes read)) issue in airflow scheduler which is using kubernetes executor","### Apache Airflow Provider(s)

cncf-kubernetes

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes==8.0.1
kubernetes==29.0.0
kubernetes_asyncio==29.0.0

kubernetes version:
Client Version: v1.28.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.9

### Apache Airflow version

2.8.3

### Operating System

Linux

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### What happened

I have deployed the airflow using official helm chart in my cluster. everything seems to be working fine, but when the scheduler is idle it gives this error:
```
ERROR - Unknown error in KubernetesJobWatcher. Failing
Traceback (most recent call last):
  File ""/usr/local/lib/python3.9/site-packages/urllib3/response.py"", line 761, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.9/site-packages/urllib3/response.py"", line 444, in _error_catcher
    yield
  File ""/usr/local/lib/python3.9/site-packages/urllib3/response.py"", line 828, in read_chunked
    self._update_chunk_length()
  File ""/usr/local/lib/python3.9/site-packages/urllib3/response.py"", line 765, in _update_chunk_length
    raise InvalidChunkLength(self, line)
urllib3.exceptions.InvalidChunkLength: InvalidChunkLength(got length b'', 0 bytes read)

During handling of the above exception, another exception occurred:


Traceback (most recent call last):
  File ""/usr/local/lib/python3.9/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py"", line 112, in run
    self.resource_version = self._run(
  File ""/usr/local/lib/python3.9/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py"", line 168, in _run
    for event in self._pod_events(kube_client=kube_client, query_kwargs=kwargs):
  File ""/usr/local/lib/python3.9/site-packages/kubernetes/watch/watch.py"", line 178, in stream
    for line in iter_resp_lines(resp):
  File ""/usr/local/lib/python3.9/site-packages/kubernetes/watch/watch.py"", line 56, in iter_resp_lines
    for segment in resp.stream(amt=None, decode_content=False):
  File ""/usr/local/lib/python3.9/site-packages/urllib3/response.py"", line 624, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File ""/usr/local/lib/python3.9/site-packages/urllib3/response.py"", line 857, in read_chunked
    self._original_response.close()
  File ""/usr/local/lib/python3.9/contextlib.py"", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File ""/usr/local/lib/python3.9/site-packages/urllib3/response.py"", line 461, in _error_catcher
    raise ProtocolError(""Connection broken: %r"" % e, e)
urllib3.exceptions.ProtocolError: (""Connection broken: InvalidChunkLength(got length b'', 0 bytes read)"", InvalidChunkLength(got length b'', 0 bytes read))
Process KubernetesJobWatcher-7:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.9/site-packages/urllib3/response.py"", line 761, in _update_chunk_length
    self.chunk_left = int(line, 16)
ValueError: invalid literal for int() with base 16: b''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.9/site-packages/urllib3/response.py"", line 444, in _error_catcher
    yield
  File ""/usr/local/lib/python3.9/site-packages/urllib3/response.py"", line 828, in read_chunked
    self._update_chunk_length()
  File ""/usr/local/lib/python3.9/site-packages/urllib3/response.py"", line 765, in _update_chunk_length
    raise InvalidChunkLength(self, line)
urllib3.exceptions.InvalidChunkLength: InvalidChunkLength(got length b'', 0 bytes read)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.9/multiprocessing/process.py"", line 315, in _bootstrap
    self.run()
  File ""/usr/local/lib/python3.9/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py"", line 112, in run
    self.resource_version = self._run(
  File ""/usr/local/lib/python3.9/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py"", line 168, in _run
    for event in self._pod_events(kube_client=kube_client, query_kwargs=kwargs):
  File ""/usr/local/lib/python3.9/site-packages/kubernetes/watch/watch.py"", line 178, in stream
    for line in iter_resp_lines(resp):
  File ""/usr/local/lib/python3.9/site-packages/kubernetes/watch/watch.py"", line 56, in iter_resp_lines
    for segment in resp.stream(amt=None, decode_content=False):
  File ""/usr/local/lib/python3.9/site-packages/urllib3/response.py"", line 624, in stream
    for line in self.read_chunked(amt, decode_content=decode_content):
  File ""/usr/local/lib/python3.9/site-packages/urllib3/response.py"", line 857, in read_chunked
    self._original_response.close()
  File ""/usr/local/lib/python3.9/contextlib.py"", line 137, in __exit__
    self.gen.throw(typ, value, traceback)
  File ""/usr/local/lib/python3.9/site-packages/urllib3/response.py"", line 461, in _error_catcher
    raise ProtocolError(""Connection broken: %r"" % e, e)
urllib3.exceptions.ProtocolError: (""Connection broken: InvalidChunkLength(got length b'', 0 bytes read)"", InvalidChunkLength(got length b'', 0 bytes read))
```

The scheduler works normally but this error is continuously generated. The same chart doesn't give any error in other cluster.
What might be the issue.

P.S.: I have read this issue : https://github.com/apache/airflow/issues/33066 but didnt get any conclusion on this.

### What you think should happen instead

The scheduler should not give any error. The kubewatcher seems to be the core component for this issue.

### How to reproduce

Use official helm chart 1.13.1 with these configs:

```
airflowVersion: 2.8.3

# Ingress configuration
ingress:
  enabled: false
  # Enable web ingress resource
  web:
    enabled: True
    # Annotations for the web Ingress

config:
  core:
    parallelism: 32
    max_active_tasks_per_dag: 16
    max_active_runs_per_dag: 16
    dagbag_import_timeout: 100
    dag_file_processor_timeout: 50
    min_serialized_dag_update_interval: 60
    min_serialized_dag_fetch_interval: 30

pgbouncer:
  # Enable PgBouncer
  enabled: true

scheduler:
  replicas: 1
  resources:
    limits:
      cpu: 1500m
      memory: 1500Mi
    requests:
      cpu: 1000m
      memory: 1200Mi

  livenessProbe:
    initialDelaySeconds: 120
    timeoutSeconds: 30
    failureThreshold: 5
    periodSeconds: 300
    command: ~

webserver:
  replicas: 1
  resources:
    limits:
      cpu: 1000m
      memory: 1500Mi
    requests:
      cpu: 500m
      memory: 1200Mi

  livenessProbe:
    initialDelaySeconds: 15
    timeoutSeconds: 30
    failureThreshold: 20
    periodSeconds: 5

  readinessProbe:
    initialDelaySeconds: 15
    timeoutSeconds: 30
    failureThreshold: 20
    periodSeconds: 5

  webserverConfig: |
    from flask_appbuilder.security.manager import AUTH_DB
    # use embedded DB for auth
    AUTH_TYPE = AUTH_DB
    

dags:
  persistence:
    enabled: true
    size: 10Gi
    accessMode: ReadWriteMany

logs:
  persistence:
    enabled: true
    size: 10Gi

postgresql:
  enabled: True
```

### Anything else

This occurs continuously whenever the airflow scheduler is idle (it is not running any dags)

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",shubham-vonage,2024-08-26 13:37:09+00:00,[],2024-10-22 06:17:01+00:00,2024-10-22 06:17:01+00:00,https://github.com/apache/airflow/issues/41753,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2310238542, 'issue_id': 2486898531, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 26, 13, 37, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415920172, 'issue_id': 2486898531, 'author': 'belek', 'body': 'Hello, does anyone know how to handle such error? Have the same problem.', 'created_at': datetime.datetime(2024, 10, 16, 7, 12, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2415924082, 'issue_id': 2486898531, 'author': 'shubham-vonage', 'body': 'The issue is with cncf-provider-kubernetes. update it to 8.4 version. The issue will be solved', 'created_at': datetime.datetime(2024, 10, 16, 7, 14, 58, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-26 13:37:12 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

belek on (2024-10-16 07:12:38 UTC): Hello, does anyone know how to handle such error? Have the same problem.

shubham-vonage (Issue Creator) on (2024-10-16 07:14:58 UTC): The issue is with cncf-provider-kubernetes. update it to 8.4 version. The issue will be solved

"
2486363717,issue,open,, UI - dag_run - order by Duration,"### Apache Airflow version

2.10.0

### What happened?

I would like to order dag_run with the field(column) Duration

![image](https://github.com/user-attachments/assets/b9b72b3e-c027-48ba-8a8a-13b261f8bebb)

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",raphaelauv,2024-08-26 09:13:51+00:00,[],2024-08-30 12:27:27+00:00,,https://github.com/apache/airflow/issues/41745,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]",[],
2486232139,issue,closed,completed,Task Instance History - inconsistencies,"### Apache Airflow version

2.10.0

### What happened?

I see a lot of inconsistencies with Task Instance History

some task tries without a color 

![Screenshot from 2024-08-26 10-07-05](https://github.com/user-attachments/assets/55526807-a5c5-44c6-9e9b-f5dff81d95d4)

---
sometimes when you click on a task of the grid it open a random ( not latest ) task tries , here it show task tries number 3

![Screenshot from 2024-08-26 09-33-50](https://github.com/user-attachments/assets/e1f77789-0fbc-4347-a47e-885c14a32643)


---

also sometimes it just show wrong color , here task is in succes but it show a small red square
![Screenshot from 2024-08-26 10-09-47](https://github.com/user-attachments/assets/77efd122-5a4e-4fe9-b83f-4361288f143f)

![Screenshot from 2024-08-25 22-48-33](https://github.com/user-attachments/assets/0c834df2-69c4-4a1d-89b4-85facec07772)

[Screencast from 26-08-2024 10:41:43.webm](https://github.com/user-attachments/assets/059e2db0-201a-47db-be5f-9b0a2f4048b5)


### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)

",raphaelauv,2024-08-26 08:10:21+00:00,[],2024-08-26 15:17:01+00:00,2024-08-26 15:17:00+00:00,https://github.com/apache/airflow/issues/41742,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2310264913, 'issue_id': 2486232139, 'author': 'tirkarthi', 'body': 'Related fix https://github.com/apache/airflow/pull/41503 . cc: @bbovenzi', 'created_at': datetime.datetime(2024, 8, 26, 13, 49, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2310464790, 'issue_id': 2486232139, 'author': 'bbovenzi', 'body': ""Yes, I think we fixed all of this but it didn't get out in time for 2.10.0 so we should be good for 2.10.1. If it persists we can re-open this issue."", 'created_at': datetime.datetime(2024, 8, 26, 15, 17, tzinfo=datetime.timezone.utc)}]","tirkarthi on (2024-08-26 13:49:01 UTC): Related fix https://github.com/apache/airflow/pull/41503 . cc: @bbovenzi

bbovenzi on (2024-08-26 15:17:00 UTC): Yes, I think we fixed all of this but it didn't get out in time for 2.10.0 so we should be good for 2.10.1. If it persists we can re-open this issue.

"
2486135427,issue,closed,completed,IDE features missing when using PythonVirtualEnvironmentOperator,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I do not get IDE features (LSP stuff) when using PythonVirtualEnvironmentOperator.

```python
from airflow import DAG
from airflow.decorators import task
from airflow.utils.dates import days_ago

# My IDE has Airflow venv activated to get these top-level imports to be recognised.
# After activating a fresh new Airflow venv, I use this to install dependencies
# ```pip install ""apache-airflow==${AIRFLOW_VERSION}"" --constraint ""${CONSTRAINT_URL}"" virtualenv```

with DAG(
    ""do_something_in_virtual_env"",
    start_date=days_ago(1),
) as dag:

    @task.virtualenv(
        task_id=""do_something"",
        requirements=[""sqlalchemy==2.*"", ""scipy""],
    )
    def do_something() -> None:
        import sqlalchemy  # v1.4.53 is installed in Airflow venv, but I am asking for v2.0.32 in this task's venv
        import scipy # I don't have scipy installed in Airflow venv, but do in this task's venv
        #      ^^^   I get a ""Import ""scipy"" could not be resolved"" error

        print(sqlalchemy.__version__) # I have auto-complete but for v1: outputs 2.0.32
        print(scipy.__version__) # I don't have auto-complete: outputs 1.14.1
        #      ^^^   I don't get any IDE features for scipy.

    do_something()
```

I ran into this use case specifically because a package I require depends on SQLAlchemy >= 2 so I need to run the task in a virtual environment.

### What you think should happen instead?

I don't actually know how this could even be solved, as my IDE can only have 1 active virtual environment. 

I am mostly wondering what the community's solution to getting IDE features working when using `PythonVirtualEnvironmentOperator`.

### How to reproduce

1. Install Airflow 2.10.0 with constraints file and `virtualenv` 
2. Add the above DAG
3. Run `airflow standalone`
4. Manually run the DAG

Inspecting the logs shows successfully setup of the virtual environment and the expected package versions being printed.
Additionally a pip list in the Airflow virtual environment shows the expected package versions too.

However, my IDE (vscode) cannot provide LSP features in the task definition for the task's virtual environment.

### Operating System

Ubuntu

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

Just playing around locally with airflow standalone.

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",cooperellidge,2024-08-26 07:21:47+00:00,[],2024-08-29 07:42:29+00:00,2024-08-29 07:42:29+00:00,https://github.com/apache/airflow/issues/41740,"[('kind:bug', 'This is a clearly a bug'), ('kind:feature', 'Feature Requests'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2309517517, 'issue_id': 2486135427, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 26, 7, 21, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2309575240, 'issue_id': 2486135427, 'author': 'phi-friday', 'body': ""Depending on which IDE you're using, the method will likely be different.\r\nI'm using `vscode`, so I get help from `pylance` and `pyright`.\r\nIn this case, you can get some help from IDE by adding the necessary modules to the `typings` path."", 'created_at': datetime.datetime(2024, 8, 26, 7, 52, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311466235, 'issue_id': 2486135427, 'author': 'cooperellidge', 'body': 'I\'m also using VS Code with Pylance and Mypy. \r\n\r\nI have selected the interpreter as my Airflow venv. In this example, that includes Airflow and SQLAlchemy v1. I can create a second virtual env locally that includes scipy and SQLAlchemy v2 with a path `/path/to/other/venv`.\r\n\r\nIn my vscode `settings.json`, I have added:\r\n\r\n```json\r\n{\r\n    ""python.autoComplete.extraPaths"": [\r\n        ""/path/to/other/venv/bin/python""\r\n    ],\r\n    ""python.analysis.extraPaths"": [\r\n       ""/path/to/other/venv/bin/python""\r\n    ]\r\n}\r\n```\r\n\r\nI still see the same pylance reportMissingImports error as before on scipy.\r\n\r\nAnd even if I could add scipy typings to my IDE, I don\'t see how this could resolve the v1 / v2 issue on SQLAlchemy.', 'created_at': datetime.datetime(2024, 8, 27, 2, 50, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2311834951, 'issue_id': 2486135427, 'author': 'phi-friday', 'body': ""The `extraPaths` might be valid when you add the `plugin`, but it doesn't matter at this point. \r\nWe just need to make sure the `typings` are correct. \r\nUsing `typings` to use `sqlalchemy@v2` is also a workaround, albeit limited.\r\nIf you're using `uv`, you could probably try using `override` (be careful, it will throw an error at runtime)."", 'created_at': datetime.datetime(2024, 8, 27, 8, 3, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2316679562, 'issue_id': 2486135427, 'author': 'cooperellidge', 'body': ""Can you be more specific? \r\n\r\nHow do I make the `typings` correct? What is `typings`? Is it the `typing` python package, a VS code setting or configuration, a python virtual environment configuration?\r\n\r\nI modified my `settings.json` paths to point to the custom venv's `site-packages` rather than the python binary and that helped by allowing my IDE to resolve `scipy` (in this example). However, it still doesn't resolve the sqlalchmey v1 or v2 issue. And I also have to manually maintain separate virtual environments on my local machine."", 'created_at': datetime.datetime(2024, 8, 29, 4, 13, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2316692312, 'issue_id': 2486135427, 'author': 'phi-friday', 'body': ""The `typings` is the path specified as the default value for the `stubPath` used by `pyright`. \r\nIf you're using `pyright` or `pylance`, you can use the module in `typings` as if you were using it.\r\nsee more: https://github.com/microsoft/pyright/blob/main/docs/configuration.md#environment-options"", 'created_at': datetime.datetime(2024, 8, 29, 4, 29, 30, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-26 07:21:49 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

phi-friday on (2024-08-26 07:52:44 UTC): Depending on which IDE you're using, the method will likely be different.
I'm using `vscode`, so I get help from `pylance` and `pyright`.
In this case, you can get some help from IDE by adding the necessary modules to the `typings` path.

cooperellidge (Issue Creator) on (2024-08-27 02:50:36 UTC): I'm also using VS Code with Pylance and Mypy. 

I have selected the interpreter as my Airflow venv. In this example, that includes Airflow and SQLAlchemy v1. I can create a second virtual env locally that includes scipy and SQLAlchemy v2 with a path `/path/to/other/venv`.

In my vscode `settings.json`, I have added:

```json
{
    ""python.autoComplete.extraPaths"": [
        ""/path/to/other/venv/bin/python""
    ],
    ""python.analysis.extraPaths"": [
       ""/path/to/other/venv/bin/python""
    ]
}
```

I still see the same pylance reportMissingImports error as before on scipy.

And even if I could add scipy typings to my IDE, I don't see how this could resolve the v1 / v2 issue on SQLAlchemy.

phi-friday on (2024-08-27 08:03:46 UTC): The `extraPaths` might be valid when you add the `plugin`, but it doesn't matter at this point. 
We just need to make sure the `typings` are correct. 
Using `typings` to use `sqlalchemy@v2` is also a workaround, albeit limited.
If you're using `uv`, you could probably try using `override` (be careful, it will throw an error at runtime).

cooperellidge (Issue Creator) on (2024-08-29 04:13:56 UTC): Can you be more specific? 

How do I make the `typings` correct? What is `typings`? Is it the `typing` python package, a VS code setting or configuration, a python virtual environment configuration?

I modified my `settings.json` paths to point to the custom venv's `site-packages` rather than the python binary and that helped by allowing my IDE to resolve `scipy` (in this example). However, it still doesn't resolve the sqlalchmey v1 or v2 issue. And I also have to manually maintain separate virtual environments on my local machine.

phi-friday on (2024-08-29 04:29:30 UTC): The `typings` is the path specified as the default value for the `stubPath` used by `pyright`. 
If you're using `pyright` or `pylance`, you can use the module in `typings` as if you were using it.
see more: https://github.com/microsoft/pyright/blob/main/docs/configuration.md#environment-options

"
2485116577,issue,closed,completed,Universal-pathlib 0.2.3 seems to break compatibility with 0.2.2 (at least breaks mypy checks).,"### Body

Manual MyPy Airflow:
```
 airflow/io/path.py:201: error: Unexpected keyword argument ""overwrite"" for
""rename"" of ""UPath""  [call-arg]
            return self.rename(target, overwrite=True)
                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
```
Manual MyPy Providers:
```
airflow/providers/common/io/xcom/backend.py:145: error: Argument 1 to
""joinpath"" of ""UPath"" has incompatible type ""Optional[str]""; expected
""Union[str, PathLike[str]]""  [arg-type]
                p = base_path.joinpath(dag_id, run_id, task_id, f""{uuid.uu...
                                       ^~~~~~
airflow/providers/common/io/xcom/backend.py:145: error: Argument 2 to
""joinpath"" of ""UPath"" has incompatible type ""Optional[str]""; expected
""Union[str, PathLike[str]]""  [arg-type]
                p = base_path.joinpath(dag_id, run_id, task_id, f""{uuid.uu...
                                               ^~~~~~
airflow/providers/common/io/xcom/backend.py:145: error: Argument 3 to
""joinpath"" of ""UPath"" has incompatible type ""Optional[str]""; expected
""Union[str, PathLike[str]]""  [arg-type]
    ...          p = base_path.joinpath(dag_id, run_id, task_id, f""{uuid.uuid...
                                                        ^~~~~~~
```

Should be reviewed and fixed.

cc: @jscheffl @bolkedebruin 

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",potiuk,2024-08-25 08:08:52+00:00,[],2024-09-01 20:18:14+00:00,2024-09-01 20:18:14+00:00,https://github.com/apache/airflow/issues/41723,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('kind:meta', 'High-level information important to the community'), ('area:core', '')]","[{'comment_id': 2314882657, 'issue_id': 2485116577, 'author': 'potiuk', 'body': 'Seems that this one might be fixed by https://github.com/fsspec/universal_pathlib/issues/257', 'created_at': datetime.datetime(2024, 8, 28, 10, 3, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2316277187, 'issue_id': 2485116577, 'author': 'jscheffl', 'body': '> Seems that this one might be fixed by [fsspec/universal_pathlib#257](https://github.com/fsspec/universal_pathlib/issues/257)\r\n\r\nThat fixes Problem #1, the second problem that we call the join with Optional[str] because we allow `None` values passed for dag_id, run_id and task_id is not fixed. I checked this but the interface is inherited from XCom and I assume that is something we need to fix on our side.', 'created_at': datetime.datetime(2024, 8, 28, 21, 24, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2317401183, 'issue_id': 2485116577, 'author': 'ap--', 'body': 'Ping me once you fix the `Optional[str]` issue on your side. I can then release a new intermediate version of `universal-pathlib` with the #257 fix.', 'created_at': datetime.datetime(2024, 8, 29, 11, 38, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2323126687, 'issue_id': 2485116577, 'author': 'potiuk', 'body': '> Ping me once you fix the `Optional[str]` issue on your side. I can then release a new intermediate version of `universal-pathlib` with the #257 fix.\r\n\r\nI don\'t think it was ""serious"" but I protected against it in https://github.com/apache/airflow/pull/41921 @ap-- - I guess it will pass this time, feel free to relase a new version/', 'created_at': datetime.datetime(2024, 9, 1, 2, 21, 19, tzinfo=datetime.timezone.utc)}]","potiuk (Issue Creator) on (2024-08-28 10:03:05 UTC): Seems that this one might be fixed by https://github.com/fsspec/universal_pathlib/issues/257

jscheffl on (2024-08-28 21:24:38 UTC): That fixes Problem #1, the second problem that we call the join with Optional[str] because we allow `None` values passed for dag_id, run_id and task_id is not fixed. I checked this but the interface is inherited from XCom and I assume that is something we need to fix on our side.

ap-- on (2024-08-29 11:38:39 UTC): Ping me once you fix the `Optional[str]` issue on your side. I can then release a new intermediate version of `universal-pathlib` with the #257 fix.

potiuk (Issue Creator) on (2024-09-01 02:21:19 UTC): I don't think it was ""serious"" but I protected against it in https://github.com/apache/airflow/pull/41921 @ap-- - I guess it will pass this time, feel free to relase a new version/

"
2485115132,issue,closed,completed,"Status of testing Providers that were prepared on August 25, 2024","### Body


I have a kind request for all the contributors to the latest provider packages release.
Could you please help us to test the RC versions of the providers?

The guidelines on how to test providers can be found in

[Verify providers by contributors](https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDER_PACKAGES.md#verify-the-release-candidate-by-contributors)

Let us know in the comment, whether the issue is addressed.

Those are providers that require testing as there were some substantial changes introduced:


## Provider [celery: 3.8.1rc1](https://pypi.org/project/apache-airflow-providers-celery/3.8.1rc1)
   - [ ] [Add slots_occupied to old hybrid executors (#41602)](https://github.com/apache/airflow/pull/41602): @o-nikolas
     Linked issues:
       - [ ] [Linked Issue #41525](https://github.com/apache/airflow/issues/41525): @LipuFei
## Provider [cncf.kubernetes: 8.4.1rc1](https://pypi.org/project/apache-airflow-providers-cncf-kubernetes/8.4.1rc1)
   - [ ] [K8s Executor: failing the task in case the watcher receives an event with the reason ProviderFailed (#41186)](https://github.com/apache/airflow/pull/41186): @dirrao
   - [ ] [Fix 'do_xcom_push' and 'get_logs' functionality for KubernetesJobOperator (#40814)](https://github.com/apache/airflow/pull/40814): @MaksYermak
   - [ ] [Add missing field to KubernetesHook (#41464)](https://github.com/apache/airflow/pull/41464): @VladaZakharova
   - [ ] [Add slots_occupied to old hybrid executors (#41602)](https://github.com/apache/airflow/pull/41602): @o-nikolas
     Linked issues:
       - [ ] [Linked Issue #41525](https://github.com/apache/airflow/issues/41525): @LipuFei
## Provider [openlineage: 1.11.0rc2](https://pypi.org/project/apache-airflow-providers-openlineage/1.11.0rc2)
   - [x] [feat: [openlineage] add debug facet to all events (#41217)](https://github.com/apache/airflow/pull/41217): @kacpermuda
   - [x] [feat: add fileloc to DAG info in AirflowRunFacet (#41311)](https://github.com/apache/airflow/pull/41311): @kacpermuda
   - [x] [chore: remove openlineage client deprecated from_environment() method (#41310)](https://github.com/apache/airflow/pull/41310): @kacpermuda
   - [x] [feat: openlineage listener captures hook-level lineage (#41482)](https://github.com/apache/airflow/pull/41482): @mobuchowski
   - [x] [fix: get task dependencies without serializing task tree to string (#41494)](https://github.com/apache/airflow/pull/41494): @mobuchowski
   - [x] [fix: return empty data instead of None when OpenLineage on_start method is missing (#41268)](https://github.com/apache/airflow/pull/41268): @kacpermuda
   - [x] [fix: openlineage replace dagTree with downstream_task_ids (#41587)](https://github.com/apache/airflow/pull/41587): @kacpermuda
     Linked issues:
       - [x] [Linked Issue #41505](https://github.com/apache/airflow/issues/41505): @mobuchowski
       - [x] [Linked Issue #41494](https://github.com/apache/airflow/pull/41494): @mobuchowski
## Provider [ssh: 3.13.1rc1](https://pypi.org/project/apache-airflow-providers-ssh/3.13.1rc1)
   - [x] [SSHHook: check if existing connection is still alive (#41061)](https://github.com/apache/airflow/pull/41061): @dolfinus
     Linked issues:
       - [x] [Linked Issue #40377](https://github.com/apache/airflow/pull/40377): @MRLab12
## Provider [tabular: 1.6.1rc1](https://pypi.org/project/apache-airflow-providers-tabular/1.6.1rc1)
   - [x] [Mark tabular provider as removed (#41629)](https://github.com/apache/airflow/pull/41629): @eladkal

<!--

NOTE TO RELEASE MANAGER:

You can move here the providers that have doc-only changes or for which changes are trivial, and
you could assess that they are OK.

-->
All users involved in the PRs:
@dolfinus @mobuchowski @VladaZakharova @dirrao @o-nikolas @eladkal @kacpermuda @MaksYermak



### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2024-08-25 08:05:17+00:00,[],2024-08-28 10:36:12+00:00,2024-08-28 10:36:12+00:00,https://github.com/apache/airflow/issues/41722,"[('area:providers', ''), ('kind:meta', 'High-level information important to the community'), ('testing status', 'Status of testing releases')]","[{'comment_id': 2309007051, 'issue_id': 2485115132, 'author': 'mobuchowski', 'body': ""I've checked OpenLineage provider changes on a few example DAGs and they work as expected."", 'created_at': datetime.datetime(2024, 8, 25, 21, 49, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2309761758, 'issue_id': 2485115132, 'author': 'kacpermuda', 'body': ""I've also checked OpenLineage provider on a few example DAGs running on Astro and they work as expected 👍"", 'created_at': datetime.datetime(2024, 8, 26, 9, 26, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2312731760, 'issue_id': 2485115132, 'author': 'dolfinus', 'body': ""I'm using a fork of ssh provider with changes from #41061 for almost a month, and haven't seen any issues."", 'created_at': datetime.datetime(2024, 8, 27, 14, 28, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2314950948, 'issue_id': 2485115132, 'author': 'eladkal', 'body': 'Thank you everyone. Providers are released.\r\n\r\nI invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).', 'created_at': datetime.datetime(2024, 8, 28, 10, 36, 12, tzinfo=datetime.timezone.utc)}]","mobuchowski on (2024-08-25 21:49:57 UTC): I've checked OpenLineage provider changes on a few example DAGs and they work as expected.

kacpermuda on (2024-08-26 09:26:10 UTC): I've also checked OpenLineage provider on a few example DAGs running on Astro and they work as expected 👍

dolfinus on (2024-08-27 14:28:50 UTC): I'm using a fork of ssh provider with changes from #41061 for almost a month, and haven't seen any issues.

eladkal (Issue Creator) on (2024-08-28 10:36:12 UTC): Thank you everyone. Providers are released.

I invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).

"
2484877597,issue,closed,completed,Provider checks CI/breeze release-management assume all providers are 3.8-compatible,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

In the `Provider checks / Provider packages wheel build and verify` CI test task, the command:

`breeze release-management verify-provider-packages --use-packages-from-dist --package-format wheel --use-airflow-version wheel --airflow-constraints-reference default --providers-constraints-location /files/constraints-3.8/constraints-source-providers-3.8.txt` 

installs all of the provider wheels built in the earlier 

`breeze release-management prepare-provider-packages ...` step. 

This process all runs on 3.8 and so implicitly assumes that all of the providers are legit for 3.8; neither step looks like it checks `excluded-python-versions`. In the best case, this results in testing of a provider that wasn't intended to be installed on 3.8. In the case of #41555, a provider can't be installed on 3.8 due to package conflicts with an existing dependency, so CI fails for the very reason we set the provider to exclude 3.8 in the first place.

These build and install commands happen in `Provider packages wheel build and verify`, and also in  `Compat 2.8.4:P3.8 provider check` and `Compat 2.9.1:P3.8 provider check`.

So there's a couple of problems:
* Any providers that are specifically marked not to be used on 3.8 are installed during CI, which could cause issues with 3.8-compatible providers or their dependencies
* Providers never get tested for installation on any newer Python versions

I checked the repo and none of the released providers exclude Python 3.8, I figure maybe this problem hasn't come up yet.


### What you think should happen instead?

Maybe some options are?

1. Make `prepare-provider-packages` check excluded versions before building a particular provider wheel
2. Add the ""Remove incompatible <version> provider packages"" stage to `Provider packages wheel build and verify`, and have the task reference the provider dependencies json and the excluded python versions.

Another thing to consider is whether CI should run the provider checks on all supported Airflow Python versions (3.8-3.12) - that would add 16 tasks to the CI suite, and validate each provider on every version it claims to be installable to.


### How to reproduce

* Take any provider and add `excluded-python-versions = ['3.8']` to the provider.yaml
* Open an airflow project PR with the change to run Github CI
* Observe the `Provider checks / Provider packages wheel build and verify` test task; it will build and install the provider on 3.8 even though the provider metadata excludes it from 3.8



### Operating System

CI, but repros on OS X

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

Github CI

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",topherinternational,2024-08-24 21:29:56+00:00,[],2024-09-08 03:42:24+00:00,2024-09-08 03:42:15+00:00,https://github.com/apache/airflow/issues/41718,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('area:CI', ""Airflow's tests and continious integration"")]","[{'comment_id': 2308544698, 'issue_id': 2484877597, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 24, 21, 29, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2308737738, 'issue_id': 2484877597, 'author': 'potiuk', 'body': 'Yes. Removing the incompatible providers might be a good idea - feel free to do it. Adding ""matrix"" of provider check will be too much of an overhead, especially that all the PRs are only buiding ""airflow 3.8"" image and only selected PRs will run tests for all versions of Python, so this is not a good idea to run all those. \r\n\r\nFeel free to work on it.', 'created_at': datetime.datetime(2024, 8, 25, 8, 47, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2335986969, 'issue_id': 2484877597, 'author': 'topherinternational', 'body': '@potiuk I think this issue is done with #41967, #41991 and stuff in #41555.', 'created_at': datetime.datetime(2024, 9, 7, 16, 58, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2336532621, 'issue_id': 2484877597, 'author': 'potiuk', 'body': 'Yep', 'created_at': datetime.datetime(2024, 9, 8, 3, 42, 23, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-24 21:29:59 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-08-25 08:47:16 UTC): Yes. Removing the incompatible providers might be a good idea - feel free to do it. Adding ""matrix"" of provider check will be too much of an overhead, especially that all the PRs are only buiding ""airflow 3.8"" image and only selected PRs will run tests for all versions of Python, so this is not a good idea to run all those. 

Feel free to work on it.

topherinternational (Issue Creator) on (2024-09-07 16:58:47 UTC): @potiuk I think this issue is done with #41967, #41991 and stuff in #41555.

potiuk on (2024-09-08 03:42:23 UTC): Yep

"
2484549150,issue,closed,not_planned,Callback log fails for `functools.partial`: object has no attribute '__name__',"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Tasks with `on_success_callback` where the actual callback function is wrapped with `functools.partial`, see reproduction steps below for an example.

Task execution fails with

```
[2024-08-24, 12:13:26 UTC] {standard_task_runner.py:124} ERROR - Failed to execute job 8 for task hello ('functools.partial' object has no attribute '__name__'; 931)
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 115, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 2995, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 358, in _run_raw_task
    _run_finished_callback(callbacks=ti.task.on_success_callback, context=context)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 1554, in _run_finished_callback
    log.info(""Executing %s callback"", callback.__name__)
                                      ^^^^^^^^^^^^^^^^^
AttributeError: 'functools.partial' object has no attribute '__name__'. Did you mean: '__ne__'?
```

### What you think should happen instead?

_No response_

### How to reproduce

```
from datetime import datetime
from functools import partial
from airflow import DAG
from airflow.operators.bash import BashOperator


def my_callback(context):
    print(""partial_callback on success callbak"")


with DAG(dag_id=""partial_callback"", start_date=datetime(2022, 1, 1), schedule=None) as dag:
    hello = BashOperator(
        task_id=""hello"",
        bash_command=""echo hello"",
        on_success_callback=partial(my_callback),
    )

```

### Operating System

Official container image

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

Might be related to this change: https://github.com/apache/airflow/pull/38892/files


A workaround is to set the `__name__` property of the partial function:

```
def _partial_with_name(func, arg):
    partial_with_name = partial(func, arg)
    partial_with_name.__name__ = arg.__name__ if hasattr(arg, ""__name__"") else ""none""
    return partial_with_name
```

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",sseelmann,2024-08-24 12:20:21+00:00,[],2024-08-24 17:35:28+00:00,2024-08-24 17:35:28+00:00,https://github.com/apache/airflow/issues/41711,"[('kind:bug', 'This is a clearly a bug'), ('area:core', '')]","[{'comment_id': 2308375795, 'issue_id': 2484549150, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 24, 12, 20, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2308466370, 'issue_id': 2484549150, 'author': 'eladkal', 'body': 'Duplicated of https://github.com/apache/airflow/issues/41563\r\nFix will be released in 2.10.1', 'created_at': datetime.datetime(2024, 8, 24, 17, 35, 24, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-24 12:20:24 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2024-08-24 17:35:24 UTC): Duplicated of https://github.com/apache/airflow/issues/41563
Fix will be released in 2.10.1

"
2483994341,issue,open,,post_execute() method does not work correctly when used along with a trigger,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I have a new provider to send jobs to a Ray cluster. One of the operators has pre_execute and post_execute operations. Here is the complete flow of control

1. pre_execute() --> Sets up a Ray cluster
2. execute() --> Triggers a new job on the cluster and defers tracking to a trigger
3. async trigger --> tracks the job execution and prints logs
4. execute_complete() --> completes main task execution after the trigger reaches terminal state
5. post_execute() --> Deletes the Ray cluster

When post_execute() starting executing it seems to be running pre_execute() code again. This is a bug. We should only see post_execute code. Logs attached.


[post_execute_bug.txt](https://github.com/user-attachments/files/16734203/post_execute_bug.txt)

### What you think should happen instead?

Only code relevant to the post_execute method must execute.

### How to reproduce

1. Install the provider using the instructions mentioned on [this](https://astronomer.github.io/astro-provider-ray/getting_started/setup.html) github page
2. Run one of the example DAGs that only uses the SubmitRayJob operator in the DAG

You will need access to a k8 cluster and will need to provide connection details as shown in step 1


### Operating System

linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",venkatajagannath,2024-08-23 22:43:19+00:00,[],2024-08-26 11:28:37+00:00,,https://github.com/apache/airflow/issues/41707,"[('kind:bug', 'This is a clearly a bug'), ('area:core', '')]","[{'comment_id': 2307895129, 'issue_id': 2483994341, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 23, 22, 43, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2308186610, 'issue_id': 2483994341, 'author': 'uranusjr', 'body': '(Copying my message from elsewhere with minor edits for context)\r\n\r\nIt’s better to use setup and teardown because they handle errors better.\r\n\r\nFor the reported issue specifically, `pre_execute` is actually executed *every time a worker is started for the task*, not only *when `execute` is called*, despite the name. (This is also true for `on_execution_callback`, by the way.) Resuming a deferred task starts a worker and causes `pre_execute` to be called.\r\n\r\nThis is kind of by design, but I guess there’s a case to be made it’s not a good design. If we are to change this, I would recommend:\r\n\r\n1. Rename `pre_execute` and `on_execution_callback` to remove `execute` from the name to avoid misunderstandings. Document them well to describe the actual behaviour.\r\n2. (Maybe?) Add a hook that’s actually only run when the first worker is started for a ti.\r\n3. (Maybe??) Add hooks that are run when a task is deferred and resumed?', 'created_at': datetime.datetime(2024, 8, 24, 8, 21, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2308397029, 'issue_id': 2483994341, 'author': 'RNHTTR', 'body': '@uranusjr Does it make sense to rename this issue to ""Rename pre_execute and on_execution_callback to remove execute from the name to avoid misunderstandings""?', 'created_at': datetime.datetime(2024, 8, 24, 13, 32, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2308458053, 'issue_id': 2483994341, 'author': 'venkatajagannath', 'body': ""@uranusjr \r\n\r\npre_execute() and post_execute() naming convention is very intuitive. Would it be possible to change the backend design instead? \r\n\r\nI've used setup and teardown. Example [here](https://github.com/astronomer/astro-provider-ray/blob/main/example_dags/setup-teardown.py). But, my current usecase is slightly different. \r\n\r\nIts a much better UX to use just one decorator to spin up/down a cluster and also execute the main job. See example [here](https://github.com/astronomer/astro-provider-ray/blob/c072a5ddfb2a5ab0c896dd5e26c021ffadf47d1e/example_dags/ray_taskflow_example.py). We also hear the same feedback from customers.\r\n\r\nOne idea:\r\n\r\n(Maybe?) we can extend setup/teardown to configure a method within a custom operator as a setup and another as a teardown. These methods will execute similar to setup/teardown in a DAG."", 'created_at': datetime.datetime(2024, 8, 24, 17, 5, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2308641331, 'issue_id': 2483994341, 'author': 'Lee-W', 'body': '> @uranusjr Does it make sense to rename this issue to ""Rename pre_execute and on_execution_callback to remove execute from the name to avoid misunderstandings""?\r\n\r\nI\'m not sure whether we already decide to change it this way. Personally, I prefer 2 and 3. 1 might not help much for one who doesn\'t understand airflow in depth. Providing 2 and 3 will give the user a sense that these things are different', 'created_at': datetime.datetime(2024, 8, 25, 3, 43, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2309906145, 'issue_id': 2483994341, 'author': 'uranusjr', 'body': 'I guess the question is, is a hook that runs every time a worker starts useful for people? We need something for that if we change the semantic of pre_execute (and on_execution_hook).', 'created_at': datetime.datetime(2024, 8, 26, 10, 43, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2309942341, 'issue_id': 2483994341, 'author': 'Lee-W', 'body': ""> I guess the question is, is a hook that runs every time a worker starts useful for people? We need something for that if we change the semantic of pre_execute (and on_execution_hook).\r\n\r\nFor operators works like [S3KeySensor](https://github.com/apache/airflow/blob/e8a59968918e84a6221cd72cb3a8c6ddb563840c/airflow/providers/amazon/aws/sensors/s3.py#L217), I think it's possible 🤔"", 'created_at': datetime.datetime(2024, 8, 26, 11, 5, 41, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-23 22:43:22 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

uranusjr on (2024-08-24 08:21:26 UTC): (Copying my message from elsewhere with minor edits for context)

It’s better to use setup and teardown because they handle errors better.

For the reported issue specifically, `pre_execute` is actually executed *every time a worker is started for the task*, not only *when `execute` is called*, despite the name. (This is also true for `on_execution_callback`, by the way.) Resuming a deferred task starts a worker and causes `pre_execute` to be called.

This is kind of by design, but I guess there’s a case to be made it’s not a good design. If we are to change this, I would recommend:

1. Rename `pre_execute` and `on_execution_callback` to remove `execute` from the name to avoid misunderstandings. Document them well to describe the actual behaviour.
2. (Maybe?) Add a hook that’s actually only run when the first worker is started for a ti.
3. (Maybe??) Add hooks that are run when a task is deferred and resumed?

RNHTTR on (2024-08-24 13:32:40 UTC): @uranusjr Does it make sense to rename this issue to ""Rename pre_execute and on_execution_callback to remove execute from the name to avoid misunderstandings""?

venkatajagannath (Issue Creator) on (2024-08-24 17:05:01 UTC): @uranusjr 

pre_execute() and post_execute() naming convention is very intuitive. Would it be possible to change the backend design instead? 

I've used setup and teardown. Example [here](https://github.com/astronomer/astro-provider-ray/blob/main/example_dags/setup-teardown.py). But, my current usecase is slightly different. 

Its a much better UX to use just one decorator to spin up/down a cluster and also execute the main job. See example [here](https://github.com/astronomer/astro-provider-ray/blob/c072a5ddfb2a5ab0c896dd5e26c021ffadf47d1e/example_dags/ray_taskflow_example.py). We also hear the same feedback from customers.

One idea:

(Maybe?) we can extend setup/teardown to configure a method within a custom operator as a setup and another as a teardown. These methods will execute similar to setup/teardown in a DAG.

Lee-W on (2024-08-25 03:43:03 UTC): I'm not sure whether we already decide to change it this way. Personally, I prefer 2 and 3. 1 might not help much for one who doesn't understand airflow in depth. Providing 2 and 3 will give the user a sense that these things are different

uranusjr on (2024-08-26 10:43:44 UTC): I guess the question is, is a hook that runs every time a worker starts useful for people? We need something for that if we change the semantic of pre_execute (and on_execution_hook).

Lee-W on (2024-08-26 11:05:41 UTC): For operators works like [S3KeySensor](https://github.com/apache/airflow/blob/e8a59968918e84a6221cd72cb3a8c6ddb563840c/airflow/providers/amazon/aws/sensors/s3.py#L217), I think it's possible 🤔

"
2483939988,issue,open,,kubernetes connection defined via env variable does not work with deferrable=True,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I try to run a Kubernetes Pod Operator (v8.4.0) with `deferrable=True` (and `do_xcom_push=True` which was also an issue in [earlier versions](https://github.com/apache/airflow/issues/32458) but not now). I define my kubernetes cluster connection in the code using `airflow.models.connection.Connection` and supply it via an [environment variable](https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html#storing-connections-in-environment-variables).

After all containers (init-container, sidecar and main container) have completed succesfully I get
```python
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 1792, in resume_execution
    return execute_callable(context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/operators/pod.py"", line 773, in trigger_reentry
    raise AirflowException(message)
airflow.exceptions.AirflowException: Traceback (most recent call last):
  File ""/home/***/.local/lib/python3.12/site-packages/***/providers/cncf/kubernetes/triggers/pod.py"", line 162, in run
    state = await self._wait_for_pod_start()
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/***/.local/lib/python3.12/site-packages/***/providers/cncf/kubernetes/triggers/pod.py"", line 223, in _wait_for_pod_start
    pod = await self.hook.get_pod(self.pod_name, self.pod_namespace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/***/.local/lib/python3.12/site-packages/***/providers/cncf/kubernetes/hooks/kubernetes.py"", line 747, in get_pod
    async with self.get_conn() as connection:
  File ""/usr/local/lib/python3.12/contextlib.py"", line 210, in __aenter__
    return await anext(self.gen)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/***/.local/lib/python3.12/site-packages/***/providers/cncf/kubernetes/hooks/kubernetes.py"", line 734, in get_conn
    kube_client = await self._load_config() or async_client.ApiClient()
                  ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/***/.local/lib/python3.12/site-packages/***/providers/cncf/kubernetes/hooks/kubernetes.py"", line 664, in _load_config
    in_cluster = self._coalesce_param(self.in_cluster, await self._get_field(""in_cluster""))
                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/***/.local/lib/python3.12/site-packages/***/providers/cncf/kubernetes/hooks/kubernetes.py"", line 724, in _get_field
    extras = await self.get_conn_extras()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/***/.local/lib/python3.12/site-packages/***/providers/cncf/kubernetes/hooks/kubernetes.py"", line 712, in get_conn_extras
    connection = await sync_to_async(self.get_connection)(self.conn_id)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/***/.local/lib/python3.12/site-packages/asgiref/sync.py"", line 468, in __call__
    ret = await asyncio.shield(exec_coro)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/concurrent/futures/thread.py"", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/***/.local/lib/python3.12/site-packages/asgiref/sync.py"", line 522, in thread_handler
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/***/.local/lib/python3.12/site-packages/***/providers/cncf/kubernetes/hooks/kubernetes.py"", line 169, in get_connection
    return super().get_connection(conn_id)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/***/.local/lib/python3.12/site-packages/***/hooks/base.py"", line 83, in get_connection
    conn = Connection.get_connection_from_secrets(conn_id)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/***/.local/lib/python3.12/site-packages/***/models/connection.py"", line 537, in get_connection_from_secrets
    raise AirflowNotFoundException(f""The conn_id `{conn_id}` isn't defined"")
***.exceptions.AirflowNotFoundException: The conn_id `k8s-conn-id` isn't defined
```

Now if I:
- set `deferrable=False` it works with environment variable connection 
- make an explicit k8s cluster connection in the UI, then `deferrable=True` works
<img width=""703"" alt=""image"" src=""https://github.com/user-attachments/assets/7cb40a9f-d3c0-4371-9672-e92a8a060c9b"">


### What you think should happen instead?

supplying connection via environment variable as described in docs should work exactly like connections via UI

### How to reproduce

make a kubernetes cluster connection 
```python
# create connection
import os
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import (
    KubernetesPodOperator,
)

extra = {
        ""kube_config"": <kube_config to your cluser>,
        ""namespace"": <your namespace>,  
        ""in_cluster"": False,
    }
conn = Connection(conn_id=""k8s-conn-id"", conn_type=""kubernetes"", description=""k8s connection"", extra=extra)
os.environ[f""AIRFLOW_CONN_{conn.conn_id.upper()}""] = conn.get_uri()

# define minimal DAG with  deferrable=True and do_xcom_push=True
with DAG( dag_id=""k8s-example"") as dag:
    k = KubernetesPodOperator(
        kubernetes_conn_id=""k8s-conn-id"",
        name=""k8s-pod"",
        cmds=[""bash"", ""-cx""],
        arguments=[""mkdir -p /airflow/xcom/;cat /tmp/mnt/hello.txt > /airflow/xcom/return.json""],
        task_id=""k8s-pod"",
        startup_timeout_seconds=1000,
        do_xcom_push=True,
        pod_template_file=""k8s_tempalte.yaml""),
        on_finish_action=""keep_pod"", 
        deferrable=True
    )
   
    # just testing XCOM
    b = BashOperator(
        bash_command=""echo \""{{ task_instance.xcom_pull('k8s-pod')[0] }}\"""",
        task_id=""pod_task_xcom_result"",
    )

    k >> b
```
and the manifest
```yaml
apiVersion: v1
kind: Pod
spec:
  initContainers:
    - name: init-container
      image: ""ubuntu:latest""
      command: [""bash"", ""-cx""]
      args: [""echo '[1,2,3,4]' > /tmp/mnt/hello.txt""]
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 1Gi
      volumeMounts:
        - name: shared-volume
          mountPath: ""/tmp/mnt""
  containers:
    - name: base
      image: ""ubuntu:latest""
      imagePullPolicy: IfNotPresent
      ports: []
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 1Gi
      volumeMounts:
        - name: shared-volume
          mountPath: ""/tmp/mnt""
  restartPolicy: Never
  volumes:
  - name: shared-volume
    emptyDir: {}
```



### Operating System

docker

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes==8.4.0
apache-airflow-providers-microsoft-azure==10.3.0

### Deployment

Docker-Compose

### Deployment details

I am running Docker locally with the official [docker-compose](https://airflow.apache.org/docs/apache-airflow/2.10.0/docker-compose.yaml) and deploying pods on Azure Kubernetes Services

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",mayorblock,2024-08-23 21:43:10+00:00,[],2025-01-12 09:27:44+00:00,,https://github.com/apache/airflow/issues/41706,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('area:async-operators', 'AIP-40: Deferrable (""Async"") Operators'), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2307848038, 'issue_id': 2483939988, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 23, 21, 43, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2307878364, 'issue_id': 2483939988, 'author': 'tirkarthi', 'body': 'Connection ID uses hypen k8s-conn-id in the screenshot. Should it be k8s_conn_id ?', 'created_at': datetime.datetime(2024, 8, 23, 22, 19, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2309385064, 'issue_id': 2483939988, 'author': 'mayorblock', 'body': '@tirkarthi good catch :) but unfortunately just a typo when transferring my code to this example. \r\n\r\nThe issue still stands. With kubernetes cluster connection served via environment variable the KPO with `deferrable=True` fails even when the pod runs successfully.', 'created_at': datetime.datetime(2024, 8, 26, 6, 0, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2317730031, 'issue_id': 2483939988, 'author': 'mayorblock', 'body': 'Hi @RNHTTR, politely nudging to hear if there is anything I can do to advance the issue? I am very willing to contribute but will need some guidance :)', 'created_at': datetime.datetime(2024, 8, 29, 13, 53, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2347443442, 'issue_id': 2483939988, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 9, 13, 0, 14, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2351107508, 'issue_id': 2483939988, 'author': 'mayorblock', 'body': 'Hi @RNHTTR, the bot message seems to imply that I should somehow respond. I am unsure what I am supposed to do? You added a pending-response tag; what does it imply?\r\n\r\nthank you in advance', 'created_at': datetime.datetime(2024, 9, 14, 18, 58, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2381033294, 'issue_id': 2483939988, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 9, 29, 0, 16, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2392424127, 'issue_id': 2483939988, 'author': 'RNHTTR', 'body': '@mayorblock sorry about that! I removed the stale/pending-response labels.\r\n\r\n> Hi @RNHTTR, politely nudging to hear if there is anything I can do to advance the issue? I am very willing to contribute but will need some guidance :)\r\n\r\n* Does this Connection work when `deferrable=False`? \r\n* Does this error occur on the latest version of Airflow / latest version of the Kubernetes provider (8.4.2)?\r\n\r\nIf so, I recommend analyzing the traceback for clues. FWIW, I think the precedence for determining Connections is:\r\n\r\n1. Secrets backend (if applicable)\r\n2. Env vars (if applicable -- they are in this case)\r\n3. Airflow DB (that is, Connections created via the UI)', 'created_at': datetime.datetime(2024, 10, 3, 22, 7, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2424702731, 'issue_id': 2483939988, 'author': 'mayorblock', 'body': ""Hi @RNHTTR , no problem :)\r\n\r\n- yes, it does work with `deferrable=False` (or with connection from airflow DB)\r\n- yes, the issue occur for the latest airflow and kubernetes provider versions\r\n\r\nI will step through and try to see where the chain falls off. I'll be back (sometime)"", 'created_at': datetime.datetime(2024, 10, 20, 7, 43, 54, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-23 21:43:13 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

tirkarthi on (2024-08-23 22:19:13 UTC): Connection ID uses hypen k8s-conn-id in the screenshot. Should it be k8s_conn_id ?

mayorblock (Issue Creator) on (2024-08-26 06:00:15 UTC): @tirkarthi good catch :) but unfortunately just a typo when transferring my code to this example. 

The issue still stands. With kubernetes cluster connection served via environment variable the KPO with `deferrable=True` fails even when the pod runs successfully.

mayorblock (Issue Creator) on (2024-08-29 13:53:43 UTC): Hi @RNHTTR, politely nudging to hear if there is anything I can do to advance the issue? I am very willing to contribute but will need some guidance :)

github-actions[bot] on (2024-09-13 00:14:12 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

mayorblock (Issue Creator) on (2024-09-14 18:58:53 UTC): Hi @RNHTTR, the bot message seems to imply that I should somehow respond. I am unsure what I am supposed to do? You added a pending-response tag; what does it imply?

thank you in advance

github-actions[bot] on (2024-09-29 00:16:33 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

RNHTTR on (2024-10-03 22:07:53 UTC): @mayorblock sorry about that! I removed the stale/pending-response labels.


* Does this Connection work when `deferrable=False`? 
* Does this error occur on the latest version of Airflow / latest version of the Kubernetes provider (8.4.2)?

If so, I recommend analyzing the traceback for clues. FWIW, I think the precedence for determining Connections is:

1. Secrets backend (if applicable)
2. Env vars (if applicable -- they are in this case)
3. Airflow DB (that is, Connections created via the UI)

mayorblock (Issue Creator) on (2024-10-20 07:43:54 UTC): Hi @RNHTTR , no problem :)

- yes, it does work with `deferrable=False` (or with connection from airflow DB)
- yes, the issue occur for the latest airflow and kubernetes provider versions

I will step through and try to see where the chain falls off. I'll be back (sometime)

"
2483871336,issue,closed,completed,GKEStartJobOperator's job_poll_interval parameter is not used by its GKEJobTrigger,"### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

apache-airflow-providers-google == 10.21.0

### Apache Airflow version

2.9.3

### Operating System

linux/arm64

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### What happened

My DAG kicks off a Kubernetes job using a GKEStartJobOperator. The operator has the following parameters set:
- deferrable=True
- wait_until_job_complete=True
- job_poll_interval=60

When the task created from the GKEStartJobOperator is executed, the deferred task polls the job every 10 seconds and logs the message `The job 'name-of-my-job' is incomplete. Sleeping for 10 sec.`

### What you think should happen instead

In the above example, the job should poll every 60 seconds, not 10 seconds. 

This is happening because GKEJobTrigger isn't being passed job_poll_interval from GKEStartJobOperator which causes a AsyncKubernetesHook.wait_until_job_complete function to default the value of poll_interval 10.

The faulty code is in airflow/airflow/providers/google/cloud/triggers/kubernetes_engine.py in the method GKEJobTrigger.run on line 320.
This:
```job: V1Job = await self.hook.wait_until_job_complete(name=self.job_name, namespace=self.job_namespace)```
Should be this:
```job: V1Job = await self.hook.wait_until_job_complete(name=self.job_name, namespace=self.job_namespace, poll_interval=self.poll_interval)```

### How to reproduce

1. In GCP, create a GKE cluster with a default node pool. 
2. Create a GCP service account with the `roles/container.developer` role. 
3. Create a connection in Airflow that uses that service account. 
4. Create a DAG that uses a GKEStartJobOperator. 
5. Configure the GKE job to run for 60 seconds. 
6. On the GKEStartJobOperator, set deferrable=True, wait_until_job_complete=True, job_poll_interval to 20 on the GKEStartJobOperator. 
7. Execute the DAG. Verify that the kubernetes job task is deferred at some point.
8. View the logs for the kubernetes job task. Verify that the task polled the job every 10 seconds, not every 20 seconds.

### Anything else

The doc string for GKEStartJobOperator says that the poll interval parameter is called `poll_interval`, but the __init__ says that it should be `job_poll_interval`. The doc string should be changed to say `job_poll_interval`.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",bwatan,2024-08-23 20:49:10+00:00,[],2024-09-01 18:56:13+00:00,2024-09-01 18:56:13+00:00,https://github.com/apache/airflow/issues/41705,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2307791383, 'issue_id': 2483871336, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 23, 20, 49, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2308393496, 'issue_id': 2483871336, 'author': 'gopidesupavan', 'body': 'looking into this.', 'created_at': datetime.datetime(2024, 8, 24, 13, 19, 29, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-23 20:49:13 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

gopidesupavan on (2024-08-24 13:19:29 UTC): looking into this.

"
2483759737,issue,open,,ONNX Model Inference Operator,"### Description

 ONNX (Open Neural Network Exchange) provides cross-platform compatibility

An operator that can run inference using ONNX models, ideal for deploying machine learning models in a standardized format can provide us with direct model invocation.


this can be solved using a pythonOperator ofc as onnxruntime can be executed with pythonruntime, but this can also be built into airflow to minimize work, a simple onnx operator structure would be something like:

```

import onnxruntime as ort
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

def run_onnx_inference():
    # Load the ONNX model
    model_path = '/path/to/your/model.onnx'
    session = ort.InferenceSession(model_path)

    # Prepare input data
    input_name = session.get_inputs()[0].name
    input_data = {""your_input_key"": your_input_data}

    # Run inference
    result = session.run(None, {input_name: input_data})
    print(result)

# Define the DAG
with DAG(
    dag_id='onnx_inference_dag',
    start_date=datetime(2023, 1, 1),
    schedule_interval='@once'
) as dag:

    # Define the task
    inference_task = PythonOperator(
        task_id='onnx_inference_task',
        python_callable=run_onnx_inference
    )


```

Looking frwd to any suggestions.

### Use case/motivation

A direct support of onnx with Airflow's DAG-based orchestration can manage the entire lifecycle of data processing and model inference in one place, providing a more cohesive and manageable workflow.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Faakhir30,2024-08-23 19:30:31+00:00,[],2024-09-04 18:16:43+00:00,,https://github.com/apache/airflow/issues/41702,"[('kind:feature', 'Feature Requests'), ('kind:new provider request', 'label to mark request for adding new provider')]","[{'comment_id': 2307693547, 'issue_id': 2483759737, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 23, 19, 30, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2309936325, 'issue_id': 2483759737, 'author': 'Rohanberiwal', 'body': 'Hi , i have worked on this  issue from past two days and I came up with a solution . I made certin chnage in the exisiting code and  added the execute function inside the operation class that does the same work that your run_onnx_intefence() does . Please see this code and tell me if the code anywhere  matches the frequency of your expections .\r\n\r\n\r\n\r\n```python\r\nimport onnxruntime as ort\r\nfrom airflow.models import BaseOperator\r\nfrom airflow.utils.decorators import apply_defaults\r\nfrom airflow import DAG\r\nfrom datetime import datetime\r\n\r\nclass ONNXInferenceOperator(BaseOperator):\r\n    @apply_defaults\r\n    def __init__(self, model_path: str, input_data: dict, *args, **kwargs):\r\n        super(ONNXInferenceOperator, self).__init__(*args, **kwargs)\r\n        self.model_path = model_path\r\n        self.input_data = input_data\r\n\r\n    def execute(self, context):\r\n        session = ort.InferenceSession(self.model_path)\r\n        input_name = session.get_inputs()[0].name\r\n        result = session.run(None, {input_name: self.input_data})\r\n        self.log.info(f""Inference result: {result}"")\r\n        return result\r\n\r\nwith DAG(\r\n    dag_id=\'onnx_inference_dag\',\r\n    start_date=datetime(2023, 1, 1),\r\n    schedule_interval=\'@once\',\r\n    catchup=False\r\n) as dag:\r\n\r\n    inference_task = ONNXInferenceOperator(\r\n        task_id=\'onnx_inference_task\',\r\n        model_path=\'/path/to/your/model.onnx\',\r\n        input_data={""your_input_key"": [[1.0, 2.0, 3.0]]}\r\n    )\r\n\r\n    inference_task', 'created_at': datetime.datetime(2024, 8, 26, 11, 1, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2312854114, 'issue_id': 2483759737, 'author': 'Rohanberiwal', 'body': 'Hi I was expecting a reply from you ,  whenver you see this do let me know . \r\nThank you .', 'created_at': datetime.datetime(2024, 8, 27, 15, 17, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2314545105, 'issue_id': 2483759737, 'author': 'eladkal', 'body': ""Hi @Rohanberiwal @Faakhir30 Airflow doesn't have Onnx provider thus if you'd like to add it to Airflow you need to follow the protocol of [adding new provider](https://github.com/apache/airflow/blob/main/PROVIDERS.rst#accepting-new-community-providers), Most of providers are [managed by the community](https://github.com/apache/airflow/blob/main/PROVIDERS.rst#community-managed-providers) rather than by Airflow."", 'created_at': datetime.datetime(2024, 8, 28, 7, 32, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2314968922, 'issue_id': 2483759737, 'author': 'Rohanberiwal', 'body': 'Yes sir  , i will read that protocal and I will get back with a solution   as soon as possible .\r\nThank you  for your reply .', 'created_at': datetime.datetime(2024, 8, 28, 10, 45, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2329345801, 'issue_id': 2483759737, 'author': 'Rohanberiwal', 'body': '# ONNX Inference Operator for Apache Airflow\r\n\r\n## Description\r\n\r\nThe `ONNXInferenceOperator` is a custom operator designed for running inference using ONNX models within an Apache Airflow DAG. This operator leverages the `onnxruntime` library to load an ONNX model and perform inference on provided input data. The results of the inference are logged and returned.\r\n\r\n### Components\r\n\r\n1. **ONNXInferenceOperator**: A custom Airflow operator that initializes with the path to the ONNX model and the input data. It performs inference in the `execute` method and logs the results.\r\n\r\n2. **run_onnx_inference**: A helper function that demonstrates how to run inference using the `onnxruntime` library directly within a PythonOperator. This function is provided as an alternative approach to using the custom operator.\r\n\r\n3. **DAG Definition**: Defines an Airflow DAG named `onnx_inference_dag` that schedules the inference task to run once.\r\n\r\n## Code\r\n\r\n```python\r\nimport onnxruntime as ort\r\nfrom airflow import DAG\r\nfrom airflow.models import BaseOperator\r\nfrom airflow.utils.decorators import apply_defaults\r\nfrom airflow.operators.python import PythonOperator\r\nfrom datetime import datetime\r\n\r\nclass ONNXInferenceOperator(BaseOperator):\r\n    @apply_defaults\r\n    def __init__(self, model_path: str, input_data: dict, *args, **kwargs):\r\n        super(ONNXInferenceOperator, self).__init__(*args, **kwargs)\r\n        self.model_path = model_path\r\n        self.input_data = input_data\r\n\r\n    def execute(self, context):\r\n        session = ort.InferenceSession(self.model_path)\r\n        input_name = session.get_inputs()[0].name\r\n        result = session.run(None, {input_name: self.input_data})\r\n        self.log.info(f""Inference result: {result}"")\r\n        return result\r\n\r\ndef run_onnx_inference():\r\n    model_path = \'/path/to/your/model.onnx\'\r\n    session = ort.InferenceSession(model_path)\r\n    input_name = session.get_inputs()[0].name\r\n    input_data = {""your_input_key"": [[1.0, 2.0, 3.0]]}\r\n    result = session.run(None, {input_name: input_data})\r\n    print(result)\r\n\r\nwith DAG(\r\n    dag_id=\'onnx_inference_dag\',\r\n    start_date=datetime(2023, 1, 1),\r\n    schedule_interval=\'@once\',\r\n    catchup=False\r\n) as dag:\r\n\r\n    inference_task = ONNXInferenceOperator(\r\n        task_id=\'onnx_inference_task\',\r\n        model_path=\'/path/to/your/model.onnx\',\r\n        input_data={""your_input_key"": [[1.0, 2.0, 3.0]]}\r\n    )', 'created_at': datetime.datetime(2024, 9, 4, 15, 14, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2329350066, 'issue_id': 2483759737, 'author': 'Rohanberiwal', 'body': 'Sir I would like to know more about the official process of gertting accpeted and work for the airflow  , I have made the solution and read the protocal but where shoudl I have to raise a vote , so I can have a comversation with the people and they accept me .  \r\nShould I add teh lable of new use in the above proposed solution ?', 'created_at': datetime.datetime(2024, 9, 4, 15, 16, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2329699998, 'issue_id': 2483759737, 'author': 'potiuk', 'body': ""It's all explained there - including (as of recently) links to examples where others attempted to propose their providers:  https://github.com/apache/airflow/blob/main/PROVIDERS.rst#accepting-new-community-providers \r\n\r\nNote - taht It's rather unll"", 'created_at': datetime.datetime(2024, 9, 4, 18, 16, 42, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-23 19:30:35 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

Rohanberiwal on (2024-08-26 11:01:59 UTC): Hi , i have worked on this  issue from past two days and I came up with a solution . I made certin chnage in the exisiting code and  added the execute function inside the operation class that does the same work that your run_onnx_intefence() does . Please see this code and tell me if the code anywhere  matches the frequency of your expections .



```python
import onnxruntime as ort
from airflow.models import BaseOperator
from airflow.utils.decorators import apply_defaults
from airflow import DAG
from datetime import datetime

class ONNXInferenceOperator(BaseOperator):
    @apply_defaults
    def __init__(self, model_path: str, input_data: dict, *args, **kwargs):
        super(ONNXInferenceOperator, self).__init__(*args, **kwargs)
        self.model_path = model_path
        self.input_data = input_data

    def execute(self, context):
        session = ort.InferenceSession(self.model_path)
        input_name = session.get_inputs()[0].name
        result = session.run(None, {input_name: self.input_data})
        self.log.info(f""Inference result: {result}"")
        return result

with DAG(
    dag_id='onnx_inference_dag',
    start_date=datetime(2023, 1, 1),
    schedule_interval='@once',
    catchup=False
) as dag:

    inference_task = ONNXInferenceOperator(
        task_id='onnx_inference_task',
        model_path='/path/to/your/model.onnx',
        input_data={""your_input_key"": [[1.0, 2.0, 3.0]]}
    )

    inference_task

Rohanberiwal on (2024-08-27 15:17:47 UTC): Hi I was expecting a reply from you ,  whenver you see this do let me know . 
Thank you .

eladkal on (2024-08-28 07:32:50 UTC): Hi @Rohanberiwal @Faakhir30 Airflow doesn't have Onnx provider thus if you'd like to add it to Airflow you need to follow the protocol of [adding new provider](https://github.com/apache/airflow/blob/main/PROVIDERS.rst#accepting-new-community-providers), Most of providers are [managed by the community](https://github.com/apache/airflow/blob/main/PROVIDERS.rst#community-managed-providers) rather than by Airflow.

Rohanberiwal on (2024-08-28 10:45:21 UTC): Yes sir  , i will read that protocal and I will get back with a solution   as soon as possible .
Thank you  for your reply .

Rohanberiwal on (2024-09-04 15:14:45 UTC): # ONNX Inference Operator for Apache Airflow

## Description

The `ONNXInferenceOperator` is a custom operator designed for running inference using ONNX models within an Apache Airflow DAG. This operator leverages the `onnxruntime` library to load an ONNX model and perform inference on provided input data. The results of the inference are logged and returned.

### Components

1. **ONNXInferenceOperator**: A custom Airflow operator that initializes with the path to the ONNX model and the input data. It performs inference in the `execute` method and logs the results.

2. **run_onnx_inference**: A helper function that demonstrates how to run inference using the `onnxruntime` library directly within a PythonOperator. This function is provided as an alternative approach to using the custom operator.

3. **DAG Definition**: Defines an Airflow DAG named `onnx_inference_dag` that schedules the inference task to run once.

## Code

```python
import onnxruntime as ort
from airflow import DAG
from airflow.models import BaseOperator
from airflow.utils.decorators import apply_defaults
from airflow.operators.python import PythonOperator
from datetime import datetime

class ONNXInferenceOperator(BaseOperator):
    @apply_defaults
    def __init__(self, model_path: str, input_data: dict, *args, **kwargs):
        super(ONNXInferenceOperator, self).__init__(*args, **kwargs)
        self.model_path = model_path
        self.input_data = input_data

    def execute(self, context):
        session = ort.InferenceSession(self.model_path)
        input_name = session.get_inputs()[0].name
        result = session.run(None, {input_name: self.input_data})
        self.log.info(f""Inference result: {result}"")
        return result

def run_onnx_inference():
    model_path = '/path/to/your/model.onnx'
    session = ort.InferenceSession(model_path)
    input_name = session.get_inputs()[0].name
    input_data = {""your_input_key"": [[1.0, 2.0, 3.0]]}
    result = session.run(None, {input_name: input_data})
    print(result)

with DAG(
    dag_id='onnx_inference_dag',
    start_date=datetime(2023, 1, 1),
    schedule_interval='@once',
    catchup=False
) as dag:

    inference_task = ONNXInferenceOperator(
        task_id='onnx_inference_task',
        model_path='/path/to/your/model.onnx',
        input_data={""your_input_key"": [[1.0, 2.0, 3.0]]}
    )

Rohanberiwal on (2024-09-04 15:16:39 UTC): Sir I would like to know more about the official process of gertting accpeted and work for the airflow  , I have made the solution and read the protocal but where shoudl I have to raise a vote , so I can have a comversation with the people and they accept me .  
Should I add teh lable of new use in the above proposed solution ?

potiuk on (2024-09-04 18:16:42 UTC): It's all explained there - including (as of recently) links to examples where others attempted to propose their providers:  https://github.com/apache/airflow/blob/main/PROVIDERS.rst#accepting-new-community-providers 

Note - taht It's rather unll

"
2482140015,issue,closed,completed,worker: Warm shutdown (MainProcess),"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Workers kept going to warm shutdown

Liveness probe failed: Error: No nodes replied within time constraint

worker: Warm shutdown (MainProcess)
[2024-08-23 02:46:40 +0000] [95] [INFO] Handling signal: term
[2024-08-23 02:46:40 +0000] [97] [INFO] Worker exiting (pid: 97)
[2024-08-23 02:46:40 +0000] [96] [INFO] Worker exiting (pid: 96)
[2024-08-23 02:46:40 +0000] [95] [INFO] Shutting down: Master

### What you think should happen instead?

The workers use to be alive.

### How to reproduce

helm repo add apache-airflow https://airflow.apache.org
helm upgrade --install airflow apache-airflow/airflow --namespace airflow --create-namespace

### Operating System

kubernetes 1.29

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

helm repo add apache-airflow https://airflow.apache.org
helm upgrade --install airflow apache-airflow/airflow --namespace airflow --create-namespace

### Anything else?

CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d celery@$(hostname)
Error: No nodes replied within time constraint

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",fzhan,2024-08-23 02:57:43+00:00,[],2024-09-11 03:19:09+00:00,2024-09-11 03:19:09+00:00,https://github.com/apache/airflow/issues/41685,"[('kind:bug', 'This is a clearly a bug'), ('area:helm-chart', 'Airflow Helm Chart'), ('pending-response', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2306090373, 'issue_id': 2482140015, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 23, 2, 57, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2321098170, 'issue_id': 2482140015, 'author': 'jscheffl', 'body': 'Warm shutdown is triggered if a SIGINT is raised to the celery process. Can you please check your K8s events is a liveness probe kicked-in or some re-balancing in your Kubernetes was requesting to shut down a node?\r\nWarm shutdown is not usually triggered by the application itself except if a deployment change is made.', 'created_at': datetime.datetime(2024, 8, 30, 12, 33, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2321101951, 'issue_id': 2482140015, 'author': 'jscheffl', 'body': 'Also it would be good - not written in text - was this happening once? After how many tasks/time? Continously? On how many nodes? Any side effects of failed tasks?', 'created_at': datetime.datetime(2024, 8, 30, 12, 34, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2342537714, 'issue_id': 2482140015, 'author': 'fzhan', 'body': '@jscheffl thanks for the update and sorry for late reply, just being experimenting with different settings.\r\n\r\nSo after a fresh deployment, with no restriction to the resources, scheduler seems to be running quite stable. The latest log I can see before a restart is:\r\n```[2024-09-10T15:25:15.529+0000] {job.py:229} INFO - Heartbeat recovered after 98.81 seconds\r\n[2024-09-10T15:26:17.797+0000] {scheduler_job_runner.py:260} INFO - Exiting gracefully upon receiving signal 15\r\n[2024-09-10T15:26:18.799+0000] {process_utils.py:132} INFO - Sending 15 to group 13035. PIDs of all processes in the group: [28335, 28336, 13035]\r\n[2024-09-10T15:26:18.799+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 13035\r\n```\r\nPrior to that, over the couple of days, it accumulated more than 400+ restarts.', 'created_at': datetime.datetime(2024, 9, 11, 3, 19, 9, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-23 02:57:46 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jscheffl on (2024-08-30 12:33:46 UTC): Warm shutdown is triggered if a SIGINT is raised to the celery process. Can you please check your K8s events is a liveness probe kicked-in or some re-balancing in your Kubernetes was requesting to shut down a node?
Warm shutdown is not usually triggered by the application itself except if a deployment change is made.

jscheffl on (2024-08-30 12:34:31 UTC): Also it would be good - not written in text - was this happening once? After how many tasks/time? Continously? On how many nodes? Any side effects of failed tasks?

fzhan (Issue Creator) on (2024-09-11 03:19:09 UTC): @jscheffl thanks for the update and sorry for late reply, just being experimenting with different settings.

So after a fresh deployment, with no restriction to the resources, scheduler seems to be running quite stable. The latest log I can see before a restart is:
```[2024-09-10T15:25:15.529+0000] {job.py:229} INFO - Heartbeat recovered after 98.81 seconds
[2024-09-10T15:26:17.797+0000] {scheduler_job_runner.py:260} INFO - Exiting gracefully upon receiving signal 15
[2024-09-10T15:26:18.799+0000] {process_utils.py:132} INFO - Sending 15 to group 13035. PIDs of all processes in the group: [28335, 28336, 13035]
[2024-09-10T15:26:18.799+0000] {process_utils.py:87} INFO - Sending the signal 15 to group 13035
```
Prior to that, over the couple of days, it accumulated more than 400+ restarts.

"
2481693483,issue,closed,completed,Airflow DAG access control permissions are not working,"### Apache Airflow Provider(s)

fab

### Versions of Apache Airflow Providers

```apache-airflow-providers-celery==3.7.3
apache-airflow-providers-common-compat==1.1.0
apache-airflow-providers-common-io==1.4.0
apache-airflow-providers-common-sql==1.15.0
apache-airflow-providers-fab==1.2.2
apache-airflow-providers-ftp==3.10.1
apache-airflow-providers-http==4.12.0
apache-airflow-providers-imap==3.6.1
apache-airflow-providers-smtp==1.7.1
apache-airflow-providers-sqlite==3.8.2```

### Apache Airflow version

2.10.0

### Operating System

PRETTY_NAME=""Ubuntu 22.04.4 LTS"" NAME=""Ubuntu"" VERSION_ID=""22.04"" VERSION=""22.04.4 LTS (Jammy Jellyfish)"" VERSION_CODENAME=jammy ID=ubuntu ID_LIKE=debian HOME_URL=""https://www.ubuntu.com/"" SUPPORT_URL=""https://help.ubuntu.com/"" BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/"" PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"" UBUNTU_CODENAME=jammy

### Deployment

Virtualenv installation

### Deployment details

_No response_

### What happened

The Airflow DAG level permission module is having an issue when we specify permission inside the individual DAG.

```Traceback (most recent call last):
  File ""/data/airflow/bin/airflow"", line 8, in <module>
    sys.exit(main())
  File ""/data/airflow/lib/python3.10/site-packages/airflow/__main__.py"", line 62, in main
    args.func(args)
  File ""/data/airflow/lib/python3.10/site-packages/airflow/cli/cli_config.py"", line 49, in command
    return func(*args, **kwargs)
  File ""/data/airflow/lib/python3.10/site-packages/airflow/utils/cli.py"", line 115, in wrapper
    return f(*args, **kwargs)
  File ""/data/airflow/lib/python3.10/site-packages/airflow/utils/providers_configuration_loader.py"", line 55, in wrapped_function
    return func(*args, **kwargs)
  File ""/data/airflow/lib/python3.10/site-packages/airflow/providers/fab/auth_manager/cli_commands/sync_perm_command.py"", line 39, in sync_perm
    appbuilder.sm.create_dag_specific_permissions()
  File ""/data/airflow/lib/python3.10/site-packages/airflow/providers/fab/auth_manager/security_manager/override.py"", line 1076, in create_dag_specific_permissions
    self.sync_perm_for_dag(dag_resource_name, dag.access_control)
  File ""/data/airflow/lib/python3.10/site-packages/airflow/providers/fab/auth_manager/security_manager/override.py"", line 1119, in sync_perm_for_dag
    self._sync_dag_view_permissions(dag_resource_name, access_control)
  File ""/data/airflow/lib/python3.10/site-packages/airflow/providers/fab/auth_manager/security_manager/override.py"", line 1174, in _sync_dag_view_permissions
    raise AirflowException(
airflow.exceptions.AirflowException: The access_control map for DAG 'DAG:example_dag_1' includes the following invalid permissions: {'DAGs'}; The set of valid permissions is: {'can_edit', 'can_read', 'can_delete'}```

/data/airflow/lib/python3.10/site-packages/airflow/providers/fab/auth_manager/security_manager/override.py 
```invalid_action_names = action_names - self.DAG_ACTIONS
            if invalid_action_names:
                raise AirflowException(
                    f""The access_control map for DAG '{dag_resource_name}' includes ""
                    f""the following invalid permissions: {invalid_action_names}; ""
                    f""The set of valid permissions is: {self.DAG_ACTIONS}""
                )
```
it is returning invalid_action_names all the time because of unexpected json. 
`{'DAGs': {'can_edit', 'can_read', 'can_delete'}}`
it should have only `{'can_edit', 'can_read', 'can_delete'}`




### What you think should happen instead

```
if isinstance(perms, (set, list)):
    # Support for old-style access_control where only the actions are specified
    updated_access_control[role][permissions.RESOURCE_DAG] = set(perms)
else:
    updated_access_control[role] = perms
```
This code is having issue.



### How to reproduce


1. Just installed 2.10.0 version. 
2. Create one empty role named - readonly
3. Create one DAG with below access control
```
from airflow import DAG
from airflow.operators.dummy_operator import DummyOperator
from datetime import datetime

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2024, 8, 1),
}

dag = DAG(
    'example_dag_1',
    default_args=default_args,
    schedule_interval='@daily',    
    access_control={ 'readonly': {'can_read', 'can_edit', 'can_delete'} },
)

t1 = DummyOperator(
    task_id='dummy_task',
    dag=dag
)

```


### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",aporwal17,2024-08-22 20:48:03+00:00,[],2024-08-24 17:41:39+00:00,2024-08-24 17:41:39+00:00,https://github.com/apache/airflow/issues/41684,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('area:auth', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:fab', '')]","[{'comment_id': 2305607457, 'issue_id': 2481693483, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 22, 20, 48, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2305676764, 'issue_id': 2481693483, 'author': 'joaopamaral', 'body': ""Hi @aporwal17, the fix for this issue in FAB versions < 1.3.0 is already merged and will be available in version 2.10.1 https://github.com/apache/airflow/pull/41549. \r\n\r\nAnd FAB version 1.3.0 is already released (this version doesn't need the fix)."", 'created_at': datetime.datetime(2024, 8, 22, 21, 22, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2305867587, 'issue_id': 2481693483, 'author': 'aporwal17', 'body': '@joaopamaral : Thanks for the update.', 'created_at': datetime.datetime(2024, 8, 22, 22, 41, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2308467989, 'issue_id': 2481693483, 'author': 'eladkal', 'body': 'closing as fixed', 'created_at': datetime.datetime(2024, 8, 24, 17, 41, 39, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-22 20:48:06 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

joaopamaral on (2024-08-22 21:22:57 UTC): Hi @aporwal17, the fix for this issue in FAB versions < 1.3.0 is already merged and will be available in version 2.10.1 https://github.com/apache/airflow/pull/41549. 

And FAB version 1.3.0 is already released (this version doesn't need the fix).

aporwal17 (Issue Creator) on (2024-08-22 22:41:14 UTC): @joaopamaral : Thanks for the update.

eladkal on (2024-08-24 17:41:39 UTC): closing as fixed

"
2481489802,issue,closed,completed,Simple auth manager,"### Description

The purpose of this task is to describe the auth manager that is intended to be the default auth manager in Airflow 3. For convenience, we'll call it simple auth manager. See [auth manager documentation](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/auth-manager.html) for more details about what is an auth manager.

### Use case/motivation

As part of AIP-79, we want to remove FAB from Airflow 3. The current default auth manager in Airflow 2 is based on FAB ([FabAuthManager](https://github.com/apache/airflow/blob/main/airflow/providers/fab/auth_manager/fab_auth_manager.py)). Therefore, to achieve this goal, we need to find a replacement of `FabAuthManager` as default auth manager in Airflow 3.

The intended usage of this new auth manager is only for development and testing purposes. It should not be used in production. It should only support some default set of Roles and no flexibility in defining roles or mapping them to capabilities (resource types it can access and whether they can be read or written). For production use cases, other auth managers will be implemented later/separately (e.g. KeyCloak auth manager, Casdoor auth manager, ...).

Simple auth manager is fully config-controlled, it will not use the database.

#### List of users

The idea is to have the list of users defined in config. The current config format used in Airflow (INI format) does not allow lists, therefore a workaround will need to be found to represent such list of users. This can be done during implementation. On the high level, the list of users will look like this. The format used here is JSON but this is just for the example, as mentioned before, the exact format will be defined during implementation.

```
users = [
  {
    username: ""admin"",
    password: ""admin""
    role: ""Admin""
  },
  {
    username: ""john"",
    password: ""my-secret-password""
    role: ""Viewer""
  }
]
```

_If the Airflow config format changes in the future (say, TOML), we will consider leveraging this new format to represent the different users._ 

#### Roles
The roles will be defined as part of the simple auth manager. It will not be configurable neither extendable. It will not be possible by the user to configure/add/modify roles in the simple auth manager. The predefined roles in the simple auth manager are the roles [coming out of the box in `FabAuthManager` as defined in the documentation](https://airflow.apache.org/docs/apache-airflow-providers-fab/stable/auth-manager/access-control.html). Here are the roles:
- Viewer
- User
- Op
- Admin

The permissions associated to each role will correspond to the permissions defined in [the documentation ](https://airflow.apache.org/docs/apache-airflow-providers-fab/stable/auth-manager/access-control.html).

#### Disable authentication

For development purposes, we might want to disable the authentication and give all permissions to anyone accessing the Airflow environment. The simple auth manager will have such option that can be set through configuration.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vincbeck,2024-08-22 18:45:49+00:00,['vincbeck'],2024-10-10 19:42:37+00:00,2024-10-10 19:42:37+00:00,https://github.com/apache/airflow/issues/41683,"[('kind:feature', 'Feature Requests'), ('area:auth', ''), ('AIP-79', '')]","[{'comment_id': 2305416378, 'issue_id': 2481489802, 'author': 'vincbeck', 'body': '@potiuk @jedcunningham', 'created_at': datetime.datetime(2024, 8, 22, 18, 46, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2305421527, 'issue_id': 2481489802, 'author': 'jedcunningham', 'body': 'An easy place to put the config (in the meantime?) would be in the webserver_config. I imagine that will live on regardless of whether FAB is used or not.', 'created_at': datetime.datetime(2024, 8, 22, 18, 49, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2305426742, 'issue_id': 2481489802, 'author': 'vincbeck', 'body': '> An easy place to put the config (in the meantime?) would be in the webserver_config. I imagine that will live on regardless of whether FAB is used or not.\r\n\r\nNice, I did not think about that. That would work nicely indeed', 'created_at': datetime.datetime(2024, 8, 22, 18, 53, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2308773283, 'issue_id': 2481489802, 'author': 'potiuk', 'body': '> An easy place to put the config (in the meantime?) would be in the webserver_config. I imagine that will live on regardless of whether FAB is used or not.\r\n\r\n> Nice, I did not think about that. That would work nicely indeed\r\n\r\nYes. Then it will make it python-configuration, not config file.\r\n\r\n> The idea is to have the list of users defined in config. The current config format used in Airflow (INI format) does not allow lists, therefore a workaround will need to be found to represent such list of users. This can be done during implementation. On the high level, the list of users will look like this. The format used here is JSON but this is just for the example, as mentioned before, the exact format will be defined during implementation.\r\n\r\nAnd yes as the next step - part of the https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-67+Multi-team+deployment+of+Airflow+components was to change airflow config to use TOML (which is basically an extension of .ini) - we could start implementing it next and move configuration there - but webserver config is better to start with.\r\n\r\n\r\nOther than that - yes - that sounds like a good description of the auth manager. Also I think it should have an easy way to disable authentication - (say via additional configuration) - then it could use ""admin"" user by default and could be set in whatever development environment we will have in Airflow 3.', 'created_at': datetime.datetime(2024, 8, 25, 10, 38, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2310355655, 'issue_id': 2481489802, 'author': 'vincbeck', 'body': '> Other than that - yes - that sounds like a good description of the auth manager. Also I think it should have an easy way to disable authentication - (say via additional configuration) - then it could use ""admin"" user by default and could be set in whatever development environment we will have in Airflow 3.\r\n\r\nTrue, I forgot to mention this feature. Let me update the issue', 'created_at': datetime.datetime(2024, 8, 26, 14, 28, 5, tzinfo=datetime.timezone.utc)}]","vincbeck (Issue Creator) on (2024-08-22 18:46:31 UTC): @potiuk @jedcunningham

jedcunningham on (2024-08-22 18:49:54 UTC): An easy place to put the config (in the meantime?) would be in the webserver_config. I imagine that will live on regardless of whether FAB is used or not.

vincbeck (Issue Creator) on (2024-08-22 18:53:11 UTC): Nice, I did not think about that. That would work nicely indeed

potiuk on (2024-08-25 10:38:38 UTC): Yes. Then it will make it python-configuration, not config file.


And yes as the next step - part of the https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-67+Multi-team+deployment+of+Airflow+components was to change airflow config to use TOML (which is basically an extension of .ini) - we could start implementing it next and move configuration there - but webserver config is better to start with.


Other than that - yes - that sounds like a good description of the auth manager. Also I think it should have an easy way to disable authentication - (say via additional configuration) - then it could use ""admin"" user by default and could be set in whatever development environment we will have in Airflow 3.

vincbeck (Issue Creator) on (2024-08-26 14:28:05 UTC): True, I forgot to mention this feature. Let me update the issue

"
2480730580,issue,closed,completed,OpenLineage - dag_run.update_state() - AssertionError: daemonic processes are not allowed to have children,"### Apache Airflow Provider(s)

openlineage

### Versions of Apache Airflow Providers

apache-airflow-providers-openlineage==1.10.0

### Apache Airflow version

2.9.3

### Operating System

NAME=""Amazon Linux"" VERSION=""2"" ID=""amzn""

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### What happened

After upgrading the openlineage provider from 1.7.1 to 1.10.0 we have been encountering the following error:

```
[2024-08-22, 08:53:52 CEST] {{taskinstance.py:2906}} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/home/airflow/airflow_env/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 466, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File ""/home/airflow/airflow_env/lib/python3.10/site-packages/airflow/models/taskinstance.py"", line 433, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File ""/home/airflow/airflow_env/lib/python3.10/site-packages/airflow/models/baseoperator.py"", line 401, in wrapper
    return func(self, *args, **kwargs)
  File ""/usr/local/airflow/repo/elt/dataflows/operators/check_operator.py"", line 43, in execute
    self.set_previous_task_instance_success(context)
  File ""/usr/local/airflow/repo/elt/dataflows/operators/check_operator.py"", line 70, in set_previous_task_instance_success
    previous_dag_run.update_state()
  File ""/home/airflow/airflow_env/lib/python3.10/site-packages/airflow/utils/session.py"", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File ""/home/airflow/airflow_env/lib/python3.10/site-packages/airflow/models/dagrun.py"", line 821, in update_state
    self.notify_dagrun_state_changed(msg=""task_failure"")
  File ""/home/airflow/airflow_env/lib/python3.10/site-packages/airflow/models/dagrun.py"", line 984, in notify_dagrun_state_changed
    get_listener_manager().hook.on_dag_run_failed(dag_run=self, msg=msg)
  File ""/home/airflow/airflow_env/lib/python3.10/site-packages/pluggy/_hooks.py"", line 513, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
  File ""/home/airflow/airflow_env/lib/python3.10/site-packages/pluggy/_manager.py"", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File ""/home/airflow/airflow_env/lib/python3.10/site-packages/pluggy/_callers.py"", line 139, in _multicall
    raise exception.with_traceback(exception.__traceback__)
  File ""/home/airflow/airflow_env/lib/python3.10/site-packages/pluggy/_callers.py"", line 103, in _multicall
    res = hook_impl.function(*args)
  File ""/home/airflow/airflow_env/lib/python3.10/site-packages/airflow/providers/openlineage/plugins/listener.py"", line 467, in on_dag_run_failed
    self.executor.submit(self.adapter.dag_failed, dag_run=dag_run, msg=msg)
  File ""/lib/python3.10/concurrent/futures/process.py"", line 738, in submit
    self._start_executor_manager_thread()
  File ""/lib/python3.10/concurrent/futures/process.py"", line 678, in _start_executor_manager_thread
    self._launch_processes()
  File ""/lib/python3.10/concurrent/futures/process.py"", line 705, in _launch_processes
    self._spawn_process()
  File ""/lib/python3.10/concurrent/futures/process.py"", line 714, in _spawn_process
    p.start()
  File ""/lib/python3.10/multiprocessing/process.py"", line 118, in start
    assert not _current_process._config.get('daemon'), \
AssertionError: daemonic processes are not allowed to have children
```

We have a check operator that runs a check that may fail. On the next run of that check, it marks the check in the previous DAGRun as success so as to not clutter the Airflow UI, then runs `update_state()` on the previous DAGRun. This is done so that the previous DAGRun itself is marked as success, iff all of its tasks are in a success/skipped state.

### What you think should happen instead

dag_run.update_state() should be callable from within an Operator without error with the OpenLineage provider enabled.

### How to reproduce

```
import logging

from airflow.models import BaseOperator, DagRun
from airflow.utils.state import TaskInstanceState


class CheckOperator(BaseOperator):

    def execute(self, context):
        log = logging.getLogger(__name__)
        log.info(""Attempting to set previous task instance state to SUCCESS"")

        dag_runs = DagRun.find(dag_id=self.dag_id)
        dag_runs.sort(key=lambda x: x.execution_date)
        dag_run_ids = [dag_run.run_id for dag_run in dag_runs]

        previous_dag_run_index = dag_run_ids.index(context[""dag_run""].run_id) - 1
        previous_dag_run = dag_runs[previous_dag_run_index]

        previous_task_instance = previous_dag_run.get_task_instance(self.task_id)
        if previous_task_instance:
            state_changed = previous_task_instance.set_state(TaskInstanceState.SUCCESS)
            if state_changed:
                # Set the entire DagRun to SUCCESS if all TIs are now set to SUCCESS/SKIPPED
                previous_dag_run.dag = self.dag
                previous_dag_run.update_state()
            else:
                log.info(""Previous task instance state was already SUCCESS"")
        else:
            log.info(""No previous task instances to mark as SUCCESS"")
```


1. Set the AIRFLOW__OPENLINEAGE__TRANSPORT variable
2. Add the above operator to a DAG
3. Trigger a manual DAG run
4. Mark it as failed, including the tasks
5. Trigger another manual DAG run
6. The error should now be shown

### Anything else

I strongly suspect this issue is caused by https://github.com/apache/airflow/pull/39235 which was [included in 1.8.0](https://airflow.apache.org/docs/apache-airflow-providers-openlineage/stable/changelog.html#id8), as the issue started for us after upgrading from 1.7.1 to 1.10.0.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",internetcoffeephone,2024-08-22 12:58:51+00:00,[],2025-01-08 18:17:50+00:00,2025-01-08 18:17:50+00:00,https://github.com/apache/airflow/issues/41676,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:openlineage', 'AIP-53')]","[{'comment_id': 2578329146, 'issue_id': 2480730580, 'author': 'mobuchowski', 'body': 'I believe this was fixed in https://github.com/apache/airflow/pull/41690', 'created_at': datetime.datetime(2025, 1, 8, 18, 17, 50, tzinfo=datetime.timezone.utc)}]","mobuchowski on (2025-01-08 18:17:50 UTC): I believe this was fixed in https://github.com/apache/airflow/pull/41690

"
2480720517,issue,closed,not_planned,"""cannot pickle '_cffi_backend.FFI' object"" error in python/branch operator processing","### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

An error is displayed (see below)  in the log when processing the branch operator. Even if the branch operator behaves correctly in terms of functionality, the error in the log is very misleading

error text:
default_host
[2024-08-22, 14:17:04 CEST] {local_task_job_runner.py:123} ▶ Pre task execution logs
[2024-08-22, 14:17:04 CEST] {baseoperator.py:405} WARNING - BranchPythonOperator.execute cannot be called outside TaskInstance!
[2024-08-22, 14:17:05 CEST] {python.py:240} INFO - Done. Returned value was: ['False_DAY_IN_INTERVAL']
[2024-08-22, 14:17:05 CEST] {branch.py:38} INFO - Branch into ['False_DAY_IN_INTERVAL']
[2024-08-22, 14:17:05 CEST] {skipmixin.py:230} INFO - Following branch ('False_DAY_IN_INTERVAL',)
[2024-08-22, 14:17:05 CEST] {skipmixin.py:278} INFO - Skipping tasks [('True_DAY_IN_INTERVAL', -1)]
[2024-08-22, 14:17:05 CEST] {taskinstance.py:340} ▼ Post task execution logs
**[2024-08-22, 14:17:05 CEST] {taskinstance.py:352} INFO - Marking task as SUCCESS**. dag_id=TEST_BUILT_IN_FCTS, task_id=IF_DAY_IN_INTERVAL_1_TO_20, run_id=manual__2024-08-22T14:16:35+02:00, execution_date=20240822T121635, start_date=20240822T121704, end_date=20240822T121705
[2024-08-22, 14:17:05 CEST] {local_task_job_runner.py:261} INFO - Task exited with return code 0
**[2024-08-22, 14:17:05 CEST] {taskinstance.py:3916} ERROR** - Error scheduling downstream tasks. Skipping it as this is entirely optional optimisation. There might be various reasons for it, please take a look at the stack trace to figure out if the root cause can be diagnosed and fixed. See the issue https://github.com/apache/airflow/issues/39717 for details and an example problem. If you would like to get help in solving root cause, open discussion with all details with your managed service support or in Airflow repository.
Traceback (most recent call last):
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 3912, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/utils/session.py"", line 94, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 3861, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/dag.py"", line 2645, in partial_subset
    dag.task_dict = {
                    ^
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/dag.py"", line 2646, in <dictcomp>
    t.task_id: _deepcopy_task(t)
               ^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/dag.py"", line 2643, in _deepcopy_task
    return copy.deepcopy(t, memo)
           ^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib64/python3.11/copy.py"", line 153, in deepcopy
    y = copier(memo)
        ^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/airflow/models/baseoperator.py"", line 1388, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
                       ^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib64/python3.11/copy.py"", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib64/python3.11/copy.py"", line 271, in _reconstruct
    state = deepcopy(state, memo)
            ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib64/python3.11/copy.py"", line 146, in deepcopy
    y = copier(x, memo)
        ^^^^^^^^^^^^^^^
  File ""/usr/lib64/python3.11/copy.py"", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
                             ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib64/python3.11/copy.py"", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib64/python3.11/copy.py"", line 271, in _reconstruct
    state = deepcopy(state, memo)
            ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib64/python3.11/copy.py"", line 146, in deepcopy
    y = copier(x, memo)
        ^^^^^^^^^^^^^^^
  File ""/usr/lib64/python3.11/copy.py"", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
                             ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib64/python3.11/copy.py"", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib64/python3.11/copy.py"", line 271, in _reconstruct
    state = deepcopy(state, memo)
            ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib64/python3.11/copy.py"", line 146, in deepcopy
    y = copier(x, memo)
        ^^^^^^^^^^^^^^^
  File ""/usr/lib64/python3.11/copy.py"", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
                             ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib64/python3.11/copy.py"", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib64/python3.11/copy.py"", line 271, in _reconstruct
    state = deepcopy(state, memo)
            ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib64/python3.11/copy.py"", line 146, in deepcopy
    y = copier(x, memo)
        ^^^^^^^^^^^^^^^
  File ""/usr/lib64/python3.11/copy.py"", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
                             ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib64/python3.11/copy.py"", line 161, in deepcopy
    rv = reductor(4)
         ^^^^^^^^^^^
TypeError: cannot pickle '_cffi_backend.FFI' object
[2024-08-22, 14:17:05 CEST] {local_task_job_runner.py:240} ▲▲▲ Log group end

### What you think should happen instead?

There should be no error in the error log.

### How to reproduce

run some dag with branch operator

### Operating System

Kubernetes on Unix platform

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Pad71,2024-08-22 12:54:17+00:00,[],2024-09-27 12:32:15+00:00,2024-09-27 12:32:15+00:00,https://github.com/apache/airflow/issues/41675,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2304618875, 'issue_id': 2480720517, 'author': 'Pad71', 'body': 'Same error is displayed in the log of python operators', 'created_at': datetime.datetime(2024, 8, 22, 13, 2, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304622389, 'issue_id': 2480720517, 'author': 'potiuk', 'body': 'What would be your proposal for better error message in this case? - We do not know why your task is not serializable, so we cannot really help here.', 'created_at': datetime.datetime(2024, 8, 22, 13, 4, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304628441, 'issue_id': 2480720517, 'author': 'potiuk', 'body': 'Generallly speaking - this is **some** problem with the DAG that it cannot be deepcopied. You can also disable ""schedule after task execution"" if you cannot change the DAG.', 'created_at': datetime.datetime(2024, 8, 22, 13, 7, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304629502, 'issue_id': 2480720517, 'author': 'Pad71', 'body': 'well, I m just airflow user and for me this is an error that was not present in the previous version.', 'created_at': datetime.datetime(2024, 8, 22, 13, 7, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304631611, 'issue_id': 2480720517, 'author': 'Pad71', 'body': '> Generallly speaking - this is **some** problem with the DAG that it cannot be deepcopied. You can also disable ""schedule after task execution"" if you cannot change the DAG.\r\n\r\nOk, thank you, if this is the best solution, I ll do it.', 'created_at': datetime.datetime(2024, 8, 22, 13, 8, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304631688, 'issue_id': 2480720517, 'author': 'potiuk', 'body': 'It was happening but was hidden and visible as error status in your tasks.', 'created_at': datetime.datetime(2024, 8, 22, 13, 8, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304640971, 'issue_id': 2480720517, 'author': 'Pad71', 'body': '> It was happening but was hidden and visible as error status in your tasks.\r\n\r\nOk, thank you again. I hope, this is not related to other my bug - [41673](https://github.com/apache/airflow/issues/41673)  , where in the 2.10.0 xcom values are not displayed correctly in the UI', 'created_at': datetime.datetime(2024, 8, 22, 13, 12, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304648828, 'issue_id': 2480720517, 'author': 'Pad71', 'body': '> Generallly speaking - this is **some** problem with the DAG that it cannot be deepcopied. You can also disable ""schedule after task execution"" if you cannot change the DAG.\r\n\r\n@potiuk  Sorry for maybe stupid question, but is there any way how to find out why dag is not deep copyable?', 'created_at': datetime.datetime(2024, 8, 22, 13, 15, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304667186, 'issue_id': 2480720517, 'author': 'potiuk', 'body': ""No - it's not stupid - but apparently the DAG uses or imports some unpicklable class or import parameter. It's hard to say what it is though."", 'created_at': datetime.datetime(2024, 8, 22, 13, 23, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304719499, 'issue_id': 2480720517, 'author': 'Pad71', 'body': ""@potiuk  Thank you again, I'll try to find it."", 'created_at': datetime.datetime(2024, 8, 22, 13, 47, 2, tzinfo=datetime.timezone.utc)}]","Pad71 (Issue Creator) on (2024-08-22 13:02:50 UTC): Same error is displayed in the log of python operators

potiuk on (2024-08-22 13:04:18 UTC): What would be your proposal for better error message in this case? - We do not know why your task is not serializable, so we cannot really help here.

potiuk on (2024-08-22 13:07:10 UTC): Generallly speaking - this is **some** problem with the DAG that it cannot be deepcopied. You can also disable ""schedule after task execution"" if you cannot change the DAG.

Pad71 (Issue Creator) on (2024-08-22 13:07:39 UTC): well, I m just airflow user and for me this is an error that was not present in the previous version.

Pad71 (Issue Creator) on (2024-08-22 13:08:36 UTC): Ok, thank you, if this is the best solution, I ll do it.

potiuk on (2024-08-22 13:08:38 UTC): It was happening but was hidden and visible as error status in your tasks.

Pad71 (Issue Creator) on (2024-08-22 13:12:35 UTC): Ok, thank you again. I hope, this is not related to other my bug - [41673](https://github.com/apache/airflow/issues/41673)  , where in the 2.10.0 xcom values are not displayed correctly in the UI

Pad71 (Issue Creator) on (2024-08-22 13:15:59 UTC): @potiuk  Sorry for maybe stupid question, but is there any way how to find out why dag is not deep copyable?

potiuk on (2024-08-22 13:23:55 UTC): No - it's not stupid - but apparently the DAG uses or imports some unpicklable class or import parameter. It's hard to say what it is though.

Pad71 (Issue Creator) on (2024-08-22 13:47:02 UTC): @potiuk  Thank you again, I'll try to find it.

"
2480625542,issue,closed,completed,Bad display of xcom values,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When displaying primitive xcom values, an error message is displayed.

version 2.9.2.
![image](https://github.com/user-attachments/assets/2fbda2ba-6711-48c2-a064-26ce444361c9)

similar task in version 2.10.0
![image](https://github.com/user-attachments/assets/aee09e82-4dd2-43a8-8851-4d585941d210)



### What you think should happen instead?

value should be displayed correctly even if it is primitive and not json.

### How to reproduce

View the xcom in a task whose xcom values are not json

### Operating System

Kubernetes on Unix platform

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Pad71,2024-08-22 12:08:42+00:00,[],2024-08-22 13:28:56+00:00,2024-08-22 13:28:48+00:00,https://github.com/apache/airflow/issues/41673,"[('kind:bug', 'This is a clearly a bug'), ('duplicate', 'Issue that is duplicated'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2304607477, 'issue_id': 2480625542, 'author': 'Pad71', 'body': 'Sometimes is xcom displayed that way (see pict below) . In version 2.9.2 it worked well.\r\n![image](https://github.com/user-attachments/assets/060ffabe-9663-4cce-bce4-ba763bf2200e)', 'created_at': datetime.datetime(2024, 8, 22, 12, 57, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304678523, 'issue_id': 2480625542, 'author': 'potiuk', 'body': 'Duplicate of #41516  - scheduled to be fixed in 2.10.1', 'created_at': datetime.datetime(2024, 8, 22, 13, 28, 48, tzinfo=datetime.timezone.utc)}]","Pad71 (Issue Creator) on (2024-08-22 12:57:25 UTC): Sometimes is xcom displayed that way (see pict below) . In version 2.9.2 it worked well.
![image](https://github.com/user-attachments/assets/060ffabe-9663-4cce-bce4-ba763bf2200e)

potiuk on (2024-08-22 13:28:48 UTC): Duplicate of #41516  - scheduled to be fixed in 2.10.1

"
2480346829,issue,open,,Logs issue,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

2.9.3, 2.9.2

### What happened?

When i try to see the logs of the tasks i only see this
`airflow-stag-worker-0.airflow-stag-worker.de.svc.cluster.local
`

### What you think should happen instead?

I think we should see the complete logs of the execution of task

### How to reproduce

Just load one DAG and execute it

### Operating System

kubernetes 

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",dpetiz,2024-08-22 09:54:46+00:00,[],2025-01-10 09:32:29+00:00,,https://github.com/apache/airflow/issues/41664,"[('kind:bug', 'This is a clearly a bug'), ('area:logging', ''), ('pending-response', ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2304254888, 'issue_id': 2480346829, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 22, 9, 54, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2309626503, 'issue_id': 2480346829, 'author': 'LuisSCorreia', 'body': 'Also having this issue.', 'created_at': datetime.datetime(2024, 8, 26, 8, 19, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2582180773, 'issue_id': 2480346829, 'author': 'shahar1', 'body': 'We need more details to reproduce the issue so it could be resolved by the community (preferrably with screenshots)', 'created_at': datetime.datetime(2025, 1, 10, 9, 32, 27, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-22 09:54:49 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

LuisSCorreia on (2024-08-26 08:19:08 UTC): Also having this issue.

shahar1 on (2025-01-10 09:32:27 UTC): We need more details to reproduce the issue so it could be resolved by the community (preferrably with screenshots)

"
2480243844,issue,closed,completed,Success callback not being executed in Custom kubernetesPodOperator,"### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I've veen trying to extend the Kubernetes Pod Operator in order to assign it a successfull callback when the pod executes with success.  The main idea here is to create a custom Operator that will execute the assigned callback everytime this Custom Operator is used. 
However, when I run it inside the DAG, it appears from the logs that the callback is not executed. 

The following presents the code of that `CustomKubernetesPodOperator` that I'm trying to build.
```python
from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator
from airflow.utils.decorators import apply_defaults

class ExtendedKubernetesPodOperator(KubernetesPodOperator):
    @apply_defaults
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def execute(self, context):
        # Execute the original KubernetesPodOperator logic
        result = super().execute(context)

        # Call the success callback if the pod was successful
        #if self.success_callback and self.pod.status.phase == 'Succeeded':
        default_success_callback(context)

        return result

# Define a default success callback function
def default_success_callback(context):
    print(f""Pod succeeded!"")

class CustomKubernetesPodOperator(ExtendedKubernetesPodOperator):
    @apply_defaults
    def __init__(self, *args, **kwargs):
        if 'on_success_callback' not in kwargs:
            kwargs['on_success_callback'] = default_success_callback
        super().__init__(*args, **kwargs)
```

And the DAG where this custom operator is used.
```python
from datetime import datetime
from airflow import DAG
#from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator
from custom_kubernetes_operator import CustomKubernetesPodOperator
import os

# Define the default_args
default_args = {
    'owner': 'HelloWorldOwner',
    'start_date': datetime(2024, 8, 22),
    'retries': 1,
}

# Define the DAG
dag = DAG(
    'hello_world_kubernetes_pod',
    default_args=default_args,
    description='Hello World DAG',
    schedule_interval=None,
    catchup=False,
)

# Define the KubernetesPodOperator
hello_world = CustomKubernetesPodOperator(
    dag=dag,
    image='ubuntu:latest',
    cmds=[""sh"", ""-c""],
    arguments=[
        """"""
        echo Hello World
        """"""
    ],
    name='hello-world-pod',
    task_id=""hello_world"",
    get_logs=True
)

# Set the task in the DAG
hello_world
```

### What you think should happen instead?

I tried to create for example a custom bash operator, similar to the Custom Operator defined above, but in this case the callback is executed with success and the message is printed in the logs. Here is the code of that custom bash operator that works. Is there something I am missing in the definition of the custom Kubernetes Pod Operator or the behaviour should be equal?

The custom bash operator:
```python
from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator
from airflow.utils.decorators import apply_defaults
from airflow.operators.bash import BashOperator

class ExtendedBashOperator(BashOperator):
    @apply_defaults
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def execute(self, context):
        result = super().execute(context)

        # Call the success callback if the pod was successful
        default_success_callback(context)

        return result

# Define a default success callback function
def default_success_callback(context):
    print(f""Pod succeeded!"")

class CustomBashOperator(ExtendedBashOperator):
    @apply_defaults
    def __init__(self, *args, **kwargs):
        if 'on_success_callback' not in kwargs:
            kwargs['on_success_callback'] = default_success_callback
        super().__init__(*args, **kwargs)
```
The DAG where the bash operator is imported.

```python
from datetime import datetime
from airflow import DAG
from custom_bash_operator import CustomBashOperator
import os


# Define the default_args
default_args = {
    'owner': 'Hello World Owner',
    'start_date': datetime(2024, 8, 22),
    'retries': 1,
}


# Define the DAG
dag = DAG(
    'hello_world_bash',
    default_args=default_args,
    description='Hello World DAG',
    schedule_interval=None,
    catchup=False,
)

# Define the KubernetesPodOperator
hello_world_bash = CustomBashOperator(
    dag=dag,
    bash_command=""echo HELLO"",
    name='hello-world-bash',
    task_id=""hello_world_bash""
)

# Set the task in the DAG
hello_world_bash
```

### How to reproduce

1. Create a custom Operator that inherits from the KubernetesPodOperator (similar to above).
2. Create a DAG that will use this Custom Operator, with no success callback defined in the task definition.
3. Run the DAG.
4. Verify that the callback is not executed. 

### Operating System

Ubuntu 22.04.2 LTS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

Native installation with pip.

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",joselfrias,2024-08-22 09:09:17+00:00,[],2024-10-21 08:52:10+00:00,2024-10-21 08:52:10+00:00,https://github.com/apache/airflow/issues/41661,"[('kind:bug', 'This is a clearly a bug'), (""Can't Reproduce"", 'The problem cannot be reproduced'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('area:providers', ''), ('pending-response', ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2304163013, 'issue_id': 2480243844, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 22, 9, 9, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2392034304, 'issue_id': 2480243844, 'author': 'gopidesupavan', 'body': 'What was the error your getting? i cant reproduce this, in my case its getting as per your expectation: see the results below.\r\n\r\n<img width=""1346"" alt=""image"" src=""https://github.com/user-attachments/assets/5f09ad3c-b4a7-473b-8a29-3c41832d7577"">', 'created_at': datetime.datetime(2024, 10, 3, 18, 15, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2420906774, 'issue_id': 2480243844, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 10, 18, 0, 15, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2426028963, 'issue_id': 2480243844, 'author': 'joselfrias', 'body': '@gopidesupavan I wasn\'t getting any error, as the message ""Pod succeeded"" would simply not appear. However, when trying this today I couldn\'t reproduce it either, the message appeared as expected when the pod finalized, not sure what changed.  I\'m closing this issue, thank you!', 'created_at': datetime.datetime(2024, 10, 21, 8, 52, 10, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-22 09:09:21 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

gopidesupavan on (2024-10-03 18:15:21 UTC): What was the error your getting? i cant reproduce this, in my case its getting as per your expectation: see the results below.

<img width=""1346"" alt=""image"" src=""https://github.com/user-attachments/assets/5f09ad3c-b4a7-473b-8a29-3c41832d7577"">

github-actions[bot] on (2024-10-18 00:15:03 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

joselfrias (Issue Creator) on (2024-10-21 08:52:10 UTC): @gopidesupavan I wasn't getting any error, as the message ""Pod succeeded"" would simply not appear. However, when trying this today I couldn't reproduce it either, the message appeared as expected when the pod finalized, not sure what changed.  I'm closing this issue, thank you!

"
2480031391,issue,closed,completed,can't scroll task tries,"### Apache Airflow version

2.10.0


### What happened?

I can't scroll task tries when there is more than 10 retries

![image](https://github.com/user-attachments/assets/3f350cbc-e41e-43e0-8874-fa62e3d8c68e)


### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",raphaelauv,2024-08-22 07:20:43+00:00,[],2024-10-14 17:40:06+00:00,2024-10-14 17:40:05+00:00,https://github.com/apache/airflow/issues/41660,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2310635747, 'issue_id': 2480031391, 'author': 'tirkarthi', 'body': 'I am unable to reproduce this on v2-10-test and 2.10.0 . cc: @bbovenzi', 'created_at': datetime.datetime(2024, 8, 26, 16, 47, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2310662660, 'issue_id': 2480031391, 'author': 'bbovenzi', 'body': '@raphaelauv what browser are you using?', 'created_at': datetime.datetime(2024, 8, 26, 17, 2, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2310663527, 'issue_id': 2480031391, 'author': 'raphaelauv', 'body': 'google chrome Version 128.0.6613.84 (Official Build) (64-bit)', 'created_at': datetime.datetime(2024, 8, 26, 17, 3, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2310666380, 'issue_id': 2480031391, 'author': 'tirkarthi', 'body': 'Just to add I was trying it on Firefox 129.0.1 in Ubuntu.', 'created_at': datetime.datetime(2024, 8, 26, 17, 4, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2310674596, 'issue_id': 2480031391, 'author': 'raphaelauv', 'body': ""maybe it's related to https://github.com/apache/airflow/issues/41742\r\n\r\nsince for a dag_run I can see 13 runs and scroll and for another I can't scroll and can't see the first 3 tries\r\n![Screenshot from 2024-08-26 19-08-21](https://github.com/user-attachments/assets/50112107-40fd-40e9-9437-b3cb685f732a)\r\n![Screenshot from 2024-08-26 19-08-13](https://github.com/user-attachments/assets/9c3babce-a716-4e6b-87d7-64eee72b8827)"", 'created_at': datetime.datetime(2024, 8, 26, 17, 9, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2329442505, 'issue_id': 2480031391, 'author': 'bbovenzi', 'body': 'I wonder if we should flip the order too. The last try is more important than the first, no?', 'created_at': datetime.datetime(2024, 9, 4, 15, 55, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2411870372, 'issue_id': 2480031391, 'author': 'raphaelauv', 'body': 'bug no more present on `2.10.2`', 'created_at': datetime.datetime(2024, 10, 14, 17, 40, 5, tzinfo=datetime.timezone.utc)}]","tirkarthi on (2024-08-26 16:47:24 UTC): I am unable to reproduce this on v2-10-test and 2.10.0 . cc: @bbovenzi

bbovenzi on (2024-08-26 17:02:57 UTC): @raphaelauv what browser are you using?

raphaelauv (Issue Creator) on (2024-08-26 17:03:27 UTC): google chrome Version 128.0.6613.84 (Official Build) (64-bit)

tirkarthi on (2024-08-26 17:04:58 UTC): Just to add I was trying it on Firefox 129.0.1 in Ubuntu.

raphaelauv (Issue Creator) on (2024-08-26 17:09:29 UTC): maybe it's related to https://github.com/apache/airflow/issues/41742

since for a dag_run I can see 13 runs and scroll and for another I can't scroll and can't see the first 3 tries
![Screenshot from 2024-08-26 19-08-21](https://github.com/user-attachments/assets/50112107-40fd-40e9-9437-b3cb685f732a)
![Screenshot from 2024-08-26 19-08-13](https://github.com/user-attachments/assets/9c3babce-a716-4e6b-87d7-64eee72b8827)

bbovenzi on (2024-09-04 15:55:46 UTC): I wonder if we should flip the order too. The last try is more important than the first, no?

raphaelauv (Issue Creator) on (2024-10-14 17:40:05 UTC): bug no more present on `2.10.2`

"
2479097894,issue,closed,completed,Unable to specify env variables using fieldPath,"### Official Helm Chart version

1.14.0

### Apache Airflow version

2.9.2

### Kubernetes Version

v1.30.2

### Helm Chart configuration

Using Argo CD to deploy the helm chart

### Docker Image customizations

No customization

### What happened

I need to be able to add env variables like the one below to my airflow containers (all of them) but the helm chart does not support these ... or at least I don't know how to do it.

```yaml
env:
    - name: DOGSTATSD_HOST_IP
      valueFrom:
        fieldRef:
          fieldPath: status.hostIP
    - name: STATSD_HOST
      valueFrom:
        fieldRef:
          fieldPath: status.hostIP
```

### What you think should happen instead

env should support specifying env variables that reference fieldRef

### How to reproduce

Try adding the env vars above and helm template will fail with:

```
Failed to load target state: failed to generate manifest for source 1 of 1: rpc error: code = Unknown desc = `helm template . --name-template airflow-core-deployment-stage --namespace airflow--core-deployment--stage --kube-version 1.29 --values <path to cached source>/charts/airflow/instances/awscmhstage2/values.yaml <api versions removed> failed exit status 1: Error: values don't meet the specifications of the schema(s) in the following chart(s): airflow: - env.5: value is required - env.5: Additional property valueFrom is not allowed - env.6: value is required - env.6: Additional property valueFrom is not allowed
```


### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",azadabbasi,2024-08-21 20:55:17+00:00,[],2024-08-21 21:03:13+00:00,2024-08-21 21:02:58+00:00,https://github.com/apache/airflow/issues/41655,"[('kind:bug', 'This is a clearly a bug'), ('area:helm-chart', 'Airflow Helm Chart'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2302997406, 'issue_id': 2479097894, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 21, 20, 55, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2303008476, 'issue_id': 2479097894, 'author': 'azadabbasi', 'body': 'I just found the solution:\r\n\r\n```yaml\r\n  extraEnv: |\r\n    - name: DOGSTATSD_HOST_IP\r\n      valueFrom:\r\n        fieldRef:\r\n          fieldPath: status.hostIP\r\n    - name: STATSD_HOST\r\n      valueFrom:\r\n        fieldRef:\r\n          fieldPath: status.hostIP\r\n```', 'created_at': datetime.datetime(2024, 8, 21, 21, 2, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2303009076, 'issue_id': 2479097894, 'author': 'azadabbasi', 'body': 'Issue resolved. Had to use extraEnv', 'created_at': datetime.datetime(2024, 8, 21, 21, 3, 11, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-21 20:55:20 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

azadabbasi (Issue Creator) on (2024-08-21 21:02:52 UTC): I just found the solution:

```yaml
  extraEnv: |
    - name: DOGSTATSD_HOST_IP
      valueFrom:
        fieldRef:
          fieldPath: status.hostIP
    - name: STATSD_HOST
      valueFrom:
        fieldRef:
          fieldPath: status.hostIP
```

azadabbasi (Issue Creator) on (2024-08-21 21:03:11 UTC): Issue resolved. Had to use extraEnv

"
