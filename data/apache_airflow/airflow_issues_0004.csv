id,type,state,state_reason,title,body,author,created_at,assignees,updated_at,closed_at,url,labels,comments_list,comment_thread
2478848954,issue,closed,completed,S3ToGCSOperator broken in deferable mode,"### Apache Airflow Provider(s)

google

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon = ""==8.27.0""
apache-airflow-providers-google = ""==10.21.1""

### Apache Airflow version

2.9.1

### Operating System

GCP Cloud Composer

### Deployment

Google Cloud Composer

### Deployment details

Manually upgraded providers to see if it was still a bug in latest version.

### What happened

Received the following error when trying to use S3ToGCSOperator in deferrable mode.
```
Traceback (most recent call last):

  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/jobs/triggerer_job_runner.py"", line 529, in cleanup_finished_triggers
    result = details[""task""].result()
             ^^^^^^^^^^^^^^^^^^^^^^^^

  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/jobs/triggerer_job_runner.py"", line 602, in run_trigger
    async for event in trigger.run():

  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/providers/google/cloud/triggers/cloud_storage_transfer_service.py"", line 77, in run
    jobs_pager = await async_hook.get_jobs(job_names=self.job_names)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/providers/google/cloud/hooks/cloud_storage_transfer_service.py"", line 581, in get_jobs
    client = await self.get_conn()
             ^^^^^^^^^^^^^^^^^^^^^

  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/providers/google/cloud/hooks/cloud_storage_transfer_service.py"", line 567, in get_conn
    credentials = (await self.get_sync_hook()).get_credentials()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File ""/opt/python3.11/lib/python3.11/site-packages/airflow/providers/google/common/hooks/base_google.py"", line 776, in get_sync_hook
    self._sync_hook = await sync_to_async(self.sync_hook_class)(**self._hook_kwargs)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

TypeError: sync_to_async.<locals>.<lambda>() got an unexpected keyword argument 'gcp_conn_id'
```

### What you think should happen instead

IT should not error when checking the job status on wakeup.

### How to reproduce

Create a S3ToGCSOperator job with deferable enabled.

### Anything else

On every run of the operator, happens after the job is queued and the code trys to check the status I think.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Logical-sh,2024-08-21 19:11:48+00:00,[],2024-08-22 16:09:46+00:00,2024-08-22 16:09:46+00:00,https://github.com/apache/airflow/issues/41652,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('good first issue', '')]","[{'comment_id': 2302815085, 'issue_id': 2478848954, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 21, 19, 11, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2303857444, 'issue_id': 2478848954, 'author': 'eladkal', 'body': 'cc @moiseenkov I think you developed the deferrable mode for this operator?', 'created_at': datetime.datetime(2024, 8, 22, 6, 8, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2303932795, 'issue_id': 2478848954, 'author': 'moiseenkov', 'body': '> cc @moiseenkov I think you developed the deferrable mode for this operator?\r\n\r\nSure, I am on it already.', 'created_at': datetime.datetime(2024, 8, 22, 7, 3, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304067202, 'issue_id': 2478848954, 'author': 'moiseenkov', 'body': 'This bug is fixed by #41417 already. Upgrading the apache-airflow-providers-google==10.22.0 will help. I tested it locally (on the current main branch) and in Composer with apache-airflow-providers-google==10.22.0rc1.', 'created_at': datetime.datetime(2024, 8, 22, 8, 21, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304740709, 'issue_id': 2478848954, 'author': 'Logical-sh', 'body': 'Thanks! Ill give that a try, I did try upgrading to most recent release last week will give that RC a shot today.', 'created_at': datetime.datetime(2024, 8, 22, 13, 56, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304766946, 'issue_id': 2478848954, 'author': 'moiseenkov', 'body': 'The new version was released a couple of hours ago:\r\n`apache-airflow-providers-google==10.22.0`', 'created_at': datetime.datetime(2024, 8, 22, 14, 7, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2305142614, 'issue_id': 2478848954, 'author': 'eladkal', 'body': 'Cool then closing as solved', 'created_at': datetime.datetime(2024, 8, 22, 16, 9, 46, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-21 19:11:51 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2024-08-22 06:08:11 UTC): cc @moiseenkov I think you developed the deferrable mode for this operator?

moiseenkov on (2024-08-22 07:03:43 UTC): Sure, I am on it already.

moiseenkov on (2024-08-22 08:21:07 UTC): This bug is fixed by #41417 already. Upgrading the apache-airflow-providers-google==10.22.0 will help. I tested it locally (on the current main branch) and in Composer with apache-airflow-providers-google==10.22.0rc1.

Logical-sh (Issue Creator) on (2024-08-22 13:56:16 UTC): Thanks! Ill give that a try, I did try upgrading to most recent release last week will give that RC a shot today.

moiseenkov on (2024-08-22 14:07:29 UTC): The new version was released a couple of hours ago:
`apache-airflow-providers-google==10.22.0`

eladkal on (2024-08-22 16:09:46 UTC): Cool then closing as solved

"
2478610045,issue,closed,completed,Incorrect link to task detail from task instance list,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

This bug has been reported and subsequently fixed in issue [38030](https://github.com/apache/airflow/issues/38030).
The bug reappeared in Airflow 2.10. 

long story short - url is incomplete, missing reference to task id.

For more details, see the  [38030](https://github.com/apache/airflow/issues/38030).


### What you think should happen instead?

The correct Link should point to the task detail, not just filter it and stay on the dag run detail.

### How to reproduce

Click the ""filter"" link in the task instance list. 

### Operating System

Kubernetes on Unix platform

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Pad71,2024-08-21 17:37:33+00:00,[],2024-09-25 06:48:47+00:00,2024-09-25 06:48:47+00:00,https://github.com/apache/airflow/issues/41651,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2302683035, 'issue_id': 2478610045, 'author': 'Pad71', 'body': 'It only seems to misbehave sometimes. I tried to find a pattern and no luck so far.', 'created_at': datetime.datetime(2024, 8, 21, 18, 12, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2373176367, 'issue_id': 2478610045, 'author': 'Pad71', 'body': 'looks like it is okay in 2.10.2', 'created_at': datetime.datetime(2024, 9, 25, 6, 48, 47, tzinfo=datetime.timezone.utc)}]","Pad71 (Issue Creator) on (2024-08-21 18:12:30 UTC): It only seems to misbehave sometimes. I tried to find a pattern and no luck so far.

Pad71 (Issue Creator) on (2024-09-25 06:48:47 UTC): looks like it is okay in 2.10.2

"
2478316100,issue,closed,completed,"Confirmation text ""Wait a minute"" is confusing","### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

New user to Airflow. Going to clear a task, and I get the confirmation page saying ""Wait a minute"". So I wait a minute.. nothing happens. Eventually I realize this means ""Check what you're about to do makes sense"", not actually to wait. I feel that this text is confusing and implies something is loading and the user should wait, not verify the action and confirm.

### What you think should happen instead?

The text should read ""Confirm"" or ""Make sure"" or similar.

### How to reproduce

Clear a task

### Operating System

N/A

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",kev-the-dev,2024-08-21 15:14:32+00:00,[],2024-08-22 14:05:35+00:00,2024-08-22 14:05:35+00:00,https://github.com/apache/airflow/issues/41649,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2302335960, 'issue_id': 2478316100, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 21, 15, 14, 34, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-21 15:14:34 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2477919355,issue,open,,Evaluate DB migration restriction when upgrading to AF 3,"As part of https://github.com/apache/airflow/pull/41120 , we pruned the Database migration files. This change means that users will have to upgrade to Airflow 2.10 before they can upgrade to Airflow 3.0. If a user tried to migrate directly from 2.7 to 3.0 -- they might get an Alembic revision error.

We should re-evaluate this before we release Airflow 3.0 and clearly document the expectation so users don't run into troubles when upgrading to 3.0 from lower Airflow versions.",kaxil,2024-08-21 12:29:34+00:00,[],2024-08-21 13:23:13+00:00,,https://github.com/apache/airflow/issues/41643,"[('area:db-migrations', 'PRs with DB migration'), ('airflow3.0:breaking', 'Candidates for Airflow 3.0 that contain breaking changes')]","[{'comment_id': 2301941780, 'issue_id': 2477919355, 'author': 'kaxil', 'body': 'fyi @ephraimbuddy', 'created_at': datetime.datetime(2024, 8, 21, 12, 32, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2302050876, 'issue_id': 2477919355, 'author': 'ashb', 'body': ""> they might get an Alembic revision error.\r\n\r\nThey will get an alembic version error `ERROR [alembic.util.messaging] Can't locate revision identified by ...`"", 'created_at': datetime.datetime(2024, 8, 21, 13, 23, 12, tzinfo=datetime.timezone.utc)}]","kaxil (Issue Creator) on (2024-08-21 12:32:33 UTC): fyi @ephraimbuddy

ashb on (2024-08-21 13:23:12 UTC): They will get an alembic version error `ERROR [alembic.util.messaging] Can't locate revision identified by ...`

"
2477789694,issue,open,,Airflow 2 to 3 auto migration rules,"# Description

## Why
As we're introducing breaking changes to the main branch, it would be better to begin recording the things we could use migration tools to help our users migrate from Airflow 2 to 3. 

The breaking changes can be found at https://github.com/apache/airflow/pulls?q=is%3Apr+label%3Aairflow3.0%3Abreaking and through [newsfragments/.*.significant.rst](https://github.com/apache/airflow/tree/main/newsfragments)

## What
* List down all the change needed based on [newsfragments/(\d{5}).significant.rst](https://github.com/apache/airflow/tree/main/newsfragments)
* Organize them into the sub issues for tracking the implementation part

## Sub-issues
* [ ] #45632
* between `45017.significant.rst` and ...
    * [ ] list down rules
* between `44080.significant.rst` and `45017.significant.rst`
    * [x] list down rules
    * [ ] #45212
    * [x] #45213
* before `44080.significant.rst`
    * [x] list down rules
    * [ ] #44556
    * [x] #44555


<details>
<summary><h3>List of significant news fragments and rules between `44080` and `45017`</h3></summary>

* #44475
    *  `TriggerRule.NONE_FAILED_OR_SKIPPED`
* #44706
    * ❌ CLI changes
* #44820
    * context key `conf`
* #45017
    * `airflow.config_templates.default_celery.DEFAULT_CELERY_CONFIG` → `airflow.providers.celery.executors.default_celery.DEFAULT_CELERY_CONFIG`
* aip-72 (before 2024/12/26)
    * cofing `core.task_runner`
    * config `core.enable_xcom_pickling`
* 45009 / aip-79
    * argument `appbuilder` in BaseAuthManager and its subclasses

</details>


<details>
<summary><h3>List of significant news fragments and rules before `44080`</h3></summary>
The following rules has been reorganized and merged into #44556 and #44555

* [x] #24842
    * ❓ I guess we don't need to do something for it based on the reason why we changed it.
* [x] #40029
    * Remove config `allow_raw_html_descriptions`
* [x] #40931
    * ❌ Model related change
* [x] #41096
    * config `scheduler.processor_poll_interval` →  `scheduler.scheduler_idle_sleep_time`
* [x] #41348
    * module `airflow.datasets` → `airflow.sdk.definitions.asset`
        * class `DatasetAlias` → `AssetAlias`
        * class `DatasetAll` → `AssetAll`
        * class `DatasetAny` → `AssetAny`
        * function `expand_alias_to_datasets` → `expand_alias_to_assets`
        * class `DatasetAliasEvent` → `AssetAliasEvent`
            * attribute `dest_dataset_uri` → `BaseAsset`
        * class `BaseDataset` → `BaseAsset`
            * method `iter_datasets` → `iter_assets`
            * method `iter_dataset_aliases` → `iter_asset_aliases`
        * class `Dataset` → `Asset`
            * method `iter_datasets` → `iter_assets`
            * method `iter_dataset_aliases` → `iter_asset_aliases`
        * class `_DatasetBooleanCondition` → `_AssetBooleanCondition`
            * method `iter_datasets` → `iter_assets`
            * method `iter_dataset_aliases` → `iter_asset_aliases`
    * module `airflow.datasets.manager` → `airflow.assets.manager`
        * variable `dataset_manager` → `asset_manager`
        * function `resolve_dataset_manager` → `resolve_asset_manager`
        * class `DatasetManager` → `AssetManager`
            * method `register_dataset_change` → `register_asset_change`
            * method `create_datasets` → `create_assets`
            * method `register_dataset_change` → `notify_asset_created`
            * method `notify_dataset_changed` → `notify_asset_changed`
            * method `notify_dataset_alias_created` → `notify_asset_alias_created`
    * module `airflow.listeners.spec.dataset` → `airflow.listeners.spec.asset`
        * function `on_dataset_created` → `on_asset_created`
        * function `on_dataset_changed` → `on_asset_changed`
    * module `airflow.timetables.datasets` → `airflow.timetables.assets`
        * class `DatasetOrTimeSchedule` → `AssetOrTimeSchedule`
    * module `airflow.datasets.metadata` → `airflow.sdk.definitions.asset.metadata`
    * module `airflow.listeners.spec.dataset` → `airflow.listeners.spec.asset`
        * function `on_dataset_created` → `on_asset_created`
        * function `on_dataset_changed` → `on_asset_changed`
    * class `airflow.timetables.datasets.DatasetOrTimeSchedule` → `airflow.timetables.assets.AssetOrTimeSchedule`
    * decorator `airflow.api_connexion.security.requires_access_dataset` → `airflow.api_connexion.security.requires_access_dataset.requires_access_asset`
    * class `airflow.auth.managers.models.resource_details.DatasetDetails` → `airflow.auth.managers.models.resource_details.AssetDetails`
    * function `airflow.auth.managers.base_auth_manager.is_authorized_dataset` → `airflow.auth.managers.base_auth_manager.is_authorized_asset`
    * class `airflow.timetables.simple.DatasetTriggeredTimetable` → `airflow.timetables.simple.AssetTriggeredTimetable`
    * in class `airflow.providers_manager.ProvidersManager`
        * method `initialize_providers_dataset_uri_resources` → `initialize_providers_asset_uri_resources`
        * property `dataset_factories` → `asset_factories`
        * property `dataset_uri_handlers` → `asset_uri_handlers`
        * property `dataset_to_openlineage_converters` → `asset_to_openlineage_converters`
    * constant `airflow.security.permissions.RESOURCE_DATASET` → `airflow.security.permissions.RESOURCE_ASSET`
    * function `airflow.www.auth.has_access_dataset` → `airflow.www.auth.has_access_dataset.has_access_asset`
    * class `airflow.lineage.hook.DatasetLineageInfo`  → `airflow.lineage.hook.AssetLineageInfo`
        * attribute `dataset` → `asset`
    * In class `airflow.lineage.hook.HookLineageCollector`
        * method `create_dataset` → `create_asset`
        * method `add_input_dataset` → `add_input_asset`
        * method `add_output_dataset` → `add_output_asset`
        * method `collected_datasets` → `collected_assets`
    * context key `triggering_dataset_events` → `triggering_asset_events`
    * resource key `dataset-uris` → `asset-uris` (for providers amazon, common.io, mysql, fab, postgres, trino)
    * In amazon provider
        * package `airflow.providers.amazon.aws.datasets`  → `airflow.providers.amazon.aws.assets`
            * in module `s3`
                * method `create_dataset` → `create_asset`
                * method `convert_dataset_to_openlineage` → `convert_asset_to_openlineage`
        * attribute `airflow.providers.amazon.auth_manager.avp.entities.AvpEntities.DATASET` → `airflow.providers.amazon.auth_manager.avp.entities.AvpEntities.ASSET`
        * `airflow.providers.amazon.auth_manager.aws_auth_manager.AwsAuthManager.is_authorized_dataset` → `airflow.providers.amazon.auth_manager.aws_auth_manager.AwsAuthManager.is_authorized_asset`
    * In Common IO Provider
        * package `airflow.providers.common.io.datasets` → `airflow.providers.common.io.assets`
            * in module `file`
                * method `create_dataset` → `create_asset`
                * method `convert_dataset_to_openlineage` → `convert_asset_to_openlineage`
    * In fab provider
        * function `airflow.providers.fab.auth_manager.fab_auth_manager.is_authorized_dataset` → `airflow.providers.fab.auth_manager.fab_auth_manager.is_authorized_asset`
    * In openlineage provider
        * in module `airflow.providers.openlineage.utils.utils`
            * class `DatasetInfo` → `AssetInfo`
            * function `translate_airflow_dataset` → `translate_airflow_asset`
    * In postgres provider
        * package `airflow.providers.postgres.datasets` → `airflow.providers.postgres.assets`
    * In mysql provider
        * package `airflow.providers.mysql.datasets` → `airflow.providers.mysql.assets`
    * In trino provider
        * package `airflow.providers.trino.datasets` → `airflow.providers.trino.assets`
    * ❌ ignored
        * `airflow.api_connexion.schemas.dataset_schema`
        * `airflow.api_ui.views.datasets`
        * `airflow.serialization.pydantic.dataset`
        * `airflow.serialization.pydantic.taskinstance`
        * `airflow.serialization.enums.DagAttributeTypes`
        * `airflow.serialization.serialized_objects`
        * `airflow.utils.context`
        * models
        * DagDependency names
        * private methods
* [x] #41366
    * #44385
* [x] #41367
    * `airflow.models.ImportError` → `airflow.models.errors.ParseImportError`
* [x] #41368
    * Remove `airflow.executors.*`
    * Remove `airflow.hooks.*`
    * Remove `airflow.macros.*`
    * Remove `airflow.operators.*`
    * Remove `airflow.sensors.*`
* [x] #41390
    * Remove package `airflow.operators.subdag`
* [x] #41391
    * `airflow.sensors.external_task.ExternalTaskSensorLink` → `airflow.sensors.external_task.ExternalDagLin`
* [x] #41393
    * in `DayOfWeekSensor`
        * parameter `use_task_execution_day` → `use_task_logical_date`
* [x] #41394
    * `airflow.models.taskMixin.TaskMixin` → `airflow.models.taskMixin.DependencyMixin`
* [x] #41395
    * Remove `airflow.executors.executor_loader.UNPICKLEABLE_EXECUTORS`
    * Remove `airflow.utils.dag_cycle_tester.test_cycle`
    * Remove `airflow.utils.file.TemporaryDirectory`
    * Remove `airflow.utils.file.mkdirs`
    * Remove `airflow.utils.state.SHUTDOWN`
    * Remove `airflow.utils.state.terminating_states`
* [x] #41420
    * ❌ Internal change
* [x] #41434
    * ❌ REST APIchange
* [x] #41440
    * ❌ Model change
* [x] #41453
    * in `DAG`
        * Remove argument `schedule_interval`
        * Remove argument `timetable`
* [x] #41496
    * Remove `airflow.utils.dates.date_range`
    * Remove `airflow.utils.dates.days_ago` → ❓ do we need to change it to `pendulum.today('UTC').add(days=-N, ...)`
* [x] #41520
    * `airflow.utils.helpers.chain` → `airflow.models.baseoperator.chain`
    * `airflow.utils.helpers.chain` → `airflow.models.baseoperator.cross_downstream`
* [x] #41533
    * `airflow.secrets.local_filesystem.load_connections` → `airflow.secrets.local_filesystem.load_connections_dict`
    * `airflow.secrets.local_filesystem.get_connection` → `airflow.secrets.local_filesystem.load_connections_dict`
* [x] #41539
    * Remove config `smtp.smtp_user`
    * Remove config `smtp.smtp_password`
* [x] #41550
    * Remove config `webserver.session_lifetime_days` → use `webserver.session_lifetime_minutes`
    * Remove config `webserver.force_log_out_after` → use `webserver.session_lifetime_minutes`
    * config section `policy` → `task_policy`
* [x] #41552
    * In `airflow.utils.log.file_task_handler.FileTaskHandler`
        * Remove parameter `filename_template`
* [x] #41579
    * Remove function `airflow.utils.decorators.apply_defaults`
* [x] #41609
    * Remove config `scheduler.dependency_detector`
* [x] #41635
    * ❌ CLI changes
* [x] #41642
    * `airflow.secrets.base_secrets.BaseSecretsBackend.get_conn_uri` → `airflow.secrets.base_secrets.BaseSecretsBackend.get_conn_value`
    * `airflow.secrets.base_secrets.BaseSecretsBackend.get_connections` → `airflow.secrets.base_secrets.BaseSecretsBackend.get_connection`
* [x] #41663
    * `airflow.api.auth.backend.basic_auth` → `airflow.providers.fab.auth_manager.api.auth.backend.basic_auth`
* [x] #41693
    * `airflow.api.auth.backend.kerberos_auth` → `airflow.providers.fab.auth_manager.api.auth.backend.kerberos_auth`
    * `airflow.auth.managers.fab.api.auth.backend.kerberos_auth` → `airflow.providers.fab.auth_manager.api.auth.backend.kerberos_auth`
* [x] #41708
    * `airflow.auth.managers.fab.fab_auth_manager` → `airflow.providers.fab.auth_manager.security_manager.override`
    * `airflow.auth.managers.fab.security_manager.override` → `airflow.providers.fab.auth_manager.security_manager.override`
* [x] #41733
    * Remove `airflow.hooks.base.BaseHook.get_connections` (❓ related to 41642)
* [x] #41735
    * Remove `airflow.kubernetes`
* [x] #41736
    * in `airflow.operators.datetime.BranchDateTimeOperator`
        * parameter `use_task_execution_day` → `use_task_logical_date`
    * in `airflow.operators.trigger_dagrun.TriggerDagRunOperator`
        * remove parameter `execution_date`
    * in `airflow.operators.weekday.BranchDayOfWeekOperator`
        * parameter `use_task_execution_day` → `use_task_logical_date`
* [x] #41737
    * Remove `airflow.triggers.external_task.TaskStateTrigger`
* [x] #41739
    * ❌ CLI changes
* [x] #41748
    * `airflow.hooks.dbapi` → `airflow.providers.common.sql.hooks.sql`
* [x] #41758
    * Remove `airflow.www.auth.has_access` → use `airflow.www.auth.has_access_*`
    * module `airflow.www.security` → `airflow.providers.fab.auth_manager.security_manager.override.FabAirflowSecurityManagerOverride`
    * `airflow.www.utils.get_sensitive_variables_fields` → `airflow.utils.log.secrets_masker.get_sensitive_variables_fields`
    * `airflow.www.utils.should_hide_value_for_key` → `airflow.utils.log.secrets_masker.should_hide_value_for_key`
* [x] #41761
    * in `BaseOperator`
        * paramter `task_concurrency` → `max_active_tis_per_dag` https://github.com/astral-sh/ruff/pull/14616
        * remove trigger_rule `dummy`
        * remove trigger_rule `none_failed_or_skipped`
    * remove config `operators.ALLOW_ILLEGAL_ARGUMENTS`
    * `airflow.models.baseoperator.BaseOperatorLink` → `airflow.models.baseoperatorlink.BaseOperatorLink`
* [x] #41762
    * Remove `airflow.models.connection.parse_netloc_to_hostname`
    * Remove `airflow.models.connection.Connection.parse_from_uri`
    * Remove `airflow.models.connection.Connection.log_info`
    * Remove `airflow.models.connection.Connection.debug_info`
* [x] #41774
    * ❌ Model change
* [x] #41776
    * ❌ Model change
* [x] #41778
    * ❌ Model change
* [x] #41779
    * ❌ Model change
* [x] #41780
    * ❌ Model change
* [x] #41784
    * ❌ Model change
* [x] #41808
    * ❌ Model change
* [x] #41814
    * ❌ overwritten by 43915
* [x] #41857
    * ❌ Package dependency change
* [x] #41910
    * Remove `airflow.api_connexion.security.requires_access` → use `requires_access_*`
* [x] #41964
    * ❌ CLI changed
    * ❌ Model change
* [x] #41975
    * Remove config `metrics.metrics_use_pattern_match`
    * Remove `airflow.metrics.validators.AllowListValidator` → suggest using `airflow.metrics.validators.PatternAllowListValidator` (not direct mapping)
    * Remove `airflow.metrics.validators.BlockListValidator` → suggest using `airflow.metrics.validators.PatternBlockListValidator` (not direct mapping)
* [x] #42023
    * ❌ Model change
* [x] #42042
    * ~~ Remove property `airflow.auth.managers.models.base_user.is_active`~~ ❌ Users are not likely to use it
* [x] #42054
    * ❓ not sure what we can do
* [x] #42060
    * Remove config `celery.stalled_task_timeout`
    * config `kubernetes_executor.worker_pods_pending_timeout` → `scheduler.task_queued_timeout`
* [x] #42088
    * config `metrics.statsd_allow_list` → `metrics.metrics_allow_list`
    * config `metrics.statsd_block_list` → `metrics.metrics_block_list`
    * config `scheduler.statsd_on` → `metrics.statsd_on`
    * config `scheduler.statsd_host` → `metrics.statsd_host`
    * config `scheduler.statsd_port` → `metrics.statsd_port`
    * config `scheduler.statsd_prefix` → `metrics.statsd_prefix`
    * config `scheduler.statsd_allow_list` → `metrics.statsd_allow_list`
    * config `scheduler.stat_name_handler` → `metrics.stat_name_handler`
    * config `scheduler.statsd_datadog_enabled` → `metrics.statsd_datadog_enabled`
    * config `scheduler.statsd_datadog_tags` → `metrics.statsd_datadog_tags`
    * config `scheduler.statsd_datadog_metrics_tags` → `metrics.statsd_datadog_metrics_tags`
    * config `scheduler.statsd_custom_client_path` → `metrics.statsd_custom_client_path`
* [x] #42100
    * config `core.interleave_timestamp_parser` → `logging.interleave_timestamp_parser`
    * config `core.base_log_folder` → `logging.base_log_folder`
    * config `core.remote_logging` → `logging.remote_logging`
    * config `core.remote_log_conn_id` → `logging.remote_log_conn_id`
    * config `core.remote_base_log_folder` → `logging.remote_base_log_folder`
    * config `core.encrypt_s3_logs` → `logging.encrypt_s3_logs`
    * config `core.logging_level` → `logging.logging_level`
    * config `core.fab_logging_level` → `logging.fab_logging_level`
    * config `core.logging_config_class` → `logging.logging_config_class`
    * config `core.colored_console_log` → `logging.colored_console_log`
    * config `core.colored_log_format` → `logging.colored_log_format`
    * config `core.colored_formatter_class` → `logging.colored_formatter_class`
    * config `core.log_format` → `logging.log_format`
    * config `core.simple_log_format` → `logging.simple_log_format`
    * config `core.task_log_prefix_template` → `logging.task_log_prefix_template`
    * config `core.log_filename_template` → `logging.log_filename_template`
    * config `core.log_processor_filename_template` → `logging.log_processor_filename_template`
    * config `core.dag_processor_manager_log_location` → `logging.dag_processor_manager_log_location`
    * config `core.task_log_reader` → `logging.task_log_reader`
* [x] #42126
    * config `core.sql_alchemy_conn` → `database.sql_alchemy_conn`
    * config `core.sql_engine_encoding` → `database.sql_engine_encoding`
    * config `core.sql_engine_collation_for_ids` → `database.sql_engine_collation_for_ids`
    * config `core.sql_alchemy_pool_enabled` → `database.sql_alchemy_pool_enabled`
    * config `core.sql_alchemy_pool_size` → `database.sql_alchemy_pool_size`
    * config `core.sql_alchemy_max_overflow` → `database.sql_alchemy_max_overflow`
    * config `core.sql_alchemy_pool_recycle` → `database.sql_alchemy_pool_recycle`
    * config `core.sql_alchemy_pool_pre_ping` → `database.sql_alchemy_pool_pre_ping`
    * config `core.sql_alchemy_schema` → `database.sql_alchemy_schema`
    * config `core.sql_alchemy_connect_args` → `database.sql_alchemy_connect_args`
    * config `core.load_default_connections` → `database.load_default_connections`
    * config `core.max_db_retries` → `database.max_db_retries`
* [x] #42129
    * config `core.worker_precheck` → `celery.worker_precheck`
    * config `scheduler.max_threads` → `scheduler.parsing_processes`
    * config `celery.default_queue` → `operators.default_queue`
    * config `admin.hide_sensitive_variable_fields` → `core.hide_sensitive_var_conn_fields`
    * config `admin.sensitive_variable_fields` → `core.sensitive_var_conn_names`
    * config `core.non_pooled_task_slot_count` → `core.default_pool_task_slot_count`
    * config `core.dag_concurrency` → `core.max_active_tasks_per_dag`
    * config `api.access_control_allow_origin` → `api.access_control_allow_origins`
    * config `api.auth_backend` → `api.auth_backends`
    * config `scheduler.deactivate_stale_dags_interval` → `scheduler.parsing_cleanup_interval`
    * config `kubernetes_executor.worker_pods_pending_timeout_check_interval` → `scheduler.task_queued_timeout_check_interval`
    * config `webserver.update_fab_perms` → `fab.update_fab_perms`
    * config `webserver.auth_rate_limited` → `fab.auth_rate_limited`
    * config `webserver.auth_rate_limit` → `fab.auth_rate_limit`
    * config section `kubernetes` → `kubernetes_executor`
* [x] #42137
    * ❌ package dependency change
* [x] #42280
    * ❌ REST APIchange
* [x] #42285
    * Remove config `core.check_slas`
    * In `DAG`
        * Remove argument `sla_miss_callback`
    * In `BaseOperator`
        * Remove argument `sla`
* [x] #42343
    * ❌ Internal change
* [x] #42436
    * ❓ Should we raise a warning if that `dag_ignore_file_syntax` has changed
* #42548
    * ❌ Model change
* [x] #42579
    * ❌ REST APIchange
* [x] #42640
    * ❌ Test change
* [x] #42647
    * ❌ Build change
* [x] #42658
    * ❌ REST API change
* [x] #42660
    * ❌ REST API change
* [x] #42739
    * ❌ Model change
* [x] #42776
    * ❌ Model change
* [x] #42953
    * ❓ Should we warn that `DAG.max_active_runs` behavior has been changed
* [x] #43067
    * ❌ Model change
* [x] #43073
    * ❌ UI change
* [x] #43096
    * `airflow.api.auth.backend.default` → ` airflow.providers.fab.auth_manager.api.auth.backend.session`
* [x] #43102
    * ❌ REST API change
* [x] #43183
    * Remove config `logging.enable_task_context_logger`
* [x] #43289
    * ❌ Remove `airflow.executors.*`
* [x] #43291
    * Remove `airflow.hook.*`
* [x] #43368
    * `trigger_rule=TriggerRule.ALWAYS` is blocked in a dynamic mapped task
* [x] #43490
    * ❌ Model change
* [x] #43530
    * function `airflow.config.get` → `airflow.config.conf.get`
    * function `airflow.config.getboolean` → `airflow.config.conf.getboolean`
    * function `airflow.config.getfloat` → `airflow.config.conf.getfloat`
    * function `airflow.config.getint` → `airflow.config.conf.getint`
    * function `airflow.config.has_option` → `airflow.config.conf.has_option`
    * function `airflow.config.remove_option` → `airflow.config.conf.remove_option`
    * function `airflow.config.as_dict` → `airflow.config.conf.as_dict`
    * function `airflow.config.set` → `airflow.config.conf.set`
    * function `airflow.config.` → `airflow.config.conf.`
* [x] #43533
    * Remove function `airflow.utils.dates.parse_execution_date`
    * Remove function `airflow.utils.dates.round_time`
    * Remove function `airflow.utils.dates.scale_time_units`
    * Remove function `airflow.utils.dates.infer_time_unit`
* [x] #43562
    * `airflow.PY36` → `if sys.version_info >== (3, 6)`
    * `airflow.PY37` → `if sys.version_info >== (3, 7)`
* [x] #43568
    * ❌ don't think we need to do anything
* [x] #43611
    * ❌ don't think we can do anything
* [x] #43612
    * ❌ More like a new feature. Change behavior but probably don't need to do something for it
* [x] #43902
    * https://github.com/apache/airflow/issues/44409
* [x] #43915
    * Remove config `core.strict_dataset_uri_validation`
* [x] #43943
    * Remove config `traces.otel_task_log_event`
* [x] #43975
    * Remove config `metrics.timer_unit_consistency`
* [x] #44080
    * ❌ DB version change
* aip-72.significant.rst

</details>

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Lee-W,2024-08-21 11:26:52+00:00,['Lee-W'],2025-01-26 09:52:30+00:00,,https://github.com/apache/airflow/issues/41641,"[('kind:feature', 'Feature Requests'), ('area:upgrade', 'Facilitating migration to a newer version of Airflow'), ('airflow3.0:candidate', 'Potential candidates for Airflow 3.0')]","[{'comment_id': 2301825432, 'issue_id': 2477789694, 'author': 'Lee-W', 'body': 'The `Rules` now is an example of how these changes can be recorded. I will check the existing breaking changes and update the rules. It would be great if folks could help update this list if you know there are breaking changes.', 'created_at': datetime.datetime(2024, 8, 21, 11, 30, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2302106137, 'issue_id': 2477789694, 'author': 'potiuk', 'body': 'I pinned the issue - this way it will show up at the top of ""Issues"" list in the repo', 'created_at': datetime.datetime(2024, 8, 21, 13, 48, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2302107544, 'issue_id': 2477789694, 'author': 'potiuk', 'body': '![image](https://github.com/user-attachments/assets/fb1fb04d-f843-4c63-a536-2d7a06cb8388)', 'created_at': datetime.datetime(2024, 8, 21, 13, 49, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2302244678, 'issue_id': 2477789694, 'author': 'Lee-W', 'body': 'Thanks!', 'created_at': datetime.datetime(2024, 8, 21, 14, 50, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2308468476, 'issue_id': 2477789694, 'author': 'eladkal', 'body': ""We can just go over all the significant newsfragments and create a rule for them or keep some reasoning why it doesn't require one"", 'created_at': datetime.datetime(2024, 8, 24, 17, 43, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2435629930, 'issue_id': 2477789694, 'author': 'kaxil', 'body': ""We should add something for the [public API change](https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html) too. API v1 won't work anymore. Those are being changed as part of AIP-84 to a new FastApi based app. GitHub project for it: https://github.com/orgs/apache/projects/414"", 'created_at': datetime.datetime(2024, 10, 24, 15, 37, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2437638900, 'issue_id': 2477789694, 'author': 'pierrejeambrun', 'body': 'Issue here to regroup Rest API breaking changes https://github.com/apache/airflow/issues/43378', 'created_at': datetime.datetime(2024, 10, 25, 12, 21, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2439879558, 'issue_id': 2477789694, 'author': 'tirkarthi', 'body': 'I have started prototyping a small package based on LibCST to build a Python 2to3 like tool for Airflow 2to3 that does simple and straight forward replacements. My main motivation was around lot of our users in our Airflow instance using `schedule_interval` in Airflow 2 that was deprecated and renamed to `schedule` in Airflow 3. It would require updating thousands of dags manually and some automation could help. This could also help in places with import statements changes .E.g. Task SDK need to be updated from `from airflow import DAG` to `from airflow.sdk import DAG`. Something like this could eventually become part of Airflow cli so that users can run `airflow migrate /airflow/dags` for migration or serve as a starter point for migration. It can update the file in place or show diff. Currently it does the following changes : \r\n\r\nDags\r\n\r\n* schedule_interval -> schedule\r\n* timetable -> schedule\r\n* concurrency -> max_active_tasks\r\n* Removal of unused full_filepath parameter\r\n* default_view (tree -> grid)\r\n\r\nOperators\r\n\r\n* task_concurrency -> max_active_tis_per_dag\r\n* trigger_rule (none_failed_or_skipped -> none_failed_min_one_success)\r\n\r\nSample file\r\n\r\n```python\r\nimport datetime\r\n\r\nfrom airflow import DAG\r\nfrom airflow.decorators import dag, task\r\nfrom airflow.operators.empty import EmptyOperator\r\nfrom airflow.timetables.events import EventsTimetable\r\n\r\n\r\nwith DAG(\r\n    dag_id=""my_dag_name"",\r\n    default_view=""tree"",\r\n    start_date=datetime.datetime(2021, 1, 1),\r\n    schedule_interval=""@daily"",\r\n    concurrency=2,\r\n):\r\n    op = EmptyOperator(\r\n        task_id=""task"", task_concurrency=1, trigger_rule=""none_failed_or_skipped""\r\n    )\r\n\r\n\r\n@dag(\r\n    default_view=""graph"",\r\n    start_date=datetime.datetime(2021, 1, 1),\r\n    schedule_interval=EventsTimetable(event_dates=[datetime.datetime(2022, 4, 5)]),\r\n    max_active_tasks=2,\r\n    full_filepath=""/tmp/test_dag.py""\r\n)\r\ndef my_decorated_dag():\r\n    op = EmptyOperator(task_id=""task"")\r\n\r\n\r\nmy_decorated_dag()\r\n```\r\n\r\nSample usage\r\n\r\n```shell\r\npython -m libcst.tool codemod dag_fixer.DagFixerCommand -u 1 tests/test_dag.py\r\nCalculating full-repo metadata...\r\nExecuting codemod...\r\nreformatted -\r\n\r\nAll done! ✨ 🍰 ✨\r\n1 file reformatted.\r\n--- /home/karthikeyan/stuff/python/libcst-tut/tests/test_dag.py\r\n+++ /home/karthikeyan/stuff/python/libcst-tut/tests/test_dag.py\r\n@@ -10,6 +10,6 @@\r\n     dag_id=""my_dag_name"",\r\n-    default_view=""tree"",\r\n+    default_view=""grid"",\r\n     start_date=datetime.datetime(2021, 1, 1),\r\n-    schedule_interval=""@daily"",\r\n-    concurrency=2,\r\n+    schedule=""@daily"",\r\n+    max_active_tasks=2,\r\n ):\r\n@@ -23,5 +23,4 @@\r\n     start_date=datetime.datetime(2021, 1, 1),\r\n-    schedule_interval=EventsTimetable(event_dates=[datetime.datetime(2022, 4, 5)]),\r\n+    schedule=EventsTimetable(event_dates=[datetime.datetime(2022, 4, 5)]),\r\n     max_active_tasks=2,\r\n-    full_filepath=""/tmp/test_dag.py""\r\n )\r\nFinished codemodding 1 files!\r\n - Transformed 1 files successfully.\r\n - Skipped 0 files.\r\n - Failed to codemod 0 files.\r\n - 0 warnings were generated.\r\n```\r\n\r\nRepo : https://github.com/tirkarthi/Airflow-2to3', 'created_at': datetime.datetime(2024, 10, 27, 6, 58, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2440002389, 'issue_id': 2477789694, 'author': 'potiuk', 'body': 'NICE! @tirkarthi -> you should start a thread about it at devlist and propose adding it to the repo. The sooner we start working on it and let poeple test it, the better it will be. And we can already start adding not only the newsfragments but also rules to the migration tools (cc: @vikramkoka @kaxil ) - we can even think about keeping a database of old-way-dags and running such migration tool on them and letting airflow scheduler from Airflow 3 process them (and maybe even execute) as part of our CI. This would tremendously help with maintaining and updating such a tool if we will make it a part of our CI pipeline.', 'created_at': datetime.datetime(2024, 10, 27, 12, 47, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2440004562, 'issue_id': 2477789694, 'author': 'potiuk', 'body': 'BTW. I like it a lot how simple it is with libCST - we previously used quite a bit more complex tool from Facebook that allowed to do refactoring at scale in parallell (https://github.com/facebookincubator/Bowler) , but it was rather brittle to develop rules for it and it had some weird problems and missing features. One thing that was vere useful - is that it had a nice ""parallelism"" features - which allowed to refactor 1000s of files in seconds (but also made it difficult to debug). \r\n\r\nI think if we get it working with libCST - it will be way more generic and maintainable, also we can easily add parallelism later on when/if we see it is slow.', 'created_at': datetime.datetime(2024, 10, 27, 12, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2440007915, 'issue_id': 2477789694, 'author': 'potiuk', 'body': 'One small watchout though -  such a tool should have a way to isolate rules - so that they are not in a single big method - some abstraction that will allow us to easily develop and selectively apply (or skip) different rules - see https://github.com/apache/airflow/tree/v1-10-test/airflow/upgrade where we have documentation and information about the upgrade check we\'ve done in Airflow 1 -> 2 migration.\r\n\r\nAlso we have to discuss, whether it should be a separate repo or whether it should be in airflow\'s monorepo. Both have pros and cons - in 1.10 we chose to keep it 1.10 branch of airflow, because it imported some of the airflow code and it was easier, but we could likely create a new repo for it, add CI there and keep it there.\r\n\r\nWe even have this archived repo https://github.com/apache/airflow-upgrade-check which we never used and archived, we could re-open it. We also have https://pypi.org/project/apache-airflow-upgrade-check/ - package in PyPI - and we could release new upgrade check versions (2.* ?) with ""apache-airflow>=2.11.0"" as dependency.\r\n\r\nAll that should likely be discussed at devlist :)', 'created_at': datetime.datetime(2024, 10, 27, 13, 3, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2440023575, 'issue_id': 2477789694, 'author': 'tirkarthi', 'body': ""Thanks @potiuk for the details. I will start a discussion on this at the devlist and continue there. Bowler looks interesting. Using `libcst.tool` from cli parallelizes the process. Right now this needs `python -m libcst.tool` to execute it as a codemod. Initially I had designed them as standalone Transformer for each category like (dag, operator) where the updated AST from one transformer can be passed to another. The codemod looked like a recommended abstraction for running it and changed it that way to later find cli accepts only one codemod at a time. I need to check how composable they are.\r\n\r\n```\r\npython -m libcst.tool codemod --help | grep -i -A 1 'jobs JOBS'\r\n  -j JOBS, --jobs JOBS  Number of jobs to use when processing files. Defaults to number of cores\r\n\r\ntime python -m libcst.tool codemod dag_fixer.DagFixerCommand -u 1 ~/airflow/dags > /dev/null 2>&1 \r\npython -m libcst.tool codemod dag_fixer.DagFixerCommand -u 1 ~/airflow/dags >  \r\n6.95s user 0.61s system 410% cpu 1.843 total\r\n\r\n# Single core\r\ntime python -m libcst.tool codemod dag_fixer.DagFixerCommand -u 1 -j 1 ~/airflow/dags > /dev/null 2>&1\r\npython -m libcst.tool codemod dag_fixer.DagFixerCommand -u 1 -j 1  > \r\n/dev/nul  4.66s user 0.38s system 99% cpu 5.035 total\r\n\r\n# 4 core\r\npython -m libcst.tool codemod dag_fixer.DagFixerCommand -u 1 -j 4 ~/airflow/dags > /dev/null 2>&1\r\npython -m libcst.tool codemod dag_fixer.DagFixerCommand -u 1 -j 4  > \r\n/dev/nul  5.45s user 0.54s system 253% cpu 2.358 total\r\n```"", 'created_at': datetime.datetime(2024, 10, 27, 13, 25, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2440158878, 'issue_id': 2477789694, 'author': 'potiuk', 'body': '> Bowler looks interesting. \r\n\r\nDon\'t be deceived by it :). \r\n\r\n It was helpful for Provider\'s migration at some point in time, but I had many rough edges - like debugging a problem was a nightmare until we learned how to do it properly, also it had some annoying limitations - you had to learn a completely new non-standard abstractions (an SQLAlchemy-like DSL to perform modifications) - which did not cover all the refactorings we wanted to do. We had to really dig-deep into the code an find some workarounds for things we wanted to do, when authors of Bowler have not thoght about them. And sometimes those were nasty workarounds.\r\n\r\n```python\r\nquery = (\r\n    Query(<paths to modify>)\r\n    .select_function(""old_name"")\r\n    .rename(""new_name"")\r\n    .diff(interactive=True)\r\n)\r\n```\r\n\r\nExample that I remember above is that we could not rename some of the object types easily because it was not ""foreseen"" (can\'t remember exactly) - we had a few surprises there.\r\n\r\nAlso Bowler seems to be not maintained for > 3 years and it means that it\'s unlikely to handle some constructs even in 3.9+ Airflow.\r\n\r\nWhat I like about libcst is that it is really ""low-level"" interface that you have to program in Python rather than in abstract DSL -  similar to ""ast"".  You write actual python code to perform what you want to perform rather than rely on incomplete abstractions, even if you have to copy&paste rename code between different ""rules"" (for example) (which you can then abstract away as \'common` util if you need, so no big deal).', 'created_at': datetime.datetime(2024, 10, 27, 20, 9, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2440160853, 'issue_id': 2477789694, 'author': 'potiuk', 'body': 'BTW. Codemod .... is also 5 years not maintained. Not that it is disqualification - but they list `python2` as their dependency ... so .....', 'created_at': datetime.datetime(2024, 10, 27, 20, 15, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2440308245, 'issue_id': 2477789694, 'author': 'Lee-W', 'body': 'I tried to use libcst in airflow as a tiny POC of this issue here https://github.com/apache/airflow/blob/5b7977a149492168688e6f013a7dcd4fe3561a49/scripts/ci/pre_commit/check_deferrable_default.py#L34. It mostly works great except for its speed. I was also thinking about whether to add these migrations thing info ruff airflow linter but not yet explore much on the rust/ruff side.', 'created_at': datetime.datetime(2024, 10, 28, 1, 22, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2441075120, 'issue_id': 2477789694, 'author': 'potiuk', 'body': ':eyes:  :eyes:  `rust` project :) ... \r\n\r\nMe :heart:  it  (but I doubt we want to invest in it as it might be difficult to maintain, unless we find quite a few committers who are somewhat ruff profficient to at least be able to review the code) . But it\'s tempting I must admit.\r\n\r\nBut to be honest - while I\'d love to finally get a serious rust project, it\'s not worth it I think we are talking of one-time migration for even a 10.000 dags it will take at most single minutes and we can turn it maybe in under one minute with rust - so not a big gain for a lot of pain :) . Or at lest this is what my intuition tells me.\r\n\r\nI think parallelism will do the job nicely. My intuition tells me (but this is just intuition and understanding on some limits ans speed of certain operation) - that we will get from  multiple 10s of minutes (when running such migration sequentially) to single minutes when we allow to run migration in parallel using multiple processors and processes - even with Python and libcst. This task is really suitable for such parallelisation because each file is complete, independent task that can be run in complete isolation from all other tasks - so spawning multiple paralllel interpreters, ideally forking them right after all the imports and common code is loaded so that they use shared memory for those - this **should** do the job nicely (at least intuitively).\r\n\r\nUsing RUST for that might be classic premature optimisation - we might likely not need it :). But would be worth to make some calculations and get some ""numbers"" for big installation - i.e. how many dags of what size are out there, and how long it will be to parse them all with libcst and write back (even unmodified or with a simple modification).  I presume that parsing and writing back will be the bulk of the job - and modifications will add very little overhead as they will be mostly operating on in memory data structures.', 'created_at': datetime.datetime(2024, 10, 28, 9, 42, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2445614119, 'issue_id': 2477789694, 'author': 'Lee-W', 'body': ""> Me ❤️ it (but I doubt we want to invest in it as it might be difficult to maintain, unless we find quite a few committers who are somewhat ruff profficient to at least be able to review the code) . But it's tempting I must admit.\r\n> \r\n> But to be honest - while I'd love to finally get a serious rust project, it's not worth it I think we are talking of one-time migration for even a 10.000 dags it will take at most single minutes and we can turn it maybe in under one minute with rust - so not a big gain for a lot of pain :) . Or at lest this is what my intuition tells me.\r\n\r\nYep, totally agree. I just want to raise this idea which might be interesting. 👀\r\n\r\n> I presume that parsing and writing back will be the bulk of the job - and modifications will add very little overhead as they will be mostly operating on in memory data structures.\r\n\r\nYep, I think you're right. My previous default deferrable script took around 10 sec to process ~400 operators. Using ast for checking took around 1 sec"", 'created_at': datetime.datetime(2024, 10, 30, 1, 16, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469286375, 'issue_id': 2477789694, 'author': 'potiuk', 'body': 'Mostly as curiosity:  One option we might consider is https://github.com/alexpovel/srgn - I\'ve heard about it recently, it\'s a ""grep that understands code"" with capabilities of running different actions.  Written in rust, and allows to add extensions apparently where you can define your own ""scopes"" of search and modification.\r\n\r\nBut I am not too convinced - this is mostly a command line tool so we would have to have a sequence of ""script commands"" to run - seems that plugging in our own rules and AST parsing should also be more flexible, even if slower.', 'created_at': datetime.datetime(2024, 11, 11, 23, 32, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2475183081, 'issue_id': 2477789694, 'author': 'Lee-W', 'body': '> Mostly as curiosity: One option we might consider is https://github.com/alexpovel/srgn - I\'ve heard about it recently, it\'s a ""grep that understands code"" with capabilities of running different actions. Written in rust, and allows to add extensions apparently where you can define your own ""scopes"" of search and modification.\r\n> \r\n> But I am not too convinced - this is mostly a command line tool so we would have to have a sequence of ""script commands"" to run - seems that plugging in our own rules and AST parsing should also be more flexible, even if slower.\r\n\r\nYep, not that convinced either. but it is always good to have an alternative we could consider 🤔', 'created_at': datetime.datetime(2024, 11, 14, 1, 45, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2482303157, 'issue_id': 2477789694, 'author': 'uranusjr', 'body': 'My best idea right now is to split this into two tools. We don’t really want to invest too much time into building a very rich CLI tool to show users what _need_ to be changed—we’ll effectively be rebuilding the error reporting interface in ruff (or flake8). Those squiggle lines, colors, error codes, and code context things are not easy to build.\r\n\r\nIt is probably easiest to tack the linter part on Ruff—it is Rust, but the code to implement a lint rule isn’t that hard if you know Python AST and just a general idea about C-like languages. The rewrite part is a lot more difficult, so it’s probably better to implement this as a different tool in Python with libcst. I’m thinking something like\r\n\r\n```console\r\n$ ruff check --select AIR\r\nThis spits out lint errors with codes like AIR005 AIR123...\r\n\r\n$ airflow2to3 --select AIR005 -- path/to/dag/file.py\r\nThis fixes the given error(s) in given file(s) in-place with a minimal CLI...\r\n```\r\n\r\nI plan to start experiementing some rules in Ruff to see how easy the first part actually is. We should be able to save a lot of effort if it is viable.', 'created_at': datetime.datetime(2024, 11, 18, 8, 48, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2482705088, 'issue_id': 2477789694, 'author': 'Lee-W', 'body': ""I tried to change the format a bit and list the rules in the following format. \r\n\r\n```markdown\r\n* [ ] link to the pr with breaking change\r\n    * [ ] things to do\r\n```\r\n\r\nOnce the `things to do` have been listed, we can check the root pr. After implementing the rule, we can mark the `things to do` as done.\r\n\r\nI also updated the format for #41366, #41367, #41368, #41391, #41393\r\n\r\nIf anyone has anything to add but do not have permission to update the description. Please just tag me and I'll take a look"", 'created_at': datetime.datetime(2024, 11, 18, 11, 2, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2484421421, 'issue_id': 2477789694, 'author': 'potiuk', 'body': '> It is probably easiest to tack the linter part on Ruff—it is Rust, but the code to implement a lint rule isn’t that hard if you know Python AST and just a general idea about C-like languages. The rewrite part is a lot more difficult, so it’s probably better to implement this as a different tool in Python with libcst. I’m thinking something like\r\n\r\nActually I am convinced too - I quite like this one after a bit of thought. This is not something that might be maintained by a lot of people and a number of contributors, and even for them, this is so far from the main ""airflow code"" - it\'s really a ""one-time"" tool - that it might be worth treating it as our first ""rust experiment"". And I quite agree that, the AST code on it\'s own is not really that ""pythonic"" and if you know what you want, and have already existing examples, adding a new rule in RUST, should not be difficult even if you do not know it (and AI driven development here might be even pretty cool exercise). I\'d myself be happy to add a few rules at some point of time and maybe even take part in implementing the tooling for rust for our CI environment.', 'created_at': datetime.datetime(2024, 11, 19, 0, 1, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2484935133, 'issue_id': 2477789694, 'author': 'Lee-W', 'body': ""The things we'll need to migrate for 41348\r\n\r\n* [ ] https://github.com/apache/airflow/pull/41348\r\n    * [ ] module `airflow.datasets` -> `airflow.sdk.definitions.asset`\r\n        * [ ] class `DatasetAlias` -> `AssetAlias`\r\n        * [ ] class `DatasetAll` -> `AssetAll`\r\n        * [ ] class `DatasetAny` -> `AssetAny`\r\n        * [ ] function `expand_alias_to_datasets` -> `expand_alias_to_assets`\r\n        * [ ] class `DatasetAliasEvent` -> `AssetAliasEvent`\r\n            * [ ] attribute `dest_dataset_uri` -> `BaseAsset`\r\n        * [ ] class `BaseDataset` -> `BaseAsset`\r\n            * [ ] method `iter_datasets` -> `iter_assets`\r\n            * [ ] method `iter_dataset_aliases` -> `iter_asset_aliases`\r\n        * [ ] class `Dataset` -> `Asset`\r\n            * [ ] method `iter_datasets` -> `iter_assets`\r\n            * [ ] method `iter_dataset_aliases` -> `iter_asset_aliases`\r\n        * [ ] class `_DatasetBooleanCondition` -> `_AssetBooleanCondition`\r\n            * [ ] method `iter_datasets` -> `iter_assets`\r\n            * [ ] method `iter_dataset_aliases` -> `iter_asset_aliases`\r\n    * [ ] module `airflow.datasets.manager` → `airflow.assets.manager`\r\n        * [ ] variable `dataset_manager` → `asset_manager`\r\n        * [ ] function `resolve_dataset_manager` → `resolve_asset_manager`\r\n        * [ ] class `DatasetManager` → `AssetManager`\r\n            * [ ] method `register_dataset_change` → `register_asset_change`\r\n            * [ ] method `create_datasets` → `create_assets`\r\n            * [ ] method `register_dataset_change` → `notify_asset_created`\r\n            * [ ] method `notify_dataset_changed` → `notify_asset_changed`\r\n            * [ ] method `notify_dataset_alias_created` → `notify_asset_alias_created`\r\n    * [ ] module `airflow.listeners.spec.dataset` → `airflow.listeners.spec.asset`\r\n        * [ ] function `on_dataset_created` → `on_asset_created`\r\n        * [ ] function `on_dataset_changed` → `on_asset_changed`\r\n    * [ ] module `airflow.timetables.datasets` → `airflow.timetables.assets`\r\n        * [ ] class `DatasetOrTimeSchedule` → `AssetOrTimeSchedule`\r\n    * [ ] module `airflow.datasets.metadata` → `airflow.sdk.definitions.asset.metadata`\r\n    * [ ] module `airflow.listeners.spec.dataset` → `airflow.listeners.spec.asset`\r\n        * [ ] function `on_dataset_created` → `on_asset_created`\r\n        * [ ] function `on_dataset_changed` → `on_asset_changed`\r\n    * [ ] class `airflow.timetables.datasets.DatasetOrTimeSchedule` → `airflow.timetables.assets.AssetOrTimeSchedule`\r\n    * [ ] decorator `airflow.api_connexion.security.requires_access_dataset` → `airflow.api_connexion.security.requires_access_dataset.requires_access_asset`\r\n    * [ ] class `airflow.auth.managers.models.resource_details.DatasetDetails` → `airflow.auth.managers.models.resource_details.AssetDetails\r\n    * [ ] function `airflow.auth.managers.base_auth_manager.is_authorized_dataset` → `airflow.auth.managers.base_auth_manager.is_authorized_asset`\r\n    * [ ] class `airflow.timetables.simple.DatasetTriggeredTimetable` → `airflow.timetables.simple.AssetTriggeredTimetable`\r\n    * in class `airflow.providers_manager.ProvidersManager`\r\n        * [ ] method `initialize_providers_dataset_uri_resources` → `initialize_providers_asset_uri_resources`\r\n        * [ ] property `dataset_factories` → `asset_factories`\r\n        * [ ] property `dataset_uri_handlers` → `asset_uri_handlers`\r\n        * [ ] property `dataset_to_openlineage_converters` → `asset_to_openlineage_converters`\r\n    * [ ] constant `airflow.security.permissions.RESOURCE_DATASET` → `airflow.security.permissions.RESOURCE_ASSET`\r\n    * [ ] function `airflow.www.auth.has_access_dataset` → `airflow.www.auth.has_access_dataset.has_access_asset`\r\n    * [ ] class `airflow.lineage.hook.DatasetLineageInfo`  → `airflow.lineage.hook.AssetLineageInfo`\r\n        * [ ] attribute `dataset` → `asset`\r\n    * In class `airflow.lineage.hook.HookLineageCollector`\r\n        * [ ] method `create_dataset` → `create_asset`\r\n        * [ ] method `add_input_dataset` → `add_input_asset`\r\n        * [ ] method `add_output_dataset` → `add_output_asset`\r\n        * [ ] method `collected_datasets` → `collected_assets`\r\n    * [ ] context key `triggering_dataset_events` → `triggering_asset_events`\r\n    * In amazon provider\r\n        * package `airflow.providers.amazon.aws.datasets`  → `airflow.providers.amazon.aws.assets`\r\n            * in module `s3`\r\n                * [ ] method `create_dataset` → `create_asset`\r\n                * [ ] method `convert_dataset_to_openlineage` → `convert_asset_to_openlineage`\r\n        * [ ] attribute `airflow.providers.amazon.auth_manager.avp.entities.AvpEntities.DATASET` → `airflow.providers.amazon.auth_manager.avp.entities.AvpEntities.ASSET`\r\n        * [ ] method `airflow.providers.amazon.auth_manager.aws_auth_manager.AwsAuthManager.is_authorized_dataset` → `airflow.providers.amazon.auth_manager.aws_auth_manager.AwsAuthManager.is_authorized_asset`\r\n        * [ ] resource key `dataset-uris` → `asset-uris`\r\n    * In Common IO Provider\r\n        * [ ] package `airflow.providers.common.io.datasets` → `airflow.providers.common.io.assets`\r\n            * in module `file`\r\n                * [ ] method `create_dataset` → `create_asset`\r\n                * [ ] method `convert_dataset_to_openlineage` → `convert_asset_to_openlineage`\r\n        * [ ] resource key `dataset-uris` → `asset-uris`\r\n    * In fab provider\r\n        * [ ] function `airflow.providers.fab.auth_manager.fab_auth_manager.is_authorized_dataset` → `airflow.providers.fab.auth_manager.fab_auth_manager.is_authorized_asset`\r\n    * In openlineage provider\r\n        * in module `airflow.providers.openlineage.utils.utils`\r\n            * [ ] class `DatasetInfo` → `AssetInfo`\r\n            * [ ] function `translate_airflow_dataset` → `translate_airflow_asset`\r\n    * In postgres provider\r\n        * [ ] package `airflow.providers.postgres.datasets` → `airflow.providers.postgres.assets`\r\n        * [ ] resource key `dataset-uris` → `asset-uris`\r\n    * In mysql provider\r\n        * [ ] package `airflow.providers.mysql.datasets` → `airflow.providers.mysql.assets`\r\n        * [ ] resource key `dataset-uris` → `asset-uris`\r\n    * In trino provider\r\n        * [ ] package `airflow.providers.trino.datasets` → `airflow.providers.trino.assets`\r\n        * [ ] resource key `dataset-uris` → `asset-uris`\r\n    * ❌ ignored\r\n        * `airflow.api_connexion.schemas.dataset_schema`\r\n        * `airflow.api_ui.views.datasets`\r\n        * `airflow.serialization.pydantic.dataset`\r\n        * `airflow.serialization.pydantic.taskinstance`\r\n        * `airflow.serialization.enums.DagAttributeTypes`\r\n        * `airflow.serialization.serialized_objects`\r\n        * `airflow.utils.context`\r\n        * models\r\n        * DagDependency names\r\n        * private methods"", 'created_at': datetime.datetime(2024, 11, 19, 7, 46, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2485407863, 'issue_id': 2477789694, 'author': 'Lee-W', 'body': ""Hi all, I'm trying to read through the significant news fragment and compile a list of rules we should migrate. It would be nice if you could take a look and check if I missed anything.\r\n\r\n* @jscheffl\r\n    * [x] https://github.com/apache/airflow/pull/40029\r\n    * [x] https://github.com/apache/airflow/pull/41733\r\n    * [x] https://github.com/apache/airflow/pull/41735\r\n    * [x] https://github.com/apache/airflow/pull/41736\r\n    * [x] https://github.com/apache/airflow/pull/41737\r\n    * [x] https://github.com/apache/airflow/pull/41739\r\n    * [x] https://github.com/apache/airflow/pull/41761\r\n    * [x] https://github.com/apache/airflow/pull/41762\r\n    * [x] https://github.com/apache/airflow/pull/41774\r\n    * [x] https://github.com/apache/airflow/pull/41776\r\n    * [x] https://github.com/apache/airflow/pull/41778\r\n    * [x] https://github.com/apache/airflow/pull/41779\r\n    * [x] https://github.com/apache/airflow/pull/41780\r\n    * [x] https://github.com/apache/airflow/pull/41808\r\n* @dirrao \r\n    * [ ] https://github.com/apache/airflow/pull/40931\r\n    * [ ] https://github.com/apache/airflow/pull/41096\r\n    * [ ] https://github.com/apache/airflow/pull/41539\r\n    * [ ] https://github.com/apache/airflow/pull/41496\r\n    * [ ] https://github.com/apache/airflow/pull/41550\r\n    * [ ] https://github.com/apache/airflow/pull/41552\r\n    * [ ] https://github.com/apache/airflow/pull/41579\r\n    * [ ] https://github.com/apache/airflow/pull/41609\r\n    * [ ] https://github.com/apache/airflow/pull/41635\r\n    * [ ] https://github.com/apache/airflow/pull/41642\r\n    * [ ] https://github.com/apache/airflow/pull/41663\r\n    * [ ] https://github.com/apache/airflow/pull/41693\r\n    * [ ] https://github.com/apache/airflow/pull/41708\r\n    * [ ] https://github.com/apache/airflow/pull/41748\r\n    * [ ] https://github.com/apache/airflow/pull/41784\r\n    * [ ] https://github.com/apache/airflow/pull/41910\r\n    * [ ] https://github.com/apache/airflow/pull/42060\r\n    * [ ] https://github.com/apache/airflow/pull/42088\r\n    * [ ] https://github.com/apache/airflow/pull/42100\r\n    * [ ] https://github.com/apache/airflow/pull/42126\r\n    * [ ] https://github.com/apache/airflow/pull/42129\r\n* @kaxil \r\n    * [ ] https://github.com/apache/airflow/pull/41390\r\n    * [ ] https://github.com/apache/airflow/pull/41393\r\n* @dstandish \r\n    * [ ] https://github.com/apache/airflow/pull/41440\r\n* @pierrejeambrun \r\n    * [ ] https://github.com/apache/airflow/pull/41857\r\n* @jedcunningham \r\n    * [ ] https://github.com/apache/airflow/pull/41964\r\n* @uranusjr \r\n    * [ ] https://github.com/apache/airflow/pull/42054"", 'created_at': datetime.datetime(2024, 11, 19, 11, 7, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2485410645, 'issue_id': 2477789694, 'author': 'Lee-W', 'body': ""@kaxil @ashb would also like to confirm whether we're still allowing users to use models in airflow 3.0? If not, should we just skip all the changes related to models. Thanks"", 'created_at': datetime.datetime(2024, 11, 19, 11, 9, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2485473821, 'issue_id': 2477789694, 'author': 'uranusjr', 'body': 'I tried my hands on implementing a rule in Ruff. This one checks if a DAG uses the `schedule` argument explicitly, and errors if there’s no such argument (i.e. user is relying on the implicit default, which changes in 3.0), or a deprecated argument is used.\r\n\r\nDoes this look reasonable enough for people to build on? I’ll produce a more detailed writeup of what to do if we feel this is the way to go.\r\n\r\nhttps://github.com/uranusjr/ruff/pull/1/files', 'created_at': datetime.datetime(2024, 11, 19, 11, 37, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2485622598, 'issue_id': 2477789694, 'author': 'ashb', 'body': '> @kaxil @ashb would also like to confirm whether we\'re still allowing users to use models in airflow 3.0? If not, should we just skip all the changes related to models. Thanks\n\nWhich models? But no, the plan is to not have/""allow"" users to import anything from airflow.models at all. Exact details and new names are to be determined yet though', 'created_at': datetime.datetime(2024, 11, 19, 12, 46, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2485653681, 'issue_id': 2477789694, 'author': 'kaxil', 'body': '>@kaxil\r\n https://github.com/apache/airflow/pull/41390\r\n https://github.com/apache/airflow/pull/41393\r\n https://github.com/apache/airflow/pull/41390\r\n\r\nDuplicate entries for SubDAGs', 'created_at': datetime.datetime(2024, 11, 19, 13, 0, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2485668236, 'issue_id': 2477789694, 'author': 'Lee-W', 'body': '> > @kaxil\r\n> > #41390\r\n> > #41393\r\n> > #41390\r\n> \r\n> Duplicate entries for SubDAGs\r\n\r\noops, just fixed!', 'created_at': datetime.datetime(2024, 11, 19, 13, 7, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2485671170, 'issue_id': 2477789694, 'author': 'Lee-W', 'body': '> > @kaxil @ashb would also like to confirm whether we\'re still allowing users to use models in airflow 3.0? If not, should we just skip all the changes related to models. Thanks\r\n> \r\n> Which models? But no, the plan is to not have/""allow"" users to import anything from airflow.models at all. Exact details and new names are to be determined yet though\r\n\r\nPretty much every model 👀 Sounds good. Just want to confirm I\'m not misunderstanding anything. I\'ll just mark it as model change and not going to migrate for now till we have anything decided', 'created_at': datetime.datetime(2024, 11, 19, 13, 8, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2485675067, 'issue_id': 2477789694, 'author': 'kaxil', 'body': ""@Lee-W There are few rules that we should add for Airflow configs too since we changed /removed them:\r\n\r\n- https://github.com/apache/airflow/pull/43975\r\n- https://github.com/apache/airflow/pull/43905\r\n\r\nand some imports which won't work:\r\n\r\n- https://github.com/apache/airflow/pull/41395\r\n- https://github.com/apache/airflow/pull/41391\r\n- https://github.com/apache/airflow/pull/43289\r\n- https://github.com/apache/airflow/pull/43533\r\n- https://github.com/apache/airflow/pull/43562\r\n- https://github.com/apache/airflow/pull/43530"", 'created_at': datetime.datetime(2024, 11, 19, 13, 10, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2485685523, 'issue_id': 2477789694, 'author': 'Lee-W', 'body': ""> @Lee-W There are few rules that we should add for Airflow configs too since we changed /removed them:\r\n> \r\n>     * [Standardize timer metrics to milliseconds and remove config #43975](https://github.com/apache/airflow/pull/43975)\r\n> \r\n>     * [Remove XCom pickling #43905](https://github.com/apache/airflow/pull/43905)\r\n> \r\n> \r\n> and some imports which won't work:\r\n> \r\n>     * [Remove deprecations from `airflow.executors` & `airflow.utils` #41395](https://github.com/apache/airflow/pull/41395)\r\n> \r\n>     * [Remove deprecated `ExternalTaskSensorLink` #41391](https://github.com/apache/airflow/pull/41391)\r\n> \r\n>     * [Remove the ability to import executors from plugins #43289](https://github.com/apache/airflow/pull/43289)\r\n> \r\n>     * [Remove redundant functions in `airflow.utils.dates` #43533](https://github.com/apache/airflow/pull/43533)\r\n> \r\n>     * [Remove deprecated Python Version identifiers #43562](https://github.com/apache/airflow/pull/43562)\r\n> \r\n>     * [Remove deprecated functions from `airflow/configuration.py` #43530](https://github.com/apache/airflow/pull/43530)\r\n\r\nThanks for reminding me! I'm still halfway to completing reading all the PRs. Will continue work on updating the list"", 'created_at': datetime.datetime(2024, 11, 19, 13, 15, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2486436443, 'issue_id': 2477789694, 'author': 'vikramkoka', 'body': ""Awesome @Lee-W , I hadn't seen this issue, so great to see the progress here. \r\nFollowing up on the action item from the last dev call, I created this page on Confluence as a draft \r\nhttps://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+3+breaking+changes"", 'created_at': datetime.datetime(2024, 11, 19, 18, 21, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2487215287, 'issue_id': 2477789694, 'author': 'Lee-W', 'body': ""> Awesome @Lee-W , I hadn't seen this issue, so great to see the progress here. Following up on the action item from the last dev call, I created this page on Confluence as a draft https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+3+breaking+changes\r\n\r\nLooks great! I'll try to update both places"", 'created_at': datetime.datetime(2024, 11, 20, 2, 33, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2487607130, 'issue_id': 2477789694, 'author': 'uranusjr', 'body': 'I also played around the fixer implementation a bit: https://github.com/uranusjr/airflow2to3\r\n\r\nStill a lot of room for improvement, but I think it is a good starting point.\r\n\r\nFWIW I also tried to create a Flake8 plugin for comparison, but got stuck resolving imports (we need to check whether a thing is actually from Airflow first to decide whether to emit errors for it). Flake8 does not seem to provide this out of the box (Ruff and LibCST each has an easy solution). I’m pretty sure there must be a solution, but the effort looking into this is probably not worthwhile unless we feel Ruff is too big a hurdle.', 'created_at': datetime.datetime(2024, 11, 20, 6, 27, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2487619236, 'issue_id': 2477789694, 'author': 'uranusjr', 'body': 'So what do people think about using Ruff for linting? This is somewhat important since we want to encourage community help to implement rules. If people don’t have strong opinions, I’ll start a lazy consensus thread in the mailing list to get a resolution.', 'created_at': datetime.datetime(2024, 11, 20, 6, 36, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2490445543, 'issue_id': 2477789694, 'author': 'Lee-W', 'body': ""Finally finishing checking all the significant before #44080 (except for the one @sunank200 's not working on). \r\n\r\nI tried my best to list down the migration as possible, but it would be nice if folks can take another look to see if I miss anything. Thanks!\r\n\r\n* @jscheffl\r\n    * [x] #40029\r\n    * [x] #41733\r\n    * [x] #41735\r\n    * [x] #41736\r\n    * [x] #41737\r\n    * [x] #41739\r\n    * [x] #41761\r\n    * [x] #41762\r\n    * [x] #41774\r\n    * [x] #41776\r\n    * [x] #41778\r\n    * [x] #41779\r\n    * [x] #41780\r\n    * [x] #41808\r\n    * [ ] #43612\r\n    * [x] #41758\r\n    * [ ] #43611\r\n* @dirrao \r\n    * [ ] #40931\r\n    * [ ] #41096\r\n    * [ ] #41539\r\n    * [ ] #41496\r\n    * [ ] #41550\r\n    * [ ] #41552\r\n    * [ ] #41579\r\n    * [ ] #41609\r\n    * [ ] #41635\r\n    * [ ] #41642\r\n    * [ ] #41663\r\n    * [ ] #41693\r\n    * [ ] #41708\r\n    * [ ] #41748\r\n    * [ ] #41784\r\n    * [ ] #41910\r\n    * [ ] #42060\r\n    * [ ] #42088\r\n    * [ ] #42100\r\n    * [ ] #42126\r\n    * [ ] #42129\r\n    * [ ] #43096\r\n    * [ ] #42647\r\n    * [ ] #42660\r\n    * [ ] #41496\r\n    * [ ] #41975\r\n    * [ ] #42776\r\n* @uranusjr \r\n    * [ ] #42054\r\n    * [ ] #24842 / #41453\r\n    * [ ] #43915\r\n    * [ ] #41453\r\n    * [ ] #42343\r\n    * [ ] #42436\r\n* @gopidesupavan\r\n    * [x] #41533\r\n* @jedcunningham\r\n    * [ ] #44080\r\n* @ashb\r\n    * [ ] #43943\r\n* @vincbeck\r\n    * [x] #43096\r\n    * [x] #41434\r\n    * [ ] #42042\r\n    * [x] #42280\r\n* @bugraoz93\r\n    * [ ] #43102\r\n* @dstandish\r\n    * [ ] #43183\r\n    * [ ] #42548\r\n    * [ ] #43067\r\n    * [ ] #42953\r\n* @Avihais12344\r\n    * [ ] #41420 / #41695\r\n* @shahar1\r\n    * [ ] #43368\r\n* @potiuk\r\n    * [ ] #42137\r\n    * [ ] #43568\r\n* @kaxil \r\n    * [ ] #41390\r\n    * [ ] #41393\r\n    * [ ] #43975\r\n    * [ ] #43530\r\n    * [ ] #43533\r\n    * [ ] #43562\r\n    * [ ] #43490\r\n    * [ ] #43291\r\n    * [ ] #43289\r\n* @dstandish \r\n    * [ ] #41440\r\n* @pierrejeambrun \r\n    * [ ] #41857\r\n* @jedcunningham \r\n    * [ ] #41964"", 'created_at': datetime.datetime(2024, 11, 21, 9, 5, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2492773416, 'issue_id': 2477789694, 'author': 'bugraoz93', 'body': 'Thanks for preparing this Lee-W!\r\n\r\nFor https://github.com/apache/airflow/pull/43102, we can only detect the changes with `Ruff` if users send requests to `PATCH` API endpoints from the Airflow code. \r\n@pierrejeambrun What do you think?', 'created_at': datetime.datetime(2024, 11, 22, 2, 49, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2492780661, 'issue_id': 2477789694, 'author': 'Lee-W', 'body': '> Thanks for preparing this Lee-W!\r\n> \r\n> For #43102, we can only detect the changes with `Ruff` if users send requests to `PATCH` API endpoints from the Airflow code. @pierrejeambrun What do you think?\r\n\r\nBut one of the questions is how we will detect it 🤔 there are plenty of ways to send a request to an API.', 'created_at': datetime.datetime(2024, 11, 22, 2, 58, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2492792075, 'issue_id': 2477789694, 'author': 'bugraoz93', 'body': 'There is no straightforward way to do it. Indeed, it can be even a standalone application communicating with the Airflow API without other interactions. It could be even in different programming languages such as `go` or scheduled/management `bash` scripts in CI.', 'created_at': datetime.datetime(2024, 11, 22, 3, 12, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2493910603, 'issue_id': 2477789694, 'author': 'vincbeck', 'body': 'I am not sure #42042. I moved out one property from `BaseUser` but I dont users are using this class directly.', 'created_at': datetime.datetime(2024, 11, 22, 14, 38, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2495030753, 'issue_id': 2477789694, 'author': 'potiuk', 'body': '> There is no straightforward way to do it. Indeed, it can be even a standalone application communicating with the Airflow API without other interactions. It could be even in different programming languages such as go or scheduled/management bash scripts in CI.\r\n\r\nI think we can only do it reasonably well if we assume the user uses Python Cliant of ours and then we should be able to say the users they could run their custom code through Ruff with the new python client installed to detect wrong parameters. Not sure if we need to have custom ruff rules for those changes, or maybe it\'s ""mypy"" kind of check for types ? I know astral works on a `mypy` replacement as well, so there is a chance that we will get mypy checks from Astral befor we publish the tool (or we could use mypy for now if needed.  Some quick check on new/old client with some test code for that might be useful.\r\n\r\nFor the rest of the clients, I think what we could also do - potentially - is to have a custom error code in fast API - where we wil handle errors generated when ""known old-style requests"" are issued to the new API and return pretty descriptive error "" You are using old-style API ble, ble, ble ... you need to change your paremeters to .... ."" - maybe we can generalize it in our API in the way to handle ""typical"" mistakes from multiple APIs by the same error handler?', 'created_at': datetime.datetime(2024, 11, 22, 22, 52, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2495541900, 'issue_id': 2477789694, 'author': 'bugraoz93', 'body': '> I think we can only do it reasonably well if we assume the user uses Python Cliant of ours and then we should be able to say the users they could run their custom code through Ruff with the new python client installed to detect wrong parameters. Not sure if we need to have custom ruff rules for those changes, or maybe it\'s ""mypy"" kind of check for types ? I know astral works on a `mypy` replacement as well, so there is a chance that we will get mypy checks from Astral befor we publish the tool (or we could use mypy for now if needed. Some quick check on new/old client with some test code for that might be useful.\r\n\r\nI agree, we should make this assumption and limit the check to a reasonable scope. I like the idea of restricting it so that only our Python Client will be affected. Otherwise, it could turn into a project of its own. :)\r\n\r\n> For the rest of the clients, I think what we could also do - potentially - is to have a custom error code in fast API - where we wil handle errors generated when ""known old-style requests"" are issued to the new API and return pretty descriptive error "" You are using old-style API ble, ble, ble ... you need to change your paremeters to .... ."" - maybe we can generalize it in our API in the way to handle ""typical"" mistakes from multiple APIs by the same error handler?\r\n\r\nI was considering a similar approach, returning an error response if the old request is provided but I wasn’t entirely sure about the scope. If the goal is to catch these issues before upgrading the version, I am unsure how we can easily provide that. Simply reading the API changes documentation seems easier than creating a transition API and asking users to route calls through it to catch errors. Otherwise, such responses would indicate that their processes have already failed with an error. \r\nIf the goal is also to warn users after upgrading the version, then this is the way to go for me too.\r\nI am just trying to better understand the scope of when and how we want to warn users.', 'created_at': datetime.datetime(2024, 11, 23, 17, 4, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2495751496, 'issue_id': 2477789694, 'author': 'potiuk', 'body': '> I was considering a similar approach, returning an error response if the old request is provided but I wasn’t entirely sure\r\nabout the scope.\r\n\r\nYeah. Maybe we can do something in Airlfow 2.11 ? Since our goal is that 2.11 should be the ""bridge"" release - maybe we could do a variation of my original proposal - see if the message coming is in ""old"" format and raise a deprecation and also manually implement ""new"" format there (that would likely require some manual modificaiton of the openapi specification there and some conditional code in 2.11 (if possible). \r\n\r\nThat could follow our pattern of ""Make sure 2.11 raises no warnings and then migration to 3 should be smooth"".', 'created_at': datetime.datetime(2024, 11, 24, 2, 3, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2498327075, 'issue_id': 2477789694, 'author': 'pierrejeambrun', 'body': '> For the rest of the clients, I think what we could also do - potentially - is to have a custom error code in fast API - where we wil handle errors generated when ""known old-style requests"" are issued to the new API and return pretty descriptive error "" You are using old-style API ble, ble, ble ... you need to change your paremeters to .... ."" - maybe we can generalize it in our API in the way to handle ""typical"" mistakes from multiple APIs by the same error handler?\r\n\r\nIndeed upgrading the client will automatically highlights type errors in users code.\r\n\r\nFor people bypassing the python client and making direct request to the API (or any other non-python system), we can indeed catch errors of such breaking change and return a clear message that this is not accepted anymore, and maybe even better give them the new way of how to achieve this. It\'s just more work but possible.\r\n\r\nOtherwise reading the significant newsfragment for the `RestAPI` would be a good start when migrating their API code.', 'created_at': datetime.datetime(2024, 11, 25, 15, 26, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499420202, 'issue_id': 2477789694, 'author': 'potiuk', 'body': '> For people bypassing the python client and making direct request to the API (or any other non-python system), we can indeed catch errors of such breaking change and return a clear message that this is not accepted anymore, and maybe even better give them the new way of how to achieve this. It\'s just more work but possible.\r\n\r\nJust to repeat above - yes. I think it\'s more work, and I think it might require some nasty workarounds in FAST API that we will have to keep forever, but **maybe** we can do a 2.11-only change that will raise warnings if the old way is used instead (and allow to use new way) ? Not sure if possible and how many such breaking channges we will have, but it would really be nice to tell the users ""if you have no warnings on 2.11, you are good to go"".', 'created_at': datetime.datetime(2024, 11, 26, 1, 54, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499598349, 'issue_id': 2477789694, 'author': 'Lee-W', 'body': ""> I am not sure #42042. I moved out one property from `BaseUser` but I dont users are using this class directly.\r\n\r\nAfter reading it again, I don't think users are using it either. 🤔 Then I'll just remove it. Thanks for checking it!"", 'created_at': datetime.datetime(2024, 11, 26, 3, 52, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507263054, 'issue_id': 2477789694, 'author': 'Lee-W', 'body': '> > For people bypassing the python client and making direct request to the API (or any other non-python system), we can indeed catch errors of such breaking change and return a clear message that this is not accepted anymore, and maybe even better give them the new way of how to achieve this. It\'s just more work but possible.\r\n> \r\n> Just to repeat above - yes. I think it\'s more work, and I think it might require some nasty workarounds in FAST API that we will have to keep forever, but **maybe** we can do a 2.11-only change that will raise warnings if the old way is used instead (and allow to use new way) ? Not sure if possible and how many such breaking channges we will have, but it would really be nice to tell the users ""if you have no warnings on 2.11, you are good to go"".\r\n\r\nI just tried to follow the API discussion 🙌 so we\'re now doing \r\n\r\n1. warning if they\'re using airflow python client\r\n2. return an error and guide in FastAPI\r\n\r\nbut wouldn\'t it be easier for us to do only the second one and solve it all at once?\r\n\r\nShould we trace those API changes only in https://github.com/apache/airflow/issues/43378?', 'created_at': datetime.datetime(2024, 11, 29, 7, 53, 20, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507518021, 'issue_id': 2477789694, 'author': 'Lee-W', 'body': 'Hi @kaxil ,\r\n\r\nwould like to confirm with you on the following rules. Does it make sense for us to block import from `airflow.executors.*` and `airflow.hook.*`? (related PRs https://github.com/apache/airflow/pull/43289,  https://github.com/apache/airflow/pull/43291', 'created_at': datetime.datetime(2024, 11, 29, 10, 26, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2507714324, 'issue_id': 2477789694, 'author': 'kaxil', 'body': 'No, no. We need to block a user from passing `executor`, `operator`, `sensors` and `hooks` when they inherit `AirflowPlugin`.\r\n\r\n```py\r\nclass AirflowTestPlugin(AirflowPlugin):\r\n    name = ""test_plugin""\r\n    # --- Invalid now\r\n    operators = [PluginOperator]\r\n    sensors = [PluginSensorOperator]\r\n    hooks = [PluginHook]\r\n    executors = [PluginExecutor]\r\n    # --- Invalid now ^^^\r\n    macros = [plugin_macro]\r\n    flask_blueprints = [bp]\r\n    appbuilder_views = [v_appbuilder_package]\r\n    appbuilder_menu_items = [appbuilder_mitem, appbuilder_mitem_toplevel]\r\n    global_operator_extra_links = [\r\n        AirflowLink(),\r\n        GithubLink(),\r\n    ]\r\n    operator_extra_links = [GoogleLink(), AirflowLink2(), CustomOpLink(), CustomBaseIndexOpLink(1)]\r\n    timetables = [CustomCronDataIntervalTimetable]\r\n    listeners = [empty_listener, ClassBasedListener()]\r\n    ti_deps = [CustomTestTriggerRule()]\r\n    priority_weight_strategies = [CustomPriorityWeightStrategy]\r\n```\r\n\r\nRef: \r\n- https://airflow.apache.org/docs/apache-airflow/1.10.15/plugins.html\r\n- https://airflow.apache.org/docs/apache-airflow/2.10.3/authoring-and-scheduling/plugins.html', 'created_at': datetime.datetime(2024, 11, 29, 12, 23, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2508140120, 'issue_id': 2477789694, 'author': 'Lee-W', 'body': ""@uranusjr and I previously discussed how to handle configuration migration. We believe it might be better to manage this process within Airflow itself, rather than using ruff. However, for other code-related changes, we will continue to leverage ruff.\r\n\r\nToday, @sunank200 and I spent a significant amount of time listing all the rules we could think of for the important files before #44040. We also noticed that the standard provider was not included in the news fragment. As a result, I created this https://github.com/apache/airflow/issues/44482.\r\n\r\nTLDR, we're spliting the list into \r\n\r\n* airflow config\r\n    * removal\r\n    * rename\r\n* ruff\r\n    * AIR301: Avoid implicit DAG schedule https://github.com/astral-sh/ruff/pull/14581\r\n    * AIR302: removal or no direct mapping\r\n    * AIR303: rename\r\n    * AIR304: moved to provider\r\n    * *AIR305: models related changes (will need to wait for AIP-72 to see what we're going to do for this)*\r\n\r\nnote that except for AIR301 (and probably AIR302?), other numbers are not confirmed. just something we're considering and will need to dicuss with the ruff team. \r\n\r\n---\r\n\r\nin this full list, #41348 is treated as a special category as it's easier for me to trace.\r\n\r\n<details>\r\n<summary>the full list</summary>\r\n## airflow config\r\n\r\n### Removal\r\n* `smtp`\r\n    * `smtp_user` (from #41539)\r\n    * `smtp_password` (from #41539)\r\n* `webserver`\r\n    * `allow_raw_html_descriptions` (from #40029)\r\n    * `session_lifetime_days` (from #41550) → use `session_lifetime_minutes`\r\n    * `force_log_out_after` (from #41550) → use `session_lifetime_minutes`\r\n* `scheduler`\r\n    * `dependency_detector` (from #41609)\r\n* `operators`\r\n    * `ALLOW_ILLEGAL_ARGUMENTS` (from #41761)\r\n* `metrics`\r\n    * `metrics_use_pattern_match` (from #41975)\r\n    * `timer_unit_consistency` (from #43975)\r\n* `celery`\r\n    * `stalled_task_timeout` (from #42060)\r\n* `core`\r\n    * `check_slas` (from #42285)\r\n    * `strict_dataset_uri_validation` (from #43915)\r\n* `logging`\r\n    * `enable_task_context_logger` (from #43183)\r\n* `traces`\r\n    * `otel_task_log_event` (from #43943)\r\n\r\n### Rename\r\n* `scheduler`\r\n    * `processor_poll_interval` →  `cheduler_idle_sleep_time` (from #41096)\r\n* `metrics`\r\n    * `statsd_allow_list` → `metrics_allow_list` (from #42088)\r\n    * `statsd_block_list` → `metrics_block_list` (from #42088)\r\n* cross section\r\n    * `scheduler.statsd_on` → `metrics.statsd_on` (from #42088)\r\n    * `scheduler.statsd_host` → `metrics.statsd_host` (from #42088)\r\n    * `scheduler.statsd_port` → `metrics.statsd_port` (from #42088)\r\n    * `scheduler.statsd_prefix` → `metrics.statsd_prefix` (from #42088)\r\n    * `scheduler.statsd_allow_list` → `metrics.statsd_allow_list` (from #42088)\r\n    * `scheduler.stat_name_handler` → `metrics.stat_name_handler` (from #42088)\r\n    * `scheduler.statsd_datadog_enabled` → `metrics.statsd_datadog_enabled` (from #42088)\r\n    * `scheduler.statsd_datadog_tags` → `metrics.statsd_datadog_tags` (from #42088)\r\n    * `scheduler.statsd_datadog_metrics_tags` → `metrics.statsd_datadog_metrics_tags` (from #42088)\r\n    * `scheduler.statsd_custom_client_path` → \r\n    * `core.sql_alchemy_conn` → `database.sql_alchemy_conn` (from #42126)\r\n    * `core.sql_engine_encoding` → `database.sql_engine_encoding` (from #42126)\r\n    * `core.sql_engine_collation_for_ids` → `database.sql_engine_collation_for_ids` (from #42126)\r\n    * `core.sql_alchemy_pool_enabled` → `database.sql_alchemy_pool_enabled` (from #42126)\r\n    * `core.sql_alchemy_pool_size` → `database.sql_alchemy_pool_size` (from #42126)\r\n    * `core.sql_alchemy_max_overflow` → `database.sql_alchemy_max_overflow` (from #42126)\r\n    * `core.sql_alchemy_pool_recycle` → `database.sql_alchemy_pool_recycle` (from #42126)\r\n    * `core.sql_alchemy_pool_pre_ping` → `database.sql_alchemy_pool_pre_ping` (from #42126)\r\n    * `core.sql_alchemy_schema` → `database.sql_alchemy_schema` (from #42126)\r\n    * `core.sql_alchemy_connect_args` → `database.sql_alchemy_connect_args` (from #42126)\r\n    * `core.load_default_connections` → `database.load_default_connections` (from #42126)\r\n    *  `core.max_db_retries` → `database.max_db_retries` (from #42126)\r\n    * `core.worker_precheck` → `celery.worker_precheck` (from #42129)\r\n    * `scheduler.max_threads` → `scheduler.parsing_processes` (from #42129)\r\n    * `celery.default_queue` → `operators.default_queue` (from #42129)\r\n    * `admin.hide_sensitive_variable_fields` → `core.hide_sensitive_var_conn_fields` (from #42129)\r\n    * `admin.sensitive_variable_fields` → `core.sensitive_var_conn_names` (from #42129)\r\n    * `core.non_pooled_task_slot_count` → `core.default_pool_task_slot_count` (from #42129)\r\n    * `core.dag_concurrency` → `core.max_active_tasks_per_dag` (from #42129)\r\n    * `api.access_control_allow_origin` → `api.access_control_allow_origins` (from #42129)\r\n    * `api.auth_backend` → `api.auth_backends` (from #42129)\r\n    * `scheduler.deactivate_stale_dags_interval` → `scheduler.parsing_cleanup_interval` (from #42129)\r\n    * `kubernetes_executor.worker_pods_pending_timeout_check_interval` → `scheduler.task_queued_timeout_check_interval` (from #42129)\r\n    * `webserver.update_fab_perms` → `fab.update_fab_perms` (from #42129)\r\n    * `webserver.auth_rate_limited` → `fab.auth_rate_limited` (from #42129)\r\n    * `webserver.auth_rate_limit` → `fab.auth_rate_limit` (from #42129)\r\n* `policy` → `task_policy` (from #41550) \r\n* `kubernetes` → `kubernetes_executor` (from #42129)\r\n\r\n## Ruff\r\n\r\n### AIR302: removal\r\n\r\n#### package\r\n* `airflow.contrib.*` (from #41366)\r\n\r\n#### module\r\n* `airflow.operators.subdag` (from #41390)\r\n* `airflow.kubernetes.*` (from #41735) → use ` airflow.providers.cncf.kubernetes`\r\n\r\n#### class\r\n* `airflow.triggers.external_task.TaskStateTrigger` (from #41737)\r\n* `airflow.metrics.validators.AllowListValidator` (from #41975) → use `airflow.metrics.validators.PatternAllowListValidator`\r\n* `airflow.metrics.validators.BlockListValidator` (from #41975) → use `airflow.metrics.validators.PatternBlockListValidator`\r\n* `airflow.metrics.validators.BlockListValidator` → suggest using `airflow.metrics.validators.PatternBlockListValidator` (not direct mapping)\r\n\r\n#### function\r\n* `airflow.utils.file.TemporaryDirectory` (from #41395)\r\n* `airflow.utils.file.mkdirs` (from #41395)\r\n* `airflow.utils.dates.date_range` (from #41496)\r\n* `airflow.utils.dates.days_ago` (from #41496) → change it to `pendulum.today('UTC').add(days=-N, ...)`\r\n* `airflow.utils.decorators.apply_defaults` (from #41579)\r\n* `airflow.hooks.base.BaseHook.get_connections` (from #41733) → use `get_connection`\r\n* `airflow.www.auth.has_access` (from #41758) → use `airflow.www.auth.has_access_*`\r\n* `airflow.api_connexion.security.requires_access` → use `requires_access_*` (from #41910)\r\n* `airflow.utils.dates.parse_execution_date` (from #43533)\r\n* `airflow.utils.dates.round_time` (from #43533)\r\n* `airflow.utils.dates.scale_time_units` (from #43533)\r\n* `airflow.utils.dates.infer_time_unit` (from #43533)\r\n\r\n#### constant / variable\r\n* `airflow.utils.dag_cycle_tester.test_cycle` (from #41395)\r\n* `airflow.utils.state.SHUTDOWN` (from #41395)\r\n* `airflow.utils.state.terminating_states` (from #41395)\r\n* `airflow.PY\\d\\d` (from #43562)\r\n\r\n#### attribute\r\n* in `airflow.utils.trigger_rule.TriggerRule`\r\n    * `DUMMY` (from #41761)\r\n    * `NONE_FAILED_OR_SKIPPED` (from #41761)\r\n* inherit from `airflow.plugins_manager.AirflowPlugin`\r\n    * `executors` (from #43289)\r\n    * `hooks` (from #43291)\r\n    * `operators`\r\n    * `sensors`\r\n\r\n#### parameter\r\n* in `DAG`\r\n    * `schedule_interval` (from #41453)\r\n    * `timetable` (from #41453)\r\n    * `sla_miss_callback` (from #42285)\r\n* in `BaseOperator`\r\n    * `sla` (from #42285)\r\n* in `airflow.utils.log.file_task_handler.FileTaskHandler`\r\n    * `filename_template` (#41552)\r\n* in `airflow.operators.trigger_dagrun.TriggerDagRunOperator`\r\n    * `execution_date` (from #41736)\r\n\r\n#### context key\r\n* `execution_date` (from #43902 https://github.com/apache/airflow/issues/44409)\r\n* `next_ds` (from #43902 https://github.com/apache/airflow/issues/44409)\r\n* `next_ds_nodash` (from #43902 https://github.com/apache/airflow/issues/44409)\r\n* `next_execution_date` (from #43902 https://github.com/apache/airflow/issues/44409)\r\n* `prev_ds` (from #43902 https://github.com/apache/airflow/issues/44409)\r\n* `prev_ds_nodash` (from #43902 https://github.com/apache/airflow/issues/44409)\r\n* `prev_execution_date` (from #43902 https://github.com/apache/airflow/issues/44409)\r\n* `prev_execution_date_success` (from #43902 https://github.com/apache/airflow/issues/44409)\r\n* `tomorrow_ds` (from #43902 https://github.com/apache/airflow/issues/44409)\r\n* `yesterday_ds` (from #43902 https://github.com/apache/airflow/issues/44409)\r\n* `yesterday_ds_nodash` (from #43902 https://github.com/apache/airflow/issues/44409)\r\n\r\n\r\n### AIR303: rename\r\n#### #41348\r\n* module `airflow.datasets` → `airflow.sdk.definitions.asset`\r\n    * class\r\n        * `DatasetAlias` → `AssetAlias`\r\n        * `DatasetAll` → `AssetAll`\r\n        * `DatasetAny` → `AssetAny`\r\n    * function\r\n        * `expand_alias_to_datasets` → `expand_alias_to_assets`\r\n    * class `DatasetAliasEvent` → `AssetAliasEvent`\r\n        * attribute `dest_dataset_uri` → `BaseAsset`\r\n    * class\r\n        * `BaseDataset` → `BaseAsset`\r\n        * `Dataset` → `Asset`\r\n        * method\r\n            * `iter_datasets` → `iter_assets`\r\n            * `iter_dataset_aliases` → `iter_asset_aliases`\r\n* module `airflow.datasets.manager` → `airflow.assets.manager`\r\n    * variable `dataset_manager` → `asset_manager`\r\n    * function `resolve_dataset_manager` → `resolve_asset_manager`\r\n    * class `DatasetManager` → `AssetManager`\r\n        * method\r\n            * `register_dataset_change` → `register_asset_change`\r\n            * `create_datasets` → `create_assets`\r\n            * `register_dataset_change` → `notify_asset_created`\r\n            * `notify_dataset_changed` → `notify_asset_changed`\r\n            * `notify_dataset_alias_created` → `notify_asset_alias_created`\r\n* module `airflow.listeners.spec.dataset` → `airflow.listeners.spec.asset`\r\n    * function\r\n        * `on_dataset_created` → `on_asset_created`\r\n        * `on_dataset_changed` → `on_asset_changed`\r\n* module `airflow.timetables.datasets` → `airflow.timetables.assets`\r\n    * class `DatasetOrTimeSchedule` → `AssetOrTimeSchedule`\r\n* class `airflow.lineage.hook.DatasetLineageInfo`  → `airflow.lineage.hook.AssetLineageInfo`\r\n    * attribute `dataset` → `asset`\r\n* package `airflow.providers.amazon.aws.datasets`  → `airflow.providers.amazon.aws.assets`\r\n    * in module `s3`\r\n        * method `create_dataset` → `create_asset`\r\n        * method `convert_dataset_to_openlineage` → `convert_asset_to_openlineage`\r\n* package `airflow.providers.common.io.datasets` → `airflow.providers.common.io.assets`\r\n    * in module `file`\r\n        * method `create_dataset` → `create_asset`\r\n        * method `convert_dataset_to_openlineage` → `convert_asset_to_openlineage`\r\n* package\r\n    * `airflow.providers.postgres.datasets` → `airflow.providers.postgres.assets`\r\n    * `airflow.providers.mysql.datasets` → `airflow.providers.mysql.assets`\r\n    * `airflow.providers.trino.datasets` → `airflow.providers.trino.assets`\r\n* module\r\n    * `airflow.datasets.metadata` → `airflow.sdk.definitions.asset.metadata`\r\n* class\r\n    * `airflow.timetables.datasets.DatasetOrTimeSchedule` → `airflow.timetables.assets.AssetOrTimeSchedule`\r\n    * `airflow.auth.managers.models.resource_details.DatasetDetails` → `airflow.auth.managers.models.resource_details.AssetDetails`\r\n    * `airflow.timetables.simple.DatasetTriggeredTimetable` → `airflow.timetables.simple.AssetTriggeredTimetable`\r\n    * `airflow.providers.openlineage.utils.utils.DatasetInfo` → `airflow.providers.openlineage.utils.utils.AssetInfo`\r\n* method\r\n    * `airflow.providers.amazon.auth_manager.aws_auth_manager.AwsAuthManager.is_authorized_dataset` → `airflow.providers.amazon.auth_manager.aws_auth_manager.AwsAuthManager.is_authorized_asset`\r\n    * `airflow.lineage.hook.HookLineageCollector.create_dataset` → `airflow.lineage.hook.HookLineageCollector.create_asset`\r\n    * `airflow.lineage.hook.HookLineageCollector.add_input_dataset` → `airflow.lineage.hook.HookLineageCollector.add_input_asset`\r\n    * `airflow.lineage.hook.HookLineageCollector.add_output_dataset` → `airflow.lineage.hook.HookLineageCollector.dd_output_asset`\r\n    * `airflow.lineage.hook.HookLineageCollector.collected_datasets` → `airflow.lineage.hook.HookLineageCollector.collected_assets`\r\n    * `airflow.providers_manager.ProvidersManager.initialize_providers_dataset_uri_resources` → `airflow.providers_manager.ProvidersManager.initialize_providers_asset_uri_resources`\r\n* function\r\n    * `airflow.api_connexion.security.requires_access_dataset` → `airflow.api_connexion.security.requires_access_dataset.requires_access_asset`\r\n    * `airflow.auth.managers.base_auth_manager.is_authorized_dataset` → `airflow.auth.managers.base_auth_manager.is_authorized_asset`\r\n    * `airflow.www.auth.has_access_dataset` → `airflow.www.auth.has_access_dataset.has_access_asset`\r\n    * `airflow.providers.fab.auth_manager.fab_auth_manager.is_authorized_dataset` → `airflow.providers.fab.auth_manager.fab_auth_manager.is_authorized_asset`\r\n    * `airflow.providers.openlineage.utils.utils.translate_airflow_dataset` → `airflow.providers.openlineage.utils.utils.translate_airflow_asset`\r\n* property\r\n    * `airflow.providers_manager.ProvidersManager.dataset_factories` → `airflow.providers_manager.ProvidersManager.asset_factories`\r\n    * `airflow.providers_manager.ProvidersManager.dataset_uri_handlers` → `airflow.providers_manager.ProvidersManager.asset_uri_handlers`\r\n    * `airflow.providers_manager.ProvidersManager.dataset_to_openlineage_converters` → `airflow.providers_manager.ProvidersManager.asset_to_openlineage_converters`\r\n* constant / variable\r\n    * `airflow.security.permissions.RESOURCE_DATASET` → `airflow.security.permissions.RESOURCE_ASSET`\r\n    * `airflow.providers.amazon.auth_manager.avp.entities.AvpEntities.DATASET` → `airflow.providers.amazon.auth_manager.avp.entities.AvpEntities.ASSET`\r\n* context key\r\n    * `triggering_dataset_events` → `triggering_asset_events`\r\n* resource key\r\n    * `dataset-uris` → `asset-uris` (for providers amazon, common.io, mysql, fab, postgres, trino)\r\n\r\n\r\n#### class\r\n* `airflow.models.ImportError` → `airflow.models.errors.ParseImportError` (from #41367)\r\n* `airflow.models.taskMixin.TaskMixin` → `airflow.models.taskMixin.DependencyMixin` (from #41394)\r\n* `airflow.sensors.external_task.ExternalTaskSensorLink` → `airflow.sensors.external_task.ExternalDagLin` (from #41391)\r\n* `airflow.operators.local_kubernetes_executor.BashOperator` → `airflow.operators.bash.BashOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.BaseBranchOperator` → `airflow.operators.branch.BaseBranchOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.EmptyOperator` → `airflow.operators.empty.EmptyOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.DummyOperator` → `airflow.operators.empty.EmptyOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.EmptyOperator` → `airflow.operators.empty.EmptyOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.DummyOperator` → `airflow.operators.empty.EmptyOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.EmailOperator` → `airflow.operators.email.EmailOperator` (from #41368)\r\n* `airflow.sensors.local_kubernetes_executor.BaseSensorOperator` → `airflow.sensors.base.BaseSensorOperator` (from #41368)\r\n* `airflow.sensors.local_kubernetes_executor.DateTimeSensor` → `airflow.sensors.date_time.DateTimeSensor` (from #41368)\r\n* `airflow.sensors.local_kubernetes_executor.ExternalTaskMarker` → `airflow.sensors.external_task.ExternalTaskMarker` (from #41368)\r\n* `airflow.sensors.local_kubernetes_executor.ExternalTaskSensor` → `airflow.sensors.external_task.ExternalTaskSensor` (from #41368)\r\n* `airflow.sensors.local_kubernetes_executor.ExternalTaskSensorLink` → `airflow.sensors.external_task.ExternalTaskSensorLink` (from #41368)\r\n* `airflow.sensors.local_kubernetes_executor.TimeDeltaSensor` → `airflow.sensors.time_delta.TimeDeltaSensor` (from #41368)\r\n\r\n#### function\r\n* `airflow.utils.helpers.chain` → `airflow.models.baseoperator.chain` (from #41520)\r\n* `airflow.utils.helpers.chain` → `airflow.models.baseoperator.cross_downstream` (from #41520)\r\n* `airflow.secrets.local_filesystem.load_connections` → `airflow.secrets.local_filesystem.load_connections_dict` (from #41533)\r\n* `airflow.secrets.local_filesystem.get_connection` → `airflow.secrets.local_filesystem.load_connections_dict` (from #41533)\r\n* `airflow.secrets.base_secrets.BaseSecretsBackend.get_conn_uri` → `airflow.secrets.base_secrets.BaseSecretsBackend.get_conn_value` (from #41642)\r\n* `airflow.secrets.base_secrets.BaseSecretsBackend.get_connections` → `airflow.secrets.base_secrets.BaseSecretsBackend.get_connection` (from #41642)\r\n* `airflow.www.utils.get_sensitive_variables_fields` → `airflow.utils.log.secrets_masker.get_sensitive_variables_fields` (from #41758)\r\n* `airflow.www.utils.should_hide_value_for_key` → `airflow.utils.log.secrets_masker.should_hide_value_for_key` (from #41758)\r\n* `airflow.configuration.get` → `airflow.configuration.conf.get` (from  #43530)\r\n* `airflow.configuration.getboolean` → `airflow.configuration.conf.getboolean` (from  #43530)\r\n* `airflow.configuration.getfloat` → `airflow.configuration.conf.getfloat` (from  #43530)\r\n* `airflow.configuration.getint` → `airflow.configuration.conf.getint` (from  #43530)\r\n* `airflow.configuration.has_option` → `airflow.configuration.conf.has_option` (from  #43530)\r\n* `airflow.configuration.remove_option` → `airflow.configuration.conf.remove_option` (from  #43530)\r\n* `airflow.configuration.as_dict` → `airflow.configuration.conf.as_dict` (from  #43530)\r\n* `airflow.configuration.set` → `airflow.configuration.conf.set` (from  #43530)\r\n\r\n#### parameter\r\n* in `DayOfWeekSensor.__init__`\r\n    * `use_task_execution_day` → `use_task_execution_day` (from #41393)\r\n* in `airflow.operators.datetime.BranchDateTimeOperator`\r\n    * `use_task_execution_day` → `use_task_logical_date` (from #41736)\r\n* in `airflow.operators.weekday.BranchDayOfWeekOperator`\r\n    * `use_task_execution_day` → `use_task_logical_date` (from #41736)\r\n* in `BaseOperator`\r\n    * `task_concurrency` → `max_active_tis_per_dag`  (from #41761) https://github.com/astral-sh/ruff/pull/14616\r\n\r\n### AIR304: moved to provider\r\n\r\n#### module\r\n* `airflow.hooks.dbapi` → `airflow.providers.common.sql.hooks.sql` (from #41748)\r\n* `airflow.api.auth.backend.default` → ` airflow.providers.fab.auth_manager.api.auth.backend.session` (from #43096)\r\n\r\n#### class\r\n* `airflow.www.security.FabAirflowSecurityManagerOverride` → `airflow.providers.fab.auth_manager.security_manager.override.FabAirflowSecurityManagerOverride` (from #41758)\r\n* `airflow.executors.local_kubernetes_executor.CeleryExecutor` → `airflow.providers.celery.executors.celery_executor.CeleryExecutor` (from #41368)\r\n* `airflow.executors.local_kubernetes_executor.CeleryKubernetesExecutor` → `airflow.providers.celery.executors.celery_kubernetes_executor.CeleryKubernetesExecutor` (from #41368)\r\n* `airflow.executors.local_kubernetes_executor.DaskExecutor` → `airflow.providers.daskexecutor.executors.dask_executor.DaskExecutor` (from #41368)\r\n* `airflow.executors.local_kubernetes_executor.KubernetesExecutor` → `airflow.providers.cncf.kubernetes.executors.kubernetes_executor.KubernetesExecutor` (from #41368)\r\n* `airflow.executors.local_kubernetes_executor.AirflowKubernetesScheduler` → `airflow.providers.cncf.kubernetes.executors.kubernetes_executor_utils.AirflowKubernetesScheduler` (from #41368)\r\n* `airflow.executors.local_kubernetes_executor.KubernetesJobWatcher` → `airflow.providers.cncf.kubernetes.executors.kubernetes_executor_utils.KubernetesJobWatcher` (from #41368)\r\n* `airflow.executors.local_kubernetes_executor.ResourceVersion` → `airflow.providers.cncf.kubernetes.executors.kubernetes_executor_utils.ResourceVersion` (from #41368)\r\n* `airflow.executors.local_kubernetes_executor.LocalKubernetesExecutor` → `airflow.providers.cncf.kubernetes.executors.LocalKubernetesExecutor` (from #41368)\r\n* `airflow.hooks.local_kubernetes_executor.S3Hook` → `airflow.providers.amazon.aws.hooks.s3.S3Hook` (from #41368)\r\n* `airflow.hooks.local_kubernetes_executor.BaseHook` → `airflow.hooks.base.BaseHook` (from #41368)\r\n* `airflow.hooks.local_kubernetes_executor.DbApiHook` → `airflow.providers.common.sql.hooks.sql.DbApiHook` (from #41368)\r\n* `airflow.hooks.local_kubernetes_executor.DockerHook` → `airflow.providers.docker.hooks.docker.DockerHook` (from #41368)\r\n* `airflow.hooks.local_kubernetes_executor.DruidDbApiHook` → `airflow.providers.apache.druid.hooks.druid.DruidDbApiHook` (from #41368)\r\n* `airflow.hooks.local_kubernetes_executor.DruidHook` → `airflow.providers.apache.druid.hooks.druid.DruidHook` (from #41368)\r\n* `airflow.hooks.local_kubernetes_executor.HiveCliHook` → `airflow.providers.apache.hive.hooks.hive.HiveCliHook` (from #41368)\r\n* `airflow.hooks.local_kubernetes_executor.HiveMetastoreHook` → `airflow.providers.apache.hive.hooks.hive.HiveMetastoreHook` (from #41368)\r\n* `airflow.hooks.local_kubernetes_executor.HiveServer2Hook` → `airflow.providers.apache.hive.hooks.hive.HiveServer2Hook` (from #41368)\r\n* `airflow.hooks.local_kubernetes_executor.HttpHook` → `airflow.providers.http.hooks.http.HttpHook` (from #41368)\r\n* `airflow.hooks.local_kubernetes_executor.JdbcHook` → `airflow.providers.jdbc.hooks.jdbc.JdbcHook` (from #41368)\r\n* `airflow.hooks.local_kubernetes_executor.jaydebeapi` → `airflow.providers.jdbc.hooks.jdbc.jaydebeapi` (from #41368)\r\n* `airflow.hooks.local_kubernetes_executor.MsSqlHook` → `airflow.providers.microsoft.mssql.hooks.mssql.MsSqlHook` (from #41368)\r\n* `airflow.hooks.local_kubernetes_executor.MySqlHook` → `airflow.providers.mysql.hooks.mysql.MySqlHook` (from #41368)\r\n* `airflow.hooks.local_kubernetes_executor.OracleHook` → `airflow.providers.oracle.hooks.oracle.OracleHook` (from #41368)\r\n* `airflow.hooks.local_kubernetes_executor.PigCliHook` → `airflow.providers.apache.pig.hooks.pig.PigCliHook` (from #41368)\r\n* `airflow.hooks.local_kubernetes_executor.PostgresHook` → `airflow.providers.postgres.hooks.postgres.PostgresHook` (from #41368)\r\n* `airflow.hooks.local_kubernetes_executor.PrestoHook` → `airflow.providers.presto.hooks.presto.PrestoHook` (from #41368)\r\n* `airflow.hooks.local_kubernetes_executor.SambaHook` → `airflow.providers.samba.hooks.samba.SambaHook` (from #41368)\r\n* `airflow.hooks.local_kubernetes_executor.SlackHook` → `airflow.providers.slack.hooks.slack.SlackHook` (from #41368)\r\n* `airflow.hooks.local_kubernetes_executor.SqliteHook` → `airflow.providers.sqlite.hooks.sqlite.SqliteHook` (from #41368)\r\n* `airflow.hooks.local_kubernetes_executor.WebHDFSHook` → `airflow.providers.apache.hdfs.hooks.webhdfs.WebHDFSHook` (from #41368)\r\n* `airflow.hooks.local_kubernetes_executor.ZendeskHook` → `airflow.providers.zendesk.hooks.zendesk.ZendeskHook` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.SQLCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLCheckOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.SQLIntervalCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLIntervalCheckOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.SQLThresholdCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLThresholdCheckOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.SQLValueCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLValueCheckOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.CheckOperator` → `airflow.providers.common.sql.operators.sql.SQLCheckOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.IntervalCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLIntervalCheckOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.ThresholdCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLThresholdCheckOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.ValueCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLValueCheckOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.TriggerDagRunLink` → `airflow.operators.trigger_dagrun.TriggerDagRunLink` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.TriggerDagRunOperator` → `airflow.operators.trigger_dagrun.TriggerDagRunOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.DockerOperator` → `airflow.providers.docker.operators.docker.DockerOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.DruidCheckOperator` → `airflow.providers.apache.druid.operators.druid_check.DruidCheckOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.GCSToS3Operator` → `airflow.providers.amazon.aws.transfers.gcs_to_s3.GCSToS3Operator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.GoogleApiToS3Operator` → `airflow.providers.amazon.aws.transfers.google_api_to_s3.GoogleApiToS3Operator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.GoogleApiToS3Transfer` → `airflow.providers.amazon.aws.transfers.google_api_to_s3.GoogleApiToS3Operator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.HiveOperator` → `airflow.providers.apache.hive.operators.hive.HiveOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.HiveStatsCollectionOperator` → `airflow.providers.apache.hive.operators.hive_stats.HiveStatsCollectionOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.HiveToDruidOperator` → `airflow.providers.apache.druid.transfers.hive_to_druid.HiveToDruidOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.HiveToDruidTransfer` → `airflow.providers.apache.druid.transfers.hive_to_druid.HiveToDruidOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.HiveToMySqlOperator` → `airflow.providers.apache.hive.transfers.hive_to_mysql.HiveToMySqlOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.HiveToMySqlTransfer` → `airflow.providers.apache.hive.transfers.hive_to_mysql.HiveToMySqlOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.HiveToSambaOperator` → `airflow.providers.apache.hive.transfers.hive_to_samba.HiveToSambaOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.SimpleHttpOperator` → `airflow.providers.http.operators.http.SimpleHttpOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.JdbcOperator` → `airflow.providers.jdbc.operators.jdbc.JdbcOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.LatestOnlyOperator` → `airflow.operators.latest_only.LatestOnlyOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.MsSqlOperator` → `airflow.providers.microsoft.mssql.operators.mssql.MsSqlOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.MsSqlToHiveOperator` → `airflow.providers.apache.hive.transfers.mssql_to_hive.MsSqlToHiveOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.MsSqlToHiveTransfer` → `airflow.providers.apache.hive.transfers.mssql_to_hive.MsSqlToHiveOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.MySqlOperator` → `airflow.providers.mysql.operators.mysql.MySqlOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.MySqlToHiveOperator` → `airflow.providers.apache.hive.transfers.mysql_to_hive.MySqlToHiveOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.MySqlToHiveTransfer` → `airflow.providers.apache.hive.transfers.mysql_to_hive.MySqlToHiveOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.OracleOperator` → `airflow.providers.oracle.operators.oracle.OracleOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.PapermillOperator` → `airflow.providers.papermill.operators.papermill.PapermillOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.PigOperator` → `airflow.providers.apache.pig.operators.pig.PigOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.Mapping` → `airflow.providers.postgres.operators.postgres.Mapping` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.PostgresOperator` → `airflow.providers.postgres.operators.postgres.PostgresOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.SQLCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLCheckOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.SQLIntervalCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLIntervalCheckOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.SQLValueCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLValueCheckOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.PrestoCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLCheckOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.PrestoIntervalCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLIntervalCheckOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.PrestoValueCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLValueCheckOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.PrestoToMySqlOperator` → `airflow.providers.mysql.transfers.presto_to_mysql.PrestoToMySqlOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.PrestoToMySqlTransfer` → `airflow.providers.mysql.transfers.presto_to_mysql.PrestoToMySqlOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.BranchPythonOperator` → `airflow.operators.python.BranchPythonOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.PythonOperator` → `airflow.operators.python.PythonOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.PythonVirtualenvOperator` → `airflow.operators.python.PythonVirtualenvOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.ShortCircuitOperator` → `airflow.operators.python.ShortCircuitOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.RedshiftToS3Operator` → `airflow.providers.amazon.aws.transfers.redshift_to_s3.RedshiftToS3Operator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.RedshiftToS3Transfer` → `airflow.providers.amazon.aws.transfers.redshift_to_s3.RedshiftToS3Operator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.S3FileTransformOperator` → `airflow.providers.amazon.aws.operators.s3_file_transform.S3FileTransformOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.S3ToHiveOperator` → `airflow.providers.apache.hive.transfers.s3_to_hive.S3ToHiveOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.S3ToHiveTransfer` → `airflow.providers.apache.hive.transfers.s3_to_hive.S3ToHiveOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.S3ToRedshiftOperator` → `airflow.providers.amazon.aws.transfers.s3_to_redshift.S3ToRedshiftOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.S3ToRedshiftTransfer` → `airflow.providers.amazon.aws.transfers.s3_to_redshift.S3ToRedshiftOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.SlackAPIOperator` → `airflow.providers.slack.operators.slack.SlackAPIOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.SlackAPIPostOperator` → `airflow.providers.slack.operators.slack.SlackAPIPostOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.BaseSQLOperator` → `airflow.providers.common.sql.operators.sql.BaseSQLOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.BranchSQLOperator` → `airflow.providers.common.sql.operators.sql.BranchSQLOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.SQLCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLCheckOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.SQLColumnCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLColumnCheckOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.SQLIntervalCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLIntervalCheckOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.SQLTableCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLTableCheckOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.SQLThresholdCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLThresholdCheckOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.SQLValueCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLValueCheckOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor._convert_to_float_if_possible` → `airflow.providers.common.sql.operators.sql._convert_to_float_if_possible` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.parse_boolean` → `airflow.providers.common.sql.operators.sql.parse_boolean` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.BranchSQLOperator` → `airflow.providers.common.sql.operators.sql.BranchSQLOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.BranchSqlOperator` → `airflow.providers.common.sql.operators.sql.BranchSQLOperator` (from #41368)\r\n* `airflow.operators.local_kubernetes_executor.SqliteOperator` → `airflow.providers.sqlite.operators.sqlite.SqliteOperator` (from #41368)\r\n* `airflow.sensors.local_kubernetes_executor.HivePartitionSensor` → `airflow.providers.apache.hive.sensors.hive_partition.HivePartitionSensor` (from #41368)\r\n* `airflow.sensors.local_kubernetes_executor.HttpSensor` → `airflow.providers.http.sensors.http.HttpSensor` (from #41368)\r\n* `airflow.sensors.local_kubernetes_executor.MetastorePartitionSensor` → `airflow.providers.apache.hive.sensors.metastore_partition.MetastorePartitionSensor` (from #41368)\r\n* `airflow.sensors.local_kubernetes_executor.NamedHivePartitionSensor` → `airflow.providers.apache.hive.sensors.named_hive_partition.NamedHivePartitionSensor` (from #41368)\r\n* `airflow.sensors.local_kubernetes_executor.S3KeySensor` → `airflow.providers.amazon.aws.sensors.s3.S3KeySensor` (from #41368)\r\n* `airflow.sensors.local_kubernetes_executor.SqlSensor` → `airflow.providers.common.sql.sensors.sql.SqlSensor` (from #41368)\r\n* `airflow.sensors.local_kubernetes_executor.SqlSensor` → `airflow.providers.common.sql.sensors.sql.SqlSensor` (from #41368)\r\n* `airflow.sensors.local_kubernetes_executor.WebHdfsSensor` → `airflow.providers.apache.hdfs.sensors.web_hdfs.WebHdfsSensor` (from #41368)\r\n\r\n#### function\r\n* `airflow.api.auth.backend.basic_auth` → `airflow.providers.fab.auth_manager.api.auth.backend.basic_auth` (from #41663)\r\n* `airflow.api.auth.backend.kerberos_auth` → airflow.executors.`airflow.providers.fab.auth_manager.api.auth.backend.kerberos_auth` (from #41693)\r\n* `airflow.auth.managers.fab.api.auth.backend.kerberos_auth` → `airflow.providers.fab.auth_manager.api.auth.backend.kerberos_auth` (from #41693)\r\n* `airflow.auth.managers.fab.fab_auth_manager` → `airflow.providers.fab.auth_manager.security_manager.override` (from #41708)\r\n* `airflow.auth.managers.fab.security_manager.override` → `airflow.providers.fab.auth_manager.security_manager.override` (from #41708)\r\n\r\n#### constant / variable\r\n* `airflow.executors.local_kubernetes_executor.ALL_NAMESPACES` → `airflow.providers.cncf.kubernetes.executors.kubernetes_executor_types.ALL_NAMESPACES` (from #41368)\r\n* `airflow.executors.local_kubernetes_executor.POD_EXECUTOR_DONE_KEY` → `airflow.providers.cncf.kubernetes.executors.kubernetes_executor_types.POD_EXECUTOR_DONE_KEY` (from #41368)\r\n* `airflow.hooks.local_kubernetes_executor.HIVE_QUEUE_PRIORITIES` → `airflow.providers.apache.hive.hooks.hive.HIVE_QUEUE_PRIORITIES` (from #41368)\r\n* `airflow.executors.local_kubernetes_executor.app` → `airflow.providers.celery.executors.celery_executor_utils.app` (from #41368)\r\n* `airflow.macros.local_kubernetes_executor.closest_ds_partition` → `airflow.providers.apache.hive.macros.hive.closest_ds_partition` (from #41368)\r\n* `airflow.macros.local_kubernetes_executor.max_partition` → `airflow.providers.apache.hive.macros.hive.max_partition` (from #41368)\r\n\r\n### AIR310: models related changes (AIP-72) not going to do it\r\n* #40931\r\n* #41440\r\n* #41761\r\n    * `airflow.models.baseoperator.BaseOperatorLink` → `airflow.models.baseoperatorlink.BaseOperatorLink`\r\n* #41762\r\n    * `airflow.models.connection.parse_netloc_to_hostname`\r\n    * `airflow.models.connection.Connection.parse_from_uri`\r\n    * `airflow.models.connection.Connection.log_info`\r\n    * `airflow.models.connection.Connection.debug_info`\r\n* #41774\r\n* #41776\r\n* #41778\r\n* #41779\r\n* #41780\r\n* #41784\r\n* #41808\r\n* #41964\r\n* #42023\r\n* #42548\r\n* #42739\r\n* #42776\r\n* #43067\r\n* #43490\r\n\r\n------\r\n\r\n\r\n</details>"", 'created_at': datetime.datetime(2024, 11, 29, 16, 47, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2541137439, 'issue_id': 2477789694, 'author': 'sunank200', 'body': 'Created the PR for `airflow config lint` : https://github.com/apache/airflow/pull/44908', 'created_at': datetime.datetime(2024, 12, 13, 10, 40, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567247877, 'issue_id': 2477789694, 'author': 'Lee-W', 'body': 'Most of the rules have now been added to `ruff` or `airflow config lint.` We currently have two open PRs waiting for the ruff team to review https://github.com/astral-sh/ruff/pull/15144 and https://github.com/astral-sh/ruff/pull/15216. One rule not yet included in the previous PRs is blocked by https://github.com/astral-sh/ruff/pull/15144, but it could be done no longer after https://github.com/astral-sh/ruff/pull/15144 is merged.', 'created_at': datetime.datetime(2025, 1, 2, 3, 9, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567446120, 'issue_id': 2477789694, 'author': 'jscheffl', 'body': '@sunank200 / @Lee-W COOL!', 'created_at': datetime.datetime(2025, 1, 2, 8, 54, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567554518, 'issue_id': 2477789694, 'author': 'potiuk', 'body': 'Nice.  Glad to see Astral team cooperates on it :). \r\n\r\nBTW. Do they have any plans to be able (if possible) to implement some kind of plugins that we would be able to release on our own maybe? I remember that in the past that was a bit problematic because of the way how RUST ABI worked (or so I remember) - but also I think this has been solved already.\r\n\r\nWhile now it can take quite some time for things to iterate and get released in the new ruff version, when Airflow 3 gets released, we will have sometimes likely a quick fix or new rule to be released fairly quickly, and I think it\'s not a good idea to overburden Astral team with reviews, merges and releases, and it woudl be cool if we could have our own ""plugin"" of sorts that we could release and implement changes on our own.\r\n\r\nHas this been discussed or considered at all @Lee-W  ? Should we start such a discussion ?', 'created_at': datetime.datetime(2025, 1, 2, 10, 26, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567737472, 'issue_id': 2477789694, 'author': 'Lee-W', 'body': '> Nice. Glad to see Astral team cooperates on it :).\r\n> \r\n> BTW. Do they have any plans to be able (if possible) to implement some kind of plugins that we would be able to release on our own maybe? I remember that in the past that was a bit problematic because of the way how RUST ABI worked (or so I remember) - but also I think this has been solved already.\r\n>\r\n> While now it can take quite some time for things to iterate and get released in the new ruff version, when Airflow 3 gets released, we will have sometimes likely a quick fix or new rule to be released fairly quickly, and I think it\'s not a good idea to overburden Astral team with reviews, merges and releases, and it woudl be cool if we could have our own ""plugin"" of sorts that we could release and implement changes on our own.\r\n\r\nI think it\'s still an open issue. https://github.com/astral-sh/ruff/issues/283 🤔\r\n\r\n> Has this been discussed or considered at all @Lee-W ? Should we start such a discussion ?\r\n\r\nIt\'s not yet been discussed. Not sure whether there will be rules really need to be released that quickly since it\'s not actually breaking Airflow and rules can be ignored 🤔', 'created_at': datetime.datetime(2025, 1, 2, 12, 57, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2567886711, 'issue_id': 2477789694, 'author': 'potiuk', 'body': '> It\'s not yet been discussed. Not sure whether there will be rules really need to be released that quickly since it\'s not actually breaking Airflow and rules can be ignored 🤔\r\n\r\nTrue, that\'s why I am not **too** worried as this is just a ""supplemental"" code. And yeah - the plugin system is still in discussion i see, and I do not think we have strong enough case to ""badly need"" it - it\'s more that I generally do not like when ""someone else"" controls some airflow-specific code than Airlfow PMC. And this is not something I have against Astral, not at all, it\'s just we as PMC do not have final saying there, and someone else can add new rules or change ours - so that\'s a bit of a danger I see). \r\n\r\nVarious scenarios here are possible - and it\'s just a little bit of an ""itch"" that I wonder if we should ""scratch"".', 'created_at': datetime.datetime(2025, 1, 2, 14, 47, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2576860156, 'issue_id': 2477789694, 'author': 'uranusjr', 'body': 'I was thinking about what exactly is needed for users to migrate. I think we probably should backport `airflow config lint` to 2.11? This way the user can choose to change the configs first before they upgrade Airflow itself.', 'created_at': datetime.datetime(2025, 1, 8, 6, 38, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2577430105, 'issue_id': 2477789694, 'author': 'kaxil', 'body': '> I was thinking about what exactly is needed for users to migrate. I think we probably should backport `airflow config lint` to 2.11? This way the user can choose to change the configs first before they upgrade Airflow itself.\r\n\r\nYeah, agreed - we should', 'created_at': datetime.datetime(2025, 1, 8, 11, 21, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2588755557, 'issue_id': 2477789694, 'author': 'Lee-W', 'body': 'https://github.com/apache/airflow/issues/45632\n\njust created an issue for backporting', 'created_at': datetime.datetime(2025, 1, 14, 2, 36, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2594013386, 'issue_id': 2477789694, 'author': 'kaxil', 'body': 'fyi: https://github.com/apache/airflow/pull/45694 contains breaking changes too', 'created_at': datetime.datetime(2025, 1, 15, 21, 52, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2595091154, 'issue_id': 2477789694, 'author': 'eladkal', 'body': 'We need to add rule to help users migrate email from core to SMTP provider\nhttps://github.com/apache/airflow/pull/45705 https://github.com/apache/airflow/pull/46041', 'created_at': datetime.datetime(2025, 1, 16, 10, 4, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2595165361, 'issue_id': 2477789694, 'author': 'Lee-W', 'body': '> We need to add rule to help users migrate email from core to SMTP provider [#45705](https://github.com/apache/airflow/pull/45705)\n\nYep, I can start the implementation once the change has been merged into main', 'created_at': datetime.datetime(2025, 1, 16, 10, 38, 11, tzinfo=datetime.timezone.utc)}]","Lee-W (Issue Creator) on (2024-08-21 11:30:06 UTC): The `Rules` now is an example of how these changes can be recorded. I will check the existing breaking changes and update the rules. It would be great if folks could help update this list if you know there are breaking changes.

potiuk on (2024-08-21 13:48:22 UTC): I pinned the issue - this way it will show up at the top of ""Issues"" list in the repo

potiuk on (2024-08-21 13:49:05 UTC): ![image](https://github.com/user-attachments/assets/fb1fb04d-f843-4c63-a536-2d7a06cb8388)

Lee-W (Issue Creator) on (2024-08-21 14:50:44 UTC): Thanks!

eladkal on (2024-08-24 17:43:33 UTC): We can just go over all the significant newsfragments and create a rule for them or keep some reasoning why it doesn't require one

kaxil on (2024-10-24 15:37:17 UTC): We should add something for the [public API change](https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html) too. API v1 won't work anymore. Those are being changed as part of AIP-84 to a new FastApi based app. GitHub project for it: https://github.com/orgs/apache/projects/414

pierrejeambrun on (2024-10-25 12:21:27 UTC): Issue here to regroup Rest API breaking changes https://github.com/apache/airflow/issues/43378

tirkarthi on (2024-10-27 06:58:02 UTC): I have started prototyping a small package based on LibCST to build a Python 2to3 like tool for Airflow 2to3 that does simple and straight forward replacements. My main motivation was around lot of our users in our Airflow instance using `schedule_interval` in Airflow 2 that was deprecated and renamed to `schedule` in Airflow 3. It would require updating thousands of dags manually and some automation could help. This could also help in places with import statements changes .E.g. Task SDK need to be updated from `from airflow import DAG` to `from airflow.sdk import DAG`. Something like this could eventually become part of Airflow cli so that users can run `airflow migrate /airflow/dags` for migration or serve as a starter point for migration. It can update the file in place or show diff. Currently it does the following changes : 

Dags

* schedule_interval -> schedule
* timetable -> schedule
* concurrency -> max_active_tasks
* Removal of unused full_filepath parameter
* default_view (tree -> grid)

Operators

* task_concurrency -> max_active_tis_per_dag
* trigger_rule (none_failed_or_skipped -> none_failed_min_one_success)

Sample file

```python
import datetime

from airflow import DAG
from airflow.decorators import dag, task
from airflow.operators.empty import EmptyOperator
from airflow.timetables.events import EventsTimetable


with DAG(
    dag_id=""my_dag_name"",
    default_view=""tree"",
    start_date=datetime.datetime(2021, 1, 1),
    schedule_interval=""@daily"",
    concurrency=2,
):
    op = EmptyOperator(
        task_id=""task"", task_concurrency=1, trigger_rule=""none_failed_or_skipped""
    )


@dag(
    default_view=""graph"",
    start_date=datetime.datetime(2021, 1, 1),
    schedule_interval=EventsTimetable(event_dates=[datetime.datetime(2022, 4, 5)]),
    max_active_tasks=2,
    full_filepath=""/tmp/test_dag.py""
)
def my_decorated_dag():
    op = EmptyOperator(task_id=""task"")


my_decorated_dag()
```

Sample usage

```shell
python -m libcst.tool codemod dag_fixer.DagFixerCommand -u 1 tests/test_dag.py
Calculating full-repo metadata...
Executing codemod...
reformatted -

All done! ✨ 🍰 ✨
1 file reformatted.
--- /home/karthikeyan/stuff/python/libcst-tut/tests/test_dag.py
+++ /home/karthikeyan/stuff/python/libcst-tut/tests/test_dag.py
@@ -10,6 +10,6 @@
     dag_id=""my_dag_name"",
-    default_view=""tree"",
+    default_view=""grid"",
     start_date=datetime.datetime(2021, 1, 1),
-    schedule_interval=""@daily"",
-    concurrency=2,
+    schedule=""@daily"",
+    max_active_tasks=2,
 ):
@@ -23,5 +23,4 @@
     start_date=datetime.datetime(2021, 1, 1),
-    schedule_interval=EventsTimetable(event_dates=[datetime.datetime(2022, 4, 5)]),
+    schedule=EventsTimetable(event_dates=[datetime.datetime(2022, 4, 5)]),
     max_active_tasks=2,
-    full_filepath=""/tmp/test_dag.py""
 )
Finished codemodding 1 files!
 - Transformed 1 files successfully.
 - Skipped 0 files.
 - Failed to codemod 0 files.
 - 0 warnings were generated.
```

Repo : https://github.com/tirkarthi/Airflow-2to3

potiuk on (2024-10-27 12:47:28 UTC): NICE! @tirkarthi -> you should start a thread about it at devlist and propose adding it to the repo. The sooner we start working on it and let poeple test it, the better it will be. And we can already start adding not only the newsfragments but also rules to the migration tools (cc: @vikramkoka @kaxil ) - we can even think about keeping a database of old-way-dags and running such migration tool on them and letting airflow scheduler from Airflow 3 process them (and maybe even execute) as part of our CI. This would tremendously help with maintaining and updating such a tool if we will make it a part of our CI pipeline.

potiuk on (2024-10-27 12:54:00 UTC): BTW. I like it a lot how simple it is with libCST - we previously used quite a bit more complex tool from Facebook that allowed to do refactoring at scale in parallell (https://github.com/facebookincubator/Bowler) , but it was rather brittle to develop rules for it and it had some weird problems and missing features. One thing that was vere useful - is that it had a nice ""parallelism"" features - which allowed to refactor 1000s of files in seconds (but also made it difficult to debug). 

I think if we get it working with libCST - it will be way more generic and maintainable, also we can easily add parallelism later on when/if we see it is slow.

potiuk on (2024-10-27 13:03:55 UTC): One small watchout though -  such a tool should have a way to isolate rules - so that they are not in a single big method - some abstraction that will allow us to easily develop and selectively apply (or skip) different rules - see https://github.com/apache/airflow/tree/v1-10-test/airflow/upgrade where we have documentation and information about the upgrade check we've done in Airflow 1 -> 2 migration.

Also we have to discuss, whether it should be a separate repo or whether it should be in airflow's monorepo. Both have pros and cons - in 1.10 we chose to keep it 1.10 branch of airflow, because it imported some of the airflow code and it was easier, but we could likely create a new repo for it, add CI there and keep it there.

We even have this archived repo https://github.com/apache/airflow-upgrade-check which we never used and archived, we could re-open it. We also have https://pypi.org/project/apache-airflow-upgrade-check/ - package in PyPI - and we could release new upgrade check versions (2.* ?) with ""apache-airflow>=2.11.0"" as dependency.

All that should likely be discussed at devlist :)

tirkarthi on (2024-10-27 13:25:34 UTC): Thanks @potiuk for the details. I will start a discussion on this at the devlist and continue there. Bowler looks interesting. Using `libcst.tool` from cli parallelizes the process. Right now this needs `python -m libcst.tool` to execute it as a codemod. Initially I had designed them as standalone Transformer for each category like (dag, operator) where the updated AST from one transformer can be passed to another. The codemod looked like a recommended abstraction for running it and changed it that way to later find cli accepts only one codemod at a time. I need to check how composable they are.

```
python -m libcst.tool codemod --help | grep -i -A 1 'jobs JOBS'
  -j JOBS, --jobs JOBS  Number of jobs to use when processing files. Defaults to number of cores

time python -m libcst.tool codemod dag_fixer.DagFixerCommand -u 1 ~/airflow/dags > /dev/null 2>&1 
python -m libcst.tool codemod dag_fixer.DagFixerCommand -u 1 ~/airflow/dags >  
6.95s user 0.61s system 410% cpu 1.843 total

# Single core
time python -m libcst.tool codemod dag_fixer.DagFixerCommand -u 1 -j 1 ~/airflow/dags > /dev/null 2>&1
python -m libcst.tool codemod dag_fixer.DagFixerCommand -u 1 -j 1  > 
/dev/nul  4.66s user 0.38s system 99% cpu 5.035 total

# 4 core
python -m libcst.tool codemod dag_fixer.DagFixerCommand -u 1 -j 4 ~/airflow/dags > /dev/null 2>&1
python -m libcst.tool codemod dag_fixer.DagFixerCommand -u 1 -j 4  > 
/dev/nul  5.45s user 0.54s system 253% cpu 2.358 total
```

potiuk on (2024-10-27 20:09:16 UTC): Don't be deceived by it :). 

 It was helpful for Provider's migration at some point in time, but I had many rough edges - like debugging a problem was a nightmare until we learned how to do it properly, also it had some annoying limitations - you had to learn a completely new non-standard abstractions (an SQLAlchemy-like DSL to perform modifications) - which did not cover all the refactorings we wanted to do. We had to really dig-deep into the code an find some workarounds for things we wanted to do, when authors of Bowler have not thoght about them. And sometimes those were nasty workarounds.

```python
query = (
    Query(<paths to modify>)
    .select_function(""old_name"")
    .rename(""new_name"")
    .diff(interactive=True)
)
```

Example that I remember above is that we could not rename some of the object types easily because it was not ""foreseen"" (can't remember exactly) - we had a few surprises there.

Also Bowler seems to be not maintained for > 3 years and it means that it's unlikely to handle some constructs even in 3.9+ Airflow.

What I like about libcst is that it is really ""low-level"" interface that you have to program in Python rather than in abstract DSL -  similar to ""ast"".  You write actual python code to perform what you want to perform rather than rely on incomplete abstractions, even if you have to copy&paste rename code between different ""rules"" (for example) (which you can then abstract away as 'common` util if you need, so no big deal).

potiuk on (2024-10-27 20:15:47 UTC): BTW. Codemod .... is also 5 years not maintained. Not that it is disqualification - but they list `python2` as their dependency ... so .....

Lee-W (Issue Creator) on (2024-10-28 01:22:07 UTC): I tried to use libcst in airflow as a tiny POC of this issue here https://github.com/apache/airflow/blob/5b7977a149492168688e6f013a7dcd4fe3561a49/scripts/ci/pre_commit/check_deferrable_default.py#L34. It mostly works great except for its speed. I was also thinking about whether to add these migrations thing info ruff airflow linter but not yet explore much on the rust/ruff side.

potiuk on (2024-10-28 09:42:16 UTC): :eyes:  :eyes:  `rust` project :) ... 

Me :heart:  it  (but I doubt we want to invest in it as it might be difficult to maintain, unless we find quite a few committers who are somewhat ruff profficient to at least be able to review the code) . But it's tempting I must admit.

But to be honest - while I'd love to finally get a serious rust project, it's not worth it I think we are talking of one-time migration for even a 10.000 dags it will take at most single minutes and we can turn it maybe in under one minute with rust - so not a big gain for a lot of pain :) . Or at lest this is what my intuition tells me.

I think parallelism will do the job nicely. My intuition tells me (but this is just intuition and understanding on some limits ans speed of certain operation) - that we will get from  multiple 10s of minutes (when running such migration sequentially) to single minutes when we allow to run migration in parallel using multiple processors and processes - even with Python and libcst. This task is really suitable for such parallelisation because each file is complete, independent task that can be run in complete isolation from all other tasks - so spawning multiple paralllel interpreters, ideally forking them right after all the imports and common code is loaded so that they use shared memory for those - this **should** do the job nicely (at least intuitively).

Using RUST for that might be classic premature optimisation - we might likely not need it :). But would be worth to make some calculations and get some ""numbers"" for big installation - i.e. how many dags of what size are out there, and how long it will be to parse them all with libcst and write back (even unmodified or with a simple modification).  I presume that parsing and writing back will be the bulk of the job - and modifications will add very little overhead as they will be mostly operating on in memory data structures.

Lee-W (Issue Creator) on (2024-10-30 01:16:07 UTC): Yep, totally agree. I just want to raise this idea which might be interesting. 👀


Yep, I think you're right. My previous default deferrable script took around 10 sec to process ~400 operators. Using ast for checking took around 1 sec

potiuk on (2024-11-11 23:32:11 UTC): Mostly as curiosity:  One option we might consider is https://github.com/alexpovel/srgn - I've heard about it recently, it's a ""grep that understands code"" with capabilities of running different actions.  Written in rust, and allows to add extensions apparently where you can define your own ""scopes"" of search and modification.

But I am not too convinced - this is mostly a command line tool so we would have to have a sequence of ""script commands"" to run - seems that plugging in our own rules and AST parsing should also be more flexible, even if slower.

Lee-W (Issue Creator) on (2024-11-14 01:45:34 UTC): Yep, not that convinced either. but it is always good to have an alternative we could consider 🤔

uranusjr on (2024-11-18 08:48:16 UTC): My best idea right now is to split this into two tools. We don’t really want to invest too much time into building a very rich CLI tool to show users what _need_ to be changed—we’ll effectively be rebuilding the error reporting interface in ruff (or flake8). Those squiggle lines, colors, error codes, and code context things are not easy to build.

It is probably easiest to tack the linter part on Ruff—it is Rust, but the code to implement a lint rule isn’t that hard if you know Python AST and just a general idea about C-like languages. The rewrite part is a lot more difficult, so it’s probably better to implement this as a different tool in Python with libcst. I’m thinking something like

```console
$ ruff check --select AIR
This spits out lint errors with codes like AIR005 AIR123...

$ airflow2to3 --select AIR005 -- path/to/dag/file.py
This fixes the given error(s) in given file(s) in-place with a minimal CLI...
```

I plan to start experiementing some rules in Ruff to see how easy the first part actually is. We should be able to save a lot of effort if it is viable.

Lee-W (Issue Creator) on (2024-11-18 11:02:02 UTC): I tried to change the format a bit and list the rules in the following format. 

```markdown
* [ ] link to the pr with breaking change
    * [ ] things to do
```

Once the `things to do` have been listed, we can check the root pr. After implementing the rule, we can mark the `things to do` as done.

I also updated the format for #41366, #41367, #41368, #41391, #41393

If anyone has anything to add but do not have permission to update the description. Please just tag me and I'll take a look

potiuk on (2024-11-19 00:01:35 UTC): Actually I am convinced too - I quite like this one after a bit of thought. This is not something that might be maintained by a lot of people and a number of contributors, and even for them, this is so far from the main ""airflow code"" - it's really a ""one-time"" tool - that it might be worth treating it as our first ""rust experiment"". And I quite agree that, the AST code on it's own is not really that ""pythonic"" and if you know what you want, and have already existing examples, adding a new rule in RUST, should not be difficult even if you do not know it (and AI driven development here might be even pretty cool exercise). I'd myself be happy to add a few rules at some point of time and maybe even take part in implementing the tooling for rust for our CI environment.

Lee-W (Issue Creator) on (2024-11-19 07:46:14 UTC): The things we'll need to migrate for 41348

* [ ] https://github.com/apache/airflow/pull/41348
    * [ ] module `airflow.datasets` -> `airflow.sdk.definitions.asset`
        * [ ] class `DatasetAlias` -> `AssetAlias`
        * [ ] class `DatasetAll` -> `AssetAll`
        * [ ] class `DatasetAny` -> `AssetAny`
        * [ ] function `expand_alias_to_datasets` -> `expand_alias_to_assets`
        * [ ] class `DatasetAliasEvent` -> `AssetAliasEvent`
            * [ ] attribute `dest_dataset_uri` -> `BaseAsset`
        * [ ] class `BaseDataset` -> `BaseAsset`
            * [ ] method `iter_datasets` -> `iter_assets`
            * [ ] method `iter_dataset_aliases` -> `iter_asset_aliases`
        * [ ] class `Dataset` -> `Asset`
            * [ ] method `iter_datasets` -> `iter_assets`
            * [ ] method `iter_dataset_aliases` -> `iter_asset_aliases`
        * [ ] class `_DatasetBooleanCondition` -> `_AssetBooleanCondition`
            * [ ] method `iter_datasets` -> `iter_assets`
            * [ ] method `iter_dataset_aliases` -> `iter_asset_aliases`
    * [ ] module `airflow.datasets.manager` → `airflow.assets.manager`
        * [ ] variable `dataset_manager` → `asset_manager`
        * [ ] function `resolve_dataset_manager` → `resolve_asset_manager`
        * [ ] class `DatasetManager` → `AssetManager`
            * [ ] method `register_dataset_change` → `register_asset_change`
            * [ ] method `create_datasets` → `create_assets`
            * [ ] method `register_dataset_change` → `notify_asset_created`
            * [ ] method `notify_dataset_changed` → `notify_asset_changed`
            * [ ] method `notify_dataset_alias_created` → `notify_asset_alias_created`
    * [ ] module `airflow.listeners.spec.dataset` → `airflow.listeners.spec.asset`
        * [ ] function `on_dataset_created` → `on_asset_created`
        * [ ] function `on_dataset_changed` → `on_asset_changed`
    * [ ] module `airflow.timetables.datasets` → `airflow.timetables.assets`
        * [ ] class `DatasetOrTimeSchedule` → `AssetOrTimeSchedule`
    * [ ] module `airflow.datasets.metadata` → `airflow.sdk.definitions.asset.metadata`
    * [ ] module `airflow.listeners.spec.dataset` → `airflow.listeners.spec.asset`
        * [ ] function `on_dataset_created` → `on_asset_created`
        * [ ] function `on_dataset_changed` → `on_asset_changed`
    * [ ] class `airflow.timetables.datasets.DatasetOrTimeSchedule` → `airflow.timetables.assets.AssetOrTimeSchedule`
    * [ ] decorator `airflow.api_connexion.security.requires_access_dataset` → `airflow.api_connexion.security.requires_access_dataset.requires_access_asset`
    * [ ] class `airflow.auth.managers.models.resource_details.DatasetDetails` → `airflow.auth.managers.models.resource_details.AssetDetails
    * [ ] function `airflow.auth.managers.base_auth_manager.is_authorized_dataset` → `airflow.auth.managers.base_auth_manager.is_authorized_asset`
    * [ ] class `airflow.timetables.simple.DatasetTriggeredTimetable` → `airflow.timetables.simple.AssetTriggeredTimetable`
    * in class `airflow.providers_manager.ProvidersManager`
        * [ ] method `initialize_providers_dataset_uri_resources` → `initialize_providers_asset_uri_resources`
        * [ ] property `dataset_factories` → `asset_factories`
        * [ ] property `dataset_uri_handlers` → `asset_uri_handlers`
        * [ ] property `dataset_to_openlineage_converters` → `asset_to_openlineage_converters`
    * [ ] constant `airflow.security.permissions.RESOURCE_DATASET` → `airflow.security.permissions.RESOURCE_ASSET`
    * [ ] function `airflow.www.auth.has_access_dataset` → `airflow.www.auth.has_access_dataset.has_access_asset`
    * [ ] class `airflow.lineage.hook.DatasetLineageInfo`  → `airflow.lineage.hook.AssetLineageInfo`
        * [ ] attribute `dataset` → `asset`
    * In class `airflow.lineage.hook.HookLineageCollector`
        * [ ] method `create_dataset` → `create_asset`
        * [ ] method `add_input_dataset` → `add_input_asset`
        * [ ] method `add_output_dataset` → `add_output_asset`
        * [ ] method `collected_datasets` → `collected_assets`
    * [ ] context key `triggering_dataset_events` → `triggering_asset_events`
    * In amazon provider
        * package `airflow.providers.amazon.aws.datasets`  → `airflow.providers.amazon.aws.assets`
            * in module `s3`
                * [ ] method `create_dataset` → `create_asset`
                * [ ] method `convert_dataset_to_openlineage` → `convert_asset_to_openlineage`
        * [ ] attribute `airflow.providers.amazon.auth_manager.avp.entities.AvpEntities.DATASET` → `airflow.providers.amazon.auth_manager.avp.entities.AvpEntities.ASSET`
        * [ ] method `airflow.providers.amazon.auth_manager.aws_auth_manager.AwsAuthManager.is_authorized_dataset` → `airflow.providers.amazon.auth_manager.aws_auth_manager.AwsAuthManager.is_authorized_asset`
        * [ ] resource key `dataset-uris` → `asset-uris`
    * In Common IO Provider
        * [ ] package `airflow.providers.common.io.datasets` → `airflow.providers.common.io.assets`
            * in module `file`
                * [ ] method `create_dataset` → `create_asset`
                * [ ] method `convert_dataset_to_openlineage` → `convert_asset_to_openlineage`
        * [ ] resource key `dataset-uris` → `asset-uris`
    * In fab provider
        * [ ] function `airflow.providers.fab.auth_manager.fab_auth_manager.is_authorized_dataset` → `airflow.providers.fab.auth_manager.fab_auth_manager.is_authorized_asset`
    * In openlineage provider
        * in module `airflow.providers.openlineage.utils.utils`
            * [ ] class `DatasetInfo` → `AssetInfo`
            * [ ] function `translate_airflow_dataset` → `translate_airflow_asset`
    * In postgres provider
        * [ ] package `airflow.providers.postgres.datasets` → `airflow.providers.postgres.assets`
        * [ ] resource key `dataset-uris` → `asset-uris`
    * In mysql provider
        * [ ] package `airflow.providers.mysql.datasets` → `airflow.providers.mysql.assets`
        * [ ] resource key `dataset-uris` → `asset-uris`
    * In trino provider
        * [ ] package `airflow.providers.trino.datasets` → `airflow.providers.trino.assets`
        * [ ] resource key `dataset-uris` → `asset-uris`
    * ❌ ignored
        * `airflow.api_connexion.schemas.dataset_schema`
        * `airflow.api_ui.views.datasets`
        * `airflow.serialization.pydantic.dataset`
        * `airflow.serialization.pydantic.taskinstance`
        * `airflow.serialization.enums.DagAttributeTypes`
        * `airflow.serialization.serialized_objects`
        * `airflow.utils.context`
        * models
        * DagDependency names
        * private methods

Lee-W (Issue Creator) on (2024-11-19 11:07:54 UTC): Hi all, I'm trying to read through the significant news fragment and compile a list of rules we should migrate. It would be nice if you could take a look and check if I missed anything.

* @jscheffl
    * [x] https://github.com/apache/airflow/pull/40029
    * [x] https://github.com/apache/airflow/pull/41733
    * [x] https://github.com/apache/airflow/pull/41735
    * [x] https://github.com/apache/airflow/pull/41736
    * [x] https://github.com/apache/airflow/pull/41737
    * [x] https://github.com/apache/airflow/pull/41739
    * [x] https://github.com/apache/airflow/pull/41761
    * [x] https://github.com/apache/airflow/pull/41762
    * [x] https://github.com/apache/airflow/pull/41774
    * [x] https://github.com/apache/airflow/pull/41776
    * [x] https://github.com/apache/airflow/pull/41778
    * [x] https://github.com/apache/airflow/pull/41779
    * [x] https://github.com/apache/airflow/pull/41780
    * [x] https://github.com/apache/airflow/pull/41808
* @dirrao 
    * [ ] https://github.com/apache/airflow/pull/40931
    * [ ] https://github.com/apache/airflow/pull/41096
    * [ ] https://github.com/apache/airflow/pull/41539
    * [ ] https://github.com/apache/airflow/pull/41496
    * [ ] https://github.com/apache/airflow/pull/41550
    * [ ] https://github.com/apache/airflow/pull/41552
    * [ ] https://github.com/apache/airflow/pull/41579
    * [ ] https://github.com/apache/airflow/pull/41609
    * [ ] https://github.com/apache/airflow/pull/41635
    * [ ] https://github.com/apache/airflow/pull/41642
    * [ ] https://github.com/apache/airflow/pull/41663
    * [ ] https://github.com/apache/airflow/pull/41693
    * [ ] https://github.com/apache/airflow/pull/41708
    * [ ] https://github.com/apache/airflow/pull/41748
    * [ ] https://github.com/apache/airflow/pull/41784
    * [ ] https://github.com/apache/airflow/pull/41910
    * [ ] https://github.com/apache/airflow/pull/42060
    * [ ] https://github.com/apache/airflow/pull/42088
    * [ ] https://github.com/apache/airflow/pull/42100
    * [ ] https://github.com/apache/airflow/pull/42126
    * [ ] https://github.com/apache/airflow/pull/42129
* @kaxil 
    * [ ] https://github.com/apache/airflow/pull/41390
    * [ ] https://github.com/apache/airflow/pull/41393
* @dstandish 
    * [ ] https://github.com/apache/airflow/pull/41440
* @pierrejeambrun 
    * [ ] https://github.com/apache/airflow/pull/41857
* @jedcunningham 
    * [ ] https://github.com/apache/airflow/pull/41964
* @uranusjr 
    * [ ] https://github.com/apache/airflow/pull/42054

Lee-W (Issue Creator) on (2024-11-19 11:09:17 UTC): @kaxil @ashb would also like to confirm whether we're still allowing users to use models in airflow 3.0? If not, should we just skip all the changes related to models. Thanks

uranusjr on (2024-11-19 11:37:27 UTC): I tried my hands on implementing a rule in Ruff. This one checks if a DAG uses the `schedule` argument explicitly, and errors if there’s no such argument (i.e. user is relying on the implicit default, which changes in 3.0), or a deprecated argument is used.

Does this look reasonable enough for people to build on? I’ll produce a more detailed writeup of what to do if we feel this is the way to go.

https://github.com/uranusjr/ruff/pull/1/files

ashb on (2024-11-19 12:46:35 UTC): Which models? But no, the plan is to not have/""allow"" users to import anything from airflow.models at all. Exact details and new names are to be determined yet though

kaxil on (2024-11-19 13:00:28 UTC): https://github.com/apache/airflow/pull/41390
 https://github.com/apache/airflow/pull/41393
 https://github.com/apache/airflow/pull/41390

Duplicate entries for SubDAGs

Lee-W (Issue Creator) on (2024-11-19 13:07:13 UTC): oops, just fixed!

Lee-W (Issue Creator) on (2024-11-19 13:08:36 UTC): Pretty much every model 👀 Sounds good. Just want to confirm I'm not misunderstanding anything. I'll just mark it as model change and not going to migrate for now till we have anything decided

kaxil on (2024-11-19 13:10:24 UTC): @Lee-W There are few rules that we should add for Airflow configs too since we changed /removed them:

- https://github.com/apache/airflow/pull/43975
- https://github.com/apache/airflow/pull/43905

and some imports which won't work:

- https://github.com/apache/airflow/pull/41395
- https://github.com/apache/airflow/pull/41391
- https://github.com/apache/airflow/pull/43289
- https://github.com/apache/airflow/pull/43533
- https://github.com/apache/airflow/pull/43562
- https://github.com/apache/airflow/pull/43530

Lee-W (Issue Creator) on (2024-11-19 13:15:06 UTC): Thanks for reminding me! I'm still halfway to completing reading all the PRs. Will continue work on updating the list

vikramkoka on (2024-11-19 18:21:17 UTC): Awesome @Lee-W , I hadn't seen this issue, so great to see the progress here. 
Following up on the action item from the last dev call, I created this page on Confluence as a draft 
https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+3+breaking+changes

Lee-W (Issue Creator) on (2024-11-20 02:33:21 UTC): Looks great! I'll try to update both places

uranusjr on (2024-11-20 06:27:59 UTC): I also played around the fixer implementation a bit: https://github.com/uranusjr/airflow2to3

Still a lot of room for improvement, but I think it is a good starting point.

FWIW I also tried to create a Flake8 plugin for comparison, but got stuck resolving imports (we need to check whether a thing is actually from Airflow first to decide whether to emit errors for it). Flake8 does not seem to provide this out of the box (Ruff and LibCST each has an easy solution). I’m pretty sure there must be a solution, but the effort looking into this is probably not worthwhile unless we feel Ruff is too big a hurdle.

uranusjr on (2024-11-20 06:36:32 UTC): So what do people think about using Ruff for linting? This is somewhat important since we want to encourage community help to implement rules. If people don’t have strong opinions, I’ll start a lazy consensus thread in the mailing list to get a resolution.

Lee-W (Issue Creator) on (2024-11-21 09:05:14 UTC): Finally finishing checking all the significant before #44080 (except for the one @sunank200 's not working on). 

I tried my best to list down the migration as possible, but it would be nice if folks can take another look to see if I miss anything. Thanks!

* @jscheffl
    * [x] #40029
    * [x] #41733
    * [x] #41735
    * [x] #41736
    * [x] #41737
    * [x] #41739
    * [x] #41761
    * [x] #41762
    * [x] #41774
    * [x] #41776
    * [x] #41778
    * [x] #41779
    * [x] #41780
    * [x] #41808
    * [ ] #43612
    * [x] #41758
    * [ ] #43611
* @dirrao 
    * [ ] #40931
    * [ ] #41096
    * [ ] #41539
    * [ ] #41496
    * [ ] #41550
    * [ ] #41552
    * [ ] #41579
    * [ ] #41609
    * [ ] #41635
    * [ ] #41642
    * [ ] #41663
    * [ ] #41693
    * [ ] #41708
    * [ ] #41748
    * [ ] #41784
    * [ ] #41910
    * [ ] #42060
    * [ ] #42088
    * [ ] #42100
    * [ ] #42126
    * [ ] #42129
    * [ ] #43096
    * [ ] #42647
    * [ ] #42660
    * [ ] #41496
    * [ ] #41975
    * [ ] #42776
* @uranusjr 
    * [ ] #42054
    * [ ] #24842 / #41453
    * [ ] #43915
    * [ ] #41453
    * [ ] #42343
    * [ ] #42436
* @gopidesupavan
    * [x] #41533
* @jedcunningham
    * [ ] #44080
* @ashb
    * [ ] #43943
* @vincbeck
    * [x] #43096
    * [x] #41434
    * [ ] #42042
    * [x] #42280
* @bugraoz93
    * [ ] #43102
* @dstandish
    * [ ] #43183
    * [ ] #42548
    * [ ] #43067
    * [ ] #42953
* @Avihais12344
    * [ ] #41420 / #41695
* @shahar1
    * [ ] #43368
* @potiuk
    * [ ] #42137
    * [ ] #43568
* @kaxil 
    * [ ] #41390
    * [ ] #41393
    * [ ] #43975
    * [ ] #43530
    * [ ] #43533
    * [ ] #43562
    * [ ] #43490
    * [ ] #43291
    * [ ] #43289
* @dstandish 
    * [ ] #41440
* @pierrejeambrun 
    * [ ] #41857
* @jedcunningham 
    * [ ] #41964

bugraoz93 on (2024-11-22 02:49:49 UTC): Thanks for preparing this Lee-W!

For https://github.com/apache/airflow/pull/43102, we can only detect the changes with `Ruff` if users send requests to `PATCH` API endpoints from the Airflow code. 
@pierrejeambrun What do you think?

Lee-W (Issue Creator) on (2024-11-22 02:58:04 UTC): But one of the questions is how we will detect it 🤔 there are plenty of ways to send a request to an API.

bugraoz93 on (2024-11-22 03:12:02 UTC): There is no straightforward way to do it. Indeed, it can be even a standalone application communicating with the Airflow API without other interactions. It could be even in different programming languages such as `go` or scheduled/management `bash` scripts in CI.

vincbeck on (2024-11-22 14:38:05 UTC): I am not sure #42042. I moved out one property from `BaseUser` but I dont users are using this class directly.

potiuk on (2024-11-22 22:52:48 UTC): I think we can only do it reasonably well if we assume the user uses Python Cliant of ours and then we should be able to say the users they could run their custom code through Ruff with the new python client installed to detect wrong parameters. Not sure if we need to have custom ruff rules for those changes, or maybe it's ""mypy"" kind of check for types ? I know astral works on a `mypy` replacement as well, so there is a chance that we will get mypy checks from Astral befor we publish the tool (or we could use mypy for now if needed.  Some quick check on new/old client with some test code for that might be useful.

For the rest of the clients, I think what we could also do - potentially - is to have a custom error code in fast API - where we wil handle errors generated when ""known old-style requests"" are issued to the new API and return pretty descriptive error "" You are using old-style API ble, ble, ble ... you need to change your paremeters to .... ."" - maybe we can generalize it in our API in the way to handle ""typical"" mistakes from multiple APIs by the same error handler?

bugraoz93 on (2024-11-23 17:04:50 UTC): I agree, we should make this assumption and limit the check to a reasonable scope. I like the idea of restricting it so that only our Python Client will be affected. Otherwise, it could turn into a project of its own. :)


I was considering a similar approach, returning an error response if the old request is provided but I wasn’t entirely sure about the scope. If the goal is to catch these issues before upgrading the version, I am unsure how we can easily provide that. Simply reading the API changes documentation seems easier than creating a transition API and asking users to route calls through it to catch errors. Otherwise, such responses would indicate that their processes have already failed with an error. 
If the goal is also to warn users after upgrading the version, then this is the way to go for me too.
I am just trying to better understand the scope of when and how we want to warn users.

potiuk on (2024-11-24 02:03:15 UTC): about the scope.

Yeah. Maybe we can do something in Airlfow 2.11 ? Since our goal is that 2.11 should be the ""bridge"" release - maybe we could do a variation of my original proposal - see if the message coming is in ""old"" format and raise a deprecation and also manually implement ""new"" format there (that would likely require some manual modificaiton of the openapi specification there and some conditional code in 2.11 (if possible). 

That could follow our pattern of ""Make sure 2.11 raises no warnings and then migration to 3 should be smooth"".

pierrejeambrun on (2024-11-25 15:26:24 UTC): Indeed upgrading the client will automatically highlights type errors in users code.

For people bypassing the python client and making direct request to the API (or any other non-python system), we can indeed catch errors of such breaking change and return a clear message that this is not accepted anymore, and maybe even better give them the new way of how to achieve this. It's just more work but possible.

Otherwise reading the significant newsfragment for the `RestAPI` would be a good start when migrating their API code.

potiuk on (2024-11-26 01:54:30 UTC): Just to repeat above - yes. I think it's more work, and I think it might require some nasty workarounds in FAST API that we will have to keep forever, but **maybe** we can do a 2.11-only change that will raise warnings if the old way is used instead (and allow to use new way) ? Not sure if possible and how many such breaking channges we will have, but it would really be nice to tell the users ""if you have no warnings on 2.11, you are good to go"".

Lee-W (Issue Creator) on (2024-11-26 03:52:26 UTC): After reading it again, I don't think users are using it either. 🤔 Then I'll just remove it. Thanks for checking it!

Lee-W (Issue Creator) on (2024-11-29 07:53:20 UTC): I just tried to follow the API discussion 🙌 so we're now doing 

1. warning if they're using airflow python client
2. return an error and guide in FastAPI

but wouldn't it be easier for us to do only the second one and solve it all at once?

Should we trace those API changes only in https://github.com/apache/airflow/issues/43378?

Lee-W (Issue Creator) on (2024-11-29 10:26:42 UTC): Hi @kaxil ,

would like to confirm with you on the following rules. Does it make sense for us to block import from `airflow.executors.*` and `airflow.hook.*`? (related PRs https://github.com/apache/airflow/pull/43289,  https://github.com/apache/airflow/pull/43291

kaxil on (2024-11-29 12:23:34 UTC): No, no. We need to block a user from passing `executor`, `operator`, `sensors` and `hooks` when they inherit `AirflowPlugin`.

```py
class AirflowTestPlugin(AirflowPlugin):
    name = ""test_plugin""
    # --- Invalid now
    operators = [PluginOperator]
    sensors = [PluginSensorOperator]
    hooks = [PluginHook]
    executors = [PluginExecutor]
    # --- Invalid now ^^^
    macros = [plugin_macro]
    flask_blueprints = [bp]
    appbuilder_views = [v_appbuilder_package]
    appbuilder_menu_items = [appbuilder_mitem, appbuilder_mitem_toplevel]
    global_operator_extra_links = [
        AirflowLink(),
        GithubLink(),
    ]
    operator_extra_links = [GoogleLink(), AirflowLink2(), CustomOpLink(), CustomBaseIndexOpLink(1)]
    timetables = [CustomCronDataIntervalTimetable]
    listeners = [empty_listener, ClassBasedListener()]
    ti_deps = [CustomTestTriggerRule()]
    priority_weight_strategies = [CustomPriorityWeightStrategy]
```

Ref: 
- https://airflow.apache.org/docs/apache-airflow/1.10.15/plugins.html
- https://airflow.apache.org/docs/apache-airflow/2.10.3/authoring-and-scheduling/plugins.html

Lee-W (Issue Creator) on (2024-11-29 16:47:07 UTC): @uranusjr and I previously discussed how to handle configuration migration. We believe it might be better to manage this process within Airflow itself, rather than using ruff. However, for other code-related changes, we will continue to leverage ruff.

Today, @sunank200 and I spent a significant amount of time listing all the rules we could think of for the important files before #44040. We also noticed that the standard provider was not included in the news fragment. As a result, I created this https://github.com/apache/airflow/issues/44482.

TLDR, we're spliting the list into 

* airflow config
    * removal
    * rename
* ruff
    * AIR301: Avoid implicit DAG schedule https://github.com/astral-sh/ruff/pull/14581
    * AIR302: removal or no direct mapping
    * AIR303: rename
    * AIR304: moved to provider
    * *AIR305: models related changes (will need to wait for AIP-72 to see what we're going to do for this)*

note that except for AIR301 (and probably AIR302?), other numbers are not confirmed. just something we're considering and will need to dicuss with the ruff team. 

---

in this full list, #41348 is treated as a special category as it's easier for me to trace.

<details>
<summary>the full list</summary>
## airflow config

### Removal
* `smtp`
    * `smtp_user` (from #41539)
    * `smtp_password` (from #41539)
* `webserver`
    * `allow_raw_html_descriptions` (from #40029)
    * `session_lifetime_days` (from #41550) → use `session_lifetime_minutes`
    * `force_log_out_after` (from #41550) → use `session_lifetime_minutes`
* `scheduler`
    * `dependency_detector` (from #41609)
* `operators`
    * `ALLOW_ILLEGAL_ARGUMENTS` (from #41761)
* `metrics`
    * `metrics_use_pattern_match` (from #41975)
    * `timer_unit_consistency` (from #43975)
* `celery`
    * `stalled_task_timeout` (from #42060)
* `core`
    * `check_slas` (from #42285)
    * `strict_dataset_uri_validation` (from #43915)
* `logging`
    * `enable_task_context_logger` (from #43183)
* `traces`
    * `otel_task_log_event` (from #43943)

### Rename
* `scheduler`
    * `processor_poll_interval` →  `cheduler_idle_sleep_time` (from #41096)
* `metrics`
    * `statsd_allow_list` → `metrics_allow_list` (from #42088)
    * `statsd_block_list` → `metrics_block_list` (from #42088)
* cross section
    * `scheduler.statsd_on` → `metrics.statsd_on` (from #42088)
    * `scheduler.statsd_host` → `metrics.statsd_host` (from #42088)
    * `scheduler.statsd_port` → `metrics.statsd_port` (from #42088)
    * `scheduler.statsd_prefix` → `metrics.statsd_prefix` (from #42088)
    * `scheduler.statsd_allow_list` → `metrics.statsd_allow_list` (from #42088)
    * `scheduler.stat_name_handler` → `metrics.stat_name_handler` (from #42088)
    * `scheduler.statsd_datadog_enabled` → `metrics.statsd_datadog_enabled` (from #42088)
    * `scheduler.statsd_datadog_tags` → `metrics.statsd_datadog_tags` (from #42088)
    * `scheduler.statsd_datadog_metrics_tags` → `metrics.statsd_datadog_metrics_tags` (from #42088)
    * `scheduler.statsd_custom_client_path` → 
    * `core.sql_alchemy_conn` → `database.sql_alchemy_conn` (from #42126)
    * `core.sql_engine_encoding` → `database.sql_engine_encoding` (from #42126)
    * `core.sql_engine_collation_for_ids` → `database.sql_engine_collation_for_ids` (from #42126)
    * `core.sql_alchemy_pool_enabled` → `database.sql_alchemy_pool_enabled` (from #42126)
    * `core.sql_alchemy_pool_size` → `database.sql_alchemy_pool_size` (from #42126)
    * `core.sql_alchemy_max_overflow` → `database.sql_alchemy_max_overflow` (from #42126)
    * `core.sql_alchemy_pool_recycle` → `database.sql_alchemy_pool_recycle` (from #42126)
    * `core.sql_alchemy_pool_pre_ping` → `database.sql_alchemy_pool_pre_ping` (from #42126)
    * `core.sql_alchemy_schema` → `database.sql_alchemy_schema` (from #42126)
    * `core.sql_alchemy_connect_args` → `database.sql_alchemy_connect_args` (from #42126)
    * `core.load_default_connections` → `database.load_default_connections` (from #42126)
    *  `core.max_db_retries` → `database.max_db_retries` (from #42126)
    * `core.worker_precheck` → `celery.worker_precheck` (from #42129)
    * `scheduler.max_threads` → `scheduler.parsing_processes` (from #42129)
    * `celery.default_queue` → `operators.default_queue` (from #42129)
    * `admin.hide_sensitive_variable_fields` → `core.hide_sensitive_var_conn_fields` (from #42129)
    * `admin.sensitive_variable_fields` → `core.sensitive_var_conn_names` (from #42129)
    * `core.non_pooled_task_slot_count` → `core.default_pool_task_slot_count` (from #42129)
    * `core.dag_concurrency` → `core.max_active_tasks_per_dag` (from #42129)
    * `api.access_control_allow_origin` → `api.access_control_allow_origins` (from #42129)
    * `api.auth_backend` → `api.auth_backends` (from #42129)
    * `scheduler.deactivate_stale_dags_interval` → `scheduler.parsing_cleanup_interval` (from #42129)
    * `kubernetes_executor.worker_pods_pending_timeout_check_interval` → `scheduler.task_queued_timeout_check_interval` (from #42129)
    * `webserver.update_fab_perms` → `fab.update_fab_perms` (from #42129)
    * `webserver.auth_rate_limited` → `fab.auth_rate_limited` (from #42129)
    * `webserver.auth_rate_limit` → `fab.auth_rate_limit` (from #42129)
* `policy` → `task_policy` (from #41550) 
* `kubernetes` → `kubernetes_executor` (from #42129)

## Ruff

### AIR302: removal

#### package
* `airflow.contrib.*` (from #41366)

#### module
* `airflow.operators.subdag` (from #41390)
* `airflow.kubernetes.*` (from #41735) → use ` airflow.providers.cncf.kubernetes`

#### class
* `airflow.triggers.external_task.TaskStateTrigger` (from #41737)
* `airflow.metrics.validators.AllowListValidator` (from #41975) → use `airflow.metrics.validators.PatternAllowListValidator`
* `airflow.metrics.validators.BlockListValidator` (from #41975) → use `airflow.metrics.validators.PatternBlockListValidator`
* `airflow.metrics.validators.BlockListValidator` → suggest using `airflow.metrics.validators.PatternBlockListValidator` (not direct mapping)

#### function
* `airflow.utils.file.TemporaryDirectory` (from #41395)
* `airflow.utils.file.mkdirs` (from #41395)
* `airflow.utils.dates.date_range` (from #41496)
* `airflow.utils.dates.days_ago` (from #41496) → change it to `pendulum.today('UTC').add(days=-N, ...)`
* `airflow.utils.decorators.apply_defaults` (from #41579)
* `airflow.hooks.base.BaseHook.get_connections` (from #41733) → use `get_connection`
* `airflow.www.auth.has_access` (from #41758) → use `airflow.www.auth.has_access_*`
* `airflow.api_connexion.security.requires_access` → use `requires_access_*` (from #41910)
* `airflow.utils.dates.parse_execution_date` (from #43533)
* `airflow.utils.dates.round_time` (from #43533)
* `airflow.utils.dates.scale_time_units` (from #43533)
* `airflow.utils.dates.infer_time_unit` (from #43533)

#### constant / variable
* `airflow.utils.dag_cycle_tester.test_cycle` (from #41395)
* `airflow.utils.state.SHUTDOWN` (from #41395)
* `airflow.utils.state.terminating_states` (from #41395)
* `airflow.PY\d\d` (from #43562)

#### attribute
* in `airflow.utils.trigger_rule.TriggerRule`
    * `DUMMY` (from #41761)
    * `NONE_FAILED_OR_SKIPPED` (from #41761)
* inherit from `airflow.plugins_manager.AirflowPlugin`
    * `executors` (from #43289)
    * `hooks` (from #43291)
    * `operators`
    * `sensors`

#### parameter
* in `DAG`
    * `schedule_interval` (from #41453)
    * `timetable` (from #41453)
    * `sla_miss_callback` (from #42285)
* in `BaseOperator`
    * `sla` (from #42285)
* in `airflow.utils.log.file_task_handler.FileTaskHandler`
    * `filename_template` (#41552)
* in `airflow.operators.trigger_dagrun.TriggerDagRunOperator`
    * `execution_date` (from #41736)

#### context key
* `execution_date` (from #43902 https://github.com/apache/airflow/issues/44409)
* `next_ds` (from #43902 https://github.com/apache/airflow/issues/44409)
* `next_ds_nodash` (from #43902 https://github.com/apache/airflow/issues/44409)
* `next_execution_date` (from #43902 https://github.com/apache/airflow/issues/44409)
* `prev_ds` (from #43902 https://github.com/apache/airflow/issues/44409)
* `prev_ds_nodash` (from #43902 https://github.com/apache/airflow/issues/44409)
* `prev_execution_date` (from #43902 https://github.com/apache/airflow/issues/44409)
* `prev_execution_date_success` (from #43902 https://github.com/apache/airflow/issues/44409)
* `tomorrow_ds` (from #43902 https://github.com/apache/airflow/issues/44409)
* `yesterday_ds` (from #43902 https://github.com/apache/airflow/issues/44409)
* `yesterday_ds_nodash` (from #43902 https://github.com/apache/airflow/issues/44409)


### AIR303: rename
#### #41348
* module `airflow.datasets` → `airflow.sdk.definitions.asset`
    * class
        * `DatasetAlias` → `AssetAlias`
        * `DatasetAll` → `AssetAll`
        * `DatasetAny` → `AssetAny`
    * function
        * `expand_alias_to_datasets` → `expand_alias_to_assets`
    * class `DatasetAliasEvent` → `AssetAliasEvent`
        * attribute `dest_dataset_uri` → `BaseAsset`
    * class
        * `BaseDataset` → `BaseAsset`
        * `Dataset` → `Asset`
        * method
            * `iter_datasets` → `iter_assets`
            * `iter_dataset_aliases` → `iter_asset_aliases`
* module `airflow.datasets.manager` → `airflow.assets.manager`
    * variable `dataset_manager` → `asset_manager`
    * function `resolve_dataset_manager` → `resolve_asset_manager`
    * class `DatasetManager` → `AssetManager`
        * method
            * `register_dataset_change` → `register_asset_change`
            * `create_datasets` → `create_assets`
            * `register_dataset_change` → `notify_asset_created`
            * `notify_dataset_changed` → `notify_asset_changed`
            * `notify_dataset_alias_created` → `notify_asset_alias_created`
* module `airflow.listeners.spec.dataset` → `airflow.listeners.spec.asset`
    * function
        * `on_dataset_created` → `on_asset_created`
        * `on_dataset_changed` → `on_asset_changed`
* module `airflow.timetables.datasets` → `airflow.timetables.assets`
    * class `DatasetOrTimeSchedule` → `AssetOrTimeSchedule`
* class `airflow.lineage.hook.DatasetLineageInfo`  → `airflow.lineage.hook.AssetLineageInfo`
    * attribute `dataset` → `asset`
* package `airflow.providers.amazon.aws.datasets`  → `airflow.providers.amazon.aws.assets`
    * in module `s3`
        * method `create_dataset` → `create_asset`
        * method `convert_dataset_to_openlineage` → `convert_asset_to_openlineage`
* package `airflow.providers.common.io.datasets` → `airflow.providers.common.io.assets`
    * in module `file`
        * method `create_dataset` → `create_asset`
        * method `convert_dataset_to_openlineage` → `convert_asset_to_openlineage`
* package
    * `airflow.providers.postgres.datasets` → `airflow.providers.postgres.assets`
    * `airflow.providers.mysql.datasets` → `airflow.providers.mysql.assets`
    * `airflow.providers.trino.datasets` → `airflow.providers.trino.assets`
* module
    * `airflow.datasets.metadata` → `airflow.sdk.definitions.asset.metadata`
* class
    * `airflow.timetables.datasets.DatasetOrTimeSchedule` → `airflow.timetables.assets.AssetOrTimeSchedule`
    * `airflow.auth.managers.models.resource_details.DatasetDetails` → `airflow.auth.managers.models.resource_details.AssetDetails`
    * `airflow.timetables.simple.DatasetTriggeredTimetable` → `airflow.timetables.simple.AssetTriggeredTimetable`
    * `airflow.providers.openlineage.utils.utils.DatasetInfo` → `airflow.providers.openlineage.utils.utils.AssetInfo`
* method
    * `airflow.providers.amazon.auth_manager.aws_auth_manager.AwsAuthManager.is_authorized_dataset` → `airflow.providers.amazon.auth_manager.aws_auth_manager.AwsAuthManager.is_authorized_asset`
    * `airflow.lineage.hook.HookLineageCollector.create_dataset` → `airflow.lineage.hook.HookLineageCollector.create_asset`
    * `airflow.lineage.hook.HookLineageCollector.add_input_dataset` → `airflow.lineage.hook.HookLineageCollector.add_input_asset`
    * `airflow.lineage.hook.HookLineageCollector.add_output_dataset` → `airflow.lineage.hook.HookLineageCollector.dd_output_asset`
    * `airflow.lineage.hook.HookLineageCollector.collected_datasets` → `airflow.lineage.hook.HookLineageCollector.collected_assets`
    * `airflow.providers_manager.ProvidersManager.initialize_providers_dataset_uri_resources` → `airflow.providers_manager.ProvidersManager.initialize_providers_asset_uri_resources`
* function
    * `airflow.api_connexion.security.requires_access_dataset` → `airflow.api_connexion.security.requires_access_dataset.requires_access_asset`
    * `airflow.auth.managers.base_auth_manager.is_authorized_dataset` → `airflow.auth.managers.base_auth_manager.is_authorized_asset`
    * `airflow.www.auth.has_access_dataset` → `airflow.www.auth.has_access_dataset.has_access_asset`
    * `airflow.providers.fab.auth_manager.fab_auth_manager.is_authorized_dataset` → `airflow.providers.fab.auth_manager.fab_auth_manager.is_authorized_asset`
    * `airflow.providers.openlineage.utils.utils.translate_airflow_dataset` → `airflow.providers.openlineage.utils.utils.translate_airflow_asset`
* property
    * `airflow.providers_manager.ProvidersManager.dataset_factories` → `airflow.providers_manager.ProvidersManager.asset_factories`
    * `airflow.providers_manager.ProvidersManager.dataset_uri_handlers` → `airflow.providers_manager.ProvidersManager.asset_uri_handlers`
    * `airflow.providers_manager.ProvidersManager.dataset_to_openlineage_converters` → `airflow.providers_manager.ProvidersManager.asset_to_openlineage_converters`
* constant / variable
    * `airflow.security.permissions.RESOURCE_DATASET` → `airflow.security.permissions.RESOURCE_ASSET`
    * `airflow.providers.amazon.auth_manager.avp.entities.AvpEntities.DATASET` → `airflow.providers.amazon.auth_manager.avp.entities.AvpEntities.ASSET`
* context key
    * `triggering_dataset_events` → `triggering_asset_events`
* resource key
    * `dataset-uris` → `asset-uris` (for providers amazon, common.io, mysql, fab, postgres, trino)


#### class
* `airflow.models.ImportError` → `airflow.models.errors.ParseImportError` (from #41367)
* `airflow.models.taskMixin.TaskMixin` → `airflow.models.taskMixin.DependencyMixin` (from #41394)
* `airflow.sensors.external_task.ExternalTaskSensorLink` → `airflow.sensors.external_task.ExternalDagLin` (from #41391)
* `airflow.operators.local_kubernetes_executor.BashOperator` → `airflow.operators.bash.BashOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.BaseBranchOperator` → `airflow.operators.branch.BaseBranchOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.EmptyOperator` → `airflow.operators.empty.EmptyOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.DummyOperator` → `airflow.operators.empty.EmptyOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.EmptyOperator` → `airflow.operators.empty.EmptyOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.DummyOperator` → `airflow.operators.empty.EmptyOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.EmailOperator` → `airflow.operators.email.EmailOperator` (from #41368)
* `airflow.sensors.local_kubernetes_executor.BaseSensorOperator` → `airflow.sensors.base.BaseSensorOperator` (from #41368)
* `airflow.sensors.local_kubernetes_executor.DateTimeSensor` → `airflow.sensors.date_time.DateTimeSensor` (from #41368)
* `airflow.sensors.local_kubernetes_executor.ExternalTaskMarker` → `airflow.sensors.external_task.ExternalTaskMarker` (from #41368)
* `airflow.sensors.local_kubernetes_executor.ExternalTaskSensor` → `airflow.sensors.external_task.ExternalTaskSensor` (from #41368)
* `airflow.sensors.local_kubernetes_executor.ExternalTaskSensorLink` → `airflow.sensors.external_task.ExternalTaskSensorLink` (from #41368)
* `airflow.sensors.local_kubernetes_executor.TimeDeltaSensor` → `airflow.sensors.time_delta.TimeDeltaSensor` (from #41368)

#### function
* `airflow.utils.helpers.chain` → `airflow.models.baseoperator.chain` (from #41520)
* `airflow.utils.helpers.chain` → `airflow.models.baseoperator.cross_downstream` (from #41520)
* `airflow.secrets.local_filesystem.load_connections` → `airflow.secrets.local_filesystem.load_connections_dict` (from #41533)
* `airflow.secrets.local_filesystem.get_connection` → `airflow.secrets.local_filesystem.load_connections_dict` (from #41533)
* `airflow.secrets.base_secrets.BaseSecretsBackend.get_conn_uri` → `airflow.secrets.base_secrets.BaseSecretsBackend.get_conn_value` (from #41642)
* `airflow.secrets.base_secrets.BaseSecretsBackend.get_connections` → `airflow.secrets.base_secrets.BaseSecretsBackend.get_connection` (from #41642)
* `airflow.www.utils.get_sensitive_variables_fields` → `airflow.utils.log.secrets_masker.get_sensitive_variables_fields` (from #41758)
* `airflow.www.utils.should_hide_value_for_key` → `airflow.utils.log.secrets_masker.should_hide_value_for_key` (from #41758)
* `airflow.configuration.get` → `airflow.configuration.conf.get` (from  #43530)
* `airflow.configuration.getboolean` → `airflow.configuration.conf.getboolean` (from  #43530)
* `airflow.configuration.getfloat` → `airflow.configuration.conf.getfloat` (from  #43530)
* `airflow.configuration.getint` → `airflow.configuration.conf.getint` (from  #43530)
* `airflow.configuration.has_option` → `airflow.configuration.conf.has_option` (from  #43530)
* `airflow.configuration.remove_option` → `airflow.configuration.conf.remove_option` (from  #43530)
* `airflow.configuration.as_dict` → `airflow.configuration.conf.as_dict` (from  #43530)
* `airflow.configuration.set` → `airflow.configuration.conf.set` (from  #43530)

#### parameter
* in `DayOfWeekSensor.__init__`
    * `use_task_execution_day` → `use_task_execution_day` (from #41393)
* in `airflow.operators.datetime.BranchDateTimeOperator`
    * `use_task_execution_day` → `use_task_logical_date` (from #41736)
* in `airflow.operators.weekday.BranchDayOfWeekOperator`
    * `use_task_execution_day` → `use_task_logical_date` (from #41736)
* in `BaseOperator`
    * `task_concurrency` → `max_active_tis_per_dag`  (from #41761) https://github.com/astral-sh/ruff/pull/14616

### AIR304: moved to provider

#### module
* `airflow.hooks.dbapi` → `airflow.providers.common.sql.hooks.sql` (from #41748)
* `airflow.api.auth.backend.default` → ` airflow.providers.fab.auth_manager.api.auth.backend.session` (from #43096)

#### class
* `airflow.www.security.FabAirflowSecurityManagerOverride` → `airflow.providers.fab.auth_manager.security_manager.override.FabAirflowSecurityManagerOverride` (from #41758)
* `airflow.executors.local_kubernetes_executor.CeleryExecutor` → `airflow.providers.celery.executors.celery_executor.CeleryExecutor` (from #41368)
* `airflow.executors.local_kubernetes_executor.CeleryKubernetesExecutor` → `airflow.providers.celery.executors.celery_kubernetes_executor.CeleryKubernetesExecutor` (from #41368)
* `airflow.executors.local_kubernetes_executor.DaskExecutor` → `airflow.providers.daskexecutor.executors.dask_executor.DaskExecutor` (from #41368)
* `airflow.executors.local_kubernetes_executor.KubernetesExecutor` → `airflow.providers.cncf.kubernetes.executors.kubernetes_executor.KubernetesExecutor` (from #41368)
* `airflow.executors.local_kubernetes_executor.AirflowKubernetesScheduler` → `airflow.providers.cncf.kubernetes.executors.kubernetes_executor_utils.AirflowKubernetesScheduler` (from #41368)
* `airflow.executors.local_kubernetes_executor.KubernetesJobWatcher` → `airflow.providers.cncf.kubernetes.executors.kubernetes_executor_utils.KubernetesJobWatcher` (from #41368)
* `airflow.executors.local_kubernetes_executor.ResourceVersion` → `airflow.providers.cncf.kubernetes.executors.kubernetes_executor_utils.ResourceVersion` (from #41368)
* `airflow.executors.local_kubernetes_executor.LocalKubernetesExecutor` → `airflow.providers.cncf.kubernetes.executors.LocalKubernetesExecutor` (from #41368)
* `airflow.hooks.local_kubernetes_executor.S3Hook` → `airflow.providers.amazon.aws.hooks.s3.S3Hook` (from #41368)
* `airflow.hooks.local_kubernetes_executor.BaseHook` → `airflow.hooks.base.BaseHook` (from #41368)
* `airflow.hooks.local_kubernetes_executor.DbApiHook` → `airflow.providers.common.sql.hooks.sql.DbApiHook` (from #41368)
* `airflow.hooks.local_kubernetes_executor.DockerHook` → `airflow.providers.docker.hooks.docker.DockerHook` (from #41368)
* `airflow.hooks.local_kubernetes_executor.DruidDbApiHook` → `airflow.providers.apache.druid.hooks.druid.DruidDbApiHook` (from #41368)
* `airflow.hooks.local_kubernetes_executor.DruidHook` → `airflow.providers.apache.druid.hooks.druid.DruidHook` (from #41368)
* `airflow.hooks.local_kubernetes_executor.HiveCliHook` → `airflow.providers.apache.hive.hooks.hive.HiveCliHook` (from #41368)
* `airflow.hooks.local_kubernetes_executor.HiveMetastoreHook` → `airflow.providers.apache.hive.hooks.hive.HiveMetastoreHook` (from #41368)
* `airflow.hooks.local_kubernetes_executor.HiveServer2Hook` → `airflow.providers.apache.hive.hooks.hive.HiveServer2Hook` (from #41368)
* `airflow.hooks.local_kubernetes_executor.HttpHook` → `airflow.providers.http.hooks.http.HttpHook` (from #41368)
* `airflow.hooks.local_kubernetes_executor.JdbcHook` → `airflow.providers.jdbc.hooks.jdbc.JdbcHook` (from #41368)
* `airflow.hooks.local_kubernetes_executor.jaydebeapi` → `airflow.providers.jdbc.hooks.jdbc.jaydebeapi` (from #41368)
* `airflow.hooks.local_kubernetes_executor.MsSqlHook` → `airflow.providers.microsoft.mssql.hooks.mssql.MsSqlHook` (from #41368)
* `airflow.hooks.local_kubernetes_executor.MySqlHook` → `airflow.providers.mysql.hooks.mysql.MySqlHook` (from #41368)
* `airflow.hooks.local_kubernetes_executor.OracleHook` → `airflow.providers.oracle.hooks.oracle.OracleHook` (from #41368)
* `airflow.hooks.local_kubernetes_executor.PigCliHook` → `airflow.providers.apache.pig.hooks.pig.PigCliHook` (from #41368)
* `airflow.hooks.local_kubernetes_executor.PostgresHook` → `airflow.providers.postgres.hooks.postgres.PostgresHook` (from #41368)
* `airflow.hooks.local_kubernetes_executor.PrestoHook` → `airflow.providers.presto.hooks.presto.PrestoHook` (from #41368)
* `airflow.hooks.local_kubernetes_executor.SambaHook` → `airflow.providers.samba.hooks.samba.SambaHook` (from #41368)
* `airflow.hooks.local_kubernetes_executor.SlackHook` → `airflow.providers.slack.hooks.slack.SlackHook` (from #41368)
* `airflow.hooks.local_kubernetes_executor.SqliteHook` → `airflow.providers.sqlite.hooks.sqlite.SqliteHook` (from #41368)
* `airflow.hooks.local_kubernetes_executor.WebHDFSHook` → `airflow.providers.apache.hdfs.hooks.webhdfs.WebHDFSHook` (from #41368)
* `airflow.hooks.local_kubernetes_executor.ZendeskHook` → `airflow.providers.zendesk.hooks.zendesk.ZendeskHook` (from #41368)
* `airflow.operators.local_kubernetes_executor.SQLCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLCheckOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.SQLIntervalCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLIntervalCheckOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.SQLThresholdCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLThresholdCheckOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.SQLValueCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLValueCheckOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.CheckOperator` → `airflow.providers.common.sql.operators.sql.SQLCheckOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.IntervalCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLIntervalCheckOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.ThresholdCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLThresholdCheckOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.ValueCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLValueCheckOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.TriggerDagRunLink` → `airflow.operators.trigger_dagrun.TriggerDagRunLink` (from #41368)
* `airflow.operators.local_kubernetes_executor.TriggerDagRunOperator` → `airflow.operators.trigger_dagrun.TriggerDagRunOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.DockerOperator` → `airflow.providers.docker.operators.docker.DockerOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.DruidCheckOperator` → `airflow.providers.apache.druid.operators.druid_check.DruidCheckOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.GCSToS3Operator` → `airflow.providers.amazon.aws.transfers.gcs_to_s3.GCSToS3Operator` (from #41368)
* `airflow.operators.local_kubernetes_executor.GoogleApiToS3Operator` → `airflow.providers.amazon.aws.transfers.google_api_to_s3.GoogleApiToS3Operator` (from #41368)
* `airflow.operators.local_kubernetes_executor.GoogleApiToS3Transfer` → `airflow.providers.amazon.aws.transfers.google_api_to_s3.GoogleApiToS3Operator` (from #41368)
* `airflow.operators.local_kubernetes_executor.HiveOperator` → `airflow.providers.apache.hive.operators.hive.HiveOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.HiveStatsCollectionOperator` → `airflow.providers.apache.hive.operators.hive_stats.HiveStatsCollectionOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.HiveToDruidOperator` → `airflow.providers.apache.druid.transfers.hive_to_druid.HiveToDruidOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.HiveToDruidTransfer` → `airflow.providers.apache.druid.transfers.hive_to_druid.HiveToDruidOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.HiveToMySqlOperator` → `airflow.providers.apache.hive.transfers.hive_to_mysql.HiveToMySqlOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.HiveToMySqlTransfer` → `airflow.providers.apache.hive.transfers.hive_to_mysql.HiveToMySqlOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.HiveToSambaOperator` → `airflow.providers.apache.hive.transfers.hive_to_samba.HiveToSambaOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.SimpleHttpOperator` → `airflow.providers.http.operators.http.SimpleHttpOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.JdbcOperator` → `airflow.providers.jdbc.operators.jdbc.JdbcOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.LatestOnlyOperator` → `airflow.operators.latest_only.LatestOnlyOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.MsSqlOperator` → `airflow.providers.microsoft.mssql.operators.mssql.MsSqlOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.MsSqlToHiveOperator` → `airflow.providers.apache.hive.transfers.mssql_to_hive.MsSqlToHiveOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.MsSqlToHiveTransfer` → `airflow.providers.apache.hive.transfers.mssql_to_hive.MsSqlToHiveOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.MySqlOperator` → `airflow.providers.mysql.operators.mysql.MySqlOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.MySqlToHiveOperator` → `airflow.providers.apache.hive.transfers.mysql_to_hive.MySqlToHiveOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.MySqlToHiveTransfer` → `airflow.providers.apache.hive.transfers.mysql_to_hive.MySqlToHiveOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.OracleOperator` → `airflow.providers.oracle.operators.oracle.OracleOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.PapermillOperator` → `airflow.providers.papermill.operators.papermill.PapermillOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.PigOperator` → `airflow.providers.apache.pig.operators.pig.PigOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.Mapping` → `airflow.providers.postgres.operators.postgres.Mapping` (from #41368)
* `airflow.operators.local_kubernetes_executor.PostgresOperator` → `airflow.providers.postgres.operators.postgres.PostgresOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.SQLCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLCheckOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.SQLIntervalCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLIntervalCheckOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.SQLValueCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLValueCheckOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.PrestoCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLCheckOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.PrestoIntervalCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLIntervalCheckOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.PrestoValueCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLValueCheckOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.PrestoToMySqlOperator` → `airflow.providers.mysql.transfers.presto_to_mysql.PrestoToMySqlOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.PrestoToMySqlTransfer` → `airflow.providers.mysql.transfers.presto_to_mysql.PrestoToMySqlOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.BranchPythonOperator` → `airflow.operators.python.BranchPythonOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.PythonOperator` → `airflow.operators.python.PythonOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.PythonVirtualenvOperator` → `airflow.operators.python.PythonVirtualenvOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.ShortCircuitOperator` → `airflow.operators.python.ShortCircuitOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.RedshiftToS3Operator` → `airflow.providers.amazon.aws.transfers.redshift_to_s3.RedshiftToS3Operator` (from #41368)
* `airflow.operators.local_kubernetes_executor.RedshiftToS3Transfer` → `airflow.providers.amazon.aws.transfers.redshift_to_s3.RedshiftToS3Operator` (from #41368)
* `airflow.operators.local_kubernetes_executor.S3FileTransformOperator` → `airflow.providers.amazon.aws.operators.s3_file_transform.S3FileTransformOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.S3ToHiveOperator` → `airflow.providers.apache.hive.transfers.s3_to_hive.S3ToHiveOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.S3ToHiveTransfer` → `airflow.providers.apache.hive.transfers.s3_to_hive.S3ToHiveOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.S3ToRedshiftOperator` → `airflow.providers.amazon.aws.transfers.s3_to_redshift.S3ToRedshiftOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.S3ToRedshiftTransfer` → `airflow.providers.amazon.aws.transfers.s3_to_redshift.S3ToRedshiftOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.SlackAPIOperator` → `airflow.providers.slack.operators.slack.SlackAPIOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.SlackAPIPostOperator` → `airflow.providers.slack.operators.slack.SlackAPIPostOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.BaseSQLOperator` → `airflow.providers.common.sql.operators.sql.BaseSQLOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.BranchSQLOperator` → `airflow.providers.common.sql.operators.sql.BranchSQLOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.SQLCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLCheckOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.SQLColumnCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLColumnCheckOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.SQLIntervalCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLIntervalCheckOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.SQLTableCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLTableCheckOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.SQLThresholdCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLThresholdCheckOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.SQLValueCheckOperator` → `airflow.providers.common.sql.operators.sql.SQLValueCheckOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor._convert_to_float_if_possible` → `airflow.providers.common.sql.operators.sql._convert_to_float_if_possible` (from #41368)
* `airflow.operators.local_kubernetes_executor.parse_boolean` → `airflow.providers.common.sql.operators.sql.parse_boolean` (from #41368)
* `airflow.operators.local_kubernetes_executor.BranchSQLOperator` → `airflow.providers.common.sql.operators.sql.BranchSQLOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.BranchSqlOperator` → `airflow.providers.common.sql.operators.sql.BranchSQLOperator` (from #41368)
* `airflow.operators.local_kubernetes_executor.SqliteOperator` → `airflow.providers.sqlite.operators.sqlite.SqliteOperator` (from #41368)
* `airflow.sensors.local_kubernetes_executor.HivePartitionSensor` → `airflow.providers.apache.hive.sensors.hive_partition.HivePartitionSensor` (from #41368)
* `airflow.sensors.local_kubernetes_executor.HttpSensor` → `airflow.providers.http.sensors.http.HttpSensor` (from #41368)
* `airflow.sensors.local_kubernetes_executor.MetastorePartitionSensor` → `airflow.providers.apache.hive.sensors.metastore_partition.MetastorePartitionSensor` (from #41368)
* `airflow.sensors.local_kubernetes_executor.NamedHivePartitionSensor` → `airflow.providers.apache.hive.sensors.named_hive_partition.NamedHivePartitionSensor` (from #41368)
* `airflow.sensors.local_kubernetes_executor.S3KeySensor` → `airflow.providers.amazon.aws.sensors.s3.S3KeySensor` (from #41368)
* `airflow.sensors.local_kubernetes_executor.SqlSensor` → `airflow.providers.common.sql.sensors.sql.SqlSensor` (from #41368)
* `airflow.sensors.local_kubernetes_executor.SqlSensor` → `airflow.providers.common.sql.sensors.sql.SqlSensor` (from #41368)
* `airflow.sensors.local_kubernetes_executor.WebHdfsSensor` → `airflow.providers.apache.hdfs.sensors.web_hdfs.WebHdfsSensor` (from #41368)

#### function
* `airflow.api.auth.backend.basic_auth` → `airflow.providers.fab.auth_manager.api.auth.backend.basic_auth` (from #41663)
* `airflow.api.auth.backend.kerberos_auth` → airflow.executors.`airflow.providers.fab.auth_manager.api.auth.backend.kerberos_auth` (from #41693)
* `airflow.auth.managers.fab.api.auth.backend.kerberos_auth` → `airflow.providers.fab.auth_manager.api.auth.backend.kerberos_auth` (from #41693)
* `airflow.auth.managers.fab.fab_auth_manager` → `airflow.providers.fab.auth_manager.security_manager.override` (from #41708)
* `airflow.auth.managers.fab.security_manager.override` → `airflow.providers.fab.auth_manager.security_manager.override` (from #41708)

#### constant / variable
* `airflow.executors.local_kubernetes_executor.ALL_NAMESPACES` → `airflow.providers.cncf.kubernetes.executors.kubernetes_executor_types.ALL_NAMESPACES` (from #41368)
* `airflow.executors.local_kubernetes_executor.POD_EXECUTOR_DONE_KEY` → `airflow.providers.cncf.kubernetes.executors.kubernetes_executor_types.POD_EXECUTOR_DONE_KEY` (from #41368)
* `airflow.hooks.local_kubernetes_executor.HIVE_QUEUE_PRIORITIES` → `airflow.providers.apache.hive.hooks.hive.HIVE_QUEUE_PRIORITIES` (from #41368)
* `airflow.executors.local_kubernetes_executor.app` → `airflow.providers.celery.executors.celery_executor_utils.app` (from #41368)
* `airflow.macros.local_kubernetes_executor.closest_ds_partition` → `airflow.providers.apache.hive.macros.hive.closest_ds_partition` (from #41368)
* `airflow.macros.local_kubernetes_executor.max_partition` → `airflow.providers.apache.hive.macros.hive.max_partition` (from #41368)

### AIR310: models related changes (AIP-72) not going to do it
* #40931
* #41440
* #41761
    * `airflow.models.baseoperator.BaseOperatorLink` → `airflow.models.baseoperatorlink.BaseOperatorLink`
* #41762
    * `airflow.models.connection.parse_netloc_to_hostname`
    * `airflow.models.connection.Connection.parse_from_uri`
    * `airflow.models.connection.Connection.log_info`
    * `airflow.models.connection.Connection.debug_info`
* #41774
* #41776
* #41778
* #41779
* #41780
* #41784
* #41808
* #41964
* #42023
* #42548
* #42739
* #42776
* #43067
* #43490

------


</details>

sunank200 on (2024-12-13 10:40:49 UTC): Created the PR for `airflow config lint` : https://github.com/apache/airflow/pull/44908

Lee-W (Issue Creator) on (2025-01-02 03:09:22 UTC): Most of the rules have now been added to `ruff` or `airflow config lint.` We currently have two open PRs waiting for the ruff team to review https://github.com/astral-sh/ruff/pull/15144 and https://github.com/astral-sh/ruff/pull/15216. One rule not yet included in the previous PRs is blocked by https://github.com/astral-sh/ruff/pull/15144, but it could be done no longer after https://github.com/astral-sh/ruff/pull/15144 is merged.

jscheffl on (2025-01-02 08:54:31 UTC): @sunank200 / @Lee-W COOL!

potiuk on (2025-01-02 10:26:24 UTC): Nice.  Glad to see Astral team cooperates on it :). 

BTW. Do they have any plans to be able (if possible) to implement some kind of plugins that we would be able to release on our own maybe? I remember that in the past that was a bit problematic because of the way how RUST ABI worked (or so I remember) - but also I think this has been solved already.

While now it can take quite some time for things to iterate and get released in the new ruff version, when Airflow 3 gets released, we will have sometimes likely a quick fix or new rule to be released fairly quickly, and I think it's not a good idea to overburden Astral team with reviews, merges and releases, and it woudl be cool if we could have our own ""plugin"" of sorts that we could release and implement changes on our own.

Has this been discussed or considered at all @Lee-W  ? Should we start such a discussion ?

Lee-W (Issue Creator) on (2025-01-02 12:57:52 UTC): I think it's still an open issue. https://github.com/astral-sh/ruff/issues/283 🤔


It's not yet been discussed. Not sure whether there will be rules really need to be released that quickly since it's not actually breaking Airflow and rules can be ignored 🤔

potiuk on (2025-01-02 14:47:50 UTC): True, that's why I am not **too** worried as this is just a ""supplemental"" code. And yeah - the plugin system is still in discussion i see, and I do not think we have strong enough case to ""badly need"" it - it's more that I generally do not like when ""someone else"" controls some airflow-specific code than Airlfow PMC. And this is not something I have against Astral, not at all, it's just we as PMC do not have final saying there, and someone else can add new rules or change ours - so that's a bit of a danger I see). 

Various scenarios here are possible - and it's just a little bit of an ""itch"" that I wonder if we should ""scratch"".

uranusjr on (2025-01-08 06:38:01 UTC): I was thinking about what exactly is needed for users to migrate. I think we probably should backport `airflow config lint` to 2.11? This way the user can choose to change the configs first before they upgrade Airflow itself.

kaxil on (2025-01-08 11:21:18 UTC): Yeah, agreed - we should

Lee-W (Issue Creator) on (2025-01-14 02:36:48 UTC): https://github.com/apache/airflow/issues/45632

just created an issue for backporting

kaxil on (2025-01-15 21:52:11 UTC): fyi: https://github.com/apache/airflow/pull/45694 contains breaking changes too

eladkal on (2025-01-16 10:04:51 UTC): We need to add rule to help users migrate email from core to SMTP provider
https://github.com/apache/airflow/pull/45705 https://github.com/apache/airflow/pull/46041

Lee-W (Issue Creator) on (2025-01-16 10:38:11 UTC): Yep, I can start the implementation once the change has been merged into main

"
2477774332,issue,open,,Ineffiction dag_run endpoint,"### Description

Hi
v1/dags/~/dagRuns/~/taskInstances?state=running
such endpoint shot. It looks like it is executing queries for all dags and all tasks and on the final step filter state. On database such shot is immediate using state index when through api it takes ages on bigger instance.

### Use case/motivation

Passing parameters to database query

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rkozlo,2024-08-21 11:19:09+00:00,[],2025-01-10 12:27:25+00:00,,https://github.com/apache/airflow/issues/41640,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API"")]","[{'comment_id': 2301806615, 'issue_id': 2477774332, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 21, 11, 19, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2582186522, 'issue_id': 2477774332, 'author': 'shahar1', 'body': ""We'll be happy if you could rephrase the suggestion, because it's not as clear."", 'created_at': datetime.datetime(2025, 1, 10, 9, 34, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2582263561, 'issue_id': 2477774332, 'author': 'rkozlo', 'body': 'It is about stage when it filters data. Seems like it sends query to database without filter on state.', 'created_at': datetime.datetime(2025, 1, 10, 10, 4, 46, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-21 11:19:12 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

shahar1 on (2025-01-10 09:34:39 UTC): We'll be happy if you could rephrase the suggestion, because it's not as clear.

rkozlo (Issue Creator) on (2025-01-10 10:04:46 UTC): It is about stage when it filters data. Seems like it sends query to database without filter on state.

"
2477641354,issue,open,,Not working in Chart multi_namespace_mode_namespace_list is not working.,"### Official Helm Chart version

1.15.0 (latest released)

### Apache Airflow version

2.9.3

### Kubernetes Version

Server Version: version.Info{Major:""1"", Minor:""30"", GitVersion:""v1.30.0"", GitCommit:""7c48c2bd72b9bf5c44d21d7338cc7bea77d0ad2a"", GitTreeState:""clean"", BuildDate:""2024-04-17T17:27:03Z"", GoVersion:""go1.22.2"", Compiler:""gc"", Platform:""linux/amd64""}

### Helm Chart configuration

executor: ""CeleryKubernetesExecutor""

multiNamespaceMode: true

env:
  - name: ""AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL""
    value: ""60""
  - name: ""AIRFLOW__KUBERNETES_EXECUTOR__MULTI_NAMESPACE_MODE_NAMESPACE_LIST""
    value: ""airflow,volumes""

### Docker Image customizations

FROM apache/airflow

USER root
RUN apt-get update \
  && apt-get install -y --no-install-recommends \
         vim \
  && apt-get autoremove -yqq --purge \
  && apt-get clean \
  && rm -rf /var/lib/apt/lists/*


USER airflow

## Extends providers
RUN pip install -U --no-cache-dir \
  apache-airflow-providers-mongo \
  clickhouse-driver \
  clickhouse-cityhash lz4 \
  airflow-clickhouse-plugin[common.sql]

## Extends packages
RUN pip install --no-cache-dir pymongo pika requests selenium lxml

### What happened

The configuration for **multi_namespace_mode_namespace_list** should be enabled, but even I've added namespace variables in env, the configuration is just enabling **multi_namespace_mode**, but it's not configuring **multi_namespace_mode_namespace_list**.

```bash
$ airflow config list
...
[kubernetes]
airflow_configmap = airflow-config
airflow_local_settings_configmap = airflow-config
multi_namespace_mode = True
namespace = airflow
pod_template_file = /opt/airflow/pod_templates/pod_template_file.yaml
worker_container_repository = airflow_local
worker_container_tag = 1.0.4

```
 **multi_namespace_mode_namespace_list**  is missing even if I set variable in .yaml

### What you think should happen instead

there should be **multi_namespace_mode_namespace_list** in the [kubernetes] section so we can choose in which namespaces we would like the schedule pods.

### How to reproduce

enable or set AIRFLOW__KUBERNETES_EXECUTOR__MULTI_NAMESPACE_MODE with a list of specific namespaces. Then try to run a KPO inside a not listed namespace. KPO task should fail. It it runs, it's because **multi_namespace_mode_namespace_list** is not applied. 

### Anything else

Thanks you very much, and sorry for my english

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",PavelJacoboGonzalez,2024-08-21 10:13:19+00:00,[],2025-01-10 09:39:36+00:00,,https://github.com/apache/airflow/issues/41636,"[('kind:bug', 'This is a clearly a bug'), ('area:helm-chart', 'Airflow Helm Chart'), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]","[{'comment_id': 2301679807, 'issue_id': 2477641354, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 21, 10, 13, 22, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-21 10:13:22 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2477131533,issue,open,,Helm chart can't remove .securityContexts.runAsUser and securityContexts.fsGroup  for Openshift compatibility,"### Official Helm Chart version

1.15.0 (latest released)

### Apache Airflow version

2.9.3

### Kubernetes Version

v1.26.15+4818370

### Helm Chart configuration

```
securityContexts: {
  pod: {
    runAsUser: 
    fsGroup:  
  }
}
```

### Docker Image customizations

_No response_

### What happened

.securityContexts.pod.runAsUser and securityContexts.pod.fsGroup should allow us to remove the values entirely for openshift compatibility. Bitnami have done the same in their image https://github.com/bitnami/charts/blob/f7bd58e4b2842e0bf1bf2dcd0288beea98dd87a9/bitnami/airflow/values.yaml#L29

This is needed when users like myself do not have cluster admin rights to assign scc's to service accounts that can use any UID and GID. By default Openshift applies the restricted-v2 SCC, which doesn't allow any id for runAsUser and fsGroup.

### What you think should happen instead

Allow .runAsUser and fsGroup values to be omitted or allow .securityContexts block to be removed so Openshifts SCC policy will apply the defaults.

### How to reproduce

In an openshift cluster, try to deploy as a project admin.

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",philthynz,2024-08-21 05:59:57+00:00,[],2024-12-18 07:12:15+00:00,,https://github.com/apache/airflow/issues/41630,"[('good first issue', ''), ('area:helm-chart', 'Airflow Helm Chart')]","[{'comment_id': 2301194334, 'issue_id': 2477131533, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 21, 6, 0, tzinfo=datetime.timezone.utc)}, {'comment_id': 2302089363, 'issue_id': 2477131533, 'author': 'potiuk', 'body': 'Feel free to contribute it - otherwise it will have to wait for someone to pick it up (marked it as good-first-issue since there are some clear examples it can be based on)', 'created_at': datetime.datetime(2024, 8, 21, 13, 39, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2303860807, 'issue_id': 2477131533, 'author': 'eladkal', 'body': 'cc @nevcohen maybe you be intreated in fixing this one?', 'created_at': datetime.datetime(2024, 8, 22, 6, 10, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2303898254, 'issue_id': 2477131533, 'author': 'philthynz', 'body': ""Actually, a good fix would be to just remove the hard coded runAsUser and fsGroup. Openshift by default will apply the restricted-v2 SCC policy, which doesn't allow this template to be deployed. Openshift will add those values.\r\n\r\nSecurity context block would also be good if it can be removed entirely for other clusters that apply SCC rules.\r\n\r\nMaybe a bool and condition for runAsUser and fsGroup that are omitted when openshift=true?"", 'created_at': datetime.datetime(2024, 8, 22, 6, 39, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304699465, 'issue_id': 2477131533, 'author': 'deveshgoyal1000', 'body': ""Hi @philthynz ,\r\n\r\nI hope you're doing well.\r\n\r\nI am working on adapting the Helm chart to support OpenShift compatibility, particularly in relation to runAsUser and fsGroup settings. Based on your suggestion, I plan to make the following changes:\r\n\r\nAdd an openshift flag: This flag will determine whether to adapt the security context for OpenShift.\r\n\r\nConditional Inclusion of securityContext: In the Helm chart templates, I will conditionally include or exclude the securityContext block based on the openshift flag and adaptSecurityContext setting.\r\n\r\nIf openshift=true: The securityContext block will be omitted, allowing OpenShift to apply its default security settings.\r\nFor other Kubernetes environments: The securityContext block will include runAsUser and fsGroup settings if they are specified.\r\nI’d like to confirm if this approach aligns with your expectations and the best practices for handling OpenShift compatibility. Additionally, if there are any other considerations or adjustments you recommend, please let me know.\r\n\r\nThanks"", 'created_at': datetime.datetime(2024, 8, 22, 13, 38, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2305423819, 'issue_id': 2477131533, 'author': 'philthynz', 'body': ""Hi @deveshgoyal1000 ,\r\n\r\nThanks for jumping in. I am well thanks.\r\n\r\nYes your plan will work. I thought it was more simple to allow runAsUser and fsGroup values to be null. Openshift would accept that too. As an example:\r\n\r\n```\r\nsecurityContext: {\r\n  runAsUser: \r\n  fsGroup:  \r\n}\r\n```\r\n\r\nBlank values are accepted in restricted-v2.\r\n\r\nBut you're right. The underlying issue here is that the securityContexts block can't be fully omitted to allow Openshift to apply the default SCC config.\r\n\r\nAs a workaround we are using helm template and then a find and replace to make runAsUser and fsGroup blank.\r\n\r\nDo you mind if I ask your time frame?"", 'created_at': datetime.datetime(2024, 8, 22, 18, 51, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2307194332, 'issue_id': 2477131533, 'author': 'deveshgoyal1000', 'body': 'I can get this done 1-2 if  but which approch should i do making runAsUser and fsGroup values to be null or Conditionally including or excluding the `securityContext` block based on this flag and the `adaptSecurityContext` setting. and updating\r\ndeployment templates with  \r\n```yaml\r\n{{- if and (eq .Values.global.compatibility.openshift.adaptSecurityContext ""auto"") (eq .Values.global.compatibility.openshift.openshift true) }}\r\nsecurityContext: {}\r\n{{- else }}\r\nsecurityContext:\r\n  runAsUser: {{ .Values.containerSecurityContext.runAsUser | default """" }}\r\n  fsGroup: {{ .Values.podSecurityContext.fsGroup | default """" }}\r\n{{- end }}', 'created_at': datetime.datetime(2024, 8, 23, 14, 18, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2307809232, 'issue_id': 2477131533, 'author': 'philthynz', 'body': 'Hey @deveshgoyal1000 many thanks for this.\r\n\r\nI would take the bitnami approach. Because users still may want to specify other attributes in the security context that Openshift would allow.\r\n\r\nSee here: https://blog.bitnami.com/2024/04/bitnami-helm-charts-are-now-more-secure.html?m=1\r\n\r\n![Screenshot_20240824_090230_Chrome](https://github.com/user-attachments/assets/218d91e2-bc48-4395-b25f-f922d1b0a159)\r\n\r\nThe runAsUser, runAsGroup, and fsGroup ahould be automatically removed. I.e, not set.', 'created_at': datetime.datetime(2024, 8, 23, 21, 4, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2308526630, 'issue_id': 2477131533, 'author': 'deveshgoyal1000', 'body': 'Hey @philthynz Thank you for pointing me to the Bitnami approach it looks great and aligns perfectly with our goals. I\'ll proceed with this method to ensure the Helm charts automatically remove runAsUser, runAsGroup, and fsGroup when deployed on OpenShift. \r\nTo ensure I’m on the right track, here’s the updated conditional logic that I’ll be applying : \r\n``` yaml \r\n{{- if and (eq .Values.global.compatibility.openshift.adaptSecurityContext ""auto"") (eq .Values.global.compatibility.openshift.enabled true) }}\r\ncontainerSecurityContext: {}\r\npodSecurityContext: {}\r\n{{- else }}\r\ncontainerSecurityContext:\r\n  runAsUser: {{ .Values.containerSecurityContext.runAsUser | default """" }}\r\n  runAsGroup: {{ .Values.containerSecurityContext.runAsGroup | default """" }}\r\npodSecurityContext:\r\n  fsGroup: {{ .Values.podSecurityContext.fsGroup | default """" }}\r\n{{- end }}\r\n```\r\n\r\nThis approach separates the containerSecurityContext and podSecurityContext, ensuring that OpenShift-specific settings are handled correctly, while also allowing other Kubernetes environments to use the specified values.\r\n\r\nPlease let me know if this aligns with your expectations or if there are any further adjustments needed.\r\nThanks!', 'created_at': datetime.datetime(2024, 8, 24, 20, 20, 43, tzinfo=datetime.timezone.utc)}, {'comment_id': 2308530649, 'issue_id': 2477131533, 'author': 'philthynz', 'body': ""Yea blank values will work. I've tested that. Many thanks."", 'created_at': datetime.datetime(2024, 8, 24, 20, 37, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2308552569, 'issue_id': 2477131533, 'author': 'deveshgoyal1000', 'body': ""Oh, that's nice! Thanks for confirming."", 'created_at': datetime.datetime(2024, 8, 24, 22, 2, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325108688, 'issue_id': 2477131533, 'author': 'Avihais12344', 'body': 'Hello, I saw this conversation, and I saw that now you can go and configure the uids and gid to `""""` in most places I have looked at. For example [at general](https://github.com/apache/airflow/blob/main/chart/values.yaml#L46), [statsd](https://github.com/apache/airflow/blob/main/chart/values.yaml#L2027) and [redis](https://github.com/apache/airflow/blob/main/chart/values.yaml#L2342). So maybe the only thing we need is to add to the docs?', 'created_at': datetime.datetime(2024, 9, 2, 17, 2, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2527290668, 'issue_id': 2477131533, 'author': 'sre42', 'body': 'Hi @deveshgoyal1000 , Do we have a PR in progress for this? We use openshift and are looking forward to these changes being added in 🙂', 'created_at': datetime.datetime(2024, 12, 9, 8, 47, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2550531243, 'issue_id': 2477131533, 'author': 'deveshgoyal1000', 'body': ""Hi yes,\r\n\r\n> Hi @deveshgoyal1000 , Do we have a PR in progress for this? We use openshift and are looking forward to these changes being added in 🙂\r\n\r\nHi, Thanks for the reminder! It seems this slipped past me. I'll start working on it and have a PR ready shortly."", 'created_at': datetime.datetime(2024, 12, 18, 7, 12, 13, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-21 06:00:00 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-08-21 13:39:59 UTC): Feel free to contribute it - otherwise it will have to wait for someone to pick it up (marked it as good-first-issue since there are some clear examples it can be based on)

eladkal on (2024-08-22 06:10:51 UTC): cc @nevcohen maybe you be intreated in fixing this one?

philthynz (Issue Creator) on (2024-08-22 06:39:56 UTC): Actually, a good fix would be to just remove the hard coded runAsUser and fsGroup. Openshift by default will apply the restricted-v2 SCC policy, which doesn't allow this template to be deployed. Openshift will add those values.

Security context block would also be good if it can be removed entirely for other clusters that apply SCC rules.

Maybe a bool and condition for runAsUser and fsGroup that are omitted when openshift=true?

deveshgoyal1000 on (2024-08-22 13:38:14 UTC): Hi @philthynz ,

I hope you're doing well.

I am working on adapting the Helm chart to support OpenShift compatibility, particularly in relation to runAsUser and fsGroup settings. Based on your suggestion, I plan to make the following changes:

Add an openshift flag: This flag will determine whether to adapt the security context for OpenShift.

Conditional Inclusion of securityContext: In the Helm chart templates, I will conditionally include or exclude the securityContext block based on the openshift flag and adaptSecurityContext setting.

If openshift=true: The securityContext block will be omitted, allowing OpenShift to apply its default security settings.
For other Kubernetes environments: The securityContext block will include runAsUser and fsGroup settings if they are specified.
I’d like to confirm if this approach aligns with your expectations and the best practices for handling OpenShift compatibility. Additionally, if there are any other considerations or adjustments you recommend, please let me know.

Thanks

philthynz (Issue Creator) on (2024-08-22 18:51:25 UTC): Hi @deveshgoyal1000 ,

Thanks for jumping in. I am well thanks.

Yes your plan will work. I thought it was more simple to allow runAsUser and fsGroup values to be null. Openshift would accept that too. As an example:

```
securityContext: {
  runAsUser: 
  fsGroup:  
}
```

Blank values are accepted in restricted-v2.

But you're right. The underlying issue here is that the securityContexts block can't be fully omitted to allow Openshift to apply the default SCC config.

As a workaround we are using helm template and then a find and replace to make runAsUser and fsGroup blank.

Do you mind if I ask your time frame?

deveshgoyal1000 on (2024-08-23 14:18:51 UTC): I can get this done 1-2 if  but which approch should i do making runAsUser and fsGroup values to be null or Conditionally including or excluding the `securityContext` block based on this flag and the `adaptSecurityContext` setting. and updating
deployment templates with  
```yaml
{{- if and (eq .Values.global.compatibility.openshift.adaptSecurityContext ""auto"") (eq .Values.global.compatibility.openshift.openshift true) }}
securityContext: {}
{{- else }}
securityContext:
  runAsUser: {{ .Values.containerSecurityContext.runAsUser | default """" }}
  fsGroup: {{ .Values.podSecurityContext.fsGroup | default """" }}
{{- end }}

philthynz (Issue Creator) on (2024-08-23 21:04:49 UTC): Hey @deveshgoyal1000 many thanks for this.

I would take the bitnami approach. Because users still may want to specify other attributes in the security context that Openshift would allow.

See here: https://blog.bitnami.com/2024/04/bitnami-helm-charts-are-now-more-secure.html?m=1

![Screenshot_20240824_090230_Chrome](https://github.com/user-attachments/assets/218d91e2-bc48-4395-b25f-f922d1b0a159)

The runAsUser, runAsGroup, and fsGroup ahould be automatically removed. I.e, not set.

deveshgoyal1000 on (2024-08-24 20:20:43 UTC): Hey @philthynz Thank you for pointing me to the Bitnami approach it looks great and aligns perfectly with our goals. I'll proceed with this method to ensure the Helm charts automatically remove runAsUser, runAsGroup, and fsGroup when deployed on OpenShift. 
To ensure I’m on the right track, here’s the updated conditional logic that I’ll be applying : 
``` yaml 
{{- if and (eq .Values.global.compatibility.openshift.adaptSecurityContext ""auto"") (eq .Values.global.compatibility.openshift.enabled true) }}
containerSecurityContext: {}
podSecurityContext: {}
{{- else }}
containerSecurityContext:
  runAsUser: {{ .Values.containerSecurityContext.runAsUser | default """" }}
  runAsGroup: {{ .Values.containerSecurityContext.runAsGroup | default """" }}
podSecurityContext:
  fsGroup: {{ .Values.podSecurityContext.fsGroup | default """" }}
{{- end }}
```

This approach separates the containerSecurityContext and podSecurityContext, ensuring that OpenShift-specific settings are handled correctly, while also allowing other Kubernetes environments to use the specified values.

Please let me know if this aligns with your expectations or if there are any further adjustments needed.
Thanks!

philthynz (Issue Creator) on (2024-08-24 20:37:55 UTC): Yea blank values will work. I've tested that. Many thanks.

deveshgoyal1000 on (2024-08-24 22:02:34 UTC): Oh, that's nice! Thanks for confirming.

Avihais12344 on (2024-09-02 17:02:40 UTC): Hello, I saw this conversation, and I saw that now you can go and configure the uids and gid to `""""` in most places I have looked at. For example [at general](https://github.com/apache/airflow/blob/main/chart/values.yaml#L46), [statsd](https://github.com/apache/airflow/blob/main/chart/values.yaml#L2027) and [redis](https://github.com/apache/airflow/blob/main/chart/values.yaml#L2342). So maybe the only thing we need is to add to the docs?

sre42 on (2024-12-09 08:47:39 UTC): Hi @deveshgoyal1000 , Do we have a PR in progress for this? We use openshift and are looking forward to these changes being added in 🙂

deveshgoyal1000 on (2024-12-18 07:12:13 UTC): Hi yes,


Hi, Thanks for the reminder! It seems this slipped past me. I'll start working on it and have a PR ready shortly.

"
2476057926,issue,open,,Webserver Graph View Partial Load,"### Description

The Graph view currently load all tasks in the DAG and then allows you to scroll around the graph. This is very inefficient for large DAGs because the user can only actually perceive a small fraction of the DAG at once. This inefficiency can crash the webserver (or require excessive resources) for dags with 5 figure numbers of tasks.

The prior art I would take inspiration from is video games, which do not load the whole world, but instead load only the parts that you are looking at and update that set as you move around. For Airflow, I think we should show only the tasks connected to the task you are currently looking at and the ones connected to those and so on until some limit on the total number of tasks (e.g. 50) is hit. To see the rest of the DAG, click one of the connected tasks and then Airflow should re-center the view on that task, loading in the new tasks and dropping the ones farthest away.

### Use case/motivation

Large DAGs should load intelligently in the Graph view without needing excessive resources on the webserver.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",collinmcnulty,2024-08-20 16:22:53+00:00,[],2025-01-10 08:51:23+00:00,,https://github.com/apache/airflow/issues/41620,"[('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2306333336, 'issue_id': 2476057926, 'author': 'eladkal', 'body': ""> The prior art I would take inspiration from is video games, which do not load the whole world, but instead load only the parts that you are looking at \r\n\r\nIn these games the entry point is very clear. It's your city/town/castle and you can move around the map as everything is fixed.\r\n\r\nHow do you suggest to actually make it with Airflow? Which task should be the first presented? How do you imagine navigation will look like? Do I know what is on the right or left side of this specific task?\r\n\r\nI think 5K tasks for specific dags is really high. Did you pack all of the tasks in nested task groups? That way you can zoom in only on what is relevant for you and possibly we can make the UI to load only the specific task group?"", 'created_at': datetime.datetime(2024, 8, 23, 5, 42, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2307792486, 'issue_id': 2476057926, 'author': 'collinmcnulty', 'body': ""I think the selected task would be the entry point, and for DAGs larger than some configured number of tasks, the Graph view should not load unless a specific task is selected from the Grid view. Or maybe you just pick any random root node and display that one.\r\n\r\nNavigation would be like it is now, that you can pan around the graph, and my clicking on a task it will change the center point, loading the new tasks as required and dropping old ones. So you can traverse the graph by clicking successive tasks to move around the graph.\r\n\r\nI work in support for Astronomer, so I'm speaking on behalf of users I've worked with, not for my own DAGs. I've observed that DAGs that are large and don't make great use of task groups exist in the wild, and there is the problem that on shared Airflow instances, even one DAG like this can start breaking the webserver for everyone. I'm certainly open to other ideas on how to make graph view efficient, but I don't think we can/should rely on DAG authors to keep graph view efficiency in mind when writing DAGs."", 'created_at': datetime.datetime(2024, 8, 23, 20, 50, 9, tzinfo=datetime.timezone.utc)}]","eladkal on (2024-08-23 05:42:58 UTC): In these games the entry point is very clear. It's your city/town/castle and you can move around the map as everything is fixed.

How do you suggest to actually make it with Airflow? Which task should be the first presented? How do you imagine navigation will look like? Do I know what is on the right or left side of this specific task?

I think 5K tasks for specific dags is really high. Did you pack all of the tasks in nested task groups? That way you can zoom in only on what is relevant for you and possibly we can make the UI to load only the specific task group?

collinmcnulty (Issue Creator) on (2024-08-23 20:50:09 UTC): I think the selected task would be the entry point, and for DAGs larger than some configured number of tasks, the Graph view should not load unless a specific task is selected from the Grid view. Or maybe you just pick any random root node and display that one.

Navigation would be like it is now, that you can pan around the graph, and my clicking on a task it will change the center point, loading the new tasks as required and dropping old ones. So you can traverse the graph by clicking successive tasks to move around the graph.

I work in support for Astronomer, so I'm speaking on behalf of users I've worked with, not for my own DAGs. I've observed that DAGs that are large and don't make great use of task groups exist in the wild, and there is the problem that on shared Airflow instances, even one DAG like this can start breaking the webserver for everyone. I'm certainly open to other ideas on how to make graph view efficient, but I don't think we can/should rely on DAG authors to keep graph view efficiency in mind when writing DAGs.

"
2475591062,issue,closed,completed,end_from_trigger leaves deferred tasks without an end_date and increasing duration,"### Apache Airflow version

2.10.0rc1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Using new `end_from_trigger` option leaves successful tasks without an end_date.  This doesn't impact downstream operations, but the ""duration"" for the task in the UI is now considered as from the start of the DagRun to ""now"" and updates with each reload.  This minimizes the utility of the the Gannt Chart and Task Duration views.

### What you think should happen instead?

Tasks that are ended from a Trigger should be updated with a proper end_date in order to properly assess task duration (including deferred time).

### How to reproduce

Defer a task with `end_from_trigger = True`.  After task is completed check task duration in Tree View or Gantt Chart.  Duration updates everytime UI is reloaded and is considered from the the time of the DagRun until now.

### Operating System

All

### Versions of Apache Airflow Providers

Latest release versions.

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

K8S with KubernetesExecutor

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",adrianrego,2024-08-20 13:00:22+00:00,[],2024-08-28 13:08:15+00:00,2024-08-28 13:08:15+00:00,https://github.com/apache/airflow/issues/41613,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('area:async-operators', 'AIP-40: Deferrable (""Async"") Operators'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2307886142, 'issue_id': 2475591062, 'author': 'tirkarthi', 'body': 'cc @sunank200', 'created_at': datetime.datetime(2024, 8, 23, 22, 29, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2309432508, 'issue_id': 2475591062, 'author': 'tirkarthi', 'body': 'state attribute of task_instance is directly set in BaseTaskEndEvent. A possible fix would be to call task_instance.set_state which will set the state and update end_date and duration attributes. I can raise a fix for this.', 'created_at': datetime.datetime(2024, 8, 26, 6, 31, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2310294170, 'issue_id': 2475591062, 'author': 'tirkarthi', 'body': ""@adrianrego  Please review #41754 . I guess you can also inherit `BaseTaskEndEvent` to override `handle_submit` to call set_state as per PR and use it as base class for custom TaskSuccessEvent, TaskFailedEvent etc. though I haven't tested it in case you need the fix before next release of 2.10.1 ."", 'created_at': datetime.datetime(2024, 8, 26, 14, 1, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2310364173, 'issue_id': 2475591062, 'author': 'adrianrego', 'body': ""@tirkarthi Thanks for the quick PR.  I'm in no position to officially review, but the changes look straight-forward and covered by a test-case.  I can wait for this to make it into the next release cycle."", 'created_at': datetime.datetime(2024, 8, 26, 14, 31, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2312620099, 'issue_id': 2475591062, 'author': 'Lee-W', 'body': ""> @tirkarthi Thanks for the quick PR. I'm in no position to officially review, but the changes look straight-forward and covered by a test-case. I can wait for this to make it into the next release cycle.\r\n\r\nEven if you don't have commitership, you can still encouraged to review a PR 🙂  I'll also try to take a look these days"", 'created_at': datetime.datetime(2024, 8, 27, 13, 49, 56, tzinfo=datetime.timezone.utc)}]","tirkarthi on (2024-08-23 22:29:55 UTC): cc @sunank200

tirkarthi on (2024-08-26 06:31:58 UTC): state attribute of task_instance is directly set in BaseTaskEndEvent. A possible fix would be to call task_instance.set_state which will set the state and update end_date and duration attributes. I can raise a fix for this.

tirkarthi on (2024-08-26 14:01:23 UTC): @adrianrego  Please review #41754 . I guess you can also inherit `BaseTaskEndEvent` to override `handle_submit` to call set_state as per PR and use it as base class for custom TaskSuccessEvent, TaskFailedEvent etc. though I haven't tested it in case you need the fix before next release of 2.10.1 .

adrianrego (Issue Creator) on (2024-08-26 14:31:39 UTC): @tirkarthi Thanks for the quick PR.  I'm in no position to officially review, but the changes look straight-forward and covered by a test-case.  I can wait for this to make it into the next release cycle.

Lee-W on (2024-08-27 13:49:56 UTC): Even if you don't have commitership, you can still encouraged to review a PR 🙂  I'll also try to take a look these days

"
2475363281,issue,open,,Multiple Airflow in one k8s namespace,"### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When we have two Airflow deployed in one k8s namespace (using k8s executor) scheduler starts to monitor pods of each other and the task queue is clogged and not cleared

### What you think should happen instead?

I think each Airflow instance must filter pods from namespace with special label (id of AF instance or something) and he must be able to separate pods.

### How to reproduce

1) Deploy two AF in one k8s namespace
2) Start running tasks (pod operator or k8s executor)

### Operating System

Ubuntu

### Versions of Apache Airflow Providers

8.3.3 k8s provider

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

It happens both in multinamespace or singlenamespace mode.

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",KochankovID,2024-08-20 11:05:48+00:00,[],2025-01-10 09:35:29+00:00,,https://github.com/apache/airflow/issues/41608,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]","[{'comment_id': 2298587591, 'issue_id': 2475363281, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 20, 11, 5, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2298721428, 'issue_id': 2475363281, 'author': 'eladkal', 'body': 'I think what you are after is https://github.com/apache/airflow/pull/35639 ?', 'created_at': datetime.datetime(2024, 8, 20, 12, 19, 20, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-20 11:05:51 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2024-08-20 12:19:20 UTC): I think what you are after is https://github.com/apache/airflow/pull/35639 ?

"
2475066151,issue,closed,not_planned,OpenTelemetry metrics are not getting sent,"### Official Helm Chart version

1.15.0 (latest released)

### Apache Airflow version

2.9.3

### Kubernetes Version

1.28.9

### Helm Chart configuration
```
config:
  metrics:
    otel_on: 'True'
    otel_host: opentelemetrycollector-service.observability
    otel_port: '4318'
    otel_prefix: 'airflow'
    otel_debugging_on: 'True'
```
### Docker Image customizations

Yes, I did extend official airflow image with some additional dependencies and minor UI changes.

### What happened

After applying values.yaml configuration I don't see any either logs about metrics or calls to the OTEL collector.

### What you think should happen instead

Logs about metrics should be shown and those metrics should be sent to OTEL collector.

### How to reproduce

Try to change airflow.cfg or use env variables to setup OpenTelemetry metrics.

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",nazarii-koblai,2024-08-20 08:44:51+00:00,[],2024-11-28 11:29:41+00:00,2024-08-20 10:05:58+00:00,https://github.com/apache/airflow/issues/41606,"[('kind:bug', 'This is a clearly a bug'), ('area:helm-chart', 'Airflow Helm Chart'), ('telemetry', 'Telemetry-related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2298311567, 'issue_id': 2475066151, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 20, 8, 44, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2505852130, 'issue_id': 2475066151, 'author': 'LSparkzwz', 'body': 'Hi @nazarii-koblai , did you manage to find a solution for this?', 'created_at': datetime.datetime(2024, 11, 28, 11, 5, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2505885307, 'issue_id': 2475066151, 'author': 'nazarii-koblai', 'body': ""Hello, @LSparkzwz, yes I did. For some reason u also need to disable statsd metrics strictly through the configuration\r\n```\r\n[metrics]\r\nstatsd_on = False\r\n```\r\nOtherwise it will be enabled even u do have something like\r\n```\r\n[metrics]\r\notel_on = True\r\n```\r\nSo the resoultion itself is to keep both of them in place:\r\n```\r\n[metrics]\r\nstatsd_on = False\r\notel_on = True\r\n```\r\nExample of the helm chart configuration:\r\n```\r\nconfig:\r\n  metrics:\r\n    statsd_on: 'False'\r\n    otel_on: 'True'\r\n    otel_host: opentelemetrycollector-service.observability\r\n    otel_port: '4318'\r\n    otel_prefix: 'airflow'\r\n    otel_debugging_on: 'True'\r\n```\r\nProbably this is somehow related to the default helm chart values."", 'created_at': datetime.datetime(2024, 11, 28, 11, 22, 57, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-20 08:44:54 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

LSparkzwz on (2024-11-28 11:05:51 UTC): Hi @nazarii-koblai , did you manage to find a solution for this?

nazarii-koblai (Issue Creator) on (2024-11-28 11:22:57 UTC): Hello, @LSparkzwz, yes I did. For some reason u also need to disable statsd metrics strictly through the configuration
```
[metrics]
statsd_on = False
```
Otherwise it will be enabled even u do have something like
```
[metrics]
otel_on = True
```
So the resoultion itself is to keep both of them in place:
```
[metrics]
statsd_on = False
otel_on = True
```
Example of the helm chart configuration:
```
config:
  metrics:
    statsd_on: 'False'
    otel_on: 'True'
    otel_host: opentelemetrycollector-service.observability
    otel_port: '4318'
    otel_prefix: 'airflow'
    otel_debugging_on: 'True'
```
Probably this is somehow related to the default helm chart values.

"
2474918850,issue,closed,completed,Running Breeze static checks (all files) on local machine results in failures,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Running `breeze static-checks --all-files` on local machine results in failures. Reported by user Tim Perry on Airflow Slack.

**Failure output reported by user:**
```
Run flynt string format converter for Python.......................................Failed
- hook id: flynt
- files were modified by this hook
Running flynt v.1.0.1
Using config file at /Users/tim/code/qualapps/apache/apache-airflow/pyproject.toml
<unknown>:79: SyntaxWarning: invalid escape sequence '\s'
<unknown>:79: SyntaxWarning: invalid escape sequence '\s'
<unknown>:79: SyntaxWarning: invalid escape sequence '\s'
Run 'ruff format' for extremely fast Python formatting.............................Failed
- hook id: ruff-format
- files were modified by this hook
1 file reformatted, 4622 files left unchanged
4623 files left unchanged
un mypy for providers.............................................................Failed
- hook id: mypy-providers
- exit code: 2
Using 'uv' to install Airflow
Using airflow version from current sources
Leaving default pydantic v2
Using 'uv' to install Airflow
airflow/providers/fab/auth_manager/cli_commands/user_command.py:215: error:
f-string expression part cannot include a backslash [syntax]
      print(f""Created the following users:\n\t{'\\n\\t'.join(users_c...
          ^
Found 1 error in 1 file (errors prevented further checking)
Error 2 returned
If you see strange stacktraces above, and can't reproduce it, please run this command and try again:
breeze ci-image build --python 3.8
You can also run `breeze down --cleanup-mypy-cache` to clean up the cache used.
Using 'uv' to install Airflow
Using airflow version from current sources
Leaving default pydantic v2
Using 'uv' to install Airflow
```

### What you think should happen instead?

The static checks should complete without failures when no code changes have been made by user on local machine.

### How to reproduce

**Steps to reproduce this issue:**
1. Pull Airflow `main` repo.
2. Setup Breeze on local machine.
3. Run static checks with Breeze using command: `breeze static-checks --all-files`

### Operating System

macOS Sonoma 14.5 with Docker 4.32.0

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",omkar-foss,2024-08-20 07:29:09+00:00,[],2024-08-21 13:52:50+00:00,2024-08-21 13:46:39+00:00,https://github.com/apache/airflow/issues/41604,"[('kind:bug', 'This is a clearly a bug'), ('area:dev-env', 'CI, pre-commit, pylint and other changes that do not change the behavior of the final code'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2298162134, 'issue_id': 2474918850, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 20, 7, 29, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2298171465, 'issue_id': 2474918850, 'author': 'omkar-foss', 'body': 'I ran `breeze static-checks --all-files` on my local machine, and I too am seeing this error:\r\n```\r\nairflow/providers/fab/auth_manager/cli_commands/user_command.py:215: error:\r\nf-string expression part cannot include a backslash [syntax]\r\n      print(f""Created the following users:\\n\\t{\'\\\\n\\\\t\'.join(users_c...\r\n```\r\n\r\nAlthough I did not get this other error - `<unknown>:79: SyntaxWarning: invalid escape sequence \'\\s\'`. This one actually is a (syntax) warning, so it\'s weird to see that it\'s causing a failure.\r\n\r\nI\'ll look into this further and raise a PR accordingly.', 'created_at': datetime.datetime(2024, 8, 20, 7, 34, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2301324587, 'issue_id': 2474918850, 'author': 'uranusjr', 'body': 'I think this can be closed?', 'created_at': datetime.datetime(2024, 8, 21, 7, 23, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2301808227, 'issue_id': 2474918850, 'author': 'omkar-foss', 'body': ""Yes, just checking on one other failure (from a syntax warning) and a confirmation from the user who reported this issue (if it's fixed for them), after which I'll close this issue."", 'created_at': datetime.datetime(2024, 8, 21, 11, 20, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2302102747, 'issue_id': 2474918850, 'author': 'potiuk', 'body': ""Let's close it now - if the user has still problem we can always re-open. Closing things fast with the notion that they can be always re-opened is the best way to deal with issue overload."", 'created_at': datetime.datetime(2024, 8, 21, 13, 46, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2302114836, 'issue_id': 2474918850, 'author': 'omkar-foss', 'body': 'Sure, sounds good.', 'created_at': datetime.datetime(2024, 8, 21, 13, 52, 49, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-20 07:29:12 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

omkar-foss (Issue Creator) on (2024-08-20 07:34:33 UTC): I ran `breeze static-checks --all-files` on my local machine, and I too am seeing this error:
```
airflow/providers/fab/auth_manager/cli_commands/user_command.py:215: error:
f-string expression part cannot include a backslash [syntax]
      print(f""Created the following users:\n\t{'\\n\\t'.join(users_c...
```

Although I did not get this other error - `<unknown>:79: SyntaxWarning: invalid escape sequence '\s'`. This one actually is a (syntax) warning, so it's weird to see that it's causing a failure.

I'll look into this further and raise a PR accordingly.

uranusjr on (2024-08-21 07:23:31 UTC): I think this can be closed?

omkar-foss (Issue Creator) on (2024-08-21 11:20:06 UTC): Yes, just checking on one other failure (from a syntax warning) and a confirmation from the user who reported this issue (if it's fixed for them), after which I'll close this issue.

potiuk on (2024-08-21 13:46:39 UTC): Let's close it now - if the user has still problem we can always re-open. Closing things fast with the notion that they can be always re-opened is the best way to deal with issue overload.

omkar-foss (Issue Creator) on (2024-08-21 13:52:49 UTC): Sure, sounds good.

"
2474033882,issue,closed,completed,XCom with / in key results in UI error,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.1 and 2.10.0

### What happened?

XComs with a forward slash (/) in the key are not escaped when requesting the value from the API. Example on Airflow 2.10.0, in the XCom tab of a task pushing an XCom:

<img width=""883"" alt=""image"" src=""https://github.com/user-attachments/assets/e5c9d308-68f7-4af3-8e1e-dd31d63ebffe"">

The / is not escaped in the call to https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html#operation/get_xcom_entry and therefore the API thinks it's a request with 2 parts, which then returns a 404 because it's an unknown request structure.

Note: the Admin -> XCom view does display the correct value, and tasks pulling the XCom are also able to function correctly, the issue in only in the UI.

### What you think should happen instead?

XCom keys should be escaped for API requests.

### How to reproduce

DAG to reproduce:
```python
import datetime

from airflow import DAG
from airflow.models import TaskInstance
from airflow.operators.python import PythonOperator

with DAG(dag_id=""xcom_example"", schedule=None, start_date=datetime.datetime(2024, 1, 1)):

    def _push_xcom(ti: TaskInstance):
        ti.xcom_push(key=""foo/bar"", value=""test"")

    push_xcom = PythonOperator(task_id=""push_xcom"", python_callable=_push_xcom)

    def _pull_xcom(ti: TaskInstance):
        print(ti.xcom_pull(task_ids=""push_xcom"", key=""foo/bar""))

    pull_xcom = PythonOperator(task_id=""pull_xcom"", python_callable=_pull_xcom)

    push_xcom >> pull_xcom
```

Trigger the DAG and browse to the XCom tab of the `push_xcom` task to see the error. Tested on Airflow 2.9.1 and 2.10.0.

### Operating System

N/A

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",BasPH,2024-08-19 19:20:39+00:00,['shahar1'],2024-12-21 17:41:18+00:00,2024-12-21 17:41:17+00:00,https://github.com/apache/airflow/issues/41598,"[('kind:bug', 'This is a clearly a bug'), ('area:API', ""Airflow's REST/HTTP API""), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2558165640, 'issue_id': 2474033882, 'author': 'shahar1', 'body': ""Apparently it's more of an API issue rather than UI, but escaping it wouldn't harm.\r\nCreated a PR to take care of both :)"", 'created_at': datetime.datetime(2024, 12, 21, 16, 23, 19, tzinfo=datetime.timezone.utc)}]","shahar1 (Assginee) on (2024-12-21 16:23:19 UTC): Apparently it's more of an API issue rather than UI, but escaping it wouldn't harm.
Created a PR to take care of both :)

"
2473793629,issue,closed,completed,BashOperator is killed before it can execute,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.8.4

### What happened?

Attempting to run a simple hello world BashOperator example:
```
from datetime import datetime
from datetime import timedelta

from airflow import DAG
from airflow.operators.bash import BashOperator

DEFAULT_ARGS = {
    'retries': 0,
    'retry_delay': timedelta(minutes=1)
}

dag = DAG(
    'hello_world_dag',
    default_args=DEFAULT_ARGS,
    description='BashOperator test',
    schedule_interval=None,
    max_active_runs=1,
    concurrency=1,
    start_date=datetime(2024, 8, 1),
    tags=['cwilde'],
    catchup=False
)

# Define the BashOperator task
hello_world_task = BashOperator(
    task_id='hello_world_task',
    bash_command='echo ""hello world""',
    dag=dag
)

# Define the task dependencies
hello_world_task
```

Task never executes, is killed immediately before it can run. This is not a timeout issue, it happens immediately after manually triggering the dag.

```
*** Found local files:
***   * /opt/airflow_logs/joblogs/dag_id=hello_world_dag/run_id=manual__2024-08-19T16:43:03.065644+00:00/task_id=hello_world_task/attempt=1.log.SchedulerJob.log
[2024-08-19, 10:43:16 MDT] {scheduler_job_runner.py:780} ERROR - Executor reports task instance <TaskInstance: hello_world_dag.hello_world_task manual__2024-08-19T16:43:03.065644+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?
```

### What you think should happen instead?

_No response_

### How to reproduce

The task should execute the bash command `echo ""hello world""` but the task is killed before the command can run

### Operating System

NAME=""Amazon Linux"" VERSION=""2"" ID=""amzn"" ID_LIKE=""centos rhel fedora"" VERSION_ID=""2"" PRETTY_NAME=""Amazon Linux 2"" ANSI_COLOR=""0;33"" CPE_NAME=""cpe:2.3:o:amazon:amazon_linux:2"" HOME_URL=""https://amazonlinux.com/"" SUPPORT_END=""2025-06-30""

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.19.0
apache-airflow-providers-celery==3.6.1
apache-airflow-providers-common-io==1.3.0
apache-airflow-providers-common-sql==1.11.1
apache-airflow-providers-ftp==3.7.0
apache-airflow-providers-http==4.10.0
apache-airflow-providers-imap==3.5.0
apache-airflow-providers-mysql==5.5.4
apache-airflow-providers-redis==3.6.0
apache-airflow-providers-slack==8.6.1
apache-airflow-providers-smtp==1.6.1
apache-airflow-providers-sqlite==3.7.1

### Deployment

Other

### Deployment details

Custom deploy on EC2

### Anything else?

This fails every time I execute the DAG

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",crutis,2024-08-19 16:56:51+00:00,[],2024-08-30 21:57:49+00:00,2024-08-30 21:57:49+00:00,https://github.com/apache/airflow/issues/41597,"[('kind:bug', 'This is a clearly a bug'), (""Can't Reproduce"", 'The problem cannot be reproduced'), ('pending-response', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2297021255, 'issue_id': 2473793629, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 19, 16, 56, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2297705491, 'issue_id': 2473793629, 'author': 'romsharon98', 'body': 'It can happen because few reasons, did you try checking scheduler logs and worker logs?', 'created_at': datetime.datetime(2024, 8, 19, 23, 47, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2299317507, 'issue_id': 2473793629, 'author': 'crutis', 'body': ""The entire worker log (same as above) is just this:\r\n```\r\n*** Found local files:\r\n***   * /opt/airflow_logs/joblogs/dag_id=hello_world_dag/run_id=manual__2024-08-19T16:43:03.065644+00:00/task_id=hello_world_task/attempt=1.log.SchedulerJob.log\r\n[2024-08-19, 10:43:16 MDT] {scheduler_job_runner.py:780} ERROR - Executor reports task instance <TaskInstance: hello_world_dag.hello_world_task manual__2024-08-19T16:43:03.065644+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?\r\n```\r\n\r\nThe scheduler log just has:\r\n```\r\n[2024-08-19T16:47:40.525+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/hello_world.py took 0.228 seconds\r\n[2024-08-19T16:48:46.990+0000] {processor.py:161} INFO - Started process (PID=15185) to work on /opt/airflow/dags/hello_world.py\r\n[2024-08-19T16:48:46.991+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/hello_world.py for tasks to queue\r\n[2024-08-19T16:48:46.991+0000] {logging_mixin.py:188} INFO - [2024-08-19T16:48:46.991+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/hello_world.py\r\n[2024-08-19T16:48:47.013+0000] {processor.py:840} INFO - DAG(s) 'hello_world_dag' retrieved from /opt/airflow/dags/hello_world.py\r\n[2024-08-19T16:48:47.110+0000] {logging_mixin.py:188} INFO - [2024-08-19T16:48:47.110+0000] {dag.py:3047} INFO - Sync 1 DAGs\r\n[2024-08-19T16:48:47.170+0000] {logging_mixin.py:188} INFO - [2024-08-19T16:48:47.170+0000] {dag.py:3834} INFO - Setting next_dagrun for hello_world_dag to None, run_after=None\r\n[2024-08-19T16:48:47.237+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/hello_world.py took 0.251 seconds\r\n[2024-08-19T16:49:54.769+0000] {processor.py:161} INFO - Started process (PID=16116) to work on /opt/airflow/dags/hello_world.py\r\n[2024-08-19T16:49:54.770+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/hello_world.py for tasks to queue\r\n[2024-08-19T16:49:54.771+0000] {logging_mixin.py:188} INFO - [2024-08-19T16:49:54.771+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/hello_world.py\r\n[2024-08-19T16:49:54.781+0000] {processor.py:840} INFO - DAG(s) 'hello_world_dag' retrieved from /opt/airflow/dags/hello_world.py\r\n[2024-08-19T16:49:54.911+0000] {logging_mixin.py:188} INFO - [2024-08-19T16:49:54.911+0000] {dag.py:3047} INFO - Sync 1 DAGs\r\n[2024-08-19T16:49:54.928+0000] {logging_mixin.py:188} INFO - [2024-08-19T16:49:54.928+0000] {dag.py:3834} INFO - Setting next_dagrun for hello_world_dag to None, run_after=None\r\n[2024-08-19T16:49:55.010+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/hello_world.py took 0.245 seconds\r\n```\r\n\r\nOr is there some other log location I should be looking in?"", 'created_at': datetime.datetime(2024, 8, 20, 16, 52, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2301653513, 'issue_id': 2473793629, 'author': 'nathadfield', 'body': '@crutis There is nothing wrong with Airflow or your DAG code because it runs without issue when I test it.\r\n\r\n```\r\n***   * /root/airflow/logs/dag_id=hello_world_dag/run_id=manual__2024-08-21T09:52:11.804629+00:00/task_id=hello_world_task/attempt=1.log\r\n[2024-08-21, 09:52:12 UTC] {local_task_job_runner.py:123} ▶ Pre task execution logs\r\n[2024-08-21, 09:52:12 UTC] {subprocess.py:63} INFO - Tmp dir root location: /tmp\r\n[2024-08-21, 09:52:12 UTC] {subprocess.py:75} INFO - Running command: [\'/usr/bin/bash\', \'-c\', \'echo ""hello world""\']\r\n[2024-08-21, 09:52:12 UTC] {subprocess.py:86} INFO - Output:\r\n[2024-08-21, 09:52:12 UTC] {subprocess.py:93} INFO - hello world\r\n[2024-08-21, 09:52:12 UTC] {subprocess.py:97} INFO - Command exited with return code 0\r\n[2024-08-21, 09:52:12 UTC] {taskinstance.py:339} ▼ Post task execution logs\r\n[2024-08-21, 09:52:12 UTC] {taskinstance.py:351} INFO - Marking task as SUCCESS. dag_id=hello_world_dag, task_id=hello_world_task, run_id=manual__2024-08-21T09:52:11.804629+00:00, execution_date=20240821T095211, start_date=20240821T095212, end_date=20240821T095212\r\n[2024-08-21, 09:52:12 UTC] {local_task_job_runner.py:261} INFO - Task exited with return code 0\r\n[2024-08-21, 09:52:12 UTC] {taskinstance.py:3888} INFO - 0 downstream tasks scheduled from follow-on schedule check\r\n[2024-08-21, 09:52:12 UTC] {local_task_job_runner.py:240} ▲▲▲ Log group end\r\n```\r\n\r\nSo, the issue you are facing is likely due to the way in which you have Airflow installed and configured.  The specific error - `[2024-08-19, 10:43:16 MDT] {scheduler_job_runner.py:780} ERROR - Executor reports task instance <TaskInstance: hello_world_dag.hello_world_task manual__2024-08-19T16:43:03.065644+00:00 [queued]> finished (failed) although the task says it\'s queued. (Info: None) Was the task killed externally?` indicates that something happened externally to Airflow that resulted in the the process running the task to be killed.\r\n\r\nI think you need to review your EC2 deployment to check that it is sufficiently resourced and configured for running all of the various Airflow components.', 'created_at': datetime.datetime(2024, 8, 21, 10, 0, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2302695326, 'issue_id': 2473793629, 'author': 'crutis', 'body': 'We agree, it _should_ work 😄  but it gets killed immediately before hitting any configured timeout. Attaching my config in case you see anything off\r\n\r\n[airflow_config.txt](https://github.com/user-attachments/files/16696375/airflow_config.txt)', 'created_at': datetime.datetime(2024, 8, 21, 18, 19, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2302747671, 'issue_id': 2473793629, 'author': 'romsharon98', 'body': ""> The entire worker log (same as above) is just this:\r\n> \r\n> ```\r\n> *** Found local files:\r\n> ***   * /opt/airflow_logs/joblogs/dag_id=hello_world_dag/run_id=manual__2024-08-19T16:43:03.065644+00:00/task_id=hello_world_task/attempt=1.log.SchedulerJob.log\r\n> [2024-08-19, 10:43:16 MDT] {scheduler_job_runner.py:780} ERROR - Executor reports task instance <TaskInstance: hello_world_dag.hello_world_task manual__2024-08-19T16:43:03.065644+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?\r\n> ```\r\n> \r\n> The scheduler log just has:\r\n> \r\n> ```\r\n> [2024-08-19T16:47:40.525+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/hello_world.py took 0.228 seconds\r\n> [2024-08-19T16:48:46.990+0000] {processor.py:161} INFO - Started process (PID=15185) to work on /opt/airflow/dags/hello_world.py\r\n> [2024-08-19T16:48:46.991+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/hello_world.py for tasks to queue\r\n> [2024-08-19T16:48:46.991+0000] {logging_mixin.py:188} INFO - [2024-08-19T16:48:46.991+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/hello_world.py\r\n> [2024-08-19T16:48:47.013+0000] {processor.py:840} INFO - DAG(s) 'hello_world_dag' retrieved from /opt/airflow/dags/hello_world.py\r\n> [2024-08-19T16:48:47.110+0000] {logging_mixin.py:188} INFO - [2024-08-19T16:48:47.110+0000] {dag.py:3047} INFO - Sync 1 DAGs\r\n> [2024-08-19T16:48:47.170+0000] {logging_mixin.py:188} INFO - [2024-08-19T16:48:47.170+0000] {dag.py:3834} INFO - Setting next_dagrun for hello_world_dag to None, run_after=None\r\n> [2024-08-19T16:48:47.237+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/hello_world.py took 0.251 seconds\r\n> [2024-08-19T16:49:54.769+0000] {processor.py:161} INFO - Started process (PID=16116) to work on /opt/airflow/dags/hello_world.py\r\n> [2024-08-19T16:49:54.770+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/hello_world.py for tasks to queue\r\n> [2024-08-19T16:49:54.771+0000] {logging_mixin.py:188} INFO - [2024-08-19T16:49:54.771+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/hello_world.py\r\n> [2024-08-19T16:49:54.781+0000] {processor.py:840} INFO - DAG(s) 'hello_world_dag' retrieved from /opt/airflow/dags/hello_world.py\r\n> [2024-08-19T16:49:54.911+0000] {logging_mixin.py:188} INFO - [2024-08-19T16:49:54.911+0000] {dag.py:3047} INFO - Sync 1 DAGs\r\n> [2024-08-19T16:49:54.928+0000] {logging_mixin.py:188} INFO - [2024-08-19T16:49:54.928+0000] {dag.py:3834} INFO - Setting next_dagrun for hello_world_dag to None, run_after=None\r\n> [2024-08-19T16:49:55.010+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/hello_world.py took 0.245 seconds\r\n> ```\r\n> \r\n> Or is there some other log location I should be looking in?\r\n\r\nWhen I asked for the worker logs, I wasn’t referring to this. I meant what @nathadfield suggested—going to your deployment and checking the logs directly in the pod. It seems like the pod might not have even started, and reviewing the events/logs there should give you the answer."", 'created_at': datetime.datetime(2024, 8, 21, 18, 52, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2302837384, 'issue_id': 2473793629, 'author': 'crutis', 'body': ""I guess this is where I'm confused, we're running on EC2 with celery, not kubernetes, we don't have any pods to get logs from"", 'created_at': datetime.datetime(2024, 8, 21, 19, 17, 25, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-19 16:56:55 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

romsharon98 on (2024-08-19 23:47:03 UTC): It can happen because few reasons, did you try checking scheduler logs and worker logs?

crutis (Issue Creator) on (2024-08-20 16:52:59 UTC): The entire worker log (same as above) is just this:
```
*** Found local files:
***   * /opt/airflow_logs/joblogs/dag_id=hello_world_dag/run_id=manual__2024-08-19T16:43:03.065644+00:00/task_id=hello_world_task/attempt=1.log.SchedulerJob.log
[2024-08-19, 10:43:16 MDT] {scheduler_job_runner.py:780} ERROR - Executor reports task instance <TaskInstance: hello_world_dag.hello_world_task manual__2024-08-19T16:43:03.065644+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?
```

The scheduler log just has:
```
[2024-08-19T16:47:40.525+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/hello_world.py took 0.228 seconds
[2024-08-19T16:48:46.990+0000] {processor.py:161} INFO - Started process (PID=15185) to work on /opt/airflow/dags/hello_world.py
[2024-08-19T16:48:46.991+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/hello_world.py for tasks to queue
[2024-08-19T16:48:46.991+0000] {logging_mixin.py:188} INFO - [2024-08-19T16:48:46.991+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/hello_world.py
[2024-08-19T16:48:47.013+0000] {processor.py:840} INFO - DAG(s) 'hello_world_dag' retrieved from /opt/airflow/dags/hello_world.py
[2024-08-19T16:48:47.110+0000] {logging_mixin.py:188} INFO - [2024-08-19T16:48:47.110+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-08-19T16:48:47.170+0000] {logging_mixin.py:188} INFO - [2024-08-19T16:48:47.170+0000] {dag.py:3834} INFO - Setting next_dagrun for hello_world_dag to None, run_after=None
[2024-08-19T16:48:47.237+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/hello_world.py took 0.251 seconds
[2024-08-19T16:49:54.769+0000] {processor.py:161} INFO - Started process (PID=16116) to work on /opt/airflow/dags/hello_world.py
[2024-08-19T16:49:54.770+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/hello_world.py for tasks to queue
[2024-08-19T16:49:54.771+0000] {logging_mixin.py:188} INFO - [2024-08-19T16:49:54.771+0000] {dagbag.py:540} INFO - Filling up the DagBag from /opt/airflow/dags/hello_world.py
[2024-08-19T16:49:54.781+0000] {processor.py:840} INFO - DAG(s) 'hello_world_dag' retrieved from /opt/airflow/dags/hello_world.py
[2024-08-19T16:49:54.911+0000] {logging_mixin.py:188} INFO - [2024-08-19T16:49:54.911+0000] {dag.py:3047} INFO - Sync 1 DAGs
[2024-08-19T16:49:54.928+0000] {logging_mixin.py:188} INFO - [2024-08-19T16:49:54.928+0000] {dag.py:3834} INFO - Setting next_dagrun for hello_world_dag to None, run_after=None
[2024-08-19T16:49:55.010+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/hello_world.py took 0.245 seconds
```

Or is there some other log location I should be looking in?

nathadfield on (2024-08-21 10:00:23 UTC): @crutis There is nothing wrong with Airflow or your DAG code because it runs without issue when I test it.

```
***   * /root/airflow/logs/dag_id=hello_world_dag/run_id=manual__2024-08-21T09:52:11.804629+00:00/task_id=hello_world_task/attempt=1.log
[2024-08-21, 09:52:12 UTC] {local_task_job_runner.py:123} ▶ Pre task execution logs
[2024-08-21, 09:52:12 UTC] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2024-08-21, 09:52:12 UTC] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'echo ""hello world""']
[2024-08-21, 09:52:12 UTC] {subprocess.py:86} INFO - Output:
[2024-08-21, 09:52:12 UTC] {subprocess.py:93} INFO - hello world
[2024-08-21, 09:52:12 UTC] {subprocess.py:97} INFO - Command exited with return code 0
[2024-08-21, 09:52:12 UTC] {taskinstance.py:339} ▼ Post task execution logs
[2024-08-21, 09:52:12 UTC] {taskinstance.py:351} INFO - Marking task as SUCCESS. dag_id=hello_world_dag, task_id=hello_world_task, run_id=manual__2024-08-21T09:52:11.804629+00:00, execution_date=20240821T095211, start_date=20240821T095212, end_date=20240821T095212
[2024-08-21, 09:52:12 UTC] {local_task_job_runner.py:261} INFO - Task exited with return code 0
[2024-08-21, 09:52:12 UTC] {taskinstance.py:3888} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-08-21, 09:52:12 UTC] {local_task_job_runner.py:240} ▲▲▲ Log group end
```

So, the issue you are facing is likely due to the way in which you have Airflow installed and configured.  The specific error - `[2024-08-19, 10:43:16 MDT] {scheduler_job_runner.py:780} ERROR - Executor reports task instance <TaskInstance: hello_world_dag.hello_world_task manual__2024-08-19T16:43:03.065644+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?` indicates that something happened externally to Airflow that resulted in the the process running the task to be killed.

I think you need to review your EC2 deployment to check that it is sufficiently resourced and configured for running all of the various Airflow components.

crutis (Issue Creator) on (2024-08-21 18:19:48 UTC): We agree, it _should_ work 😄  but it gets killed immediately before hitting any configured timeout. Attaching my config in case you see anything off

[airflow_config.txt](https://github.com/user-attachments/files/16696375/airflow_config.txt)

romsharon98 on (2024-08-21 18:52:32 UTC): When I asked for the worker logs, I wasn’t referring to this. I meant what @nathadfield suggested—going to your deployment and checking the logs directly in the pod. It seems like the pod might not have even started, and reviewing the events/logs there should give you the answer.

crutis (Issue Creator) on (2024-08-21 19:17:25 UTC): I guess this is where I'm confused, we're running on EC2 with celery, not kubernetes, we don't have any pods to get logs from

"
2473785203,issue,open,,Nested task group names are not generated properly,"### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When the same task group gets called multiple times within a task group, a `airflow.exceptions.DuplicateTaskIdFound` is raised. See `print_k_group_inner` in the following example. Note that if you comment out one of the `print_k_group_inner`, the top-level task groups are named correctly to `print_k_group` and `print_k_group__1`.

### What you think should happen instead?

There should be 4 task groups created and no errors raised.

-`print_k_group.print_k_group_inner`
-`print_k_group.print_k_group_inner__1`
-`print_k_group__1.print_k_group_inner`
-`print_k_group__1.print_k_group_inner__1`

### How to reproduce

```
from airflow.decorators import dag, task_group, task
from pendulum import datetime


@dag(
    start_date=datetime(2023, 7, 1),
    schedule=None,
    catchup=False,
    tags=[""@task_group"", ""dependencies""],
)
def test_tg():
    @task
    def get_k():
        return 10

    @task
    def print_k(k):
        print(k)

    @task_group
    def print_k_group(k):
        print_k_group_inner(k)
        print_k_group_inner(k)

    @task_group
    def print_k_group_inner(k):
        print_k(k)

    k = get_k()
    print_k_group(k)
    print_k_group(k)


test_tg()
```

### Operating System

Ubuntu 22.04.4 LTS

### Versions of Apache Airflow Providers

None

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jerryqhyu,2024-08-19 16:51:44+00:00,[],2025-01-10 09:07:26+00:00,,https://github.com/apache/airflow/issues/41596,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:TaskGroup', '')]","[{'comment_id': 2297012577, 'issue_id': 2473785203, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 19, 16, 51, 48, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-19 16:51:48 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2473120884,issue,closed,completed,Support for workload identity authentication from the Databricks provider,"### Description

Currently, only managed identity is supported for connecting from Airflow to Databricks using the `use_azure_managed_identity` parameter. However, I have a use case where an Airflow instance is running on an AKS cluster. That's why this would be a great improvement in authentication without using any secrets.

### Use case/motivation

_No response_

### Related issues

#39521 

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",basvandriel,2024-08-19 11:23:33+00:00,['basvandriel'],2024-11-06 17:26:28+00:00,2024-11-06 17:26:28+00:00,https://github.com/apache/airflow/issues/41586,"[('kind:feature', 'Feature Requests'), ('provider:databricks', '')]","[{'comment_id': 2301659255, 'issue_id': 2473120884, 'author': 'nathadfield', 'body': '@basvandriel Are you planning to implement this yourself?', 'created_at': datetime.datetime(2024, 8, 21, 10, 3, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2301662912, 'issue_id': 2473120884, 'author': 'basvandriel', 'body': 'Yes, I am working on this as we speak :)\r\n________________________________\r\nVan: Nathan Hadfield ***@***.***>\r\nVerzonden: Wednesday, August 21, 2024 12:03:25 PM\r\nAan: apache/airflow ***@***.***>\r\nCC: Bas van Driel ***@***.***>; Mention ***@***.***>\r\nOnderwerp: Re: [apache/airflow] Support for workload identity authentication from the Databricks provider (Issue #41586)\r\n\r\n\r\n@basvandriel<https://github.com/basvandriel> Are you planning to implement this yourself?\r\n\r\n—\r\nReply to this email directly, view it on GitHub<https://github.com/apache/airflow/issues/41586#issuecomment-2301659255>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/ABIKS5GLCB6OUJ7VF2JXTZDZSRQW3AVCNFSM6AAAAABMXT4MXWVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGMBRGY2TSMRVGU>.\r\nYou are receiving this because you were mentioned.Message ID: ***@***.***>', 'created_at': datetime.datetime(2024, 8, 21, 10, 4, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2302358156, 'issue_id': 2473120884, 'author': 'pmooij', 'body': 'Nice @basvandriel, I would like to use this as well 👍', 'created_at': datetime.datetime(2024, 8, 21, 15, 20, 47, tzinfo=datetime.timezone.utc)}]","nathadfield on (2024-08-21 10:03:03 UTC): @basvandriel Are you planning to implement this yourself?

basvandriel (Issue Creator) on (2024-08-21 10:04:55 UTC): Yes, I am working on this as we speak :)
________________________________
Van: Nathan Hadfield ***@***.***>
Verzonden: Wednesday, August 21, 2024 12:03:25 PM
Aan: apache/airflow ***@***.***>
CC: Bas van Driel ***@***.***>; Mention ***@***.***>
Onderwerp: Re: [apache/airflow] Support for workload identity authentication from the Databricks provider (Issue #41586)


@basvandriel<https://github.com/basvandriel> Are you planning to implement this yourself?

—
Reply to this email directly, view it on GitHub<https://github.com/apache/airflow/issues/41586#issuecomment-2301659255>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/ABIKS5GLCB6OUJ7VF2JXTZDZSRQW3AVCNFSM6AAAAABMXT4MXWVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGMBRGY2TSMRVGU>.
You are receiving this because you were mentioned.Message ID: ***@***.***>

pmooij on (2024-08-21 15:20:47 UTC): Nice @basvandriel, I would like to use this as well 👍

"
2472821466,issue,closed,completed,Logout link in no roles and permissions error page is broken,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

logout endpoint was made a POST endpoint in https://github.com/apache/airflow/pull/40145 . With this change the /logout link in error page when there are no roles and permissions is still a GET request causing the request to be invalid. 

https://github.com/apache/airflow/blob/75fb7acbaca09a040067f0a5a37637ff44eb9e14/airflow/www/templates/airflow/no_roles_permissions.html#L32-L34

### What you think should happen instead?

_No response_

### How to reproduce

1. Initialize a new database.
2. Create an admin user.
3. Login as the user without roles and permission to get to the error page.
4. Click on logout link.

### Operating System

Ubuntu

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",tirkarthi,2024-08-19 08:52:35+00:00,['rohitnandi12'],2024-08-28 18:16:20+00:00,2024-08-28 18:16:20+00:00,https://github.com/apache/airflow/issues/41580,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2296118774, 'issue_id': 2472821466, 'author': 'potiuk', 'body': 'cc: @shahar1 -> small follow-up to #40145', 'created_at': datetime.datetime(2024, 8, 19, 9, 36, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304451830, 'issue_id': 2472821466, 'author': 'rohitnandi12', 'body': 'Hi @potiuk @shahar1 ,\r\n\r\nThis issue seems straightforward and I am confident to solve this. This is also going to be my first step towards Open Source contribution. I request you to please assign this issue to me. \r\n\r\nRegards,\r\nRohit Nandi', 'created_at': datetime.datetime(2024, 8, 22, 11, 37, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304456348, 'issue_id': 2472821466, 'author': 'shahar1', 'body': ""> Hi @potiuk @shahar1 ,\r\n> \r\n> This issue seems straightforward and I am confident to solve this. This is also going to be my first step towards Open Source contribution. I request you to please assign this issue to me.\r\n> \r\n> Regards, Rohit Nandi\r\n\r\nWelcome abroad! :)\r\nI assigned you, I'd be happy to review your PR when you're done."", 'created_at': datetime.datetime(2024, 8, 22, 11, 39, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2313944097, 'issue_id': 2472821466, 'author': 'gagan-bhullar-tech', 'body': '@shahar1 Fixed this issue in linked PR. Can you please assign this issue to me.', 'created_at': datetime.datetime(2024, 8, 28, 2, 4, 42, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-08-19 09:36:21 UTC): cc: @shahar1 -> small follow-up to #40145

rohitnandi12 (Assginee) on (2024-08-22 11:37:29 UTC): Hi @potiuk @shahar1 ,

This issue seems straightforward and I am confident to solve this. This is also going to be my first step towards Open Source contribution. I request you to please assign this issue to me. 

Regards,
Rohit Nandi

shahar1 on (2024-08-22 11:39:58 UTC): Welcome abroad! :)
I assigned you, I'd be happy to review your PR when you're done.

gagan-bhullar-tech on (2024-08-28 02:04:42 UTC): @shahar1 Fixed this issue in linked PR. Can you please assign this issue to me.

"
2472766300,issue,closed,completed,Cannot execute Dynamic Task Mapping when `default_arg` is present.,"### Apache Airflow version

2.10.0rc1

### If ""Other Airflow 2 version"" selected, which one?

2.10.0(not rc)

### What happened?

Cannot execute Dynamic Task Mapping when `default_arg` is present.

```shell
dddbfe3b933a
*** Found local files:
***   * /opt/airflow/logs/dag_id=dynamic_task_error/run_id=manual__2024-08-19T08:19:45.119366+00:00/task_id=add/map_index=0/attempt=1.log
[2024-08-19, 17:19:47 KST] {local_task_job_runner.py:123} ▼ Pre task execution logs
[2024-08-19, 17:19:47 KST] {taskinstance.py:2603} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: dynamic_task_error.add manual__2024-08-19T08:19:45.119366+00:00 map_index=0 [queued]>
[2024-08-19, 17:19:47 KST] {taskinstance.py:2603} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: dynamic_task_error.add manual__2024-08-19T08:19:45.119366+00:00 map_index=0 [queued]>
[2024-08-19, 17:19:47 KST] {taskinstance.py:2856} INFO - Starting attempt 1 of 1
[2024-08-19, 17:19:47 KST] {taskinstance.py:2879} INFO - Executing <Mapped(_PythonDecoratedOperator): add> on 2024-08-19 08:19:45.119366+00:00
[2024-08-19, 17:19:47 KST] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=18581) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2024-08-19, 17:19:47 KST] {standard_task_runner.py:72} INFO - Started process 18604 to run task
[2024-08-19, 17:19:47 KST] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'dynamic_task_error', 'add', 'manual__2024-08-19T08:19:45.119366+00:00', '--job-id', '4088', '--raw', '--subdir', 'DAGS_FOLDER/dynamic_test.py', '--cfg-path', '/tmp/tmpz8eds5gr', '--map-index', '0']
[2024-08-19, 17:19:47 KST] {standard_task_runner.py:105} INFO - Job 4088: Subtask add
[2024-08-19, 17:19:47 KST] {task_command.py:467} INFO - Running <TaskInstance: dynamic_task_error.add manual__2024-08-19T08:19:45.119366+00:00 map_index=0 [running]> on host dddbfe3b933a
[2024-08-19, 17:19:47 KST] {taskinstance.py:3301} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3105, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context, jinja_env=jinja_env)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3524, in render_templates
    original_task.render_template_fields(context, jinja_env)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/mappedoperator.py"", line 913, in render_template_fields
    unmapped_task = self.unmap(mapped_kwargs)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/mappedoperator.py"", line 824, in unmap
    op = self.operator_class(**kwargs, _airflow_from_mapped=True)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 490, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/python.py"", line 52, in __init__
    super().__init__(
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 490, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/decorators/base.py"", line 258, in __init__
    super().__init__(task_id=task_id, **kwargs_to_upstream, **kwargs)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 490, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py"", line 222, in __init__
    super().__init__(**kwargs)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 490, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 938, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to _PythonDecoratedOperator (task_id: add__1). Invalid arguments were:
**kwargs: {'key': 'value', 'arbitrary': 'value'}
[2024-08-19, 17:19:47 KST] {taskinstance.py:3349} ERROR - Unable to unmap task to determine if we need to send an alert email
[2024-08-19, 17:19:47 KST] {taskinstance.py:1225} INFO - Marking task as FAILED. dag_id=dynamic_task_error, task_id=add, run_id=manual__2024-08-19T08:19:45.119366+00:00, map_index=0, execution_date=20240819T081945, start_date=20240819T081947, end_date=20240819T081947
[2024-08-19, 17:19:47 KST] {taskinstance.py:340} ▶ Post task execution logs
```

### What you think should happen instead?

Unused arguments should be ignored (i.e., they should behave the same as any other task).

As you can see in the example below, `Task <success>` is executed, but `Task <added_values>` is not.

### How to reproduce

```python
from __future__ import annotations

from airflow.decorators import dag, task
from pendulum import datetime


@dag(
    start_date=datetime(2024, 1, 1),
    schedule=None,
    catchup=False,
    default_args={""key"": ""value"", ""arbitrary"": ""value"", ""do_xcom_push"": False},
)
def dynamic_task_error() -> None:
    @task.python()
    def success() -> str:
        return ""success""

    @task.python(do_xcom_push=True)
    def add(x: int, y: int) -> int:
        return x + y

    do_success = success()
    added_values = add.partial(y=10).expand(x=[1, 2, 3])

    _ = do_success >> added_values


dynamic_task_error()

```

### Operating System

PRETTY_NAME=""Debian GNU/Linux 12 (bookworm)"" NAME=""Debian GNU/Linux"" VERSION_ID=""12"" VERSION=""12 (bookworm)"" VERSION_CODENAME=bookworm ID=debian HOME_URL=""https://www.debian.org/"" SUPPORT_URL=""https://www.debian.org/support"" BUG_REPORT_URL=""https://bugs.debian.org/""

### Versions of Apache Airflow Providers

apache-airflow-providers-celery==3.7.3
apache-airflow-providers-common-compat==1.1.0
apache-airflow-providers-common-io==1.4.0
apache-airflow-providers-common-sql==1.15.0
apache-airflow-providers-docker==3.12.3
apache-airflow-providers-fab==1.2.2
apache-airflow-providers-ftp==3.10.1
apache-airflow-providers-http==4.12.0
apache-airflow-providers-imap==3.6.1
apache-airflow-providers-jdbc==4.4.0
apache-airflow-providers-odbc==4.6.3
apache-airflow-providers-postgres==5.11.3
apache-airflow-providers-redis==3.7.1
apache-airflow-providers-smtp==1.7.1
apache-airflow-providers-sqlite==3.8.2

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",phi-friday,2024-08-19 08:26:27+00:00,[],2024-08-19 09:01:29+00:00,2024-08-19 08:50:02+00:00,https://github.com/apache/airflow/issues/41578,"[('kind:bug', 'This is a clearly a bug'), ('duplicate', 'Issue that is duplicated'), ('area:core', ''), ('area:dynamic-task-mapping', 'AIP-42'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2296009190, 'issue_id': 2472766300, 'author': 'eladkal', 'body': ""Isn't that duplicate of https://github.com/apache/airflow/issues/33600 ?"", 'created_at': datetime.datetime(2024, 8, 19, 8, 43, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296022674, 'issue_id': 2472766300, 'author': 'phi-friday', 'body': ""> Isn't that duplicate of #33600 ?\r\n\r\nIt's the exact same problem. Close the issue due to duplication."", 'created_at': datetime.datetime(2024, 8, 19, 8, 50, 2, tzinfo=datetime.timezone.utc)}]","eladkal on (2024-08-19 08:43:45 UTC): Isn't that duplicate of https://github.com/apache/airflow/issues/33600 ?

phi-friday (Issue Creator) on (2024-08-19 08:50:02 UTC): It's the exact same problem. Close the issue due to duplication.

"
2472741471,issue,closed,completed,"Status of testing Providers that were prepared on August 19, 2024","### Body

I have a kind request for all the contributors to the latest provider packages release.
Could you please help us to test the RC versions of the providers?

The guidelines on how to test providers can be found in

[Verify providers by contributors](https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDER_PACKAGES.md#verify-the-release-candidate-by-contributors)

Let us know in the comment, whether the issue is addressed.

Those are providers that require testing as there were some substantial changes introduced:


## Provider [amazon: 8.28.0rc1](https://pypi.org/project/apache-airflow-providers-amazon/8.28.0rc1)
   - [ ] [Add incremental export and cross account export functionality in `DynamoDBToS3Operator` (#41304)](https://github.com/apache/airflow/pull/41304): @Ghoul-SSZ
     Linked issues:
       - [ ] [Linked Issue #40737](https://github.com/apache/airflow/issues/40737): @Kuhlmann-Itagyba-bah
   - [ ] [EKS Overrides for AWS Batch submit_job (#40718)](https://github.com/apache/airflow/pull/40718): @ssilb4
   - [x] [Fix `AwsTaskLogFetcher` missing logs (#41515)](https://github.com/apache/airflow/pull/41515): @vincbeck
     Linked issues:
       - [ ] [Linked Issue #40875](https://github.com/apache/airflow/issues/40875): @yaningz
   - [x] [Issue-41243 Unpin the moto dependency (#41256)](https://github.com/apache/airflow/pull/41256): @vikramaditya91
     Linked issues:
       - [x] [Linked Issue #41243](https://github.com/apache/airflow/issues/41243): @potiuk
   - [ ] [Fix RedshiftDataOperator not running in deferred mode when it should (#41206)](https://github.com/apache/airflow/pull/41206): @borismo
   - [ ] [Partial fix for example_dynamodb_to_s3.py (#41517)](https://github.com/apache/airflow/pull/41517): @ferruzzi
   - [x] [Remove deprecated code is AWS provider (#41407)](https://github.com/apache/airflow/pull/41407): @vincbeck
     Linked issues:
       - [x] [Linked Issue #41396](https://github.com/apache/airflow/pull/41396): @eladkal
   - [x] [Limit moto temporarily - 5.0.12 is breaking our tests (#41244)](https://github.com/apache/airflow/pull/41244): @potiuk
## Provider [apache.spark: 4.10.0rc1](https://pypi.org/project/apache-airflow-providers-apache-spark/4.10.0rc1)
   - [x] [fix: resolve AirflowProviderDeprecationWarning in spark provider (#41358)](https://github.com/apache/airflow/pull/41358): @phi-friday
     Linked issues:
## Provider [celery: 3.8.0rc1](https://pypi.org/project/apache-airflow-providers-celery/3.8.0rc1)
   - [x] [Remove deprecated SubDags (#41390)](https://github.com/apache/airflow/pull/41390): @kaxil
## Provider [cncf.kubernetes: 8.4.0rc1](https://pypi.org/project/apache-airflow-providers-cncf-kubernetes/8.4.0rc1)
   - [x] [Remove deprecated SubDags (#41390)](https://github.com/apache/airflow/pull/41390): @kaxil
   - [x] [spark kubernetes operator arguments description reordering (#41372)](https://github.com/apache/airflow/pull/41372): @dirrao
## Provider [common.sql: 1.16.0rc1](https://pypi.org/project/apache-airflow-providers-common-sql/1.16.0rc1)
   - [x] [fix: rm deprecated import in `common.sql` (#41461)](https://github.com/apache/airflow/pull/41461): @phi-friday
     Linked issues:
       - [x] [Linked Issue #41460](https://github.com/apache/airflow/issues/41460): @phi-friday
## Provider [docker: 3.13.0rc1](https://pypi.org/project/apache-airflow-providers-docker/3.13.0rc1)
   - [x] [feat(docker): Replace `use_dill` with `serializer` (#41356)](https://github.com/apache/airflow/pull/41356): @phi-friday
## Provider [elasticsearch: 5.5.0rc1](https://pypi.org/project/apache-airflow-providers-elasticsearch/5.5.0rc1)
   - [x] [add execute_sql API to fix ElasticSearchSQLHook (#41537)](https://github.com/apache/airflow/pull/41537): @Owen-CH-Leung
     Linked issues:
       - [x] [Linked Issue #41486](https://github.com/apache/airflow/issues/41486): @Pooort
## Provider [fab: 1.3.0rc1](https://pypi.org/project/apache-airflow-providers-fab/1.3.0rc1)
   - [x] [Feature: Allow set Dag Run resource into Dag Level permission (#40703)](https://github.com/apache/airflow/pull/40703): @joaopamaral
   - [x] [Remove deprecated SubDags (#41390)](https://github.com/apache/airflow/pull/41390): @kaxil
## Provider [google: 10.22.0rc1](https://pypi.org/project/apache-airflow-providers-google/10.22.0rc1)
   - [x] [Add `CloudRunServiceHook` and `CloudRunCreateServiceOperator` (#40008)](https://github.com/apache/airflow/pull/40008): @jx2lee
     Linked issues:
       - [x] [Linked Issue #38760](https://github.com/apache/airflow/issues/38760): @ophie200
   - [ ] [add missing sync_hook_class to CloudDataTransferServiceAsyncHook (#41417)](https://github.com/apache/airflow/pull/41417): @Lee-W
   - [x] [Refactor DataprocCreateBatchOperator and Dataproc system tests (#41527)](https://github.com/apache/airflow/pull/41527): @moiseenkov
   - [x] [Upgrade package gcloud-aio-auth>=5.2.0 (#41262)](https://github.com/apache/airflow/pull/41262): @moiseenkov
## Provider [microsoft.azure: 10.4.0rc1](https://pypi.org/project/apache-airflow-providers-microsoft-azure/10.4.0rc1)
   - [x] [Microsoft Power BI operator to refresh the dataset (#40356)](https://github.com/apache/airflow/pull/40356): @ambika-garg
   - [x] [Export Azure Container Instance log messages to XCOM (#41142)](https://github.com/apache/airflow/pull/41142): @perry2of5
## Provider [microsoft.mssql: 3.9.0rc1](https://pypi.org/project/apache-airflow-providers-microsoft-mssql/3.9.0rc1)
   - [x] [Add methodtools as dependency to mssql provider (#41392)](https://github.com/apache/airflow/pull/41392): @potiuk
     Linked issues:
       - [x] [Linked Issue #41330](https://github.com/apache/airflow/issues/41330): @rawwar
## Provider [openlineage: 1.11.0rc1](https://pypi.org/project/apache-airflow-providers-openlineage/1.11.0rc1)
   - [ ] [feat: [openlineage] add debug facet to all events (#41217)](https://github.com/apache/airflow/pull/41217): @kacpermuda
   - [ ] [feat: add fileloc to DAG info in AirflowRunFacet (#41311)](https://github.com/apache/airflow/pull/41311): @kacpermuda
   - [ ] [chore: remove openlineage client deprecated from_environment() method (#41310)](https://github.com/apache/airflow/pull/41310): @kacpermuda
   - [ ] [fix: get task dependencies without serializing task tree to string (#41494)](https://github.com/apache/airflow/pull/41494): @mobuchowski
   - [ ] [fix: return empty data instead of None when OpenLineage on_start method is missing (#41268)](https://github.com/apache/airflow/pull/41268): @kacpermuda
## Provider [papermill: 3.8.0rc1](https://pypi.org/project/apache-airflow-providers-papermill/3.8.0rc1)
   - [ ] [restore python 3.12 support for papermill (#41548)](https://github.com/apache/airflow/pull/41548): @morokosi
## Provider [snowflake: 5.7.0rc1](https://pypi.org/project/apache-airflow-providers-snowflake/5.7.0rc1)
   - [x] [Fix: Pass hook parameters to SnowflakeSqlApiHook and prep them for AP… (#41150)](https://github.com/apache/airflow/pull/41150): @BTeclaw
     Linked issues:
       - [x] [Linked Issue #39622](https://github.com/apache/airflow/issues/39622): @alexcsolomon1
## Provider [yandex: 3.12.0rc1](https://pypi.org/project/apache-airflow-providers-yandex/3.12.0rc1)
   - [ ] [providers/yandex: fix typing (#40997)](https://github.com/apache/airflow/pull/40997): @got686-yandex
## Provider [ydb: 1.3.0rc1](https://pypi.org/project/apache-airflow-providers-ydb/1.3.0rc1)
   - [x] [ydb provider: add database to table name in bulk upsert, use bulk upsert in system test (#41303)](https://github.com/apache/airflow/pull/41303): @uzhastik

<!--

NOTE TO RELEASE MANAGER:

You can move here the providers that have doc-only changes or for which changes are trivial, and
you could assess that they are OK.

-->
All users involved in the PRs:
@morokosi @mobuchowski @borismo @dirrao @phi-friday @Ghoul-SSZ @joaopamaral @BTeclaw @potiuk @vikramaditya91 @uzhastik @ambika-garg @Owen-CH-Leung @moiseenkov @ssilb4 @got686-yandex @kaxil @vincbeck @Le



### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2024-08-19 08:14:55+00:00,[],2024-08-22 10:44:29+00:00,2024-08-22 10:44:29+00:00,https://github.com/apache/airflow/issues/41577,"[('area:providers', ''), ('kind:meta', 'High-level information important to the community'), ('testing status', 'Status of testing releases')]","[{'comment_id': 2296096501, 'issue_id': 2472741471, 'author': 'moiseenkov', 'body': 'Hi,\r\n#41527, #41262 work as expected', 'created_at': datetime.datetime(2024, 8, 19, 9, 25, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296139664, 'issue_id': 2472741471, 'author': 'sc250072', 'body': 'Hi, since there haven’t been any recent updates for the Teradata Provider beyond version 2.5.0, could you explain why a new release version is needed?\r\n\r\nhttps://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fpypi.org%2Fproject%2Fapache-airflow-providers-teradata%2F2.6.0rc1%2F&data=05%7C02%7CSATISH.CHINTHANIPPU%40teradata.com%7Cc798de64676b411f9b7a08dcc0279df7%7C9151cbaafc6b4f4889bb8c4a71982138%7C0%7C0%7C638596523614516170%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C0%7C%7C%7C&sdata=uL1wmpzD4EDpAJZPPvDPVbqC%2BbuDSIhwniZPqKyMQ3M%3D&reserved=0', 'created_at': datetime.datetime(2024, 8, 19, 9, 46, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296167648, 'issue_id': 2472741471, 'author': 'phi-friday', 'body': ""1. #41356 works fine.\r\n2. #41358 I didn't use `spark` so I can't check, but given that `_sql` is simply an alias for `sql`, it should be fine.\r\n3. #41461 I couldn't find a good way to verify `common.sql`, but since we've only removed the part that generates a more detailed error message, it should be fine."", 'created_at': datetime.datetime(2024, 8, 19, 9, 59, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296272842, 'issue_id': 2472741471, 'author': 'potiuk', 'body': '> Hi, since there haven’t been any recent updates for the Teradata Provider beyond version 2.5.0, could you explain why a new release version is needed?\r\n\r\nAccording to our rules - periodically we bump all providers min-airflow version. https://github.com/apache/airflow/blob/main/PROVIDERS.rst#upgrading-minimum-supported-version-of-airflow -> then we relaease all providers with min-airflow version bumped - we also remove all pre-min-airflow backports, this allows to keep airflow providers free from back-compatibility issues.', 'created_at': datetime.datetime(2024, 8, 19, 10, 46, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296307037, 'issue_id': 2472741471, 'author': 'potiuk', 'body': 'Checked that all  my changes are in.', 'created_at': datetime.datetime(2024, 8, 19, 11, 5, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296550520, 'issue_id': 2472741471, 'author': 'jx2lee', 'body': '@eladkal \r\n#40008 works fine! (unittest & example run)', 'created_at': datetime.datetime(2024, 8, 19, 13, 13, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296644731, 'issue_id': 2472741471, 'author': 'Owen-CH-Leung', 'body': '@eladkal the `ElasticSearchSQLHook` is now working as expected. \r\n\r\n![image](https://github.com/user-attachments/assets/6b94fd51-869f-43fd-9fbb-c5dfd2bc4eff)', 'created_at': datetime.datetime(2024, 8, 19, 13, 56, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296695690, 'issue_id': 2472741471, 'author': 'vikramaditya91', 'body': '> https://github.com/apache/airflow/pull/41256: @vikramaditya91\r\n\r\nWorks fine', 'created_at': datetime.datetime(2024, 8, 19, 14, 18, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296804228, 'issue_id': 2472741471, 'author': 'kacpermuda', 'body': ""#41494 tried to fix the OOM error in the scheduler that the OpenLineage can cause when generating a dag_tree from a huge DAG (related issue: #41505). It works but we've just got information about another production case where the scheduler went OOM with another complex DAG. There is a fix prepared in #41587 that will remove the dag_tree entirely so there will no more errors like this. I'd like to **request an rc2 for OpenLineage provider** (@eladkal) that will include that fix, as it is a bug that can cause some problems in bigger deployments."", 'created_at': datetime.datetime(2024, 8, 19, 15, 5, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2297347779, 'issue_id': 2472741471, 'author': 'joaopamaral', 'body': 'Tested https://github.com/apache/airflow/pull/40703 with both `access_control` formats and it\'s working fine:\r\n\r\n<img width=""1468"" alt=""image"" src=""https://github.com/user-attachments/assets/7ade62db-13cf-448c-a489-6f8d327af0b7"">', 'created_at': datetime.datetime(2024, 8, 19, 20, 2, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2298013178, 'issue_id': 2472741471, 'author': 'eladkal', 'body': '@kacpermuda I will exclude openlineage from this release', 'created_at': datetime.datetime(2024, 8, 20, 5, 44, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2298464877, 'issue_id': 2472741471, 'author': 'uzhastik', 'body': 'ydb provider works fine: https://github.com/apache/airflow/pull/41303', 'created_at': datetime.datetime(2024, 8, 20, 9, 59, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2298672243, 'issue_id': 2472741471, 'author': 'ambika-garg', 'body': 'Hi, #40356 work as expected', 'created_at': datetime.datetime(2024, 8, 20, 11, 53, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2298758340, 'issue_id': 2472741471, 'author': 'dirrao', 'body': 'Hi, \r\nhttps://github.com/apache/airflow/pull/41372 address the documentation changes. So, no functionality change.', 'created_at': datetime.datetime(2024, 8, 20, 12, 38, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2298843235, 'issue_id': 2472741471, 'author': 'BTeclaw', 'body': '#41150 - Works as expected - unit test + functional check, details below:\r\n1. Connection definition (different role and warehouse than is used on Operator definition)\r\n![connection_definition](https://github.com/user-attachments/assets/d3c13d4d-19dc-4f8d-84b6-ae64cca3dbd4)\r\n2. DAG definition, mind the different warehouse, role and schema\r\n![dag_definition](https://github.com/user-attachments/assets/a873b285-3ca5-4add-b081-9765ea67c04f)\r\n3. Queries executed on a warehouse defined when declaring the SnowflakeSqlApiOperator and not the connection\r\n![proper_warehouse](https://github.com/user-attachments/assets/dffec819-5d62-4521-97ce-c45097ecd54c)\r\n4. DDL executing role is also properly forwarded through the SnowflakeSqlApiOperator\r\n![proper_owner_of_table](https://github.com/user-attachments/assets/03612dc9-cc3d-4087-a38e-0c53733cf420)', 'created_at': datetime.datetime(2024, 8, 20, 13, 18, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2302383194, 'issue_id': 2472741471, 'author': 'perry2of5', 'body': '41142 passes test, but:\r\n\r\nI noticed is that starting with  2.9.3 with  microsoft-azure providers 10.3.0 the return value quits being put into XCOM (it is blank in the UI). Then with airflow 2.10.0 with microsoft-azure providers 10.3.0 the key shows up in XCOM but it says ""No value found for XCom key"". So something broke between 2.9.2 / 10.1.2 and 2.10.0/10.3.0.\r\n\r\nWith all that said, my actual change to put the last line (or all lines) of the logs into XCOM worked. So, I think we need a new defect logged to see why return value isn\'t showing up correctly any more.\r\n\r\nHere is my operator. I\'d been testing my changes with do_xcom_push=False since I didn\'t care about the normal return_value...obscured the fact something else broke :(\r\n\r\n```\r\n  aciOperator = AzureContainerInstancesOperator(\r\n      ci_conn_id=""azure-container-instance-conn-id"",\r\n      registry_conn_id=""acr-conn-id"",\r\n      resource_group=""redacted"",\r\n      name=""http2blob{{ ds }}"",\r\n      image=\'redacted\',\r\n      region=""WestUS2"",\r\n      environment_variables={\r\n      redacted\r\n      },\r\n      volumes=[],\r\n      memory_in_gb=1.0,\r\n      cpu=1.0,\r\n      task_id=""start-download-aci"",\r\n      retries=0,\r\n      do_xcom_push=True,\r\n      # xcom_all=True,\r\n      post_execute=_post_execute,\r\n  )\r\n```\r\n\r\nAlso, I did some more testing and multiple_outputs=True also fails back in 2.9.2 with microsoft-azure providers 10.1.2. This actually makes sense because the operator returns a single value, not a dictionary so I think this wasn\'t actually an issue. So, I\'m saying this was tester error unless someone tells me otherwise.', 'created_at': datetime.datetime(2024, 8, 21, 15, 31, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2302718898, 'issue_id': 2472741471, 'author': 'eladkal', 'body': '@perry2of5 there is only 1 question relevant here. Is there regression in `apache-airflow-providers-microsoft-azure` from `10.4.0rc1` to `10.3.0` ? All the rest is possible bugs that do not affect our decision about releasing. \r\n\r\nPlease clarify explicitly what worked on `10.3.0` and does not work anymore on `10.4.0rc1`', 'created_at': datetime.datetime(2024, 8, 21, 18, 34, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2302739131, 'issue_id': 2472741471, 'author': 'perry2of5', 'body': 'I did not find any regression from 10.3.0 to 10.4.0rc1.', 'created_at': datetime.datetime(2024, 8, 21, 18, 47, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304051098, 'issue_id': 2472741471, 'author': 'eladkal', 'body': ""> I did not find any regression from 10.3.0 to 10.4.0rc1.\r\n\r\nThen it's not blocking the release.\r\nFeel free to raise PR to address the bugs you mentioned"", 'created_at': datetime.datetime(2024, 8, 22, 8, 12, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2304350654, 'issue_id': 2472741471, 'author': 'eladkal', 'body': 'Thank you everyone. Providers are released.\r\nProvider `openlineage` is excluded and will followup with rc2\r\n\r\nI invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).', 'created_at': datetime.datetime(2024, 8, 22, 10, 44, 29, tzinfo=datetime.timezone.utc)}]","moiseenkov on (2024-08-19 09:25:33 UTC): Hi,
#41527, #41262 work as expected

sc250072 on (2024-08-19 09:46:12 UTC): Hi, since there haven’t been any recent updates for the Teradata Provider beyond version 2.5.0, could you explain why a new release version is needed?

https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fpypi.org%2Fproject%2Fapache-airflow-providers-teradata%2F2.6.0rc1%2F&data=05%7C02%7CSATISH.CHINTHANIPPU%40teradata.com%7Cc798de64676b411f9b7a08dcc0279df7%7C9151cbaafc6b4f4889bb8c4a71982138%7C0%7C0%7C638596523614516170%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C0%7C%7C%7C&sdata=uL1wmpzD4EDpAJZPPvDPVbqC%2BbuDSIhwniZPqKyMQ3M%3D&reserved=0

phi-friday on (2024-08-19 09:59:21 UTC): 1. #41356 works fine.
2. #41358 I didn't use `spark` so I can't check, but given that `_sql` is simply an alias for `sql`, it should be fine.
3. #41461 I couldn't find a good way to verify `common.sql`, but since we've only removed the part that generates a more detailed error message, it should be fine.

potiuk on (2024-08-19 10:46:16 UTC): According to our rules - periodically we bump all providers min-airflow version. https://github.com/apache/airflow/blob/main/PROVIDERS.rst#upgrading-minimum-supported-version-of-airflow -> then we relaease all providers with min-airflow version bumped - we also remove all pre-min-airflow backports, this allows to keep airflow providers free from back-compatibility issues.

potiuk on (2024-08-19 11:05:09 UTC): Checked that all  my changes are in.

jx2lee on (2024-08-19 13:13:44 UTC): @eladkal 
#40008 works fine! (unittest & example run)

Owen-CH-Leung on (2024-08-19 13:56:19 UTC): @eladkal the `ElasticSearchSQLHook` is now working as expected. 

![image](https://github.com/user-attachments/assets/6b94fd51-869f-43fd-9fbb-c5dfd2bc4eff)

vikramaditya91 on (2024-08-19 14:18:40 UTC): Works fine

kacpermuda on (2024-08-19 15:05:19 UTC): #41494 tried to fix the OOM error in the scheduler that the OpenLineage can cause when generating a dag_tree from a huge DAG (related issue: #41505). It works but we've just got information about another production case where the scheduler went OOM with another complex DAG. There is a fix prepared in #41587 that will remove the dag_tree entirely so there will no more errors like this. I'd like to **request an rc2 for OpenLineage provider** (@eladkal) that will include that fix, as it is a bug that can cause some problems in bigger deployments.

joaopamaral on (2024-08-19 20:02:25 UTC): Tested https://github.com/apache/airflow/pull/40703 with both `access_control` formats and it's working fine:

<img width=""1468"" alt=""image"" src=""https://github.com/user-attachments/assets/7ade62db-13cf-448c-a489-6f8d327af0b7"">

eladkal (Issue Creator) on (2024-08-20 05:44:26 UTC): @kacpermuda I will exclude openlineage from this release

uzhastik on (2024-08-20 09:59:17 UTC): ydb provider works fine: https://github.com/apache/airflow/pull/41303

ambika-garg on (2024-08-20 11:53:39 UTC): Hi, #40356 work as expected

dirrao on (2024-08-20 12:38:19 UTC): Hi, 
https://github.com/apache/airflow/pull/41372 address the documentation changes. So, no functionality change.

BTeclaw on (2024-08-20 13:18:44 UTC): #41150 - Works as expected - unit test + functional check, details below:
1. Connection definition (different role and warehouse than is used on Operator definition)
![connection_definition](https://github.com/user-attachments/assets/d3c13d4d-19dc-4f8d-84b6-ae64cca3dbd4)
2. DAG definition, mind the different warehouse, role and schema
![dag_definition](https://github.com/user-attachments/assets/a873b285-3ca5-4add-b081-9765ea67c04f)
3. Queries executed on a warehouse defined when declaring the SnowflakeSqlApiOperator and not the connection
![proper_warehouse](https://github.com/user-attachments/assets/dffec819-5d62-4521-97ce-c45097ecd54c)
4. DDL executing role is also properly forwarded through the SnowflakeSqlApiOperator
![proper_owner_of_table](https://github.com/user-attachments/assets/03612dc9-cc3d-4087-a38e-0c53733cf420)

perry2of5 on (2024-08-21 15:31:26 UTC): 41142 passes test, but:

I noticed is that starting with  2.9.3 with  microsoft-azure providers 10.3.0 the return value quits being put into XCOM (it is blank in the UI). Then with airflow 2.10.0 with microsoft-azure providers 10.3.0 the key shows up in XCOM but it says ""No value found for XCom key"". So something broke between 2.9.2 / 10.1.2 and 2.10.0/10.3.0.

With all that said, my actual change to put the last line (or all lines) of the logs into XCOM worked. So, I think we need a new defect logged to see why return value isn't showing up correctly any more.

Here is my operator. I'd been testing my changes with do_xcom_push=False since I didn't care about the normal return_value...obscured the fact something else broke :(

```
  aciOperator = AzureContainerInstancesOperator(
      ci_conn_id=""azure-container-instance-conn-id"",
      registry_conn_id=""acr-conn-id"",
      resource_group=""redacted"",
      name=""http2blob{{ ds }}"",
      image='redacted',
      region=""WestUS2"",
      environment_variables={
      redacted
      },
      volumes=[],
      memory_in_gb=1.0,
      cpu=1.0,
      task_id=""start-download-aci"",
      retries=0,
      do_xcom_push=True,
      # xcom_all=True,
      post_execute=_post_execute,
  )
```

Also, I did some more testing and multiple_outputs=True also fails back in 2.9.2 with microsoft-azure providers 10.1.2. This actually makes sense because the operator returns a single value, not a dictionary so I think this wasn't actually an issue. So, I'm saying this was tester error unless someone tells me otherwise.

eladkal (Issue Creator) on (2024-08-21 18:34:25 UTC): @perry2of5 there is only 1 question relevant here. Is there regression in `apache-airflow-providers-microsoft-azure` from `10.4.0rc1` to `10.3.0` ? All the rest is possible bugs that do not affect our decision about releasing. 

Please clarify explicitly what worked on `10.3.0` and does not work anymore on `10.4.0rc1`

perry2of5 on (2024-08-21 18:47:10 UTC): I did not find any regression from 10.3.0 to 10.4.0rc1.

eladkal (Issue Creator) on (2024-08-22 08:12:09 UTC): Then it's not blocking the release.
Feel free to raise PR to address the bugs you mentioned

eladkal (Issue Creator) on (2024-08-22 10:44:29 UTC): Thank you everyone. Providers are released.
Provider `openlineage` is excluded and will followup with rc2

I invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).

"
2472360787,issue,closed,completed,Gets a `DatasetEvent` that was created later than `logical_date`.,"### Apache Airflow version

2.10.0rc1

### If ""Other Airflow 2 version"" selected, which one?

2.10.0(not rc)

### What happened?

I was expecting to be able to access the `DatasetEvent` of the currently triggered `DatasetEvent`, but it accesses the last created `DatasetEvent`.

### What you think should happen instead?

Only `DatasetEvent` created before `logical_date` should be looked up.

### How to reproduce

```python
from __future__ import annotations

from collections.abc import Generator

from airflow.datasets import Dataset, DatasetAlias
from airflow.datasets.metadata import Metadata
from airflow.decorators import dag, task
from airflow.operators.bash import BashOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from pendulum import datetime

dataset_alias = DatasetAlias(""my_dataset_alias"")
dataset = Dataset(""my_dataset"")


@dag(start_date=datetime(2024, 1, 1), schedule=None, catchup=False)
def dataset_producer() -> None:
    @task.python(outlets=[dataset_alias])
    def with_extra() -> Generator[Metadata, None, None]:
        import json

        from airflow.operators.python import get_current_context

        context = get_current_context()
        conf = dict(context[""dag_run""].conf)
        conf = json.dumps(conf)

        yield Metadata(
            target=""my_dataset"",
            extra={""key"": ""value"", ""conf"": conf},
            alias=""my_dataset_alias"",
        )

    with_extra()


@dag(start_date=datetime(2024, 1, 1), schedule=None, catchup=False)
def dataset_producer_twice() -> None:
    first = TriggerDagRunOperator(
        task_id=""first"",
        trigger_dag_id=""dataset_producer"",
        deferrable=False,
        wait_for_completion=False,
        conf={""task_id"": ""first""},
    )
    second = TriggerDagRunOperator(
        task_id=""second"",
        trigger_dag_id=""dataset_producer"",
        deferrable=False,
        wait_for_completion=False,
        conf={""task_id"": ""second""},
    )
    wait = BashOperator(task_id=""wait"", bash_command=""sleep 5"")
    _ = [first, wait] >> second


@dag(start_date=datetime(2024, 1, 1), schedule=dataset_alias, catchup=False)
def dataset_consumer() -> None:
    wait = BashOperator(task_id=""wait"", bash_command=""sleep 10"")

    @task.python(inlets=[dataset_alias])
    def consume_dataset() -> None:
        from pprint import pprint

        from airflow.datasets import DatasetAlias
        from airflow.operators.python import get_current_context

        context = get_current_context()
        inlet_events = context[""inlet_events""]
        target_events = inlet_events[DatasetAlias(""my_dataset_alias"")]
        event = target_events[-1]
        pprint(event.extra)

    show = consume_dataset()
    _ = wait >> show


dataset_producer()
dataset_consumer()
dataset_producer_twice()
```

You can see the same logs in both `DagRun`s.
```[2024-08-19, 12:06:20 KST] {logging_mixin.py:190} INFO - {'conf': '{""task_id"": ""second""}', 'key': 'value'}``` However, the first `DagRun` was triggered by the following `DatasetEvent `.
![스크린샷 2024-08-19 오후 12 13 32](https://github.com/user-attachments/assets/9d58b452-69d1-4e46-a25d-7f0f16fb951c)


### Operating System

PRETTY_NAME=""Debian GNU/Linux 12 (bookworm)"" NAME=""Debian GNU/Linux"" VERSION_ID=""12"" VERSION=""12 (bookworm)"" VERSION_CODENAME=bookworm ID=debian HOME_URL=""https://www.debian.org/"" SUPPORT_URL=""https://www.debian.org/support"" BUG_REPORT_URL=""https://bugs.debian.org/""

### Versions of Apache Airflow Providers

apache-airflow-providers-celery==3.7.3
apache-airflow-providers-common-compat==1.1.0
apache-airflow-providers-common-io==1.4.0
apache-airflow-providers-common-sql==1.15.0
apache-airflow-providers-docker==3.12.3
apache-airflow-providers-fab==1.2.2
apache-airflow-providers-ftp==3.10.1
apache-airflow-providers-http==4.12.0
apache-airflow-providers-imap==3.6.1
apache-airflow-providers-jdbc==4.4.0
apache-airflow-providers-odbc==4.6.3
apache-airflow-providers-postgres==5.11.3
apache-airflow-providers-redis==3.7.1
apache-airflow-providers-smtp==1.7.1
apache-airflow-providers-sqlite==3.8.2

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",phi-friday,2024-08-19 03:11:36+00:00,[],2024-08-19 03:26:32+00:00,2024-08-19 03:26:32+00:00,https://github.com/apache/airflow/issues/41571,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('area:datasets', 'Issues related to the datasets feature')]","[{'comment_id': 2295595760, 'issue_id': 2472360787, 'author': 'phi-friday', 'body': 'This is solved by using `triggering_dataset_events`.  😅', 'created_at': datetime.datetime(2024, 8, 19, 3, 26, 32, tzinfo=datetime.timezone.utc)}]","phi-friday (Issue Creator) on (2024-08-19 03:26:32 UTC): This is solved by using `triggering_dataset_events`.  😅

"
2471999638,issue,closed,completed,callback without __name__ are not allowed when taskinstance executing,"### Apache Airflow version

2.10.0

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

_run_finished_callback now logs

```
callback.__name__
```

however, BaseNotifier without .__name__ attribute either.

It can be a bug ""BaseNotifer"" allowed in the callbacks of a dag

Or it just missing of documentation or incorrect documentation of [callback](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/callbacks.html)

### What you think should happen instead?

if we allow Notifer as callback,
then we must make function/Notfier callback execute correctly. i.e., modify the log of `callback.__name__`.

if we believe callbacks must with "".__name__"" attributes, documents required.

### How to reproduce

the example dag will fail with airflow=2.10.0

```python
import datetime
import pendulum

from airflow import DAG
from airflow.operators.empty import EmptyOperator
from airflow.providers.discord.notifications.discord import DiscordNotifier
from airflow.providers.slack.notifications.slack_webhook import send_slack_webhook_notification


def task_failure_alert(context):
    print(f""Task has failed, task_instance_key_str: {context['task_instance_key_str']}"")

def dag_success_alert(context):
    print(f""DAG has succeeded, run_id: {context['run_id']}"")

with DAG(
    dag_id=""example_callback"",
    schedule=None,
    start_date=pendulum.datetime(2021, 1, 1, tz=""UTC""),
    dagrun_timeout=datetime.timedelta(minutes=60),
    catchup=False,
    on_success_callback=[
      send_slack_webhook_notification(slack_webhook_conn_id=""slack_default"", text="" {{ dag.dag_id }}""),
      DiscordNotifier(discord_conn_id=""discord_default"", text= {{ dag.dag_id }}""),
    ],
    on_failure_callback=task_failure_alert,
    tags=[""example""],
):
    task1 = EmptyOperator(task_id=""task1"")
    task2 = EmptyOperator(task_id=""task2"")
    task3 = EmptyOperator(task_id=""task3"", on_success_callback=[dag_success_alert])
    task1 >> task2 >> task3
```

### Operating System

Debian GNU/Linux 12

### Versions of Apache Airflow Providers

apache-airflow-providers-discord         | 3.7.1
apache-airflow-providers-slack           | 8.8.0


### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",obarisk,2024-08-18 14:29:40+00:00,[],2024-08-23 03:00:59+00:00,2024-08-23 03:00:59+00:00,https://github.com/apache/airflow/issues/41563,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:core', ''), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2296053421, 'issue_id': 2471999638, 'author': 'tirkarthi', 'body': 'It looks like `__name__` is not present on instance of a class and in this case a notifier instance is used in the callback. The change was added in #38892.\r\n\r\nDocs : `https://docs.python.org/3.10/library/stdtypes.html#definition.__name__`\r\n\r\n> definition.__name__[¶](https://docs.python.org/3.10/library/stdtypes.html#definition.__name__)\r\n> The name of the class, function, method, descriptor, or generator instance.\r\n\r\nhttps://stackoverflow.com/questions/14514838/why-do-python-instances-have-no-name-attribute\r\n\r\ncc: @romsharon98', 'created_at': datetime.datetime(2024, 8, 19, 9, 4, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296830017, 'issue_id': 2471999638, 'author': 'obarisk', 'body': 'try to fix it.\r\n\r\nhttps://github.com/apache/airflow/pull/41591', 'created_at': datetime.datetime(2024, 8, 19, 15, 17, 4, tzinfo=datetime.timezone.utc)}]","tirkarthi on (2024-08-19 09:04:26 UTC): It looks like `__name__` is not present on instance of a class and in this case a notifier instance is used in the callback. The change was added in #38892.

Docs : `https://docs.python.org/3.10/library/stdtypes.html#definition.__name__`


https://stackoverflow.com/questions/14514838/why-do-python-instances-have-no-name-attribute

cc: @romsharon98

obarisk (Issue Creator) on (2024-08-19 15:17:04 UTC): try to fix it.

https://github.com/apache/airflow/pull/41591

"
2471963927,issue,open,,Values in SparkKubernetesOperator YAML needs to be defined,"### Apache Airflow Provider(s)

cncf-kubernetes

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes==8.3.1

### Apache Airflow version

2.9.2

### Operating System

Ubuntu 24.04 LTS

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### What happened

I want to create a SparkKubernetesOperator task. So I make a manifest and pass it from params. My manifest:

```yaml
spark:
# spark spec
kubernetes:
  env_vars: []
# other kubernetes spec
```

If I don't set `env_vars`, it raises an error:
```
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py"", line 401, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/spark_kubernetes.py"", line 283, in execute
    self.launcher = CustomObjectLauncher(
                    ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/custom_object_launcher.py"", line 221, in __init__
    self.body: dict = self.get_body()
                      ^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/custom_object_launcher.py"", line 242, in get_body
    k8s_spec: dict = KubernetesSpec(**self.template_body[""kubernetes""])
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/custom_object_launcher.py"", line 84, in __init__
    self.set_attribute()
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/cncf/kubernetes/operators/custom_object_launcher.py"", line 87, in set_attribute
    self.env_vars = convert_env_vars(self.env_vars) if self.env_vars else []
                                                       ^^^^^^^^^^^^^
AttributeError: 'KubernetesSpec' object has no attribute 'env_vars'
```

It can be happened for all variables in `kubernetes` spec.

### What you think should happen instead

This variable has a default value.

### How to reproduce

Just remove the line that `env_vars` exists.

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",amirshabanics,2024-08-18 13:10:24+00:00,[],2024-11-11 12:16:43+00:00,,https://github.com/apache/airflow/issues/41562,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]","[{'comment_id': 2295257736, 'issue_id': 2471963927, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 18, 13, 10, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2384306222, 'issue_id': 2471963927, 'author': 'gopidesupavan', 'body': 'If you dont need `env_vars`, then the `kubernetes` block its self is not required? are you passing any other variables under `kubernetes` except env_vars? if none of the variables your using you can simply remove the block it should work.', 'created_at': datetime.datetime(2024, 9, 30, 22, 44, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2463006887, 'issue_id': 2471963927, 'author': 'Flametaa', 'body': ""+1 on this. I also think that we should add a documentation on how the `template_spec` should be structured. It's not really user friendly right now. I am willing to contribute to this if needed."", 'created_at': datetime.datetime(2024, 11, 7, 19, 3, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2463011229, 'issue_id': 2471963927, 'author': 'amirshabanics', 'body': '> If you dont need `env_vars`, then the `kubernetes` block its self is not required? are you passing any other variables under `kubernetes` except env_vars? if none of the variables your using you can simply remove the block it should work.\r\n\r\nYou can try. If I remove any of the Kubernetes variables raise an exception.', 'created_at': datetime.datetime(2024, 11, 7, 19, 6, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-18 13:10:27 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

gopidesupavan on (2024-09-30 22:44:57 UTC): If you dont need `env_vars`, then the `kubernetes` block its self is not required? are you passing any other variables under `kubernetes` except env_vars? if none of the variables your using you can simply remove the block it should work.

Flametaa on (2024-11-07 19:03:34 UTC): +1 on this. I also think that we should add a documentation on how the `template_spec` should be structured. It's not really user friendly right now. I am willing to contribute to this if needed.

amirshabanics (Issue Creator) on (2024-11-07 19:06:00 UTC): You can try. If I remove any of the Kubernetes variables raise an exception.

"
2470827929,issue,open,,`airflow db migrate` throws dataset alias reference columns and Cryptographic Paramiko warnings,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10

### What happened?

After upgrading to airflow 2.10.0 from 2.8.1, `airflow db migrate` was initiated and two warnings were displayed.

`/anaconda3/lib/python3.9/site-packages/airflow/migrations/versions/0151_2_10_0_dag_schedule_dataset_alias_reference.py:46 SAWarning: Table 'dag_schedule_dataset_alias_reference' specifies columns 'dag_id' as primary_key=True, not matching locally specified columns 'alias_id', 'dag_id'; setting the current primary key columns to 'alias_id', 'dag_id'. This warning may become an exception in a future release`

`/anaconda3/lib/python3.9/site-packages/paramiko/transport.py:236 CryptographyDeprecationWarning: Blowfish has been deprecated and will be removed in a future release`

### What you think should happen instead?

The Paramiko warning may work itself out in a future release by not being used anymore, however I am not sure if this warning is normal behavior. The second warning

`/anaconda3/lib/python3.9/site-packages/airflow/migrations/versions/0151_2_10_0_dag_schedule_dataset_alias_reference.py:46 SAWarning: Table 'dag_schedule_dataset_alias_reference' specifies columns 'dag_id' as primary_key=True, not matching locally specified columns 'alias_id', 'dag_id'; setting the current primary key columns to 'alias_id', 'dag_id'. This warning may become an exception in a future release`

...is more of a concern and seems to be releated to column names.

### How to reproduce

OS: CentOS7

pip list

```
Package                                  Version
---------------------------------------- --------------------
aiohttp                                  3.8.1
aiosignal                                1.2.0
alabaster                                0.7.12
alembic                                  1.13.1
anaconda-client                          1.9.0
anaconda-navigator                       2.1.1
anaconda-project                         0.10.1
annotated-types                          0.6.0
anyio                                    3.5.0
apache-airflow                           2.10.0
apache-airflow-providers-amazon          8.7.0
apache-airflow-providers-common-compat   1.1.0
apache-airflow-providers-common-io       1.3.0
apache-airflow-providers-common-sql      1.7.2
apache-airflow-providers-fab             1.0.2
apache-airflow-providers-ftp             3.5.2
apache-airflow-providers-http            4.5.2
apache-airflow-providers-imap            3.3.2
apache-airflow-providers-microsoft-mssql 3.4.2
apache-airflow-providers-mysql           5.3.1
apache-airflow-providers-postgres        5.6.1
apache-airflow-providers-slack           7.3.1
apache-airflow-providers-smtp            1.6.1
apache-airflow-providers-sqlite          3.4.3
apache-airflow-providers-ssh             3.7.3
apispec                                  6.3.0
appdirs                                  1.4.4
argcomplete                              1.12.3
argh                                     0.26.2
argon2-cffi                              20.1.0
arrow                                    0.13.1
asgiref                                  3.6.0
asn1crypto                               1.5.1
astroid                                  2.6.6
astropy                                  4.3.1
async-generator                          1.10
async-timeout                            4.0.2
atomicwrites                             1.4.0
attrs                                    23.1.0
autopep8                                 1.5.7
azure-common                             1.1.28
azure-nspkg                              3.0.2
Babel                                    2.10.1
backcall                                 0.2.0
backoff                                  2.2.1
backports.functools-lru-cache            1.6.4
backports.shutil-get-terminal-size       1.0.0
backports.tempfile                       1.0
backports.weakref                        1.0.post1
bcrypt                                   3.2.0
beautifulsoup4                           4.10.0
billiard                                 3.6.4.0
binaryornot                              0.4.4
bitarray                                 2.3.0
bkcharts                                 0.2
black                                    19.10b0
bleach                                   4.0.0
blinker                                  1.7.0
bokeh                                    2.4.3
boto                                     2.49.0
boto3                                    1.28.45
botocore                                 1.31.45
Bottleneck                               1.3.2
brotlipy                                 0.7.0
cached-property                          1.5.2
cachelib                                 0.9.0
cachetools                               4.2.2
cattrs                                   22.1.0
certifi                                  2020.12.5
cffi                                     1.15.0
chardet                                  4.0.0
charset-normalizer                       2.0.12
click                                    8.1.2
clickclick                               20.10.2
cloudpickle                              2.0.0
clyent                                   1.2.2
colorama                                 0.4.4
colorlog                                 6.8.2
commonmark                               0.9.1
conda                                    4.10.3
conda-build                              3.21.5
conda-content-trust                      0+unknown
conda-pack                               0.6.0
conda-package-handling                   1.7.3
conda-repo-cli                           1.0.4
conda-token                              0.3.0
conda-verify                             3.4.2
ConfigUpdater                            3.1.1
confire                                  0.2.0
connexion                                2.14.2
contextlib2                              0.6.0.post1
crcmod                                   1.7
cron-descriptor                          1.2.24
croniter                                 2.0.3
cryptography                             42.0.5
cx-Oracle                                8.3.0
cycler                                   0.10.0
Cython                                   0.29.24
cytoolz                                  0.11.0
daal4py                                  2021.3.0
dask                                     2022.10.2
debugpy                                  1.4.1
decorator                                5.1.0
defusedxml                               0.7.1
Deprecated                               1.2.13
diff-match-patch                         20200713
dill                                     0.3.1.1
distlib                                  0.3.4
distributed                              2022.10.2
dnspython                                2.2.1
docopt                                   0.6.2
docutils                                 0.18.1
email-validator                          1.2.0
entrypoints                              0.3
et-xmlfile                               1.1.0
eventlet                                 0.33.0
exceptiongroup                           1.0.0rc9
execnet                                  1.9.0
executing                                0.8.3
fastavro                                 1.4.10
fastcache                                1.1.0
fastjsonschema                           2.15.3
filelock                                 3.6.0
fissix                                   21.11.13
flake8                                   3.9.2
flaky                                    3.7.0
Flask                                    2.2.2
Flask-AppBuilder                         4.4.1
Flask-Babel                              2.0.0
Flask-Caching                            2.3.0
Flask-JWT-Extended                       4.4.2
Flask-Limiter                            3.3.0
Flask-Login                              0.6.2
Flask-OpenID                             1.3.0
Flask-Session                            0.4.0
Flask-SQLAlchemy                         2.5.1
Flask-WTF                                1.2.1
fonttools                                4.25.0
frozenlist                               1.3.0
fsspec                                   2023.12.2
future                                   0.18.2
geoip2                                   4.5.0
geomet                                   0.2.1.post1
gevent                                   21.8.0
glob2                                    0.7
gmpy2                                    2.0.8
google-analytics-data                    0.12.1
google-api-core                          1.31.5
google-api-python-client                 2.52.0
google-auth                              1.35.0
google-auth-httplib2                     0.1.0
google-auth-oauthlib                     0.5.0
google-crc32c                            1.3.0
google-re2                               1.1
google-resumable-media                   2.3.2
googleapis-common-protos                 1.54.0
graphviz                                 0.20
greenlet                                 1.1.2
grpcio                                   1.44.0
gspread                                  5.8.0
gspread-dataframe                        3.2.2
gunicorn                                 20.1.0
h11                                      0.14.0
h5py                                     3.3.0
HeapDict                                 1.0.1
html5lib                                 1.1
httpcore                                 1.0.5
httplib2                                 0.19.1
httpx                                    0.27.0
humanize                                 4.0.0
hypothesis                               6.54.6
identify                                 2.4.12
idna                                     3.3
ijson                                    3.1.4
imagecodecs                              2021.8.26
imageio                                  2.9.0
imagesize                                1.3.0
importlib_metadata                       7.1.0
importlib-resources                      5.12.0
incremental                              21.3.0
inflection                               0.5.1
iniconfig                                1.1.1
intervaltree                             3.1.0
ipykernel                                6.4.1
ipython                                  7.29.0
ipython-genutils                         0.2.0
ipywidgets                               7.6.5
iso8601                                  1.0.2
isodate                                  0.6.1
isort                                    5.9.3
itsdangerous                             2.1.2
JayDeBeApi                               1.2.3
jdcal                                    1.4.1
jedi                                     0.18.0
jeepney                                  0.8.0
Jinja2                                   3.0.3
jinja2-time                              0.2.0
jmespath                                 0.10.0
joblib                                   1.1.0
JPype1                                   1.3.0
json-merge-patch                         0.2
json5                                    0.9.6
jsondiff                                 2.0.0
jsonpath-ng                              1.5.3
jsonschema                               4.19.0
jsonschema-specifications                2023.7.1
jupyter                                  1.0.0
jupyter-client                           6.1.12
jupyter-console                          6.4.0
jupyter-core                             4.10.0
jupyter-server                           1.4.1
jupyterlab                               3.2.1
jupyterlab-pygments                      0.1.2
jupyterlab-server                        2.8.2
jupyterlab-widgets                       1.0.0
keyring                                  23.1.0
kiwisolver                               1.3.1
krb5                                     0.3.0
kylinpy                                  2.8.4
lazy-object-proxy                        1.7.1
ldap3                                    2.9.1
libarchive-c                             2.9
limits                                   3.4.0
linkify-it-py                            2.0.0
llvmlite                                 0.37.0
locket                                   1.0.0
lockfile                                 0.12.2
luigi                                    3.0.3
lxml                                     4.8.0
mailchimp-marketing                      3.0.80
Mako                                     1.2.0
Markdown                                 3.3.6
markdown-it-py                           2.1.0
MarkupSafe                               2.1.1
marshmallow                              3.20.1
marshmallow-enum                         1.5.1
marshmallow-oneofschema                  3.0.1
marshmallow-sqlalchemy                   0.26.1
matplotlib                               3.4.3
matplotlib-inline                        0.1.3
maxminddb                                2.2.0
mccabe                                   0.6.1
mdit-py-plugins                          0.3.0
mdurl                                    0.1.1
methodtools                              0.4.7
mistune                                  0.8.4
mkl-fft                                  1.3.1
mkl-random                               1.2.2
mkl-service                              2.4.0
mock                                     4.0.3
monotonic                                1.6
more-itertools                           8.10.0
moreorless                               0.4.0
mpmath                                   1.2.1
msgpack                                  1.0.3
multi-key-dict                           2.0.3
multidict                                6.0.2
multipledispatch                         0.6.0
munkres                                  1.1.4
mypy                                     0.910
mypy-boto3-appflow                       1.24.36.post1
mypy-boto3-rds                           1.26.144
mypy-boto3-redshift-data                 1.26.109
mypy-boto3-s3                            1.26.155
mypy-extensions                          0.4.3
mysql-connector-python                   8.0.28
mysqlclient                              2.1.0
navigator-updater                        0.2.1
nbclassic                                0.2.6
nbclient                                 0.5.3
nbconvert                                6.1.0
nbformat                                 5.1.3
neo4j                                    4.4.3
nest-asyncio                             1.5.5
networkx                                 2.6.3
nltk                                     3.6.5
nodeenv                                  1.6.0
nose                                     1.3.7
notebook                                 6.4.5
ntlm-auth                                1.5.0
numba                                    0.54.1
numexpr                                  2.7.3
numpy                                    1.22.3
numpydoc                                 1.1.0
oauth2client                             4.1.3
oauthlib                                 3.2.0
olefile                                  0.46
openapi-schema-validator                 0.1.6
openapi-spec-validator                   0.3.3
openpyxl                                 3.0.9
opentelemetry-api                        1.15.0
opentelemetry-exporter-otlp              1.15.0
opentelemetry-exporter-otlp-proto-grpc   1.15.0
opentelemetry-exporter-otlp-proto-http   1.15.0
opentelemetry-proto                      1.15.0
opentelemetry-sdk                        1.15.0
opentelemetry-semantic-conventions       0.36b0
ordered-set                              4.1.0
orjson                                   3.6.8
oscrypto                                 1.3.0
packaging                                23.2
pandas                                   1.3.4
pandocfilters                            1.4.3
parameterized                            0.8.1
paramiko                                 2.9.2
parso                                    0.8.3
partd                                    1.2.0
path                                     16.0.0
pathlib                                  1.0.1
pathlib2                                 2.3.6
pathspec                                 0.9.0
patsy                                    0.5.2
pbr                                      5.8.1
pendulum                                 2.1.2
pep8                                     1.7.1
petl                                     1.7.8
pexpect                                  4.8.0
pickleshare                              0.7.5
Pillow                                   8.4.0
pip                                      24.2
pipdeptree                               2.2.1
pkginfo                                  1.8.2
platformdirs                             2.5.2
pluggy                                   1.5.0
ply                                      3.11
plyvel                                   1.4.0
portalocker                              2.4.0
poyo                                     0.5.0
prison                                   0.2.1
prometheus-client                        0.14.1
prompt-toolkit                           3.0.29
proto-plus                               1.20.6
protobuf                                 3.20.1
psutil                                   5.9.0
psycopg2                                 2.9.3
psycopg2-binary                          2.9.3
ptyprocess                               0.7.0
pure-eval                                0.2.2
pure-sasl                                0.6.2
py                                       1.11.0
py4j                                     0.10.9.3
pyarrow                                  10.0.0
pyasn1                                   0.4.8
pyasn1-modules                           0.2.8
pycodestyle                              2.8.0
pycosat                                  0.6.3
pycountry                                22.3.5
pycparser                                2.21
pycryptodome                             3.14.1
pycryptodomex                            3.14.1
pycurl                                   7.44.1
pydantic                                 2.5.2
pydantic_core                            2.14.5
pydocstyle                               6.1.1
pydot                                    1.4.2
pyenchant                                3.2.2
pyerfa                                   2.0.0
pyflakes                                 2.4.0
Pygments                                 2.12.0
PyJWT                                    2.6.0
pykerberos                               1.2.4
pylint                                   2.9.6
pyls-spyder                              0.4.0
pymongo                                  3.12.3
pymssql                                  2.2.5
PyNaCl                                   1.5.0
pyodbc                                   4.0.32
pyOpenSSL                                24.1.0
pyparsing                                2.4.7
pyrsistent                               0.18.1
pysftp                                   0.2.9
PySocks                                  1.7.1
pyspark                                  3.2.1
pytest                                   6.2.4
python-daemon                            3.0.1
python-dateutil                          2.8.2
python-http-client                       3.3.7
python-lsp-black                         1.0.0
python-lsp-jsonrpc                       1.0.0
python-lsp-server                        1.2.4
python-nvd3                              0.15.0
python-slugify                           6.1.1
python3-openid                           3.2.0
pytz                                     2021.3
pytzdata                                 2020.1
PyWavelets                               1.1.1
pyxdg                                    0.27
PyYAML                                   5.4.1
pyzmq                                    22.3.0
QDarkStyle                               3.0.2
qstylizer                                0.1.10
QtAwesome                                1.0.2
qtconsole                                5.1.1
QtPy                                     1.10.0
redis                                    3.5.3
redshift-connector                       2.0.904
referencing                              0.30.2
regex                                    2021.8.3
requests                                 2.27.1
requests-oauthlib                        1.3.1
requests-toolbelt                        1.0.0
rfc3339-validator                        0.1.4
rfc3986                                  1.5.0
rich                                     12.6.0
rich_argparse                            1.1.0
rope                                     0.19.0
rpds-py                                  0.10.0
rsa                                      4.8
Rtree                                    0.9.7
ruamel-yaml-conda                        0.15.100
Rx                                       3.2.0
s3transfer                               0.6.1
sasl                                     0.3.1
scikit-image                             0.18.3
scikit-learn                             0.24.2
scikit-learn-intelex                     2021.20210714.170444
scipy                                    1.7.1
scramp                                   1.4.1
seaborn                                  0.11.2
SecretStorage                            3.3.1
semver                                   2.13.0
Send2Trash                               1.8.0
sendgrid                                 6.9.7
sentinels                                1.0.0
sentry-sdk                               1.5.10
setproctitle                             1.3.3
setuptools                               67.7.2
simplegeneric                            0.8.1
singledispatch                           3.7.0
sip                                      4.19.13
six                                      1.16.0
slack-sdk                                3.15.2
slackclient                              1.3.2
smmap                                    5.0.0
sniffio                                  1.2.0
snowballstemmer                          2.2.0
sortedcollections                        2.1.0
sortedcontainers                         2.4.0
soupsieve                                2.3.2.post1
Sphinx                                   5.3.0
sphinxcontrib-applehelp                  1.0.2
sphinxcontrib-devhelp                    1.0.2
sphinxcontrib-htmlhelp                   2.0.0
sphinxcontrib-jsmath                     1.0.1
sphinxcontrib-qthelp                     1.0.3
sphinxcontrib-serializinghtml            1.1.5
sphinxcontrib-websupport                 1.2.4
spyder-kernels                           2.1.3
SQLAlchemy                               1.4.50
SQLAlchemy-JSONField                     1.0.0
sqlalchemy-redshift                      0.8.9
SQLAlchemy-Utils                         0.38.2
sqlparse                                 0.4.2
sshtunnel                                0.4.0
starkbank-ecdsa                          2.0.3
statsd                                   3.3.0
statsmodels                              0.12.2
swagger-ui-bundle                        0.0.9
sympy                                    1.9
tables                                   3.6.1
tabulate                                 0.8.9
TBB                                      0.2
tblib                                    1.7.0
tenacity                                 8.0.1
termcolor                                1.1.0
terminado                                0.9.4
testpath                                 0.5.0
text-unidecode                           1.3
textdistance                             4.2.1
textwrap3                                0.9.2
threadpoolctl                            2.2.0
three-merge                              0.1.1
thrift                                   0.16.0
tifffile                                 2021.7.2
tinycss                                  0.4
toml                                     0.10.2
tomli                                    2.0.1
toolz                                    0.11.2
tornado                                  6.1
tqdm                                     4.64.0
traitlets                                5.1.1
typed-ast                                1.4.3
types-boto                               2.49.14
types-certifi                            2021.10.8.1
types-croniter                           1.0.9
types-cryptography                       3.3.20
types-Deprecated                         1.2.6
types-docutils                           0.18.3
types-freezegun                          1.1.9
types-Markdown                           3.3.13
types-paramiko                           2.8.19
types-protobuf                           3.19.18
types-PyMySQL                            1.0.18
types-python-dateutil                    2.8.12
types-python-slugify                     5.0.4
types-pytz                               2021.3.6
types-PyYAML                             6.0.7
types-redis                              4.2.0
types-requests                           2.27.20
types-setuptools                         57.4.14
types-six                                1.16.15
types-tabulate                           0.8.7
types-termcolor                          1.1.3
types-toml                               0.10.5
types-urllib3                            1.26.13
typing_extensions                        4.9.0
tzdata                                   2022.1
ua-parser                                0.10.0
uc-micro-py                              1.0.1
ujson                                    4.0.2
unicodecsv                               0.14.1
Unidecode                                1.3.4
universal_pathlib                        0.2.2
uritemplate                              3.0.1
urllib3                                  1.26.9
userpath                                 1.8.0
vine                                     5.0.0
volatile                                 2.1.0
watchdog                                 2.1.3
watchtower                               2.0.1
wcwidth                                  0.2.5
webencodings                             0.5.1
websocket-client                         1.3.2
Werkzeug                                 2.2.2
wheel                                    0.37.0
whichcraft                               0.6.1
widgetsnbextension                       3.5.1
wirerope                                 0.4.7
wrapt                                    1.14.0
WTForms                                  2.3.3
wurlitzer                                2.1.1
xlrd                                     2.0.1
XlsxWriter                               3.0.1
xlwt                                     1.3.0
xmltodict                                0.12.0
yamllint                                 1.26.3
yapf                                     0.31.0
yarl                                     1.7.2
zict                                     2.1.0
zipp                                     3.8.0
zope.event                               4.5.0
zope.interface                           5.4.0
```

`pip install --upgrade apache-airflow` (from 2.8.1)
`airflow db migrate`

### Operating System

CentOS 7

### Versions of Apache Airflow Providers

```
apache-airflow-providers-amazon==8.7.0
apache-airflow-providers-common-compat==1.1.0
apache-airflow-providers-common-io==1.3.0
apache-airflow-providers-common-sql==1.7.2
apache-airflow-providers-fab==1.0.2
apache-airflow-providers-ftp==3.5.2
apache-airflow-providers-http==4.5.2
apache-airflow-providers-imap==3.3.2
apache-airflow-providers-microsoft-mssql==3.4.2
apache-airflow-providers-mysql==5.3.1
apache-airflow-providers-postgres==5.6.1
apache-airflow-providers-slack==7.3.1
apache-airflow-providers-smtp==1.6.1
apache-airflow-providers-sqlite==3.4.3
apache-airflow-providers-ssh==3.7.3
```

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

This is the first time this warning has popped up.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",anthony-joyce,2024-08-16 19:19:14+00:00,[],2025-02-02 08:27:32+00:00,,https://github.com/apache/airflow/issues/41547,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:datasets', 'Issues related to the datasets feature'), ('area:db-migrations', 'PRs with DB migration'), ('affected_version:2.10', 'Issues Reported for 2.10')]",[],
2470695755,issue,closed,completed,Add Backfill Button to DAGs Overview and Individual DAG Pages in the Webserver UI,"### Description

This feature request proposes the addition of a ""Backfill"" button to the Apache Airflow webserver UI, both on the DAGs overview page and within individual DAG pages. This button would allow users to easily initiate backfill operations directly from the web interface without needing to use the command line interface (CLI).

![637656DC-1783-42AD-9E75-C7C84A72D4D9](https://github.com/user-attachments/assets/611f47ea-43f9-4264-98a9-8d5f17806089)


### Use case/motivation

Currently, Airflow users must use the CLI to perform backfills, which can be cumbersome and time-consuming, especially for users who are more comfortable with a graphical interface. By adding a ""Backfill"" button to the webserver UI, users would gain the ability to:

- Select a date range directly within the UI for backfilling.
- Initiate backfills with a single click, improving usability and reducing the need to switch between the UI and CLI.
- Monitor the status of the backfill operation within the same interface they use for all other DAG interactions.

This feature would significantly streamline the backfill process, making Airflow more user-friendly for those who prefer using the web UI over the CLI. It would also reduce the risk of errors associated with manual CLI commands, such as incorrect date inputs or DAG IDs.


### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",pedro-cf,2024-08-16 17:41:49+00:00,[],2024-08-16 18:13:48+00:00,2024-08-16 18:13:48+00:00,https://github.com/apache/airflow/issues/41542,"[('kind:feature', 'Feature Requests'), ('duplicate', 'Issue that is duplicated'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2293947151, 'issue_id': 2470695755, 'author': 'potiuk', 'body': 'This is far more than addinga ""button"". This is planned for Airflow 3 and the whole Airlfow Improvement Proposal about how to implement it https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-78+Scheduler-managed+backfill', 'created_at': datetime.datetime(2024, 8, 16, 18, 13, 48, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-08-16 18:13:48 UTC): This is far more than addinga ""button"". This is planned for Airflow 3 and the whole Airlfow Improvement Proposal about how to implement it https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-78+Scheduler-managed+backfill

"
2470542722,issue,closed,completed,Access control error after upgrading from version 2.9.3 to 2.10.0,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.0

### What happened?

After upgrading my Airflow deployment to the latest version, I noticed many DAG Import Errors mentioning access control

### What you think should happen instead?

I have always created my dags with similar permissions such as these: `{""EXAMPLE"":[""can_read"",""can_edit""]}`, on access_control. Unless the documentation hasn't been updated yet, it should still work as intended.

### How to reproduce

Deployment with Airflow 2.10.0 with Python 3.10 and with a dag created with similar access_control: `{""EXAMPLE"":[""can_read"",""can_edit""]}`

### Operating System

Amazon Linux 2

### Versions of Apache Airflow Providers

```
apache-airflow-providers-amazon==8.27.0
apache-airflow-providers-cncf-kubernetes==8.3.4
apache-airflow-providers-common-compat==1.1.0
apache-airflow-providers-common-io==1.4.0
apache-airflow-providers-common-sql==1.15.0
apache-airflow-providers-fab==1.2.2
apache-airflow-providers-ftp==3.10.1
apache-airflow-providers-http==4.12.0
apache-airflow-providers-imap==3.6.1
apache-airflow-providers-mysql==5.6.3
apache-airflow-providers-postgres==5.11.3
apache-airflow-providers-smtp==1.7.1
apache-airflow-providers-sqlite==3.8.2
```

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",FelipeRamos-neuro,2024-08-16 16:01:04+00:00,[],2024-08-20 19:42:07+00:00,2024-08-20 19:42:06+00:00,https://github.com/apache/airflow/issues/41540,"[('kind:bug', 'This is a clearly a bug'), ('area:upgrade', 'Facilitating migration to a newer version of Airflow'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2293802672, 'issue_id': 2470542722, 'author': 'FelipeRamos-neuro', 'body': 'Some prints I forgot to add to the issue\r\n\r\n![Captura de tela de 2024-08-16 12-41-16](https://github.com/user-attachments/assets/450b864b-d9c1-4fff-be3b-74f818d91d66)\r\n![Captura de tela de 2024-08-16 12-41-33](https://github.com/user-attachments/assets/1bdff012-892c-4265-bb02-cc93de98077f)', 'created_at': datetime.datetime(2024, 8, 16, 16, 26, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2294349121, 'issue_id': 2470542722, 'author': 'joaopamaral', 'body': ""Hey @FelipeRamos-neuro, I was able to reproduce the error. I'll open the PR to fix it."", 'created_at': datetime.datetime(2024, 8, 16, 21, 39, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2294409786, 'issue_id': 2470542722, 'author': 'joaopamaral', 'body': ""FYI, the error is that we are changing the access_control format (to include the resource name) in airflow 2.10, but old FAB version doesn't know how to handle it. I'll include in the PR to send the new format only if the FAB version can deal the new format.\r\n\r\nhttps://github.com/apache/airflow/blob/8c2f56375a4560361c5f96d54c92c46674473387/airflow/models/dag.py#L937-L948"", 'created_at': datetime.datetime(2024, 8, 16, 22, 41, 20, tzinfo=datetime.timezone.utc)}]","FelipeRamos-neuro (Issue Creator) on (2024-08-16 16:26:18 UTC): Some prints I forgot to add to the issue

![Captura de tela de 2024-08-16 12-41-16](https://github.com/user-attachments/assets/450b864b-d9c1-4fff-be3b-74f818d91d66)
![Captura de tela de 2024-08-16 12-41-33](https://github.com/user-attachments/assets/1bdff012-892c-4265-bb02-cc93de98077f)

joaopamaral on (2024-08-16 21:39:48 UTC): Hey @FelipeRamos-neuro, I was able to reproduce the error. I'll open the PR to fix it.

joaopamaral on (2024-08-16 22:41:20 UTC): FYI, the error is that we are changing the access_control format (to include the resource name) in airflow 2.10, but old FAB version doesn't know how to handle it. I'll include in the PR to send the new format only if the FAB version can deal the new format.

https://github.com/apache/airflow/blob/8c2f56375a4560361c5f96d54c92c46674473387/airflow/models/dag.py#L937-L948

"
2470453142,issue,open,,Abilitiy to set dag priority,"### Body

When the scheduler chooses which dag runs to create, and which to examine for scheduling, users want to be able to set priorities for this.

user comment: ""i want to set this dag to run first""

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",dstandish,2024-08-16 15:04:58+00:00,[],2024-08-16 15:07:20+00:00,,https://github.com/apache/airflow/issues/41538,"[('area:Scheduler', 'including HA (high availability) scheduler'), ('kind:feature', 'Feature Requests'), ('kind:meta', 'High-level information important to the community'), ('airflow3.0:candidate', 'Potential candidates for Airflow 3.0')]",[],
2470168570,issue,closed,completed,Deprecation of operators should be prominently highlighted instead of the one line remark in the documentation,"### What do you see as an issue?

While working with GCP BigQuery operators, I have noticed that some of the operators are deprecated. As the message was not prominent enough, first I failed to notice it, coded some of my DAGs with the old operators and then needed to re-write them so as to avoid deprecated functions. This is not great when it comes to documentation UX.

For example: https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/bigquery/index.html#airflow.providers.google.cloud.operators.bigquery.BigQueryExecuteQueryOperator

Unless you explicitly double check or look for the deprecation notice, it is very easy to overlook it.

![image](https://github.com/user-attachments/assets/1e6dc85f-ba70-4a8f-869e-f92c2de6b077)


### Solving the problem

I would propose adding the deprecation notice more prominently and changing the title background for additional highlighting.
If we know the operator is deprecated, all it takes is a little bit of CSS magic to make it absolutely stand out, say

- Striking the name through
- Different background color
- Explicit statement of deprecation in the header

I imagine _something_ like this would be good (at least it is hard to miss the point 😊). Again, nothing more than a few lines of CSS should do the trick:

![image](https://github.com/user-attachments/assets/2c8bbea6-9991-4718-857e-c0098812fb57)




### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",peter-gergely-horvath,2024-08-16 12:21:36+00:00,['geraj1010'],2024-11-29 00:52:47+00:00,2024-11-29 00:52:47+00:00,https://github.com/apache/airflow/issues/41532,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('kind:documentation', ''), ('pending-response', ''), ('type:doc-only', 'Changelog: Doc Only')]","[{'comment_id': 2293682944, 'issue_id': 2470168570, 'author': 'kaxil', 'body': 'I like your proposed solution @peter-gergely-horvath , would you like to help out with the PR? Might just be a change in this file:\r\n\r\nhttps://github.com/apache/airflow/blob/main/docs/sphinx_design/static/custom.css', 'created_at': datetime.datetime(2024, 8, 16, 15, 6, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2300677916, 'issue_id': 2470168570, 'author': 'geraj1010', 'body': ""@kaxil I'd be happy to take this on, if the issue is still open."", 'created_at': datetime.datetime(2024, 8, 21, 3, 55, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2302086369, 'issue_id': 2470168570, 'author': 'potiuk', 'body': 'Assigned you', 'created_at': datetime.datetime(2024, 8, 21, 13, 38, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325098897, 'issue_id': 2470168570, 'author': 'peter-gergely-horvath', 'body': ""> I like your proposed solution @peter-gergely-horvath , would you like to help out with the PR? Might just be a change in this file:\r\n> \r\n> https://github.com/apache/airflow/blob/main/docs/sphinx_design/static/custom.css\r\n\r\nSorry guys, I only hacked the rendered page DOM inside the browser's dev tools to show what I mean here on a screenshot. I don't think I understand how the doc pages are generated."", 'created_at': datetime.datetime(2024, 9, 2, 16, 52, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2344384464, 'issue_id': 2470168570, 'author': 'RNHTTR', 'body': ""> > I like your proposed solution @peter-gergely-horvath , would you like to help out with the PR? Might just be a change in this file:\r\n> > https://github.com/apache/airflow/blob/main/docs/sphinx_design/static/custom.css\r\n> \r\n> Sorry guys, I only hacked the rendered page DOM inside the browser's dev tools to show what I mean here on a screenshot. I don't think I understand how the doc pages are generated.\r\n\r\nHere's the [README](https://github.com/apache/airflow/blob/main/docs/README.rst) for building docs and editing docs"", 'created_at': datetime.datetime(2024, 9, 11, 18, 25, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2375486585, 'issue_id': 2470168570, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 9, 26, 0, 14, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2378597002, 'issue_id': 2470168570, 'author': 'peter-gergely-horvath', 'body': 'The task is now picked up by [geraj1010](https://github.com/geraj1010). I look forward to seeing the new design. :)', 'created_at': datetime.datetime(2024, 9, 27, 7, 29, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2405429377, 'issue_id': 2470168570, 'author': 'ferruzzi', 'body': '@geraj1010  Are you still working on this one?', 'created_at': datetime.datetime(2024, 10, 10, 15, 24, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2406282201, 'issue_id': 2470168570, 'author': 'geraj1010', 'body': '@ferruzzi Yes I am. I recently just regained access to my account, so I have been delayed. Happy to collaborate on this.', 'created_at': datetime.datetime(2024, 10, 11, 0, 24, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2407823769, 'issue_id': 2470168570, 'author': 'omkar-foss', 'body': ""Hi, as discussed on [this slack thread](https://apache-airflow.slack.com/archives/CJ1LVREHX/p1728613078488099), there are a few Sphinx directives that can be explored that help do this without direct CSS changes:\r\n\r\n- `.. error::` - https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-error\r\n- `.. note::` - https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-note\r\n- `.. attention::` - https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-attention\r\n\r\nI suppose any that looks fine and catches the user's attention should be good enough.\r\n\r\ncc: @ashb (thanks for the suggestion)"", 'created_at': datetime.datetime(2024, 10, 11, 17, 13, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448718776, 'issue_id': 2470168570, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 10, 31, 0, 15, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461122602, 'issue_id': 2470168570, 'author': 'geraj1010', 'body': 'Greetings, I have been dragging my feet on this issue, but I am starting to look into it now. I think the `attention` or `warning` directive fits for this, as mentioned by @omkar-foss.', 'created_at': datetime.datetime(2024, 11, 7, 1, 16, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2467139167, 'issue_id': 2470168570, 'author': 'geraj1010', 'body': '@peter-gergely-horvath I was able to add some additional highlighting to show that the operator is deprecated. It\'s not exactly like your mockup, but at least it stands out more than just plain text.\r\n\r\n<img width=""617"" alt=""image"" src=""https://github.com/user-attachments/assets/94b1b73a-7547-4732-8967-d98823cbf7ab"">\r\n\r\n Thoughts?', 'created_at': datetime.datetime(2024, 11, 11, 3, 2, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469086590, 'issue_id': 2470168570, 'author': 'potiuk', 'body': 'Looks good to me.', 'created_at': datetime.datetime(2024, 11, 11, 21, 42, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2469674169, 'issue_id': 2470168570, 'author': 'geraj1010', 'body': 'Thank you @potiuk.', 'created_at': datetime.datetime(2024, 11, 12, 5, 59, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2471582721, 'issue_id': 2470168570, 'author': 'ferruzzi', 'body': 'I like it too.  Definitely an improvement.', 'created_at': datetime.datetime(2024, 11, 12, 21, 6, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2471611766, 'issue_id': 2470168570, 'author': 'peter-gergely-horvath', 'body': 'I would love to see title colour change and strike-through, but if that is difficult to implement, I think this is quite fine. :)', 'created_at': datetime.datetime(2024, 11, 12, 21, 23, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2471624463, 'issue_id': 2470168570, 'author': 'kaxil', 'body': 'And it can be iterative too: this is already a good improvement :)', 'created_at': datetime.datetime(2024, 11, 12, 21, 30, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2472363345, 'issue_id': 2470168570, 'author': 'geraj1010', 'body': ""> I would love to see title colour change and strike-through, but if that is difficult to implement, I think this is quite fine. :)\r\n\r\nUnfortunately, I think that would be difficult to implement. From what I understand, the documentation for the operators (and sensors) in the Python API section are gathered via Sphinx's AutoAPI extension at document generation runtime. This means that all styling is done on-the-fly and the `index.rst` files are not directly accessible until after rendering (in a phantom folder called `_api`). \r\n\r\nI was thinking maybe we could achieve the original desired styling, by customizing the AutoAPI extension to look for any classes with the `@deprecated` decorator and apply a special custom `..py:class` directive instead of the normal one. I think that would be very neat, but also a lot more work.\r\n\r\nThank you for the comment :)"", 'created_at': datetime.datetime(2024, 11, 13, 4, 43, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2472688497, 'issue_id': 2470168570, 'author': 'omkar-foss', 'body': ""@geraj1010 it's looking nice & catchy! The user will definitely notice it while going through the doc I suppose, great work :)"", 'created_at': datetime.datetime(2024, 11, 13, 7, 29, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2505033166, 'issue_id': 2470168570, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 11, 28, 0, 16, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2505393732, 'issue_id': 2470168570, 'author': 'omkar-foss', 'body': ""PR #43909 for this issue is active and approved, but just needs a rebase (this issue isn't necessarily stale as such)."", 'created_at': datetime.datetime(2024, 11, 28, 7, 3, 6, tzinfo=datetime.timezone.utc)}]","kaxil on (2024-08-16 15:06:49 UTC): I like your proposed solution @peter-gergely-horvath , would you like to help out with the PR? Might just be a change in this file:

https://github.com/apache/airflow/blob/main/docs/sphinx_design/static/custom.css

geraj1010 (Assginee) on (2024-08-21 03:55:55 UTC): @kaxil I'd be happy to take this on, if the issue is still open.

potiuk on (2024-08-21 13:38:33 UTC): Assigned you

peter-gergely-horvath (Issue Creator) on (2024-09-02 16:52:06 UTC): Sorry guys, I only hacked the rendered page DOM inside the browser's dev tools to show what I mean here on a screenshot. I don't think I understand how the doc pages are generated.

RNHTTR on (2024-09-11 18:25:48 UTC): Here's the [README](https://github.com/apache/airflow/blob/main/docs/README.rst) for building docs and editing docs

github-actions[bot] on (2024-09-26 00:14:33 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

peter-gergely-horvath (Issue Creator) on (2024-09-27 07:29:03 UTC): The task is now picked up by [geraj1010](https://github.com/geraj1010). I look forward to seeing the new design. :)

ferruzzi on (2024-10-10 15:24:31 UTC): @geraj1010  Are you still working on this one?

geraj1010 (Assginee) on (2024-10-11 00:24:23 UTC): @ferruzzi Yes I am. I recently just regained access to my account, so I have been delayed. Happy to collaborate on this.

omkar-foss on (2024-10-11 17:13:56 UTC): Hi, as discussed on [this slack thread](https://apache-airflow.slack.com/archives/CJ1LVREHX/p1728613078488099), there are a few Sphinx directives that can be explored that help do this without direct CSS changes:

- `.. error::` - https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-error
- `.. note::` - https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-note
- `.. attention::` - https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-attention

I suppose any that looks fine and catches the user's attention should be good enough.

cc: @ashb (thanks for the suggestion)

github-actions[bot] on (2024-10-31 00:15:28 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

geraj1010 (Assginee) on (2024-11-07 01:16:49 UTC): Greetings, I have been dragging my feet on this issue, but I am starting to look into it now. I think the `attention` or `warning` directive fits for this, as mentioned by @omkar-foss.

geraj1010 (Assginee) on (2024-11-11 03:02:34 UTC): @peter-gergely-horvath I was able to add some additional highlighting to show that the operator is deprecated. It's not exactly like your mockup, but at least it stands out more than just plain text.

<img width=""617"" alt=""image"" src=""https://github.com/user-attachments/assets/94b1b73a-7547-4732-8967-d98823cbf7ab"">

 Thoughts?

potiuk on (2024-11-11 21:42:31 UTC): Looks good to me.

geraj1010 (Assginee) on (2024-11-12 05:59:06 UTC): Thank you @potiuk.

ferruzzi on (2024-11-12 21:06:38 UTC): I like it too.  Definitely an improvement.

peter-gergely-horvath (Issue Creator) on (2024-11-12 21:23:04 UTC): I would love to see title colour change and strike-through, but if that is difficult to implement, I think this is quite fine. :)

kaxil on (2024-11-12 21:30:48 UTC): And it can be iterative too: this is already a good improvement :)

geraj1010 (Assginee) on (2024-11-13 04:43:27 UTC): Unfortunately, I think that would be difficult to implement. From what I understand, the documentation for the operators (and sensors) in the Python API section are gathered via Sphinx's AutoAPI extension at document generation runtime. This means that all styling is done on-the-fly and the `index.rst` files are not directly accessible until after rendering (in a phantom folder called `_api`). 

I was thinking maybe we could achieve the original desired styling, by customizing the AutoAPI extension to look for any classes with the `@deprecated` decorator and apply a special custom `..py:class` directive instead of the normal one. I think that would be very neat, but also a lot more work.

Thank you for the comment :)

omkar-foss on (2024-11-13 07:29:45 UTC): @geraj1010 it's looking nice & catchy! The user will definitely notice it while going through the doc I suppose, great work :)

github-actions[bot] on (2024-11-28 00:16:24 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

omkar-foss on (2024-11-28 07:03:06 UTC): PR #43909 for this issue is active and approved, but just needs a rebase (this issue isn't necessarily stale as such).

"
2470028079,issue,closed,completed,wait_for_downstream doesn't seem to work as expected,"### Apache Airflow version

2.10.0

### What happened?

Running this test code on 16. August. It does catchup.
```
from airflow.decorators import dag
from airflow.operators.bash import BashOperator

from datetime import datetime, timedelta

default_args = {
    'depends_on_past': True,
    'wait_for_downstream': True,
}

@dag(
    start_date=datetime(2024, 8, 10),
    default_args=default_args,
    schedule_interval='0 6 * * *'
)
def test2():
    t1 = BashOperator(task_id='t1', bash_command=""sleep 10"")
    t2 = BashOperator(task_id='t2', bash_command=""sleep 10"")
    t3 = BashOperator(task_id='t3', bash_command=""sleep 10"")
    t4 = BashOperator(task_id='t4', bash_command=""sleep 10"")
    t5 = BashOperator(task_id='t5', bash_command=""sleep 10"")

    t1 >> t2 >> t3 >> t4 >> t5

test2()
```

It proceeds to running tasks on the next day before the earlier day is finished.

<img width=""297"" alt=""Pasted Graphic"" src=""https://github.com/user-attachments/assets/a29e702e-a74d-44c8-b6ec-9221342cb7aa"">


### What you think should happen instead?



I'm assuming that it should run all the tasks for Aug 13, before proceeding to any task on Aug 14

### How to reproduce

Run the provided dag code.

### Operating System

Docker

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",kakoni,2024-08-16 10:56:55+00:00,[],2024-08-17 14:36:56+00:00,2024-08-17 13:13:57+00:00,https://github.com/apache/airflow/issues/41526,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('area:core', '')]","[{'comment_id': 2293292518, 'issue_id': 2470028079, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 16, 10, 56, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2293656275, 'issue_id': 2470028079, 'author': 'romsharon98', 'body': ""The behavior is as expected.\r\nIt start a new DagRun because the default max_active_runs is 16.\r\nBut it does not start the first task until the first and second task don't finish successfully in the DagRun before."", 'created_at': datetime.datetime(2024, 8, 16, 14, 51, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2294856522, 'issue_id': 2470028079, 'author': 'kakoni', 'body': '@romsharon98 Thank you, I guess I misunderstood how wait_for_downstream flag works. So the corrective action would be to set max_active_runs to 1 for this dag I guess.', 'created_at': datetime.datetime(2024, 8, 17, 13, 13, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2294877468, 'issue_id': 2470028079, 'author': 'romsharon98', 'body': '> @romsharon98 Thank you, I guess I misunderstood how wait_for_downstream flag works. So the corrective action would be to set max_active_runs to 1 for this dag I guess. \n\nExactly', 'created_at': datetime.datetime(2024, 8, 17, 14, 36, 55, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-16 10:56:58 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

romsharon98 on (2024-08-16 14:51:22 UTC): The behavior is as expected.
It start a new DagRun because the default max_active_runs is 16.
But it does not start the first task until the first and second task don't finish successfully in the DagRun before.

kakoni (Issue Creator) on (2024-08-17 13:13:57 UTC): @romsharon98 Thank you, I guess I misunderstood how wait_for_downstream flag works. So the corrective action would be to set max_active_runs to 1 for this dag I guess.

romsharon98 on (2024-08-17 14:36:55 UTC): Exactly

"
2469953557,issue,closed,completed,AttributeError: 'CeleryKubernetesExecutor' object has no attribute 'slots_occupied',"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.0 Python 3.12

### What happened?

**UPDATE:** I think the `CeleryKubernetesExecutor` class should inherit `BaseExecutor` instead of `LoggingMixin`. `BaseExecutor` has the `slots_occupied` method implemented.

I get an error in the scheduler when I use `CeleryKubernetesExecutor`. My Airflow installation is on Kubernetes with the official Helm chart.

```
/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/cncf/kubernetes/executors/kubernetes_executor.py:143 FutureWarning: The config section [kubernetes] has been renamed to [kubernetes_executor]. Please update your `conf.get*` call to use the new name
[2024-08-16T11:55:08.527+0200] {executor_loader.py:254} INFO - Loaded executor: CeleryKubernetesExecutor
[2024-08-16T11:55:08.826+0200] {scheduler_job_runner.py:935} INFO - Starting the scheduler
[2024-08-16T11:55:08.827+0200] {scheduler_job_runner.py:942} INFO - Processing each file at most -1 times
[2024-08-16T11:55:08.828+0200] {ecs_executor.py:181} INFO - Loading Connection information
[2024-08-16T11:55:09.724+0200] {base.py:84} INFO - Retrieving connection 'aws_default'
[2024-08-16T11:55:11.322+0200] {executor_loader.py:254} INFO - Loaded executor: airflow.providers.amazon.aws.executors.ecs.AwsEcsExecutor
[2024-08-16T11:55:11.323+0200] {kubernetes_executor.py:287} INFO - Start Kubernetes executor
[2024-08-16T11:55:11.435+0200] {kubernetes_executor_utils.py:140} INFO - Event: and now my watch begins starting at resource_version: 0
[2024-08-16T11:55:11.623+0200] {kubernetes_executor.py:208} INFO - Found 0 queued task instances
[2024-08-16T11:55:11.625+0200] {ecs_executor.py:125} INFO - Starting ECS Executor and determining health...
[2024-08-16T11:55:12.119+0200] {ecs_executor.py:173} INFO - ECS Executor health check has succeeded.
[2024-08-16T11:55:12.120+0200] {scheduler_job_runner.py:1843} INFO - Adopting or resetting orphaned tasks for active dag runs
[2024-08-16T11:55:12.246+0200] {scheduler_job_runner.py:1001} ERROR - Exception when executing SchedulerJob._run_scheduler_loop
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/scheduler_job_runner.py"", line 984, in _execute
    self._run_scheduler_loop()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1121, in _run_scheduler_loop
    num_queued_tis = self._do_scheduling(session)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/scheduler_job_runner.py"", line 1264, in _do_scheduling
    num_queued_tis = self._critical_section_enqueue_task_instances(session=session)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/jobs/scheduler_job_runner.py"", line 718, in _critical_section_enqueue_task_instances
    num_occupied_slots = sum([executor.slots_occupied for executor in self.job.executors])
                              ^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'CeleryKubernetesExecutor' object has no attribute 'slots_occupied'
[2024-08-16T11:55:12.330+0200] {kubernetes_executor.py:752} INFO - Shutting down Kubernetes executor
[2024-08-16T11:55:12.529+0200] {scheduler_job_runner.py:1014} INFO - Exited execute loop
```

### What you think should happen instead?

_No response_

### How to reproduce

Install Airflow with the official Helm chart with `CeleryKubernetesExecutor` and the scheduler container will raise this exception.

### Operating System

Kubernetes, Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",LipuFei,2024-08-16 10:11:14+00:00,[],2024-12-17 19:27:47+00:00,2024-08-20 06:12:00+00:00,https://github.com/apache/airflow/issues/41525,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2293562045, 'issue_id': 2469953557, 'author': 'salam-abdul', 'body': ""I'm getting a related error message on Airflow 2.10 and Python 3.11\r\n`AttributeError: 'LocalKubernetesExecutor' object has no attribute 'slots_occupied'`"", 'created_at': datetime.datetime(2024, 8, 16, 13, 57, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2293586004, 'issue_id': 2469953557, 'author': 'potiuk', 'body': 'CC: @o-nikolas -> seems to be result of #40017', 'created_at': datetime.datetime(2024, 8, 16, 14, 11, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2293700446, 'issue_id': 2469953557, 'author': 'o-nikolas', 'body': ""I'll look into fixing this issue But I encourage folks to use Multiple Executor Configuration instead of these old (and should be deprecated) hybrid executors. These are exactly the types of issues that arise from these statically combined executors, since they're full of bespoke logic and hard coupling.\n\n@potiuk what steps need to be followed for us to deprecate these old hybrid executors for airflow 3.0? Or is that not possible because they are in provider packages?"", 'created_at': datetime.datetime(2024, 8, 16, 15, 17, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2293719334, 'issue_id': 2469953557, 'author': 'potiuk', 'body': '>  @potiuk what steps need to be followed for us to deprecate these old hybrid executors for airflow 3.0? Or is that not possible because they are in provider packages\r\n\r\nWe can deprecate-with-warnings those and remove them from providers when the time comes (which might or might not be faster than getting rid of Airflow 2 support from those providers.', 'created_at': datetime.datetime(2024, 8, 16, 15, 29, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296899749, 'issue_id': 2469953557, 'author': 'zoid-w', 'body': 'On a related note ; the current helm chart (1.15.0) does not yet support the hybrid executor approach introduced in airflow 2.10.0.\r\n\r\nIn the helm chart there is a constraint on an ENUM of the ""old"" executors.\r\nNot sure what the best approach would be to validate the executor input (if possible) considering the possibility to specify your own custom executor.', 'created_at': datetime.datetime(2024, 8, 19, 15, 49, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2338724200, 'issue_id': 2469953557, 'author': 'KulykDmytro', 'body': '@potiuk issue still exists on\r\n* Python 3.11\r\n* Airflow 2.10.1', 'created_at': datetime.datetime(2024, 9, 9, 17, 49, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339028408, 'issue_id': 2469953557, 'author': 'potiuk', 'body': 'Did you try with latest provider?', 'created_at': datetime.datetime(2024, 9, 9, 20, 29, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339029396, 'issue_id': 2469953557, 'author': 'potiuk', 'body': 'cc: @o-nikolas', 'created_at': datetime.datetime(2024, 9, 9, 20, 30, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339508119, 'issue_id': 2469953557, 'author': 'eladkal', 'body': '> @potiuk issue still exists on\r\n> \r\n> * Python 3.11\r\n> * Airflow 2.10.1\r\n\r\nplease specify version of Celery provider that you are using', 'created_at': datetime.datetime(2024, 9, 10, 2, 54, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2546776507, 'issue_id': 2469953557, 'author': 'rickyzhang82', 'body': '@potiuk ,\r\n\r\nI checked out the tag: `2.10.2` from airflow Git repo (commit  35087d7d10714130cc3e9e9730e34b07fc56938d) and still ran into the same attribute error:\r\n\r\n```\r\n  File ""/home/Ricky/sas-u-home/rizhan/repo/github/airflow/airflow/jobs/scheduler_job_runner.py"", line 718, in <listcomp>\r\n    num_occupied_slots = sum([executor.slots_occupied for executor in self.job.executors])\r\nAttributeError: \'SasKubernetesCeleryExecutor\' object has no attribute \'slots_occupied\'\r\n```\r\nHowever, we didn\'t see the same issue from Airflow in our production build which pip from PyPi with `2.10.2` tag. \r\n\r\nThe source code from PyPi is not the **same** even though pointing to `2.10.2`\r\n\r\nThe one from PyPi --  airflow/providers/celery/executors/celery_kubernetes_executor.py has the fix but it is not in the `2.10.2` release tag\r\n\r\n```\r\n119     @property\r\n120     def slots_occupied(self):\r\n121         """"""Number of tasks this executor instance is currently managing.""""""\r\n122         return len(self.running) + len(self.queued_tasks)\r\n```\r\n\r\nI wonder why?', 'created_at': datetime.datetime(2024, 12, 16, 21, 7, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2546889237, 'issue_id': 2469953557, 'author': 'o-nikolas', 'body': ""I see the that class in this case is `SasKubernetesCeleryExecutor`, what is that? Is that a class you've created that subclasses from the existing `KubernetesCeleryExecutor`?\r\n\r\nI'm not entirely sure why the code is not in the repo tag, but you see it in pypi. But the fix is definitely pushed, I just double checked main. I don't do releases personally, so perhaps @eladkal or @potiuk would know?\r\n\r\nAlso, I still encourage folks to stop using these old static hybrid executors and please use the new multiple executor configuration :smiley:"", 'created_at': datetime.datetime(2024, 12, 16, 21, 49, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2547544420, 'issue_id': 2469953557, 'author': 'eladkal', 'body': 'Providers are released separately and independently from Airflow core. https://github.com/apache/airflow/pull/41602 was released in celery provider 3.8.1', 'created_at': datetime.datetime(2024, 12, 17, 5, 34, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2548697201, 'issue_id': 2469953557, 'author': 'rickyzhang82', 'body': '@eladkal,\r\n\r\nI followed the doc `contributing-docs/07_local_virtualenv.rst` to create my dev playpen. I ran the following\r\n\r\n```\r\npip install -e "".[devel,postgres,redis,celery,cncf-kubernetes]"" --constraint ""https://raw.githubusercontent.com/apache/airflow/constraints-2.10.2/constraints-3.9.txt""\r\n```\r\n\r\nIt doesn\'t seem that my local playpen pick up the right version of the providers w.r.t `2.10.2` core. \r\n\r\nHow would you make them matched?', 'created_at': datetime.datetime(2024, 12, 17, 15, 5, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2549421972, 'issue_id': 2469953557, 'author': 'potiuk', 'body': ""> How would you make them matched?\r\n\r\nYou can't have them from a single checkout - it's impossible. As also explained in another discussion (and abobe by @eladkal) providers and airflow are released separately - and our cotributor guide assumes you are working on main where you are working on latest airflow + latest providers at the same time. \r\n\r\nI am not sure that trying to have one devplay with the different versions of providers and airflow installed from sources is even something that makes sense for historical version because you are talking about potentially cross-matching and installing sources from different tags - with might or miugh not work. \r\n\r\nAs explained elsewhere - tis will be easier once we implement https://github.com/apache/airflow/issues/44511  - but until then you can try to - for example - install airflow in non editable mode, and then prepare the sources of the provider in the version you want by checking out the tag you want - look at the tags we have starting with `providers-*` and using `breeze release-management prepare-provider-packages YOUR_PROVIDER  --skip-deleting-generated-files` - that will generate you the source folder structure (including `pyproject.toml` which you will be able to install your provider in the right version in `--editable` mode - additionally to airflow installed there.\r\n\r\nThat's one of the ways how you can achieve it."", 'created_at': datetime.datetime(2024, 12, 17, 19, 27, 46, tzinfo=datetime.timezone.utc)}]","salam-abdul on (2024-08-16 13:57:38 UTC): I'm getting a related error message on Airflow 2.10 and Python 3.11
`AttributeError: 'LocalKubernetesExecutor' object has no attribute 'slots_occupied'`

potiuk on (2024-08-16 14:11:36 UTC): CC: @o-nikolas -> seems to be result of #40017

o-nikolas on (2024-08-16 15:17:45 UTC): I'll look into fixing this issue But I encourage folks to use Multiple Executor Configuration instead of these old (and should be deprecated) hybrid executors. These are exactly the types of issues that arise from these statically combined executors, since they're full of bespoke logic and hard coupling.

@potiuk what steps need to be followed for us to deprecate these old hybrid executors for airflow 3.0? Or is that not possible because they are in provider packages?

potiuk on (2024-08-16 15:29:22 UTC): We can deprecate-with-warnings those and remove them from providers when the time comes (which might or might not be faster than getting rid of Airflow 2 support from those providers.

zoid-w on (2024-08-19 15:49:56 UTC): On a related note ; the current helm chart (1.15.0) does not yet support the hybrid executor approach introduced in airflow 2.10.0.

In the helm chart there is a constraint on an ENUM of the ""old"" executors.
Not sure what the best approach would be to validate the executor input (if possible) considering the possibility to specify your own custom executor.

KulykDmytro on (2024-09-09 17:49:55 UTC): @potiuk issue still exists on
* Python 3.11
* Airflow 2.10.1

potiuk on (2024-09-09 20:29:40 UTC): Did you try with latest provider?

potiuk on (2024-09-09 20:30:11 UTC): cc: @o-nikolas

eladkal on (2024-09-10 02:54:51 UTC): please specify version of Celery provider that you are using

rickyzhang82 on (2024-12-16 21:07:52 UTC): @potiuk ,

I checked out the tag: `2.10.2` from airflow Git repo (commit  35087d7d10714130cc3e9e9730e34b07fc56938d) and still ran into the same attribute error:

```
  File ""/home/Ricky/sas-u-home/rizhan/repo/github/airflow/airflow/jobs/scheduler_job_runner.py"", line 718, in <listcomp>
    num_occupied_slots = sum([executor.slots_occupied for executor in self.job.executors])
AttributeError: 'SasKubernetesCeleryExecutor' object has no attribute 'slots_occupied'
```
However, we didn't see the same issue from Airflow in our production build which pip from PyPi with `2.10.2` tag. 

The source code from PyPi is not the **same** even though pointing to `2.10.2`

The one from PyPi --  airflow/providers/celery/executors/celery_kubernetes_executor.py has the fix but it is not in the `2.10.2` release tag

```
119     @property
120     def slots_occupied(self):
121         """"""Number of tasks this executor instance is currently managing.""""""
122         return len(self.running) + len(self.queued_tasks)
```

I wonder why?

o-nikolas on (2024-12-16 21:49:09 UTC): I see the that class in this case is `SasKubernetesCeleryExecutor`, what is that? Is that a class you've created that subclasses from the existing `KubernetesCeleryExecutor`?

I'm not entirely sure why the code is not in the repo tag, but you see it in pypi. But the fix is definitely pushed, I just double checked main. I don't do releases personally, so perhaps @eladkal or @potiuk would know?

Also, I still encourage folks to stop using these old static hybrid executors and please use the new multiple executor configuration :smiley:

eladkal on (2024-12-17 05:34:15 UTC): Providers are released separately and independently from Airflow core. https://github.com/apache/airflow/pull/41602 was released in celery provider 3.8.1

rickyzhang82 on (2024-12-17 15:05:51 UTC): @eladkal,

I followed the doc `contributing-docs/07_local_virtualenv.rst` to create my dev playpen. I ran the following

```
pip install -e "".[devel,postgres,redis,celery,cncf-kubernetes]"" --constraint ""https://raw.githubusercontent.com/apache/airflow/constraints-2.10.2/constraints-3.9.txt""
```

It doesn't seem that my local playpen pick up the right version of the providers w.r.t `2.10.2` core. 

How would you make them matched?

potiuk on (2024-12-17 19:27:46 UTC): You can't have them from a single checkout - it's impossible. As also explained in another discussion (and abobe by @eladkal) providers and airflow are released separately - and our cotributor guide assumes you are working on main where you are working on latest airflow + latest providers at the same time. 

I am not sure that trying to have one devplay with the different versions of providers and airflow installed from sources is even something that makes sense for historical version because you are talking about potentially cross-matching and installing sources from different tags - with might or miugh not work. 

As explained elsewhere - tis will be easier once we implement https://github.com/apache/airflow/issues/44511  - but until then you can try to - for example - install airflow in non editable mode, and then prepare the sources of the provider in the version you want by checking out the tag you want - look at the tags we have starting with `providers-*` and using `breeze release-management prepare-provider-packages YOUR_PROVIDER  --skip-deleting-generated-files` - that will generate you the source folder structure (including `pyproject.toml` which you will be able to install your provider in the right version in `--editable` mode - additionally to airflow installed there.

That's one of the ways how you can achieve it.

"
2469713230,issue,open,,DetachedInstanceError when using Variable in custom timetable plugin,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.0

### What happened?

I want to build a custom timetable plugin that can get holidays from meta database variables.
The data can fetch from variables.
But there is an exception that throw from dag.py and the dag register failure.

> Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py"", line 182, in _run_file_processor
    _handle_dag_file_processing()
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py"", line 163, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 79, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py"", line 859, in process_file
    serialize_errors = DagFileProcessor.save_dag_to_db(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/api_internal/internal_api_call.py"", line 115, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 79, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/dag_processing/processor.py"", line 895, in save_dag_to_db
    import_errors = DagBag._sync_to_db(dags=dags, processor_subdir=dag_directory, session=session)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py"", line 659, in _sync_to_db
    for attempt in run_with_db_retries(logger=log):
  File ""/home/airflow/.local/lib/python3.12/site-packages/tenacity/__init__.py"", line 347, in __iter__
    do = self.iter(retry_state=retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/tenacity/__init__.py"", line 314, in iter
    return fut.result()
           ^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/concurrent/futures/_base.py"", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/concurrent/futures/_base.py"", line 401, in __get_result
    raise self._exception
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py"", line 675, in _sync_to_db
    DAG.bulk_write_to_db(dags.values(), processor_subdir=processor_subdir, session=session)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dag.py"", line 3191, in bulk_write_to_db
    orm_dag_links = orm_dag.dag_owner_links or []
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/attributes.py"", line 487, in __get__
    return self.impl.get(state, dict_)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/attributes.py"", line 959, in get
    value = self._fire_loader_callables(state, key, passive)
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/attributes.py"", line 995, in _fire_loader_callables
    return self.callable_(state, passive)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/sqlalchemy/orm/strategies.py"", line 863, in _load_for_state
    raise orm_exc.DetachedInstanceError(
sqlalchemy.orm.exc.DetachedInstanceError: Parent instance <DagModel at 0x7f579f911010> is not bound to a Session; lazy load operation of attribute 'dag_owner_links' cannot proceed (Background on this error at: https://sqlalche.me/e/14/bhk3)


Test Dag code
```
with DAG(
    dag_id=""test"",
    description = 'Testing',
    start_date = datetime.datetime(2024, 8, 1, 0, 0),
    schedule=CustomTimetable(),
    catchup=False
) as dag:
    start = EmptyOperator(task_id=""start"")
    end = EmptyOperator(task_id=""end"")

start >> end
```
[
](url)Plugin code
```
class CustomTimetable(Timetable):
    def test_variable(self):
        holiday = Variable.get(""HolidaysFor2024"")
        print(""Test: "" + holiday)
```

it still have the same error If i use the session directly.
```
class CustomTimetable(Timetable):
    def test_variable(self):
        with create_session() as session:
            holiday = session.scalar(select(Variable).where(Variable.key == ""HolidaysFor2024"").limit(1))
            print(""Test: "" + holiday.get_val())
```

### What you think should happen instead?

_No response_

### How to reproduce

```
class CustomTimetable(Timetable):
    def test_variable(self):
        holiday = Variable.get(""HolidaysFor2024"")
        print(""Test: "" + holiday)

    def infer_manual_data_interval(
            self,
            run_after: DateTime
    ) -> DataInterval:
        return DataInterval(start=run_after, end=run_after)

    def next_dagrun_info(
        self,
        *,
        last_automated_data_interval: DataInterval | None,
        restriction: TimeRestriction,
    ) -> DagRunInfo | None:

        self.test_variable()

        now = DateTime.utcnow().replace(tzinfo=UTC)
        return DagRunInfo.interval(start=now, end=now)

class CustomTimetablePlugin(AirflowPlugin):
    name = ""custom_timetable_plugin""
    timetables = [CustomTimetable]

```
or
```
class CustomTimetable(Timetable):
    def test_variable(self):
        with create_session() as session:
            holiday = session.scalar(select(Variable).where(Variable.key == ""HolidaysFor2024"").limit(1))
            print(""Test: "" + holiday.get_val())

   ....
```

### Operating System

linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other Docker-based deployment

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",yi-cheng-chen-taiwan,2024-08-16 07:48:33+00:00,[],2025-01-10 08:47:48+00:00,,https://github.com/apache/airflow/issues/41523,"[('kind:bug', 'This is a clearly a bug'), ('area:MetaDB', 'Meta Database related issues.'), ('area:core', ''), ('area:DAG-processing', '')]","[{'comment_id': 2293007654, 'issue_id': 2469713230, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 16, 7, 48, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2462574326, 'issue_id': 2469713230, 'author': 'OfekMarks', 'body': 'Same thing just happened to me. Is there a solution?', 'created_at': datetime.datetime(2024, 11, 7, 15, 46, 51, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-16 07:48:36 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

OfekMarks on (2024-11-07 15:46:51 UTC): Same thing just happened to me. Is there a solution?

"
2469387738,issue,closed,completed,"Is it possible to remove the unique constraint on (dag_id, execution_date) in the future version?","### Description

Currently, Apache Airflow enforces a unique constraint on the combination of dag_id and execution_date for identifying DAG runs. This constraint ensures that each DAG run is uniquely identified by its DAG ID and execution date. However, there are scenarios where this constraint can be limiting and can not schedule multiple DAG run at the same execution_date. Additionally, recent versions of Airflow have introduced another unique key (dag_id, run_id), which should be sufficient to uniquely identify DAG runs.

### Use case/motivation

Use Case Example: Cannot Schedule DAG Run at the Same Execution Date:
Scenario: A data engineering team needs to reprocess a daily ETL pipeline due to an upstream data issue. The pipeline is scheduled to run at midnight every day.
Problem: Due to the unique constraint on (dag_id, execution_date), the team cannot schedule a run of the DAG for the same execution date (midnight of the same day) without changing the execution date or creating a new DAG.
Impact: This limitation forces the team to implement workarounds, such as modifying the execution date or duplicating the DAG, which complicates workflow management and increases the risk of errors.
Solution: Removing the unique constraint on (dag_id, execution_date) would allow the team to schedule multiple runs of the DAG for the same execution date, simplifying reprocessing and improving workflow efficiency.

### Related issues

https://github.com/apache/airflow/issues/15150

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",sideef5ect,2024-08-16 03:38:09+00:00,[],2024-08-16 05:04:19+00:00,2024-08-16 05:03:59+00:00,https://github.com/apache/airflow/issues/41522,"[('area:MetaDB', 'Meta Database related issues.'), ('kind:feature', 'Feature Requests'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2292670553, 'issue_id': 2469387738, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 16, 3, 38, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2292805529, 'issue_id': 2469387738, 'author': 'tirkarthi', 'body': 'Related https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-83+Remove+Execution+Date+Unique+Constraint+from+DAG+Run', 'created_at': datetime.datetime(2024, 8, 16, 4, 54, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2292813624, 'issue_id': 2469387738, 'author': 'sideef5ect', 'body': 'Thanks for sharing the status', 'created_at': datetime.datetime(2024, 8, 16, 5, 4, 18, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-16 03:38:12 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

tirkarthi on (2024-08-16 04:54:24 UTC): Related https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-83+Remove+Execution+Date+Unique+Constraint+from+DAG+Run

sideef5ect (Issue Creator) on (2024-08-16 05:04:18 UTC): Thanks for sharing the status

"
2469222492,issue,open,,"Rename the concept of user-facing ""Active"" DAG in UI & API","Same as https://github.com/apache/airflow/issues/39593 but given we are now releasing Airflow 3.0 we can change this in a backwards-incompatible way.


Apart from just the UI, we need to change the filter in the API too. Example, we have `only_active` filter in ""List DAGs"" endpoint: https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html#operation/get_dags


![image](https://github.com/user-attachments/assets/4d9abab4-e58e-4b8c-871d-81d892ae7ddf)",kaxil,2024-08-16 00:55:37+00:00,[],2024-11-17 17:54:43+00:00,,https://github.com/apache/airflow/issues/41519,"[('area:API', ""Airflow's REST/HTTP API""), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('airflow3.0:breaking', 'Candidates for Airflow 3.0 that contain breaking changes')]","[{'comment_id': 2292539544, 'issue_id': 2469222492, 'author': 'kaxil', 'body': ""Some suggestions from the Dev call on 8th August:\r\n\r\n* @dstandish : Unpaused & Paused. \r\n* @bbovenzi : Enabled/paused\r\n* @ashb 's comment: I like pause/unpaused, stale (vs not stale, but not stale doesn’t really show up anywhere) for the states/values in API and DB. (Or paused/enabled is free to if “stale” is the state for dags-where-file-doesn’t-exist anymore)\r\n\r\ncc @eladkal"", 'created_at': datetime.datetime(2024, 8, 16, 1, 9, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2292837702, 'issue_id': 2469222492, 'author': 'dstandish', 'body': 'I think it might be helpful to consider UI and API independently.\r\n\r\nFirst question, what functionality do we want in the UI?\r\n\r\nI think I always assumed that it just meant paused vs unpaused.\r\n\r\nAnd, if you don\'t have orphaned dags, that is acutally true, I think.\r\n\r\nHere\'s a proposal.  Just add one more category: ""missing"" a.k.a. ""orphaned"" a.k.a. ""inactive""\r\n\r\nAnd then you just have 4 categories: all, enabled, disabled, missing\r\n\r\nEnabled / disabled seems like better terminology than paused / unpaused.  Paused vs unpaused makes less sense for unscheduled dags.', 'created_at': datetime.datetime(2024, 8, 16, 5, 31, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2385267950, 'issue_id': 2469222492, 'author': 'bbovenzi', 'body': 'I am playing around with renaming it in the new UI. I still not sure exactly what to put:\r\n\r\n<img width=""605"" alt=""Screenshot 2024-10-01 at 10 25 59\u202fAM"" src=""https://github.com/user-attachments/assets/ac9a5286-8689-4a54-a6ca-d433a17afa64"">\r\n<img width=""546"" alt=""Screenshot 2024-10-01 at 10 26 05\u202fAM"" src=""https://github.com/user-attachments/assets/3f03f01b-7846-4f14-9112-de4a5cea258d"">', 'created_at': datetime.datetime(2024, 10, 1, 9, 24, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2481401279, 'issue_id': 2469222492, 'author': 'raphaelauv', 'body': 'Also a new option (and button) to disable new dag_run but keep running one finishing\r\n\r\nso stop the scheduler to create a new dag_run based on schedule ( like when dag is disable ) and not put in running queued dags\r\nBUT\r\nkeep scheduling tasks of dags already in running state\r\n\r\nso dag could be `running_and_scheduling` or `running` or `stop` state\r\n\r\nwdyt ?', 'created_at': datetime.datetime(2024, 11, 17, 17, 54, 12, tzinfo=datetime.timezone.utc)}]","kaxil (Issue Creator) on (2024-08-16 01:09:53 UTC): Some suggestions from the Dev call on 8th August:

* @dstandish : Unpaused & Paused. 
* @bbovenzi : Enabled/paused
* @ashb 's comment: I like pause/unpaused, stale (vs not stale, but not stale doesn’t really show up anywhere) for the states/values in API and DB. (Or paused/enabled is free to if “stale” is the state for dags-where-file-doesn’t-exist anymore)

cc @eladkal

dstandish on (2024-08-16 05:31:03 UTC): I think it might be helpful to consider UI and API independently.

First question, what functionality do we want in the UI?

I think I always assumed that it just meant paused vs unpaused.

And, if you don't have orphaned dags, that is acutally true, I think.

Here's a proposal.  Just add one more category: ""missing"" a.k.a. ""orphaned"" a.k.a. ""inactive""

And then you just have 4 categories: all, enabled, disabled, missing

Enabled / disabled seems like better terminology than paused / unpaused.  Paused vs unpaused makes less sense for unscheduled dags.

bbovenzi on (2024-10-01 09:24:40 UTC): I am playing around with renaming it in the new UI. I still not sure exactly what to put:

<img width=""605"" alt=""Screenshot 2024-10-01 at 10 25 59 AM"" src=""https://github.com/user-attachments/assets/ac9a5286-8689-4a54-a6ca-d433a17afa64"">
<img width=""546"" alt=""Screenshot 2024-10-01 at 10 26 05 AM"" src=""https://github.com/user-attachments/assets/3f03f01b-7846-4f14-9112-de4a5cea258d"">

raphaelauv on (2024-11-17 17:54:12 UTC): Also a new option (and button) to disable new dag_run but keep running one finishing

so stop the scheduler to create a new dag_run based on schedule ( like when dag is disable ) and not put in running queued dags
BUT
keep scheduling tasks of dags already in running state

so dag could be `running_and_scheduling` or `running` or `stop` state

wdyt ?

"
2468982786,issue,closed,completed,XCom Display for INT value shows error in web UI,"### Apache Airflow version

2.10.0rc1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When one task returns an integer value, the ""new"" XCom display shows an error JSON instead of the value like:

![image](https://github.com/user-attachments/assets/14341605-406f-432f-9d58-55ca8588d5af)

Noteworthy>: JSON or string or empty value works.


### What you think should happen instead?

The real INT value should be displayed

### How to reproduce

Run the DAG example_dynamic_task_mapping and check the XCom result of ""add_one"" task

### Operating System

not relevant

### Versions of Apache Airflow Providers

not relevant

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

not relevant

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jscheffl,2024-08-15 21:16:47+00:00,[],2024-08-20 03:56:04+00:00,2024-08-20 03:56:04+00:00,https://github.com/apache/airflow/issues/41514,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2292295447, 'issue_id': 2468982786, 'author': 'jscheffl', 'body': 'okay, same problem for bool and float.', 'created_at': datetime.datetime(2024, 8, 15, 21, 31, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2292806469, 'issue_id': 2468982786, 'author': 'tirkarthi', 'body': 'Faced similar issue : https://github.com/apache/airflow/pull/39918#issuecomment-2248601544', 'created_at': datetime.datetime(2024, 8, 16, 4, 55, 36, tzinfo=datetime.timezone.utc)}]","jscheffl (Issue Creator) on (2024-08-15 21:31:51 UTC): okay, same problem for bool and float.

tirkarthi on (2024-08-16 04:55:36 UTC): Faced similar issue : https://github.com/apache/airflow/pull/39918#issuecomment-2248601544

"
2468639686,issue,open,,airflow.cfg can't pull secrets from LocalFilesystemBackend,"### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When starting an Airflow instance or checking a property via `airflow config list/get-value` properties that have the `_secret` suffix return their literal value and the main property isn't set at all. The default values instead are applied.

### What you think should happen instead?

According to the docs, the following snippet should retrieve the key `sql_alchemy_conn` from the secret backend (the local filesystem one, in my case) and assign it to the `sql_alchemy_conn` under the `[database]` section.  
```
[database]
sql_alchemy_conn_secret = sql_alchemy_conn
```
https://airflow.apache.org/docs/apache-airflow/stable/howto/set-config.html

### How to reproduce

Reproducing requires only configuring the following properties in airflow.cfg and the .env file in the same directory. The secret backend still works from within a DAG, however, it just seems to not be invoked while parsing the airflow.cfg. I tried bringing the [secrets] section to the top of the airflow.cfg, since by default it comes after [database], but that had no effect.
```
[secrets]
backend = airflow.secrets.local_filesystem.LocalFilesystemBackend
backend_kwargs = {""variables_file_path"": ""./.env""}

[database]
sql_alchemy_conn_secret = sql_alchemy_conn
```
and an .env file (or yaml or json) like this
```
sql_alchemy_conn=postgresql://airflow_user@localhost/airflow_db
```

### Operating System

Arch linux 6.10.4-arch2-1

### Versions of Apache Airflow Providers

apache-airflow-providers-celery==3.7.2
apache-airflow-providers-common-io==1.3.2
apache-airflow-providers-common-sql==1.14.2
apache-airflow-providers-fab==1.2.2
apache-airflow-providers-ftp==3.10.0
apache-airflow-providers-http==4.12.0
apache-airflow-providers-imap==3.6.1
apache-airflow-providers-smtp==1.7.1
apache-airflow-providers-sqlite==3.8.1

### Deployment

Virtualenv installation

### Deployment details

venv setup from python 3.12.4 
`pip install ""apache-airflow[celery]==2.9.3"" --constraint ""./constraints-3.8.txt""`

### Anything else?

This is from a completely fresh installation. The issue also appears on an ARM Mac. I don't have the other possible secret backends set up, so I don't know how those behave. I can also understand that this might be a documentation issue and this isn't supported anymore, since the secrets backend would be retrieving secrets for the same file it is configured in itself.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",thedustinmiller,2024-08-15 18:15:27+00:00,[],2025-01-10 11:19:02+00:00,,https://github.com/apache/airflow/issues/41512,"[('area:secrets', ''), ('kind:documentation', ''), ('area:core', '')]","[{'comment_id': 2291888858, 'issue_id': 2468639686, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 15, 18, 15, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2568193432, 'issue_id': 2468639686, 'author': 'kurtrwall', 'body': 'This is just not implemented for the LocalFilesystemBackend unfortunately. I\'m not sure if you could consider it a bug, per se, but it\'s certainly implied it should work in the documentation (IIRC). Create a pip installable secrets backend yourself, here\'s the code I use and it works well, though you should know it will leak secrets to anyone with access to the `airflow config` CLI.\r\n\r\n```python\r\nclass YourLocalFilesystemBackend(LocalFilesystemBackend):\r\n    """"""Airflow filesystem secrets backend that can pull config values from secrets""""""\r\n\r\n    def get_config(self, key: str) -> str | None:\r\n        config = super().get_config(key)\r\n        if config is not None:\r\n            return config\r\n        var = self.get_variable(key)\r\n        if var is not None:\r\n            return var\r\n        conn = self.get_connection(key)\r\n        if conn is not None:\r\n            return conn.get_uri()\r\n```', 'created_at': datetime.datetime(2025, 1, 2, 18, 28, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2582076664, 'issue_id': 2468639686, 'author': 'shahar1', 'body': 'I feel quite uncomfortable with implementing it, knowing that it may become a security issue as stated above.\r\nHowever, I support adding a paragraph to the docs about it with a very explicit warning.', 'created_at': datetime.datetime(2025, 1, 10, 8, 46, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2582471420, 'issue_id': 2468639686, 'author': 'potiuk', 'body': 'No. We should raise ""Not Implemented"" error in this case. If we implement it, this will give false sense of security - raising NotImplemented is expicitly stating that .... it\'s not implemented - and it can also explain why', 'created_at': datetime.datetime(2025, 1, 10, 11, 19, 1, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-15 18:15:30 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

kurtrwall on (2025-01-02 18:28:46 UTC): This is just not implemented for the LocalFilesystemBackend unfortunately. I'm not sure if you could consider it a bug, per se, but it's certainly implied it should work in the documentation (IIRC). Create a pip installable secrets backend yourself, here's the code I use and it works well, though you should know it will leak secrets to anyone with access to the `airflow config` CLI.

```python
class YourLocalFilesystemBackend(LocalFilesystemBackend):
    """"""Airflow filesystem secrets backend that can pull config values from secrets""""""

    def get_config(self, key: str) -> str | None:
        config = super().get_config(key)
        if config is not None:
            return config
        var = self.get_variable(key)
        if var is not None:
            return var
        conn = self.get_connection(key)
        if conn is not None:
            return conn.get_uri()
```

shahar1 on (2025-01-10 08:46:04 UTC): I feel quite uncomfortable with implementing it, knowing that it may become a security issue as stated above.
However, I support adding a paragraph to the docs about it with a very explicit warning.

potiuk on (2025-01-10 11:19:01 UTC): No. We should raise ""Not Implemented"" error in this case. If we implement it, this will give false sense of security - raising NotImplemented is expicitly stating that .... it's not implemented - and it can also explain why

"
2468454622,issue,closed,completed,docker decorator wrong signature in the pyi,"### What do you see as an issue?

should be `list[Mount] | None` instead of `list[str] | None`

### Solving the problem

small PR to fix the signature

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",benbenbang,2024-08-15 16:38:34+00:00,[],2024-08-15 22:38:36+00:00,2024-08-15 22:38:36+00:00,https://github.com/apache/airflow/issues/41508,"[('kind:bug', 'This is a clearly a bug'), ('kind:documentation', ''), ('provider:docker', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2468206711,issue,closed,completed,`get_tree_view` can consume extreme amounts of memory.,"### Apache Airflow version

2.10.0rc1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

`get_tree_view` in degenerated case can take a lot of memory.

For a DAG 

```
    with DAG(""aaa_big_get_tree_view"", schedule=None) as dag:
        first_set = [LongEmptyOperator(task_id=f""hello_{i}_{'a' * 230}"") for i in range(900)]
        chain(*first_set)

        last_task_in_first_set = first_set[-1]

        chain(
            last_task_in_first_set, [LongEmptyOperator(task_id=f""world_{i}_{'a' * 230}"") for i in range(900)]
        )

        chain(
            last_task_in_first_set, [LongEmptyOperator(task_id=f""this_{i}_{'a' * 230}"") for i in range(900)]
        )

        chain(last_task_in_first_set, [LongEmptyOperator(task_id=f""is_{i}_{'a' * 230}"") for i in range(900)])

        chain(
            last_task_in_first_set, [LongEmptyOperator(task_id=f""silly_{i}_{'a' * 230}"") for i in range(900)]
        )

        chain(
            last_task_in_first_set, [LongEmptyOperator(task_id=f""stuff_{i}_{'a' * 230}"") for i in range(900)]
        )
```

serializing it can take 2.7GB

```
root@a24bae3584cb:/opt/airflow# pytest --memray tests/providers/openlineage/utils/test_utils.py::test_get_dag_tree_large_dag
=========================================================================================================================================================================== test session starts ============================================================================================================================================================================
platform linux -- Python 3.12.5, pytest-8.3.2, pluggy-1.5.0 -- /usr/local/bin/python
cachedir: .pytest_cache
rootdir: /opt/airflow
configfile: pyproject.toml
plugins: memray-1.7.0, timeouts-1.2.1, icdiff-0.9, mock-3.14.0, rerunfailures-14.0, requests-mock-1.12.1, xdist-3.6.1, asyncio-0.23.8, anyio-4.4.0, instafail-0.5.0, cov-5.0.0, time-machine-2.15.0, custom-exit-code-0.3.0
asyncio: mode=Mode.STRICT
setup timeout: 0.0s, execution timeout: 0.0s, teardown timeout: 0.0s
collected 1 item

tests/providers/openlineage/utils/test_utils.py::test_get_dag_tree_large_dag PASSED                                                                                                                                                                                                                                                                                  [100%]


============================================================================================================================================================================== MEMRAY REPORT ===============================================================================================================================================================================
Allocation results for tests/providers/openlineage/utils/test_utils.py::test_get_dag_tree_large_dag at the high watermark

	 📦 Total memory allocated: 5.4GiB
	 📏 Total allocations: 23
	 📊 Histogram of allocation sizes: |▁▁█  |
	 🥇 Biggest allocating functions:
		- _safe_get_dag_tree_view:/opt/airflow/airflow/providers/openlineage/utils/utils.py:446 -> 2.7GiB
		- get_tree_view:/opt/airflow/airflow/models/dag.py:2445 -> 2.7GiB
		- __setattr__:/opt/airflow/airflow/models/baseoperator.py:1191 -> 1.3MiB
		- __setattr__:/opt/airflow/airflow/models/baseoperator.py:1191 -> 1.3MiB
		- __setattr__:/opt/airflow/airflow/models/baseoperator.py:1191 -> 1.3MiB


=================================================================================================================================================================== Warning summary. Total: 3, Unique: 3 ===================================================================================================================================================================
airflow: total 1, unique 1
  collect: total 1, unique 1
other: total 2, unique 2
  collect: total 2, unique 2
Warnings saved into /opt/airflow/tests/warnings.txt file.
============================================================================================================================================================================ 1 passed in 8.60s =============================================================================================================================================================================
```

https://github.com/apache/airflow/pull/41494

### What you think should happen instead?

I think tree_view format should be changed to one that does not require extraordinary amount of whitespace in deeply nested cases.

Would be good to know in which cases it's being used though.

### How to reproduce

You can use above dag.

### Operating System

Docker/breeze on MacOS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",mobuchowski,2024-08-15 14:45:13+00:00,['jedcunningham'],2024-09-03 01:42:38+00:00,2024-09-03 01:42:38+00:00,https://github.com/apache/airflow/issues/41505,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2293050972, 'issue_id': 2468206711, 'author': 'jedcunningham', 'body': 'This is a better reproduction DAG:\r\n\r\n```\r\nwith DAG(""aaa_repro"", schedule=None):\r\n    start = EmptyOperator(task_id=""start"")\r\n\r\n    a = [\r\n        start\r\n        >> EmptyOperator(task_id=f""a_1_{i}"")\r\n        >> EmptyOperator(task_id=f""a_2_{i}"")\r\n        >> EmptyOperator(task_id=f""a_3_{i}"")\r\n        for i in range(200)\r\n    ]\r\n\r\n    middle = EmptyOperator(task_id=""middle"")\r\n\r\n    b = [\r\n        middle\r\n        >> EmptyOperator(task_id=f""b_1_{i}"")\r\n        >> EmptyOperator(task_id=f""b_2_{i}"")\r\n        >> EmptyOperator(task_id=f""b_3_{i}"")\r\n        for i in range(200)\r\n    ]\r\n\r\n    middle2 = EmptyOperator(task_id=""middle2"")\r\n\r\n    c = [\r\n        middle2\r\n        >> EmptyOperator(task_id=f""c_1_{i}"")\r\n        >> EmptyOperator(task_id=f""c_2_{i}"")\r\n        >> EmptyOperator(task_id=f""c_3_{i}"")\r\n        for i in range(200)\r\n    ]\r\n\r\n    end = EmptyOperator(task_id=""end"")\r\n\r\n    start >> a >> middle >> b >> middle2 >> c >> end\r\n```\r\n\r\nIt uses 5+GB and takes just under 8 minutes to generate on my machine.\r\n\r\nAnd I believe it\'s less about the whitespace and more that we duplicate tasks in the output. For example, this DAG:\r\n\r\n```\r\nwith DAG(""aaa_runaway"", schedule=None):\r\n    start = EmptyOperator(task_id=""start"")\r\n    x = [EmptyOperator(task_id=f""x_{i}"") for i in range(3)]\r\n    middle = EmptyOperator(task_id=""middle"")\r\n    y = [EmptyOperator(task_id=f""y_{i}"") for i in range(3)]\r\n    end = EmptyOperator(task_id=""end"")\r\n\r\n    start >> x >> middle >> y >> end\r\n```\r\n\r\nwhich results in this output:\r\n```\r\n<Task(EmptyOperator): start>\r\n    <Task(EmptyOperator): x_0>\r\n        <Task(EmptyOperator): middle>\r\n            <Task(EmptyOperator): y_0>\r\n                <Task(EmptyOperator): end>\r\n            <Task(EmptyOperator): y_1>\r\n                <Task(EmptyOperator): end>\r\n            <Task(EmptyOperator): y_2>\r\n                <Task(EmptyOperator): end>\r\n    <Task(EmptyOperator): x_1>\r\n        <Task(EmptyOperator): middle>\r\n            <Task(EmptyOperator): y_0>\r\n                <Task(EmptyOperator): end>\r\n            <Task(EmptyOperator): y_1>\r\n                <Task(EmptyOperator): end>\r\n            <Task(EmptyOperator): y_2>\r\n                <Task(EmptyOperator): end>\r\n    <Task(EmptyOperator): x_2>\r\n        <Task(EmptyOperator): middle>\r\n            <Task(EmptyOperator): y_0>\r\n                <Task(EmptyOperator): end>\r\n            <Task(EmptyOperator): y_1>\r\n                <Task(EmptyOperator): end>\r\n            <Task(EmptyOperator): y_2>\r\n                <Task(EmptyOperator): end>\r\n```\r\n\r\nNote how we get 9 `ends`. If you have a sufficiently complex DAG, this format becomes really problematic.', 'created_at': datetime.datetime(2024, 8, 16, 8, 17, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2293069069, 'issue_id': 2468206711, 'author': 'jedcunningham', 'body': 'It looks like `get_tree_view` is only used in a test and is relatively new from #37162. Probably should deprecate it in 2 so we can remove it in 3.', 'created_at': datetime.datetime(2024, 8, 16, 8, 27, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325453811, 'issue_id': 2468206711, 'author': 'jedcunningham', 'body': 'I\'ve marked the whole ""tree"" concept - the `--tree` flag and the helper functions - as deprecated in 2.10 and removed them in 3.', 'created_at': datetime.datetime(2024, 9, 3, 1, 42, 38, tzinfo=datetime.timezone.utc)}]","jedcunningham (Assginee) on (2024-08-16 08:17:30 UTC): This is a better reproduction DAG:

```
with DAG(""aaa_repro"", schedule=None):
    start = EmptyOperator(task_id=""start"")

    a = [
        start
        for i in range(200)
    ]

    middle = EmptyOperator(task_id=""middle"")

    b = [
        middle
        for i in range(200)
    ]

    middle2 = EmptyOperator(task_id=""middle2"")

    c = [
        middle2
        for i in range(200)
    ]

    end = EmptyOperator(task_id=""end"")

    start >> a >> middle >> b >> middle2 >> c >> end
```

It uses 5+GB and takes just under 8 minutes to generate on my machine.

And I believe it's less about the whitespace and more that we duplicate tasks in the output. For example, this DAG:

```
with DAG(""aaa_runaway"", schedule=None):
    start = EmptyOperator(task_id=""start"")
    x = [EmptyOperator(task_id=f""x_{i}"") for i in range(3)]
    middle = EmptyOperator(task_id=""middle"")
    y = [EmptyOperator(task_id=f""y_{i}"") for i in range(3)]
    end = EmptyOperator(task_id=""end"")

    start >> x >> middle >> y >> end
```

which results in this output:
```
<Task(EmptyOperator): start>
    <Task(EmptyOperator): x_0>
        <Task(EmptyOperator): middle>
            <Task(EmptyOperator): y_0>
                <Task(EmptyOperator): end>
            <Task(EmptyOperator): y_1>
                <Task(EmptyOperator): end>
            <Task(EmptyOperator): y_2>
                <Task(EmptyOperator): end>
    <Task(EmptyOperator): x_1>
        <Task(EmptyOperator): middle>
            <Task(EmptyOperator): y_0>
                <Task(EmptyOperator): end>
            <Task(EmptyOperator): y_1>
                <Task(EmptyOperator): end>
            <Task(EmptyOperator): y_2>
                <Task(EmptyOperator): end>
    <Task(EmptyOperator): x_2>
        <Task(EmptyOperator): middle>
            <Task(EmptyOperator): y_0>
                <Task(EmptyOperator): end>
            <Task(EmptyOperator): y_1>
                <Task(EmptyOperator): end>
            <Task(EmptyOperator): y_2>
                <Task(EmptyOperator): end>
```

Note how we get 9 `ends`. If you have a sufficiently complex DAG, this format becomes really problematic.

jedcunningham (Assginee) on (2024-08-16 08:27:19 UTC): It looks like `get_tree_view` is only used in a test and is relatively new from #37162. Probably should deprecate it in 2 so we can remove it in 3.

jedcunningham (Assginee) on (2024-09-03 01:42:38 UTC): I've marked the whole ""tree"" concept - the `--tree` flag and the helper functions - as deprecated in 2.10 and removed them in 3.

"
2468110633,issue,closed,completed,Incorrect try number producing invalid span id for OTEL airflow,"### Apache Airflow version

2.10.0rc1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When OTEL Airflow is enabled, there are series of IDs for trace and span being generated.
Part of the ID generation process involves using 'try number' of the task instance to produce a unique ID representation of it. Try number has been somewhat unstable in Airflow, where many of its deficiencies were fixed in here: https://github.com/apache/airflow/pull/39336. Now, as try number issues are resolved, it is necessary to fix the OTEL airflow to not:
- increment or decrement try_number of task instance when generating span id
- increment or decrement try_number when instrumenting information related to task instance

Currently, the task instance contains span_id that may not be correct depending on the try_number.

### What you think should happen instead?

- spans containing span_id should reflect the correct try_number of the task instance
- information about try number is correctly displayed when emitting span

### How to reproduce

Run an example DAG file, and monitor the span id being generated. Observe that span id generated from the span link does not match that of task instance span due to the fact that task instance will have the try_number decremented conditionally.

### Operating System

All OS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",howardyoo,2024-08-15 14:00:29+00:00,[],2024-08-16 13:16:37+00:00,2024-08-16 13:16:36+00:00,https://github.com/apache/airflow/issues/41501,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2467907164,issue,closed,not_planned,LambdaInvokeFunctionOperator throwing ReadTimeout error even when the actual lambda invocation completed within the time limits,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When invoking lambda functions using `LambdaInvokeFunctionOperator`, the task continues to run even after the actual lambda invocation is completed. It then throws a `ReadTimeoutError`.

It is more common with Lambda functions that take more than 13 minutes to run. For Lambda functions that take more than 4 minutes, this is common when multiple tasks with `LambdaInvokeFunctionOperator` are triggered(I.e., Invoke the same lambda).

I have followed the recommended settings as mentioned [here](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/lambda.html#invoke-an-aws-lambda-function):

My AWS connection extra parameter has the following json
```
{
""config_kwargs"": {
    ""connect_timeout"": 5,
    ""read_timeout"": 900,
    ""tcp_keepalive"": true,
    ""retries"": {
      ""max_attempts"": 0
    }
  }
}
```

I did set the max timeout to 15 minutes for the lambda function on AWS.

For the mentioned recommendations on the docs:

1. [NAT Gateway Troubleshooting: Internet connection drops after 350 seconds](https://docs.aws.amazon.com/vpc/latest/userguide/nat-gateway-troubleshooting.html#nat-gateway-troubleshooting-timeout)

>  I have noticed this issue even with lambda functions that take 4 minutes to run. However, ReadTimeouts occur relatively rarely and mostly happen when running multiple invocations in quick succession.


2. [Using TCP keepalive under Linux](https://tldp.org/HOWTO/TCP-Keepalive-HOWTO/usingkeepalive.html)

> I have updated `sysctl.conf` as below:
```
echo ""net.ipv4.tcp_keepalive_time = 320"" >> /etc/sysctl.conf 
echo ""net.ipv4.tcp_keepalive_intvl = 60"" >> /etc/sysctl.conf 
echo ""net.ipv4.tcp_keepalive_probes = 20"" >> /etc/sysctl.conf
```

### What you think should happen instead?

Tasks Should not run beyond the actual completion of the lambda invocations. 

### How to reproduce

Use the following   DAG:

```
from datetime import datetime
from airflow import DAG
from airflow.providers.amazon.aws.operators.lambda_function import LambdaInvokeFunctionOperator

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2024, 1, 1),
}

with DAG('invoke_lambda_dag', default_args=default_args, schedule_interval=None, catchup=False) as dag:
    invoke_lambda_task = LambdaInvokeFunctionOperator(
        task_id='invoke_lambda',
        function_name='runForFifteenMinutes',
        payload='{""key1"": ""value1"",""key2"": ""value2"",""key3"": ""value3""}',
        aws_conn_id='aws'
    )
```

Create an AWS connection with the following json in the extra(You might need to add AWS `aws_session_token` and `region_name` to the extra:

```
{
""config_kwargs"": {
    ""connect_timeout"": 5,
    ""read_timeout"": 900,
    ""tcp_keepalive"": true,
    ""retries"": {
      ""max_attempts"": 0
    }
  }
}

```

On AWS, create a lambda function, and update timeout to 15 minutes(That is the max possible value)

You can add `time.sleep(780)`(13 minutes) to your lambda code so that it runs for 13 minutes.

Also decrease sleep time to 4 minutes and trigger the DAG multiple times quickly to reproduce ReadTimeout's

### Operating System

ubuntu-22.04

### Versions of Apache Airflow Providers

```apache-airflow-providers-amazon==8.27.0```

### Deployment

Docker-Compose

### Deployment details

Docker file update as below 
```
FROM apache/airflow:2.9.3
ADD requirements.txt .
RUN pip install apache-airflow==${AIRFLOW_VERSION} -r requirements.txt

RUN apt-get update && apt-get install -y procps

# Set TCP keepalive settings
RUN echo ""net.ipv4.tcp_keepalive_time = 600"" >> /etc/sysctl.conf && \
    echo ""net.ipv4.tcp_keepalive_intvl = 60"" >> /etc/sysctl.conf && \
    echo ""net.ipv4.tcp_keepalive_probes = 20"" >> /etc/sysctl.conf

# Apply sysctl settings
RUN sysctl -p
```


Docker-compose.yml
```
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# ""License""); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

# Basic Airflow cluster configuration for CeleryExecutor with Redis and PostgreSQL.
#
# WARNING: This configuration is for local development. Do not use it in a production deployment.
#
# This configuration supports basic configuration using environment variables or an .env file
# The following variables are supported:
#
# AIRFLOW_IMAGE_NAME           - Docker image name used to run Airflow.
#                                Default: apache/airflow:2.9.3
# AIRFLOW_UID                  - User ID in Airflow containers
#                                Default: 50000
# AIRFLOW_PROJ_DIR             - Base path to which all the files will be volumed.
#                                Default: .
# Those configurations are useful mostly in case of standalone testing/running Airflow in test/try-out mode
#
# _AIRFLOW_WWW_USER_USERNAME   - Username for the administrator account (if requested).
#                                Default: airflow
# _AIRFLOW_WWW_USER_PASSWORD   - Password for the administrator account (if requested).
#                                Default: airflow
# _PIP_ADDITIONAL_REQUIREMENTS - Additional PIP requirements to add when starting all containers.
#                                Use this option ONLY for quick checks. Installing requirements at container
#                                startup is done EVERY TIME the service is started.
#                                A better way is to build a custom image or extend the official image
#                                as described in https://airflow.apache.org/docs/docker-stack/build.html.
#                                Default: ''
#
# Feel free to modify this file to suit your needs.
---
x-airflow-common:
  &airflow-common
  # In order to add custom dependencies or upgrade provider packages you can use your extended image.
  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml
  # and uncomment the ""build"" line below, Then run `docker-compose build` to build the images.
  # image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.9.3}
  build: .
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    # yamllint disable rule:line-length
    # Use simple http server on scheduler for health checks
    # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server
    # yamllint enable rule:line-length
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks
    # for other purpose (development, test and especially production usage) build/extend Airflow image.
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
    # The following line can be used to set a custom config file, stored in the local config folder
    # If you want to use it, outcomment it and replace airflow.cfg with the name of your config file
    # AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'
  volumes:
    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
  user: ""${AIRFLOW_UID:-50000}:0""
  depends_on:
    &airflow-common-depends-on
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: [""CMD"", ""pg_isready"", ""-U"", ""airflow""]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always

  redis:
    # Redis is limited to 7.2-bookworm due to licencing change
    # https://redis.io/blog/redis-adopts-dual-source-available-licensing/
    image: redis:7.2-bookworm
    expose:
      - 6379
    healthcheck:
      test: [""CMD"", ""redis-cli"", ""ping""]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    restart: always

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - ""8080:8080""
    healthcheck:
      test: [""CMD"", ""curl"", ""--fail"", ""http://localhost:8080/health""]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: [""CMD"", ""curl"", ""--fail"", ""http://localhost:8974/health""]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-worker:
    <<: *airflow-common
    command: celery worker
    healthcheck:
      # yamllint disable rule:line-length
      test:
        - ""CMD-SHELL""
        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d ""celery@$${HOSTNAME}"" || celery --app airflow.executors.celery_executor.app inspect ping -d ""celery@$${HOSTNAME}""'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    environment:
      <<: *airflow-common-env
      # Required to handle warm shutdown of the celery workers properly
      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation
      DUMB_INIT_SETSID: ""0""
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-triggerer:
    <<: *airflow-common
    command: triggerer
    healthcheck:
      test: [""CMD-SHELL"", 'airflow jobs check --job-type TriggererJob --hostname ""$${HOSTNAME}""']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    # yamllint disable rule:line-length
    command:
      - -c
      - |
        if [[ -z ""${AIRFLOW_UID}"" ]]; then
          echo
          echo -e ""\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m""
          echo ""If you are on Linux, you SHOULD follow the instructions below to set ""
          echo ""AIRFLOW_UID environment variable, otherwise files will be owned by root.""
          echo ""For other operating systems you can get rid of the warning with manually created .env file:""
          echo ""    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user""
          echo
        fi
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources=""false""
        if (( mem_available < 4000 )) ; then
          echo
          echo -e ""\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m""
          echo ""At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))""
          echo
          warning_resources=""true""
        fi
        if (( cpus_available < 2 )); then
          echo
          echo -e ""\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m""
          echo ""At least 2 CPUs recommended. You have $${cpus_available}""
          echo
          warning_resources=""true""
        fi
        if (( disk_available < one_meg * 10 )); then
          echo
          echo -e ""\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m""
          echo ""At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))""
          echo
          warning_resources=""true""
        fi
        if [[ $${warning_resources} == ""true"" ]]; then
          echo
          echo -e ""\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m""
          echo ""Please follow the instructions to increase amount of resources available:""
          echo ""   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin""
          echo
        fi
        mkdir -p /sources/logs /sources/dags /sources/plugins
        chown -R ""${AIRFLOW_UID}:0"" /sources/{logs,dags,plugins}
        exec /entrypoint airflow version
    # yamllint enable rule:line-length
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
      _PIP_ADDITIONAL_REQUIREMENTS: ''
    user: ""0:0""
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}:/sources

  airflow-cli:
    <<: *airflow-common
    profiles:
      - debug
    environment:
      <<: *airflow-common-env
      CONNECTION_CHECK_MAX_COUNT: ""0""
    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
    command:
      - bash
      - -c
      - airflow

  # You can enable flower by adding ""--profile flower"" option e.g. docker-compose --profile flower up
  # or by explicitly targeted on the command line e.g. docker-compose up flower.
  # See: https://docs.docker.com/compose/profiles/
  flower:
    <<: *airflow-common
    command: celery flower
    profiles:
      - flower
    ports:
      - ""5555:5555""
    healthcheck:
      test: [""CMD"", ""curl"", ""--fail"", ""http://localhost:5555/""]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

volumes:
  postgres-db-volume:
```

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rawwar,2024-08-15 11:51:08+00:00,[],2024-10-17 04:38:54+00:00,2024-09-20 00:14:39+00:00,https://github.com/apache/airflow/issues/41498,"[('kind:bug', 'This is a clearly a bug'), (""Can't Reproduce"", 'The problem cannot be reproduced'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('good first issue', ''), ('pending-response', ''), ('area:core', '')]","[{'comment_id': 2291317337, 'issue_id': 2467907164, 'author': 'eladkal', 'body': 'cc @vincbeck @o-nikolas looks like a valid bug', 'created_at': datetime.datetime(2024, 8, 15, 13, 57, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2294438830, 'issue_id': 2467907164, 'author': 'gopidesupavan', 'body': 'Hi @rawwar i have tried the steps you have mentioned from my local, but not been able to re produce the issue, in my case its getting succeeded. is there any other information help to reproduce this issue? could you please provide. \r\n\r\nFirst task: \r\n\r\n<img width=""1225"" alt=""image"" src=""https://github.com/user-attachments/assets/ccaed10f-1982-4788-b1c1-1d8d80d6f590"">\r\n \r\nBetween there are couple of tasks triggered with seconds gap.\r\n\r\nlast task with after 4minutes.\r\n\r\n<img width=""1153"" alt=""image"" src=""https://github.com/user-attachments/assets/92573a58-df8f-4322-9eca-981210488e17"">', 'created_at': datetime.datetime(2024, 8, 16, 23, 21, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2314893089, 'issue_id': 2467907164, 'author': 'rawwar', 'body': '@gopidesupavan and I connected last week to replicate the issue. But the problem is only happening on my local setup. I am testing this out on a fresh EC2 instance and will provide an update.', 'created_at': datetime.datetime(2024, 8, 28, 10, 7, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2345009560, 'issue_id': 2467907164, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 9, 12, 0, 14, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2362459840, 'issue_id': 2467907164, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 9, 20, 0, 14, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2417168879, 'issue_id': 2467907164, 'author': 'sachinbct', 'body': 'I am also getting the same issue when i am deploying it on AWS.   Can anyone help for resolution?', 'created_at': datetime.datetime(2024, 10, 16, 15, 25, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2417266553, 'issue_id': 2467907164, 'author': 'rawwar', 'body': '@sachinbct , can you please share more details about your env/setup?', 'created_at': datetime.datetime(2024, 10, 16, 16, 1, 29, tzinfo=datetime.timezone.utc)}, {'comment_id': 2418024705, 'issue_id': 2467907164, 'author': 'michael-lok', 'body': 'I am getting the same issue from AWS MWAA (Managed Workflows for Apache Airflow).\r\n\r\nMy extras config contains the following:\r\n\r\n```\r\n{\r\n    ""config_kwargs"": {\r\n        ""read_timeout"": 900,\r\n        ""connect_timeout"": 900,\r\n        ""tcp_keepalive"": true\r\n}\r\n```\r\n\r\nHowever, the LambdaFunctionInvokeOperator is failing after 5:30 minutes. Lambda function invocation itself is successful after ~1.5 minutes.', 'created_at': datetime.datetime(2024, 10, 16, 21, 47, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2418473257, 'issue_id': 2467907164, 'author': 'sachinbct', 'body': ""@michael-lok - I had also done the same extra configs.  Even I tried to change botocore_config which we pass to InvoleAWSLambdaOperator as below -\r\nconfig = Config(\r\n        connect_timeout=900,\r\n        read_timeout=900,\r\n        retries={\r\n            'max_attempts': 5,\r\n            'mode': 'adaptive'\r\n        },\r\n        tcp_keepalive=True\r\n    )\r\nbut still no luck.\r\nAirflow setup includes, input and output S3 buckets on AWS"", 'created_at': datetime.datetime(2024, 10, 17, 4, 29, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2418482036, 'issue_id': 2467907164, 'author': 'rawwar', 'body': '@sachinbct , I think this is only happening on deployments on AWS. And, the fix needs to be done in botocore: https://github.com/boto/botocore/pull/3140 . I verified this fix and its working once you update the botocore with these changes', 'created_at': datetime.datetime(2024, 10, 17, 4, 38, 53, tzinfo=datetime.timezone.utc)}]","eladkal on (2024-08-15 13:57:58 UTC): cc @vincbeck @o-nikolas looks like a valid bug

gopidesupavan on (2024-08-16 23:21:34 UTC): Hi @rawwar i have tried the steps you have mentioned from my local, but not been able to re produce the issue, in my case its getting succeeded. is there any other information help to reproduce this issue? could you please provide. 

First task: 

<img width=""1225"" alt=""image"" src=""https://github.com/user-attachments/assets/ccaed10f-1982-4788-b1c1-1d8d80d6f590"">
 
Between there are couple of tasks triggered with seconds gap.

last task with after 4minutes.

<img width=""1153"" alt=""image"" src=""https://github.com/user-attachments/assets/92573a58-df8f-4322-9eca-981210488e17"">

rawwar (Issue Creator) on (2024-08-28 10:07:55 UTC): @gopidesupavan and I connected last week to replicate the issue. But the problem is only happening on my local setup. I am testing this out on a fresh EC2 instance and will provide an update.

github-actions[bot] on (2024-09-12 00:14:32 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-09-20 00:14:39 UTC): This issue has been closed because it has not received response from the issue author.

sachinbct on (2024-10-16 15:25:52 UTC): I am also getting the same issue when i am deploying it on AWS.   Can anyone help for resolution?

rawwar (Issue Creator) on (2024-10-16 16:01:29 UTC): @sachinbct , can you please share more details about your env/setup?

michael-lok on (2024-10-16 21:47:46 UTC): I am getting the same issue from AWS MWAA (Managed Workflows for Apache Airflow).

My extras config contains the following:

```
{
    ""config_kwargs"": {
        ""read_timeout"": 900,
        ""connect_timeout"": 900,
        ""tcp_keepalive"": true
}
```

However, the LambdaFunctionInvokeOperator is failing after 5:30 minutes. Lambda function invocation itself is successful after ~1.5 minutes.

sachinbct on (2024-10-17 04:29:05 UTC): @michael-lok - I had also done the same extra configs.  Even I tried to change botocore_config which we pass to InvoleAWSLambdaOperator as below -
config = Config(
        connect_timeout=900,
        read_timeout=900,
        retries={
            'max_attempts': 5,
            'mode': 'adaptive'
        },
        tcp_keepalive=True
    )
but still no luck.
Airflow setup includes, input and output S3 buckets on AWS

rawwar (Issue Creator) on (2024-10-17 04:38:53 UTC): @sachinbct , I think this is only happening on deployments on AWS. And, the fix needs to be done in botocore: https://github.com/boto/botocore/pull/3140 . I verified this fix and its working once you update the botocore with these changes

"
2467838907,issue,closed,completed,Make `dag_ids` parameter optional for `dagStats` REST API,"### Description

Today, the `dagStats` REST API [requires a list of `dag_ids` as input](https://github.com/dondaum/airflow/blob/619d98932a023c80c24ca97e28d2fe3f9f11ba77/airflow/api_connexion/openapi/v1.yaml#L2164-L2177).

Can we make this parameter optional, so that if no `dag_ids` are provided, it will return all the stats of all the dags? This is the current behavior of the `dag_stats` endpoint, so it'd be good to have parity with the REST API so that the UI could use this same REST API in the future.

### Use case/motivation

Clients should be able to make a REST API call to get all the dag stats without requiring specific dag ids. 

### Related issues

https://github.com/apache/airflow/pull/41017 (see specific discussion in the PR here: https://github.com/apache/airflow/pull/41017#discussion_r1698824381)

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",reganbaum,2024-08-15 10:54:22+00:00,"['dondaum', 'michaeljs-c']",2024-10-13 12:57:52+00:00,2024-10-13 12:57:52+00:00,https://github.com/apache/airflow/issues/41495,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('good first issue', ''), ('contributors-workshop', ""Issues that are good for first-time contributor's workshop"")]","[{'comment_id': 2293741825, 'issue_id': 2467838907, 'author': 'dondaum', 'body': 'I would be happy to help with this one again.', 'created_at': datetime.datetime(2024, 8, 16, 15, 44, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2310227181, 'issue_id': 2467838907, 'author': 'dondaum', 'body': 'Friendly reminder.', 'created_at': datetime.datetime(2024, 8, 26, 13, 31, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2332108313, 'issue_id': 2467838907, 'author': 'Yusin0903', 'body': 'Hi, Can I try this issue?', 'created_at': datetime.datetime(2024, 9, 5, 15, 58, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2332226645, 'issue_id': 2467838907, 'author': 'potiuk', 'body': 'Feel free to collaborate on that one @dondaum @Yusin0903', 'created_at': datetime.datetime(2024, 9, 5, 17, 0, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333388711, 'issue_id': 2467838907, 'author': 'dondaum', 'body': ""@Yusin0903 I've already contributed to a similar feature request. So from my side you can take care of this and I can help you if you need help. What do you think?"", 'created_at': datetime.datetime(2024, 9, 6, 7, 5, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2334483572, 'issue_id': 2467838907, 'author': 'Yusin0903', 'body': '@dondaum ok, thank you! I will try it.', 'created_at': datetime.datetime(2024, 9, 6, 17, 8, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2396898499, 'issue_id': 2467838907, 'author': 'Yusin0903', 'body': '@dondaum Hi, because I am busy recently. I think I can not do it so well. Can u continue executing it?', 'created_at': datetime.datetime(2024, 10, 7, 13, 13, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2399054497, 'issue_id': 2467838907, 'author': 'dondaum', 'body': '> @dondaum Hi, because I am busy recently. I think I can not do it so well. Can u continue executing it?\r\n\r\nSure. I will take this one over.', 'created_at': datetime.datetime(2024, 10, 8, 7, 25, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2400806717, 'issue_id': 2467838907, 'author': 'michaeljs-c', 'body': ""Hi @dondaum I'm looking to make my first contribution. Could I take part in this one?"", 'created_at': datetime.datetime(2024, 10, 8, 20, 56, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401518427, 'issue_id': 2467838907, 'author': 'dondaum', 'body': ""> Hi @dondaum I'm looking to make my first contribution. Could I take part in this one?\r\n\r\nSure. I think it is hard to split it up. So from my side you can take care of this and I can help you if you need help. What do you think?"", 'created_at': datetime.datetime(2024, 10, 9, 7, 12, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2402464174, 'issue_id': 2467838907, 'author': 'dondaum', 'body': '@potiuk can you please assign @michaeljs-c', 'created_at': datetime.datetime(2024, 10, 9, 14, 11, 51, tzinfo=datetime.timezone.utc)}]","dondaum (Assginee) on (2024-08-16 15:44:08 UTC): I would be happy to help with this one again.

dondaum (Assginee) on (2024-08-26 13:31:51 UTC): Friendly reminder.

Yusin0903 on (2024-09-05 15:58:37 UTC): Hi, Can I try this issue?

potiuk on (2024-09-05 17:00:22 UTC): Feel free to collaborate on that one @dondaum @Yusin0903

dondaum (Assginee) on (2024-09-06 07:05:34 UTC): @Yusin0903 I've already contributed to a similar feature request. So from my side you can take care of this and I can help you if you need help. What do you think?

Yusin0903 on (2024-09-06 17:08:10 UTC): @dondaum ok, thank you! I will try it.

Yusin0903 on (2024-10-07 13:13:40 UTC): @dondaum Hi, because I am busy recently. I think I can not do it so well. Can u continue executing it?

dondaum (Assginee) on (2024-10-08 07:25:48 UTC): Sure. I will take this one over.

michaeljs-c (Assginee) on (2024-10-08 20:56:31 UTC): Hi @dondaum I'm looking to make my first contribution. Could I take part in this one?

dondaum (Assginee) on (2024-10-09 07:12:45 UTC): Sure. I think it is hard to split it up. So from my side you can take care of this and I can help you if you need help. What do you think?

dondaum (Assginee) on (2024-10-09 14:11:51 UTC): @potiuk can you please assign @michaeljs-c

"
2466910426,issue,closed,completed,Email on failure stuck indefinitely (not timing out),"### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I’ve run into an issue where the email on failure feature causes a task to be indefinitly stuck without ever timing out. This problem was caused by firewall restrictions preventing outgoing traffic but shouldnt the emailing process timeout after 30s by default?

### What you think should happen instead?

The email process should timeout.

### How to reproduce

Block outgoing traffic via firewall restrictions.

### Operating System

Ubuntu

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",pedro-cf,2024-08-14 21:59:41+00:00,[],2024-09-06 19:11:49+00:00,2024-09-06 19:11:49+00:00,https://github.com/apache/airflow/issues/41490,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('pending-response', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2295424864, 'issue_id': 2466910426, 'author': 'sunank200', 'body': '@pedro-cf Thank you for submitting the issue. Have you verified the smtp_timeout value in your airflow.cfg or environment variables? Also, please check if other settings like smtp_retry_limit and smtp_ssl are configured, as they might be affecting the behavior. Additionally, could you provide the logs from a task that got stuck due to this issue for further analysis?', 'created_at': datetime.datetime(2024, 8, 18, 22, 58, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2295995800, 'issue_id': 2466910426, 'author': 'pedro-cf', 'body': ""> @pedro-cf Thank you for submitting the issue. Have you verified the smtp_timeout value in your airflow.cfg or environment variables? Also, please check if other settings like smtp_retry_limit and smtp_ssl are configured, as they might be affecting the behavior. Additionally, could you provide the logs from a task that got stuck due to this issue for further analysis?\n\n@sunank200 \n\n- smtp_timeout: default\n- smtp_retry_limit: default\n- smtp_ssl: configured\n\nCurrently on vacation so don't have access to logs. Will provide asap."", 'created_at': datetime.datetime(2024, 8, 19, 8, 37, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2325403636, 'issue_id': 2466910426, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 9, 3, 0, 13, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333997688, 'issue_id': 2466910426, 'author': 'pedro-cf', 'body': 'Sorry I am unable to provide the logs @sunank200', 'created_at': datetime.datetime(2024, 9, 6, 12, 59, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2334665360, 'issue_id': 2466910426, 'author': 'potiuk', 'body': 'So we close this one until someone can reproduce the error.', 'created_at': datetime.datetime(2024, 9, 6, 19, 11, 49, tzinfo=datetime.timezone.utc)}]","sunank200 on (2024-08-18 22:58:14 UTC): @pedro-cf Thank you for submitting the issue. Have you verified the smtp_timeout value in your airflow.cfg or environment variables? Also, please check if other settings like smtp_retry_limit and smtp_ssl are configured, as they might be affecting the behavior. Additionally, could you provide the logs from a task that got stuck due to this issue for further analysis?

pedro-cf (Issue Creator) on (2024-08-19 08:37:31 UTC): @sunank200 

- smtp_timeout: default
- smtp_retry_limit: default
- smtp_ssl: configured

Currently on vacation so don't have access to logs. Will provide asap.

github-actions[bot] on (2024-09-03 00:13:50 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

pedro-cf (Issue Creator) on (2024-09-06 12:59:11 UTC): Sorry I am unable to provide the logs @sunank200

potiuk on (2024-09-06 19:11:49 UTC): So we close this one until someone can reproduce the error.

"
2466566505,issue,open,,ECS EcsTaskStateSensor and EcsRunTaskOperator marked as Zombie and retried as they don't seem to be updating their heartbeat,"### Apache Airflow Provider(s)

amazon

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.24.0
Airflow==2.9.2

### Apache Airflow version

2.9.2

### Operating System

Debian GNU/Linux 12 (bookworm)

### Deployment

Other Docker-based deployment

### Deployment details

Airflow deployed on AWS ECS as docker containers with tasks run as ECS Tasks using `EcsRunTaskOperator`

### What happened

Long running Tasks (say greater than 1 hour) run via `EcsRunTaskOperator` execute correctly but after 60 mins or so Airflow scheduler treats them as Zombies and retries them creating duplicate tasks.

If we set `wait_for_creation` as False and use a Sensor like `EcsTaskStateSensor` to monitor state of ECS Task that also gets marked as a zombie and retried

Relevant Log lines
From the task logs
```
[2024-08-14, 11:04:39 PDT] {ecs.py:176} INFO - Task state: RUNNING, waiting for: STOPPED
[2024-08-14, 11:05:17 PDT] {scheduler_job_runner.py:1737} ERROR - Detected zombie job: {'full_filepath': '/opt/airflow/dags/test_ecs_task_dag.py', 'processor_subdir': '/opt/airflow/dags', 'msg': ""{'DAG Id': 'test_ecs_task_dag', 'Task Id': 'await_ecs_task_run', 'Run Id': 'manual__2024-08-14T10:49:23-07:00', 'Hostname': 'somehost.us-east-2.compute.internal', 'External Executor Id': '20d42f77-bd77-46ec-bf16-cd4e5faae379'}"", 'simple_task_instance': <airflow.models.taskinstance.SimpleTaskInstance object at 0x7f3c22cd3dd0>, 'is_failure_callback': True} (See https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/tasks.html#zombie-undead-tasks)
[2024-08-14, 11:05:39 PDT] {ecs.py:176} INFO - Task state: RUNNING, waiting for: STOPPED
```
From celery executors taskmeta
```
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.12/site-packages/celery/app/trace.py"", line 453, in trace_task
    R = retval = fun(*args, **kwargs)
                 ^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/celery/app/trace.py"", line 736, in __protected_call__
    return self.run(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/celery/executors/celery_executor_utils.py"", line 136, in execute_command
    _execute_in_fork(command_to_exec, celery_task_id)
  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/providers/celery/executors/celery_executor_utils.py"", line 151, in _execute_in_fork
    raise AirflowException(msg)
airflow.exceptions.AirflowException: Celery command failed on host: ip-10-1-27-248.us-east-2.compute.internal with celery_task_id fff791e8-62c0-4885-9d3e-e8ec46daae34 (PID: 1239, Return Code: 9)
```

Relevant Airflow config for zombie
`scheduler_zombie_task_threshold = 300` 


The scheduler container resources are not an issue, same for the task run using `EcsRunTaskOperator`. Both have health CPU and Mem available to them 

The container run as part of the task runs a shell script as its init which internally runs long running tasks

### What you think should happen instead

The Operator should either publish heartbeats at a faster cadence or allow for configuring heartbeat pushes?
If that is not the root cause can we have better mechanism for retries in this case? `EcsRunTaskOperator` has a reattach mechanism which generates the `startedBy` on each retry. Is it possible to allow users to set `startedBy` to allow retries to attach to already running tasks started by a previous task run?

### How to reproduce

Any Task that takes more than 60 mins ends up getting marked as Zombie. Sometimes even earlier than 60 mins 
Any ECS Task that takes more than 15 mins will also fail

We can look at it in the airflow db uder the jobs table. The `lastest_heartbeat` for the `EcsRunTaskOperator` never updates

### Anything else

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",prateeklohchubh,2024-08-14 18:38:20+00:00,[],2024-11-12 21:05:59+00:00,,https://github.com/apache/airflow/issues/41487,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('good first issue', '')]","[{'comment_id': 2289574166, 'issue_id': 2466566505, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 14, 18, 38, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2466632733, 'issue_id': 2466566505, 'author': 'eladkal', 'body': 'cc @vincbeck @ferruzzi @o-nikolas', 'created_at': datetime.datetime(2024, 11, 10, 8, 7, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2471581633, 'issue_id': 2466566505, 'author': 'ferruzzi', 'body': ""If you are up for taking on the fix, feel free to tag me in the PR and I'll give it a review.  :+1:"", 'created_at': datetime.datetime(2024, 11, 12, 21, 5, 58, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-14 18:38:22 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

eladkal on (2024-11-10 08:07:15 UTC): cc @vincbeck @ferruzzi @o-nikolas

ferruzzi on (2024-11-12 21:05:58 UTC): If you are up for taking on the fix, feel free to tag me in the PR and I'll give it a review.  :+1:

"
2466483537,issue,closed,completed,ElasticsearchSQLHook fails with AttributeError: __enter__,"### Apache Airflow Provider(s)

elasticsearch

### Versions of Apache Airflow Providers

apache-airflow-providers-common-io==1.3.2
apache-airflow-providers-common-sql==1.14.0
apache-airflow-providers-elasticsearch==5.4.1
apache-airflow-providers-fab==1.1.1
apache-airflow-providers-ftp==3.9.1
apache-airflow-providers-http==4.11.1
apache-airflow-providers-imap==3.6.1
apache-airflow-providers-smtp==1.7.1
apache-airflow-providers-sqlite==3.8.1

### Apache Airflow version

v2.9.2

### Operating System

v2.9.2

### Deployment

Virtualenv installation

### Deployment details

_No response_

### What happened

[Documentation's usage example](https://airflow.apache.org/docs/apache-airflow-providers-elasticsearch/5.4.1/hooks/elasticsearch_sql_hook.html) for ElasticsearchSQLHook:
```
from airflow.providers.elasticsearch.hooks.elasticsearch import ElasticsearchSQLHook

es = ElasticsearchSQLHook(elasticsearch_conn_id='elasticsearch')

# Handle ES conn with context manager
with es.get_conn() as es_conn:
    tables = es_conn.execute(""SHOW TABLES"")
    for table, *_ in tables:
        print(f""table: {table}"")
```
fails with:
```
Traceback (most recent call last):
  File ""/home/port/projects/Encortex/airflow/new_test.py"", line 6, in <module>
    with es.get_conn() as es_conn:
AttributeError: __enter__
```

### What you think should happen instead

_No response_

### How to reproduce

Implement [Documentation's usage example](https://airflow.apache.org/docs/apache-airflow-providers-elasticsearch/5.4.1/hooks/elasticsearch_sql_hook.html).

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Pooort,2024-08-14 17:55:23+00:00,['Owen-CH-Leung'],2024-08-17 04:15:41+00:00,2024-08-17 04:15:41+00:00,https://github.com/apache/airflow/issues/41486,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('provider:elasticsearch', '')]","[{'comment_id': 2289536211, 'issue_id': 2466483537, 'author': 'eladkal', 'body': 'Which version of elastic are you running?', 'created_at': datetime.datetime(2024, 8, 14, 18, 21, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2289586370, 'issue_id': 2466483537, 'author': 'Pooort', 'body': '8.13.1', 'created_at': datetime.datetime(2024, 8, 14, 18, 45, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2291006215, 'issue_id': 2466483537, 'author': 'eladkal', 'body': 'cc @Owen-CH-Leung maybe you can take a look?', 'created_at': datetime.datetime(2024, 8, 15, 10, 2, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2291241109, 'issue_id': 2466483537, 'author': 'Owen-CH-Leung', 'body': 'Sure. Will take a look soon', 'created_at': datetime.datetime(2024, 8, 15, 13, 11, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2292650950, 'issue_id': 2466483537, 'author': 'Owen-CH-Leung', 'body': ""Indeed, I can reproduce the same error as @Pooort pointed out: \r\n\r\n![image](https://github.com/user-attachments/assets/3d7717a0-ceaf-4eb4-91b0-b0f1c13354cd)\r\n\r\nThe root cause is that the current implementation of `ElasticsearchSQLHook` is dependent on the unofficial ES library `elasticsearch-dbapi` which we have already deprecated it since it doesn't support ES8.\r\n\r\nLuckily the official es python client has a SQL client available so we can easily refactor our implementation : \r\n\r\nhttps://github.com/elastic/elasticsearch-py/blob/main/elasticsearch/_sync/client/sql.py#L26\r\n\r\nI'll file a PR to refactor it soon"", 'created_at': datetime.datetime(2024, 8, 16, 3, 11, 6, tzinfo=datetime.timezone.utc)}]","eladkal on (2024-08-14 18:21:25 UTC): Which version of elastic are you running?

Pooort (Issue Creator) on (2024-08-14 18:45:18 UTC): 8.13.1

eladkal on (2024-08-15 10:02:07 UTC): cc @Owen-CH-Leung maybe you can take a look?

Owen-CH-Leung (Assginee) on (2024-08-15 13:11:03 UTC): Sure. Will take a look soon

Owen-CH-Leung (Assginee) on (2024-08-16 03:11:06 UTC): Indeed, I can reproduce the same error as @Pooort pointed out: 

![image](https://github.com/user-attachments/assets/3d7717a0-ceaf-4eb4-91b0-b0f1c13354cd)

The root cause is that the current implementation of `ElasticsearchSQLHook` is dependent on the unofficial ES library `elasticsearch-dbapi` which we have already deprecated it since it doesn't support ES8.

Luckily the official es python client has a SQL client available so we can easily refactor our implementation : 

https://github.com/elastic/elasticsearch-py/blob/main/elasticsearch/_sync/client/sql.py#L26

I'll file a PR to refactor it soon

"
2465573043,issue,open,,Airflow SFTPToGCSOperator sftp file exist check ,"### Description

Currently, the operator fails if a file is not found on the SFTP server. To make this behavior more flexible, we could introduce a new parameter, `fail_on_sftp_file_not_exist`, allowing users to customize the operator's response in such scenarios. By setting this parameter, users can choose whether the operator should raise an error when a file is missing or simply log a message and continue processing.

### Use case/motivation

This enhancement would give users greater control over the operator's behaviour, particularly in environments where missing files are expected or where continued processing is preferred.

### Related issues

A similar [issue](https://github.com/apache/airflow/issues/40576) has been opened for the `SFTPToS3Operator`.

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)

- [PR](https://github.com/apache/airflow/pull/41479)",kandharvishnu,2024-08-14 11:25:26+00:00,['kandharvishnu'],2024-08-22 01:42:22+00:00,,https://github.com/apache/airflow/issues/41472,"[('provider:google', 'Google (including GCP) related issues'), ('kind:feature', 'Feature Requests'), ('good first issue', '')]","[{'comment_id': 2290195349, 'issue_id': 2465573043, 'author': 'geraj1010', 'body': 'Is there an issue with using an `SFTPSensor` for checking file existence, prior to running `SFTPToGCSOperator`?', 'created_at': datetime.datetime(2024, 8, 15, 1, 3, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2296575917, 'issue_id': 2465573043, 'author': 'kandharvishnu', 'body': '> Is there an issue with using an `SFTPSensor` for checking file existence, prior to running `SFTPToGCSOperator`?\r\n\r\nThe SFTP sensor will wait until the file is detected. This issue is to determine whether the task should fail if the file is not found.', 'created_at': datetime.datetime(2024, 8, 19, 13, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2300595119, 'issue_id': 2465573043, 'author': 'geraj1010', 'body': ""> > Is there an issue with using an `SFTPSensor` for checking file existence, prior to running `SFTPToGCSOperator`?\r\n> \r\n> The SFTP sensor will wait until the file is detected. This issue is to determine whether the task should fail if the file is not found.\r\n\r\nI see. In that case, I don't think it makes sense to have the task succeed if a file is not found. A 'success' would not have much meaning anymore besides that the task simply ran. The interpretation of 'success' would become ambiguous i.e. how can you tell if a file was indeed moved or not without having to further dig into the logs? In addition, it doesn't feel very 'airflow-like' since we would be overloading the operator with functionality that already exists through a sensor.\r\n\r\nI'm not sure what your DAG looks like, but perhaps you can use a `SFTPSensor` and trigger rules for your downstream tasks? There is `soft_fail` for sensors too, if you want the sensor to 'skip' instead of 'fail'."", 'created_at': datetime.datetime(2024, 8, 21, 3, 38, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2301174472, 'issue_id': 2465573043, 'author': 'kandharvishnu', 'body': ""> > > Is there an issue with using an `SFTPSensor` for checking file existence, prior to running `SFTPToGCSOperator`?\r\n> > \r\n> > \r\n> > The SFTP sensor will wait until the file is detected. This issue is to determine whether the task should fail if the file is not found.\r\n> \r\n> I see. In that case, I don't think it makes sense to have the task succeed if a file is not found. A 'success' would not have much meaning anymore besides that the task simply ran. The interpretation of 'success' would become ambiguous i.e. how can you tell if a file was indeed moved or not without having to further dig into the logs? In addition, it doesn't feel very 'airflow-like' since we would be overloading the operator with functionality that already exists through a sensor.\r\n>  There is `soft_fail` for sensors too, if you want the sensor to 'skip' instead of 'fail'.\r\n\r\nI agree with your point, but this is an optional parameter. By default, it will fail the task if the file is not found. If the user sets the `fail_on_sftp_file_not_exist` to False, it would make some difference. \r\n\r\n> I'm not sure what your DAG looks like, but perhaps you can use a `SFTPSensor` and trigger rules for your downstream tasks?\r\n\r\nThe final task of my DAG is looking for a file, and the dag_run should not fail, though the file is not present. And I don't want to use the sensor to poke for a file."", 'created_at': datetime.datetime(2024, 8, 21, 5, 40, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2303477666, 'issue_id': 2465573043, 'author': 'geraj1010', 'body': ""> > > > Is there an issue with using an `SFTPSensor` for checking file existence, prior to running `SFTPToGCSOperator`?\r\n> > > \r\n> > > \r\n> > > The SFTP sensor will wait until the file is detected. This issue is to determine whether the task should fail if the file is not found.\r\n> > \r\n> > \r\n> > I see. In that case, I don't think it makes sense to have the task succeed if a file is not found. A 'success' would not have much meaning anymore besides that the task simply ran. The interpretation of 'success' would become ambiguous i.e. how can you tell if a file was indeed moved or not without having to further dig into the logs? In addition, it doesn't feel very 'airflow-like' since we would be overloading the operator with functionality that already exists through a sensor.\r\n> > There is `soft_fail` for sensors too, if you want the sensor to 'skip' instead of 'fail'.\r\n> \r\n> I agree with your point, but this is an optional parameter. By default, it will fail the task if the file is not found. If the user sets the `fail_on_sftp_file_not_exist` to False, it would make some difference.\r\n> \r\n> > I'm not sure what your DAG looks like, but perhaps you can use a `SFTPSensor` and trigger rules for your downstream tasks?\r\n> \r\n> The final task of my DAG is looking for a file, and the dag_run should not fail, though the file is not present. And I don't want to use the sensor to poke for a file.\r\n\r\nThat's unfortunate you don't want to use a sensor for checking for file existence. If the sensor failed (i.e. file didn't exist after some time), you could have it `soft_fail` and it would skip (instead of fail) and you could choose which downstream tasks get skipped or ran based on trigger rules. \r\n\r\nAt any rate, I think as long as you use the trigger rule `all_done` for one (or more) of the downstream tasks, the dag_run will still be marked as successful. So you could have your `SFTPToGCSOperator` task fail (because the file doesn't exist on SFTP), and have the trigger rule for all other downstream tasks set to `all_done`, and they will still run and the dag_run will still be considered successful."", 'created_at': datetime.datetime(2024, 8, 22, 1, 42, 21, tzinfo=datetime.timezone.utc)}]","geraj1010 on (2024-08-15 01:03:54 UTC): Is there an issue with using an `SFTPSensor` for checking file existence, prior to running `SFTPToGCSOperator`?

kandharvishnu (Issue Creator) on (2024-08-19 13:26:00 UTC): The SFTP sensor will wait until the file is detected. This issue is to determine whether the task should fail if the file is not found.

geraj1010 on (2024-08-21 03:38:48 UTC): I see. In that case, I don't think it makes sense to have the task succeed if a file is not found. A 'success' would not have much meaning anymore besides that the task simply ran. The interpretation of 'success' would become ambiguous i.e. how can you tell if a file was indeed moved or not without having to further dig into the logs? In addition, it doesn't feel very 'airflow-like' since we would be overloading the operator with functionality that already exists through a sensor.

I'm not sure what your DAG looks like, but perhaps you can use a `SFTPSensor` and trigger rules for your downstream tasks? There is `soft_fail` for sensors too, if you want the sensor to 'skip' instead of 'fail'.

kandharvishnu (Issue Creator) on (2024-08-21 05:40:46 UTC): I agree with your point, but this is an optional parameter. By default, it will fail the task if the file is not found. If the user sets the `fail_on_sftp_file_not_exist` to False, it would make some difference. 


The final task of my DAG is looking for a file, and the dag_run should not fail, though the file is not present. And I don't want to use the sensor to poke for a file.

geraj1010 on (2024-08-22 01:42:21 UTC): That's unfortunate you don't want to use a sensor for checking for file existence. If the sensor failed (i.e. file didn't exist after some time), you could have it `soft_fail` and it would skip (instead of fail) and you could choose which downstream tasks get skipped or ran based on trigger rules. 

At any rate, I think as long as you use the trigger rule `all_done` for one (or more) of the downstream tasks, the dag_run will still be marked as successful. So you could have your `SFTPToGCSOperator` task fail (because the file doesn't exist on SFTP), and have the trigger rule for all other downstream tasks set to `all_done`, and they will still run and the dag_run will still be considered successful.

"
2465536411,issue,closed,completed,"""execute cannot be called outside TaskInstance"" warning fired despite being in a TaskInstance","### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

BigQueryTableExistenceSensor fire an ""execute cannot be called outside TaskInstance"" warning on execute

My observation: The ExecutorSafeguard is being called twice for a single sensor execution

Reason is, that the first encounter is over the TaskInstance `_execute_callable` (as intended). Now the decorater checks, if it is executed within a Taskinstance. After that, it will call the ""real"" `execute` function, which is also decorated and therefor will do the check again, but without a sentinel, which leads to a failed check and fires a warning (or raise a exception if allow_nested_operators is set).

See also: https://github.com/apache/airflow/discussions/41426

And there was also already opened an issue with the same problem but not solved, see [#39413
](https://github.com/apache/airflow/issues/39413)

### What you think should happen instead?

Decorater should not fire a warning. Maybe it should be checked, if the decorater was already called and therefor did already checked it.



### How to reproduce

```python
from airflow import DAG
from airflow.providers.google.cloud.sensors.bigquery import BigQueryTableExistenceSensor
from airflow.utils.dates import days_ago
from airflow.models.baseoperator import BaseOperator


class LoggingBaseOperator(BaseOperator):
    def execute(self, context):
        self.log.info(f""Executing {self.__class__.__name__}"")
        return super().execute(context)


class LoggingBigQueryTableExistenceSensor(
    BigQueryTableExistenceSensor, LoggingBaseOperator
):
    def poke(self, context):
        self.log.info(f""Poking {self.__class__.__name__}"")
        return True  


dag = DAG(
    ""test_bigquery_sensor_double_execution"",
    default_args={
        ""start_date"": days_ago(1),
    },
    description=""A simple DAG to test BigQueryTableExistenceSensor execution"",
    schedule_interval=None,
)

sensor_task = LoggingBigQueryTableExistenceSensor(
    task_id=""test_sensor"",
    project_id=""your-project-id"",
    dataset_id=""your-dataset-id"",
    table_id=""your-table-id"",
    poke_interval=60,
    dag=dag,
)
```
[log_bg.txt](https://github.com/user-attachments/files/16611919/log_bg.txt)


### Operating System

Ubuntu 22.04.4 LTS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

Docker image: apache/airflow:2.9.3-python3.11

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",SKalide,2024-08-14 11:05:39+00:00,[],2025-02-04 09:51:39+00:00,2024-10-19 04:24:14+00:00,https://github.com/apache/airflow/issues/41470,"[('kind:bug', 'This is a clearly a bug'), ('provider:google', 'Google (including GCP) related issues'), ('area:providers', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2288460254, 'issue_id': 2465536411, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 14, 11, 5, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2302875072, 'issue_id': 2465536411, 'author': 'SKalide', 'body': 'I\'ve looked into this issue more deeply and I believe I can provide a solution. I could use some guidance on whether I should submit a PR for this.\r\n\r\nThe problem, in my opinion, is that after `_execute_callable` is called, it enters the `ExecutorSafeguard` wrapper. There, it checks if it\'s called inside a `TaskInstance` (hence the `_execute_callable`) and then returns the execution function itself. Because the `execute` method of BigQueryTableExistenceSensor is also decorated, it automatically re-enters the `ExecutorSafeguard` wrapper, but this time it appears as if it was called outside a Task Instance, causing the check to fail. With `allow_nested_operators`, it only fires a warning instead of failing.\r\n\r\nMy proposed solution is to check if the `ExecutorSafeguard` has already been called in the current execution context. If so, we can skip the check for nested calls. This can be achieved by setting a flag using thread-local storage to ensure thread safety.\r\n\r\n```python\r\nclass ExecutorSafeguard:\r\n    """"""\r\n    The ExecutorSafeguard decorator.\r\n\r\n    Checks if the execute method of an operator isn\'t manually called outside\r\n    the TaskInstance as we want to avoid bad mixing between decorated and\r\n    classic operators.\r\n    """"""\r\n\r\n    test_mode = conf.getboolean(""core"", ""unit_test_mode"")\r\n    _local = local()\r\n\r\n    @classmethod\r\n    def decorator(cls, func):\r\n        @wraps(func)\r\n        def wrapper(self, *args, **kwargs):\r\n            if (\r\n                getattr(ExecutorSafeguard._local, ""in_executor_safeguard"", False)\r\n                and self.allow_nested_operators\r\n            ):\r\n                # If already in ExecutorSafeguard, call execution function - recursive call\r\n                return func(self, *args, **kwargs)\r\n            ExecutorSafeguard._local.in_executor_safeguard = True\r\n\r\n            try:\r\n                from airflow.decorators.base import DecoratedOperator\r\n\r\n                sentinel = kwargs.pop(f""{self.__class__.__name__}__sentinel"", None)\r\n\r\n                if (\r\n                    not cls.test_mode\r\n                    and sentinel != _sentinel\r\n                    and not isinstance(self, DecoratedOperator)\r\n                ):\r\n                    message = (\r\n                        f""{self.__class__.__name__}.{func.__name__} cannot be called outside TaskInstance!""\r\n                    )\r\n                    raise AirflowException(message)\r\n                return func(self, *args, **kwargs)\r\n            finally:\r\n                ExecutorSafeguard._local.in_executor_safeguard = False\r\n\r\n        return wrapper\r\n```\r\n\r\nThis solution would suppress the warning for legitimate nested calls while still protecting against unintended external calls. The use of `local()` ensures that this check is thread-safe.\r\n\r\nI\'m willing to create a Pull Request with this solution if the maintainers think this approach is appropriate. What do you think about this solution? Are there any aspects I should consider or modify before proceeding with a PR?', 'created_at': datetime.datetime(2024, 8, 21, 19, 37, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2400722201, 'issue_id': 2465536411, 'author': 'zachliu', 'body': '@SKalide i think you can simplify your example:\r\n\r\n```python\r\nfrom airflow import DAG\r\nfrom airflow.utils.dates import days_ago\r\nfrom airflow.operators.bash import BashOperator\r\n\r\n\r\nclass MyBashOperator(BashOperator):\r\n    def execute(self, context):\r\n        self.log.info(f""Executing {self.__class__.__name__}"")\r\n        return super().execute(context)\r\n\r\n\r\ndag = DAG(\r\n    ""test_bash_execution"",\r\n    default_args={\r\n        ""start_date"": days_ago(1),\r\n    },\r\n    description=""A simple DAG to test MyBashOperator execution"",\r\n    schedule_interval=None,\r\n)\r\n\r\ntask = MyBashOperator(\r\n    task_id=""my_bash_operator"",\r\n    bash_command=""ls -lah"",\r\n    dag=dag,\r\n)\r\n```\r\n\r\nthis produces the same warning message\r\nby using this simpler example, i believe it\'s more convincing that this issue is ""serious"" :grin:', 'created_at': datetime.datetime(2024, 10, 8, 20, 4, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401447224, 'issue_id': 2465536411, 'author': 'dabla', 'body': '> @SKalide i think you can simplify your example:\r\n> \r\n> ```python\r\n> from airflow import DAG\r\n> from airflow.utils.dates import days_ago\r\n> from airflow.operators.bash import BashOperator\r\n> \r\n> \r\n> class MyBashOperator(BashOperator):\r\n>     def execute(self, context):\r\n>         self.log.info(f""Executing {self.__class__.__name__}"")\r\n>         return super().execute(context)\r\n> \r\n> \r\n> dag = DAG(\r\n>     ""test_bash_execution"",\r\n>     default_args={\r\n>         ""start_date"": days_ago(1),\r\n>     },\r\n>     description=""A simple DAG to test MyBashOperator execution"",\r\n>     schedule_interval=None,\r\n> )\r\n> \r\n> task = MyBashOperator(\r\n>     task_id=""my_bash_operator"",\r\n>     bash_command=""ls -lah"",\r\n>     dag=dag,\r\n> )\r\n> ```\r\n> \r\n> this produces the same warning message by using this simpler example, i believe it\'s more convincing that this issue is ""serious"" 😁\r\n\r\nI think I understand the issue of the warning and I think some refactoring will be needed to avoid the warning in the case you extend an existing operator, as that should still be allowed.  Will have to create a test case for it and then check how we can prevent the warning in that case.  The warning is there to prevent people from calling other operators from python operators, but in you example its not the case as it\'s inheritance.', 'created_at': datetime.datetime(2024, 10, 9, 6, 40, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401453034, 'issue_id': 2465536411, 'author': 'dabla', 'body': '> I\'ve looked into this issue more deeply and I believe I can provide a solution. I could use some guidance on whether I should submit a PR for this.\r\n> \r\n> The problem, in my opinion, is that after `_execute_callable` is called, it enters the `ExecutorSafeguard` wrapper. There, it checks if it\'s called inside a `TaskInstance` (hence the `_execute_callable`) and then returns the execution function itself. Because the `execute` method of BigQueryTableExistenceSensor is also decorated, it automatically re-enters the `ExecutorSafeguard` wrapper, but this time it appears as if it was called outside a Task Instance, causing the check to fail. With `allow_nested_operators`, it only fires a warning instead of failing.\r\n> \r\n> My proposed solution is to check if the `ExecutorSafeguard` has already been called in the current execution context. If so, we can skip the check for nested calls. This can be achieved by setting a flag using thread-local storage to ensure thread safety.\r\n> \r\n> ```python\r\n> class ExecutorSafeguard:\r\n>     """"""\r\n>     The ExecutorSafeguard decorator.\r\n> \r\n>     Checks if the execute method of an operator isn\'t manually called outside\r\n>     the TaskInstance as we want to avoid bad mixing between decorated and\r\n>     classic operators.\r\n>     """"""\r\n> \r\n>     test_mode = conf.getboolean(""core"", ""unit_test_mode"")\r\n>     _local = local()\r\n> \r\n>     @classmethod\r\n>     def decorator(cls, func):\r\n>         @wraps(func)\r\n>         def wrapper(self, *args, **kwargs):\r\n>             if (\r\n>                 getattr(ExecutorSafeguard._local, ""in_executor_safeguard"", False)\r\n>                 and self.allow_nested_operators\r\n>             ):\r\n>                 # If already in ExecutorSafeguard, call execution function - recursive call\r\n>                 return func(self, *args, **kwargs)\r\n>             ExecutorSafeguard._local.in_executor_safeguard = True\r\n> \r\n>             try:\r\n>                 from airflow.decorators.base import DecoratedOperator\r\n> \r\n>                 sentinel = kwargs.pop(f""{self.__class__.__name__}__sentinel"", None)\r\n> \r\n>                 if (\r\n>                     not cls.test_mode\r\n>                     and sentinel != _sentinel\r\n>                     and not isinstance(self, DecoratedOperator)\r\n>                 ):\r\n>                     message = (\r\n>                         f""{self.__class__.__name__}.{func.__name__} cannot be called outside TaskInstance!""\r\n>                     )\r\n>                     raise AirflowException(message)\r\n>                 return func(self, *args, **kwargs)\r\n>             finally:\r\n>                 ExecutorSafeguard._local.in_executor_safeguard = False\r\n> \r\n>         return wrapper\r\n> ```\r\n> \r\n> This solution would suppress the warning for legitimate nested calls while still protecting against unintended external calls. The use of `local()` ensures that this check is thread-safe.\r\n> \r\n> I\'m willing to create a Pull Request with this solution if the maintainers think this approach is appropriate. What do you think about this solution? Are there any aspects I should consider or modify before proceeding with a PR?\r\n\r\nI like the approach, you can open a PR and add you example as a test case, that way we are sure this case is not only fixed but also tested against regression.', 'created_at': datetime.datetime(2024, 10, 9, 6, 44, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401543031, 'issue_id': 2465536411, 'author': 'dabla', 'body': '> ```python\r\n> class LoggingBaseOperator(BaseOperator):\r\n>     def execute(self, context):\r\n>         self.log.info(f""Executing {self.__class__.__name__}"")\r\n>         return super().execute(context)\r\n> ```\r\n\r\nI\'ve been able to reproduce the issue in a test case, will open a PR for it which will fix it, will try the solution proposed by @SKalide', 'created_at': datetime.datetime(2024, 10, 9, 7, 26, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401682183, 'issue_id': 2465536411, 'author': 'dabla', 'body': '> I\'ve looked into this issue more deeply and I believe I can provide a solution. I could use some guidance on whether I should submit a PR for this.\r\n> \r\n> The problem, in my opinion, is that after `_execute_callable` is called, it enters the `ExecutorSafeguard` wrapper. There, it checks if it\'s called inside a `TaskInstance` (hence the `_execute_callable`) and then returns the execution function itself. Because the `execute` method of BigQueryTableExistenceSensor is also decorated, it automatically re-enters the `ExecutorSafeguard` wrapper, but this time it appears as if it was called outside a Task Instance, causing the check to fail. With `allow_nested_operators`, it only fires a warning instead of failing.\r\n> \r\n> My proposed solution is to check if the `ExecutorSafeguard` has already been called in the current execution context. If so, we can skip the check for nested calls. This can be achieved by setting a flag using thread-local storage to ensure thread safety.\r\n> \r\n> ```python\r\n> class ExecutorSafeguard:\r\n>     """"""\r\n>     The ExecutorSafeguard decorator.\r\n> \r\n>     Checks if the execute method of an operator isn\'t manually called outside\r\n>     the TaskInstance as we want to avoid bad mixing between decorated and\r\n>     classic operators.\r\n>     """"""\r\n> \r\n>     test_mode = conf.getboolean(""core"", ""unit_test_mode"")\r\n>     _local = local()\r\n> \r\n>     @classmethod\r\n>     def decorator(cls, func):\r\n>         @wraps(func)\r\n>         def wrapper(self, *args, **kwargs):\r\n>             if (\r\n>                 getattr(ExecutorSafeguard._local, ""in_executor_safeguard"", False)\r\n>                 and self.allow_nested_operators\r\n>             ):\r\n>                 # If already in ExecutorSafeguard, call execution function - recursive call\r\n>                 return func(self, *args, **kwargs)\r\n>             ExecutorSafeguard._local.in_executor_safeguard = True\r\n> \r\n>             try:\r\n>                 from airflow.decorators.base import DecoratedOperator\r\n> \r\n>                 sentinel = kwargs.pop(f""{self.__class__.__name__}__sentinel"", None)\r\n> \r\n>                 if (\r\n>                     not cls.test_mode\r\n>                     and sentinel != _sentinel\r\n>                     and not isinstance(self, DecoratedOperator)\r\n>                 ):\r\n>                     message = (\r\n>                         f""{self.__class__.__name__}.{func.__name__} cannot be called outside TaskInstance!""\r\n>                     )\r\n>                     raise AirflowException(message)\r\n>                 return func(self, *args, **kwargs)\r\n>             finally:\r\n>                 ExecutorSafeguard._local.in_executor_safeguard = False\r\n> \r\n>         return wrapper\r\n> ```\r\n> \r\n> This solution would suppress the warning for legitimate nested calls while still protecting against unintended external calls. The use of `local()` ensures that this check is thread-safe.\r\n> \r\n> I\'m willing to create a Pull Request with this solution if the maintainers think this approach is appropriate. What do you think about this solution? Are there any aspects I should consider or modify before proceeding with a PR?\r\n\r\nThe above proposed solution doesn\'t work for all test cases, especially in case of mixed usage with task decorated function calling execute methods on native operator. I\'ve inspired the final working solution from it and now it should also support the case you presented.', 'created_at': datetime.datetime(2024, 10, 9, 8, 30, 12, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401696393, 'issue_id': 2465536411, 'author': 'dabla', 'body': ""I've created a [PR](https://github.com/apache/airflow/pull/42849) which should fix this issue."", 'created_at': datetime.datetime(2024, 10, 9, 8, 36, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401785465, 'issue_id': 2465536411, 'author': 'dabla', 'body': ""The build of the [PR](https://github.com/apache/airflow/pull/42849) passed so once it's merged I think we can close this issue."", 'created_at': datetime.datetime(2024, 10, 9, 9, 16, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2448808728, 'issue_id': 2465536411, 'author': 'sam-gen-cop', 'body': 'Hi @dabla, I\'m using AWS MWAA 2.10.1 and I still get the same warning. Just wondering whether the issue is fixed in 2.10.1 or only in 2.9.3\r\n\r\nScript,\r\n```\r\nfrom airflow import DAG\r\nfrom airflow.utils.dates import days_ago\r\nfrom airflow.operators.bash import BashOperator\r\nfrom airflow.utils.log.logging_mixin import LoggingMixin\r\n\r\nclass MyBashOperator(BashOperator, LoggingMixin):\r\n    def execute(self, context):\r\n        self.log.info(""Executing MyBashOperator directly in the DAG definition"")\r\n        return super().execute(context)\r\n\r\ndag = DAG(\r\n    ""test_bash_execution"",\r\n    default_args={\r\n        ""start_date"": days_ago(1),\r\n    },\r\n    schedule_interval=None,\r\n)\r\n\r\n# Instantiate the operator\r\nmy_bash_task = MyBashOperator(\r\n    task_id=""my_bash_operator"",\r\n    bash_command=""echo \'Testing direct execution\'"",\r\n    dag=dag,\r\n)\r\n```\r\n\r\nLogs,\r\n<img width=""816"" alt=""image"" src=""https://github.com/user-attachments/assets/a4fc4980-6751-4a85-97a1-e64fd0a4086c"">', 'created_at': datetime.datetime(2024, 10, 31, 1, 17, 49, tzinfo=datetime.timezone.utc)}, {'comment_id': 2450121982, 'issue_id': 2465536411, 'author': 'dabla', 'body': '> Hi @dabla, I\'m using AWS MWAA 2.10.1 and I still get the same warning. Just wondering whether the issue is fixed in 2.10.1 or only in 2.9.3\r\n> \r\n> Script,\r\n> \r\n> ```\r\n> from airflow import DAG\r\n> from airflow.utils.dates import days_ago\r\n> from airflow.operators.bash import BashOperator\r\n> from airflow.utils.log.logging_mixin import LoggingMixin\r\n> \r\n> class MyBashOperator(BashOperator, LoggingMixin):\r\n>     def execute(self, context):\r\n>         self.log.info(""Executing MyBashOperator directly in the DAG definition"")\r\n>         return super().execute(context)\r\n> \r\n> dag = DAG(\r\n>     ""test_bash_execution"",\r\n>     default_args={\r\n>         ""start_date"": days_ago(1),\r\n>     },\r\n>     schedule_interval=None,\r\n> )\r\n> \r\n> # Instantiate the operator\r\n> my_bash_task = MyBashOperator(\r\n>     task_id=""my_bash_operator"",\r\n>     bash_command=""echo \'Testing direct execution\'"",\r\n>     dag=dag,\r\n> )\r\n> ```\r\n> \r\n> Logs, <img alt=""image"" width=""816"" src=""https://private-user-images.githubusercontent.com/182846498/381787558-a4fc4980-6751-4a85-97a1-e64fd0a4086c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzAzODczMzgsIm5iZiI6MTczMDM4NzAzOCwicGF0aCI6Ii8xODI4NDY0OTgvMzgxNzg3NTU4LWE0ZmM0OTgwLTY3NTEtNGE4NS05N2ExLWU2NGZkMGE0MDg2Yy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMDMxJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTAzMVQxNTAzNThaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT03ZjQwNjJhNDRiNDIwMTBjMDVkZjBlNWI4Zjc1NThlYTMwNmYwZjRkNjk1MWRmOTQ1YTNlODI3NjhjNDQ3Nzg4JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9._acIyB7ewJl_8Qjfgw8-_mMk0LDX5J5ISGisI7sFsTw"">\r\n\r\nThe fix is not in Airflow 2.10.1 nor 2.10.2, I just checked the source code. I suppose it will be in 2.10.3 @potiuk @uranusjr ?\r\n\r\nHmm seem I can\'t find it in [rc1](https://github.com/apache/airflow/issues/43441) of 2.10.3?', 'created_at': datetime.datetime(2024, 10, 31, 15, 7, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2459272506, 'issue_id': 2465536411, 'author': 'dabla', 'body': '> Hi @dabla, I\'m using AWS MWAA 2.10.1 and I still get the same warning. Just wondering whether the issue is fixed in 2.10.1 or only in 2.9.3\r\n> \r\n> Script,\r\n> \r\n> ```\r\n> from airflow import DAG\r\n> from airflow.utils.dates import days_ago\r\n> from airflow.operators.bash import BashOperator\r\n> from airflow.utils.log.logging_mixin import LoggingMixin\r\n> \r\n> class MyBashOperator(BashOperator, LoggingMixin):\r\n>     def execute(self, context):\r\n>         self.log.info(""Executing MyBashOperator directly in the DAG definition"")\r\n>         return super().execute(context)\r\n> \r\n> dag = DAG(\r\n>     ""test_bash_execution"",\r\n>     default_args={\r\n>         ""start_date"": days_ago(1),\r\n>     },\r\n>     schedule_interval=None,\r\n> )\r\n> \r\n> # Instantiate the operator\r\n> my_bash_task = MyBashOperator(\r\n>     task_id=""my_bash_operator"",\r\n>     bash_command=""echo \'Testing direct execution\'"",\r\n>     dag=dag,\r\n> )\r\n> ```\r\n> \r\n> Logs, <img alt=""image"" width=""816"" src=""https://private-user-images.githubusercontent.com/182846498/381787558-a4fc4980-6751-4a85-97a1-e64fd0a4086c.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MzA4ODkxMTAsIm5iZiI6MTczMDg4ODgxMCwicGF0aCI6Ii8xODI4NDY0OTgvMzgxNzg3NTU4LWE0ZmM0OTgwLTY3NTEtNGE4NS05N2ExLWU2NGZkMGE0MDg2Yy5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjQxMTA2JTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI0MTEwNlQxMDI2NTBaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT00YzQ3OGE1MzJlZTkyYjBmNWY5N2M0MTk2MWJmZGY5MjNmMzk1NTdkNGE5MGU1MDQxZmM5YjM4OThiYmViYWNmJlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.69u-HvJ6oqDJtfQ67BrCIIoFTuOfG6QfceXufwpB-6o"">\r\n\r\n@sam-gen-cop The fix is now available in Airflow 2.10.3', 'created_at': datetime.datetime(2024, 11, 6, 10, 27, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2594146161, 'issue_id': 2465536411, 'author': 'npatel44', 'body': ""This Warning still happens for custom Operator written on top of Provider's operators. (Multiple levels of nesting) . Logic of this PR should work for any levels of nesting so not sure what's the issue but I still see this with airflow 2.10.2 and 2.10.4"", 'created_at': datetime.datetime(2025, 1, 15, 23, 31, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2596751923, 'issue_id': 2465536411, 'author': 'potiuk', 'body': '> This Warning still happens for custom Operator written on top of Provider\'s operators. (Multiple levels of nesting) . Logic of this PR should work for any levels of nesting so not sure what\'s the issue but I still see this with airflow 2.10.2 and 2.10.4\n\nCan you open a new issue for that with the detail of your case please ? Or even better - contributing a fix - following that one here should be relatively simple so maybe you or your company would like to contribute it? Just to put it in perspective - this is an open-source project that a lot of people contribute to, so when you create such an issue, you will have to wait until somoene picks it up and fix it. But the most certain way is to implement and contribute a fix yourself. The second best is to find someone who can do it - maybe even paying them. The third best is to open a detailed issue that makes it easy to reproduce by anyone, we can then mark it as ""good first issue"" for example and hopefully sooner or later one of the contributors will fix it.\n\nFollowing one of those routes is the best way to help in solving your issue @npatel44', 'created_at': datetime.datetime(2025, 1, 16, 19, 46, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2596769590, 'issue_id': 2465536411, 'author': 'dabla', 'body': 'I’m wondering if there wasn’t even an additional fix for this issue but only made it in main/Airflow 3 codebase but could be mistalen ofc.', 'created_at': datetime.datetime(2025, 1, 16, 19, 51, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2633391896, 'issue_id': 2465536411, 'author': 'adam-aczel-towa', 'body': ""I'm getting the same warning for a ShortCircuitOperator in Version 2.10.4\n`{baseoperator.py:421} WARNING - ShortCircuitOperator.execute cannot be called outside TaskInstance!`\n\nThe DAG configuration looks like this:\n```python\n  # Create Airflow Operators for each task dynamically\n  task_objects = {}\n  for task_id, python_callable in tasks:\n      if task_id.startswith('get_'):\n          task_objects[task_id] = ShortCircuitOperator(\n              task_id=task_id,\n              python_callable=python_callable,\n              op_kwargs={\n                  'credentials': credentials\n              },\n              dag=dag,\n          )\n      else:\n          task_objects[task_id] = PythonOperator(\n              task_id=task_id,\n              python_callable=python_callable,\n              op_kwargs={\n                  'credentials': credentials\n              },\n              dag=dag,\n          )\n```"", 'created_at': datetime.datetime(2025, 2, 4, 9, 50, 22, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-14 11:05:42 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

SKalide (Issue Creator) on (2024-08-21 19:37:18 UTC): I've looked into this issue more deeply and I believe I can provide a solution. I could use some guidance on whether I should submit a PR for this.

The problem, in my opinion, is that after `_execute_callable` is called, it enters the `ExecutorSafeguard` wrapper. There, it checks if it's called inside a `TaskInstance` (hence the `_execute_callable`) and then returns the execution function itself. Because the `execute` method of BigQueryTableExistenceSensor is also decorated, it automatically re-enters the `ExecutorSafeguard` wrapper, but this time it appears as if it was called outside a Task Instance, causing the check to fail. With `allow_nested_operators`, it only fires a warning instead of failing.

My proposed solution is to check if the `ExecutorSafeguard` has already been called in the current execution context. If so, we can skip the check for nested calls. This can be achieved by setting a flag using thread-local storage to ensure thread safety.

```python
class ExecutorSafeguard:
    """"""
    The ExecutorSafeguard decorator.

    Checks if the execute method of an operator isn't manually called outside
    the TaskInstance as we want to avoid bad mixing between decorated and
    classic operators.
    """"""

    test_mode = conf.getboolean(""core"", ""unit_test_mode"")
    _local = local()

    @classmethod
    def decorator(cls, func):
        @wraps(func)
        def wrapper(self, *args, **kwargs):
            if (
                getattr(ExecutorSafeguard._local, ""in_executor_safeguard"", False)
                and self.allow_nested_operators
            ):
                # If already in ExecutorSafeguard, call execution function - recursive call
                return func(self, *args, **kwargs)
            ExecutorSafeguard._local.in_executor_safeguard = True

            try:
                from airflow.decorators.base import DecoratedOperator

                sentinel = kwargs.pop(f""{self.__class__.__name__}__sentinel"", None)

                if (
                    not cls.test_mode
                    and sentinel != _sentinel
                    and not isinstance(self, DecoratedOperator)
                ):
                    message = (
                        f""{self.__class__.__name__}.{func.__name__} cannot be called outside TaskInstance!""
                    )
                    raise AirflowException(message)
                return func(self, *args, **kwargs)
            finally:
                ExecutorSafeguard._local.in_executor_safeguard = False

        return wrapper
```

This solution would suppress the warning for legitimate nested calls while still protecting against unintended external calls. The use of `local()` ensures that this check is thread-safe.

I'm willing to create a Pull Request with this solution if the maintainers think this approach is appropriate. What do you think about this solution? Are there any aspects I should consider or modify before proceeding with a PR?

zachliu on (2024-10-08 20:04:50 UTC): @SKalide i think you can simplify your example:

```python
from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.operators.bash import BashOperator


class MyBashOperator(BashOperator):
    def execute(self, context):
        self.log.info(f""Executing {self.__class__.__name__}"")
        return super().execute(context)


dag = DAG(
    ""test_bash_execution"",
    default_args={
        ""start_date"": days_ago(1),
    },
    description=""A simple DAG to test MyBashOperator execution"",
    schedule_interval=None,
)

task = MyBashOperator(
    task_id=""my_bash_operator"",
    bash_command=""ls -lah"",
    dag=dag,
)
```

this produces the same warning message
by using this simpler example, i believe it's more convincing that this issue is ""serious"" :grin:

dabla on (2024-10-09 06:40:31 UTC): I think I understand the issue of the warning and I think some refactoring will be needed to avoid the warning in the case you extend an existing operator, as that should still be allowed.  Will have to create a test case for it and then check how we can prevent the warning in that case.  The warning is there to prevent people from calling other operators from python operators, but in you example its not the case as it's inheritance.

dabla on (2024-10-09 06:44:06 UTC): I like the approach, you can open a PR and add you example as a test case, that way we are sure this case is not only fixed but also tested against regression.

dabla on (2024-10-09 07:26:37 UTC): I've been able to reproduce the issue in a test case, will open a PR for it which will fix it, will try the solution proposed by @SKalide

dabla on (2024-10-09 08:30:12 UTC): The above proposed solution doesn't work for all test cases, especially in case of mixed usage with task decorated function calling execute methods on native operator. I've inspired the final working solution from it and now it should also support the case you presented.

dabla on (2024-10-09 08:36:48 UTC): I've created a [PR](https://github.com/apache/airflow/pull/42849) which should fix this issue.

dabla on (2024-10-09 09:16:24 UTC): The build of the [PR](https://github.com/apache/airflow/pull/42849) passed so once it's merged I think we can close this issue.

sam-gen-cop on (2024-10-31 01:17:49 UTC): Hi @dabla, I'm using AWS MWAA 2.10.1 and I still get the same warning. Just wondering whether the issue is fixed in 2.10.1 or only in 2.9.3

Script,
```
from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.operators.bash import BashOperator
from airflow.utils.log.logging_mixin import LoggingMixin

class MyBashOperator(BashOperator, LoggingMixin):
    def execute(self, context):
        self.log.info(""Executing MyBashOperator directly in the DAG definition"")
        return super().execute(context)

dag = DAG(
    ""test_bash_execution"",
    default_args={
        ""start_date"": days_ago(1),
    },
    schedule_interval=None,
)

# Instantiate the operator
my_bash_task = MyBashOperator(
    task_id=""my_bash_operator"",
    bash_command=""echo 'Testing direct execution'"",
    dag=dag,
)
```

Logs,
<img width=""816"" alt=""image"" src=""https://github.com/user-attachments/assets/a4fc4980-6751-4a85-97a1-e64fd0a4086c"">

dabla on (2024-10-31 15:07:21 UTC): The fix is not in Airflow 2.10.1 nor 2.10.2, I just checked the source code. I suppose it will be in 2.10.3 @potiuk @uranusjr ?

Hmm seem I can't find it in [rc1](https://github.com/apache/airflow/issues/43441) of 2.10.3?

dabla on (2024-11-06 10:27:53 UTC): @sam-gen-cop The fix is now available in Airflow 2.10.3

npatel44 on (2025-01-15 23:31:06 UTC): This Warning still happens for custom Operator written on top of Provider's operators. (Multiple levels of nesting) . Logic of this PR should work for any levels of nesting so not sure what's the issue but I still see this with airflow 2.10.2 and 2.10.4

potiuk on (2025-01-16 19:46:41 UTC): Can you open a new issue for that with the detail of your case please ? Or even better - contributing a fix - following that one here should be relatively simple so maybe you or your company would like to contribute it? Just to put it in perspective - this is an open-source project that a lot of people contribute to, so when you create such an issue, you will have to wait until somoene picks it up and fix it. But the most certain way is to implement and contribute a fix yourself. The second best is to find someone who can do it - maybe even paying them. The third best is to open a detailed issue that makes it easy to reproduce by anyone, we can then mark it as ""good first issue"" for example and hopefully sooner or later one of the contributors will fix it.

Following one of those routes is the best way to help in solving your issue @npatel44

dabla on (2025-01-16 19:51:27 UTC): I’m wondering if there wasn’t even an additional fix for this issue but only made it in main/Airflow 3 codebase but could be mistalen ofc.

adam-aczel-towa on (2025-02-04 09:50:22 UTC): I'm getting the same warning for a ShortCircuitOperator in Version 2.10.4
`{baseoperator.py:421} WARNING - ShortCircuitOperator.execute cannot be called outside TaskInstance!`

The DAG configuration looks like this:
```python
  # Create Airflow Operators for each task dynamically
  task_objects = {}
  for task_id, python_callable in tasks:
      if task_id.startswith('get_'):
          task_objects[task_id] = ShortCircuitOperator(
              task_id=task_id,
              python_callable=python_callable,
              op_kwargs={
                  'credentials': credentials
              },
              dag=dag,
          )
      else:
          task_objects[task_id] = PythonOperator(
              task_id=task_id,
              python_callable=python_callable,
              op_kwargs={
                  'credentials': credentials
              },
              dag=dag,
          )
```

"
2465490024,issue,open,,Difficulty with Clearing and Restarting Tasks in Airflow UI for bakcfill DAGs,"### Apache Airflow version

2.10.0rc1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I executed the following command to backfill Airflow DAGs:

`airflow dags backfill -s ""$current_date"" -e ""$next_day"" ""$DAG_ID"" --reset-dagruns`
However, I encountered several issues:

I cannot clear tasks from the Airflow UI.
If the backfill process is interrupted or killed, I cannot restart or resume the DAG from the UI.

### What you think should happen instead?

_No response_

### How to reproduce

Run the backfill command:

`airflow dags backfill -s ""$current_date"" -e ""$next_day"" ""$DAG_ID"" --reset-dagruns`
Check the Airflow UI:

Attempt to clear tasks and verify if the option is functional.
Try to restart or resume the DAG if the process is killed.

### Operating System

cat /etc/os-release

### Versions of Apache Airflow Providers

Version: [v2.8.0](https://pypi.python.org/pypi/apache-airflow/2.8.0)

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",ImanLakjiri,2024-08-14 10:41:13+00:00,['ImanLakjiri'],2024-08-23 09:36:40+00:00,,https://github.com/apache/airflow/issues/41468,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2288417593, 'issue_id': 2465490024, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 14, 10, 41, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2288799328, 'issue_id': 2465490024, 'author': 'vatsrahul1001', 'body': ""Verify that this issue persists, but note that it wasn't introduced with 2.10.rc1; it also existed in 2.9.3."", 'created_at': datetime.datetime(2024, 8, 14, 13, 43, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2306699908, 'issue_id': 2465490024, 'author': 'GabrielEisenbergOlympus', 'body': 'I have also experienced this issue. After tasks were backfilled and then cleared over the same time period, a clear does not initiate queuing of the tasks. Note that this existed in both 2.8.1 and now in 2.9.2 in MWAA.\r\n\r\n![image](https://github.com/user-attachments/assets/07870f90-5d98-4ab0-8080-cd02b1aa5dd1)', 'created_at': datetime.datetime(2024, 8, 23, 9, 36, 39, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-14 10:41:15 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

vatsrahul1001 on (2024-08-14 13:43:41 UTC): Verify that this issue persists, but note that it wasn't introduced with 2.10.rc1; it also existed in 2.9.3.

GabrielEisenbergOlympus on (2024-08-23 09:36:39 UTC): I have also experienced this issue. After tasks were backfilled and then cleared over the same time period, a clear does not initiate queuing of the tasks. Note that this existed in both 2.8.1 and now in 2.9.2 in MWAA.

![image](https://github.com/user-attachments/assets/07870f90-5d98-4ab0-8080-cd02b1aa5dd1)

"
2465270467,issue,closed,completed,Selecting 'Running' Button Updates 'All' Button Count Incorrectly,"### Apache Airflow version

2.10.0rc1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Clicking the 'Running' button incorrectly updates the count displayed by the 'All' button to match the 'Running' count. This behavior is not present in version 11.8.0. Also, I need to click on the Running button again to get the list of all DAGS

### What you think should happen instead?

Click on Running button should not change All button count

### How to reproduce

Create a deployment with rc and click on running button if there is no DAG running then ALL count also gets update to 0. If few dags are running then same count updates to ALL

### Operating System

Unix

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",vatsrahul1001,2024-08-14 08:54:07+00:00,[],2024-08-14 12:45:47+00:00,2024-08-14 12:45:47+00:00,https://github.com/apache/airflow/issues/41465,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2288323429, 'issue_id': 2465270467, 'author': 'vatsrahul1001', 'body': 'cc: @bbovenzi', 'created_at': datetime.datetime(2024, 8, 14, 9, 49, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2288493705, 'issue_id': 2465270467, 'author': 'vatsrahul1001', 'body': ""Closing this as it's part of a new implementation in [PR](https://github.com/apache/airflow/issues/41465). I strongly suggest documenting this, as it may cause confusion for users who are not aware of this feature"", 'created_at': datetime.datetime(2024, 8, 14, 11, 26, 8, tzinfo=datetime.timezone.utc)}]","vatsrahul1001 (Issue Creator) on (2024-08-14 09:49:47 UTC): cc: @bbovenzi

vatsrahul1001 (Issue Creator) on (2024-08-14 11:26:08 UTC): Closing this as it's part of a new implementation in [PR](https://github.com/apache/airflow/issues/41465). I strongly suggest documenting this, as it may cause confusion for users who are not aware of this feature

"
2465148140,issue,closed,completed,Task Try History in UI Wrong Color in Status Badges before Task Run,"### Apache Airflow version

2.10.0rc1

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Related: #40304

I assume this is relating to #39336 as well - we have changed try-number handling in parallel to have try versions displayed on UI. At least there is a small glitch in 2.10.0rc1:

When a task is re-tried and BEFORE it gets running the task state colors in the tabs to select the try to display are shifted by one position. For example you retry a task by clearing then while it is in queue, the attempt 2 state is displayed in attempt 1 in the tab:

![image](https://github.com/user-attachments/assets/96fba1c0-746e-46d3-87f7-e548e1ce8c14)

Also if you flip back to the first attempt you see in the details it is success but the tab header has the queued color:

![image](https://github.com/user-attachments/assets/5f697678-fa3b-45d0-ab97-3d1f1d82013b)

Once a task then gets into running state, the colors of the tags are ""correct"" again:

![image](https://github.com/user-attachments/assets/8af207f8-8834-4e74-ad0e-bc702b69dd3f)


FYI @bbovenzi 


### What you think should happen instead?

Tab color should match the state.

### How to reproduce

Run a task, wait for completion/fail. Clear task while the scheduler/worker is busy or stopped - then you see the new attempt in UI but as long as not starting the status color is shifted by one position.

### Operating System

unrelevant.

### Versions of Apache Airflow Providers

unrelevant.

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

No specific details. Postgres.

### Anything else?

Browser: Firefox.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jscheffl,2024-08-14 07:53:39+00:00,['bbovenzi'],2024-08-15 03:46:02+00:00,2024-08-15 03:46:02+00:00,https://github.com/apache/airflow/issues/41462,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2288846042, 'issue_id': 2465148140, 'author': 'bbovenzi', 'body': ""Yes, I noticed this issue I've already been working on a fix on a local branch"", 'created_at': datetime.datetime(2024, 8, 14, 14, 1, 22, tzinfo=datetime.timezone.utc)}]","bbovenzi (Assginee) on (2024-08-14 14:01:22 UTC): Yes, I noticed this issue I've already been working on a fix on a local branch

"
2465115587,issue,closed,completed,`DbApiHook` import error in `common.sql`,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

```shell
Traceback (most recent call last):
  File ""/opt/airflow/airflow/models/taskinstance.py"", line 766, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File ""/opt/airflow/airflow/models/taskinstance.py"", line 732, in _execute_callable
    return ExecutionCallableRunner(
  File ""/opt/airflow/airflow/utils/operator_helpers.py"", line 252, in run
    return self.func(*args, **kwargs)
  File ""/opt/airflow/airflow/models/baseoperator.py"", line 406, in wrapper
    return func(self, *args, **kwargs)
  File ""/opt/airflow/airflow/providers/common/sql/operators/sql.py"", line 1182, in execute
    record = self.get_db_hook().get_first(self.sql, self.parameters)
  File ""/opt/airflow/airflow/providers/common/sql/operators/sql.py"", line 194, in get_db_hook
    return self._hook
  File ""/usr/local/lib/python3.8/functools.py"", line 967, in __get__
    val = self.func(instance)
  File ""/opt/airflow/airflow/providers/common/sql/operators/sql.py"", line 156, in _hook
    from airflow.hooks.dbapi_hook import DbApiHook as _DbApiHook
ModuleNotFoundError: No module named 'airflow.hooks.dbapi_hook'
```

### What you think should happen instead?

Now that we've removed the deprecated imports from #41368 , we need to modify the following code

https://github.com/apache/airflow/blob/fcc9f3dc0eb0b9c53f59bbe6619a175779db2cbc/airflow/providers/common/sql/operators/sql.py#L155-L178

### How to reproduce

Here's the test where I found the error.

https://github.com/apache/airflow/blob/fcc9f3dc0eb0b9c53f59bbe6619a175779db2cbc/tests/providers/common/sql/operators/test_sql.py#L615-L622

https://github.com/apache/airflow/blob/fcc9f3dc0eb0b9c53f59bbe6619a175779db2cbc/tests/providers/common/sql/operators/test_sql.py#L1020-L1033

### Operating System

Not required

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",phi-friday,2024-08-14 07:35:26+00:00,[],2024-08-14 08:42:51+00:00,2024-08-14 08:42:50+00:00,https://github.com/apache/airflow/issues/41460,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('provider:common-sql', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2464781334,issue,closed,completed,Airflow DAG Pod stuck in NotReady state ,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.8.1

### What happened?

When I trigger the DAG it created multiple pods but after completion of dag run it's pod stuck in the NotReady state and it remains there in the cluster , I tried alot currently I am using kubernetes executor and I can't change direct to celery as most of the things will impact because of this 

### What you think should happen instead?

There must be any way to do this so once dag run is successfully done it's pod will be deleted .

### How to reproduce

Want some solution or feature so it will delete the pod of dag once dag run is done 

### Operating System

macos 

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Ajit-mycom,2024-08-14 02:49:50+00:00,[],2024-08-14 10:11:01+00:00,2024-08-14 10:11:00+00:00,https://github.com/apache/airflow/issues/41455,"[('kind:bug', 'This is a clearly a bug'), ('kind:feature', 'Feature Requests'), ('area:helm-chart', 'Airflow Helm Chart'), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2464776686,issue,open,,Airflow Clusterrole and ClusterroleBinding for Sparkapplications ,"### Official Helm Chart version

1.13.1

### Apache Airflow version

2.8.1

### Kubernetes Version

1.29

### Helm Chart configuration

_No response_

### Docker Image customizations

_No response_

### What happened

I have deployed Apache Airflow with the multiNamespaceMode: true setting, which resulted in the creation of a ClusterRole with

permissions limited to pods only. However, I need to add permissions for sparkapplications in the API group sparkoperator.k8s.io

because I am encountering the following error:

cannot create resource ""sparkapplications"" in API group ""sparkoperator.k8s.io"" 

I explored two potential solutions:

Manual Role Editing: I found that manually editing or patching the ClusterRole is not permitted, which does not provide a

sustainable solution.

Local Chart Modification: While modifying the ClusterRole in a local Airflow chart could resolve the issue, it introduces complexity

as it necessitates using a custom chart for upgrades and feature management.

Given these constraints, I am seeking guidance from the Airflow team on how to address this issue. Is there an alternative

approach to manage sparkapplications permissions within the current deployment setup? Your assistance would be greatly

appreciated.

### What you think should happen instead

there must be clusterrole and clusterrolebinding for sparkapplications currently we have only for pods which works with multinamespace = true

### How to reproduce

I have deployed Apache Airflow with the multiNamespaceMode: true setting, which resulted in the creation of a ClusterRole with

permissions limited to pods only. However, I need to add permissions for sparkapplications in the API group sparkoperator.k8s.io

because I am encountering the following error:

cannot create resource ""sparkapplications"" in API group ""sparkoperator.k8s.io"" 

I explored two potential solutions:

Manual Role Editing: I found that manually editing or patching the ClusterRole is not permitted, which does not provide a

sustainable solution.

Local Chart Modification: While modifying the ClusterRole in a local Airflow chart could resolve the issue, it introduces complexity

as it necessitates using a custom chart for upgrades and feature management.

Given these constraints, I am seeking guidance from the Airflow team on how to address this issue. Is there an alternative

approach to manage sparkapplications permissions within the current deployment setup? Your assistance would be greatly

appreciated.

### Anything else

No

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Ajit-mycom,2024-08-14 02:43:59+00:00,[],2025-01-10 08:31:05+00:00,,https://github.com/apache/airflow/issues/41454,"[('kind:bug', 'This is a clearly a bug'), ('area:helm-chart', 'Airflow Helm Chart'), ('pending-response', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2287728764, 'issue_id': 2464776686, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 14, 2, 44, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2298045440, 'issue_id': 2464776686, 'author': 'Ajit-mycom', 'body': 'If anyone have any idea on this please help me with that', 'created_at': datetime.datetime(2024, 8, 20, 6, 12, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2582051268, 'issue_id': 2464776686, 'author': 'shahar1', 'body': ""The steps for reproduction are unclear (basically copy-paste of the problem statement) - we'd be happy if you could provide minimal yet clear steps to reproduce it, otherwise it's unlikely that we'll be able to help."", 'created_at': datetime.datetime(2025, 1, 10, 8, 31, 4, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-14 02:44:02 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

Ajit-mycom (Issue Creator) on (2024-08-20 06:12:35 UTC): If anyone have any idea on this please help me with that

shahar1 on (2025-01-10 08:31:04 UTC): The steps for reproduction are unclear (basically copy-paste of the problem statement) - we'd be happy if you could provide minimal yet clear steps to reproduce it, otherwise it's unlikely that we'll be able to help.

"
2464645579,issue,closed,completed,Is it possible to avoid a running dag run that would be affected if the dag file is changed? ,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.2.3

### What happened?

For running dag runs, any changes to the dag file should affect it.





### What you think should happen instead?

For running dag runs, any changes to the dag file should not affect it.



### How to reproduce

create a dag file with a dag id of ""dag-001"" containing a task A and B. A will take 30 minutes to execute, I run the dag file(first run), and while it's running I update the file and add a new task after B, and then run it again(second run).
My expectation is that the first run should only run tasks A and B, but it will also run C, even though I didn't add C at startup.

### Operating System

linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",shawn-ann,2024-08-14 01:10:04+00:00,[],2024-08-14 05:09:56+00:00,2024-08-14 01:18:01+00:00,https://github.com/apache/airflow/issues/41452,"[('kind:bug', 'This is a clearly a bug'), ('area:helm-chart', 'Airflow Helm Chart'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2287592593, 'issue_id': 2464645579, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 14, 1, 10, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2287611369, 'issue_id': 2464645579, 'author': 'potiuk', 'body': 'Please do not open issues for discussions - especially if you already opened one', 'created_at': datetime.datetime(2024, 8, 14, 1, 17, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2287619005, 'issue_id': 2464645579, 'author': 'potiuk', 'body': 'Patiently wait until someone will respond there (and for sure few hours for respeons is not patient - taking into account this forum where volunteers are helping others in their free time)', 'created_at': datetime.datetime(2024, 8, 14, 1, 18, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2287867778, 'issue_id': 2464645579, 'author': 'shawn-ann', 'body': 'Thanks', 'created_at': datetime.datetime(2024, 8, 14, 5, 9, 56, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-14 01:10:08 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

potiuk on (2024-08-14 01:17:03 UTC): Please do not open issues for discussions - especially if you already opened one

potiuk on (2024-08-14 01:18:01 UTC): Patiently wait until someone will respond there (and for sure few hours for respeons is not patient - taking into account this forum where volunteers are helping others in their free time)

shawn-ann (Issue Creator) on (2024-08-14 05:09:56 UTC): Thanks

"
2464185959,issue,closed,completed,ModuleNotFoundError: No module named 'methodtools' due to breaking changes made in a new release 3.8.0 of apache-airflow-providers-microsoft-mssql library,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.7.3

### What happened?

After upgrading from apache-airflow-providers-microsoft-mssql = ""==3.7.2"" to apache-airflow-providers-microsoft-mssql = ""==3.8.0"" while also upgrading from composer-2.8.7-airflow-2.7.3 to composer-2.8.8-airflow-2.7.3, I saw a DAG import error, and had to specify and install methodtools = ""==0.4.7"" to get past the error. I was expecting this dependent package get installed automatically. Google Support says the mssql.py script is the one requesting the dependency but after checking the readme for the apache-ariflow-providers-microsoft-mssql, it is not listed as a needed package [2][3]. For a fix on this you would have to reach out to the package administrator team, as this dependency is not listed and therefore airflow did not have any instruction to download it. 

[1] File ""/opt/python3.11/lib/python3.11/site-packages/airflow/providers/google/cloud/transfers/mssql_to_gcs.py"", line 27, in from airflow.providers.microsoft.mssql.hooks.mssql import MsSqlHook File ""/opt/python3.11/lib/python3.11/site-packages/airflow/providers/microsoft/mssql/hooks/mssql.py"", line 26, in from methodtools import lru_cache ModuleNotFoundError: No module named 'methodtools'
[2] 
``apache-airflow``                       ``>=2.7.0``
``apache-airflow-providers-common-sql``  ``>=1.14.1``
``pymssql``                              ``>=2.3.0``
[3] https://pypi.org/project/apache-airflow-providers-microsoft-mssql/

### What you think should happen instead?

I think methodtools should be included in the dependent packages of the apache-airflow-providers-microsoft-mssql provider

### How to reproduce

update apache-airflow-providers-microsoft-mssql library to 3.8.0



### Operating System

Ubuntu 20.04.6 LTS

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.27.0
apache-airflow-providers-apache-beam @ file:///usr/local/lib/airflow-pypi-dependencies-2.7.3/python3.11/apache_airflow_providers_apache_beam-5.7.1-py3-none-any.whl
apache-airflow-providers-celery @ file:///usr/local/lib/airflow-pypi-dependencies-2.7.3/python3.11/apache_airflow_providers_celery-3.6.0-py3-none-any.whl
apache-airflow-providers-cncf-kubernetes @ file:///usr/local/lib/airflow-pypi-dependencies-2.7.3/python3.11/apache_airflow_providers_cncf_kubernetes-8.3.3-py3-none-any.whl
apache-airflow-providers-common-compat==1.1.0
apache-airflow-providers-common-sql @ file:///usr/local/lib/airflow-pypi-dependencies-2.7.3/python3.11/apache_airflow_providers_common_sql-1.14.2-py3-none-any.whl
apache-airflow-providers-dbt-cloud @ file:///usr/local/lib/airflow-pypi-dependencies-2.7.3/python3.11/apache_airflow_providers_dbt_cloud-3.9.0-py3-none-any.whl
apache-airflow-providers-ftp @ file:///usr/local/lib/airflow-pypi-dependencies-2.7.3/python3.11/apache_airflow_providers_ftp-3.10.0-py3-none-any.whl
apache-airflow-providers-google @ file:///usr/local/lib/airflow-pypi-dependencies-2.7.3/python3.11/apache_airflow_providers_google-10.21.0-py3-none-any.whl
apache-airflow-providers-hashicorp @ file:///usr/local/lib/airflow-pypi-dependencies-2.7.3/python3.11/apache_airflow_providers_hashicorp-3.7.1-py3-none-any.whl
apache-airflow-providers-http @ file:///usr/local/lib/airflow-pypi-dependencies-2.7.3/python3.11/apache_airflow_providers_http-4.12.0-py3-none-any.whl
apache-airflow-providers-imap @ file:///usr/local/lib/airflow-pypi-dependencies-2.7.3/python3.11/apache_airflow_providers_imap-3.6.1-py3-none-any.whl
apache-airflow-providers-microsoft-mssql==3.8.0
apache-airflow-providers-mysql @ file:///usr/local/lib/airflow-pypi-dependencies-2.7.3/python3.11/apache_airflow_providers_mysql-5.6.2-py3-none-any.whl
apache-airflow-providers-postgres @ file:///usr/local/lib/airflow-pypi-dependencies-2.7.3/python3.11/apache_airflow_providers_postgres-5.11.2-py3-none-any.whl
apache-airflow-providers-sendgrid @ file:///usr/local/lib/airflow-pypi-dependencies-2.7.3/python3.11/apache_airflow_providers_sendgrid-3.5.1-py3-none-any.whl
apache-airflow-providers-sqlite @ file:///usr/local/lib/airflow-pypi-dependencies-2.7.3/python3.11/apache_airflow_providers_sqlite-3.8.1-py3-none-any.whl
apache-airflow-providers-ssh @ file:///usr/local/lib/airflow-pypi-dependencies-2.7.3/python3.11/apache_airflow_providers_ssh-3.11.2-py3-none-any.whl
apache-airflow-providers-tableau==4.5.2

### Deployment

Google Cloud Composer

### Deployment details

composer-2.8.8-airflow-2.7.3

### Anything else?

This problem occurs every time the parser tries to import a DAG that uses the provider. I had to install `methodtools = ""==0.4.7""` manually/explicitly to get past the issue.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",thelastmessiha,2024-08-13 20:44:03+00:00,[],2024-08-13 23:46:02+00:00,2024-08-13 23:46:02+00:00,https://github.com/apache/airflow/issues/41446,"[('kind:bug', 'This is a clearly a bug'), ('duplicate', 'Issue that is duplicated'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:microsoft-mssql', '')]","[{'comment_id': 2287400406, 'issue_id': 2464185959, 'author': 'potiuk', 'body': 'Already addressed in #41392', 'created_at': datetime.datetime(2024, 8, 13, 23, 46, 2, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-08-13 23:46:02 UTC): Already addressed in #41392

"
2464056498,issue,closed,completed,DockerSwarmOperator retrieve_output XCom not working,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.10.0rc1

### What happened?

Using a `DockerSwarmOperator`, with `retrieve_output=True` and `retrieve_output_path='/path/to/pickle/file'`, the content of the file located at `/path/to/pickle/file` is not set as XCom prior to deletion of the container.

This functionality is described in the `DockerOperator`[documentation](https://airflow.apache.org/docs/apache-airflow-providers-docker/stable/_api/airflow/providers/docker/operators/docker/index.html#airflow.providers.docker.operators.docker.DockerOperator). As `DockerSwarmOperator` inherits from `DockerOperator` (cf. [source](https://airflow.apache.org/docs/apache-airflow-providers-docker/stable/_modules/airflow/providers/docker/operators/docker_swarm.html#DockerSwarmOperator)), I was expecting the `retrieve_output` feature to work using `DockerSwarmOperator`.

### What you think should happen instead?

- The same behavior as when using `DockerOperator`, i.e. the contents of the file located at `/path/to/pickle/file` is set as XCom (""return_value"" key) when `retrieve_output=True` and `retrieve_output_path='/path/to/pickle/file'`.

### How to reproduce

Below is an example of how to reproduce the issue.
The `docker_url` argument must be adapted to the deployment context.

The DAG is made up of 2 identical tasks, using either a DockerOperator (`write_xcom_docker`) or a DockerSwarmOperator (`write_xcom_docker_swarm`). In these two tasks :
- A python script writes a dictionary to a Pickle file (`/tmp/variable.pickle`).
- The path to this Pickle file is associated with the `retrieve_output_path argument` in the Docker(Swarm)Operator.

```py
from airflow import DAG
from airflow.utils.dates import days_ago
from datetime import timedelta

from docker import types
from airflow.providers.docker.operators.docker import DockerOperator
from airflow.providers.docker.operators.docker_swarm import DockerSwarmOperator

params = {
    'dag_id': 'test_xcom_docker_docker_swarm',
    'catchup': False,
    'max_active_runs': 1,
    'default_args': {
        'owner': 'airflow',
        'start_date': days_ago(1),
        'retries': 1,
        'retry_delay': timedelta(minutes=5)
    }
}

with DAG(**params) as dag:
    write_xcom_docker = DockerOperator(
        task_id='write_xcom_docker',
        image='python:latest',
        api_version='auto',
        command=""""""python -c '

import os
import pickle

capitals = {
  ""Canada"": ""Ottawa"", 
  ""England"": ""London"",
  ""France"": ""Paris"",
  ""Germany"": ""Berlin"", 
}

file_path = ""/tmp/variable.pickle""
with open(file_path, ""wb+"") as file:
    pickle.dump(capitals, file)

    '
    """""",
        retrieve_output=True,
        retrieve_output_path='/tmp/variable.pickle')

    write_xcom_docker_swarm = DockerSwarmOperator(
        task_id='write_xcom_docker_swarm',
        image='python:latest',
        api_version='auto',
        command=""""""python -c '

import os
import pickle

capitals = {
  ""Canada"": ""Ottawa"", 
  ""England"": ""London"",
  ""France"": ""Paris"",
  ""Germany"": ""Berlin"", 
}

file_path = ""/tmp/variable.pickle""
with open(file_path, ""wb+"") as file:
    pickle.dump(capitals, file)

    '
    """""",
        retrieve_output=True,
        retrieve_output_path='/tmp/variable.pickle',
        mode=types.ServiceMode(mode=""replicated"", replicas=2))

write_xcom_docker >> write_xcom_docker_swarm
```

- For the task using a `DockerOperator`, the content of the Pickle file is set as XCom as expected.

![image](https://github.com/user-attachments/assets/77e6dd49-0812-4996-ac86-e0beb711107f)

- For the task using a `DockerSwarmOperator`, nothing is set as XCom.

![image](https://github.com/user-attachments/assets/1653f042-2812-4e0d-97ee-798092342186)

### Operating System

Docker host : Debian GNU/Linux 11 (bullseye)
Container OS : Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

apache-airflow-providers-docker==3.12.3

### Deployment

Other Docker-based deployment

### Deployment details

Airflow was deployed using Docker Swarm, based on Docker version 26.1.4 (build 5650f9b)

### Anything else?

The issue has also been tested in version `2.9.2` (using apache-airflow-providers-docker==3.12.0), with the same result.

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rgriffier,2024-08-13 19:23:18+00:00,['rgriffier'],2024-08-22 08:35:24+00:00,2024-08-22 08:35:24+00:00,https://github.com/apache/airflow/issues/41445,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('provider:docker', '')]","[{'comment_id': 2286972005, 'issue_id': 2464056498, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 13, 19, 23, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2293379940, 'issue_id': 2464056498, 'author': 'rgriffier', 'body': 'A PR has been created: https://github.com/apache/airflow/pull/41531', 'created_at': datetime.datetime(2024, 8, 16, 12, 0, 58, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-13 19:23:21 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

rgriffier (Issue Creator) on (2024-08-16 12:00:58 UTC): A PR has been created: https://github.com/apache/airflow/pull/41531

"
2463820131,issue,closed,completed,UI: Trigger DAG with config form transforms array of objects into array of strings,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.6.3

### What happened?

In the Trigger DAG end point, using the DAG conf Parameters form will incorrectly convert a array of object to an array of string when the parameter type is defined as:

```
""jobs_list"": Param(
            [ {""name"": ""account_name"", ""country"": ""country_name""} ],
            type=""array"",
            items={
                ""type"": ""object"",
                ""properties"": {""name"": {""type"": ""string""}, ""country_name"": {""type"": ""string""}},
            },
)
```

This can be seen by comparing the value in the field to what is produced in the generated JSON config : 
<img width=""805"" alt=""Capture d’écran 2024-08-13 à 18 51 21"" src=""https://github.com/user-attachments/assets/d399af9e-1542-47e5-955d-0c0656358039"">

If this JSON configuration is submitted, then the validation will raise an error because `""{""name"": ""account_name"", ""country"": ""country_name""}""` is not a valid object but a string 

However, if I trigger the dag without specifying params, the default ones work ! 

### What you think should happen instead?

When a parameter is defined as an array of objects, the DAG Conf Parameters form should generate a value in the Generated JSON config that is an array of objects.

```
Param(
              [ {""name"": ""account_name"", ""country"": ""country_name""} ],
              type=""array"",
              items={
                ""type"": ""object"",
                ""properties"": {""name"": {""type"": ""string""}, ""country_name"": {""type"": ""string""}},
               },
            ),
```
It should be:

```
 [ {""name"": ""account_name"", ""country"": ""country_name""} ]
```

instead of
`[ ""{""name"": ""account_name"", ""country"": ""country_name""} ""]`

### How to reproduce

1. Create a DAG that has a parameter with previous definition of array of objects : I took it from Airflow Official [Dag Example](https://airflow.apache.org/docs/apache-airflow/stable/_modules/airflow/example_dags/example_params_ui_tutorial.html)

2. Navigate to the Trigger DAG Run for that DAG
3. Add an object in the DAG Conf Parameters form
4. Note the value in the Generated JSON config has quotes
5. Press submit, and see an error message like:

### Operating System

Amazon Linux AMI

### Versions of Apache Airflow Providers

_No response_

### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",maylisdvoodoo,2024-08-13 17:03:35+00:00,[],2024-08-14 21:38:30+00:00,2024-08-14 21:38:14+00:00,https://github.com/apache/airflow/issues/41439,"[('kind:bug', 'This is a clearly a bug'), ('duplicate', 'Issue that is duplicated'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2286717439, 'issue_id': 2463820131, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 13, 17, 3, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2288052285, 'issue_id': 2463820131, 'author': 'jscheffl', 'body': 'I assume the ""array"" option was mainly intended for an array of strings. Therefore the implementation is limited at the moment. There was also a parallel bug and improvement made in Airflow 2.7 line. \r\n\r\nIs the approach as it was fixed in https://github.com/apache/airflow/pull/32734 working as alternative... can you try upgrading to at least 2.7.1 and check again?', 'created_at': datetime.datetime(2024, 8, 14, 7, 35, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2288232133, 'issue_id': 2463820131, 'author': 'maylisdvoodoo', 'body': ""Thanks @jscheffl for your quick answer. \r\nI am unfortunately not the one in charge of updating Airflow version for my company. As we are hundreds using it, there is a clear process. I asked the team in charge, update is planned so I'll need to wait and find another solution for now."", 'created_at': datetime.datetime(2024, 8, 14, 9, 2, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2289956036, 'issue_id': 2463820131, 'author': 'jscheffl', 'body': ""Great. Then I'd close the issue assuming it is a duplicate, if the upgrade does not work as I promise, then let's re-open. Maybe the issue you have is an argument to upgrade, you also will have a lot of security fixes with newer releses included and a couple of cool new features."", 'created_at': datetime.datetime(2024, 8, 14, 21, 38, 14, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-13 17:03:39 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

jscheffl on (2024-08-14 07:35:15 UTC): I assume the ""array"" option was mainly intended for an array of strings. Therefore the implementation is limited at the moment. There was also a parallel bug and improvement made in Airflow 2.7 line. 

Is the approach as it was fixed in https://github.com/apache/airflow/pull/32734 working as alternative... can you try upgrading to at least 2.7.1 and check again?

maylisdvoodoo (Issue Creator) on (2024-08-14 09:02:28 UTC): Thanks @jscheffl for your quick answer. 
I am unfortunately not the one in charge of updating Airflow version for my company. As we are hundreds using it, there is a clear process. I asked the team in charge, update is planned so I'll need to wait and find another solution for now.

jscheffl on (2024-08-14 21:38:14 UTC): Great. Then I'd close the issue assuming it is a duplicate, if the upgrade does not work as I promise, then let's re-open. Maybe the issue you have is an argument to upgrade, you also will have a lot of security fixes with newer releses included and a couple of cool new features.

"
2463563725,issue,closed,completed,Airflow scheduler misses success events of worker pods on kubernetes,"### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

We had several sensors that failed to be rescheduled by the scheduler because it still thought that the worker tasks were running.

The root cause was that the scheduler missed an update event from worker task because the Kubernetes node, where the Airflow worker pod was running on, got deleted soon after the worker finished successful. This does not follow the assumption in the code that a delete of the worker is only issued by Airflow itself. The wrong code is in `kubernetes_executor_utils.py`:

```
  if (
     event[""type""] == ""DELETED""
     or POD_EXECUTOR_DONE_KEY in pod.metadata.labels
     or pod.metadata.deletion_timestamp
  ):
    self.log.info(
        ""Skipping event for Succeeded pod %s - event for this pod already sent to executor"",
         pod_name,
    )
    return
```
In the logs we see only skipping event messages for the worker pods instead of first an event that was processed.
The comment of the if check say the following which is not necessarily true:
```
# We get multiple events once the pod hits a terminal state, and we only want to
# send it along to the scheduler once.
# If our event type is DELETED, we have the POD_EXECUTOR_DONE_KEY, or the pod has
# a deletion timestamp, we've already seen the initial Succeeded event and sent it
# along to the scheduler.
```


Our sensor failed such that it needed to be rescheduled and the task got requeued. 
At that time the scheduler never scheduled the task as it thought there was still one running and logged the following:

`{base_executor.py:284} INFO - queued but still running;`

The only way to fix it was to restart the scheduler as then the internal state of the scheduler was in sync with kubernetes.


### What you think should happen instead?

The fundamental problem is that the watcher for events on kubernetes pods skipped the successful event of the worker.

In order to make sure we process all events there are 2 options:
- remove the skipping of events as it is wrong in certain edge situations. I investigated when it was added and it was not really added for a reason, just as a behind the scene optimisation as far as I can tell. https://github.com/apache/airflow/pull/30872
- if you want to handle this reliably in Kubernetes you should use finalizers for your worker pods. This way you can guarantee that you do not miss any events and can use the finalizer to make sure you only process success events once. 

### How to reproduce

The issue is difficult to reproduce reliably. We do notice it on our huge production from time to time. 
It is however easy to see that the code is wrong in certain edge cases 

### Operating System

kubernetes: apache/airflow:slim-2.9.3-python3.11

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes==8.0.1
apache-airflow-providers-common-io==1.3.2
apache-airflow-providers-common-sql==1.14.2
apache-airflow-providers-fab==1.2.2
apache-airflow-providers-ftp==3.10.0
apache-airflow-providers-http==4.12.0
apache-airflow-providers-imap==3.6.1
apache-airflow-providers-opsgenie==4.0.0
apache-airflow-providers-postgres==5.11.2
apache-airflow-providers-slack==7.3.2
apache-airflow-providers-smtp==1.7.1
apache-airflow-providers-sqlite==3.8.1


### Deployment

Other 3rd-party Helm chart

### Deployment details

Kubernetes deployment

### Anything else?

/

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",nclaeys,2024-08-13 14:55:49+00:00,[],2024-10-11 15:12:36+00:00,2024-10-11 15:12:36+00:00,https://github.com/apache/airflow/issues/41436,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]","[{'comment_id': 2301814614, 'issue_id': 2463563725, 'author': 'vlieven', 'body': 'I was able to resolve the issue I reported in #40516 by removing the wrong piece of code in `kubernetes_executor_utils.py` as suggested. Maybe that case can be used as example why this functionality should be changed?', 'created_at': datetime.datetime(2024, 8, 21, 11, 23, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2336458286, 'issue_id': 2463563725, 'author': 'gautampa7', 'body': 'Hi, \r\n\r\nWe are having multiple airflow instances and we observed that the scheduler thinks that tasks are still running and fails to schedule anything else i.e gets stuck whenever GKE cluster node upgrades are happening. We suspect that when a task pod running on the node is deleted/drained by the cluster (during the node upgrade process) the scheduler goes into this state. The only fix was to restart the scheduler and it started working again as expected. \r\n\r\nWe have weekly GKE upgrades scheduled and are noticing this issue in our airflow instances exactly during the upgrade.\r\n\r\nI think as @nclaeys  suggested and tested by @vlieven we should be removing that block which wrongly assumes that the pod termination is only issued by airflow scheduler and not from the cluster.', 'created_at': datetime.datetime(2024, 9, 7, 21, 34, 28, tzinfo=datetime.timezone.utc)}]","vlieven on (2024-08-21 11:23:50 UTC): I was able to resolve the issue I reported in #40516 by removing the wrong piece of code in `kubernetes_executor_utils.py` as suggested. Maybe that case can be used as example why this functionality should be changed?

gautampa7 on (2024-09-07 21:34:28 UTC): Hi, 

We are having multiple airflow instances and we observed that the scheduler thinks that tasks are still running and fails to schedule anything else i.e gets stuck whenever GKE cluster node upgrades are happening. We suspect that when a task pod running on the node is deleted/drained by the cluster (during the node upgrade process) the scheduler goes into this state. The only fix was to restart the scheduler and it started working again as expected. 

We have weekly GKE upgrades scheduled and are noticing this issue in our airflow instances exactly during the upgrade.

I think as @nclaeys  suggested and tested by @vlieven we should be removing that block which wrongly assumes that the pod termination is only issued by airflow scheduler and not from the cluster.

"
2463428569,issue,closed,completed,DAGs are not marked as stale if the AIRFLOW__CORE__DAGS_FOLDER changes,"### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Suppose we start Airflow with the following DAGs located in /<user_root>/airflow/dags:

    abc.py
    xyz.py

Later, we decide to change the default DAG folder to /<user_root>/airflow/dags_backup/, which contains different DAG files, by setting the environment variable AIRFLOW__CORE__DAGS_FOLDER=/<user_root>/airflow/dags_backup/. After restarting the scheduler, the new folder includes the following DAGs:

    efg.py
    hij.py

At this point, the Airflow UI will display all the DAGs from both sets of files (abc.py, xyz.py, efg.py, and hij.py), even though the current dags_folder no longer contains the original abc.py and xyz.py.

Note: This behavior has been observed even for active DAGs.

### What you think should happen instead?

The expectation is that the airflow UI should only dags belonging to the new dags folder `/<user_root>/airflow/dags_backup/`.

### How to reproduce

1. Start Airflow with some dags in the `dags` folder.
2. Change the default dag folder using `AIRFLOW__CORE__DAGS_FOLDER` to another folder containing a different set of dags. 
3. Notice both sets of dags are visible on the UI. 

### Operating System

Ventura - Mac

### Versions of Apache Airflow Providers

_No response_

### Deployment

Virtualenv installation

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",utkarsharma2,2024-08-13 13:57:11+00:00,[],2024-08-28 09:07:25+00:00,2024-08-28 09:07:25+00:00,https://github.com/apache/airflow/issues/41432,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:DAG-processing', ''), ('affected_version:2.9', 'Issues Reported for 2.9')]",[],
2463172029,issue,open,,Ability to delete a Dataset from the UI and/or supported by the Airflow REST API,"### Description

Ability to delete a Dataset (not Dataset event) via the Airflow UI which is backed by Airflow REST API . This API endpoint could also be released separately.

### Use case/motivation

If a user creates a dataset 
- which is incorrectly spelled
- or wants to use in the current environment temporarily 
- or has migrated a DAG that produced a particular Dataset

In all the above scenarios, the user might require to delete the Datasets that are not required and have a cleaner view from the Airflow Datasets UI.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",manmeetkaur,2024-08-13 11:59:16+00:00,[],2024-08-18 22:49:32+00:00,,https://github.com/apache/airflow/issues/41431,"[('kind:feature', 'Feature Requests'), ('area:API', ""Airflow's REST/HTTP API""), ('duplicate', 'Issue that is duplicated'), ('area:datasets', 'Issues related to the datasets feature')]","[{'comment_id': 2295422523, 'issue_id': 2463172029, 'author': 'sunank200', 'body': 'This PR seems related: https://github.com/apache/airflow/pull/36781 \r\n\r\nAlso it seems duplicate of issue of https://github.com/apache/airflow/issues/36308', 'created_at': datetime.datetime(2024, 8, 18, 22, 48, 41, tzinfo=datetime.timezone.utc)}]","sunank200 on (2024-08-18 22:48:41 UTC): This PR seems related: https://github.com/apache/airflow/pull/36781 

Also it seems duplicate of issue of https://github.com/apache/airflow/issues/36308

"
2463105050,issue,open,,Scheduler job exit on database dead lock,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.7.2

### What happened?

The scheduler job raise exception on database dead lock and exist.

Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/dist-packages/airflow/jobs/scheduler_job_runner.py"", line 845, in _execute
    self._run_scheduler_loop()
  File ""/usr/local/lib/python3.8/dist-packages/airflow/jobs/scheduler_job_runner.py"", line 991, in _run_scheduler_loop
    next_event = timers.run(blocking=False)
  File ""/usr/lib/python3.8/sched.py"", line 151, in run
    action(*argument, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/airflow/utils/event_scheduler.py"", line 37, in repeat
    action(*args, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/airflow/utils/session.py"", line 77, in wrapper
    return func(*args, session=session, **kwargs)
  File ""/usr/local/lib/python3.8/dist-packages/airflow/jobs/scheduler_job_runner.py"", line 1680, in check_trigger_timeouts
    num_timed_out_tasks = session.execute(
  File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/orm/session.py"", line 1717, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py"", line 1710, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/sql/elements.py"", line 334, in _execute_on_connection
    return connection._execute_clauseelement(
  File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py"", line 1577, in _execute_clauseelement
    ret = self._execute_context(
  File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py"", line 1953, in _execute_context
    self._handle_dbapi_exception(
  File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py"", line 2134, in _handle_dbapi_exception
    util.raise_(
  File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/util/compat.py"", line 211, in raise_
    raise exception
  File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/base.py"", line 1910, in _execute_context
    self.dialect.do_execute(
  File ""/usr/local/lib/python3.8/dist-packages/sqlalchemy/engine/default.py"", line 736, in do_execute
    cursor.execute(statement, parameters)
  File ""/usr/local/lib/python3.8/dist-packages/MySQLdb/cursors.py"", line 179, in execute
    res = self._query(mogrified_query)
  File ""/usr/local/lib/python3.8/dist-packages/MySQLdb/cursors.py"", line 330, in _query
    db.query(q)
  File ""/usr/local/lib/python3.8/dist-packages/MySQLdb/connections.py"", line 255, in query
    _mysql.connection.query(self, query)
sqlalchemy.exc.OperationalError: (MySQLdb.OperationalError) (1213, 'Deadlock found when trying to get lock; try restarting transaction')
[SQL: UPDATE task_instance SET state=%s, updated_at=%s, trigger_id=%s, next_method=%s, next_kwargs=%s WHERE task_instance.state = %s AND task_instance.trigger_timeout < %s]
[parameters: (<TaskInstanceState.SCHEDULED: 'scheduled'>, datetime.datetime(2024, 8, 2, 13, 14, 22, 215659), None, '__fail__', '{""__var"": {""error"": ""Trigger/execution timeout""}, ""__type"": ""dict""}', <TaskInstanceState.DEFERRED: 'deferred'>, datetime.datetime(2024, 8, 2, 13, 14, 22, 202306, tzinfo=Timezone('UTC')))]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2024-08-02T06:14:22.258-0700[0m] {[34mkubernetes_executor.py:[0m706} INFO[0m - Shutting down Kubernetes executor[0m

### What you think should happen instead?

Retry should be applied to this deadlock, as the trigger already includes a retry on update. Both should handle the same scenario when updating the task_instance associated with the trigger.

### How to reproduce

1. Set scheduler instance number >= 2, Triggerer instance number >= 2
2. Trigger 20 DAG runs, each with at least one triggered job with runtime > 1 mins

### Operating System

Ubuntu

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",TakawaAkirayo,2024-08-13 11:25:35+00:00,['TakawaAkirayo'],2024-08-20 05:54:00+00:00,,https://github.com/apache/airflow/issues/41428,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('area:MetaDB', 'Meta Database related issues.'), ('area:core', ''), ('affected_version:2.7', 'Issues Reported for 2.7')]",[],
2462769169,issue,closed,not_planned,Use `set` instead of `list` for dag's tags,"### Description

Instead of using `list` for holding the tags of a specific dag (as we can see at the [docs](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/dag/index.html#airflow.models.dag.DAG)), I suggest using `set` instead, specifically [`MutableSet` for the type hinting](https://docs.python.org/3/library/collections.abc.html#collections.abc.MutableSet), and [python `set`](https://docs.python.org/3/tutorial/datastructures.html#sets) as the default implementation.

### Use case/motivation

I would want to add multiple tags (even the same ones) faster, without worrying about duplicates, and reduce the memory usage because of it. Also, it's good to check if a certain tag already exists.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Avihais12344,2024-08-13 08:46:33+00:00,[],2024-09-14 00:14:14+00:00,2024-09-14 00:14:14+00:00,https://github.com/apache/airflow/issues/41420,"[('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('kind:feature', 'Feature Requests'), ('pending-response', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2285703691, 'issue_id': 2462769169, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 13, 8, 46, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2295421335, 'issue_id': 2462769169, 'author': 'sunank200', 'body': ""@Avihais12344 Thanks for raising the issue. I had follow-up questions:\r\n- Will this change to `set` break any existing DAGs or workflows expecting a list? I think it would. Wouldn't this be breaking change? \r\n- Are there any edge cases or performance regressions to consider with this change?\r\n- How would we ensure the use of `set` is consistent across the codebase, particularly in how tags are stored/retrieved?"", 'created_at': datetime.datetime(2024, 8, 18, 22, 43, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2302508632, 'issue_id': 2462769169, 'author': 'Avihais12344', 'body': ""Hello @sunank200, thanks for responding.\r\n\r\nIt would break existing code expecting tags as list, that's why I wanted to discuss it here.\r\nThe interface is different from list, but other than that, it would not be too much than different, we can think of it as a unique list. So I think there would not be edge case, as we alreay take the unique items from the tags list.\r\n\r\nWe can ensure the use of `set` accross the codebase by using typing and linters that would enforce the right types. I would also use the `collections.abc` type of `MutableSet` instead of the builtin `set`, to allow any type of `set`."", 'created_at': datetime.datetime(2024, 8, 21, 16, 31, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2306983159, 'issue_id': 2462769169, 'author': 'Avihais12344', 'body': 'Created a PR for this [here](https://github.com/apache/airflow/pull/41695).', 'created_at': datetime.datetime(2024, 8, 23, 12, 23, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2334958959, 'issue_id': 2462769169, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 9, 7, 0, 13, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2350730794, 'issue_id': 2462769169, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 9, 14, 0, 14, 13, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-13 08:46:35 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

sunank200 on (2024-08-18 22:43:31 UTC): @Avihais12344 Thanks for raising the issue. I had follow-up questions:
- Will this change to `set` break any existing DAGs or workflows expecting a list? I think it would. Wouldn't this be breaking change? 
- Are there any edge cases or performance regressions to consider with this change?
- How would we ensure the use of `set` is consistent across the codebase, particularly in how tags are stored/retrieved?

Avihais12344 (Issue Creator) on (2024-08-21 16:31:23 UTC): Hello @sunank200, thanks for responding.

It would break existing code expecting tags as list, that's why I wanted to discuss it here.
The interface is different from list, but other than that, it would not be too much than different, we can think of it as a unique list. So I think there would not be edge case, as we alreay take the unique items from the tags list.

We can ensure the use of `set` accross the codebase by using typing and linters that would enforce the right types. I would also use the `collections.abc` type of `MutableSet` instead of the builtin `set`, to allow any type of `set`.

Avihais12344 (Issue Creator) on (2024-08-23 12:23:38 UTC): Created a PR for this [here](https://github.com/apache/airflow/pull/41695).

github-actions[bot] on (2024-09-07 00:13:48 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-09-14 00:14:13 UTC): This issue has been closed because it has not received response from the issue author.

"
2462684494,issue,closed,not_planned,Error when you pass string arguments with white spaces on kubernetes pod operator,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.2

### What happened?

When you pass arguments with white spaces on a kubernetes pod operator, a carriage return is added after each whitespace on the rendered template and the operator reads each line as distinct params. We tested this on 2.9.2 and previous versions. The same happens with apache-airflow-providers-cncf-kubernetes 8.3.1 and previous version.
For example: 

if the argument is: {""data"": ""value with spaces""}

The rendered template prints this:

'--data',
'{""data"": ""value '
'with '
'spaces""}'


But it should be like this:
'--data',
'{""data"": ""value with spaces""}'


### What you think should happen instead?

The rendered template should not add a return carriage after white spaces

### How to reproduce

Passing arguments with a whitespace on a kubernetes pod operator

### Operating System

Ubuntu 22.04 container

### Versions of Apache Airflow Providers

apache-airflow==2.9.2
apache-airflow-providers-cncf-kubernetes 8.3.1

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

airflow install on linux box.

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",abmramirez,2024-08-13 08:06:05+00:00,[],2024-09-21 00:14:19+00:00,2024-09-21 00:14:18+00:00,https://github.com/apache/airflow/issues/41419,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('area:providers', ''), ('pending-response', ''), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2285617980, 'issue_id': 2462684494, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 13, 8, 6, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2295423563, 'issue_id': 2462684494, 'author': 'sunank200', 'body': '@abmramirez you might want to ensure that the arguments are properly escaped or quoted. Does the following work for you?\r\n```\r\ntask = KubernetesPodOperator(\r\n    task_id=\'example_task\',\r\n    namespace=\'default\',\r\n    image=\'your-image\',\r\n    cmds=[""/bin/bash"", ""-c""],\r\n    arguments=[\'--data\', \'{""data"": ""value with spaces""}\'],\r\n    name=\'example-pod\',\r\n    in_cluster=True,\r\n    is_delete_operator_pod=True,\r\n    get_logs=True,\r\n)\r\n```', 'created_at': datetime.datetime(2024, 8, 18, 22, 52, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2320577346, 'issue_id': 2462684494, 'author': 'abmramirez', 'body': '> @abmramirez you might want to ensure that the arguments are properly escaped or quoted. Does the following work for you?\r\n> \r\n> ```\r\n> task = KubernetesPodOperator(\r\n>     task_id=\'example_task\',\r\n>     namespace=\'default\',\r\n>     image=\'your-image\',\r\n>     cmds=[""/bin/bash"", ""-c""],\r\n>     arguments=[\'--data\', \'{""data"": ""value with spaces""}\'],\r\n>     name=\'example-pod\',\r\n>     in_cluster=True,\r\n>     is_delete_operator_pod=True,\r\n>     get_logs=True,\r\n> )\r\n> ```\r\nHi,\r\n\r\nThat is how we are sending the parameters and the error still occurs.', 'created_at': datetime.datetime(2024, 8, 30, 9, 6, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2350730810, 'issue_id': 2462684494, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 9, 14, 0, 14, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2364773277, 'issue_id': 2462684494, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 9, 21, 0, 14, 17, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-13 08:06:09 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

sunank200 on (2024-08-18 22:52:59 UTC): @abmramirez you might want to ensure that the arguments are properly escaped or quoted. Does the following work for you?
```
task = KubernetesPodOperator(
    task_id='example_task',
    namespace='default',
    image='your-image',
    cmds=[""/bin/bash"", ""-c""],
    arguments=['--data', '{""data"": ""value with spaces""}'],
    name='example-pod',
    in_cluster=True,
    is_delete_operator_pod=True,
    get_logs=True,
)
```

abmramirez (Issue Creator) on (2024-08-30 09:06:45 UTC): Hi,

That is how we are sending the parameters and the error still occurs.

github-actions[bot] on (2024-09-14 00:14:14 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-09-21 00:14:17 UTC): This issue has been closed because it has not received response from the issue author.

"
2461488551,issue,open,,"Airflow Variable Events (Create, Edit, Delete) Do Not Appear In The Audit Log If Triggered From A DAG File","### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

If an Airflow variable is created, updated, or deleted from a DAG file using Variable.set(), Variable.update(), or Variable.delete() methods, these events do not appear in the audit log. On the other hand, if a variable is created, updated, or deleted directly in the UI, these events appear in the audit logs. 

### What you think should happen instead?

Create, update, and delete events for Airflow variables should appear in audit logs even if they are triggered from a DAG file. After reviewing [this PR](https://github.com/apache/airflow/pull/24079/files), I think this can be fixed by adding the `@action_logging` decorator to the set(), update(), and delete() methods in airflow.models.Variable. I'm not sure if this is the correct solution or if other changes will also need to be made.

### How to reproduce

1. Create a DAG containing a task with one or all of these methods:
- Variable.set()
- Variable.update()
- Variable.delete()

2. Trigger a DAG run and let it complete.
3. Check the cluster audit logs for events relating to variable creation, updates, or deletion. These only appear if the changes are made directly on the UI.

### Operating System

MacOS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

Tested on Astro CLI

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",karenbraganz,2024-08-12 16:58:42+00:00,[],2024-09-15 11:42:45+00:00,,https://github.com/apache/airflow/issues/41409,"[('kind:bug', 'This is a clearly a bug'), ('area:core', '')]","[{'comment_id': 2284505102, 'issue_id': 2461488551, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 12, 16, 58, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2293392224, 'issue_id': 2461488551, 'author': 'pushpendu91', 'body': 'Even though we can add below code for example in the set method to add audit log for variable creation from DAG and it is working fine as shown in the screenshot, but without proper dag_id, owner etc. it does not makes sense. \r\n\r\n`session.add( Log( event=""variable.create"", dag_id="""", owner=""scheduler"", owner_display_name=""Scheduler"", extra=f""key={key}, val={stored_value}, description={description}"", ) )`\r\n\r\n![Screenshot 2024-08-16 at 12 46 27\u202fAM](https://github.com/user-attachments/assets/55bdac42-36e3-4f5f-9193-49c605705d21)\r\n\r\n\r\nAnd because airflow.models.variable does not have dag or dagrun context it is not possible without some core code changes to get the dag_id, task_id, run_id, owner etc. details to be logged. If community agrees we can add these basic details(event, extra, owner=‘scheduler’ etc.) for set, update and delete method for Variables.', 'created_at': datetime.datetime(2024, 8, 16, 12, 9, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2351552684, 'issue_id': 2461488551, 'author': 'nicolasge', 'body': ""Personally I believe it's a great feature we need to have, otherwise we don't know who changed the variables within the DAGs."", 'created_at': datetime.datetime(2024, 9, 15, 11, 42, 43, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-12 16:58:45 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

pushpendu91 on (2024-08-16 12:09:37 UTC): Even though we can add below code for example in the set method to add audit log for variable creation from DAG and it is working fine as shown in the screenshot, but without proper dag_id, owner etc. it does not makes sense. 

`session.add( Log( event=""variable.create"", dag_id="""", owner=""scheduler"", owner_display_name=""Scheduler"", extra=f""key={key}, val={stored_value}, description={description}"", ) )`

![Screenshot 2024-08-16 at 12 46 27 AM](https://github.com/user-attachments/assets/55bdac42-36e3-4f5f-9193-49c605705d21)


And because airflow.models.variable does not have dag or dagrun context it is not possible without some core code changes to get the dag_id, task_id, run_id, owner etc. details to be logged. If community agrees we can add these basic details(event, extra, owner=‘scheduler’ etc.) for set, update and delete method for Variables.

nicolasge on (2024-09-15 11:42:43 UTC): Personally I believe it's a great feature we need to have, otherwise we don't know who changed the variables within the DAGs.

"
2461209246,issue,closed,completed, Incorrect DAG Run ID opened when clicking on run_id link from DAG run list,"### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When I try to open an older DAG run by clicking on the run_id link from the DAG run list, it incorrectly opens the DAG run with a different run_id. This issue makes it difficult to view the details of specific older DAG runs.

### What you think should happen instead?

Clicking on a run_id link from the dag run list should open the DAG run corresponding to that run_id.

### How to reproduce

- Navigate to the DAG runs list.
- Select a specific DAG to view its run history.
- Click on the run_id link of an older DAG run in the list.
- Notice that the page loads a DAG run with an incorrect run_id.

### Operating System

Linux ad25902d8cef 6.7.12-orbstack-00201-g2ddb8f197a46 #1 SMP Tue May 21 04:38:26 UTC 2024 aarch64 GNU/Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

Start an Airflow instance using the AstroCLI command `astro dev start`, run an example dag multiple times, and try accessing dag_run older than 25 from the dag runs list page of the Airflow UI.

### Anything else?

Click on the older run_id link from the dag run list

<img width=""1394"" alt=""Screenshot 2024-08-12 at 8 14 16 PM"" src=""https://github.com/user-attachments/assets/bf2765d7-99a0-46d0-b2b9-eea8af72314e"">

It will open the graph view. Click on any task in the graph view, and it will show the details for the incorrect dag run_id.

<img width=""1428"" alt=""Screenshot 2024-08-12 at 8 14 27 PM"" src=""https://github.com/user-attachments/assets/73bf497f-a4ec-4e71-8d65-00f421bc437e"">


### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",hkc-8010,2024-08-12 14:47:04+00:00,[],2024-09-10 19:09:39+00:00,2024-09-10 19:09:39+00:00,https://github.com/apache/airflow/issues/41406,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('affected_version:2.10', 'Issues Reported for 2.10')]","[{'comment_id': 2284347352, 'issue_id': 2461209246, 'author': 'tirkarthi', 'body': 'Looks like a duplicate of https://github.com/apache/airflow/issues/40214', 'created_at': datetime.datetime(2024, 8, 12, 15, 55, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2284463845, 'issue_id': 2461209246, 'author': 'RNHTTR', 'body': '> Looks like a duplicate of #40214\r\n\r\nThat Issue appears to be solved in 2.9.3. @hkc-8010 can you please confirm that this is occurring on 2.9.3?', 'created_at': datetime.datetime(2024, 8, 12, 16, 35, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2284472889, 'issue_id': 2461209246, 'author': 'hkc-8010', 'body': 'It appears that the issue has been resolved for the dag_id link, which points to the Grid view. However, the run_id link, pointing to the Graph view, is still incorrect. I was able to replicate this issue on version 2.9.3.', 'created_at': datetime.datetime(2024, 8, 12, 16, 40, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2284595931, 'issue_id': 2461209246, 'author': 'tirkarthi', 'body': '@hkc-8010  Does run_id in the details tab match the expected value and the query string in the url? Some changes were done in https://github.com/apache/airflow/issues/35946', 'created_at': datetime.datetime(2024, 8, 12, 17, 50, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2285295399, 'issue_id': 2461209246, 'author': 'hkc-8010', 'body': '@tirkarthi - Here is hyperlink attached to the run_id in the Dag run list - [manual__2024-08-09T08:42:25.524227+00:00](http://localhost:8080/dags/example_astronauts/grid?dag_run_id=manual__2024-08-09T08%3A42%3A25.524227%2B00%3A00&tab=graph)\r\n\r\nThe hyperlink appears correct, but when it opens the Graph view and I click on a task, it displays an incorrect `run_id`—specifically, the `run_id` from the previous run instead of the desired one.\r\n\r\nIt seems the hyperlink is updating the calendar tab to the exact `run_id` value, which causes the Graph or Grid view to display only historical DAG runs, excluding the desired DAG run.\r\n \r\n<img width=""828"" alt=""image"" src=""https://github.com/user-attachments/assets/06c1be20-d612-4a96-8bf3-b93d4dddeae0"">', 'created_at': datetime.datetime(2024, 8, 13, 3, 59, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2285299738, 'issue_id': 2461209246, 'author': 'hkc-8010', 'body': ""I believe this [PR](https://github.com/apache/airflow/pull/40764) might resolve the issue. I'll test it on version 2.10.0 and will update you here. Thanks!"", 'created_at': datetime.datetime(2024, 8, 13, 4, 5, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2285435068, 'issue_id': 2461209246, 'author': 'hkc-8010', 'body': 'I just verified on 2.10. Still facing the same issue.', 'created_at': datetime.datetime(2024, 8, 13, 6, 28, 52, tzinfo=datetime.timezone.utc)}, {'comment_id': 2316316501, 'issue_id': 2461209246, 'author': 'karenbraganz', 'body': 'Hem and I tested this out in Airflow 2.10 (Astro runtime 12.0.0) and noticed that this bug still exists but only for certain types of runs (when the run_id timestamp is not at the 0th second of the minute).\r\n\r\n- When testing this out with runs scheduled to run every minute, I did not see this bug. I was able to go to Browse -> DAG Runs, find a DAG run older than the 25th record, click on the run ID, and open the correct DAG run page.\r\n\r\n- However, when testing this out with manual runs, the bug became apparent. For example, I triggered a manual run with `run_id=manual__2024-08-28T13:54:14.979062+00:00`. \r\n\r\n  - After over 25 runs, I tried opening this DAG run by clicking on the run_id in Browse-> DAG Runs. This opened the DAG without a specific run highlighted (see below).\r\n  \r\n  ![image](https://github.com/user-attachments/assets/161a4b5f-a802-4bee-854f-2abca8d6a1a9)\r\n\r\n  -  When I clicked on a task in the graph view, it highlighted the run that had occurred just before the run I had clicked on in the grid. The `run_id` for this was `scheduled__2024-08-28T13:54:00+00:00` as indicated in the screenshot below. The run I wanted to open was not part of the grid. Hem observed this as well in his tests.\r\n  \r\n  ![image](https://github.com/user-attachments/assets/efedf055-a33c-4d4b-b708-9ab03f29a447)\r\n\r\n  - In contrast, when I opened the DAG run for `run_id=scheduled__2024-08-28T13:54:00+00:00`, it opened and highlighted the correct DAG run. This might suggest that the bug only appears for manual runs, but this is not true as described in the next bullet point. \r\n\r\n- When I modified the DAG to have `schedule=@continuous`, the bug appeared once again for scheduled runs indicating that it is not exclusive to manual runs. For example, when I clicked on a run_id which was `scheduled__2024-08-28T16:47:22.632138+00:00`, I saw the same bug as before where it opened a page without any DAG run highlighted. When i clicked on one of the tasks in the graph, it highlighted the run previous to what I clicked on in the grid. The `run_id` for this was `scheduled__2024-08-28T16:47:20.415112+00:00`, and the run I was looking for was not visible in the grid. Hem observed this as well in his tests.\r\n\r\n- Based on these observations, I believe the bug appears only for DAGs with run ID timestamps that are not at the 0th second of the minute. Due to the bug, you are only able to see the run previous to the run you wanted to locate. It does not depend on whether the run was scheduled or manually triggered. For example, the bug appeared for `run_id=scheduled__2024-08-28T16:47:22.632138+00:00` but not for `run_id=scheduled__2024-08-28T13:54:00+00:00`.', 'created_at': datetime.datetime(2024, 8, 28, 21, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2341355310, 'issue_id': 2461209246, 'author': 'pierrejeambrun', 'body': 'Fix here:\r\nhttps://github.com/apache/airflow/pull/42138\r\n\r\nLet me know if this is the expected behavior. Marked for backporting so hopefully we can get that in the next patch release.', 'created_at': datetime.datetime(2024, 9, 10, 16, 1, 26, tzinfo=datetime.timezone.utc)}]","tirkarthi on (2024-08-12 15:55:27 UTC): Looks like a duplicate of https://github.com/apache/airflow/issues/40214

RNHTTR on (2024-08-12 16:35:23 UTC): That Issue appears to be solved in 2.9.3. @hkc-8010 can you please confirm that this is occurring on 2.9.3?

hkc-8010 (Issue Creator) on (2024-08-12 16:40:28 UTC): It appears that the issue has been resolved for the dag_id link, which points to the Grid view. However, the run_id link, pointing to the Graph view, is still incorrect. I was able to replicate this issue on version 2.9.3.

tirkarthi on (2024-08-12 17:50:22 UTC): @hkc-8010  Does run_id in the details tab match the expected value and the query string in the url? Some changes were done in https://github.com/apache/airflow/issues/35946

hkc-8010 (Issue Creator) on (2024-08-13 03:59:46 UTC): @tirkarthi - Here is hyperlink attached to the run_id in the Dag run list - [manual__2024-08-09T08:42:25.524227+00:00](http://localhost:8080/dags/example_astronauts/grid?dag_run_id=manual__2024-08-09T08%3A42%3A25.524227%2B00%3A00&tab=graph)

The hyperlink appears correct, but when it opens the Graph view and I click on a task, it displays an incorrect `run_id`—specifically, the `run_id` from the previous run instead of the desired one.

It seems the hyperlink is updating the calendar tab to the exact `run_id` value, which causes the Graph or Grid view to display only historical DAG runs, excluding the desired DAG run.
 
<img width=""828"" alt=""image"" src=""https://github.com/user-attachments/assets/06c1be20-d612-4a96-8bf3-b93d4dddeae0"">

hkc-8010 (Issue Creator) on (2024-08-13 04:05:21 UTC): I believe this [PR](https://github.com/apache/airflow/pull/40764) might resolve the issue. I'll test it on version 2.10.0 and will update you here. Thanks!

hkc-8010 (Issue Creator) on (2024-08-13 06:28:52 UTC): I just verified on 2.10. Still facing the same issue.

karenbraganz on (2024-08-28 21:57:00 UTC): Hem and I tested this out in Airflow 2.10 (Astro runtime 12.0.0) and noticed that this bug still exists but only for certain types of runs (when the run_id timestamp is not at the 0th second of the minute).

- When testing this out with runs scheduled to run every minute, I did not see this bug. I was able to go to Browse -> DAG Runs, find a DAG run older than the 25th record, click on the run ID, and open the correct DAG run page.

- However, when testing this out with manual runs, the bug became apparent. For example, I triggered a manual run with `run_id=manual__2024-08-28T13:54:14.979062+00:00`. 

  - After over 25 runs, I tried opening this DAG run by clicking on the run_id in Browse-> DAG Runs. This opened the DAG without a specific run highlighted (see below).
  
  ![image](https://github.com/user-attachments/assets/161a4b5f-a802-4bee-854f-2abca8d6a1a9)

  -  When I clicked on a task in the graph view, it highlighted the run that had occurred just before the run I had clicked on in the grid. The `run_id` for this was `scheduled__2024-08-28T13:54:00+00:00` as indicated in the screenshot below. The run I wanted to open was not part of the grid. Hem observed this as well in his tests.
  
  ![image](https://github.com/user-attachments/assets/efedf055-a33c-4d4b-b708-9ab03f29a447)

  - In contrast, when I opened the DAG run for `run_id=scheduled__2024-08-28T13:54:00+00:00`, it opened and highlighted the correct DAG run. This might suggest that the bug only appears for manual runs, but this is not true as described in the next bullet point. 

- When I modified the DAG to have `schedule=@continuous`, the bug appeared once again for scheduled runs indicating that it is not exclusive to manual runs. For example, when I clicked on a run_id which was `scheduled__2024-08-28T16:47:22.632138+00:00`, I saw the same bug as before where it opened a page without any DAG run highlighted. When i clicked on one of the tasks in the graph, it highlighted the run previous to what I clicked on in the grid. The `run_id` for this was `scheduled__2024-08-28T16:47:20.415112+00:00`, and the run I was looking for was not visible in the grid. Hem observed this as well in his tests.

- Based on these observations, I believe the bug appears only for DAGs with run ID timestamps that are not at the 0th second of the minute. Due to the bug, you are only able to see the run previous to the run you wanted to locate. It does not depend on whether the run was scheduled or manually triggered. For example, the bug appeared for `run_id=scheduled__2024-08-28T16:47:22.632138+00:00` but not for `run_id=scheduled__2024-08-28T13:54:00+00:00`.

pierrejeambrun on (2024-09-10 16:01:26 UTC): Fix here:
https://github.com/apache/airflow/pull/42138

Let me know if this is the expected behavior. Marked for backporting so hopefully we can get that in the next patch release.

"
2461116362,issue,open,,Task policy is not setting for retries,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.2

### What happened?

We moved our cluster from KubernetesExecutor to CeleryKubernetesExecutor. We have a lot of dags with `executor_config` parameter. So, we have situation where we have a lot of dags with `executor_config`, which need KubernetesExecutor because it uses custom image. And we have rest of dags without this parameter, so they should execute on the workers.

To achieve this, we create simple `cluster_policy` to task level
```
from airflow.models.baseoperator import BaseOperator
from airflow.policies import hookimpl


@hookimpl
def task_policy(task: BaseOperator):
    use_kubernetes_if_config(task=task)


def use_kubernetes_if_config(task: BaseOperator):
    if task.executor_config:
        task.queue = 'kubernetes'
```
but we have problem - this policy is not working for retries. First run of task is executing in Kubernetes, but retries are executing on the celery and are failing because of lack of libraries (we need custom image there)

### What you think should happen instead?

Retries of task should also execute in Kubernetes pod using kuberneted queue

### How to reproduce

Apply cluster policy from description and dag with task with `executor_config` with custom image

### Operating System

Debian GNU/Linux 12

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.19.0
apache-airflow-providers-celery==3.7.2
apache-airflow-providers-cncf-kubernetes==7.13.0
apache-airflow-providers-common-io==1.3.2
apache-airflow-providers-common-sql==1.14.0
apache-airflow-providers-docker==3.12.0
apache-airflow-providers-elasticsearch==5.4.1
apache-airflow-providers-fab==1.1.1
apache-airflow-providers-ftp==3.9.1
apache-airflow-providers-google==10.19.0
apache-airflow-providers-grpc==3.5.1
apache-airflow-providers-hashicorp==3.7.1
apache-airflow-providers-http==4.11.1
apache-airflow-providers-imap==3.6.1
apache-airflow-providers-microsoft-azure==10.1.1
apache-airflow-providers-mysql==5.6.1
apache-airflow-providers-odbc==4.6.1
apache-airflow-providers-openlineage==1.8.0
apache-airflow-providers-pagerduty==3.7.2
apache-airflow-providers-postgres==5.11.1
apache-airflow-providers-redis==3.7.1
apache-airflow-providers-sendgrid==3.5.1
apache-airflow-providers-sftp==4.10.1
apache-airflow-providers-slack==8.7.1
apache-airflow-providers-smtp==1.7.1
apache-airflow-providers-snowflake==5.5.1
apache-airflow-providers-sqlite==3.8.1
apache-airflow-providers-ssh==3.11.1
apache-airflow-providers-telegram==3.1.1
apache-airflow-providers-trino==5.7.1

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Executor: CeleryKubernetesExecutor
k8s_version = ""1.27""

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",ruffury,2024-08-12 14:09:58+00:00,['gopidesupavan'],2024-09-30 22:48:28+00:00,,https://github.com/apache/airflow/issues/41405,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]","[{'comment_id': 2284094288, 'issue_id': 2461116362, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 12, 14, 10, 3, tzinfo=datetime.timezone.utc)}, {'comment_id': 2308526488, 'issue_id': 2461116362, 'author': 'gopidesupavan', 'body': '@RNHTTR going to look at this, bit takes time first time to kubernetes area in airflow, ill try crack it :).', 'created_at': datetime.datetime(2024, 8, 24, 20, 19, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2322024379, 'issue_id': 2461116362, 'author': 'RNHTTR', 'body': ""> @RNHTTR going to look at this, bit takes time first time to kubernetes area in airflow, ill try crack it :).\r\n\r\nAwesome! I've assigned you. I don't think this actually needs to be running on a Kubernetes cluster to reproduce. You could probably reproduce using a very simple [cluster policy](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/cluster-policies.html#how-do-define-a-policy-function)."", 'created_at': datetime.datetime(2024, 8, 30, 17, 25, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2384309509, 'issue_id': 2461116362, 'author': 'gopidesupavan', 'body': 'Thanks @RNHTTR for assigning :)  i will have look in this week.', 'created_at': datetime.datetime(2024, 9, 30, 22, 48, 27, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-12 14:10:03 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

gopidesupavan (Assginee) on (2024-08-24 20:19:59 UTC): @RNHTTR going to look at this, bit takes time first time to kubernetes area in airflow, ill try crack it :).

RNHTTR on (2024-08-30 17:25:15 UTC): Awesome! I've assigned you. I don't think this actually needs to be running on a Kubernetes cluster to reproduce. You could probably reproduce using a very simple [cluster policy](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/cluster-policies.html#how-do-define-a-policy-function).

gopidesupavan (Assginee) on (2024-09-30 22:48:27 UTC): Thanks @RNHTTR for assigning :)  i will have look in this week.

"
2460422633,issue,closed,not_planned,Microsoft-Azure - ms graph hook - Tenant ID fetched from wrong extra_dejson key.,"### Apache Airflow Provider(s)

microsoft-azure

### Versions of Apache Airflow Providers

```
apache-airflow==2.9.1
apache-airflow-providers-microsoft-azure==10.1.1
```

### Apache Airflow version

2.9.1

### Operating System

Ubuntu 22.04.4 LTS

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

The general [Azure connection](https://airflow.apache.org/docs/apache-airflow-providers-microsoft-azure/stable/connections/azure.html) has parameter `tenantId`.

However, the [MS Graph operators](https://airflow.apache.org/docs/apache-airflow-providers-microsoft-azure/stable/operators/msgraph.html) look for the key `tenant_id` ([code](https://github.com/apache/airflow/blob/main/airflow/providers/microsoft/azure/hooks/msgraph.py#L193)).

### What happened

```
{msgraph.py:214} INFO - Tenant id: None
(....)
ERROR - tenant_id should be an Azure Active Directory tenant's id (also called its 'directory id')
```

### What you think should happen instead

The tenant_id should be read from `extra_dejson.tenantId` instead of `extra_dejson.tenant_id`. We can keep both to maintain backwards compatibility. 

### How to reproduce

- Create a Microsoft Azure connection with client_id, client_secret and tenant id.
- Try to use the Microsoft Graph API Operator 

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",w0ut0,2024-08-12 09:01:40+00:00,['w0ut0'],2024-10-23 06:25:49+00:00,2024-10-23 06:25:49+00:00,https://github.com/apache/airflow/issues/41399,"[('kind:bug', 'This is a clearly a bug'), ('provider:microsoft-azure', 'Azure-related issues'), ('area:providers', '')]","[{'comment_id': 2431023933, 'issue_id': 2460422633, 'author': 'eladkal', 'body': 'Fixed in https://github.com/apache/airflow/pull/41331', 'created_at': datetime.datetime(2024, 10, 23, 6, 25, 49, tzinfo=datetime.timezone.utc)}]","eladkal on (2024-10-23 06:25:49 UTC): Fixed in https://github.com/apache/airflow/pull/41331

"
2459779419,issue,open,,Setup Github Runners via Kubernetes Controller on our AWS instance,"### Body

We should set-up the https://github.com/actions/actions-runner-controller on Airflow AWS instance - so that it can speed up our builds and allow for ARM builds to be prepared.

Currently we only utillse public runners and (for docs publishing only) - self-hosted runners of the ASF infrastructure (but there are far to small number of those to rely on anything else.

We need to implement the runners - especially that we have a lot of credits from AWS now.

The work is mostly done by @hussein-awala  and he will be able to share the status with anyone who picks it up.

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",potiuk,2024-08-11 21:59:57+00:00,[],2024-08-11 22:20:05+00:00,,https://github.com/apache/airflow/issues/41388,"[('kind:meta', 'High-level information important to the community'), ('area:self-hosted-runners', '')]",[],
2459454796,issue,open,,Bug Report: Issues with Task Group Dependencies and Dynamic Task Outputs in Airflow,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.6.3

### What happened?

Environment: Ubuntu 20.1 , Python 3.10.14

**Description:**

I am experiencing issues with task group dependencies and dynamic task outputs when using the .expand method in Airflow. Below is the DAG code snippet that illustrates the problem:

```
from airflow.decorators import dag, task_group, task
from pendulum import datetime
from airflow.operators.empty import EmptyOperator

from constants.common_constants import TASK_BEGIN


@task_group(group_id=""Fetch_and_Process_Data"", tooltip=""This task group is very important!"")
def demo_tash_group(my_num):
    @task
    def fetch_data(num):
        print(num)

    @task
    def process_data(num):
        print(num)

    @task
    def copy_s3_to_ticket_staging(num):
        print(num)

    @task
    def copy_s3_to_transaction_staging(num):
        print(num)
 
    @task
    def copy_s3_to_payment_staging(num):
        print(num)
    
    @task
    def copy_s3_into_main_tables(num):
        print(num)

    # Setting dependencies
    fetch_data(my_num) >> process_data(my_num) >> [copy_s3_to_ticket_staging(my_num), copy_s3_to_transaction_staging(my_num), copy_s3_to_payment_staging(my_num)] >> copy_s3_into_main_tables(my_num)


@dag(
    start_date=datetime(2022, 12, 1),
    schedule=None,
    catchup=False,
    tags=[""task""]
)
def task_group_mapping():
    begin = EmptyOperator(task_id=TASK_BEGIN)

    @task()
    def get_config_data():
        # It will be a list of dictionaries, fetched at the time of DAG execution from my db
        return [19, 23, 42, 8, 7, 108]
    
    task_group = demo_tash_group.expand(my_num=get_config_data())

    end = EmptyOperator(task_id=""end"")

    # Setting dependencies
    begin >> task_group >> end


task_group_mapping()
```

### Problem Scenarios:

**Scenario 1:**

Issue: When I define get_config_data as a normal function (without the @task decorator), my DAG is created correctly with the task chain as defined. However, this approach isn't viable since it will execute every time the scheduler parses the file, which degrades performance, especially because my case involves fetching data from a database.
this is how the dag looks like
![normalfunction](https://github.com/user-attachments/assets/7efee703-d5fd-4889-954c-0b9a5da55dd6)


**Scenario 2:**

Issue: To address the performance issue, I added the @task decorator to get_config_data. This solved the first problem by ensuring the task only executes when the DAG runs. However, it introduced a new issue: the tasks using the output of get_config_data are getting attached to it in the DAG chain, whereas I want them to be attached only to the first task of the task group.
![fucnntionastask](https://github.com/user-attachments/assets/469c2d55-99d3-48a4-88d0-69389aae02f8)


**Scenario 3:**
Attempted Solution: I tried using the output of get_config_data in the first task of demo_tash_group and then pushing my_num into XCom, expecting the remaining tasks to fetch from XCom. However, this didn’t work because when fetching the XCom value, it retrieves a list of all XComs for every dynamic task. This makes it impossible to fetch the XCom value for a specific task group.








### What you think should happen instead?

Expected Behavior:
I expect the tasks within the task group to maintain the intended dependencies and not be directly attached to get_config_data, while still allowing the use of dynamic task outputs.

### How to reproduce

To reproduce the issue, follow these steps:

Setup Airflow Environment:

Ensure you have a working Airflow environment. The issue has been observed in [your Airflow version]. Make sure your setup matches or is compatible with the version where the bug was encountered.
Create a New DAG:

Create a new DAG file in your Airflow DAGs folder using the provided code snippet. Ensure you have the necessary imports and helper files (constants.common_constants, helper.base_helper) or adjust the code to remove those dependencies if they're specific to your environment.
Scenario 1 - Define get_config_data as a Normal Function:

In the DAG file, define get_config_data as a normal function without the @task decorator.
Load the DAG in the Airflow UI. You should see that the task chain is created as expected, with the task group dependencies preserved.
Note: While this approach works, it's not ideal because the function executes every time the scheduler parses the file, leading to potential performance degradation.
Scenario 2 - Use the @task Decorator for get_config_data:

Now, add the @task decorator to get_config_data in the DAG file to ensure it only runs when the DAG is triggered.
Load the DAG in the Airflow UI again.
Observe that the tasks using the output of get_config_data are now directly attached to it in the DAG chain, breaking the intended task group dependencies.
Scenario 3 - Attempt to Use XCom:

To try and solve the issue from Scenario 2, modify the DAG so that the output of get_config_data is used in the first task of demo_tash_group.
Push the value of my_num into XCom within the first task and attempt to retrieve it in the subsequent tasks.
Observe that when fetching the XCom value, you receive a list of all XComs for every dynamic task, rather than the specific XCom value for the intended task group.
Document the Observations:

Take snapshots of the DAG structures in the Airflow UI for both Scenario 1 and Scenario 2 to illustrate the difference in task dependencies and the issues encountered.
By following these steps, you should be able to reproduce the bug and observe the issues with task group dependencies and dynamic task outputs when using the .expand method in Airflow.

### Operating System

Ubuntu

### Versions of Apache Airflow Providers

_No response_

### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Aqib5wani,2024-08-11 06:53:56+00:00,[],2024-09-27 08:42:29+00:00,,https://github.com/apache/airflow/issues/41378,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:dynamic-task-mapping', 'AIP-42'), ('area:TaskGroup', '')]","[{'comment_id': 2282647362, 'issue_id': 2459454796, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 11, 6, 54, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-11 06:54:00 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2459085762,issue,open,,Attribute error: 'MsSqlHook' object has no attribute 'get_conn_id' due to breaking changes made in a new release 3.8.0 of apache-airflow-providers-microsoft-mssql library,"### Apache Airflow Provider(s)

microsoft-mssql

### Versions of Apache Airflow Providers

apache-airflow-providers-microsoft-mssql - 3.8.0

### Apache Airflow version

2.7.3

### Operating System

linux

### Deployment

Astronomer

### Deployment details

_No response_

### What happened

We are getting attribute error 'MsSqlHook' object has no attribute 'get_conn_id' due to breaking changes made in a new release 3.8.0 of apache-airflow-providers-microsoft-mssql library

### What you think should happen instead

_No response_

### How to reproduce

update apache-airflow-providers-microsoft-mssql library to 3.8.0

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Ishwin9,2024-08-10 13:30:59+00:00,[],2024-11-11 06:07:25+00:00,,https://github.com/apache/airflow/issues/41373,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('provider:common-sql', ''), ('provider:microsoft-mssql', '')]","[{'comment_id': 2281782872, 'issue_id': 2459085762, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 10, 13, 31, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2281795115, 'issue_id': 2459085762, 'author': 'Ishwin9', 'body': '![image](https://github.com/user-attachments/assets/574bcf97-badd-4e2d-9d42-ee69d1e504a2)', 'created_at': datetime.datetime(2024, 8, 10, 13, 33, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2282194691, 'issue_id': 2459085762, 'author': 'romsharon98', 'body': 'It looks like it been solved with this PR: https://github.com/apache/airflow/pull/40615', 'created_at': datetime.datetime(2024, 8, 10, 16, 2, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2282201809, 'issue_id': 2459085762, 'author': 'Ishwin9', 'body': 'I am not sure if this is same. I had to downgrade the provider to 3.7.2 version and it was successful after that. This seems like a new bug after the new release of this provider', 'created_at': datetime.datetime(2024, 8, 10, 16, 29, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2467313951, 'issue_id': 2459085762, 'author': 'xy-qimu', 'body': 'Has this problem been solved?', 'created_at': datetime.datetime(2024, 11, 11, 6, 7, 24, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-10 13:31:02 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

Ishwin9 (Issue Creator) on (2024-08-10 13:33:25 UTC): ![image](https://github.com/user-attachments/assets/574bcf97-badd-4e2d-9d42-ee69d1e504a2)

romsharon98 on (2024-08-10 16:02:07 UTC): It looks like it been solved with this PR: https://github.com/apache/airflow/pull/40615

Ishwin9 (Issue Creator) on (2024-08-10 16:29:41 UTC): I am not sure if this is same. I had to downgrade the provider to 3.7.2 version and it was successful after that. This seems like a new bug after the new release of this provider

xy-qimu on (2024-11-11 06:07:24 UTC): Has this problem been solved?

"
2458531057,issue,closed,completed,[Providers] Airbyte new deployment version changed API endpoints,"### Apache Airflow Provider(s)

airbyte

### Versions of Apache Airflow Providers

apache-airflow-providers-airbyte==3.5.1

### Apache Airflow version

2.8.1

### Operating System

MWAA/AL

### Deployment

Amazon (AWS) MWAA

### Deployment details

Standard MWAA deployment. Nothing really specific or particular

### What happened

After updating to one of the latest Airbyte versions (and using acbtl installation method, as they have as standard on their documentation) the Airbyte provider from Airflow cannot connect to the api server. The reason is that the endpoints have changed, according to Airbyte's documentation:

https://github.com/airbytehq/airbyte/issues/43423

https://reference.airbyte.com/reference/standalone-server-deprecation-and-migration-to-airbyte-server

### What you think should happen instead

Provider should be updated so that it points to the correct endpoint of the new api for Airbyte server

### How to reproduce

Deploy MWAA. Perform a fresh install of Airbyte using current documentation and standards for OSS installation (abctl).
Use Airflow's Airbyte provider to do anything in the Airbyte server and you will get a connection refused error, as it is pointing to the wrong endpoint.

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",baugarcia,2024-08-09 19:20:17+00:00,['marcosmarxm'],2024-08-28 18:41:52+00:00,2024-08-28 18:41:51+00:00,https://github.com/apache/airflow/issues/41365,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('provider:airbyte', '')]","[{'comment_id': 2278596186, 'issue_id': 2458531057, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 9, 19, 20, 19, tzinfo=datetime.timezone.utc)}, {'comment_id': 2280163576, 'issue_id': 2458531057, 'author': 'Lee-W', 'body': '@josix I think you might be interested in this one 🙂', 'created_at': datetime.datetime(2024, 8, 10, 8, 1, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2280354521, 'issue_id': 2458531057, 'author': 'josix', 'body': 'Yeah, let me check it', 'created_at': datetime.datetime(2024, 8, 10, 8, 39, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2282250537, 'issue_id': 2458531057, 'author': 'marcosmarxm', 'body': 'https://github.com/apache/airflow/pull/41122 implement the new Auth API method.', 'created_at': datetime.datetime(2024, 8, 10, 19, 13, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2282359152, 'issue_id': 2458531057, 'author': 'Lee-W', 'body': 'Thanks for letting us know! Let me reassign it to you', 'created_at': datetime.datetime(2024, 8, 11, 2, 34, 17, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-09 19:20:19 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

Lee-W on (2024-08-10 08:01:51 UTC): @josix I think you might be interested in this one 🙂

josix on (2024-08-10 08:39:25 UTC): Yeah, let me check it

marcosmarxm (Assginee) on (2024-08-10 19:13:59 UTC): https://github.com/apache/airflow/pull/41122 implement the new Auth API method.

Lee-W on (2024-08-11 02:34:17 UTC): Thanks for letting us know! Let me reassign it to you

"
2458153673,issue,open,,Race condition Airflow's Celery executor timeout and import redis leave a broken import,"### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Hello, 

This is a copy-paste of an issue we also post on celery/kombu and redis/redis-py. We include a MCVE example at the end of this post.

TL;DR: The `with timeout(seconds=OPERATION_TIMEOUT):` in `airflow.executors.celery_executor.send_task_to_executor` might leave a very broken import of redis & may affect another package that we haven't discovered yet. This is a race condition and very hard to debug at first. To reproduce this bug, we have to

- have a celery timeout (we kept it at default 1.0 second)
- the timeout should happen during an import
- a long import (we aren't sure the import of redis is long and this bug happens mostly with redis package, maybe this is a confirmation bias)

Relates:

- [**apache/airflow** discussion #36097 _CeleryExecutor is failing to launch tasks with redis error_](https://github.com/apache/airflow/discussions/36097)
- [**apache/airflow** issue #33744 _Celery Executor is not working with redis-py 5.0.0_](https://github.com/apache/airflow/issues/33744)
- [**celery/kombu** issue #1815 _import exception raised in transport/redis ""module 'redis' has no attribute 'client' ""_](https://github.com/celery/kombu/issues/1815)

Our environment:

```
airflow: this happens with both 2.6 and latest 2.9.3 version
helm chart: 1.9, 1.14 or 1.15
python: 3.11.9
redis: 4.6.0 (airflow 2.6) and 5.0.7 (airflow 2.9)
kombu: 5.3.1 (airflow 2.6) and 5.3.7 (airflow 2.9)
```

We 've observed this issue since at least several months ago with our airflow deployment using official helm chart, we have the same issue as in related issues/discussion:

```
Aug 8 08:29:02 airflow-XXX-scheduler-XXX-zhtb8 scheduler ERROR {timeout.py:68} ERROR - Process timed out, PID: 7
Aug 8 08:29:02 airflow-XXX-scheduler-XXX-zhtb8 scheduler {celery_executor.py:279} INFO - [Try 1 of 3] Task Timeout Error for Task: (TaskInstanceKey(dag_id='XXX', task_id='XXX', run_id='manual__2024-06-19T14:06:39+00:00', try_number=51, map_index=-1)).
Aug 8 08:29:03 airflow-XXX-scheduler-XXX-zhtb8 scheduler {celery_executor.py:290} ERROR - Error sending Celery task: module 'redis' has no attribute 'client'
Aug 8 08:29:03 airflow-XXX-scheduler-XXX-zhtb8 scheduler Celery Task ID: TaskInstanceKey(dag_id='XXX', task_id='XXX', run_id='manual__2024-06-19T14:06:39+00:00', try_number=51, map_index=-1)
Aug 8 08:29:03 airflow-XXX-scheduler-XXX-zhtb8 scheduler Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/celery/executors/celery_executor_utils.py"", line 220, in send_task_to_executor
    result = task_to_run.apply_async(args=[command], queue=queue)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/celery/app/task.py"", line 594, in apply_async
    return app.send_task(
           ^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/celery/app/base.py"", line 797, in send_task
    with self.producer_or_acquire(producer) as P:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/celery/app/base.py"", line 932, in producer_or_acquire
    producer, self.producer_pool.acquire, block=True,
              ^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/celery/app/base.py"", line 1354, in producer_pool
    return self.amqp.producer_pool
           ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/celery/app/amqp.py"", line 591, in producer_pool
    self.app.connection_for_write()]
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/celery/app/base.py"", line 829, in connection_for_write
    return self._connection(url or self.conf.broker_write_url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/celery/app/base.py"", line 880, in _connection
    return self.amqp.Connection(
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/kombu/connection.py"", line 201, in __init__
    if not get_transport_cls(transport).can_parse_url:
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/kombu/transport/__init__.py"", line 90, in get_transport_cls
    _transport_cache[transport] = resolve_transport(transport)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/kombu/transport/__init__.py"", line 75, in resolve_transport
    return symbol_by_name(transport)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/airflow/.local/lib/python3.11/site-packages/kombu/utils/imports.py"", line 59, in symbol_by_name
    module = imp(module_name, package=package, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""<frozen importlib._XXX>"", line 1204, in _gcd_import
  File ""<frozen importlib._XXX>"", line 1176, in _find_and_load
  File ""<frozen importlib._XXX>"", line 1147, in _find_and_load_unlocked
  File ""<frozen importlib._XXX>"", line 690, in _load_unlocked
  File ""<frozen importlib._XXX_external>"", line 940, in exec_module
  File ""<frozen importlib._XXX>"", line 241, in _call_with_frames_removed
  File ""/home/airflow/.local/lib/python3.11/site-packages/kombu/transport/redis.py"", line 285, in <module>
    class PrefixedRedisPipeline(GlobalKeyPrefixMixin, redis.client.Pipeline):
                                                      ^^^^^^^^^^^^
Aug 8 08:29:03 airflow-XXX-scheduler-XXX-zhtb8 scheduler AttributeError: module 'redis' has no attribute 'client'
```

We've verified and we have neither redis folder nor `redis.py` file from our dev, this is a very sporadic error where most of the time it works, then it stops working for unknown reason, and once if happens, the scheduler is broken and couldn't schedule anything (same error message) until we restart the scheduler process (restart the pod)

This happens quite randomly (one in tens or fifty deployments of helm chart), and we couldn't reproduce it for sure for debugging purpose.

What we found out is that if this happens, this bug won't disappear until we restart (kill) the scheduler pod.
We could reproduce randomly with these steps in a test airflow:

- `kubectl delete po -l release=airflow-XXX,component=scheduler --force --grace-period 0`
- Clear a DAG task and hope that the bug happens, this should be immediate, if not, repeat the whole process

At first, we suspect that this is a case of race condition in importing redis package, because we inject debug code before the line `class PrefixedRedisPipeline(GlobalKeyPrefixMixin, redis.client.Pipeline):` with `print(sys.path)`, `print(redis)`, `print(redis.__version__)`, ... and everything is okay, except `print(dir(redis))` gives a different result:

```
sys.path=['/home/airflow/.local/bin', '/usr/local/lib/python311.zip', '/usr/local/lib/python3.11', '/usr/local/lib/python3.11/lib-dynload', '/home/airflow/.local/lib/python3.11/site-packages', '/opt/airflow/dags/repo/', '/opt/airflow/config', '/opt/airflow/plugins']

redis=<module 'redis' from '/home/airflow/.local/lib/python3.11/site-packages/redis/__init__.py'>

redis.__version__=5.0.7

dir(redis)=['AuthenticationError', 'AuthenticationWrongNumberOfArgsError', 'BlockingConnectionPool', 'BusyLoadingError', 'ChildDeadlockedError', 'Connection', 'ConnectionError', 'ConnectionPool', 'CredentialProvider', 'DataError', 'InvalidResponse', 'OutOfMemoryError', 'PubSubError', 'ReadOnlyError', 'Redis', 'RedisCluster', 'RedisError', 'ResponseError', 'SSLConnection', 'Sentinel', 'SentinelConnectionPool', 'SentinelManagedConnection', 'SentinelManagedSSLConnection', 'StrictRedis', 'TimeoutError', 'UnixDomainSocketConnection', 'UsernamePasswordCredentialProvider', 'VERSION', 'WatchError', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'asyncio', 'cluster', 'default_backoff', 'from_url', 'int_or_str', 'metadata', 'sentinel', 'sys']
```

compared to a python shell session inside the same container:

```
Python 3.11.9 (main, Jul 23 2024, 07:22:56) [GCC 12.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import redis
>>> redis
<module 'redis' from '/home/airflow/.local/lib/python3.11/site-packages/redis/__init__.py'>
>>> redis.__version__
'5.0.7'
>>> dir(redis)
['AuthenticationError', 'AuthenticationWrongNumberOfArgsError', 'BlockingConnectionPool', 'BusyLoadingError', 'ChildDeadlockedError', 'Connection', 'ConnectionError', 'ConnectionPool', 'CredentialProvider', 'DataError', 'InvalidResponse', 'OutOfMemoryError', 'PubSubError', 'ReadOnlyError', 'Redis', 'RedisCluster', 'RedisError', 'ResponseError', 'SSLConnection', 'Sentinel', 'SentinelConnectionPool', 'SentinelManagedConnection', 'SentinelManagedSSLConnection', 'StrictRedis', 'TimeoutError', 'UnixDomainSocketConnection', 'UsernamePasswordCredentialProvider', 'VERSION', 'WatchError', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_parsers', 'asyncio', 'backoff', 'client', 'cluster', 'commands', 'compat', 'connection', 'crc', 'credentials', 'default_backoff', 'exceptions', 'from_url', 'int_or_str', 'lock', 'metadata', 'retry', 'sentinel', 'sys', 'typing', 'utils']
```

We noted that `dir(redis)` inside the troublesome scheduler lacks several attributes, notably `redis.client`

Another thing we discovered is that in every case, there is always a Timeout (as you could see the log above), and sure enough, we found out later that the bug always happens while the process of importing `redis` is interrupted by Timeout (we print line number in `redis/__init__.py` and the importing didn't run till the end). In very rare case, `airflow.utils.timeout` doesn't work as inteded, the timeout error is printed out in the middle of `import redis` but the `import redis` still run till the end, in this case, the bug couldn't happens. But most of the time, the timeout interrupt the import.

With this idea, we injected a `sleep` at the end of `redis/__init__.py` and sure enough, we could reproduce this bug every time.

So an interrupted import give a different import than a normal `import`, it seems that the broken import doesn't import not-public member in package, such as redis.client in this case, Redis, StrictRedis are exposed explicitly but redis.client is set ""impliciteley""

```python
# redis/__init__.py
from redis.client import Redis, StrictRedis
```

I've found one comment from discuss.python.org:

> ```python
> import asyncio
> original = id(asyncio)
> from asyncio.base_events import BaseEventLoop
> assert id(asyncio) == original
> assert asyncio.base_events.BaseEventLoop is BaseEventLoop
> ```
>
> From which it should be clear that asyncio.base_events is indeed guaranteed to be set after the from-import.
>
> -- <cite>[discuss.python.org](https://discuss.python.org/t/why-do-relative-from-imports-also-add-the-submodule-itself-to-the-namespace/24440/2)</cite>

In our case, if we try to reimport an interrupted import, this isn't true anymore, the submodule isn't set at all. We didn't dig further in internal python to find out why this happens.

We see at least four options to fix this bug:

- Increase celery's operation_timeout (by config or env var). This isn't error-proof but at least reduce drastically the number of this bug
- inject `from redis import client` to `redis/__init__.py`
- patch `kombu/transport/redis.py` with
  ```python
  from redis import client
  ```
  and replace every `redis.client` by `client`

We opt for the second method in our dev at the moment

We aren't sure that this bug happens enough to be taken into consideration in upstream? But at least other dev won't loose days of debugging session as us ^^

This raise another question: Could the Timeout or another mechanism break the import and introduce this bug in another package or another hard-to-catch race condition bug?

### What you think should happen instead?

The airflow timeout should not leave a broken import. We are not sure if this should be addressed to redis/redis-py though, we post the same issue in celery/kombu and redis/redis-py.

### How to reproduce

This is a race condition which is hard to duplicate in local machine, so what we did is to introduce a delay to the very first import of redis to reproduce this bug for sure. 

Please find below a compressed file of three file:
- mock_kombu_transport_redis.py
- mock_airflow.py
- and redis.patch/__init__.py, we need to add the content of this file at the end of `redis-py` package's ` __init__.py` to add the delay in import
[mcve.zip](https://github.com/user-attachments/files/16563453/mcve.zip)

`python -m mock_airflow` gives the result:

```
FunctionTimedOut

After failed import redis
'redis' in sys.modules: True
'redis.client' in sys.modules: True

Reimport mock_kombu_transport_redis
After reimport redis
'redis' in sys.modules: True
'redis.client' in sys.modules: True
'client' in dir(redis): False
getattr(redis, 'client', None)=None

After reimport redis.client
'client' in dir(redis): False
getattr(redis, 'client', None)=None

After reimport: from redis import client
client=<module 'redis.client' from '***/site-packages/redis/client.py'>
'client' in dir(redis): False
getattr(redis, 'client', None)=None
client.Pipeline=<class 'redis.client.Pipeline'>
```

The important line is `'client' in dir(redis): False` event after a reimport of failed import and if we uncomment the line of ` print(redis.client)`, it will raise an error:
```
AttributeError: module 'redis' has no attribute 'client'
```

If we inject `from redis import client` to `redis/__init__.py`, the mcve gives the diffent output:

```
...
After reimport redis
'redis' in sys.modules: True
'redis.client' in sys.modules: True
'client' in dir(redis): True
getattr(redis, 'client', None)=<module 'redis.client' from 'XXX/site-packages/redis/client.py'>
...
```
(Changes: `client' in dir(redis): True` and `getattr(redis, 'client', None)=`)

### Operating System

Debian GNU/Linux 12 (bookworm)

### Versions of Apache Airflow Providers

apache-airflow-providers-celery==3.7.2
apache-airflow-providers-redis==3.7.1

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Helm: 3.11.3
Helm chart: apache/airflow:1.15.0
k8s: 1.28

### Anything else?

A race condition which happen very rarely at the start of scheduler service, but as we deploy to dev/staging alot each day, this happens several time a day. Once it happens, it won't go away until restart of this scheduler service. Condition:
- have a celery timeout (we kept it at default 1.0 second)
- the timeout should happen during an import
- a long import (we aren't sure the import of redis is long and this bug happens mostly with redis package, maybe this is a confirmation bias)

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",quangvnm,2024-08-09 15:17:34+00:00,['quangvnm'],2024-11-12 11:41:29+00:00,,https://github.com/apache/airflow/issues/41359,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('area:core', ''), ('provider:redis', ''), ('provider:celery', '')]","[{'comment_id': 2278188059, 'issue_id': 2458153673, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 9, 15, 17, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2278245629, 'issue_id': 2458153673, 'author': 'quangvnm', 'body': 'Crossed-post:\r\nredis/redis-py#3353\r\ncelery/kombu#2096\r\n\r\nAnd a very helpful response from [discuss.python.org](https://discuss.python.org/t/60422/2)', 'created_at': datetime.datetime(2024, 8, 9, 15, 47, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2333820954, 'issue_id': 2458153673, 'author': 'DartVeDroid', 'body': ""Happens to us for some time, too. The same AttributeError, the same inability to work until you restart the pod.\r\n\r\nThat's quite a coincidence: there was an another bug in kombu itself that affected redis deployments (https://github.com/celery/kombu/pull/2007). IIRC, it affected a couple Airflow versions rendering them unstable.\r\n\r\nNow, IMO, the usability problem is the same in both cases: container doesn't even crash, thus k8s can't auto-solve it by restarting the pod."", 'created_at': datetime.datetime(2024, 9, 6, 11, 8, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2356558671, 'issue_id': 2458153673, 'author': 'alimoezzi', 'body': 'I also have experience this in k8s deployment. Would only be fix by manual intervention.', 'created_at': datetime.datetime(2024, 9, 17, 17, 52, 37, tzinfo=datetime.timezone.utc)}, {'comment_id': 2461808867, 'issue_id': 2458153673, 'author': 'eladkal', 'body': 'According to https://github.com/celery/kombu/pull/2007#issuecomment-2248133561\r\nThe issue is fixed in [Celery v5.5.0b1](https://github.com/celery/celery/releases/tag/v5.5.0b1) so we might want to bump min version for Celery to avoid this issue for everyone.\r\n\r\nCan anyone confirm if upgrading to Celery 5.5.0 solves the problem? (Celery 5.5.0 not released yet, but rc1 is averrable so if anyone can confirm this solves the issue by checking it in test enviroment that would be great)', 'created_at': datetime.datetime(2024, 11, 7, 10, 3, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2470298389, 'issue_id': 2458153673, 'author': 'DartVeDroid', 'body': ""If I'm not mistaken, I did try it when kombu 5.4.0 was released, and it didn't help.\r\nAs far as I can tell, this issue is related to Airflow's celery provider only: when I upped the timeout value from 1 to 300, it stopped occurring.\r\n\r\nAnyway, it's rather strange to me that this value is so small by default: scheduler container does not only schedule tasks, it also runs DAG analyzer, and in a real environment with cpu limit, or with badly written DAGs, analysis could easily take more than 1s, making send/fetch operations unsuccessful every now and then.\r\n\r\nUPD:\r\nPlease don't blindly believe me, though. We run custom build/deploy configuration, so it's very much possible that it's our local problem in this instance."", 'created_at': datetime.datetime(2024, 11, 12, 11, 35, 38, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-09 15:17:37 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

quangvnm (Issue Creator) on (2024-08-09 15:47:04 UTC): Crossed-post:
redis/redis-py#3353
celery/kombu#2096

And a very helpful response from [discuss.python.org](https://discuss.python.org/t/60422/2)

DartVeDroid on (2024-09-06 11:08:40 UTC): Happens to us for some time, too. The same AttributeError, the same inability to work until you restart the pod.

That's quite a coincidence: there was an another bug in kombu itself that affected redis deployments (https://github.com/celery/kombu/pull/2007). IIRC, it affected a couple Airflow versions rendering them unstable.

Now, IMO, the usability problem is the same in both cases: container doesn't even crash, thus k8s can't auto-solve it by restarting the pod.

alimoezzi on (2024-09-17 17:52:37 UTC): I also have experience this in k8s deployment. Would only be fix by manual intervention.

eladkal on (2024-11-07 10:03:05 UTC): According to https://github.com/celery/kombu/pull/2007#issuecomment-2248133561
The issue is fixed in [Celery v5.5.0b1](https://github.com/celery/celery/releases/tag/v5.5.0b1) so we might want to bump min version for Celery to avoid this issue for everyone.

Can anyone confirm if upgrading to Celery 5.5.0 solves the problem? (Celery 5.5.0 not released yet, but rc1 is averrable so if anyone can confirm this solves the issue by checking it in test enviroment that would be great)

DartVeDroid on (2024-11-12 11:35:38 UTC): If I'm not mistaken, I did try it when kombu 5.4.0 was released, and it didn't help.
As far as I can tell, this issue is related to Airflow's celery provider only: when I upped the timeout value from 1 to 300, it stopped occurring.

Anyway, it's rather strange to me that this value is so small by default: scheduler container does not only schedule tasks, it also runs DAG analyzer, and in a real environment with cpu limit, or with badly written DAGs, analysis could easily take more than 1s, making send/fetch operations unsuccessful every now and then.

UPD:
Please don't blindly believe me, though. We run custom build/deploy configuration, so it's very much possible that it's our local problem in this instance.

"
2456325627,issue,closed,not_planned,DAGs go missing after a while,"### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

2.9.3

### What happened?

I am using the airflow locally using a custom Dockerfile and a docker-compose from the official URL with some small customization. I usually have a work flow like Extras, Transform and Load in separate DAGs and the las task for the ET are calling the next DAG in the flow.

My issue is that when I start to develop new DAGs locally, random tags start to go missing from the Webserver UI. when I go in the container and run the command ""airflow tags list"" my dogs are shown there (same with ""airflow tags report""), but they are not present in the UI. If I run the command ""airflow db init"" or ""airflow db migrate"" the DAGs go back to show in the Webserver UI for a short time (around 30 seconds) and then go missing again.

### What you think should happen instead?

The DAGs should be showing in the Webserver UI.

### How to reproduce

Honestly, I have no idea how to reproduce the errors, since I can't find anything in the logs.

### Operating System

PRETTY_NAME=""Debian GNU/Linux 12 (bookworm)"" NAME=""Debian GNU/Linux"" VERSION_ID=""12"" VERSION=""12 (bookworm)"" VERSION_CODENAME=bookworm ID=debian HOME_URL=""https://www.debian.org/"" SUPPORT_URL=""https://www.debian.org/support"" BUG_REPORT_URL=""https://bugs.debian.org/""

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

This problem seems to happen when I run the ""docker compose down && docker compose up -d"" often when developing.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",gabriel-attie,2024-08-08 17:47:22+00:00,[],2024-09-18 00:14:36+00:00,2024-09-18 00:14:35+00:00,https://github.com/apache/airflow/issues/41340,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('area:webserver', 'Webserver related Issues'), ('pending-response', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2276354312, 'issue_id': 2456325627, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 8, 17, 47, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277610395, 'issue_id': 2456325627, 'author': 'josix', 'body': ""Thank you for reporting this issue. To help us diagnose and reproduce the problem, could you please provide:\r\n\r\n1. Example DAGs that you are using when the issue occurs.\r\n1. The custom Dockerfile you're using.\r\n1. The docker-compose.yml file with your customizations.\r\n1. Any specific steps or operations that lead to the issue, it would be helpful if some attached screenshots are possible.\r\n\r\nThis information will help to better understand and address the problem. Thanks!"", 'created_at': datetime.datetime(2024, 8, 9, 10, 9, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2282231664, 'issue_id': 2456325627, 'author': 'dimon222', 'body': 'I noticed similar thing on my installation with version 2.9.2. It\'s possible that the problem have been present for some time. It definitely doesnt sound like expected behaviourr. I suspect there\'s some kind of race condition due to very long parsing as in my case I deal with over 2k dags setup. Unable to see exact conditions that cause this.\r\n\r\nThe dags may suddenly reappear and then disaplear all over for the course of day. It almost seems like data gets removed for little period instead of ""update"" operation this way causing conditions when dag isn\'t in database so webserver doesn\'t retrieve it.\r\nFrom user experience it looks like setup with over 2k dags with frequently running scheduler if you spam F5 while looking of dashboard of webserver the number of dags you get as visible changes each time.', 'created_at': datetime.datetime(2024, 8, 10, 18, 10, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2283867696, 'issue_id': 2456325627, 'author': 'gabriel-attie', 'body': '@josix:\r\n\r\n1 - The DAGs doesn\'t really matter since they disappear randomly. But here is one example:\r\n\r\n```python\r\nimport logging\r\n\r\nfrom airflow.decorators import dag, task\r\nfrom airflow.models.param import Param\r\nfrom airflow.operators.python import get_current_context\r\n\r\nfrom common.tasks.general import trigger_another_dag\r\nfrom common.tasks.teams import notify_failure\r\nfrom common.services.etl import ETLService\r\nfrom common.settings.car import *\r\nfrom common.settings.dags import default_args\r\nfrom common.settings.monitoring import UPDATE_RUNNING, WEEKLY\r\nfrom common.settings.envs import START_DATE\r\n\r\n\r\nlogger = logging.getLogger(""airflow.task"")\r\netl_service = ETLService()\r\n\r\n\r\n@dag(\r\n    default_args=default_args,\r\n    schedule_interval=""@weekly"",\r\n    start_date=START_DATE,\r\n    catchup=False,\r\n    tags=[""public""],\r\n    params={\r\n         ""ignore_discrepancy"": Param(False, type=""boolean""),\r\n         ""emergency_mode"": Param(None, type=[""null"", ""string""])\r\n    },\r\n)\r\ndef update_car_extract():\r\n    @task(on_failure_callback=notify_failure)\r\n    def main_run_attrs() -> dict:\r\n        """"""Core function Create monitoring instance\r\n\r\n        :return: dict with data to monitoring this project\r\n        """"""\r\n        from common.models.car import DataModel\r\n\r\n        context = get_current_context()\r\n\r\n        return etl_service.start_monitoring(\r\n            DataModel, context, UPDATE_RUNNING, WEEKLY\r\n        )\r\n\r\n    @task(on_failure_callback=notify_failure)\r\n    def download_file_to_s3() -> str:\r\n        """"""Core function to downloads files from source directly to S3.\r\n        It can be set to run in emergency mode by a DAG conf.\r\n\r\n        :return: string with s3 path to raw data\r\n        """"""\r\n        from common.models.sema_mt_car import DataModel\r\n\r\n        context = get_current_context()\r\n\r\n        # Set the url to download\r\n        source_urls = {""zip"": SOURCE}\r\n        logger.info(f""The Source URLS: {source_urls}"")\r\n\r\n        result_download = etl_service.download_file(\r\n            context, DataModel, source_urls, use_raw=False\r\n        )\r\n\r\n        return result_download\r\n\r\n    dag_conf = {\r\n        ""main_run_attrs"": main_run_attrs(),\r\n        ""raw_data_zip_path"": download_file_to_s3(),\r\n    }\r\n    trigger_another_dag(\r\n        dag_conf,\r\n        ""trigger_transform_dag"",\r\n        ""update_car_transform"",\r\n    )\r\n\r\nupdate_car_extract()\r\n```\r\n\r\n2 - Dockerfile\r\n```Dockerfile\r\nFROM apache/airflow:2.9.3-python3.11\r\nCOPY requirements.txt /requirements.txt\r\nRUN pip install --upgrade pip --trusted-host pypi.org --trusted-host files.pythonhosted.org\r\nRUN pip install --no-cache-dir -r /requirements.txt --trusted-host pypi.org --trusted-host files.pythonhosted.org\r\nUSER root\r\nRUN apt-get update && \\\r\n    apt-get install --allow-downgrades -y libpq5=15.6-0+deb12u1 libmariadb3=1:10.11.6-0+deb12u1\r\nRUN apt-get install -y libgdal-dev \\\r\n    gdal-bin \\\r\n    gcc \\\r\n    g++\r\nRUN sudo apt-get install unrar-free -y\r\nRUN sudo pip install geopandas --trusted-host pypi.org --trusted-host files.pythonhosted.org\r\nRUN sudo pip install --global-option=build_ext --global-option=""-I/usr/include/gdal"" GDAL==`gdal-config --version` --trusted-host pypi.org --trusted-host files.pythonhosted.org\r\nRUN sudo pip install --no-cache-dir rasterio --trusted-host pypi.org --trusted-host files.pythonhosted.org\r\nRUN apt-get clean\r\nUSER airflow\r\n```\r\n\r\n3 - docker-compose.yaml\r\n```docker-compose.yaml\r\nx-airflow-common:\r\n  &airflow-common\r\n  # In order to add custom dependencies or upgrade provider packages you can use your extended image.\r\n  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml\r\n  # and uncomment the ""build"" line below, Then run `docker-compose build` to build the images.\r\n  image: my-tag:latest\r\n  # build: .\r\n  environment:\r\n    &airflow-common-env\r\n    AIRFLOW__CORE__EXECUTOR: LocalExecutor\r\n    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\r\n    AIRFLOW__CORE__FERNET_KEY: \'\'\r\n    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: \'true\'\r\n    AIRFLOW__CORE__LOAD_EXAMPLES: \'false\'\r\n    AIRFLOW__API__AUTH_BACKENDS: \'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session\'\r\n    AIRFLOW__WEBSERVER__SHOW_TRIGGER_FORM_IF_NO_PARAMS: \'true\'\r\n    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: \'true\'\r\n    AIRFLOW__CORE__DEFAULT_TIMEZONE: \'America/Sao_Paulo\'\r\n    AIRFLOW__WEBSERVER__DAG_ORIENTATION: \'TB\'\r\n    AIRFLOW__LOGGING__COLORED_CONSOLE_LOG: \'true\'\r\n    AIRFLOW__SCHEDULER__SCHEDULER_ZOMBIE_TASK_THRESHOLD: 600\r\n    # yamllint disable rule:line-length\r\n    # Use simple http server on scheduler for health checks\r\n    # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server\r\n    # yamllint enable rule:line-length\r\n    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: \'true\'\r\n    # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks\r\n    # for other purpose (development, test and especially production usage) build/extend Airflow image.\r\n    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}\r\n    # The following line can be used to set a custom config file, stored in the local config folder\r\n    # If you want to use it, outcomment it and replace airflow.cfg with the name of your config file\r\n    # AIRFLOW_CONFIG: \'/opt/airflow/config/airflow.cfg\'\r\n  volumes:\r\n    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\r\n    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\r\n    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\r\n    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\r\n    - ${AIRFLOW_PROJ_DIR:-.}/common:/opt/airflow/plugins/common\r\n    - $HOME/.aws:/home/airflow/.aws\r\n  user: ""${AIRFLOW_UID:-50000}:0""\r\n  depends_on:\r\n    &airflow-common-depends-on\r\n    postgres:\r\n      condition: service_healthy\r\n\r\nservices:\r\n  postgres:\r\n    image: postgis/postgis:13-3.4\r\n    platform: linux/amd64\r\n    environment:\r\n      POSTGRES_USER: airflow\r\n      POSTGRES_PASSWORD: airflow\r\n      POSTGRES_DB: airflow\r\n    volumes:\r\n      - postgres-db-volume:/var/lib/postgresql/data\r\n    ports:\r\n      - ""5432:5432""\r\n    healthcheck:\r\n      test: [""CMD"", ""pg_isready"", ""-U"", ""airflow""]\r\n      interval: 10s\r\n      retries: 5\r\n      start_period: 5s\r\n    restart: always\r\n\r\n  airflow-webserver:\r\n    <<: *airflow-common\r\n    command: webserver\r\n    ports:\r\n      - ""8080:8080""\r\n    healthcheck:\r\n      test: [""CMD"", ""curl"", ""--fail"", ""http://localhost:8080/health""]\r\n      interval: 30s\r\n      timeout: 10s\r\n      retries: 5\r\n      start_period: 30s\r\n    restart: always\r\n    depends_on:\r\n      <<: *airflow-common-depends-on\r\n      airflow-init:\r\n        condition: service_completed_successfully\r\n\r\n  airflow-scheduler:\r\n    <<: *airflow-common\r\n    command: scheduler\r\n    healthcheck:\r\n      test: [""CMD"", ""curl"", ""--fail"", ""http://localhost:8974/health""]\r\n      interval: 30s\r\n      timeout: 10s\r\n      retries: 5\r\n      start_period: 30s\r\n    restart: always\r\n    depends_on:\r\n      <<: *airflow-common-depends-on\r\n      airflow-init:\r\n        condition: service_completed_successfully\r\n\r\n  airflow-triggerer:\r\n    <<: *airflow-common\r\n    command: triggerer\r\n    healthcheck:\r\n      test: [""CMD-SHELL"", \'airflow jobs check --job-type TriggererJob --hostname ""$${HOSTNAME}""\']\r\n      interval: 30s\r\n      timeout: 10s\r\n      retries: 5\r\n      start_period: 30s\r\n    restart: always\r\n    depends_on:\r\n      <<: *airflow-common-depends-on\r\n      airflow-init:\r\n        condition: service_completed_successfully\r\n\r\n  airflow-init:\r\n    <<: *airflow-common\r\n    entrypoint: /bin/bash\r\n    # yamllint disable rule:line-length\r\n    command:\r\n      - -c\r\n      - |\r\n        if [[ -z ""${AIRFLOW_UID}"" ]]; then\r\n          echo\r\n          echo -e ""\\033[1;33mWARNING!!!: AIRFLOW_UID not set!\\e[0m""\r\n          echo ""If you are on Linux, you SHOULD follow the instructions below to set ""\r\n          echo ""AIRFLOW_UID environment variable, otherwise files will be owned by root.""\r\n          echo ""For other operating systems you can get rid of the warning with manually created .env file:""\r\n          echo ""    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user""\r\n          echo\r\n        fi\r\n        one_meg=1048576\r\n        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))\r\n        cpus_available=$$(grep -cE \'cpu[0-9]+\' /proc/stat)\r\n        disk_available=$$(df / | tail -1 | awk \'{print $$4}\')\r\n        warning_resources=""false""\r\n        if (( mem_available < 4000 )) ; then\r\n          echo\r\n          echo -e ""\\033[1;33mWARNING!!!: Not enough memory available for Docker.\\e[0m""\r\n          echo ""At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))""\r\n          echo\r\n          warning_resources=""true""\r\n        fi\r\n        if (( cpus_available < 2 )); then\r\n          echo\r\n          echo -e ""\\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\\e[0m""\r\n          echo ""At least 2 CPUs recommended. You have $${cpus_available}""\r\n          echo\r\n          warning_resources=""true""\r\n        fi\r\n        if (( disk_available < one_meg * 10 )); then\r\n          echo\r\n          echo -e ""\\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\\e[0m""\r\n          echo ""At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))""\r\n          echo\r\n          warning_resources=""true""\r\n        fi\r\n        if [[ $${warning_resources} == ""true"" ]]; then\r\n          echo\r\n          echo -e ""\\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\\e[0m""\r\n          echo ""Please follow the instructions to increase amount of resources available:""\r\n          echo ""   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin""\r\n          echo\r\n        fi\r\n        mkdir -p /sources/logs /sources/dags /sources/plugins /sources/common\r\n        chown -R ""${AIRFLOW_UID}:0"" /sources/{logs,dags,plugins,common}\r\n        exec /entrypoint airflow version\r\n    # yamllint enable rule:line-length\r\n    environment:\r\n      <<: *airflow-common-env\r\n      _AIRFLOW_DB_MIGRATE: \'true\'\r\n      _AIRFLOW_WWW_USER_CREATE: \'true\'\r\n      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}\r\n      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}\r\n      _PIP_ADDITIONAL_REQUIREMENTS: \'\'\r\n    user: ""0:0""\r\n    volumes:\r\n      - ${AIRFLOW_PROJ_DIR:-.}:/sources\r\n\r\n  airflow-cli:\r\n    <<: *airflow-common\r\n    profiles:\r\n      - debug\r\n    environment:\r\n      <<: *airflow-common-env\r\n      CONNECTION_CHECK_MAX_COUNT: ""0""\r\n    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252\r\n    command:\r\n      - bash\r\n      - -c\r\n      - airflow\r\n\r\n  # You can enable flower by adding ""--profile flower"" option e.g. docker-compose --profile flower up\r\n  # or by explicitly targeted on the command line e.g. docker-compose up flower.\r\n  # See: https://docs.docker.com/compose/profiles/\r\n\r\nvolumes:\r\n  postgres-db-volume:\r\n```\r\n\r\n4 - There are no specific conditions in where the dogs go missing. I do suspect thought on the docker compose down and up too frequently.\r\n\r\n\r\nIn the moment I do not have screenshots showing the how the files goes missing in the web server. But it literally just goes missing, from 16 DAGs for example, I refresh the page (F5) and it\'s now with 14 DAGs.\r\n\r\nFor context: in our dev and production environment this does not occur. Only in the local environment. Usually in the local I have around 30 DAGs and in production we have around 300+ with codes going to 2k lines.', 'created_at': datetime.datetime(2024, 8, 12, 12, 39, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2284608020, 'issue_id': 2456325627, 'author': 'gabriel-attie', 'body': 'Here I have the problema again. I have 19 DAGs in my local airflow in the moment. 7 just went missing.\r\n\r\n![image](https://github.com/user-attachments/assets/771c1cd6-6665-4895-adec-289649e96c30)\r\n\r\nBut if I run ""airflow dags list"" I can see all the 19 DAGs:\r\n![image](https://github.com/user-attachments/assets/0cb40895-cbce-4e9d-94e7-95b48b4523d2)\r\n\r\nAfter running ""airflow db migrate"" the DAGs show up in the Webserver again:\r\n![image](https://github.com/user-attachments/assets/7bffda06-ad4d-4fe3-b6c1-f2e3047b6e22)\r\n\r\nI have no idea how to reproduce it, but it seems its always after I stop the containers and run them again.', 'created_at': datetime.datetime(2024, 8, 12, 17, 57, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2308954930, 'issue_id': 2456325627, 'author': 'jsjasonseba', 'body': 'Hi @gabriel-attie, do you find any error in the docker container logs?', 'created_at': datetime.datetime(2024, 8, 25, 18, 45, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2310708143, 'issue_id': 2456325627, 'author': 'gabriel-attie', 'body': ""> Hi @gabriel-attie, do you find any error in the docker container logs?\r\n\r\nNothing related to any missing DAGs. (2 import errors which I'm aware of - not an issue in local)."", 'created_at': datetime.datetime(2024, 8, 26, 17, 28, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339368838, 'issue_id': 2456325627, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 9, 10, 0, 14, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357237721, 'issue_id': 2456325627, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 9, 18, 0, 14, 35, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-08 17:47:25 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

josix on (2024-08-09 10:09:01 UTC): Thank you for reporting this issue. To help us diagnose and reproduce the problem, could you please provide:

1. Example DAGs that you are using when the issue occurs.
1. The custom Dockerfile you're using.
1. The docker-compose.yml file with your customizations.
1. Any specific steps or operations that lead to the issue, it would be helpful if some attached screenshots are possible.

This information will help to better understand and address the problem. Thanks!

dimon222 on (2024-08-10 18:10:09 UTC): I noticed similar thing on my installation with version 2.9.2. It's possible that the problem have been present for some time. It definitely doesnt sound like expected behaviourr. I suspect there's some kind of race condition due to very long parsing as in my case I deal with over 2k dags setup. Unable to see exact conditions that cause this.

The dags may suddenly reappear and then disaplear all over for the course of day. It almost seems like data gets removed for little period instead of ""update"" operation this way causing conditions when dag isn't in database so webserver doesn't retrieve it.
From user experience it looks like setup with over 2k dags with frequently running scheduler if you spam F5 while looking of dashboard of webserver the number of dags you get as visible changes each time.

gabriel-attie (Issue Creator) on (2024-08-12 12:39:01 UTC): @josix:

1 - The DAGs doesn't really matter since they disappear randomly. But here is one example:

```python
import logging

from airflow.decorators import dag, task
from airflow.models.param import Param
from airflow.operators.python import get_current_context

from common.tasks.general import trigger_another_dag
from common.tasks.teams import notify_failure
from common.services.etl import ETLService
from common.settings.car import *
from common.settings.dags import default_args
from common.settings.monitoring import UPDATE_RUNNING, WEEKLY
from common.settings.envs import START_DATE


logger = logging.getLogger(""airflow.task"")
etl_service = ETLService()


@dag(
    default_args=default_args,
    schedule_interval=""@weekly"",
    start_date=START_DATE,
    catchup=False,
    tags=[""public""],
    params={
         ""ignore_discrepancy"": Param(False, type=""boolean""),
         ""emergency_mode"": Param(None, type=[""null"", ""string""])
    },
)
def update_car_extract():
    @task(on_failure_callback=notify_failure)
    def main_run_attrs() -> dict:
        """"""Core function Create monitoring instance

        :return: dict with data to monitoring this project
        """"""
        from common.models.car import DataModel

        context = get_current_context()

        return etl_service.start_monitoring(
            DataModel, context, UPDATE_RUNNING, WEEKLY
        )

    @task(on_failure_callback=notify_failure)
    def download_file_to_s3() -> str:
        """"""Core function to downloads files from source directly to S3.
        It can be set to run in emergency mode by a DAG conf.

        :return: string with s3 path to raw data
        """"""
        from common.models.sema_mt_car import DataModel

        context = get_current_context()

        # Set the url to download
        source_urls = {""zip"": SOURCE}
        logger.info(f""The Source URLS: {source_urls}"")

        result_download = etl_service.download_file(
            context, DataModel, source_urls, use_raw=False
        )

        return result_download

    dag_conf = {
        ""main_run_attrs"": main_run_attrs(),
        ""raw_data_zip_path"": download_file_to_s3(),
    }
    trigger_another_dag(
        dag_conf,
        ""trigger_transform_dag"",
        ""update_car_transform"",
    )

update_car_extract()
```

2 - Dockerfile
```Dockerfile
FROM apache/airflow:2.9.3-python3.11
COPY requirements.txt /requirements.txt
RUN pip install --upgrade pip --trusted-host pypi.org --trusted-host files.pythonhosted.org
RUN pip install --no-cache-dir -r /requirements.txt --trusted-host pypi.org --trusted-host files.pythonhosted.org
USER root
RUN apt-get update && \
    apt-get install --allow-downgrades -y libpq5=15.6-0+deb12u1 libmariadb3=1:10.11.6-0+deb12u1
RUN apt-get install -y libgdal-dev \
    gdal-bin \
    gcc \
    g++
RUN sudo apt-get install unrar-free -y
RUN sudo pip install geopandas --trusted-host pypi.org --trusted-host files.pythonhosted.org
RUN sudo pip install --global-option=build_ext --global-option=""-I/usr/include/gdal"" GDAL==`gdal-config --version` --trusted-host pypi.org --trusted-host files.pythonhosted.org
RUN sudo pip install --no-cache-dir rasterio --trusted-host pypi.org --trusted-host files.pythonhosted.org
RUN apt-get clean
USER airflow
```

3 - docker-compose.yaml
```docker-compose.yaml
x-airflow-common:
  &airflow-common
  # In order to add custom dependencies or upgrade provider packages you can use your extended image.
  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml
  # and uncomment the ""build"" line below, Then run `docker-compose build` to build the images.
  image: my-tag:latest
  # build: .
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    AIRFLOW__WEBSERVER__SHOW_TRIGGER_FORM_IF_NO_PARAMS: 'true'
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
    AIRFLOW__CORE__DEFAULT_TIMEZONE: 'America/Sao_Paulo'
    AIRFLOW__WEBSERVER__DAG_ORIENTATION: 'TB'
    AIRFLOW__LOGGING__COLORED_CONSOLE_LOG: 'true'
    AIRFLOW__SCHEDULER__SCHEDULER_ZOMBIE_TASK_THRESHOLD: 600
    # yamllint disable rule:line-length
    # Use simple http server on scheduler for health checks
    # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server
    # yamllint enable rule:line-length
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks
    # for other purpose (development, test and especially production usage) build/extend Airflow image.
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
    # The following line can be used to set a custom config file, stored in the local config folder
    # If you want to use it, outcomment it and replace airflow.cfg with the name of your config file
    # AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'
  volumes:
    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs
    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins
    - ${AIRFLOW_PROJ_DIR:-.}/common:/opt/airflow/plugins/common
    - $HOME/.aws:/home/airflow/.aws
  user: ""${AIRFLOW_UID:-50000}:0""
  depends_on:
    &airflow-common-depends-on
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgis/postgis:13-3.4
    platform: linux/amd64
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    ports:
      - ""5432:5432""
    healthcheck:
      test: [""CMD"", ""pg_isready"", ""-U"", ""airflow""]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: always

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - ""8080:8080""
    healthcheck:
      test: [""CMD"", ""curl"", ""--fail"", ""http://localhost:8080/health""]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: [""CMD"", ""curl"", ""--fail"", ""http://localhost:8974/health""]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-triggerer:
    <<: *airflow-common
    command: triggerer
    healthcheck:
      test: [""CMD-SHELL"", 'airflow jobs check --job-type TriggererJob --hostname ""$${HOSTNAME}""']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    # yamllint disable rule:line-length
    command:
      - -c
      - |
        if [[ -z ""${AIRFLOW_UID}"" ]]; then
          echo
          echo -e ""\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m""
          echo ""If you are on Linux, you SHOULD follow the instructions below to set ""
          echo ""AIRFLOW_UID environment variable, otherwise files will be owned by root.""
          echo ""For other operating systems you can get rid of the warning with manually created .env file:""
          echo ""    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user""
          echo
        fi
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources=""false""
        if (( mem_available < 4000 )) ; then
          echo
          echo -e ""\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m""
          echo ""At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))""
          echo
          warning_resources=""true""
        fi
        if (( cpus_available < 2 )); then
          echo
          echo -e ""\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m""
          echo ""At least 2 CPUs recommended. You have $${cpus_available}""
          echo
          warning_resources=""true""
        fi
        if (( disk_available < one_meg * 10 )); then
          echo
          echo -e ""\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m""
          echo ""At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))""
          echo
          warning_resources=""true""
        fi
        if [[ $${warning_resources} == ""true"" ]]; then
          echo
          echo -e ""\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m""
          echo ""Please follow the instructions to increase amount of resources available:""
          echo ""   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin""
          echo
        fi
        mkdir -p /sources/logs /sources/dags /sources/plugins /sources/common
        chown -R ""${AIRFLOW_UID}:0"" /sources/{logs,dags,plugins,common}
        exec /entrypoint airflow version
    # yamllint enable rule:line-length
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
      _PIP_ADDITIONAL_REQUIREMENTS: ''
    user: ""0:0""
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}:/sources

  airflow-cli:
    <<: *airflow-common
    profiles:
      - debug
    environment:
      <<: *airflow-common-env
      CONNECTION_CHECK_MAX_COUNT: ""0""
    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
    command:
      - bash
      - -c
      - airflow

  # You can enable flower by adding ""--profile flower"" option e.g. docker-compose --profile flower up
  # or by explicitly targeted on the command line e.g. docker-compose up flower.
  # See: https://docs.docker.com/compose/profiles/

volumes:
  postgres-db-volume:
```

4 - There are no specific conditions in where the dogs go missing. I do suspect thought on the docker compose down and up too frequently.


In the moment I do not have screenshots showing the how the files goes missing in the web server. But it literally just goes missing, from 16 DAGs for example, I refresh the page (F5) and it's now with 14 DAGs.

For context: in our dev and production environment this does not occur. Only in the local environment. Usually in the local I have around 30 DAGs and in production we have around 300+ with codes going to 2k lines.

gabriel-attie (Issue Creator) on (2024-08-12 17:57:45 UTC): Here I have the problema again. I have 19 DAGs in my local airflow in the moment. 7 just went missing.

![image](https://github.com/user-attachments/assets/771c1cd6-6665-4895-adec-289649e96c30)

But if I run ""airflow dags list"" I can see all the 19 DAGs:
![image](https://github.com/user-attachments/assets/0cb40895-cbce-4e9d-94e7-95b48b4523d2)

After running ""airflow db migrate"" the DAGs show up in the Webserver again:
![image](https://github.com/user-attachments/assets/7bffda06-ad4d-4fe3-b6c1-f2e3047b6e22)

I have no idea how to reproduce it, but it seems its always after I stop the containers and run them again.

jsjasonseba on (2024-08-25 18:45:05 UTC): Hi @gabriel-attie, do you find any error in the docker container logs?

gabriel-attie (Issue Creator) on (2024-08-26 17:28:36 UTC): Nothing related to any missing DAGs. (2 import errors which I'm aware of - not an issue in local).

github-actions[bot] on (2024-09-10 00:14:15 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-09-18 00:14:35 UTC): This issue has been closed because it has not received response from the issue author.

"
2456299648,issue,closed,not_planned,Add a CoalesceOperator to Airflow Providers,"### Description

Coalesce is one of the industry-leaders in Snowflake transformations and orchestration efforts. Currently, there is not a provider available to run Coalesce workflows via Airflow. This issue proposes a new provider (Coalesce) with a `CoalesceOperator` that allows for a Coalesce workflows to be triggered.

### Use case/motivation

With a CoalesceOperator, Airflow users can orchestrate their Coalesce workflows in a DAG. This is what Airflow is built for; orchestrating disparate components of a data ecosystem, all while maintaining a single pane of glass into execution details.

Coalesce is used in a somewhat similar way to dbt, which is wildly popular among Airflow users. This operator is just the first component of a larger provider offering (hooks, sensors, etc.).

### Related issues

NA, there are no related issues associated with this.

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jroach-astronomer,2024-08-08 17:30:37+00:00,[],2024-09-10 00:14:18+00:00,2024-09-10 00:14:17+00:00,https://github.com/apache/airflow/issues/41339,"[('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('kind:feature', 'Feature Requests'), ('pending-response', ''), ('kind:new provider request', 'label to mark request for adding new provider')]","[{'comment_id': 2276988478, 'issue_id': 2456299648, 'author': 'eladkal', 'body': 'Please follow the procedure of accepting new provider\r\nhttps://github.com/apache/airflow/blob/main/PROVIDERS.rst#accepting-new-community-providers', 'created_at': datetime.datetime(2024, 8, 9, 1, 33, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277880406, 'issue_id': 2456299648, 'author': 'jroach-astronomer', 'body': '@eladkal, thanks for the documentation. I went ahead and read through it, but I was still unclear on next-steps. Do I go ahead and start a thread with the `devlist`? Is there another way to begin the discussion? Should I develop the PR before or after the discuss? Thanks for the help!', 'created_at': datetime.datetime(2024, 8, 9, 12, 53, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2283168079, 'issue_id': 2456299648, 'author': 'eladkal', 'body': 'First step is to start a discussion in the mailing list with your proposal. If this is not clear from the guide then please feel free to improve it', 'created_at': datetime.datetime(2024, 8, 12, 6, 3, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2283301615, 'issue_id': 2456299648, 'author': 'potiuk', 'body': 'Also search archives https://lists.apache.org/list.html?dev@airflow.apache.org with ""new provider"" discussions - and you will see how such discussions looked in the past. This is a very good way to understand whether you want to get the new provider at all in the first place, and what are the chances that your proposal will be accepted.', 'created_at': datetime.datetime(2024, 8, 12, 7, 44, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2283305100, 'issue_id': 2456299648, 'author': 'potiuk', 'body': '(and you will also see that for example if you want to make it happen with external service, managing and committing to having a system test dashboard is basically a hard-prerequisite now).\r\n\r\nSee the dashboards managed by Amazon, Google, Astronomer and Teradata https://airflow.apache.org/ecosystem/#airflow-provider-system-test-dashboards', 'created_at': datetime.datetime(2024, 8, 12, 7, 46, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2323558041, 'issue_id': 2456299648, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 9, 2, 0, 14, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2339368867, 'issue_id': 2456299648, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 9, 10, 0, 14, 17, tzinfo=datetime.timezone.utc)}]","eladkal on (2024-08-09 01:33:57 UTC): Please follow the procedure of accepting new provider
https://github.com/apache/airflow/blob/main/PROVIDERS.rst#accepting-new-community-providers

jroach-astronomer (Issue Creator) on (2024-08-09 12:53:53 UTC): @eladkal, thanks for the documentation. I went ahead and read through it, but I was still unclear on next-steps. Do I go ahead and start a thread with the `devlist`? Is there another way to begin the discussion? Should I develop the PR before or after the discuss? Thanks for the help!

eladkal on (2024-08-12 06:03:57 UTC): First step is to start a discussion in the mailing list with your proposal. If this is not clear from the guide then please feel free to improve it

potiuk on (2024-08-12 07:44:18 UTC): Also search archives https://lists.apache.org/list.html?dev@airflow.apache.org with ""new provider"" discussions - and you will see how such discussions looked in the past. This is a very good way to understand whether you want to get the new provider at all in the first place, and what are the chances that your proposal will be accepted.

potiuk on (2024-08-12 07:46:30 UTC): (and you will also see that for example if you want to make it happen with external service, managing and committing to having a system test dashboard is basically a hard-prerequisite now).

See the dashboards managed by Amazon, Google, Astronomer and Teradata https://airflow.apache.org/ecosystem/#airflow-provider-system-test-dashboards

github-actions[bot] on (2024-09-02 00:14:44 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-09-10 00:14:17 UTC): This issue has been closed because it has not received response from the issue author.

"
2455926325,issue,closed,completed,[OpenAI Provider] Support Batch API in OpenAI Hook ,"### Description

For generating a large number of responses from OpenAI's LLM, we can leverage the [Batch API](https://platform.openai.com/docs/guides/batch/overview) to send asynchronous groups of requests and wait for their completion when rapid response is not required. This approach can also help reduce costs in use cases such as prompt evaluations and applying prompts to each entity in large datasets. Supporting this functionality would be particularly useful for workflows in Airflow, which is commonly used for batch processing of different datasets. It would be great if the OpenAI provider could support the Batch API.



### Use case/motivation

To support Batch API, the OpenAI Hook would need to include the following behaviors:

1. Support for OpenAI().files.create, which is already implemented.
1. Support for OpenAI().batches.create to submit a job for OpenAI to make predictions on the file.
1. Support other behaviors that batches have such as retrieve and cancel. 

I think that would be nice if we have a new deferrable operator to submit the batch prediction and retrieve the results once finished, that would be helpful.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",josix,2024-08-08 14:15:17+00:00,['josix'],2024-08-22 10:34:36+00:00,2024-08-22 10:34:36+00:00,https://github.com/apache/airflow/issues/41336,"[('kind:feature', 'Feature Requests'), ('area:async-operators', 'AIP-40: Deferrable (""Async"") Operators'), ('provider:openai', '')]","[{'comment_id': 2275943256, 'issue_id': 2455926325, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 8, 14, 15, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2276180044, 'issue_id': 2455926325, 'author': 'Lee-W', 'body': ""Sounds like a good idea. As you checked the you're willing to submit a PR box, I'll assign this one to you."", 'created_at': datetime.datetime(2024, 8, 8, 16, 4, 27, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-08 14:15:21 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

Lee-W on (2024-08-08 16:04:27 UTC): Sounds like a good idea. As you checked the you're willing to submit a PR box, I'll assign this one to you.

"
2455519839,issue,closed,completed,Graph view crashes when a DAG is empty,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When an empty DAG is created without any tasks, the DAG can be parsed and triggered in the UI, but the UI crashes when you click on _Graph_. The entire screen (except the top bar) disappears.

```
Uncaught TypeError: u.nodes.children[0] is undefined
    gQ http://localhost:28080/static/dist/grid.abc695e791ed71742eea.js:2
    wa http://localhost:28080/static/dist/grid.abc695e791ed71742eea.js:2
    Ms http://localhost:28080/static/dist/grid.abc695e791ed71742eea.js:2
    wl http://localhost:28080/static/dist/grid.abc695e791ed71742eea.js:2
    vu http://localhost:28080/static/dist/grid.abc695e791ed71742eea.js:2
    mu http://localhost:28080/static/dist/grid.abc695e791ed71742eea.js:2
    gu http://localhost:28080/static/dist/grid.abc695e791ed71742eea.js:2
    iu http://localhost:28080/static/dist/grid.abc695e791ed71742eea.js:2
    ru http://localhost:28080/static/dist/grid.abc695e791ed71742eea.js:2
    _ http://localhost:28080/static/dist/grid.abc695e791ed71742eea.js:2
    O http://localhost:28080/static/dist/grid.abc695e791ed71742eea.js:2
grid.abc695e791ed71742eea.js:2:2750893
```

### What you think should happen instead?

The UI should load correctly. The graph should be empty (no blocks), but other controls should still load.

### How to reproduce

1. Load this DAG into Airflow

    ```python
    from airflow.decorators import dag

    @dag(schedule=None)
    def empty():
        pass

    empty()

2. Trigger a run from the Home page.

3. Click into the DAG detail view
4. Clikc on _Graph_


### Operating System

n/a

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",uranusjr,2024-08-08 11:08:38+00:00,[],2024-08-08 15:56:58+00:00,2024-08-08 15:56:58+00:00,https://github.com/apache/airflow/issues/41332,"[('kind:bug', 'This is a clearly a bug'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2275884038, 'issue_id': 2455519839, 'author': 'josix', 'body': 'I made [this change](https://github.com/josix/airflow/commit/7a6b04dc0f0a96b56812323eb299c2bbbdd93354) to let the page not become blank, but is it okay to display empty dag like this?\r\n\r\n\r\n![image](https://github.com/user-attachments/assets/f03e4b98-9b60-46d9-8ac3-f1cd55a36aa4)', 'created_at': datetime.datetime(2024, 8, 8, 13, 49, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2276073611, 'issue_id': 2455519839, 'author': 'romsharon98', 'body': '> I made [this change](https://github.com/josix/airflow/commit/7a6b04dc0f0a96b56812323eb299c2bbbdd93354) to let the page not become blank, but is it okay to display empty dag like this?\r\n> \r\n> ![image](https://private-user-images.githubusercontent.com/18432820/356245257-f03e4b98-9b60-46d9-8ac3-f1cd55a36aa4.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjMxMzAyMzAsIm5iZiI6MTcyMzEyOTkzMCwicGF0aCI6Ii8xODQzMjgyMC8zNTYyNDUyNTctZjAzZTRiOTgtOWI2MC00NmQ5LThhYzMtZjFjZDU1YTM2YWE0LnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA4MDglMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwODA4VDE1MTIxMFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTc2MWE3ZTk0NGY2ZGIxMjUxMTA1ODQxYjcwMDY5NzQ3MmFmNDg2YjdhNzg5NGI2ZjQ2NzNlZWRlOTU0MzI1NWImWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.CIq26CDUjKEHRfNyC7BC9wK2bpOTPMYWet-UJkY_O7g)\r\n\r\nYes, I think this is how it should look.', 'created_at': datetime.datetime(2024, 8, 8, 15, 12, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2276091223, 'issue_id': 2455519839, 'author': 'uranusjr', 'body': 'Agreed, I think an empty page is good enough. An empty DAG is a user error anyway, and it’s probably good enough we don’t crash.', 'created_at': datetime.datetime(2024, 8, 8, 15, 20, 10, tzinfo=datetime.timezone.utc)}]","josix on (2024-08-08 13:49:42 UTC): I made [this change](https://github.com/josix/airflow/commit/7a6b04dc0f0a96b56812323eb299c2bbbdd93354) to let the page not become blank, but is it okay to display empty dag like this?


![image](https://github.com/user-attachments/assets/f03e4b98-9b60-46d9-8ac3-f1cd55a36aa4)

romsharon98 on (2024-08-08 15:12:34 UTC): Yes, I think this is how it should look.

uranusjr (Issue Creator) on (2024-08-08 15:20:10 UTC): Agreed, I think an empty page is good enough. An empty DAG is a user error anyway, and it’s probably good enough we don’t crash.

"
2455466945,issue,closed,completed,Missing dependency `methodtools` when using `microsoft-mssql==3.8.0` with `airflow==2.7.3`,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

[apache-airflow-providers-microsoft-mssql ](https://airflow.apache.org/docs/apache-airflow-providers-microsoft-mssql/3.8.0/index.html#requirements) documentation mentions that it  3.8.0 supports airflow>2.7.0.  But, it causes the following import error.

```
from airflow.providers.microsoft.mssql.hooks.mssql import MsSqlHook
  File ""/usr/local/lib/python3.11/site-packages/airflow/providers/microsoft/mssql/hooks/mssql.py"", line 26, in <module>
    from methodtools import lru_cache
ModuleNotFoundError: No module named 'methodtools'
```

### What you think should happen instead?

`methodtools` is not part of the requirements in Airflow 2.7.3.  Documentation should be updated for apache-airflow-providers-microsoft-mssql to show the compatible Airflow version.

### How to reproduce

Use apache-airflow-providers-microsoft-mssql==3.8.0 along with airflow==2.7.3. Then, try to import  `from airflow.providers.microsoft.mssql.hooks.mssql import MsSqlHook`

### Operating System

ubuntu-22.04

### Versions of Apache Airflow Providers

_No response_

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rawwar,2024-08-08 10:40:32+00:00,['vatsrahul1001'],2024-08-12 00:27:45+00:00,2024-08-12 00:27:44+00:00,https://github.com/apache/airflow/issues/41330,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('good first issue', ''), ('area:core', ''), ('provider:microsoft-mssql', '')]","[{'comment_id': 2282939001, 'issue_id': 2455466945, 'author': 'potiuk', 'body': 'Indeed', 'created_at': datetime.datetime(2024, 8, 12, 0, 11, 47, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-08-12 00:11:47 UTC): Indeed

"
2455450419,issue,open,reopened,Hash Virtual Environment Cache Based on Actual Package Versions in `PythonVirtualEnvOperator`,"### Description

Update the `PythonVirtualEnvOperator` to hash the virtual environment cache based on the actual versions of the installed packages rather than just the checksum of the requirements. This change would ensure that the cache reflects the true state of the environment, avoiding issues with packages tagged as **""latest""** or other dynamic versioning.

### Use case/motivation

Currently, when using the `PythonVirtualEnvOperator`, if dependencies in the requirements use tags like **""latest""**, the checksum used for caching remains unchanged even if the package versions are updated. This can lead to situations where outdated versions of packages are used from the cache, causing potential inconsistencies and issues in workflows. By hashing the cache based on the actual versions of installed packages, the virtual environment would be refreshed appropriately whenever package versions change, ensuring that the most current versions are used.

### Related issues

_No response_

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",pedro-cf,2024-08-08 10:31:52+00:00,['pedro-cf'],2024-08-12 14:35:31+00:00,,https://github.com/apache/airflow/issues/41328,"[('kind:feature', 'Feature Requests'), ('good first issue', ''), ('area:core', ''), ('area:core-operators', 'Operators, Sensors and hooks within Core Airflow')]","[{'comment_id': 2282951794, 'issue_id': 2455450419, 'author': 'potiuk', 'body': ""You can't do it. You do not know what versions will be installed before you install it, at which point calculating hash is already too late - because you already installed the venv. \r\n\r\nTechnically speaking -if you do not specify `==` in all requirements (which you should in this case if you want reproducibilitty) using last installed venv snapshot is fully correct (it still follows the specification you gave it).\r\n\r\nIf you want full reproducibility - just pin all your requirements, that's really the only way."", 'created_at': datetime.datetime(2024, 8, 12, 0, 33, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2283478774, 'issue_id': 2455450419, 'author': 'pedro-cf', 'body': ""> You can't do it. You do not know what versions will be installed before you install it, at which point calculating hash is already too late - because you already installed the venv.\r\n> \r\n> Technically speaking -if you do not specify `==` in all requirements (which you should in this case if you want reproducibilitty) using last installed venv snapshot is fully correct (it still follows the specification you gave it).\r\n> \r\n> If you want full reproducibility - just pin all your requirements, that's really the only way.\r\n\r\nIt is possible to perform a  `pip download -r requirements.txt` which will  technically parse the versions and download them **_if_** they are missing from the download location.\r\n\r\nexample:\r\n\r\n`requirements.txt`\r\n```bash\r\npandas\r\ncolormap==1.0.4\r\n```\r\n\r\n`pip download -r requirements.txt`\r\n```bash\r\nCollecting pandas (from -r requirements.txt (line 1))\r\n  Using cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\r\nCollecting colormap==1.0.4 (from -r requirements.txt (line 2))\r\n  Using cached colormap-1.0.4.tar.gz (17 kB)\r\n  Preparing metadata (setup.py) ... done\r\nCollecting numpy>=1.22.4 (from pandas->-r requirements.txt (line 1))\r\n  Using cached numpy-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\r\nCollecting python-dateutil>=2.8.2 (from pandas->-r requirements.txt (line 1))\r\n  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\r\nCollecting pytz>=2020.1 (from pandas->-r requirements.txt (line 1))\r\n  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\r\nCollecting tzdata>=2022.7 (from pandas->-r requirements.txt (line 1))\r\n  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\r\nCollecting six>=1.5 (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1))\r\n  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\r\n```\r\n\r\n`tree .`\r\n```bash\r\n.\r\n├── colormap-1.0.4.tar.gz\r\n├── numpy-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n├── pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n├── python_dateutil-2.9.0.post0-py2.py3-none-any.whl\r\n├── pytz-2024.1-py2.py3-none-any.whl\r\n├── requirements.txt\r\n├── six-1.16.0-py2.py3-none-any.whl\r\n└── tzdata-2024.1-py2.py3-none-any.whl\r\n```\r\n\r\nDownload when the packages are already downloaded:\r\n`pip download -r requirements.txt` \r\n```\r\nCollecting pandas (from -r requirements.txt (line 1))\r\n  File was already downloaded /mnt/c/git/tst/tst/pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\nCollecting colormap==1.0.4 (from -r requirements.txt (line 2))\r\n  File was already downloaded /mnt/c/git/tst/tst/colormap-1.0.4.tar.gz\r\n  Preparing metadata (setup.py) ... done\r\nCollecting numpy>=1.22.4 (from pandas->-r requirements.txt (line 1))\r\n  File was already downloaded /mnt/c/git/tst/tst/numpy-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\nCollecting python-dateutil>=2.8.2 (from pandas->-r requirements.txt (line 1))\r\n  File was already downloaded /mnt/c/git/tst/tst/python_dateutil-2.9.0.post0-py2.py3-none-any.whl\r\nCollecting pytz>=2020.1 (from pandas->-r requirements.txt (line 1))\r\n  File was already downloaded /mnt/c/git/tst/tst/pytz-2024.1-py2.py3-none-any.whl\r\nCollecting tzdata>=2022.7 (from pandas->-r requirements.txt (line 1))\r\n  File was already downloaded /mnt/c/git/tst/tst/tzdata-2024.1-py2.py3-none-any.whl\r\nCollecting six>=1.5 (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1))\r\n  File was already downloaded /mnt/c/git/tst/tst/six-1.16.0-py2.py3-none-any.whl\r\nSuccessfully downloaded colormap pandas numpy python-dateutil pytz tzdata six\r\n```\r\n\r\nWe could possibly parse this output to generate the hash ?"", 'created_at': datetime.datetime(2024, 8, 12, 9, 19, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2283504672, 'issue_id': 2455450419, 'author': 'potiuk', 'body': 'Yes but why are we using Cache in this case at all if it is going to take about the same amount of time as pip download every time we attempt to use the venv?\n\nIt negates the cache savings', 'created_at': datetime.datetime(2024, 8, 12, 9, 33, 38, tzinfo=datetime.timezone.utc)}, {'comment_id': 2283508768, 'issue_id': 2455450419, 'author': 'pedro-cf', 'body': ""> Yes but why are we using Cache in this case at all if it is going to take about the same amount of time as pip download every time we attempt to use the venv?\r\n> \r\n> It negates the cache savings\r\n\r\nif the .whl are already downloaded, they will not be re-downloaded, and also with `pip download` we don't install anything"", 'created_at': datetime.datetime(2024, 8, 12, 9, 35, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2283519407, 'issue_id': 2455450419, 'author': 'potiuk', 'body': ""Sure you can attempt to maje PR if you think it's worth it. You seem to have good idea what to do in this case. I think it's a little too .uch complicating things but if you would like to spend time on it and make tests etc. -feel free. However please make it a optional flag so that it does not happen by default as even resolution and downloading wheels will add quite an overhead in a number of cases"", 'created_at': datetime.datetime(2024, 8, 12, 9, 41, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2283567431, 'issue_id': 2455450419, 'author': 'pedro-cf', 'body': ""there's also the option to use `pip index versions <package_name>` , example:\r\n\r\n`pip index versions colormap`\r\n\r\n```bash\r\ncolormap (1.1.0)\r\nAvailable versions: 1.1.0, 1.0.6, 1.0.4, 1.0.3, 1.0.2, 1.0.1, 1.0.0, 0.9.10, 0.9.9, 0.9.8, 0.9.7, 0.9.6, 0.9.5, 0.9.4, 0.9.3, 0.9.2, 0.9.1, 0.9.0\r\n```"", 'created_at': datetime.datetime(2024, 8, 12, 10, 6, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2283573906, 'issue_id': 2455450419, 'author': 'potiuk', 'body': 'Just one important comment - with this change virtualenvs will stop being immutable. Which means that you will have to handle case where venv is being reinstalled while being used (for example by another celery process on the same machine) - so likely reinstallation will have to handle symbolic links and atomic renames of changed venv. Also it will likely need to include some way of disposing the old venvs.', 'created_at': datetime.datetime(2024, 8, 12, 10, 10, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2283605346, 'issue_id': 2455450419, 'author': 'pedro-cf', 'body': '> Just one important comment - with this change virtualenvs will stop being immutable. Which means that you will have to handle case where venv is being reinstalled while being used (for example by another celery process on the same machine) - so likely reinstallation will have to handle symbolic links and atomic renames of changed venv. Also it will likely need to include some way of disposing the old venvs.\r\n\r\nIf the **""latest""** version of a package changes the hash for the respective **venv** would change too. The only thing I wanted to achieve was to parse dynamic versions into absolute version like f.e.:\r\n\r\n-  `colormap>=1.0.0` or `colormap`  are converted into  `colormap==1.1.0`', 'created_at': datetime.datetime(2024, 8, 12, 10, 27, 23, tzinfo=datetime.timezone.utc)}, {'comment_id': 2284153528, 'issue_id': 2455450419, 'author': 'potiuk', 'body': '> If the ""latest"" version of a package changes the hash for the respective venv would change too. The only thing I wanted to achieve was to paryse dynamic versions into absolute version like f.e.:\r\n> colormap>=1.0.0 or colormap are converted into colormap==1.1.0\r\n\r\nSure. The problem with that approach is that it has the potential of balloning a number of venvs - for example boto3 releases a new version every day or so - which means that if you are dynamically recreating the venv besed on latest version available in pypi and have boto3> x.y.z - it will create a new copy of the venv every day. Previously this happened only when you actuallly changed dependency requirements. \r\n\r\nBut yes if their hashes will be different and stored separately, they will be essentially immutable (but there will be many more of those potentially and a strategy need to be worked out how to dispose the old ones essentially as they will grow in totally uncontrollable way potentially - without any DAG author action). I wonder what would be the proposal for that - because even then it could be that some tasks are still using the old version of venv with different hash, when the new one is being installed and used for a different task.\r\n\r\n> The only thing I wanted to achieve was to parse dynamic versions into absolute version like f.e.:\r\n> colormap>=1.0.0 or colormap are converted into colormap==1.1.0\r\n> pip index versions colormap\r\n\r\nPip index is not nearly enough. You have to run algorithm to resolve the dependencies - because new versions of requirements might have different limitations - so you actually have to perform full pip install resolution to perform such installation  -you cannot just take ""latest"" of all dependencies that are specified with lower bound). For example if colrmap==1.1.0 has foobar<3.2 and you already had foobar 3.2 installed (because colormap == 1.0.0 did not have that limit) - pip will have to resolve the dependencies and decide whether to downgrade foobar or simply not upgrade to the newer colormap (otherwise it will end up with conflict). So any time when you want to check for the ""non-conflicting"" dependencies, you basically have to do full dependency resolution with `--eager-upgrade` resolution strategy or perform a completely new installation (and dependency resolution) without looking what you have already installed in the target venv.\r\n\r\nThis is the overhead that will need to happen on every single run of a task with such venv definition - regardless if cache is there, because you need to that resolution in order to calculate the new hash and compare it with the existing one.  - this is why it\'s an overhead as sometimes such resolution might mean some back-tracking and downloading multiple versions of the same package - even if locally you already have current version of the dependency in cache. It can take even minutes  sometimes (and this was the main reason why we wanted to implement caching - to save time on the dependency resolution and downloading). \r\n\r\nDependency resolution in PyPI can be (and often is) quite time/network consuming. \r\n\r\nBasically you have two options now :\r\n\r\n1) No cache - then you always get latest (at the expense of dependency resolution and downloading packages). Often slow and not predictable.\r\n\r\n2) Cache - then you always get the ""first matching requirements installed"" at that machine - which makes it potentially inconsistent between runs on different machines (but with very little overhead of only first time resolution and installation)\r\n\r\nEssentially, what you want is option 3)\r\n\r\n3) Cache but check if cache needs to be invalidated because some new dependencies have been released since the last time the task has been run -> which is something in-between.  Part of the process is faster (if nothing changed, you only pay the price of performing resolution - which might, or might not be slow and is somewhat unprecdictable (depends on packages released by 3rd-parties).  Also with the drawback of potentially leaving behind many versions of venvs - where they can grow in non-controllable way over time. So we need to find a solution for managing those.\r\n\r\nBut yes, if you want to pursue that and propose PR - feel free.', 'created_at': datetime.datetime(2024, 8, 12, 14, 33, 23, tzinfo=datetime.timezone.utc)}]","potiuk on (2024-08-12 00:33:11 UTC): You can't do it. You do not know what versions will be installed before you install it, at which point calculating hash is already too late - because you already installed the venv. 

Technically speaking -if you do not specify `==` in all requirements (which you should in this case if you want reproducibilitty) using last installed venv snapshot is fully correct (it still follows the specification you gave it).

If you want full reproducibility - just pin all your requirements, that's really the only way.

pedro-cf (Issue Creator) on (2024-08-12 09:19:50 UTC): It is possible to perform a  `pip download -r requirements.txt` which will  technically parse the versions and download them **_if_** they are missing from the download location.

example:

`requirements.txt`
```bash
pandas
colormap==1.0.4
```

`pip download -r requirements.txt`
```bash
Collecting pandas (from -r requirements.txt (line 1))
  Using cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)
Collecting colormap==1.0.4 (from -r requirements.txt (line 2))
  Using cached colormap-1.0.4.tar.gz (17 kB)
  Preparing metadata (setup.py) ... done
Collecting numpy>=1.22.4 (from pandas->-r requirements.txt (line 1))
  Using cached numpy-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)
Collecting python-dateutil>=2.8.2 (from pandas->-r requirements.txt (line 1))
  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)
Collecting pytz>=2020.1 (from pandas->-r requirements.txt (line 1))
  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)
Collecting tzdata>=2022.7 (from pandas->-r requirements.txt (line 1))
  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)
Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1))
  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)
```

`tree .`
```bash
.
├── colormap-1.0.4.tar.gz
├── numpy-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
├── pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
├── python_dateutil-2.9.0.post0-py2.py3-none-any.whl
├── pytz-2024.1-py2.py3-none-any.whl
├── requirements.txt
├── six-1.16.0-py2.py3-none-any.whl
└── tzdata-2024.1-py2.py3-none-any.whl
```

Download when the packages are already downloaded:
`pip download -r requirements.txt` 
```
Collecting pandas (from -r requirements.txt (line 1))
  File was already downloaded /mnt/c/git/tst/tst/pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
Collecting colormap==1.0.4 (from -r requirements.txt (line 2))
  File was already downloaded /mnt/c/git/tst/tst/colormap-1.0.4.tar.gz
  Preparing metadata (setup.py) ... done
Collecting numpy>=1.22.4 (from pandas->-r requirements.txt (line 1))
  File was already downloaded /mnt/c/git/tst/tst/numpy-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
Collecting python-dateutil>=2.8.2 (from pandas->-r requirements.txt (line 1))
  File was already downloaded /mnt/c/git/tst/tst/python_dateutil-2.9.0.post0-py2.py3-none-any.whl
Collecting pytz>=2020.1 (from pandas->-r requirements.txt (line 1))
  File was already downloaded /mnt/c/git/tst/tst/pytz-2024.1-py2.py3-none-any.whl
Collecting tzdata>=2022.7 (from pandas->-r requirements.txt (line 1))
  File was already downloaded /mnt/c/git/tst/tst/tzdata-2024.1-py2.py3-none-any.whl
Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1))
  File was already downloaded /mnt/c/git/tst/tst/six-1.16.0-py2.py3-none-any.whl
Successfully downloaded colormap pandas numpy python-dateutil pytz tzdata six
```

We could possibly parse this output to generate the hash ?

potiuk on (2024-08-12 09:33:38 UTC): Yes but why are we using Cache in this case at all if it is going to take about the same amount of time as pip download every time we attempt to use the venv?

It negates the cache savings

pedro-cf (Issue Creator) on (2024-08-12 09:35:51 UTC): if the .whl are already downloaded, they will not be re-downloaded, and also with `pip download` we don't install anything

potiuk on (2024-08-12 09:41:36 UTC): Sure you can attempt to maje PR if you think it's worth it. You seem to have good idea what to do in this case. I think it's a little too .uch complicating things but if you would like to spend time on it and make tests etc. -feel free. However please make it a optional flag so that it does not happen by default as even resolution and downloading wheels will add quite an overhead in a number of cases

pedro-cf (Issue Creator) on (2024-08-12 10:06:35 UTC): there's also the option to use `pip index versions <package_name>` , example:

`pip index versions colormap`

```bash
colormap (1.1.0)
Available versions: 1.1.0, 1.0.6, 1.0.4, 1.0.3, 1.0.2, 1.0.1, 1.0.0, 0.9.10, 0.9.9, 0.9.8, 0.9.7, 0.9.6, 0.9.5, 0.9.4, 0.9.3, 0.9.2, 0.9.1, 0.9.0
```

potiuk on (2024-08-12 10:10:08 UTC): Just one important comment - with this change virtualenvs will stop being immutable. Which means that you will have to handle case where venv is being reinstalled while being used (for example by another celery process on the same machine) - so likely reinstallation will have to handle symbolic links and atomic renames of changed venv. Also it will likely need to include some way of disposing the old venvs.

pedro-cf (Issue Creator) on (2024-08-12 10:27:23 UTC): If the **""latest""** version of a package changes the hash for the respective **venv** would change too. The only thing I wanted to achieve was to parse dynamic versions into absolute version like f.e.:

-  `colormap>=1.0.0` or `colormap`  are converted into  `colormap==1.1.0`

potiuk on (2024-08-12 14:33:23 UTC): Sure. The problem with that approach is that it has the potential of balloning a number of venvs - for example boto3 releases a new version every day or so - which means that if you are dynamically recreating the venv besed on latest version available in pypi and have boto3> x.y.z - it will create a new copy of the venv every day. Previously this happened only when you actuallly changed dependency requirements. 

But yes if their hashes will be different and stored separately, they will be essentially immutable (but there will be many more of those potentially and a strategy need to be worked out how to dispose the old ones essentially as they will grow in totally uncontrollable way potentially - without any DAG author action). I wonder what would be the proposal for that - because even then it could be that some tasks are still using the old version of venv with different hash, when the new one is being installed and used for a different task.


Pip index is not nearly enough. You have to run algorithm to resolve the dependencies - because new versions of requirements might have different limitations - so you actually have to perform full pip install resolution to perform such installation  -you cannot just take ""latest"" of all dependencies that are specified with lower bound). For example if colrmap==1.1.0 has foobar<3.2 and you already had foobar 3.2 installed (because colormap == 1.0.0 did not have that limit) - pip will have to resolve the dependencies and decide whether to downgrade foobar or simply not upgrade to the newer colormap (otherwise it will end up with conflict). So any time when you want to check for the ""non-conflicting"" dependencies, you basically have to do full dependency resolution with `--eager-upgrade` resolution strategy or perform a completely new installation (and dependency resolution) without looking what you have already installed in the target venv.

This is the overhead that will need to happen on every single run of a task with such venv definition - regardless if cache is there, because you need to that resolution in order to calculate the new hash and compare it with the existing one.  - this is why it's an overhead as sometimes such resolution might mean some back-tracking and downloading multiple versions of the same package - even if locally you already have current version of the dependency in cache. It can take even minutes  sometimes (and this was the main reason why we wanted to implement caching - to save time on the dependency resolution and downloading). 

Dependency resolution in PyPI can be (and often is) quite time/network consuming. 

Basically you have two options now :

1) No cache - then you always get latest (at the expense of dependency resolution and downloading packages). Often slow and not predictable.

2) Cache - then you always get the ""first matching requirements installed"" at that machine - which makes it potentially inconsistent between runs on different machines (but with very little overhead of only first time resolution and installation)

Essentially, what you want is option 3)

3) Cache but check if cache needs to be invalidated because some new dependencies have been released since the last time the task has been run -> which is something in-between.  Part of the process is faster (if nothing changed, you only pay the price of performing resolution - which might, or might not be slow and is somewhat unprecdictable (depends on packages released by 3rd-parties).  Also with the drawback of potentially leaving behind many versions of venvs - where they can grow in non-controllable way over time. So we need to find a solution for managing those.

But yes, if you want to pursue that and propose PR - feel free.

"
2455152949,issue,closed,not_planned,SparkSubmitHook requires yarn binary,"### Apache Airflow Provider(s)

apache-spark

### Versions of Apache Airflow Providers

4.9.0

### Apache Airflow version

2.9.3

### Operating System

Debian GNU/Linux 12 (bookworm)

### Deployment

Docker-Compose

### Deployment details

_No response_

### What happened

I try to use spark submit with --master yarn --deploy-mode cluster parameters.
And I wanna to kill application on cluster when I terminate SparkSubmitOperator.

SparkSubmitHook has a on_kill() method, which run `yarn application -kill`  command:
https://github.com/apache/airflow/blob/45658a8963761ce8a565b481156c847e493fce67/airflow/providers/apache/spark/hooks/spark_submit.py#L709
But it's not worked, because of no such binary in the PATH (`yarn`).
My Airflow instance run on host, without hadoop installation.

In the hook docstring only `spark-submit`  mentioned as required, no word about `yarn`
https://github.com/apache/airflow/blob/45658a8963761ce8a565b481156c847e493fce67/airflow/providers/apache/spark/hooks/spark_submit.py#L42

### What you think should happen instead

As an option, change the state of application with rest api
https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html

### How to reproduce

Use Spark connection type with deploy mode `cluster` and host 'yarn' fields.
Run  SparkSubmitOperator and mark state as ""failed"".

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",EugeneMSOff,2024-08-08 08:07:12+00:00,[],2024-09-25 00:14:56+00:00,2024-09-25 00:14:55+00:00,https://github.com/apache/airflow/issues/41324,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('area:providers', ''), ('kind:feature', 'Feature Requests'), ('good first issue', ''), ('pending-response', ''), ('provider:apache-spark', '')]","[{'comment_id': 2275207910, 'issue_id': 2455152949, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 8, 8, 7, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2295417146, 'issue_id': 2455152949, 'author': 'sunank200', 'body': ""@EugeneMSOff Is the YARN binary available in the PATH of your Airflow instance? It seems that the YARN binary may not be available in your current environment, which could be the reason you're unable to kill Spark applications on the YARN cluster using the `on_kill()` method. Would you like to attempt installing the YARN binary in your Docker setup and try again?\r\n\r\nIf you wish, you can create a PR to clarify that when YARN is used as the cluster manager (i.e., when --master yarn is specified), the YARN binary must be available in the PATH of the Airflow instance. This is necessary for operations such as killing YARN applications via the `on_kill()` method in `SparkSubmitHook`.\r\n\r\nAlternatively, we could consider adding a new feature, using the YARN ResourceManager REST API to manage the application state (e.g., killing), if the YARN binary is unavailable in `on_kill()` method."", 'created_at': datetime.datetime(2024, 8, 18, 22, 25, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2323558051, 'issue_id': 2455152949, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 9, 2, 0, 14, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2324575032, 'issue_id': 2455152949, 'author': 'EugeneMSOff', 'body': ""@sunank200 first of all, thanks for your attention.\r\n\r\nAs I said, such binary is not in PATH of my Airflow instance.\r\n`yarn` tool is provided by `hadoop` installation.\r\nIf I just copy it to Airflow instance, it won't be enough"", 'created_at': datetime.datetime(2024, 9, 2, 11, 56, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2357237747, 'issue_id': 2455152949, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 9, 18, 0, 14, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2372612099, 'issue_id': 2455152949, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 9, 25, 0, 14, 55, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-08 08:07:16 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

sunank200 on (2024-08-18 22:25:39 UTC): @EugeneMSOff Is the YARN binary available in the PATH of your Airflow instance? It seems that the YARN binary may not be available in your current environment, which could be the reason you're unable to kill Spark applications on the YARN cluster using the `on_kill()` method. Would you like to attempt installing the YARN binary in your Docker setup and try again?

If you wish, you can create a PR to clarify that when YARN is used as the cluster manager (i.e., when --master yarn is specified), the YARN binary must be available in the PATH of the Airflow instance. This is necessary for operations such as killing YARN applications via the `on_kill()` method in `SparkSubmitHook`.

Alternatively, we could consider adding a new feature, using the YARN ResourceManager REST API to manage the application state (e.g., killing), if the YARN binary is unavailable in `on_kill()` method.

github-actions[bot] on (2024-09-02 00:14:45 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

EugeneMSOff (Issue Creator) on (2024-09-02 11:56:10 UTC): @sunank200 first of all, thanks for your attention.

As I said, such binary is not in PATH of my Airflow instance.
`yarn` tool is provided by `hadoop` installation.
If I just copy it to Airflow instance, it won't be enough

github-actions[bot] on (2024-09-18 00:14:36 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-09-25 00:14:55 UTC): This issue has been closed because it has not received response from the issue author.

"
2453794684,issue,open,,Airflow UI Event Log Notification,"### Description

@o-nikolas @vincbeck 

During the town hall meeting - it was brought up that important scheduler events in the Event Tab may not be easily visible or discoverable, with folks used to looking at the task logs. 

One idea, was to have a count of Event Logs associated with a particular task run - e.g. `[Event Logs (5)]`

### Use case/motivation

_No response_

### Related issues
https://github.com/apache/airflow/pull/40967
https://github.com/apache/airflow/pull/40867

### Are you willing to submit a PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",fritz-astronomer,2024-08-07 15:39:46+00:00,[],2024-09-27 16:03:38+00:00,,https://github.com/apache/airflow/issues/41312,"[('kind:feature', 'Feature Requests'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]",[],
2453374651,issue,closed,completed,Airflow DAG throws GlueJobOperator is not JSON serializable,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

2.8.1

### What happened?

Below Airflow task throws an error:

```
[2024-08-07T09:05:00.142+0000] {{xcom.py:664}} ERROR - Object of type GlueJobOperator is not JSON serializable. If you are using pickle instead of JSON for XCom, then you need to enable pickle support for XCom in your airflow config or make sure to decorate your object with attr.
```


Code:

```
@task
def lag_tasks_with_filter(a, b, c, d, e, f, g, h):

    return GlueJobOperator(
        task_id=f""create_lags_task_{a}_{b}_w{c}_lag{d}_filter{e}"",
        job_name=config.generate_job_name(f""lag{d}-weeks{c}-"" + f""filter{e}-job-{a}-{b}""),
        script_location=config.get_bridge_script(""lags_bridge_script.py""),
        iam_role_name=f,
        script_args={
                ""--lagWithCatPath"": f""s3://{g}/output/with_cat"" + f""/a={a}/demographic={b}"",
                ""--rawDataInputPath"": f""s3://{h}/output/oneyear"" + f""/a={a}/demographic_code={b}/"",
                ""--numberOfLagWeeks"": str(d),
                ""--windowSizeWeeks"": str(create_job_kwargs),
                ""--filterCol"": e,
                ""--taskId"": f""create_lags_task_{a}_{b}_w{c}_lag{d}_filter{e}"",    
        },
        create_job_kwargs={
                ""WorkerType"": ""G.2X"",
                ""NumberOfWorkers"": 5,
                ""GlueVersion"": ""4.0"",
                ""DefaultArguments"": {
                                    ""--job-language"": ""python"",
                                    ""--enable-job-insights"": ""true"",
                                    ""--enable-metrics"": ""true"",
                                    ""--enable-auto-scaling"": ""true"",
                                    ""--enable-observability-metrics"": ""true"",
                                    ""--TempDir"": f""s3://{config.get_environment_variable('glue_tmp_dir_location', default_var='undefined')}"",
                                    ""--extra-py-files"": config.get_asset_file_location(
                                        ""ctc_telligence_forecasting_data_product-0.0.1-py3-none-any.whl""
                                    ),
                                    ""--enable-spark-ui"": ""true"",
                                    ""--spark-event-logs-path"": f""s3://{config.get_environment_variable('glue_spark_ui_logs_location', default_var='undefined')}"",
                                },
        },
        update_config=True,
    )

ts = DummyOperator(task_id='start')
te = DummyOperator(task_id='end')
t1 = lag_tasks_with_filter.partial(f=stage3_task_role, g=intermittent_data_location, h=playground_bucket).expand(a=as, b=bs, c=cs, d=ds, e=es)


# setting dependencies
ts >> t1 >> te
```

When removing return, DAG passes but Glue jobs don't get created and triggered. I want to keep @task decorator syntax since it allows for creating mapped instances with expand().

Thanks in advance for any help!

### What you think should happen instead?

Glue jobs should get created in AWS.

### How to reproduce

Please use above provided code for `@task`.

### Operating System

NA

### Versions of Apache Airflow Providers

_No response_

### Deployment

Amazon (AWS) MWAA

### Deployment details

Airflow version == 2.8.1

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",mszpot-future-processing,2024-08-07 12:34:03+00:00,[],2024-08-16 14:00:53+00:00,2024-08-08 07:26:34+00:00,https://github.com/apache/airflow/issues/41306,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2273366802, 'issue_id': 2453374651, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 7, 12, 34, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275130163, 'issue_id': 2453374651, 'author': 'Lee-W', 'body': 'Hi, I think this is expected behavior. As stated in https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html#using-the-taskflow-api-with-complex-conflicting-python-dependencies, `you have to make sure the functions are serializable and that they only use local imports for additional dependencies you use`.', 'created_at': datetime.datetime(2024, 8, 8, 7, 24, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275132477, 'issue_id': 2453374651, 'author': 'Lee-W', 'body': ""I think https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/dynamic-task-mapping.html#mapping-over-result-of-classic-operators is what you're looking for."", 'created_at': datetime.datetime(2024, 8, 8, 7, 26, 11, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275133053, 'issue_id': 2453374651, 'author': 'Lee-W', 'body': ""I'll close this one. Feel free to reopen if there's more to discuss. Thanks 🙂"", 'created_at': datetime.datetime(2024, 8, 8, 7, 26, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275165669, 'issue_id': 2453374651, 'author': 'mszpot-future-processing', 'body': 'I rewrote the script:\r\n\r\n```\r\ndef lag_tasks_with_filter(\r\n        channel,\r\n        demo,\r\n        window_size,\r\n        lag_week,\r\n        filter_col,\r\n        lag_task_role,\r\n        intermittent_data_location,\r\n        playground_bucket\r\n    ):\r\n\r\n    return GlueJobOperator(\r\n            task_id=f""create_lags_task_{channel}_{demo}_w{window_size}_lag{lag_week}_filter{filter_col}"",\r\n            job_name=config.generate_job_name(f""lag{lag_week}-weeks{window_size}-"" + f""filter{filter_col}-job-{channel}-{demo}""),\r\n            script_location=config.get_bridge_script(""lags_bridge_script.py""),\r\n            iam_role_name=lag_task_role,\r\n            script_args={\r\n                    ""--lagWithCatPath"": f""s3://{intermittent_data_location}/output/with_cat"" + f""/channel={channel}/demographic={demo}"",\r\n                    ""--rawDataInputPath"": f""s3://{playground_bucket}/output/oneyear"" + f""/channel={channel}/demographic_code={demo}/"",\r\n                    ""--numberOfLagWeeks"": str(lag_week),\r\n                    ""--windowSizeWeeks"": str(window_size),\r\n                    ""--filterCol"": filter_col,\r\n                    ""--taskId"": f""create_lags_task_{channel}_{demo}_w{window_size}_lag{lag_week}_filter{filter_col}"",    \r\n            },\r\n            create_job_kwargs={\r\n                    ""WorkerType"": ""G.2X"",\r\n                    ""NumberOfWorkers"": 5,\r\n                    ""GlueVersion"": ""4.0"",\r\n                    ""DefaultArguments"": {\r\n                                        ""--job-language"": ""python"",\r\n                                        ""--enable-job-insights"": ""true"",\r\n                                        ""--enable-metrics"": ""true"",\r\n                                        ""--enable-auto-scaling"": ""true"",\r\n                                        ""--enable-observability-metrics"": ""true"",\r\n                                        ""--TempDir"": f""s3://{config.get_environment_variable(\'glue_tmp_dir_location\', default_var=\'undefined\')}"",\r\n                                        ""--extra-py-files"": config.get_asset_file_location(\r\n                                            ""ctc_telligence_forecasting_data_product-0.0.1-py3-none-any.whl""\r\n                                        ),\r\n                                        ""--enable-spark-ui"": ""true"",\r\n                                        ""--spark-event-logs-path"": f""s3://{config.get_environment_variable(\'glue_spark_ui_logs_location\', default_var=\'undefined\')}"",\r\n                                    },\r\n            },\r\n            update_config=True,\r\n        )\r\n\r\n@dag(dag_id=\'chore_task_group_stage3\', schedule=None, catchup=False)\r\ndef pipeline():\r\n\r\n    ts = DummyOperator(task_id=\'start\')\r\n    te = DummyOperator(task_id=\'end\')\r\n    t1 = lag_tasks_with_filter.partial(lag_task_role=stage3_task_role, intermittent_data_location=intermittent_data_location, playground_bucket=playground_bucket).expand(channel=channels, demo=demos, window_size=window_sizes, lag_week=lag_weeks, filter_col=filter_cols)\r\n\r\n    ts >> t1 >> te\r\n\r\npipeline()\r\n```\r\n\r\nBut now getting:\r\n```\r\nAttributeError: \'function\' object has no attribute \'partial\'\r\n```', 'created_at': datetime.datetime(2024, 8, 8, 7, 44, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275175673, 'issue_id': 2453374651, 'author': 'mszpot-future-processing', 'body': ""@Lee-W , I can't re-open the issue. Are you able to do so?"", 'created_at': datetime.datetime(2024, 8, 8, 7, 49, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275185828, 'issue_id': 2453374651, 'author': 'Lee-W', 'body': 'Instead of returning an operator through taskflow and than use `partial`, we should use `partial` directly on the operator instead', 'created_at': datetime.datetime(2024, 8, 8, 7, 55, 39, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275205447, 'issue_id': 2453374651, 'author': 'mszpot-future-processing', 'body': 'I added `return GlueJobOperator(.....).partial(...` and got new error:\r\n\r\n```\r\nBroken DAG: [/usr/local/airflow/dags/chore-check-task-group/stage3.py] Traceback (most recent call last):\r\n  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/models/taskmixin.py"", line 262, in set_downstream\r\n    self._set_relatives(task_or_task_list, upstream=False, edge_modifier=edge_modifier)\r\n  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/models/taskmixin.py"", line 214, in _set_relatives\r\n    task_object.update_relative(self, not upstream, edge_modifier=edge_modifier)\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nAttributeError: \'function\' object has no attribute \'update_relative\'\r\n```', 'created_at': datetime.datetime(2024, 8, 8, 8, 5, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275212169, 'issue_id': 2453374651, 'author': 'Lee-W', 'body': ""Something like the following should be used instead\r\n\r\n```python\r\nts = DummyOperator(task_id='start')\r\nte = DummyOperator(task_id='end')\r\nt1 = GlueJobOperator(.....).partial(...).expand(...)\r\n\r\n# setting dependencies\r\nts >> t1 >> te\r\n```"", 'created_at': datetime.datetime(2024, 8, 8, 8, 9, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275221052, 'issue_id': 2453374651, 'author': 'mszpot-future-processing', 'body': 'And what about variables I use inside GlueOperator?\r\nWill test below:\r\n\r\n```\r\nt1 = GlueJobOperator(\r\n            task_id=f""create_task_{abc}_{def}"",\r\n            job_name=config.generate_job_name(f""job{abc}-{def}""),\r\n            script_location=config.get_bridge_script(""bridge_script.py""),\r\n            iam_role_name=lag_task_role....).partial(abc=abc).expand(def=def....\r\n```', 'created_at': datetime.datetime(2024, 8, 8, 8, 14, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275225397, 'issue_id': 2453374651, 'author': 'Lee-W', 'body': 'We can use another operator (probably Python or Bash?) to generate the list of args https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/dynamic-task-mapping.html#task-generated-mapping', 'created_at': datetime.datetime(2024, 8, 8, 8, 16, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275232863, 'issue_id': 2453374651, 'author': 'mszpot-future-processing', 'body': 'Will play around with it but the goal is to have n gluejobs generated with various values injected with `expand`', 'created_at': datetime.datetime(2024, 8, 8, 8, 20, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275285089, 'issue_id': 2453374651, 'author': 'mszpot-future-processing', 'body': ""Generating list of args won't help because I need to iterate n-gluejobs with various parms. It seems I may need to go with classic loop without dyanmic mapping."", 'created_at': datetime.datetime(2024, 8, 8, 8, 47, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275298066, 'issue_id': 2453374651, 'author': 'Lee-W', 'body': 'The list you generate and passes into `expand` will later be used to generate n-gluejobs\r\n\r\n```python\r\nts = DummyOperator(task_id=\'start\')\r\nte = DummyOperator(task_id=\'end\')\r\nt1 = GlueJobOperator(.....).partial(...).expand(x=[1, 2, 3])\r\n\r\n\r\nts >> t1 >> te\r\n\r\n# will be translated in to something similar to ""ts >> [t1, t1, t1] >> te""\r\n# but x will be passed separately\r\n```\r\n\r\nif you\'re able to use classic loop, I think dynamic task mapping should work 🤔', 'created_at': datetime.datetime(2024, 8, 8, 8, 53, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275308633, 'issue_id': 2453374651, 'author': 'mszpot-future-processing', 'body': ""This approach leaves unrecognized parameter. Inside GlueJobOperator I'm suing few variables. Even adding expand ends with error because:\r\n\r\n```\r\nNameError: name 'abc_parameter' is not defined\r\n```"", 'created_at': datetime.datetime(2024, 8, 8, 8, 59, 21, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275324098, 'issue_id': 2453374651, 'author': 'Lee-W', 'body': ""I don't quite understand 🤔  Could you please share an example? Thanks"", 'created_at': datetime.datetime(2024, 8, 8, 9, 6, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275332823, 'issue_id': 2453374651, 'author': 'mszpot-future-processing', 'body': ""Will do so but a bit later if that's okay."", 'created_at': datetime.datetime(2024, 8, 8, 9, 10, 58, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275995248, 'issue_id': 2453374651, 'author': 'mszpot-future-processing', 'body': 'Ok, @Lee-W, goal is to have single task instance that will create n-number of glue jobs using `expand` method.\r\n\r\n![obraz](https://github.com/user-attachments/assets/817f11cd-bade-454a-a1d7-016fff8ee94d)\r\n![obraz](https://github.com/user-attachments/assets/cdca7da4-4928-44bc-9497-32811d3ef472)\r\n\r\nEach Glue will have a set of static arguments (`partial`), rest is going to be injected with `expand`.\r\n\r\nCurrent code I got is as follows and fails due to `return` not being able to serialize `GlueJobOperator`:\r\n\r\n\r\n```\r\nimport os\r\nimport sys\r\nfrom datetime import datetime, timedelta\r\n\r\nfrom airflow import DAG\r\nfrom airflow.operators.dummy_operator import DummyOperator\r\nfrom airflow.providers.amazon.aws.operators.glue import GlueJobOperator\r\nfrom airflow.utils.task_group import TaskGroup\r\nfrom airflow.decorators import task_group, task, dag\r\nfrom airflow.operators.python import PythonOperator\r\n\r\nsys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))\r\n\r\nfrom utils.environment_config import EnvironmentConfig  # noqa: E402\r\n\r\nconfig = EnvironmentConfig(__file__)\r\n\r\nimport json\r\n\r\n\r\nparams_one = [""value""]\r\nparams_two = [""1"",""2""]\r\n\r\nparams_three = [4, 12, 52]\r\nparams_four = [3]\r\nparam_five = [""col""]\r\n\r\nplayground_bucket = config.get_environment_variable(""playground_bucket_name"", default_var=""undefined"")\r\nintermittent_data_location = config.get_environment_variable(""stage3_output_intermittent_location"", default_var=""undefined"")\r\nstage3_task_role = config.get_environment_variable(""stage3_task_role"", default_var=""undefined"")\r\njoin_bridge_script = config.get_bridge_script(""join_bridge_script.py"")\r\n\r\n\r\n#default_args={ ""donot_pickle"": ""True"" }\r\n@dag(dag_id=\'chore_task_group_stage3\', schedule=None, catchup=False)\r\ndef pipeline():\r\n\r\n    @task\r\n    def lag_tasks_with_filter(\r\n        param_one,\r\n        demo,\r\n        param_three,\r\n        param_four,\r\n        ,\r\n        lag_task_role,\r\n        intermittent_data_location,\r\n        playground_bucket\r\n    ):\r\n\r\n        return GlueJobOperator(\r\n            task_id=f""create_task_{param_one}_{param_two}_w{param_three}param_four{param_four}param_five{param_five}"",\r\n            job_name=config.generate_job_name(f""param_four{param_four}-weeks{param_three}-"" + f""filter{param_five}-job-{param_one}-{param_two}""),\r\n            script_location=config.get_bridge_script(""lags_bridge_script.py""),\r\n            iam_role_name=lag_task_role,\r\n            script_args={\r\n                    ""--lagWithCatPath"": f""s3://{intermittent_data_location}/output/with_cat"" + f""/param_one={param_one}/param_two={param_two}"",\r\n                    ""--rawDataInputPath"": f""s3://{playground_bucket}/output/oneyear"" + f""/param_one={param_one}/param_two={param_two}/"",\r\n                    ""--numberOfLagWeeks"": str(param_four),\r\n                    ""--windowSizeWeeks"": str(param_three),\r\n                    ""--filterCol"": param_five,\r\n                    ""--taskId"": f""create_task_{param_one}_{param_two}_w{param_three}param_four{param_four}param_five{param_five}"",    \r\n            },\r\n            create_job_kwargs={\r\n                    ""WorkerType"": ""G.2X"",\r\n                    ""NumberOfWorkers"": 5,\r\n                    ""GlueVersion"": ""4.0"",\r\n                    ""DefaultArguments"": {\r\n                                        ""--job-language"": ""python"",\r\n                                        ""--enable-job-insights"": ""true"",\r\n                                        ""--enable-metrics"": ""true"",\r\n                                        ""--enable-auto-scaling"": ""true"",\r\n                                        ""--enable-observability-metrics"": ""true"",\r\n                                        ""--TempDir"": f""s3://{config.get_environment_variable(\'glue_tmp_dir_location\', default_var=\'undefined\')}"",\r\n                                        ""--extra-py-files"": config.get_asset_file_location(\r\n                                            ""ctc_telligence_forecasting_data_product-0.0.1-py3-none-any.whl""\r\n                                        ),\r\n                                        ""--enable-spark-ui"": ""true"",\r\n                                        ""--spark-event-logs-path"": f""s3://{config.get_environment_variable(\'glue_spark_ui_logs_location\', default_var=\'undefined\')}"",\r\n                                    },\r\n            },\r\n            update_config=True,\r\n        )\r\n\r\n\r\n    ts = DummyOperator(task_id=\'start\')\r\n    te = DummyOperator(task_id=\'end\')\r\n    t1 = lag_tasks_with_filter.partial(lag_task_role=stage3_task_role, intermittent_data_location=intermittent_data_location, playground_bucket=playground_bucket).expand(param_one=params_one, param_two=params_two, param_three=params_three, param_four=params_four, param_five=param_five)\r\n\r\n    # setting dependencies\r\n    ts >> t1 >> te\r\n\r\npipeline()\r\n```', 'created_at': datetime.datetime(2024, 8, 8, 14, 37, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2276100211, 'issue_id': 2453374651, 'author': 'Lee-W', 'body': ""I think https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/dynamic-task-mapping.html#assigning-multiple-parameters-to-a-non-taskflow-operator is something you're looking for them. As those value are static, you can just use a for look to generate that list of dict and pass it"", 'created_at': datetime.datetime(2024, 8, 8, 15, 23, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277252567, 'issue_id': 2453374651, 'author': 'mszpot-future-processing', 'body': 'I tried that approach, `expand` returned an error that method is not recognized.', 'created_at': datetime.datetime(2024, 8, 9, 6, 41, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2278118901, 'issue_id': 2453374651, 'author': 'Lee-W', 'body': 'what about making that method a task 🤔', 'created_at': datetime.datetime(2024, 8, 9, 14, 46, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2278122797, 'issue_id': 2453374651, 'author': 'mszpot-future-processing', 'body': 'But is already decorated with such :)', 'created_at': datetime.datetime(2024, 8, 9, 14, 47, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2278149828, 'issue_id': 2453374651, 'author': 'Lee-W', 'body': 'forget about making the method a task. that\'s wrong. something like the following should work. you just need to rewrite `gen_kwargs` and use `GlueJobOperator` instead \r\n\r\n```python\r\nfrom __future__ import annotations\r\n\r\nfrom datetime import datetime\r\n\r\nfrom airflow import DAG\r\nfrom airflow.operators.python import PythonOperator\r\n\r\n\r\ndef print_args(x, y):\r\n    print(x)\r\n    print(y)\r\n    return x + y\r\n\r\n\r\ndef gen_kwargs():\r\n    return [\r\n        {""op_kwargs"": {""x"": 1, ""y"": 2}, ""show_return_value_in_logs"": True},\r\n        {""op_kwargs"": {""x"": 3, ""y"": 4}, ""show_return_value_in_logs"": False},\r\n    ]\r\n\r\n\r\nwith DAG(dag_id=""mapped_python"", start_date=datetime(2020, 4, 7), catchup=False) as dag:\r\n    PythonOperator.partial(task_id=""task-1"", python_callable=print_args).expand_kwargs(gen_kwargs())\r\n```', 'created_at': datetime.datetime(2024, 8, 9, 14, 57, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2278169981, 'issue_id': 2453374651, 'author': 'mszpot-future-processing', 'body': ""I did exactly that but got error that `param_one` is not recognized because it's used inside `GlueJobOperator` before being called with `expand`. Maybe I'm missing sth so will try and let you know later.\r\n\r\nGreat thanks for your commitment to help. I do appreciate the effort."", 'created_at': datetime.datetime(2024, 8, 9, 15, 7, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2293567440, 'issue_id': 2453374651, 'author': 'mszpot-future-processing', 'body': ""@Lee-W - working solution in below post:\r\nhttps://stackoverflow.com/questions/78842962/airflow-dag-throws-gluejoboperator-is-not-json-serializable\r\n\r\nIt was a matter of taking mappings into a seperate function and calling it with `partial` and `expand_kwargs` on `GlueJobOperator`.\r\nCombination from solution didn't come to my mind."", 'created_at': datetime.datetime(2024, 8, 16, 14, 0, 52, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-07 12:34:06 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

Lee-W on (2024-08-08 07:24:40 UTC): Hi, I think this is expected behavior. As stated in https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html#using-the-taskflow-api-with-complex-conflicting-python-dependencies, `you have to make sure the functions are serializable and that they only use local imports for additional dependencies you use`.

Lee-W on (2024-08-08 07:26:11 UTC): I think https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/dynamic-task-mapping.html#mapping-over-result-of-classic-operators is what you're looking for.

Lee-W on (2024-08-08 07:26:34 UTC): I'll close this one. Feel free to reopen if there's more to discuss. Thanks 🙂

mszpot-future-processing (Issue Creator) on (2024-08-08 07:44:54 UTC): I rewrote the script:

```
def lag_tasks_with_filter(
        channel,
        demo,
        window_size,
        lag_week,
        filter_col,
        lag_task_role,
        intermittent_data_location,
        playground_bucket
    ):

    return GlueJobOperator(
            task_id=f""create_lags_task_{channel}_{demo}_w{window_size}_lag{lag_week}_filter{filter_col}"",
            job_name=config.generate_job_name(f""lag{lag_week}-weeks{window_size}-"" + f""filter{filter_col}-job-{channel}-{demo}""),
            script_location=config.get_bridge_script(""lags_bridge_script.py""),
            iam_role_name=lag_task_role,
            script_args={
                    ""--lagWithCatPath"": f""s3://{intermittent_data_location}/output/with_cat"" + f""/channel={channel}/demographic={demo}"",
                    ""--rawDataInputPath"": f""s3://{playground_bucket}/output/oneyear"" + f""/channel={channel}/demographic_code={demo}/"",
                    ""--numberOfLagWeeks"": str(lag_week),
                    ""--windowSizeWeeks"": str(window_size),
                    ""--filterCol"": filter_col,
                    ""--taskId"": f""create_lags_task_{channel}_{demo}_w{window_size}_lag{lag_week}_filter{filter_col}"",    
            },
            create_job_kwargs={
                    ""WorkerType"": ""G.2X"",
                    ""NumberOfWorkers"": 5,
                    ""GlueVersion"": ""4.0"",
                    ""DefaultArguments"": {
                                        ""--job-language"": ""python"",
                                        ""--enable-job-insights"": ""true"",
                                        ""--enable-metrics"": ""true"",
                                        ""--enable-auto-scaling"": ""true"",
                                        ""--enable-observability-metrics"": ""true"",
                                        ""--TempDir"": f""s3://{config.get_environment_variable('glue_tmp_dir_location', default_var='undefined')}"",
                                        ""--extra-py-files"": config.get_asset_file_location(
                                            ""ctc_telligence_forecasting_data_product-0.0.1-py3-none-any.whl""
                                        ),
                                        ""--enable-spark-ui"": ""true"",
                                        ""--spark-event-logs-path"": f""s3://{config.get_environment_variable('glue_spark_ui_logs_location', default_var='undefined')}"",
                                    },
            },
            update_config=True,
        )

@dag(dag_id='chore_task_group_stage3', schedule=None, catchup=False)
def pipeline():

    ts = DummyOperator(task_id='start')
    te = DummyOperator(task_id='end')
    t1 = lag_tasks_with_filter.partial(lag_task_role=stage3_task_role, intermittent_data_location=intermittent_data_location, playground_bucket=playground_bucket).expand(channel=channels, demo=demos, window_size=window_sizes, lag_week=lag_weeks, filter_col=filter_cols)

    ts >> t1 >> te

pipeline()
```

But now getting:
```
AttributeError: 'function' object has no attribute 'partial'
```

mszpot-future-processing (Issue Creator) on (2024-08-08 07:49:57 UTC): @Lee-W , I can't re-open the issue. Are you able to do so?

Lee-W on (2024-08-08 07:55:39 UTC): Instead of returning an operator through taskflow and than use `partial`, we should use `partial` directly on the operator instead

mszpot-future-processing (Issue Creator) on (2024-08-08 08:05:57 UTC): I added `return GlueJobOperator(.....).partial(...` and got new error:

```
Broken DAG: [/usr/local/airflow/dags/chore-check-task-group/stage3.py] Traceback (most recent call last):
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/models/taskmixin.py"", line 262, in set_downstream
    self._set_relatives(task_or_task_list, upstream=False, edge_modifier=edge_modifier)
  File ""/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/models/taskmixin.py"", line 214, in _set_relatives
    task_object.update_relative(self, not upstream, edge_modifier=edge_modifier)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'function' object has no attribute 'update_relative'
```

Lee-W on (2024-08-08 08:09:35 UTC): Something like the following should be used instead

```python
ts = DummyOperator(task_id='start')
te = DummyOperator(task_id='end')
t1 = GlueJobOperator(.....).partial(...).expand(...)

# setting dependencies
ts >> t1 >> te
```

mszpot-future-processing (Issue Creator) on (2024-08-08 08:14:17 UTC): And what about variables I use inside GlueOperator?
Will test below:

```
t1 = GlueJobOperator(
            task_id=f""create_task_{abc}_{def}"",
            job_name=config.generate_job_name(f""job{abc}-{def}""),
            script_location=config.get_bridge_script(""bridge_script.py""),
            iam_role_name=lag_task_role....).partial(abc=abc).expand(def=def....
```

Lee-W on (2024-08-08 08:16:42 UTC): We can use another operator (probably Python or Bash?) to generate the list of args https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/dynamic-task-mapping.html#task-generated-mapping

mszpot-future-processing (Issue Creator) on (2024-08-08 08:20:26 UTC): Will play around with it but the goal is to have n gluejobs generated with various values injected with `expand`

mszpot-future-processing (Issue Creator) on (2024-08-08 08:47:08 UTC): Generating list of args won't help because I need to iterate n-gluejobs with various parms. It seems I may need to go with classic loop without dyanmic mapping.

Lee-W on (2024-08-08 08:53:57 UTC): The list you generate and passes into `expand` will later be used to generate n-gluejobs

```python
ts = DummyOperator(task_id='start')
te = DummyOperator(task_id='end')
t1 = GlueJobOperator(.....).partial(...).expand(x=[1, 2, 3])


ts >> t1 >> te

# will be translated in to something similar to ""ts >> [t1, t1, t1] >> te""
# but x will be passed separately
```

if you're able to use classic loop, I think dynamic task mapping should work 🤔

mszpot-future-processing (Issue Creator) on (2024-08-08 08:59:21 UTC): This approach leaves unrecognized parameter. Inside GlueJobOperator I'm suing few variables. Even adding expand ends with error because:

```
NameError: name 'abc_parameter' is not defined
```

Lee-W on (2024-08-08 09:06:42 UTC): I don't quite understand 🤔  Could you please share an example? Thanks

mszpot-future-processing (Issue Creator) on (2024-08-08 09:10:58 UTC): Will do so but a bit later if that's okay.

mszpot-future-processing (Issue Creator) on (2024-08-08 14:37:40 UTC): Ok, @Lee-W, goal is to have single task instance that will create n-number of glue jobs using `expand` method.

![obraz](https://github.com/user-attachments/assets/817f11cd-bade-454a-a1d7-016fff8ee94d)
![obraz](https://github.com/user-attachments/assets/cdca7da4-4928-44bc-9497-32811d3ef472)

Each Glue will have a set of static arguments (`partial`), rest is going to be injected with `expand`.

Current code I got is as follows and fails due to `return` not being able to serialize `GlueJobOperator`:


```
import os
import sys
from datetime import datetime, timedelta

from airflow import DAG
from airflow.operators.dummy_operator import DummyOperator
from airflow.providers.amazon.aws.operators.glue import GlueJobOperator
from airflow.utils.task_group import TaskGroup
from airflow.decorators import task_group, task, dag
from airflow.operators.python import PythonOperator

sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))

from utils.environment_config import EnvironmentConfig  # noqa: E402

config = EnvironmentConfig(__file__)

import json


params_one = [""value""]
params_two = [""1"",""2""]

params_three = [4, 12, 52]
params_four = [3]
param_five = [""col""]

playground_bucket = config.get_environment_variable(""playground_bucket_name"", default_var=""undefined"")
intermittent_data_location = config.get_environment_variable(""stage3_output_intermittent_location"", default_var=""undefined"")
stage3_task_role = config.get_environment_variable(""stage3_task_role"", default_var=""undefined"")
join_bridge_script = config.get_bridge_script(""join_bridge_script.py"")


#default_args={ ""donot_pickle"": ""True"" }
@dag(dag_id='chore_task_group_stage3', schedule=None, catchup=False)
def pipeline():

    @task
    def lag_tasks_with_filter(
        param_one,
        demo,
        param_three,
        param_four,
        ,
        lag_task_role,
        intermittent_data_location,
        playground_bucket
    ):

        return GlueJobOperator(
            task_id=f""create_task_{param_one}_{param_two}_w{param_three}param_four{param_four}param_five{param_five}"",
            job_name=config.generate_job_name(f""param_four{param_four}-weeks{param_three}-"" + f""filter{param_five}-job-{param_one}-{param_two}""),
            script_location=config.get_bridge_script(""lags_bridge_script.py""),
            iam_role_name=lag_task_role,
            script_args={
                    ""--lagWithCatPath"": f""s3://{intermittent_data_location}/output/with_cat"" + f""/param_one={param_one}/param_two={param_two}"",
                    ""--rawDataInputPath"": f""s3://{playground_bucket}/output/oneyear"" + f""/param_one={param_one}/param_two={param_two}/"",
                    ""--numberOfLagWeeks"": str(param_four),
                    ""--windowSizeWeeks"": str(param_three),
                    ""--filterCol"": param_five,
                    ""--taskId"": f""create_task_{param_one}_{param_two}_w{param_three}param_four{param_four}param_five{param_five}"",    
            },
            create_job_kwargs={
                    ""WorkerType"": ""G.2X"",
                    ""NumberOfWorkers"": 5,
                    ""GlueVersion"": ""4.0"",
                    ""DefaultArguments"": {
                                        ""--job-language"": ""python"",
                                        ""--enable-job-insights"": ""true"",
                                        ""--enable-metrics"": ""true"",
                                        ""--enable-auto-scaling"": ""true"",
                                        ""--enable-observability-metrics"": ""true"",
                                        ""--TempDir"": f""s3://{config.get_environment_variable('glue_tmp_dir_location', default_var='undefined')}"",
                                        ""--extra-py-files"": config.get_asset_file_location(
                                            ""ctc_telligence_forecasting_data_product-0.0.1-py3-none-any.whl""
                                        ),
                                        ""--enable-spark-ui"": ""true"",
                                        ""--spark-event-logs-path"": f""s3://{config.get_environment_variable('glue_spark_ui_logs_location', default_var='undefined')}"",
                                    },
            },
            update_config=True,
        )


    ts = DummyOperator(task_id='start')
    te = DummyOperator(task_id='end')
    t1 = lag_tasks_with_filter.partial(lag_task_role=stage3_task_role, intermittent_data_location=intermittent_data_location, playground_bucket=playground_bucket).expand(param_one=params_one, param_two=params_two, param_three=params_three, param_four=params_four, param_five=param_five)

    # setting dependencies
    ts >> t1 >> te

pipeline()
```

Lee-W on (2024-08-08 15:23:44 UTC): I think https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/dynamic-task-mapping.html#assigning-multiple-parameters-to-a-non-taskflow-operator is something you're looking for them. As those value are static, you can just use a for look to generate that list of dict and pass it

mszpot-future-processing (Issue Creator) on (2024-08-09 06:41:59 UTC): I tried that approach, `expand` returned an error that method is not recognized.

Lee-W on (2024-08-09 14:46:01 UTC): what about making that method a task 🤔

mszpot-future-processing (Issue Creator) on (2024-08-09 14:47:28 UTC): But is already decorated with such :)

Lee-W on (2024-08-09 14:57:42 UTC): forget about making the method a task. that's wrong. something like the following should work. you just need to rewrite `gen_kwargs` and use `GlueJobOperator` instead 

```python
from __future__ import annotations

from datetime import datetime

from airflow import DAG
from airflow.operators.python import PythonOperator


def print_args(x, y):
    print(x)
    print(y)
    return x + y


def gen_kwargs():
    return [
        {""op_kwargs"": {""x"": 1, ""y"": 2}, ""show_return_value_in_logs"": True},
        {""op_kwargs"": {""x"": 3, ""y"": 4}, ""show_return_value_in_logs"": False},
    ]


with DAG(dag_id=""mapped_python"", start_date=datetime(2020, 4, 7), catchup=False) as dag:
    PythonOperator.partial(task_id=""task-1"", python_callable=print_args).expand_kwargs(gen_kwargs())
```

mszpot-future-processing (Issue Creator) on (2024-08-09 15:07:44 UTC): I did exactly that but got error that `param_one` is not recognized because it's used inside `GlueJobOperator` before being called with `expand`. Maybe I'm missing sth so will try and let you know later.

Great thanks for your commitment to help. I do appreciate the effort.

mszpot-future-processing (Issue Creator) on (2024-08-16 14:00:52 UTC): @Lee-W - working solution in below post:
https://stackoverflow.com/questions/78842962/airflow-dag-throws-gluejoboperator-is-not-json-serializable

It was a matter of taking mappings into a seperate function and calling it with `partial` and `expand_kwargs` on `GlueJobOperator`.
Combination from solution didn't come to my mind.

"
2453315238,issue,closed,not_planned,DatasetOrTimeSchedule causes deadlock,"### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I want to use [`DatasetOrTimeSchedule` ](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/timetable.html#dataset-event-based-scheduling-with-time-based-scheduling) as a `schedule` in my DAG.

When I do that and wait a day for the schedule to trigger and also the dataset trigger from another DAG, I indeed see 2 DAG runs triggered, one triggered by the time and one by the dataset (has the dataset icon as well).
Now, the problem is, the time triggered one (displayed first) is `queued`, and the dataset triggered on is `running`, but all the tasks have `no status`. They keep being in this state ad infinitum without anything running.

What's also weird is that the time triggered one is queued just one second after the dataset triggered one:
Queued At: `2024-08-07, 02:01:45 CEST`
And the Dataset triggered one:
Queued at	`2024-08-07, 02:01:44 CEST`
Started	`2024-08-07, 02:01:44 CEST`

Which I find weird because it should've been queued at 02:00:00 CEST according to its schedule.

I should note I have `max_active_runs=1` and `depends_on_past=True`.

### What you think should happen instead?

The Dataset trigger should not influence the time schedule. They should run independently. The time schedule run should be queued and start running at its scheduled time. The tasks should actually run. There should be no deadlock.

### How to reproduce

From the docs:
```
from airflow.timetables.datasets import DatasetOrTimeSchedule
from airflow.timetables.trigger import CronTriggerTimetable


@dag(
    schedule=DatasetOrTimeSchedule(
        timetable=CronTriggerTimetable(""0 1 * * 3"", timezone=""UTC""), datasets=(dag1_dataset | dag2_dataset)
    )
    # Additional arguments here, replace this comment with actual arguments
)
def example_dag():
    # DAG tasks go here
    pass
```

Set `max_active_tasks=1` and `depends_on_past=True`.
Have a second DAG trigger this DAG with `dag1_dataset` or `dag2_dataset`

### Operating System

Our Airflow runs on kubernetes on EKS

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

We use conveyor https://docs.conveyordata.com/technical-reference/airflow/airflow-installation-details 

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",RobbertDM,2024-08-07 12:04:03+00:00,[],2024-09-12 00:14:36+00:00,2024-09-12 00:14:36+00:00,https://github.com/apache/airflow/issues/41305,"[('kind:bug', 'This is a clearly a bug'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('pending-response', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('area:datasets', 'Issues related to the datasets feature'), ('affected_version:2.9', 'Issues Reported for 2.9')]","[{'comment_id': 2273309598, 'issue_id': 2453315238, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 7, 12, 4, 8, tzinfo=datetime.timezone.utc)}, {'comment_id': 2295405814, 'issue_id': 2453315238, 'author': 'sunank200', 'body': 'I have following DAG:\r\n```\r\n\r\nwith DAG(\r\n    dag_id=""dataset_produces_1"",\r\n    catchup=False,\r\n    start_date=pendulum.datetime(2021, 1, 1, tz=""UTC""),\r\n    schedule=""@daily"",\r\n    tags=[""produces"", ""dataset-scheduled""],\r\n) as dag1:\r\n    # [START task_outlet]\r\n    BashOperator(outlets=[dag1_dataset], task_id=""producing_task_1"", bash_command=""sleep 5"", retries=2)\r\n    # [END task_outlet]\r\n\r\nwith DAG(\r\n    dag_id=""dataset_produces_2"",\r\n    catchup=False,\r\n    start_date=pendulum.datetime(2021, 1, 1, tz=""UTC""),\r\n    schedule=None,\r\n    tags=[""produces"", ""dataset-scheduled""],\r\n) as dag2:\r\n    BashOperator(outlets=[dag2_dataset], task_id=""producing_task_2"", bash_command=""sleep 10"")\r\n\r\ndefault_args = {\r\n    \'depends_on_past\': True,\r\n}\r\nwith DAG(\r\n    dag_id=""conditional_dataset_and_time_based_timetable"",\r\n    catchup=False,\r\n    start_date=pendulum.datetime(2021, 1, 1, tz=""UTC""),\r\n    schedule=DatasetOrTimeSchedule(\r\n        timetable=CronTriggerTimetable(""* * * * *"", timezone=""UTC""), datasets=(dag1_dataset & dag2_dataset)\r\n    ),\r\n    tags=[""dataset-time-based-timetable""],\r\n    max_active_tasks=1,\r\n    default_args=default_args,\r\n) as dag8:\r\n    BashOperator(\r\n        outlets=[Dataset(""s3://dataset_time_based/dataset_other_unknown.txt"")],\r\n        task_id=""conditional_dataset_and_time_based_timetable"",\r\n        bash_command=""sleep 5"",\r\n    )\r\n```\r\n\r\nBut this runs fine for me with the correct status for each task. I am running it locally using Breeze using a local executor. Is this correct step to replicate this @RobbertDM ?', 'created_at': datetime.datetime(2024, 8, 18, 21, 45, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2330357385, 'issue_id': 2453315238, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 9, 5, 0, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2345009616, 'issue_id': 2453315238, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 9, 12, 0, 14, 35, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-07 12:04:08 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

sunank200 on (2024-08-18 21:45:22 UTC): I have following DAG:
```

with DAG(
    dag_id=""dataset_produces_1"",
    catchup=False,
    start_date=pendulum.datetime(2021, 1, 1, tz=""UTC""),
    schedule=""@daily"",
    tags=[""produces"", ""dataset-scheduled""],
) as dag1:
    # [START task_outlet]
    BashOperator(outlets=[dag1_dataset], task_id=""producing_task_1"", bash_command=""sleep 5"", retries=2)
    # [END task_outlet]

with DAG(
    dag_id=""dataset_produces_2"",
    catchup=False,
    start_date=pendulum.datetime(2021, 1, 1, tz=""UTC""),
    schedule=None,
    tags=[""produces"", ""dataset-scheduled""],
) as dag2:
    BashOperator(outlets=[dag2_dataset], task_id=""producing_task_2"", bash_command=""sleep 10"")

default_args = {
    'depends_on_past': True,
}
with DAG(
    dag_id=""conditional_dataset_and_time_based_timetable"",
    catchup=False,
    start_date=pendulum.datetime(2021, 1, 1, tz=""UTC""),
    schedule=DatasetOrTimeSchedule(
        timetable=CronTriggerTimetable(""* * * * *"", timezone=""UTC""), datasets=(dag1_dataset & dag2_dataset)
    ),
    tags=[""dataset-time-based-timetable""],
    max_active_tasks=1,
    default_args=default_args,
) as dag8:
    BashOperator(
        outlets=[Dataset(""s3://dataset_time_based/dataset_other_unknown.txt"")],
        task_id=""conditional_dataset_and_time_based_timetable"",
        bash_command=""sleep 5"",
    )
```

But this runs fine for me with the correct status for each task. I am running it locally using Breeze using a local executor. Is this correct step to replicate this @RobbertDM ?

github-actions[bot] on (2024-09-05 00:14:00 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-09-12 00:14:35 UTC): This issue has been closed because it has not received response from the issue author.

"
2451214036,issue,closed,completed,taskflow does not understand * (argument unpack operator): ValueError: The key 'how' in args is a part of kwargs and therefore reserved.,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.2

### What happened?

Executing the attached repro in airflow results in a runtime error 

ValueError: The key 'how' in args is a part of kwargs and therefore reserved.


### What you think should happen instead?

This dag should execute properly

### How to reproduce

```
from airflow.decorators import dag, task
@dag(
    schedule=None,
    catchup=False,
    tags=[""example""],
)
def demo_bug():
    @task()
    def input1() -> str:
        return ""A""
    
    @task()
    def input2() -> str:
        return ""B""
    
    @task()
    def bug(*str, how=""cow""):
        pass

    bug(input1, input2, how=""now"")

demo_bug()
```

### Operating System

Ubuntu 22.04.4 LTS

### Versions of Apache Airflow Providers

None relevant

### Deployment

Virtualenv installation

### Deployment details

Nothing of note

### Anything else?

All the time

### Are you willing to submit PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",jabbera,2024-08-06 15:52:53+00:00,['jabbera'],2024-08-30 06:24:00+00:00,2024-08-30 06:24:00+00:00,https://github.com/apache/airflow/issues/41286,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:core', '')]","[{'comment_id': 2273810933, 'issue_id': 2451214036, 'author': 'atomic77', 'body': ""I've got this reproduced locally against `main` and would like to give it a try."", 'created_at': datetime.datetime(2024, 8, 7, 16, 0, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2274163460, 'issue_id': 2451214036, 'author': 'jabbera', 'body': ""> I've got this reproduced locally against `main` and would like to give it a try.\r\n\r\nI gave it a try. Never committed to airflow before so it's a shot in the dark."", 'created_at': datetime.datetime(2024, 8, 7, 19, 7, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2274203580, 'issue_id': 2451214036, 'author': 'atomic77', 'body': ""> I gave it a try. Never committed to airflow before so it's a shot in the dark.\r\n\r\nOh no worries - I was looking around for something to fix and saw the `Yes I am willing to submit a PR!` box unchecked so thought you were just reporting the issue."", 'created_at': datetime.datetime(2024, 8, 7, 19, 31, 56, tzinfo=datetime.timezone.utc)}]","atomic77 on (2024-08-07 16:00:15 UTC): I've got this reproduced locally against `main` and would like to give it a try.

jabbera (Issue Creator) on (2024-08-07 19:07:27 UTC): I gave it a try. Never committed to airflow before so it's a shot in the dark.

atomic77 on (2024-08-07 19:31:56 UTC): Oh no worries - I was looking around for something to fix and saw the `Yes I am willing to submit a PR!` box unchecked so thought you were just reporting the issue.

"
2450843554,issue,closed,completed,"Airflow REST GET API call for TaskInstances returns unordered items, making paged retrieval impossible","### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.1, but nothing in the release notes thereafter indicates this issue has been fixed.

### What happened?

When using paged retrieval:

```
https://airflow/api/v1/dags/DAG/dagRuns/rc--2024-08-02_01-30/taskInstances?limit=100&offset=0
https://airflow/api/v1/dags/DAG/dagRuns/rc--2024-08-02_01-30/taskInstances?limit=100&offset=100
https://airflow/api/v1/dags/DAG/dagRuns/rc--2024-08-02_01-30/taskInstances?limit=100&offset=200
```

the task instance results are unordered, ensuring with near certainty that all subsequent paged retrievals (with offset > 0) return some tasks already returned before, and fail to return tasks that should have been in the page.  Thus, concatenating the tasks yields the correct *number* of tasks, but some are repeated and some are missing.

Full retrieval as per

```
https://airflow/api/v1/dags/DAG/dagRuns/rc--2024-08-02_01-30/taskInstances?limit=300
```

returns each task instance only once, and returns all task instances exhaustively.

### What you think should happen instead?

Task instances from API call should either be returned in some specific order, or there should be a query string parameter allowing us users to specify a designated order (such as task instance creation / appearance date).



### How to reproduce

Just issue a taskInstances GET call with a small limit, then with an offset equivalent to the small limit.  Tasks that were on the first batch are likely to appear on the second batch too.

### Operating System

Ubuntu 22.04 container

### Versions of Apache Airflow Providers

Not relevant.

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

We use a very vanilla Kubernetes.

### Anything else?

Always.

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

I cannot, but I can recommend the following: all REST endpoints that list items in a paged manner must return items in a stable sort order, or at least allow the end user to specify a stable sort order criteria and document that unordered items are returned by default.

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",DFINITYManu,2024-08-06 13:04:34+00:00,['omkar-foss'],2024-08-21 14:07:35+00:00,2024-08-21 14:07:35+00:00,https://github.com/apache/airflow/issues/41283,"[('kind:bug', 'This is a clearly a bug'), ('area:API', ""Airflow's REST/HTTP API""), ('good first issue', ''), ('area:core', '')]","[{'comment_id': 2271245456, 'issue_id': 2450843554, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 6, 13, 4, 36, tzinfo=datetime.timezone.utc)}, {'comment_id': 2272552205, 'issue_id': 2450843554, 'author': 'omkar-foss', 'body': ""I'm looking into this, will raise a PR in a while."", 'created_at': datetime.datetime(2024, 8, 7, 3, 37, 24, tzinfo=datetime.timezone.utc)}, {'comment_id': 2273391601, 'issue_id': 2450843554, 'author': 'omkar-foss', 'body': ""@DFINITYManu I've raised #41307 which should be able to resolve your above mentioned issue.\r\n\r\nI've set default ordering by `start_date` (ascending) as it seems like a sensible date to sort by, but you can specify `order_by` as any other fields [here](https://github.com/apache/airflow/pull/41307/files#diff-68400eed8ebd42bdf7521d1425a5ec66b4bdfd973034b5c7a8030f571e35af4dR286-R303) as per your preference. Hoping this'll be useful.\r\n\r\nAs a sidenote - while looking into this, I came across some other GET endpoints where `order_by` query param can be added, I'll raise separate issues and PRs for those incrementally."", 'created_at': datetime.datetime(2024, 8, 7, 12, 47, 15, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-06 13:04:36 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

omkar-foss (Assginee) on (2024-08-07 03:37:24 UTC): I'm looking into this, will raise a PR in a while.

omkar-foss (Assginee) on (2024-08-07 12:47:15 UTC): @DFINITYManu I've raised #41307 which should be able to resolve your above mentioned issue.

I've set default ordering by `start_date` (ascending) as it seems like a sensible date to sort by, but you can specify `order_by` as any other fields [here](https://github.com/apache/airflow/pull/41307/files#diff-68400eed8ebd42bdf7521d1425a5ec66b4bdfd973034b5c7a8030f571e35af4dR286-R303) as per your preference. Hoping this'll be useful.

As a sidenote - while looking into this, I came across some other GET endpoints where `order_by` query param can be added, I'll raise separate issues and PRs for those incrementally.

"
2450284186,issue,closed,not_planned,Clearing a Mapped Task (with Downstream included) clears all Downstream tasks instead of mapped downstream tasks,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.2

### What happened?

If a mapped task is Cleared (with Downstream), all downstream tasks are cleared.

Clearing from the parent task. UI correctly shows 'All mapped tasks' for Map Index with the expected 1598 affected tasks:
![image](https://github.com/user-attachments/assets/606ab098-ed66-4174-bf4f-f4b790e27177)

Clearing the Mapped task. UI correctly shows the Map Index ie. 0 but the number of affected tasks are 1523:
![image](https://github.com/user-attachments/assets/06255fa5-cc77-46e6-83c7-fac66d661d34)
This equals the first scenario (a total of 1598 tasks) minus the number of Mapped tasks (76) plus the mapped task being cleared itself.
![image](https://github.com/user-attachments/assets/b4bbad13-7d80-4547-98d3-bf46bc1831b8)



### What you think should happen instead?

I would expect only mapped tasks with the same Map Index and/or direct downstream tasks to be affected when a Mapped task is Cleared (with Downstream). In this specific scenario it would be equal to 24 affected tasks.

### How to reproduce

Clear a mapped task (with Downstream included) and observe the number of affected tasks.

### Operating System

Ubuntu 22.0.4

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

AWS EKS

### Anything else?

100% Reproducible

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",gavinhonl,2024-08-06 08:35:26+00:00,[],2024-12-14 07:10:39+00:00,2024-12-14 07:10:38+00:00,https://github.com/apache/airflow/issues/41278,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:dynamic-task-mapping', 'AIP-42')]","[{'comment_id': 2271073484, 'issue_id': 2450284186, 'author': 'harjeevanmaan', 'body': 'I would like to work on this issue. Similar issue raised here:  #40543', 'created_at': datetime.datetime(2024, 8, 6, 11, 33, 42, tzinfo=datetime.timezone.utc)}, {'comment_id': 2272588694, 'issue_id': 2450284186, 'author': 'gavinhonl', 'body': '@harjeevanmaan - Thanks, no issues on my end.', 'created_at': datetime.datetime(2024, 8, 7, 4, 21, 4, tzinfo=datetime.timezone.utc)}, {'comment_id': 2274244473, 'issue_id': 2450284186, 'author': 'eladkal', 'body': 'cc @bbovenzi this is somehow related to issue I discussed with you about clearing procedure of specific index with this index downstream', 'created_at': datetime.datetime(2024, 8, 7, 19, 57, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2542918068, 'issue_id': 2450284186, 'author': 'shahar1', 'body': 'In a short delay - closing this issue as duplicate of #40543.\r\nIf any of you want to continue working on it, please ask for assignment there.', 'created_at': datetime.datetime(2024, 12, 14, 7, 10, 38, tzinfo=datetime.timezone.utc)}]","harjeevanmaan on (2024-08-06 11:33:42 UTC): I would like to work on this issue. Similar issue raised here:  #40543

gavinhonl (Issue Creator) on (2024-08-07 04:21:04 UTC): @harjeevanmaan - Thanks, no issues on my end.

eladkal on (2024-08-07 19:57:22 UTC): cc @bbovenzi this is somehow related to issue I discussed with you about clearing procedure of specific index with this index downstream

shahar1 on (2024-12-14 07:10:38 UTC): In a short delay - closing this issue as duplicate of #40543.
If any of you want to continue working on it, please ask for assignment there.

"
2450230838,issue,closed,completed,Executor reports task instance <TaskInstance: XXX [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.2

### What happened?

Case 2 of issue #39717 

Task fails with the message:
Executor reports task instance <TaskInstance: CSDISPATCHER_OTHERS.dispatch_restores scheduled__2024-08-05T19:47:00+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally.

Despite that the Airflow task has an external_executor_id, it is not registered in flower ( celery)

Dagrun Running | Task instance's dagrun was not in the 'running' state but in the state 'failed'. -- | -- Task Instance State | Task is in the 'failed' state.
external_executor_id a01b358f-ad34-4b16-81b9-fd69218f613e does not exist in flower / celery

look at the timestamps in the logs:
![image](https://github.com/user-attachments/assets/9fb967ab-e353-413e-adbb-3e82672a6b2c)

![image](https://github.com/user-attachments/assets/cb5921a3-ad6e-438c-92f0-c87ce0a4f0ee)

there is a gap of 10minutes betwwen the Start ( dummy task) and the dispatch_restores task.
And this behaviour is recurrent, (the 10m gap)

and in the task log:
'attempt=1.log.SchedulerJob.log'
(tkapp) ttauto@slautop02$ cat attempt=1.log.SchedulerJob.log
[2024-08-05T21:02:15.585+0100] {event_scheduler.py:40} WARNING - Marking task instance <TaskInstance: CSDISPATCHER_OTHERS.dispatch_restores scheduled__2024-08-05T19:47:00+00:00 [queued]> stuck in queued as failed. If the task instance has available retries, it will be retried.
[2024-08-05T21:02:16.750+0100] {scheduler_job_runner.py:843} ERROR - Executor reports task instance <TaskInstance: CSDISPATCHER_OTHERS.dispatch_restores scheduled__2024-08-05T19:47:00+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?



### What you think should happen instead?

The external task should exist in celery and the reason for the task to be marked as failed should be explicit.
There is no other info that justifies the task to fail.

### How to reproduce

I just happens

### Operating System

NAME=""Red Hat Enterprise Linux"" VERSION=""8.9 (Ootpa)"" ID=""rhel"" ID_LIKE=""fedora"" VERSION_ID=""8.9"" PLATFORM_ID=""platform:el8"" PRETTY_NAME=""Red Hat Enterprise Linux 8.9 (Ootpa)"" ANSI_COLOR=""0;31"" CPE_NAME=""cpe:/o:redhat:enterprise_linux:8::baseos"" HOME_URL=""https://www.redhat.com/"" DOCUMENTATION_URL=""https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8"" BUG_REPORT_URL=""https://bugzilla.redhat.com/""  REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 8"" REDHAT_BUGZILLA_PRODUCT_VERSION=8.9 REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux"" REDHAT_SUPPORT_PRODUCT_VERSION=""8.9""

### Versions of Apache Airflow Providers

apache-airflow==2.9.2
apache-airflow-providers-celery==3.7.2
apache-airflow-providers-common-io==1.3.2
apache-airflow-providers-common-sql==1.14.0
apache-airflow-providers-fab==1.1.1
apache-airflow-providers-ftp==3.9.1
apache-airflow-providers-hashicorp==3.7.1
apache-airflow-providers-http==4.11.1
apache-airflow-providers-imap==3.6.1
apache-airflow-providers-postgres==5.11.1
apache-airflow-providers-sftp==4.10.1
apache-airflow-providers-smtp==1.7.1
apache-airflow-providers-sqlite==3.8.1
apache-airflow-providers-ssh==3.11.1


### Deployment

Other

### Deployment details

Just simple airflow install on linux box.
Using PostgresQLm Rabbitmq and celery executor

### Anything else?

randomly

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",trlopes1974,2024-08-06 08:08:23+00:00,[],2024-10-12 22:21:25+00:00,2024-10-12 22:21:25+00:00,https://github.com/apache/airflow/issues/41276,"[('kind:bug', 'This is a clearly a bug'), (""Can't Reproduce"", 'The problem cannot be reproduced'), ('area:core', '')]","[{'comment_id': 2271143465, 'issue_id': 2450230838, 'author': 'scaoupgrade', 'body': 'That 10 minutes gap could be because of this config:\r\nhttps://github.com/apache/airflow/blob/6229c1c98a2a78d00255d6b6c5b70d032cff9b80/airflow/config_templates/config.yml#L2600-L2606', 'created_at': datetime.datetime(2024, 8, 6, 12, 12, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2271569597, 'issue_id': 2450230838, 'author': 'trlopes1974', 'body': ""Maybe, it makes some sense as we do not have that setting in the configuration.\r\nBut, what is bothering me is WHY? why did it timeout after being queued? We have no exhaustion anywhere, not in CPU, Memory, pool slots, concurrency. I'd say that at the current time we have a very  light system...\r\n\r\nSo, I believe that the real question is, why is the task queued but never started?\r\n\r\n+-1year with some cleanups\r\n![image](https://github.com/user-attachments/assets/6aa42a5d-8980-4f8f-b31b-3202f99f276f)\r\n\r\n\r\nAll timeouts on our config:\r\ndagbag_import_timeout = 120.0\r\ndag_file_processor_timeout = 180\r\ndefault_task_execution_timeout =\r\nweb_server_master_timeout = 120\r\nweb_server_worker_timeout = 120\r\nsmtp_timeout = 30\r\noperation_timeout = 2\r\nstalled_task_timeout = 0\r\ndefault_timeout = 604800"", 'created_at': datetime.datetime(2024, 8, 6, 15, 27, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2271973022, 'issue_id': 2450230838, 'author': 'scaoupgrade', 'body': ""> Maybe, it makes some sense as we do not have that setting in the configuration. But, what is bothering me is WHY? why did it timeout after being queued? We have no exhaustion anywhere, not in CPU, Memory, pool slots, concurrency. I'd say that at the current time we have a very light system...\r\n> \r\n> So, I believe that the real question is, why is the task queued but never started?\r\n> \r\n> +-1year with some cleanups ![image](https://private-user-images.githubusercontent.com/16724800/355507694-6aa42a5d-8980-4f8f-b31b-3202f99f276f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjI5NzE4NDksIm5iZiI6MTcyMjk3MTU0OSwicGF0aCI6Ii8xNjcyNDgwMC8zNTU1MDc2OTQtNmFhNDJhNWQtODk4MC00ZjhmLWIzMWItMzIwMmY5OWYyNzZmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA4MDYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwODA2VDE5MTIyOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTI2NjYzZDk2YzNkZmNhNjNjNzM3YWM0MDczMmY0Nzk4MDI2ZTM5OTQyYjEzNjU3ODZlMGU1ZDFmYTA0MTZhMzgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.KROI5i_Ta7uAa9f4BEntuF3Ibz3TbkhZ5FPdi1-ZkGk)\r\n> \r\n> All timeouts on our config: dagbag_import_timeout = 120.0 dag_file_processor_timeout = 180 default_task_execution_timeout = web_server_master_timeout = 120 web_server_worker_timeout = 120 smtp_timeout = 30 operation_timeout = 2 stalled_task_timeout = 0 default_timeout = 604800\r\n\r\nIt's the same case for me when the incident happened the other day. Workers are all online, but no task gets executed. I notice something abnormal in scheduler log when this happens: all tasks in a dag was repeatedly queued for thousands of times in one second. Looks like scheduler gets into a strange state"", 'created_at': datetime.datetime(2024, 8, 6, 19, 14, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2272228174, 'issue_id': 2450230838, 'author': 'trlopes1974', 'body': ""There is something messed up.\r\nToday I had a filesystem sensor that failed by timeout (airflow task) but\r\nthe celery task was successful ...\r\n\r\nA terça, 6/08/2024, 20:15, scaoupgrade ***@***.***> escreveu:\r\n\r\n> Maybe, it makes some sense as we do not have that setting in the\r\n> configuration. But, what is bothering me is WHY? why did it timeout after\r\n> being queued? We have no exhaustion anywhere, not in CPU, Memory, pool\r\n> slots, concurrency. I'd say that at the current time we have a very light\r\n> system...\r\n>\r\n> So, I believe that the real question is, why is the task queued but never\r\n> started?\r\n>\r\n> +-1year with some cleanups [image: image]\r\n> <https://private-user-images.githubusercontent.com/16724800/355507694-6aa42a5d-8980-4f8f-b31b-3202f99f276f.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MjI5NzE4NDksIm5iZiI6MTcyMjk3MTU0OSwicGF0aCI6Ii8xNjcyNDgwMC8zNTU1MDc2OTQtNmFhNDJhNWQtODk4MC00ZjhmLWIzMWItMzIwMmY5OWYyNzZmLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDA4MDYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwODA2VDE5MTIyOVomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTI2NjYzZDk2YzNkZmNhNjNjNzM3YWM0MDczMmY0Nzk4MDI2ZTM5OTQyYjEzNjU3ODZlMGU1ZDFmYTA0MTZhMzgmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.KROI5i_Ta7uAa9f4BEntuF3Ibz3TbkhZ5FPdi1-ZkGk>\r\n>\r\n> All timeouts on our config: dagbag_import_timeout = 120.0\r\n> dag_file_processor_timeout = 180 default_task_execution_timeout =\r\n> web_server_master_timeout = 120 web_server_worker_timeout = 120\r\n> smtp_timeout = 30 operation_timeout = 2 stalled_task_timeout = 0\r\n> default_timeout = 604800\r\n>\r\n> It's the same case for me when the incident happened the other day.\r\n> Workers are all online, but no task gets executed. I notice something\r\n> abnormal in scheduler log when this happens: all tasks in a dag was\r\n> repeatedly queued for thousands of times in one second. Looks like\r\n> scheduler gets into a strange state\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/apache/airflow/issues/41276#issuecomment-2271973022>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AD7TGQF6SPFC3OJWACXLBN3ZQEOEPAVCNFSM6AAAAABMBY37NCVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDENZRHE3TGMBSGI>\r\n> .\r\n> You are receiving this because you authored the thread.Message ID:\r\n> ***@***.***>\r\n>"", 'created_at': datetime.datetime(2024, 8, 6, 22, 3, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2346751278, 'issue_id': 2450230838, 'author': 'vaibhavnsingh', 'body': '+ 1', 'created_at': datetime.datetime(2024, 9, 12, 16, 29, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2382383312, 'issue_id': 2450230838, 'author': 'EvertonSA', 'body': ""> Workers are all online, but no task gets executed.\r\n\r\n\r\nI had the same, we noticed it was very aligned with Redis instabilities. after we migrated to saas redis, we did not see it anymore. Also allowing keda to auto scaling to 0 naturally restarted workers.  \r\n\r\nmini scheduler was also disabled, so I'm unsure what fixed."", 'created_at': datetime.datetime(2024, 9, 30, 7, 59, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2395819083, 'issue_id': 2450230838, 'author': 'xudalei1977', 'body': 'I use Amazon Managed Workflow Apache Airflow 2.9.2, met the issue when execute a simple dag, the Task log is:\r\n[2024-10-07T03:09:30.278+0000] {scheduler_job_runner.py:769} ERROR - Executor reports task instance <TaskInstance: example_dag.example_task_1 manual__2024-10-07T03:09:07.236750+00:00 [queued]> finished (failed) although the task says it\'s queued. (Info: None) Was the task killed externally?\r\n\r\nthe schedule log as:\r\n1728270570271,""[\x1b[34m2024-10-07T03:09:30.239+0000\x1b[0m] {\x1b[34mcelery_executor.py:\x1b[0m290} ERROR\x1b[0m - Error sending Celery task: Timeout, PID: 21\r\n""\r\n1728270570271,""Celery Task ID: TaskInstanceKey(dag_id=\'example_dag\', task_id=\'example_task_1\', run_id=\'manual__2024-10-07T03:09:07.236750+00:00\', try_number=1, map_index=-1)\r\n""\r\n1728270570271,""Traceback (most recent call last):\r\n""\r\n1728270570271,""  File """"/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/celery/executors/celery_executor_utils.py"""", line 220, in send_task_to_executor\r\n""\r\n1728270570271,""    result = task_to_run.apply_async(args=[command], queue=queue)\r\n""\r\n1728270570271,""             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n""\r\n1728270570271,""  File """"/usr/local/airflow/.local/lib/python3.11/site-packages/celery/app/task.py"""", line 594, in apply_async\r\n""\r\n1728270570271,""    return app.send_task(\r\n""\r\n1728270570271,""           ^^^^^^^^^^^^^^\r\n""\r\n1728270570271,""  File """"/usr/local/airflow/.local/lib/python3.11/site-packages/celery/app/base.py"""", line 801, in send_task\r\n""\r\n1728270570271,""    amqp.send_task_message(P, name, message, **options)\r\n""\r\n1728270570272,""  File """"/usr/local/airflow/.local/lib/python3.11/site-packages/celery/app/amqp.py"""", line 518, in send_task_message\r\n""\r\n1728270570272,""    ret = producer.publish(\r\n""\r\n1728270570272,""          ^^^^^^^^^^^^^^^^^\r\n""\r\n1728270570272,""  File """"/usr/local/airflow/.local/lib/python3.11/site-packages/kombu/messaging.py"""", line 186, in publish', 'created_at': datetime.datetime(2024, 10, 7, 3, 24, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2397548181, 'issue_id': 2450230838, 'author': 'vaibhavnsingh', 'body': 'Please check this property AIRFLOW__CELERY__OPERATION_TIMEOUT. In our environment we were getting this airflowtasktimeout error. We had to increase the time from 1.0 sec .', 'created_at': datetime.datetime(2024, 10, 7, 17, 55, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2401105937, 'issue_id': 2450230838, 'author': 'xudalei1977', 'body': '> Please check this property AIRFLOW__CELERY__OPERATION_TIMEOUT. In our environment we were getting this airflowtasktimeout error. We had to increase the time from 1.0 sec .\r\n\r\nthanks very much, it works.', 'created_at': datetime.datetime(2024, 10, 9, 1, 42, 22, tzinfo=datetime.timezone.utc)}]","scaoupgrade on (2024-08-06 12:12:50 UTC): That 10 minutes gap could be because of this config:
https://github.com/apache/airflow/blob/6229c1c98a2a78d00255d6b6c5b70d032cff9b80/airflow/config_templates/config.yml#L2600-L2606

trlopes1974 (Issue Creator) on (2024-08-06 15:27:40 UTC): Maybe, it makes some sense as we do not have that setting in the configuration.
But, what is bothering me is WHY? why did it timeout after being queued? We have no exhaustion anywhere, not in CPU, Memory, pool slots, concurrency. I'd say that at the current time we have a very  light system...

So, I believe that the real question is, why is the task queued but never started?

+-1year with some cleanups
![image](https://github.com/user-attachments/assets/6aa42a5d-8980-4f8f-b31b-3202f99f276f)


All timeouts on our config:
dagbag_import_timeout = 120.0
dag_file_processor_timeout = 180
default_task_execution_timeout =
web_server_master_timeout = 120
web_server_worker_timeout = 120
smtp_timeout = 30
operation_timeout = 2
stalled_task_timeout = 0
default_timeout = 604800

scaoupgrade on (2024-08-06 19:14:57 UTC): It's the same case for me when the incident happened the other day. Workers are all online, but no task gets executed. I notice something abnormal in scheduler log when this happens: all tasks in a dag was repeatedly queued for thousands of times in one second. Looks like scheduler gets into a strange state

trlopes1974 (Issue Creator) on (2024-08-06 22:03:48 UTC): There is something messed up.
Today I had a filesystem sensor that failed by timeout (airflow task) but
the celery task was successful ...

A terça, 6/08/2024, 20:15, scaoupgrade ***@***.***> escreveu:

vaibhavnsingh on (2024-09-12 16:29:02 UTC): + 1

EvertonSA on (2024-09-30 07:59:53 UTC): I had the same, we noticed it was very aligned with Redis instabilities. after we migrated to saas redis, we did not see it anymore. Also allowing keda to auto scaling to 0 naturally restarted workers.  

mini scheduler was also disabled, so I'm unsure what fixed.

xudalei1977 on (2024-10-07 03:24:32 UTC): I use Amazon Managed Workflow Apache Airflow 2.9.2, met the issue when execute a simple dag, the Task log is:
[2024-10-07T03:09:30.278+0000] {scheduler_job_runner.py:769} ERROR - Executor reports task instance <TaskInstance: example_dag.example_task_1 manual__2024-10-07T03:09:07.236750+00:00 [queued]> finished (failed) although the task says it's queued. (Info: None) Was the task killed externally?

the schedule log as:
1728270570271,""[[34m2024-10-07T03:09:30.239+0000[0m] {[34mcelery_executor.py:[0m290} ERROR[0m - Error sending Celery task: Timeout, PID: 21
""
1728270570271,""Celery Task ID: TaskInstanceKey(dag_id='example_dag', task_id='example_task_1', run_id='manual__2024-10-07T03:09:07.236750+00:00', try_number=1, map_index=-1)
""
1728270570271,""Traceback (most recent call last):
""
1728270570271,""  File """"/usr/local/airflow/.local/lib/python3.11/site-packages/airflow/providers/celery/executors/celery_executor_utils.py"""", line 220, in send_task_to_executor
""
1728270570271,""    result = task_to_run.apply_async(args=[command], queue=queue)
""
1728270570271,""             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
""
1728270570271,""  File """"/usr/local/airflow/.local/lib/python3.11/site-packages/celery/app/task.py"""", line 594, in apply_async
""
1728270570271,""    return app.send_task(
""
1728270570271,""           ^^^^^^^^^^^^^^
""
1728270570271,""  File """"/usr/local/airflow/.local/lib/python3.11/site-packages/celery/app/base.py"""", line 801, in send_task
""
1728270570271,""    amqp.send_task_message(P, name, message, **options)
""
1728270570272,""  File """"/usr/local/airflow/.local/lib/python3.11/site-packages/celery/app/amqp.py"""", line 518, in send_task_message
""
1728270570272,""    ret = producer.publish(
""
1728270570272,""          ^^^^^^^^^^^^^^^^^
""
1728270570272,""  File """"/usr/local/airflow/.local/lib/python3.11/site-packages/kombu/messaging.py"""", line 186, in publish

vaibhavnsingh on (2024-10-07 17:55:10 UTC): Please check this property AIRFLOW__CELERY__OPERATION_TIMEOUT. In our environment we were getting this airflowtasktimeout error. We had to increase the time from 1.0 sec .

xudalei1977 on (2024-10-09 01:42:22 UTC): thanks very much, it works.

"
2449496501,issue,closed,completed,Scheduled DAGs with cron expression,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.2

### What happened?

DAGs scheduled with cron expression that contains specifc day of week defined are not running at the scheduled time.

DAGs scheduled with the cron expression examples below don't work:
`0 17 * * 1`
`0 17 * * 1-2`
`0 17 * * 1-6`
`0 17 * * mon`
`0 17 * * MON`

DAGs scheduled with the cron expression example below work fine:
`0 17 * * *`

### What you think should happen instead?

_No response_

### How to reproduce

Create a DAG with `schedule` or `schedule_interval` with specifc day of week defined in cron expression, example: `schedule=""0 17 * * 1""`

### Operating System

Amazon Linux / Amazon MWAA

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon==8.24.0
apache-airflow-providers-slack==8.7.1
apache-airflow-providers-pagerduty==3.7.1


### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",galmeida-hedgepoint,2024-08-05 21:17:13+00:00,[],2024-08-06 19:18:50+00:00,2024-08-06 19:18:50+00:00,https://github.com/apache/airflow/issues/41270,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2269934847, 'issue_id': 2449496501, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 5, 21, 17, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2271649681, 'issue_id': 2449496501, 'author': 'tirkarthi', 'body': 'Can you please add a sample dag to reproduce this locally? By don\'t work do you mean the dag is triggered on incorrect dates or not triggered at all?\r\n\r\nI tried a dag with schedule as `0 17 * * 1-2` and it renders the planned dates as expected in calendar with next dagrun also being calculated correctly.\r\n\r\nSample dag\r\n\r\n```python\r\nfrom datetime import datetime, timedelta\r\n\r\nfrom airflow import DAG\r\nfrom airflow.decorators import task\r\n\r\nwith DAG(\r\n    dag_id=""error_color"",\r\n    start_date=datetime(2024, 1, 1),\r\n    catchup=False,\r\n    schedule_interval=""0 17 * * 1-2"",\r\n) as dag:\r\n\r\n    @task\r\n    def empty():\r\n        pass\r\n\r\n\r\n    empty()\r\n```\r\n\r\n![image](https://github.com/user-attachments/assets/7eb846be-ffe8-4fa2-9ea0-8f7ed813ec85)', 'created_at': datetime.datetime(2024, 8, 6, 16, 8, 10, tzinfo=datetime.timezone.utc)}, {'comment_id': 2271845940, 'issue_id': 2449496501, 'author': 'galmeida-hedgepoint', 'body': 'The dag is **not**  triggered at all\r\n\r\n```python\r\nimport os\r\nimport datetime\r\nimport pendulum\r\nfrom airflow import DAG\r\nfrom airflow.operators.empty import EmptyOperator\r\n\r\nwith DAG(\r\n    dag_id=""error_color"",\r\n    schedule_interval=""0 17 * * 1-6"",\r\n    start_date=pendulum.yesterday(),\r\n    dagrun_timeout = datetime.timedelta(minutes = 60),\r\n    max_active_runs = 1,\r\n    catchup=False\r\n) as dag:\r\n    \r\n    EmptyOperator(task_id = ""empty_task"")\r\n```', 'created_at': datetime.datetime(2024, 8, 6, 17, 59, 57, tzinfo=datetime.timezone.utc)}, {'comment_id': 2271943008, 'issue_id': 2449496501, 'author': 'tirkarthi', 'body': ""This could be an issue due to start_date being dynamic. It's strongly recommended to use a static start_date.\r\n\r\nhttps://forum.astronomer.io/t/airflow-start-date-concepts/393\r\nhttps://stackoverflow.com/questions/41134524/why-is-it-recommended-against-using-a-dynamic-start-date-in-airflow\r\nhttps://airflow.apache.org/docs/apache-airflow/stable/faq.html#what-s-the-deal-with-start-date"", 'created_at': datetime.datetime(2024, 8, 6, 18, 56, 50, tzinfo=datetime.timezone.utc)}, {'comment_id': 2271979731, 'issue_id': 2449496501, 'author': 'galmeida-hedgepoint', 'body': 'I have just tested and now it is working perfectly! Thank you\r\n\r\n![image](https://github.com/user-attachments/assets/751f54ee-4006-4f66-8fe1-563467ec4fc9)', 'created_at': datetime.datetime(2024, 8, 6, 19, 18, 50, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-05 21:17:16 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

tirkarthi on (2024-08-06 16:08:10 UTC): Can you please add a sample dag to reproduce this locally? By don't work do you mean the dag is triggered on incorrect dates or not triggered at all?

I tried a dag with schedule as `0 17 * * 1-2` and it renders the planned dates as expected in calendar with next dagrun also being calculated correctly.

Sample dag

```python
from datetime import datetime, timedelta

from airflow import DAG
from airflow.decorators import task

with DAG(
    dag_id=""error_color"",
    start_date=datetime(2024, 1, 1),
    catchup=False,
    schedule_interval=""0 17 * * 1-2"",
) as dag:

    @task
    def empty():
        pass


    empty()
```

![image](https://github.com/user-attachments/assets/7eb846be-ffe8-4fa2-9ea0-8f7ed813ec85)

galmeida-hedgepoint (Issue Creator) on (2024-08-06 17:59:57 UTC): The dag is **not**  triggered at all

```python
import os
import datetime
import pendulum
from airflow import DAG
from airflow.operators.empty import EmptyOperator

with DAG(
    dag_id=""error_color"",
    schedule_interval=""0 17 * * 1-6"",
    start_date=pendulum.yesterday(),
    dagrun_timeout = datetime.timedelta(minutes = 60),
    max_active_runs = 1,
    catchup=False
) as dag:
    
    EmptyOperator(task_id = ""empty_task"")
```

tirkarthi on (2024-08-06 18:56:50 UTC): This could be an issue due to start_date being dynamic. It's strongly recommended to use a static start_date.

https://forum.astronomer.io/t/airflow-start-date-concepts/393
https://stackoverflow.com/questions/41134524/why-is-it-recommended-against-using-a-dynamic-start-date-in-airflow
https://airflow.apache.org/docs/apache-airflow/stable/faq.html#what-s-the-deal-with-start-date

galmeida-hedgepoint (Issue Creator) on (2024-08-06 19:18:50 UTC): I have just tested and now it is working perfectly! Thank you

![image](https://github.com/user-attachments/assets/751f54ee-4006-4f66-8fe1-563467ec4fc9)

"
2449424162,issue,closed,completed,Docker + Kubernetes + HyperV + Helm persistent volume doesn't mount,"### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

I'm trying to use:

```
        dags:
          persistence:
            enabled: true
            existingClaim: airflow

```

To mount a shared DAG folder. Host machine is Windows 11. I'm running Docker Desktop + Kubernetes + HyperV backend.

My yaml looks like:

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: airflow
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 8Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /host_mnt/c/users/xxx/.airflow
```

I've tried other formats like `/run/desktop/mnt/host/c/users/xxx/.airflow` (this one works on a WSL backend), `/users/xxx/.airflow, etc`. `/users/xxx/.airflow` has full permissions on the the Windows box.

### What you think should happen instead?

_No response_

### How to reproduce

N/A

### Operating System

Windows 11

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",SledgeHammer01,2024-08-05 20:27:39+00:00,[],2024-08-05 22:25:37+00:00,2024-08-05 22:25:36+00:00,https://github.com/apache/airflow/issues/41269,"[('kind:bug', 'This is a clearly a bug'), ('area:helm-chart', 'Airflow Helm Chart'), ('area:core', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues'), ('needs-triage', ""label for new issues that we didn't triage yet"")]",[],
2448420373,issue,closed,completed,wrong display of multiline messages in the log after filtering by message level,"### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

Problem displaying an error in the tab Logs in the task detail.

If the error text in the log is on multiple lines, it is displayed correctly in the basic view without filtering (see the picture)
![image](https://github.com/user-attachments/assets/d9827e7e-7dbe-4d88-98af-f82598cf8c58)

But if I use filter by message level (e.g. error, see the pict) , then only the first line of the same error message is displayed. The information on the other lines for this error message is not visible (see the picture)
![image](https://github.com/user-attachments/assets/00813f80-8744-41dd-b7d0-28c6c738f4c5)





### What you think should happen instead?

A multi-line message should be displayed in its entirety even after applying the message level filter

### How to reproduce

Display a multi-line message (typically an error message) in the task log and then apply a filter to the ERROR level

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",Pad71,2024-08-05 12:22:08+00:00,['jason810496'],2024-12-03 14:52:52+00:00,2024-12-03 14:52:52+00:00,https://github.com/apache/airflow/issues/41265,"[('kind:bug', 'This is a clearly a bug'), ('area:logging', ''), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('affected_version:2.9', 'Issues Reported for 2.9')]","[{'comment_id': 2269056177, 'issue_id': 2448420373, 'author': 'Pad71', 'body': 'the same problem with displaying multiline message is also when filtering to other levels (e.g. INFO etc.)', 'created_at': datetime.datetime(2024, 8, 5, 13, 16, 44, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275139305, 'issue_id': 2448420373, 'author': 'Lee-W', 'body': 'Thanks for reporting this. I just verified this happens in the latest main ([45658a8963761ce8a565b481156c847e493fce67](https://github.com/apache/airflow/tree/45658a8963761ce8a565b481156c847e493fce67))', 'created_at': datetime.datetime(2024, 8, 8, 7, 30, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2275142276, 'issue_id': 2448420373, 'author': 'Lee-W', 'body': 'cc @bbovenzi', 'created_at': datetime.datetime(2024, 8, 8, 7, 32, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2329553022, 'issue_id': 2448420373, 'author': 'bbovenzi', 'body': ""Talking with Ash about his TaskSDK work. Logs will be sent to the UI in json format so we won't need all of this regex string processing to try to add features to logs. But this is important enough to fix for 2.10.x or 2.11"", 'created_at': datetime.datetime(2024, 9, 4, 16, 50, 45, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499690566, 'issue_id': 2448420373, 'author': 'jason810496', 'body': 'Hi @Lee-W, I could try resolve this issue, could you assign to me, thanks !', 'created_at': datetime.datetime(2024, 11, 26, 5, 26, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2499766673, 'issue_id': 2448420373, 'author': 'Lee-W', 'body': '> Hi @Lee-W, I could try resolve this issue, could you assign to me, thanks !\r\n\r\nDefinitely, just assigned it to you 🙂', 'created_at': datetime.datetime(2024, 11, 26, 6, 25, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2514781414, 'issue_id': 2448420373, 'author': 'jason810496', 'body': 'Hi @Lee-W, I think this issue can be close since the PR has been merged, thanks 🙌', 'created_at': datetime.datetime(2024, 12, 3, 14, 48, 18, tzinfo=datetime.timezone.utc)}, {'comment_id': 2514792685, 'issue_id': 2448420373, 'author': 'Lee-W', 'body': ""Sure. Thanks for reminding me 🙂  Next time, you can mention `close: #pr-number` in your pr, and it'll automatically close the issue"", 'created_at': datetime.datetime(2024, 12, 3, 14, 52, 52, tzinfo=datetime.timezone.utc)}]","Pad71 (Issue Creator) on (2024-08-05 13:16:44 UTC): the same problem with displaying multiline message is also when filtering to other levels (e.g. INFO etc.)

Lee-W on (2024-08-08 07:30:31 UTC): Thanks for reporting this. I just verified this happens in the latest main ([45658a8963761ce8a565b481156c847e493fce67](https://github.com/apache/airflow/tree/45658a8963761ce8a565b481156c847e493fce67))

Lee-W on (2024-08-08 07:32:17 UTC): cc @bbovenzi

bbovenzi on (2024-09-04 16:50:45 UTC): Talking with Ash about his TaskSDK work. Logs will be sent to the UI in json format so we won't need all of this regex string processing to try to add features to logs. But this is important enough to fix for 2.10.x or 2.11

jason810496 (Assginee) on (2024-11-26 05:26:15 UTC): Hi @Lee-W, I could try resolve this issue, could you assign to me, thanks !

Lee-W on (2024-11-26 06:25:30 UTC): Definitely, just assigned it to you 🙂

jason810496 (Assginee) on (2024-12-03 14:48:18 UTC): Hi @Lee-W, I think this issue can be close since the PR has been merged, thanks 🙌

Lee-W on (2024-12-03 14:52:52 UTC): Sure. Thanks for reminding me 🙂  Next time, you can mention `close: #pr-number` in your pr, and it'll automatically close the issue

"
2447995385,issue,closed,completed,[BUG] nested task group info unavailable in task OL event,"### Apache Airflow Provider(s)

openlineage

### Versions of Apache Airflow Providers

apache-airflow-providers-openlineage==1.9.1

### Apache Airflow version

Any version above 2.7.0

### Operating System

Mac OS 14.5

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

Using breeze to run Airflow

### What happened

In case of nested task groups, we are not getting task group names groups levels except the inner most.

eg. in this case group1 is inside group3
```
""task_group"": {
    ""downstream_group_ids"": ""[]"",
    ""downstream_task_ids"": ""[]"",
    ""group_id"": ""group1"",
    ""prefix_group_id"": true,
    ""tooltip"": """",
    ""upstream_group_ids"": ""[]"",
    ""upstream_task_ids"": ""[]""
},
""task_id"": ""group3.group1.task2"",
```


### What you think should happen instead

group_id should have been ""group3.group1""

### How to reproduce

DAG code:
```from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.utils.task_group import TaskGroup
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'catchup': False,
}

def my_function():
    print(""Hello World"")

with DAG('task_group_test_nested4',
         default_args=default_args,
         schedule_interval=timedelta(days=1),
         catchup=False) as dag:

    task1 = PythonOperator(task_id='task1', python_callable=my_function)

    with TaskGroup(group_id='group3') as group3:
        with TaskGroup(group_id='group1') as group1:
            task2 = PythonOperator(task_id='task2', python_callable=my_function)
            task3 = PythonOperator(task_id='task3', python_callable=my_function)

        task4 = PythonOperator(task_id='task4', python_callable=my_function)
        task5 = PythonOperator(task_id='task5', python_callable=my_function)

    task1 >> group3


```


### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",rahul-madaan,2024-08-05 08:55:31+00:00,[],2024-08-06 08:03:22+00:00,2024-08-06 08:03:22+00:00,https://github.com/apache/airflow/issues/41261,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('area:logging', ''), ('area:TaskGroup', ''), ('needs-triage', ""label for new issues that we didn't triage yet""), ('provider:openlineage', 'AIP-53')]","[{'comment_id': 2268529874, 'issue_id': 2447995385, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 5, 8, 55, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2269218380, 'issue_id': 2447995385, 'author': 'kacpermuda', 'body': ""TLDR; I don't think current information about task group is insufficient and we can probably close this issue.\r\n\r\nI believe when using AirflowJobFacet included in DAG START event you can assign each task to a specific task group and then see all the upstream task groups to that taskgroup, creating a holistic view. So the scenario for the consumer would be something like: we get DAG start event, we extract all the tasks and task groups assigned to them, for each task group we find parent task group, recursively, and that gives that the information about the whole relationship between task groups even if task_group_ids are not present cause the user disabled this with `prefix_group_id=False`. Finally, for each task, we can map the task id from task event and assign this whole structure we created before."", 'created_at': datetime.datetime(2024, 8, 5, 14, 29, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2270637112, 'issue_id': 2447995385, 'author': 'rahul-madaan', 'body': 'Thanks for the explanation @kacpermuda, closing this issue', 'created_at': datetime.datetime(2024, 8, 6, 8, 3, 9, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-05 08:55:34 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

kacpermuda on (2024-08-05 14:29:32 UTC): TLDR; I don't think current information about task group is insufficient and we can probably close this issue.

I believe when using AirflowJobFacet included in DAG START event you can assign each task to a specific task group and then see all the upstream task groups to that taskgroup, creating a holistic view. So the scenario for the consumer would be something like: we get DAG start event, we extract all the tasks and task groups assigned to them, for each task group we find parent task group, recursively, and that gives that the information about the whole relationship between task groups even if task_group_ids are not present cause the user disabled this with `prefix_group_id=False`. Finally, for each task, we can map the task id from task event and assign this whole structure we created before.

rahul-madaan (Issue Creator) on (2024-08-06 08:03:09 UTC): Thanks for the explanation @kacpermuda, closing this issue

"
2446581739,issue,closed,completed,Moto 5.0.12 breaks amazon provider's RDS tests,"### Body

The **just released** moto `5.0.12` breaks tests for AWS RDS 

https://github.com/apache/airflow/actions/runs/10230423335/job/28305276935#step:7:919

The way to reproduce it:

* Enter breeze `breeze` (you get latest constraints dependencies there)
* Run the test `pytest tests/providers/amazon/aws/sensors/test_rds.py::TestRdsExportTaskExistenceSensor::test_export_task_poke_false`
* It succeeds
* Upgrade moto to latest version `pip install --upgrade moto`:

```
Installing collected packages: moto
  Attempting uninstall: moto
    Found existing installation: moto 5.0.11
    Uninstalling moto-5.0.11:
      Successfully uninstalled moto-5.0.11
Successfully installed moto-5.0.12
```
* run the same test again

Fails with 

```
  raise error_class(parsed_response, operation_name)
E           botocore.errorfactory.ExportTaskNotFoundFault: An error occurred (ExportTaskNotFound) when calling the DescribeExportTasks operation: Cannot cancel export task because a task with the identifier my-db-instance-snap-export is not exist.
```


### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",potiuk,2024-08-03 20:26:52+00:00,"['ferruzzi', 'Taragolis', 'o-nikolas', 'vincbeck']",2024-08-13 23:51:12+00:00,2024-08-13 23:51:12+00:00,https://github.com/apache/airflow/issues/41243,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('kind:meta', 'High-level information important to the community')]","[{'comment_id': 2267662899, 'issue_id': 2446581739, 'author': 'vikramaditya91', 'body': 'The reason for the failure is\r\nhttps://github.com/getmoto/moto/pull/7861\r\n\r\nspecifically the \r\n```patch\r\n+            ""ExportTaskNotFound"",\r\n-            ""ExportTaskNotFoundFault"",\r\n ```\r\n \r\n in the `class ExportTaskNotFoundError`', 'created_at': datetime.datetime(2024, 8, 4, 20, 43, 30, tzinfo=datetime.timezone.utc)}]","vikramaditya91 on (2024-08-04 20:43:30 UTC): The reason for the failure is
https://github.com/getmoto/moto/pull/7861

specifically the 
```patch
+            ""ExportTaskNotFound"",
-            ""ExportTaskNotFoundFault"",
 ```
 
 in the `class ExportTaskNotFoundError`

"
2446424589,issue,closed,completed,"Status of testing Providers that were prepared on August 03, 2024","### Body


I have a kind request for all the contributors to the latest provider packages release.
Could you please help us to test the RC versions of the providers?

The guidelines on how to test providers can be found in

[Verify providers by contributors](https://github.com/apache/airflow/blob/main/dev/README_RELEASE_PROVIDER_PACKAGES.md#verify-the-release-candidate-by-contributors)

Let us know in the comment, whether the issue is addressed.

Those are providers that require testing as there were some substantial changes introduced:


## Provider [amazon: 8.27.0rc2](https://pypi.org/project/apache-airflow-providers-amazon/8.27.0rc2)
   - [x] [Add RedriveExecution support to StepFunctionStartExecutionOperator (#40976)](https://github.com/apache/airflow/pull/40976): @gopidesupavan
   - [ ] [AIP-62: add hook lineage collection support for S3Hook  (#40819)](https://github.com/apache/airflow/pull/40819): @mobuchowski
   - [x] [Introduce Amazon Kinesis Analytics V2 (Managed Service for Apache Flink application)  (#40765)](https://github.com/apache/airflow/pull/40765): @gopidesupavan
   - [ ] [Make EMR Container Trigger max attempts retries match the Operator (#41008)](https://github.com/apache/airflow/pull/41008): @o-nikolas
     Linked issues:
       - [ ] [Linked Issue #40483](https://github.com/apache/airflow/issues/40483): @akomisarek
   - [ ] [Fix `RdsStopDbOperator` operator in deferrable mode (#41059)](https://github.com/apache/airflow/pull/41059): @vincbeck
   - [ ] [Fix `RedshiftCreateClusterOperator` to always specify `PubliclyAccessible` (#40872)](https://github.com/apache/airflow/pull/40872): @vincbeck
   - [ ] [Fix AWS Redshift operators and sensors (#41191)](https://github.com/apache/airflow/pull/41191): @vincbeck
   - [ ] [Fix `EmrServerlessStartJobOperator` (#41103)](https://github.com/apache/airflow/pull/41103): @vincbeck
   - [ ] [Update `example_redshift` and `example_redshift_s3_transfers` to use `RedshiftDataHook` instead of `RedshiftSQLHook` (#40970)](https://github.com/apache/airflow/pull/40970): @vincbeck
   - [x] [openlineage: migrate OpenLineage provider to V2 facets. (#39530)](https://github.com/apache/airflow/pull/39530): @JDarDagran
   - [x] [[AIP-62] Translate AIP-60 URI to OpenLineage (#40173)](https://github.com/apache/airflow/pull/40173): @kacpermuda
     Linked issues:
       - [x] [Linked Issue #40335](https://github.com/apache/airflow/pull/40335): @JDarDagran
       - [x] [Linked Issue #38767](https://github.com/apache/airflow/issues/38767): @mobuchowski
   - [x] [Move AWS Managed Service for Apache Flink sensor states to Hook (#40896)](https://github.com/apache/airflow/pull/40896): @gopidesupavan
   - [ ] [Replace usages of task context logger with the log table (#40867)](https://github.com/apache/airflow/pull/40867): @dstandish
   - [ ] [Deprecate `SageMakerTrainingPrintLogTrigger` (#41158)](https://github.com/apache/airflow/pull/41158): @vincbeck
## Provider [apache.beam: 5.7.2rc2](https://pypi.org/project/apache-airflow-providers-apache-beam/5.7.2rc2)
   - [ ] [Update default value for job_name in DataflowConfiguration (#40645)](https://github.com/apache/airflow/pull/40645): @e-galan
     Linked issues:
       - [ ] [Linked Issue #40515](https://github.com/apache/airflow/issues/40515): @turb
## Provider [apache.drill: 2.7.3rc1](https://pypi.org/project/apache-airflow-providers-apache-drill/2.7.3rc1)
   - [x] [Clean up remaining getattr connection DbApiHook (#40665)](https://github.com/apache/airflow/pull/40665): @dabla
## Provider [apache.druid: 3.10.2rc1](https://pypi.org/project/apache-airflow-providers-apache-druid/3.10.2rc1)
   - [x] [Clean up remaining getattr connection DbApiHook (#40665)](https://github.com/apache/airflow/pull/40665): @dabla
## Provider [apache.impala: 1.4.2rc1](https://pypi.org/project/apache-airflow-providers-apache-impala/1.4.2rc1)
   - [x] [Clean up remaining getattr connection DbApiHook (#40665)](https://github.com/apache/airflow/pull/40665): @dabla
## Provider [apache.pinot: 4.4.2rc1](https://pypi.org/project/apache-airflow-providers-apache-pinot/4.4.2rc1)
   - [x] [Clean up remaining getattr connection DbApiHook (#40665)](https://github.com/apache/airflow/pull/40665): @dabla
## Provider [apprise: 1.3.2rc1](https://pypi.org/project/apache-airflow-providers-apprise/1.3.2rc1)
   - [ ] [Fix default behaviour for init function in AppriseNotifier (#41054)](https://github.com/apache/airflow/pull/41054): @romsharon98
## Provider [celery: 3.7.3rc1](https://pypi.org/project/apache-airflow-providers-celery/3.7.3rc1)
   - [ ] [Increase broker's visibility timeout to 24hrs (#40879)](https://github.com/apache/airflow/pull/40879): @ephraimbuddy
## Provider [cncf.kubernetes: 8.3.4rc1](https://pypi.org/project/apache-airflow-providers-cncf-kubernetes/8.3.4rc1)
   - [ ] [Pass content of kube/config file to triggerer as a dictionary (#41178)](https://github.com/apache/airflow/pull/41178): @VladaZakharova
   - [ ] [Fix confusing log message in kubernetes executor (#41035)](https://github.com/apache/airflow/pull/41035): @RNHTTR
   - [ ] [Fix ApiException handling when adopting completed pods (#41109)](https://github.com/apache/airflow/pull/41109): @jedcunningham
## Provider [common.compat: 1.1.0rc1](https://pypi.org/project/apache-airflow-providers-common-compat/1.1.0rc1)
   - [ ] [AIP-62: add method to common.compat to not force hooks to try/except every time (#40812)](https://github.com/apache/airflow/pull/40812): @mobuchowski
   - [x] [openlineage: migrate OpenLineage provider to V2 facets. (#39530)](https://github.com/apache/airflow/pull/39530): @JDarDagran
   - [ ] [AIP-62: add hook lineage collection support for S3Hook  (#40819)](https://github.com/apache/airflow/pull/40819): @mobuchowski
## Provider [common.io: 1.4.0rc1](https://pypi.org/project/apache-airflow-providers-common-io/1.4.0rc1)
   - [x] [[AIP-62] Translate AIP-60 URI to OpenLineage (#40173)](https://github.com/apache/airflow/pull/40173): @kacpermuda
     Linked issues:
       - [x] [Linked Issue #40335](https://github.com/apache/airflow/pull/40335): @JDarDagran
       - [x] [Linked Issue #38767](https://github.com/apache/airflow/issues/38767): @mobuchowski
   - [ ] [AIP-62: add file dataset type support into common.io provider (#40817)](https://github.com/apache/airflow/pull/40817): @mobuchowski
   - [x] [openlineage: migrate OpenLineage provider to V2 facets. (#39530)](https://github.com/apache/airflow/pull/39530): @JDarDagran
   - [ ] [AIP-62: add hook lineage collection support for S3Hook  (#40819)](https://github.com/apache/airflow/pull/40819): @mobuchowski
## Provider [common.sql: 1.15.0rc1](https://pypi.org/project/apache-airflow-providers-common-sql/1.15.0rc1)
   - [x] [Create SQLAlchemy engine from connection in DB Hook and added autocommit param to insert_rows method (#40669)](https://github.com/apache/airflow/pull/40669): @dabla
## Provider [databricks: 6.8.0rc1](https://pypi.org/project/apache-airflow-providers-databricks/6.8.0rc1)
   - [x] [Add DatabricksWorkflowPlugin (#40724)](https://github.com/apache/airflow/pull/40724): @pankajkoti
     Linked issues:
       - [x] [Linked Issue #40153](https://github.com/apache/airflow/pull/40153): @pankajkoti
       - [x] [Linked Issue #40714](https://github.com/apache/airflow/pull/40714): @jscheffl
       - [x] [Linked Issue #40708](https://github.com/apache/airflow/pull/40708): @jscheffl
   - [x] [DatabricksPlugin - Fix dag view redirect URL by using url_for redirect (#41040)](https://github.com/apache/airflow/pull/41040): @pankajkoti
   - [x] [Fix named parameters templating in Databricks operators (#40864)](https://github.com/apache/airflow/pull/40864): @boraberke
     Linked issues:
       - [x] [Linked Issue #40788](https://github.com/apache/airflow/issues/40788): @vatsrahul1001
       - [x] [Linked Issue #40471](https://github.com/apache/airflow/pull/40471): @boraberke
   - [x] [[Databricks Provider] Revert PRs #40864 and #40471 (#41050)](https://github.com/apache/airflow/pull/41050): @pankajkoti
     Linked issues:
       - [x] [Linked Issue #40864](https://github.com/apache/airflow/pull/40864): @boraberke
       - [x] [Linked Issue #35433](https://github.com/apache/airflow/issues/35433): @josh-fell
       - [x] [Linked Issue #40471](https://github.com/apache/airflow/pull/40471): @boraberke
## Provider [docker: 3.12.3rc1](https://pypi.org/project/apache-airflow-providers-docker/3.12.3rc1)
   - [x] [DockerSwarmOperator: Support line breaks in service logs (#40705)](https://github.com/apache/airflow/pull/40705): @mr1holmes
     Linked issues:
       - [x] [Linked Issue #40571](https://github.com/apache/airflow/issues/40571): @bladerail
## Provider [elasticsearch: 5.4.2rc1](https://pypi.org/project/apache-airflow-providers-elasticsearch/5.4.2rc1)
   - [x] [Clean up remaining getattr connection DbApiHook (#40665)](https://github.com/apache/airflow/pull/40665): @dabla
## Provider [exasol: 4.5.3rc1](https://pypi.org/project/apache-airflow-providers-exasol/4.5.3rc1)
   - [x] [Clean up remaining getattr connection DbApiHook (#40665)](https://github.com/apache/airflow/pull/40665): @dabla
## Provider [ftp: 3.10.1rc1](https://pypi.org/project/apache-airflow-providers-ftp/3.10.1rc1)
   - [ ] [openlineage: migrate OpenLineage provider to V2 facets. (#39530)](https://github.com/apache/airflow/pull/39530): @JDarDagran
## Provider [google: 10.21.1rc2](https://pypi.org/project/apache-airflow-providers-google/10.21.1rc2)
   - [x] [Dynamic import MySqlHook/PostgresHook in CloudSQLDatabaseHook (#41009)](https://github.com/apache/airflow/pull/41009): @jhongy1994
     Linked issues:
       - [x] [Linked Issue #40175](https://github.com/apache/airflow/issues/40175): @bourliam
   - [x] [Make google_analytics_admin system tests self sufficient (#40951)](https://github.com/apache/airflow/pull/40951): @moiseenkov
   - [ ] [Fix Custom Training Job operators to accept results without a model in def mode (#40685)](https://github.com/apache/airflow/pull/40685): @e-galan
     Linked issues:
       - [ ] [Linked Issue #40476](https://github.com/apache/airflow/issues/40476): @vignesh-sc
   - [x] [Fix behavior for reattach_state parameter in BigQueryInsertJobOperator (#40664)](https://github.com/apache/airflow/pull/40664): @VladaZakharova
   - [x] [Fix cloudsql-query system tests (#41092)](https://github.com/apache/airflow/pull/41092): @moiseenkov
   - [x] [Refactor dataproc system tests (#40720)](https://github.com/apache/airflow/pull/40720): @VladaZakharova
   - [ ] [openlineage: migrate OpenLineage provider to V2 facets. (#39530)](https://github.com/apache/airflow/pull/39530): @JDarDagran
   - [ ] [Resolve google cloud sql deprecation in tests (#40834)](https://github.com/apache/airflow/pull/40834): @dirrao
   - [ ] [Update default value for job_name in DataflowConfiguration (#40645)](https://github.com/apache/airflow/pull/40645): @e-galan
     Linked issues:
       - [ ] [Linked Issue #40515](https://github.com/apache/airflow/issues/40515): @turb
## Provider [jdbc: 4.4.0rc1](https://pypi.org/project/apache-airflow-providers-jdbc/4.4.0rc1)
   - [ ] [Create SQLAlchemy engine from connection in DB Hook and added autocommit param to insert_rows method (#40669)](https://github.com/apache/airflow/pull/40669): @dabla
   - [x] [Clean up remaining getattr connection DbApiHook (#40665)](https://github.com/apache/airflow/pull/40665): @dabla
## Provider [microsoft.azure: 10.3.0rc1](https://pypi.org/project/apache-airflow-providers-microsoft-azure/10.3.0rc1)
   - [ ] [Added priority to Azure Container Instances (#40616)](https://github.com/apache/airflow/pull/40616): @liamalsopp
     Linked issues:
       - [ ] [Linked Issue #40574](https://github.com/apache/airflow/issues/40574): @liamalsopp
   - [x] [Bump minimum version for azure containerinstance. (#40767)](https://github.com/apache/airflow/pull/40767): @potiuk
     Linked issues:
       - [x] [Linked Issue #40616](https://github.com/apache/airflow/pull/40616): @liamalsopp
## Provider [microsoft.mssql: 3.8.0rc1](https://pypi.org/project/apache-airflow-providers-microsoft-mssql/3.8.0rc1)
   - [x] [Added support for replace option when using insert_rows with MsSqlHook (#40836)](https://github.com/apache/airflow/pull/40836): @dabla
   - [x] [Clean up remaining getattr connection DbApiHook (#40665)](https://github.com/apache/airflow/pull/40665): @dabla
## Provider [mysql: 5.6.3rc1](https://pypi.org/project/apache-airflow-providers-mysql/5.6.3rc1)
   - [x] [Clean up remaining getattr connection DbApiHook (#40665)](https://github.com/apache/airflow/pull/40665): @dabla
## Provider [odbc: 4.6.3rc1](https://pypi.org/project/apache-airflow-providers-odbc/4.6.3rc1)
   - [x] [Clean up remaining getattr connection DbApiHook (#40665)](https://github.com/apache/airflow/pull/40665): @dabla
## Provider [openlineage: 1.10.0rc1](https://pypi.org/project/apache-airflow-providers-openlineage/1.10.0rc1)
   - [x] [openlineage: add AirflowRun on COMPLETE/FAIL events (#40996)](https://github.com/apache/airflow/pull/40996): @mobuchowski
   - [x] [openlineage: extend custom_run_facets to on_failed and on_complete (#40953)](https://github.com/apache/airflow/pull/40953): @kacpermuda
     Linked issues:
       - [ ] [Linked Issue #38982](https://github.com/apache/airflow/pull/38982): @anandhimurali
   - [x] [openlineage: migrate OpenLineage provider to V2 facets. (#39530)](https://github.com/apache/airflow/pull/39530): @JDarDagran
   - [x] [openlineage: Add AirflowDagRunFacet to dag runEvents (#40854)](https://github.com/apache/airflow/pull/40854): @dolfinus
     Linked issues:
       - [x] [Linked Issue #40798](https://github.com/apache/airflow/issues/40798): @dolfinus
   - [x] [[AIP-62] Translate AIP-60 URI to OpenLineage (#40173)](https://github.com/apache/airflow/pull/40173): @kacpermuda
     Linked issues:
       - [x] [Linked Issue #40335](https://github.com/apache/airflow/pull/40335): @JDarDagran
       - [x] [Linked Issue #38767](https://github.com/apache/airflow/issues/38767): @mobuchowski
   - [x] [Ability to add custom facet in OpenLineage events (#38982)](https://github.com/apache/airflow/pull/38982): @anandhimurali
   - [x] [AIP-62: add method to common.compat to not force hooks to try/except every time (#40812)](https://github.com/apache/airflow/pull/40812): @mobuchowski
   - [x] [openlineage: use airflow provided getters in conf (#40790)](https://github.com/apache/airflow/pull/40790): @kacpermuda
     Linked issues:
       - [x] [Linked Issue #40780](https://github.com/apache/airflow/pull/40780): @mobuchowski
       - [x] [Linked Issue #40589](https://github.com/apache/airflow/pull/40589): @mobuchowski
   - [x] [openlineage: add config to include 'full' task info based on conf setting (#40589)](https://github.com/apache/airflow/pull/40589): @mobuchowski
   - [x] [Add log_url to OpenLineage AirflowRunFacet (#40797)](https://github.com/apache/airflow/pull/40797): @dolfinus
   - [x] [openlineage: add deferrable information to task info in airflowRunFacet (#40682)](https://github.com/apache/airflow/pull/40682): @kacpermuda
     Linked issues:
       - [x] [Linked Issue #38264](https://github.com/apache/airflow/pull/38264): @kacpermuda
   - [x] [openlineage: adjust DefaultExtractor's on_failure detection for airflow 2.10 fix (#41094)](https://github.com/apache/airflow/pull/41094): @kacpermuda
     Linked issues:
       - [x] [Linked Issue #41053](https://github.com/apache/airflow/pull/41053): @kacpermuda
   - [x] [openlineage: make value of slots in attrs.define consistent (#40992)](https://github.com/apache/airflow/pull/40992): @kacpermuda
     Linked issues:
       - [x] [Linked Issue #40972](https://github.com/apache/airflow/pull/40972): @JDarDagran
   - [x] [openlineage: Set `slots` to True for facets used in dag level (#40972)](https://github.com/apache/airflow/pull/40972): @JDarDagran
   - [x] [openlineage: fix / add some task attributes in AirflowRunFacet (#40725)](https://github.com/apache/airflow/pull/40725): @kacpermuda
     Linked issues:
       - [x] [Linked Issue #40371](https://github.com/apache/airflow/pull/40371): @mobuchowski
   - [x] [openlineage: replace dt.now with airflow.utils.timezone.utcnow (#40887)](https://github.com/apache/airflow/pull/40887): @kacpermuda
   - [x] [openlineage: remove deprecated parentRun facet key (#40681)](https://github.com/apache/airflow/pull/40681): @kacpermuda
## Provider [pgvector: 1.2.2rc1](https://pypi.org/project/apache-airflow-providers-pgvector/1.2.2rc1)
   - [ ] [updating pgvector to !=0.3.0 to  avoid recent issue in latest releases(0.3.0) (#40683)](https://github.com/apache/airflow/pull/40683): @vatsrahul1001
## Provider [postgres: 5.11.3rc1](https://pypi.org/project/apache-airflow-providers-postgres/5.11.3rc1)
   - [x] [Clean up remaining getattr connection DbApiHook (#40665)](https://github.com/apache/airflow/pull/40665): @dabla
## Provider [qdrant: 1.1.2rc1](https://pypi.org/project/apache-airflow-providers-qdrant/1.1.2rc1)
   - [x] [Qdrant Provider: Bump Qdrant client to latest (#40831)](https://github.com/apache/airflow/pull/40831): @Anush008
## Provider [sftp: 4.10.3rc1](https://pypi.org/project/apache-airflow-providers-sftp/4.10.3rc1)
   - [x] [openlineage: migrate OpenLineage provider to V2 facets. (#39530)](https://github.com/apache/airflow/pull/39530): @JDarDagran
## Provider [slack: 8.8.0rc1](https://pypi.org/project/apache-airflow-providers-slack/8.8.0rc1)
   - [ ] [Add the ability to toggle the unfurl behavior for slack notifier (#40694)](https://github.com/apache/airflow/pull/40694): @armand-sauzay
   - [ ] [Add newly added unfurl args to the docstring of SlackNotifier (#40709)](https://github.com/apache/airflow/pull/40709): @armand-sauzay
## Provider [snowflake: 5.6.1rc1](https://pypi.org/project/apache-airflow-providers-snowflake/5.6.1rc1)
   - [x] [openlineage: migrate OpenLineage provider to V2 facets. (#39530)](https://github.com/apache/airflow/pull/39530): @JDarDagran
   - [x] [Clean up remaining getattr connection DbApiHook (#40665)](https://github.com/apache/airflow/pull/40665): @dabla
## Provider [sqlite: 3.8.2rc1](https://pypi.org/project/apache-airflow-providers-sqlite/3.8.2rc1)
   - [x] [Clean up remaining getattr connection DbApiHook (#40665)](https://github.com/apache/airflow/pull/40665): @dabla
## Provider [ssh: 3.12.0rc1](https://pypi.org/project/apache-airflow-providers-ssh/3.12.0rc1)
   - [x] [Add on kill to ssh (#40377)](https://github.com/apache/airflow/pull/40377): @MRLab12
     Linked issues:
       - [x] [Linked Issue #40343](https://github.com/apache/airflow/issues/40343): @potiuk
## Provider [teradata: 2.5.0rc1](https://pypi.org/project/apache-airflow-providers-teradata/2.5.0rc1)
   - [x] [Implemented Query Band Support for the Teradata provider (#40716)](https://github.com/apache/airflow/pull/40716): @sc250072
   - [x] [Clean up remaining getattr connection DbApiHook (#40665)](https://github.com/apache/airflow/pull/40665): @dabla
## Provider [ydb: 1.2.0rc1](https://pypi.org/project/apache-airflow-providers-ydb/1.2.0rc1)
   - [x] [ydb provider: add bulk upsert support (#40631)](https://github.com/apache/airflow/pull/40631): @uzhastik
   - [x] [Clean up remaining getattr connection DbApiHook (#40665)](https://github.com/apache/airflow/pull/40665): @dabla

<!--

NOTE TO RELEASE MANAGER:

You can move here the providers that have doc-only changes or for which changes are trivial, and
you could assess that they are OK.

-->
All users involved in the PRs:
@MRLab12 @uzhastik @dstandish @kacpermuda @jedcunningham @mr1holmes @gopidesupavan @moiseenkov @Anush008 @dirrao @o-nikolas @vatsrahul1001 @dabla @romsharon98 @e-galan @JDarDagran @ephraimbuddy

### Committer

- [X] I acknowledge that I am a maintainer/committer of the Apache Airflow project.",eladkal,2024-08-03 15:42:55+00:00,[],2024-08-06 20:39:29+00:00,2024-08-06 20:39:28+00:00,https://github.com/apache/airflow/issues/41237,"[('area:providers', ''), ('kind:meta', 'High-level information important to the community'), ('testing status', 'Status of testing releases')]","[{'comment_id': 2267518501, 'issue_id': 2446424589, 'author': 'jhongy1994', 'body': 'I checked #41009 (Linked issue #40175) and working as expected :)', 'created_at': datetime.datetime(2024, 8, 4, 12, 4, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2268294343, 'issue_id': 2446424589, 'author': 'moiseenkov', 'body': '#40951, #41092 work as expected', 'created_at': datetime.datetime(2024, 8, 5, 6, 43, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2268296472, 'issue_id': 2446424589, 'author': 'Anush008', 'body': '> Qdrant Provider: Bump Qdrant client to latest (#40831): @Anush008\n\nTested and verified working.', 'created_at': datetime.datetime(2024, 8, 5, 6, 44, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2268296941, 'issue_id': 2446424589, 'author': 'VladaZakharova', 'body': 'https://github.com/apache/airflow/pull/40664 and https://github.com/apache/airflow/pull/40720 work as expected', 'created_at': datetime.datetime(2024, 8, 5, 6, 45, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2268303311, 'issue_id': 2446424589, 'author': 'potiuk', 'body': 'Checked my changes - both look good !', 'created_at': datetime.datetime(2024, 8, 5, 6, 49, 17, tzinfo=datetime.timezone.utc)}, {'comment_id': 2268648523, 'issue_id': 2446424589, 'author': 'pankajkoti', 'body': 'Tested my changes in the Databricks provider, all look good!', 'created_at': datetime.datetime(2024, 8, 5, 9, 45, 54, tzinfo=datetime.timezone.utc)}, {'comment_id': 2268825522, 'issue_id': 2446424589, 'author': 'dolfinus', 'body': '#40854 and #40797 are fine', 'created_at': datetime.datetime(2024, 8, 5, 11, 16, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2269134134, 'issue_id': 2446424589, 'author': 'sc250072', 'body': 'https://github.com/apache/airflow/pull/40716 Tested and working fine. All system tests are getting success.', 'created_at': datetime.datetime(2024, 8, 5, 13, 52, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2269302768, 'issue_id': 2446424589, 'author': 'dabla', 'body': 'Tested following PR with odbc/jdbc/mssql and looking good:\r\n\r\n- [Clean up remaining getattr connection DbApiHook (#40665)](https://github.com/apache/airflow/pull/40665)\r\n- [Create SQLAlchemy engine from connection in DB Hook and added autocommit param to insert_rows method (#40669)](https://github.com/apache/airflow/pull/40669)\r\n- [Added support for replace option when using insert_rows with MsSqlHook (#40836)](https://github.com/apache/airflow/pull/40836)', 'created_at': datetime.datetime(2024, 8, 5, 15, 7, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2269335490, 'issue_id': 2446424589, 'author': 'kacpermuda', 'body': ""I've tested all OpenLineage provider changes and my PRs with some example DAGs, all seems to work fine."", 'created_at': datetime.datetime(2024, 8, 5, 15, 22, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2269946271, 'issue_id': 2446424589, 'author': 'gopidesupavan', 'body': 'Have tested #40976 , #40765 and #40896. All looks good. thank you.', 'created_at': datetime.datetime(2024, 8, 5, 21, 25, 35, tzinfo=datetime.timezone.utc)}, {'comment_id': 2270133013, 'issue_id': 2446424589, 'author': 'JDarDagran', 'body': 'Tested all my changes - all look fine.', 'created_at': datetime.datetime(2024, 8, 6, 0, 14, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2271471544, 'issue_id': 2446424589, 'author': 'uzhastik', 'body': 'ydb provider works as expected, checked https://github.com/apache/airflow/pull/40631\r\nthank you!', 'created_at': datetime.datetime(2024, 8, 6, 14, 43, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2271821435, 'issue_id': 2446424589, 'author': 'mr1holmes', 'body': 'Tested https://github.com/apache/airflow/pull/40705\r\nWorks as expected.', 'created_at': datetime.datetime(2024, 8, 6, 17, 45, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2272108762, 'issue_id': 2446424589, 'author': 'eladkal', 'body': 'Thank you everyone. Providers are released.\r\n\r\nI invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).', 'created_at': datetime.datetime(2024, 8, 6, 20, 39, 29, tzinfo=datetime.timezone.utc)}]","jhongy1994 on (2024-08-04 12:04:51 UTC): I checked #41009 (Linked issue #40175) and working as expected :)

moiseenkov on (2024-08-05 06:43:09 UTC): #40951, #41092 work as expected

Anush008 on (2024-08-05 06:44:41 UTC): Tested and verified working.

VladaZakharova on (2024-08-05 06:45:01 UTC): https://github.com/apache/airflow/pull/40664 and https://github.com/apache/airflow/pull/40720 work as expected

potiuk on (2024-08-05 06:49:17 UTC): Checked my changes - both look good !

pankajkoti on (2024-08-05 09:45:54 UTC): Tested my changes in the Databricks provider, all look good!

dolfinus on (2024-08-05 11:16:35 UTC): #40854 and #40797 are fine

sc250072 on (2024-08-05 13:52:14 UTC): https://github.com/apache/airflow/pull/40716 Tested and working fine. All system tests are getting success.

dabla on (2024-08-05 15:07:13 UTC): Tested following PR with odbc/jdbc/mssql and looking good:

- [Clean up remaining getattr connection DbApiHook (#40665)](https://github.com/apache/airflow/pull/40665)
- [Create SQLAlchemy engine from connection in DB Hook and added autocommit param to insert_rows method (#40669)](https://github.com/apache/airflow/pull/40669)
- [Added support for replace option when using insert_rows with MsSqlHook (#40836)](https://github.com/apache/airflow/pull/40836)

kacpermuda on (2024-08-05 15:22:55 UTC): I've tested all OpenLineage provider changes and my PRs with some example DAGs, all seems to work fine.

gopidesupavan on (2024-08-05 21:25:35 UTC): Have tested #40976 , #40765 and #40896. All looks good. thank you.

JDarDagran on (2024-08-06 00:14:32 UTC): Tested all my changes - all look fine.

uzhastik on (2024-08-06 14:43:40 UTC): ydb provider works as expected, checked https://github.com/apache/airflow/pull/40631
thank you!

mr1holmes on (2024-08-06 17:45:41 UTC): Tested https://github.com/apache/airflow/pull/40705
Works as expected.

eladkal (Issue Creator) on (2024-08-06 20:39:29 UTC): Thank you everyone. Providers are released.

I invite everyone to help improve providers for the next release, a list of open issues can be found [here](https://github.com/apache/airflow/issues?q=is%3Aopen+is%3Aissue+label%3Aarea%3Aproviders).

"
2446174073,issue,open,,The remote task tracked by the trigger failed due to the reassignment of the trigger task,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.7.2

### What happened?

The trigger instance handled by triggerer-2 was canceled due to reassignment to triggerer-1.
As a result, triggerer-2 cleaned up the local trigger instance, which terminated the remote job using its ID.
The trigger instance in triggerer-2 keep polling the status of remote job with the same id, but detected that the remote job was killed, so completed its run, and exited.
Consequently, the remote job failed due to the reassignment.

Production log summarized by time sequence
```
Time: 2024-07-11, 00:00:12 -07
Event: Start to run on worker
Runtime: worker
Log: {task_command.py:416} INFO - Running <TaskInstance: f67fb155-6ff2-4fde-bbb2-6f0ef16af05e.91fade50-a3da-4966-a0ff-44c24d59303a scheduled__2024-07-11T06:00:00+00:00 [running]> on host worker-host-3
----------------------------------

Time: 2024-07-11, 00:00:14 -07
Event: Defer to trigger
Runtime: worker
Log: Pausing task as DEFERRED. dag_id=f67fb155-6ff2-4fde-bbb2-6f0ef16af05e, task_id=91fade50-a3da-4966-a0ff-44c24d59303a, execution_date=20240711T060000, start_date=20240711T070011
----------------------------------

Time: 2024-07-11T00:00:54.788-0700
Event: Trigger instance-1 started
Runtime: triggerer-2
Log: {triggerer_job_runner.py:595} INFO - trigger f67fb155-6ff2-4fde-bbb2-6f0ef16af05e/scheduled__2024-07-11T06:00:00+00:00/91fade50-a3da-4966-a0ff-44c24d59303a/-1/1 (ID 78297) starting
----------------------------------

Time: 2024-07-11T00:00:54.788-0700
Event: Trigger instance-1 check job status
Runtime: triggerer-2
Log: {datax_trigger.py:26} INFO - Check if job finished.
----------------------------------

Time: 2024-07-11T00:00:54.823-0700
Event: Trigger instance-1 get job running
Runtime: triggerer-2
Log: {datax_trigger.py:32} INFO - {""code"":""0000"",""message"":""success"",""result"":{""dagRunId"":""manual__2024-07-11T07:00:43.473585+00:00"",""startTime"":""2024-07-11T00:00:14+00:00"",""endTime"":null,""status"":""RUNNING"",""extraInfo"":{""dagId"":""f67fb155-6ff2-4fde-bbb2-6f0ef16af05e-61566"",""status"":""<strong style=\""color:blue;\"">RUNNING</strong>""}}}
----------------------------------

Time: 2024-07-11T00:01:00.503-0700
Event: Triggerer-2 cancel trigger due to find the trigger has been occupied by another triggerer process
Runetime: triggerer-2
Log: {triggerer_job_runner.py:607} ERROR - Trigger cancelled; message=
----------------------------------

Time: 2024-07-11, 00:01:00 -07
Event: Kyuubi Trigger cleanup called
Runtime: triggerer-2
Log: {datax_trigger.py:51} INFO - Trigger is cancelled, clean up now.
----------------------------------

Time: 2024-07-11T00:01:00.977-0700
Event: Trigger instance-1 cleanup
Runeimte: triggerer-2
Log: {triggerer_job_runner.py:621} INFO - trigger f67fb155-6ff2-4fde-bbb2-6f0ef16af05e/scheduled__2024-07-11T06:00:00+00:00/91fade50-a3da-4966-a0ff-44c24d59303a/-1/1 (ID 78297) completed
----------------------------------

Time: 2024-07-11, 00:02:00 -07
Event: Trigger check if job success
Runtime: triggerer-1
Log: {datax_trigger.py:26} INFO - Check if job finished.
----------------------------------

Time: 2024-07-11, 00:02:00 -07
Event: check job status
Runtime: triggerer-1
Log: {datax_trigger.py:32} INFO - {""code"":""0000"",""message"":""success"",""result"":{""dagRunId"":""manual__2024-07-11T07:00:43.473585+00:00"",""startTime"":""2024-07-11T00:00:14+00:00"",""endTime"":null,""status"":""FAILED"",""extraInfo"":{""executionLogAddress"":""<a target=\""_blank\"" href=\""https://host/api/v1/logs?JobId=79028\"">Execution Log</a>"",""dagId"":""f67fb155-6ff2-4fde-bbb2-6f0ef16af05e-61566"",""status"":""<strong style=\""color:red;\"">FAILED</strong>""}}}
----------------------------------
```

### What you think should happen instead?

Triggers will be canceled and cleaned up by the current triggerer process if the current triggerer finds that those running trigger instances have been reassigned to other triggerers.
However, if the cleanup is called prematurely while the trigger is still in an active state, it could impact the same trigger when reassigned to another triggerer process.
Therefore, introducing the capability to disable this cleanup behavior in such scenarios.

### How to reproduce

1. Set the triggerer_health_check_threshold to a small number like 3(s).
2. Make triggerer instances at least 2.
3. Use blocking io like http request in the trigger's run method.
4. Trigger multiple deferred tasks(for our cases: 100+ trigger jobs) in the same time. 

### Operating System

Ubuntu

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",TakawaAkirayo,2024-08-03 06:39:18+00:00,['TakawaAkirayo'],2024-09-02 07:22:49+00:00,,https://github.com/apache/airflow/issues/41231,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:Triggerer', '')]","[{'comment_id': 2267489753, 'issue_id': 2446174073, 'author': 'tirkarthi', 'body': 'Can you please add sample code to reproduce this along with the trigger implementation? Does the trigger implement cleanup method?', 'created_at': datetime.datetime(2024, 8, 4, 10, 18, 51, tzinfo=datetime.timezone.utc)}, {'comment_id': 2270708754, 'issue_id': 2446174073, 'author': 'TakawaAkirayo', 'body': '@tirkarthi Below code can reproduce this issue along with the trigger\'s implementation with clean up\r\nI raised a PR for this issue: https://github.com/apache/airflow/pull/41232/files#diff-02359b2cc957e3a9be05ce055322558b36359c13572afc38f14f62a6dfae01c3 \r\n```\r\nclass RemoteJob:\r\n    RUNNING_STATUS = ""running""\r\n    FAILED_STATUS = ""failed""\r\n    SUCCESS_STATUS = ""success""\r\n\r\n    def __init__(self, status):\r\n        self.status = status\r\n\r\n    def get_status(self):\r\n        return self.status\r\n\r\n    def set_status(self, status):\r\n        self.status = status\r\n\r\n    def kill(self):\r\n        self.set_status(RemoteJob.FAILED_STATUS)\r\n\r\n\r\nclass RemoteService:\r\n    _instance = None\r\n\r\n    event_queue: list[tuple[RemoteJobTrigger, int]]\r\n    remote_jobs: dict[int, RemoteJob]\r\n\r\n    def __new__(cls):\r\n        if cls._instance is None:\r\n            cls._instance = super().__new__(cls)\r\n            cls._instance.event_queue = []\r\n            cls._instance.remote_jobs = {\r\n                1: RemoteJob(RemoteJob.RUNNING_STATUS),\r\n            }\r\n        return cls._instance\r\n\r\n    def complete_job(self, job_id: int):\r\n        self.remote_jobs[job_id].set_status(RemoteJob.SUCCESS_STATUS)\r\n\r\n    def kill_job(self, caller: RemoteJobTrigger, job_id: int):\r\n        self.event_queue.append((caller, job_id))\r\n        self.remote_jobs[job_id].kill()\r\n\r\n    def get_job(self, job_id: int):\r\n        return self.remote_jobs[job_id]\r\n\r\n    def reset(self):\r\n        self.event_queue = []\r\n        self.remote_jobs = {\r\n            1: RemoteJob(RemoteJob.RUNNING_STATUS),\r\n        }\r\n\r\n\r\nclass TriggerInstances:\r\n    _instance = None\r\n\r\n    def __new__(cls):\r\n        if cls._instance is None:\r\n            cls._instance = super().__new__(cls)\r\n            cls._instance.trigger_instances = []\r\n        return cls._instance\r\n\r\n    def reset(self):\r\n        self.trigger_instances.clear()\r\n\r\n\r\nclass RemoteJobTrigger(BaseTrigger):\r\n    def __init__(self, remote_job_id):\r\n        super().__init__()\r\n        self.remote_job_id = remote_job_id\r\n        self.finished = False\r\n        self.cleanup_done = False\r\n        self.last_status = None\r\n        TriggerInstances().trigger_instances.append(self)\r\n\r\n    def get_status(self):\r\n        return RemoteService().get_job(self.remote_job_id).get_status()\r\n\r\n    async def run(self):\r\n        print(f""Trigger object {id(self)}: start to run"")\r\n\r\n        while self.get_status() == RemoteJob.RUNNING_STATUS:\r\n            await asyncio.sleep(0.1)\r\n        self.finished = True\r\n        print(f""Trigger object {id(self)}: finished with status: {self.get_status()}"")\r\n        yield TriggerEvent(self.remote_job_id)\r\n\r\n    async def cleanup(self) -> None:\r\n        RemoteService().kill_job(self, self.remote_job_id)\r\n        self.cleanup_done = True\r\n        print(f""Trigger object {id(self)}: cleanup done"")\r\n\r\n    def has_been_cleaned(self):\r\n        return self.cleanup_done\r\n\r\n    def failed(self):\r\n        return self.get_status() == RemoteJob.FAILED_STATUS\r\n\r\n    def serialize(self):\r\n        return (\r\n            ""tests.jobs.test_triggerer_job.RemoteJobTrigger"",\r\n            {""remote_job_id"": self.remote_job_id},\r\n        )\r\n\r\n\r\n@pytest.mark.asyncio\r\nasync def test_disable_trigger_cleanup_on_reassigned_to_other_triggerers(session, tmp_path):\r\n\r\n    from typing import Callable, Awaitable\r\n    from airflow.configuration import conf\r\n    from airflow.jobs.triggerer_job_runner import TriggerDetails\r\n\r\n    remote_service = RemoteService()\r\n    # The trigger will be created sequentially,\r\n    # so we can use a global variable to track the trigger instances\r\n    trigger_instances_holder = TriggerInstances()\r\n\r\n    # Build the scenario that the trigger reassigned to another triggerer\r\n    # due to the first triggerer missed heartbeat\r\n    async def make_triggerer2_occupy_the_trigger_of_triggerer1(\r\n        check: Callable[[TriggerDetails, TriggerDetails], Awaitable[None]]\r\n    ):\r\n\r\n        # Create trigger to check the remote job status\r\n        trigger = RemoteJobTrigger(1)\r\n        # Remove this one from the trigger_instances_holder, we only need to track the instances\r\n        # that created by the triggerer job runner\r\n        del trigger_instances_holder.trigger_instances[0]\r\n\r\n        trigger_orm = Trigger.from_object(trigger)\r\n        trigger_orm.id = 1\r\n        session.add(trigger_orm)\r\n\r\n        dag = DagModel(dag_id=""test-dag"")\r\n        dag_run = DagRun(dag.dag_id, run_id=""abc"", run_type=""none"")\r\n        ti = TaskInstance(\r\n            PythonOperator(task_id=""dummy-task"", python_callable=print),\r\n            run_id=dag_run.run_id,\r\n            state=TaskInstanceState.DEFERRED,\r\n        )\r\n        ti.dag_id = dag.dag_id\r\n        ti.trigger_id = 1\r\n        session.add(dag)\r\n        session.add(dag_run)\r\n        session.add(ti)\r\n\r\n        job1 = Job()\r\n        health_check_threshold_value = conf.getfloat(""triggerer"", ""triggerer_health_check_threshold"")\r\n        # triggerer1 miss heartbeat due to high host load, so triggerer2 can pick up the trigger\r\n        job1.latest_heartbeat = timezone.utcnow() - datetime.timedelta(\r\n            seconds=health_check_threshold_value + 1)\r\n\r\n        job2 = Job()\r\n        session.add(job1)\r\n        session.add(job2)\r\n\r\n        session.commit()\r\n\r\n        job_runner1 = TriggererJobRunner(job1)\r\n        job_runner2 = TriggererJobRunner(job2)\r\n\r\n        # job_runner1 load triggers, it will pick up the trigger\r\n        job_runner1.load_triggers()\r\n        assert len(job_runner1.trigger_runner.to_create) == 1\r\n        # Create the trigger and start running\r\n        await job_runner1.trigger_runner.create_triggers()\r\n        assert len(job_runner1.trigger_runner.triggers) == 1\r\n\r\n        job2.heartbeat(lambda x: {}, session)\r\n        job_runner2.load_triggers()\r\n        assert len(job_runner2.trigger_runner.to_create) == 1\r\n        await job_runner2.trigger_runner.create_triggers()\r\n        # Now job_runner2 will take over the trigger for execution\r\n        assert len(job_runner2.trigger_runner.triggers) == 1\r\n\r\n        # Globally, there are two trigger instances\r\n        assert len(trigger_instances_holder.trigger_instances) == 2\r\n\r\n        # job_runner2 performs heartbeat normally\r\n        job2.heartbeat(lambda x: {}, session)\r\n\r\n        # Let the job_runner2 start to run\r\n        job_runner2.trigger_runner.daemon = True\r\n        job_runner2.trigger_runner.start()\r\n\r\n        # Now the job_runner1 will find that the trigger has been reassigned to someone else\r\n        # and will cancel the trigger\r\n        job_runner1.load_triggers()\r\n        job_runner1.trigger_runner.daemon = True\r\n        job_runner1.trigger_runner.start()\r\n\r\n        try:\r\n            trigger_details_of_job1 = job_runner1.trigger_runner.triggers[1]\r\n            trigger_details_of_job2 = job_runner2.trigger_runner.triggers[1]\r\n            await check(trigger_details_of_job1, trigger_details_of_job2)\r\n        finally:\r\n            job_runner1.trigger_runner.stop = True\r\n            job_runner1.trigger_runner.join(30)\r\n\r\n            job_runner2.trigger_runner.stop = True\r\n            job_runner2.trigger_runner.join(30)\r\n\r\n            # clean up state\r\n            remote_service.reset()\r\n            trigger_instances_holder.reset()\r\n            session.delete(trigger_orm)\r\n            session.delete(ti)\r\n            session.delete(dag_run)\r\n            session.delete(dag)\r\n            session.commit()\r\n\r\n    async def expect_the_trigger_will_be_cleanup_on_reassigned(\r\n        trigger_details1: TriggerDetails, trigger_details2: TriggerDetails\r\n    ):\r\n        # The trigger of job_runner2 should finish because the trigger of job_runner1 has been cancelled,\r\n        # and it will terminate the remote job when cleanup is called\r\n        trigger_of_job_2 = trigger_instances_holder.trigger_instances[1]\r\n        while trigger_of_job_2.has_been_cleaned() is False:\r\n            await asyncio.sleep(0.1)\r\n        # If reach here, the trigger of job_runner2 should be finished and cleaned up\r\n        assert trigger_of_job_2.finished is True\r\n        # The remote job should fail due to the trigger of job_runner1 has been cancelled,\r\n        # and it has cleaned up the same remote job\r\n        assert trigger_of_job_2.failed() is True\r\n\r\n        # The cancelled trigger should already be cleaned up\r\n        cancelled_trigger = trigger_instances_holder.trigger_instances[0]\r\n        assert cancelled_trigger.has_been_cleaned() is True\r\n\r\n        # The remote job should have failed because the trigger of job_runner1 was cancelled\r\n        assert remote_service.get_job(1).get_status() == RemoteJob.FAILED_STATUS\r\n\r\n        # There should be two kill events, each from a different trigger instance\r\n        # managed by separate triggerers\r\n        assert len(remote_service.event_queue) == 2\r\n        # The first kill job request comes from the canceled trigger of job_runner1\r\n        (cancel_caller, remote_job_id) = remote_service.event_queue[0]\r\n        assert cancel_caller == cancelled_trigger and remote_job_id == 1\r\n\r\n    await make_triggerer2_occupy_the_trigger_of_triggerer1(expect_the_trigger_will_be_cleanup_on_reassigned)\r\n```', 'created_at': datetime.datetime(2024, 8, 6, 8, 38, 53, tzinfo=datetime.timezone.utc)}, {'comment_id': 2270799382, 'issue_id': 2446174073, 'author': 'tirkarthi', 'body': 'cleanup method is called when trigger is cancelled due to triggerer restart, reassignment of trigger etc. You need to implement something like safe_to_cancel method in the linked PR before doing the actual cancellation.\r\n\r\nhttps://github.com/apache/airflow/pull/39442', 'created_at': datetime.datetime(2024, 8, 6, 9, 18, 46, tzinfo=datetime.timezone.utc)}, {'comment_id': 2272558126, 'issue_id': 2446174073, 'author': 'TakawaAkirayo', 'body': '> cleanup method is called when trigger is cancelled due to triggerer restart, reassignment of trigger etc. You need to implement something like safe_to_cancel method in the linked PR before doing the actual cancellation.\r\n> \r\n> #39442\r\n\r\nMany thanks for the suggestion, safe_to_cancel is a good idea, I think give the trigger more context like why this is canceled could help', 'created_at': datetime.datetime(2024, 8, 7, 3, 45, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2323998763, 'issue_id': 2446174073, 'author': 'EMT-Swaroop', 'body': 'Hey Team,\r\n\r\nWe are using Airflow 2.9.3 version in dev, preprod and prod. But in preprod and prod triggerer is continuously getting restarted but not in dev. We have checked the resource limits, and the consumption is normal for CPU and memory. No DAG changes have also been done. Nothing from k8s side as well.\r\n\r\nBelow are the logs we are getting\r\n\r\n[Info] Handling Signal: term\r\n[Info] Worker exiting (pid: 24)\r\n[Info] Worker exiting (pid: 25)\r\n{triggerer_job_runner.py: 350} Exited Trigger Loop\r\n\r\nWe are using the default probe values from parent chart.\r\n\r\nPlease let me know if there is any reason or fix.', 'created_at': datetime.datetime(2024, 9, 2, 7, 22, 48, tzinfo=datetime.timezone.utc)}]","tirkarthi on (2024-08-04 10:18:51 UTC): Can you please add sample code to reproduce this along with the trigger implementation? Does the trigger implement cleanup method?

TakawaAkirayo (Issue Creator) on (2024-08-06 08:38:53 UTC): @tirkarthi Below code can reproduce this issue along with the trigger's implementation with clean up
I raised a PR for this issue: https://github.com/apache/airflow/pull/41232/files#diff-02359b2cc957e3a9be05ce055322558b36359c13572afc38f14f62a6dfae01c3 
```
class RemoteJob:
    RUNNING_STATUS = ""running""
    FAILED_STATUS = ""failed""
    SUCCESS_STATUS = ""success""

    def __init__(self, status):
        self.status = status

    def get_status(self):
        return self.status

    def set_status(self, status):
        self.status = status

    def kill(self):
        self.set_status(RemoteJob.FAILED_STATUS)


class RemoteService:
    _instance = None

    event_queue: list[tuple[RemoteJobTrigger, int]]
    remote_jobs: dict[int, RemoteJob]

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance.event_queue = []
            cls._instance.remote_jobs = {
                1: RemoteJob(RemoteJob.RUNNING_STATUS),
            }
        return cls._instance

    def complete_job(self, job_id: int):
        self.remote_jobs[job_id].set_status(RemoteJob.SUCCESS_STATUS)

    def kill_job(self, caller: RemoteJobTrigger, job_id: int):
        self.event_queue.append((caller, job_id))
        self.remote_jobs[job_id].kill()

    def get_job(self, job_id: int):
        return self.remote_jobs[job_id]

    def reset(self):
        self.event_queue = []
        self.remote_jobs = {
            1: RemoteJob(RemoteJob.RUNNING_STATUS),
        }


class TriggerInstances:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance.trigger_instances = []
        return cls._instance

    def reset(self):
        self.trigger_instances.clear()


class RemoteJobTrigger(BaseTrigger):
    def __init__(self, remote_job_id):
        super().__init__()
        self.remote_job_id = remote_job_id
        self.finished = False
        self.cleanup_done = False
        self.last_status = None
        TriggerInstances().trigger_instances.append(self)

    def get_status(self):
        return RemoteService().get_job(self.remote_job_id).get_status()

    async def run(self):
        print(f""Trigger object {id(self)}: start to run"")

        while self.get_status() == RemoteJob.RUNNING_STATUS:
            await asyncio.sleep(0.1)
        self.finished = True
        print(f""Trigger object {id(self)}: finished with status: {self.get_status()}"")
        yield TriggerEvent(self.remote_job_id)

    async def cleanup(self) -> None:
        RemoteService().kill_job(self, self.remote_job_id)
        self.cleanup_done = True
        print(f""Trigger object {id(self)}: cleanup done"")

    def has_been_cleaned(self):
        return self.cleanup_done

    def failed(self):
        return self.get_status() == RemoteJob.FAILED_STATUS

    def serialize(self):
        return (
            ""tests.jobs.test_triggerer_job.RemoteJobTrigger"",
            {""remote_job_id"": self.remote_job_id},
        )


@pytest.mark.asyncio
async def test_disable_trigger_cleanup_on_reassigned_to_other_triggerers(session, tmp_path):

    from typing import Callable, Awaitable
    from airflow.configuration import conf
    from airflow.jobs.triggerer_job_runner import TriggerDetails

    remote_service = RemoteService()
    # The trigger will be created sequentially,
    # so we can use a global variable to track the trigger instances
    trigger_instances_holder = TriggerInstances()

    # Build the scenario that the trigger reassigned to another triggerer
    # due to the first triggerer missed heartbeat
    async def make_triggerer2_occupy_the_trigger_of_triggerer1(
        check: Callable[[TriggerDetails, TriggerDetails], Awaitable[None]]
    ):

        # Create trigger to check the remote job status
        trigger = RemoteJobTrigger(1)
        # Remove this one from the trigger_instances_holder, we only need to track the instances
        # that created by the triggerer job runner
        del trigger_instances_holder.trigger_instances[0]

        trigger_orm = Trigger.from_object(trigger)
        trigger_orm.id = 1
        session.add(trigger_orm)

        dag = DagModel(dag_id=""test-dag"")
        dag_run = DagRun(dag.dag_id, run_id=""abc"", run_type=""none"")
        ti = TaskInstance(
            PythonOperator(task_id=""dummy-task"", python_callable=print),
            run_id=dag_run.run_id,
            state=TaskInstanceState.DEFERRED,
        )
        ti.dag_id = dag.dag_id
        ti.trigger_id = 1
        session.add(dag)
        session.add(dag_run)
        session.add(ti)

        job1 = Job()
        health_check_threshold_value = conf.getfloat(""triggerer"", ""triggerer_health_check_threshold"")
        # triggerer1 miss heartbeat due to high host load, so triggerer2 can pick up the trigger
        job1.latest_heartbeat = timezone.utcnow() - datetime.timedelta(
            seconds=health_check_threshold_value + 1)

        job2 = Job()
        session.add(job1)
        session.add(job2)

        session.commit()

        job_runner1 = TriggererJobRunner(job1)
        job_runner2 = TriggererJobRunner(job2)

        # job_runner1 load triggers, it will pick up the trigger
        job_runner1.load_triggers()
        assert len(job_runner1.trigger_runner.to_create) == 1
        # Create the trigger and start running
        await job_runner1.trigger_runner.create_triggers()
        assert len(job_runner1.trigger_runner.triggers) == 1

        job2.heartbeat(lambda x: {}, session)
        job_runner2.load_triggers()
        assert len(job_runner2.trigger_runner.to_create) == 1
        await job_runner2.trigger_runner.create_triggers()
        # Now job_runner2 will take over the trigger for execution
        assert len(job_runner2.trigger_runner.triggers) == 1

        # Globally, there are two trigger instances
        assert len(trigger_instances_holder.trigger_instances) == 2

        # job_runner2 performs heartbeat normally
        job2.heartbeat(lambda x: {}, session)

        # Let the job_runner2 start to run
        job_runner2.trigger_runner.daemon = True
        job_runner2.trigger_runner.start()

        # Now the job_runner1 will find that the trigger has been reassigned to someone else
        # and will cancel the trigger
        job_runner1.load_triggers()
        job_runner1.trigger_runner.daemon = True
        job_runner1.trigger_runner.start()

        try:
            trigger_details_of_job1 = job_runner1.trigger_runner.triggers[1]
            trigger_details_of_job2 = job_runner2.trigger_runner.triggers[1]
            await check(trigger_details_of_job1, trigger_details_of_job2)
        finally:
            job_runner1.trigger_runner.stop = True
            job_runner1.trigger_runner.join(30)

            job_runner2.trigger_runner.stop = True
            job_runner2.trigger_runner.join(30)

            # clean up state
            remote_service.reset()
            trigger_instances_holder.reset()
            session.delete(trigger_orm)
            session.delete(ti)
            session.delete(dag_run)
            session.delete(dag)
            session.commit()

    async def expect_the_trigger_will_be_cleanup_on_reassigned(
        trigger_details1: TriggerDetails, trigger_details2: TriggerDetails
    ):
        # The trigger of job_runner2 should finish because the trigger of job_runner1 has been cancelled,
        # and it will terminate the remote job when cleanup is called
        trigger_of_job_2 = trigger_instances_holder.trigger_instances[1]
        while trigger_of_job_2.has_been_cleaned() is False:
            await asyncio.sleep(0.1)
        # If reach here, the trigger of job_runner2 should be finished and cleaned up
        assert trigger_of_job_2.finished is True
        # The remote job should fail due to the trigger of job_runner1 has been cancelled,
        # and it has cleaned up the same remote job
        assert trigger_of_job_2.failed() is True

        # The cancelled trigger should already be cleaned up
        cancelled_trigger = trigger_instances_holder.trigger_instances[0]
        assert cancelled_trigger.has_been_cleaned() is True

        # The remote job should have failed because the trigger of job_runner1 was cancelled
        assert remote_service.get_job(1).get_status() == RemoteJob.FAILED_STATUS

        # There should be two kill events, each from a different trigger instance
        # managed by separate triggerers
        assert len(remote_service.event_queue) == 2
        # The first kill job request comes from the canceled trigger of job_runner1
        (cancel_caller, remote_job_id) = remote_service.event_queue[0]
        assert cancel_caller == cancelled_trigger and remote_job_id == 1

    await make_triggerer2_occupy_the_trigger_of_triggerer1(expect_the_trigger_will_be_cleanup_on_reassigned)
```

tirkarthi on (2024-08-06 09:18:46 UTC): cleanup method is called when trigger is cancelled due to triggerer restart, reassignment of trigger etc. You need to implement something like safe_to_cancel method in the linked PR before doing the actual cancellation.

https://github.com/apache/airflow/pull/39442

TakawaAkirayo (Issue Creator) on (2024-08-07 03:45:32 UTC): Many thanks for the suggestion, safe_to_cancel is a good idea, I think give the trigger more context like why this is canceled could help

EMT-Swaroop on (2024-09-02 07:22:48 UTC): Hey Team,

We are using Airflow 2.9.3 version in dev, preprod and prod. But in preprod and prod triggerer is continuously getting restarted but not in dev. We have checked the resource limits, and the consumption is normal for CPU and memory. No DAG changes have also been done. Nothing from k8s side as well.

Below are the logs we are getting

[Info] Handling Signal: term
[Info] Worker exiting (pid: 24)
[Info] Worker exiting (pid: 25)
{triggerer_job_runner.py: 350} Exited Trigger Loop

We are using the default probe values from parent chart.

Please let me know if there is any reason or fix.

"
2445914448,issue,closed,completed,TriggerDagRunOperator doesn't allow empty failed_states,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.7.1

### What happened?



In some cases, our company need to mark TriggerDagRunOperator as Success even if the triggered DAG failed. 
But TriggerDagRunOperator doesn't allow that. When I set failed_states to be an empty list, the following code will always add ""failed"" to failed_states
https://github.com/apache/airflow/blob/e3525cf4aa13dccc8c8b04603eef9f9692818a3b/airflow/operators/trigger_dagrun.py#L153

### What you think should happen instead?

It should be like this:
```
        if failed_states or failed_states == []:
            self.failed_states = [DagRunState(s) for s in failed_states]
        else:
            self.failed_states = [DagRunState.FAILED]
```

### How to reproduce

Try the setting as follows, and let the triggered dag failed.
```
TriggerDagRunOperator(
            task_id='abc',
            trigger_dag_id=dag_id,
            execution_date='{{execution_date}}',
            wait_for_completion=True,
            poke_interval=60 * 30,  # 30 mins
            allowed_states=[State.SUCCESS, State.FAILED],
            failed_states=[],
            deferrable=True,
        )
```

### Operating System

linux

### Versions of Apache Airflow Providers

airflow 2.7.3/ Cloud composer 2.8.1

### Deployment

Google Cloud Composer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",zhijie-dv01,2024-08-03 00:33:04+00:00,[],2024-08-04 19:59:07+00:00,2024-08-04 19:52:14+00:00,https://github.com/apache/airflow/issues/41229,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:core-operators', 'Operators, Sensors and hooks within Core Airflow'), ('affected_version:2.7', 'Issues Reported for 2.7')]","[{'comment_id': 2266300773, 'issue_id': 2445914448, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 3, 0, 33, 7, tzinfo=datetime.timezone.utc)}, {'comment_id': 2267098664, 'issue_id': 2445914448, 'author': 'romsharon98', 'body': 'Why you would like to pass it empty list?\r\nPassing `None` will solve your problem.', 'created_at': datetime.datetime(2024, 8, 3, 18, 43, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2267147939, 'issue_id': 2445914448, 'author': 'zhijie-dv01', 'body': '> Why you would like to pass it empty list? Passing `None` will solve your problem.\r\n\r\n@romsharon98  In the if else condition, passing None will still go to ""else"" and then add DagRunState.FAILED to failed_states', 'created_at': datetime.datetime(2024, 8, 3, 20, 57, 27, tzinfo=datetime.timezone.utc)}, {'comment_id': 2267339242, 'issue_id': 2445914448, 'author': 'phi-friday', 'body': 'Until this issue is resolved,\r\nYou might want to try using `post_execute` or `on_execute_callback` or `on_failure_callback`.\r\nMaybe that will help.', 'created_at': datetime.datetime(2024, 8, 4, 5, 9, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2267619794, 'issue_id': 2445914448, 'author': 'zhijie-dv01', 'body': '@romsharon98 @hussein-awala  I’m unsure about the collaboration process here. I indicated my willingness to submit a PR and proposed a solution, yet my idea was used to raise a PR by someone else. Could future opportunities be left for new contributors like myself?', 'created_at': datetime.datetime(2024, 8, 4, 17, 53, 59, tzinfo=datetime.timezone.utc)}, {'comment_id': 2267651830, 'issue_id': 2445914448, 'author': 'potiuk', 'body': ""> @romsharon98 @hussein-awala I’m unsure about the collaboration process here. I indicated my willingness to submit a PR and proposed a solution, yet my idea was used to raise a PR by someone else. Could future opportunities be left for new contributors like myself?\r\n\r\nHappens. You need to allow for some room of uncertainty and overlap. Sometimes two people will work on the same things. Sometimes people will say they work on something - but they don't. This is life happening. You can try to prevent this from happening and put a guard on every step or you allow for imperfectness and occasional mishups.\r\n\r\nAccept the imperfectness. Done is better than perfect. There are plenty of things to work on, and there is also a saying that copying is the best flatter you can have - also actually when your idea is done by someone else is  THE BEST THING EVER. You sparked the idea, someone else did it. Your idea got implemented and you spent 0 time on it, and can come with few more ideas in this time. The overall community effect of it is hugely positive,"", 'created_at': datetime.datetime(2024, 8, 4, 19, 58, 30, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-03 00:33:07 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

romsharon98 on (2024-08-03 18:43:59 UTC): Why you would like to pass it empty list?
Passing `None` will solve your problem.

zhijie-dv01 (Issue Creator) on (2024-08-03 20:57:27 UTC): @romsharon98  In the if else condition, passing None will still go to ""else"" and then add DagRunState.FAILED to failed_states

phi-friday on (2024-08-04 05:09:26 UTC): Until this issue is resolved,
You might want to try using `post_execute` or `on_execute_callback` or `on_failure_callback`.
Maybe that will help.

zhijie-dv01 (Issue Creator) on (2024-08-04 17:53:59 UTC): @romsharon98 @hussein-awala  I’m unsure about the collaboration process here. I indicated my willingness to submit a PR and proposed a solution, yet my idea was used to raise a PR by someone else. Could future opportunities be left for new contributors like myself?

potiuk on (2024-08-04 19:58:30 UTC): Happens. You need to allow for some room of uncertainty and overlap. Sometimes two people will work on the same things. Sometimes people will say they work on something - but they don't. This is life happening. You can try to prevent this from happening and put a guard on every step or you allow for imperfectness and occasional mishups.

Accept the imperfectness. Done is better than perfect. There are plenty of things to work on, and there is also a saying that copying is the best flatter you can have - also actually when your idea is done by someone else is  THE BEST THING EVER. You sparked the idea, someone else did it. Your idea got implemented and you spent 0 time on it, and can come with few more ideas in this time. The overall community effect of it is hugely positive,

"
2445442693,issue,closed,completed,Unexpected String Concatenation Issue in Airflow 2.7.2,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.7.2

### What happened?

I am encountering an unexpected issue with string concatenation in Airflow 2.7.2 using Python 3.11.5. The issue only occurs in Airflow, while the same string concatenation works correctly in local unit tests.

### What you think should happen instead?

The concatenated string values_str_3 should be correctly formatted and match the output of values_str and values_str_2

### How to reproduce

Use the following code snippet in an Airflow DAG or script:

```
values = ['1234', '5678', 'ABC_123', 'xyz-calc', '2024-01-01', 'NULL', '9876', 'NULL', 'example', 42, '2024-07-28T01:23:45.678', '2024-07-28T02:34:56.789', '2024-07-28T03:45:67.890', 'user_test', 'complete', '2024-07-28T04:56:78.901', '2024-07-28T05:67:89.012', 'NULL', 'spark-calc-1234-driver', 'NULL', 'NULL', 'XYZ']

values_str_list = []
for value in values:
    if isinstance(value, int):
        values_str_list.append(str(value))
    elif value == 'NULL':
        values_str_list.append('NULL')
    else:
        values_str_list.append(f""'{value}'"")

values_str = ','.join(values_str_list)  # This concatenation works correctly
print(""Concatenated string values_str:"")
print(values_str)

values_str_2 = ', '.join(values_str_list)  # This concatenation works correctly
print(""Concatenated string values_str_2:"")
print(values_str_2)

values_str_3 = ',\n    '.join(values_str_list)  # This concatenation does NOT work correctly
print(""Concatenated string values_str_3:"")
print(values_str_3)
logging.info(""Concatenated string values_str_3:"")
logging.info(values_str_3)
```

The concatenated string values_str_3 is incorrectly formatted in Airflow logs.

Logs:

```
[2024-07-29, 22:12:39 UTC] {logging_mixin.py:151} INFO - Concatenated string values_str:
[2024-07-29, 22:12:39 UTC] {logging_mixin.py:151} INFO - 1234,5678,'ABC_123','xyz-calc','2024-01-01',NULL,'9876',NULL,'example',42,'2024-07-28T01:23:45.678','2024-07-28T02:34:56.789','2024-07-28T03:45:67.890','user_test','complete','2024-07-28T04:56:78.901','2024-07-28T05:67:89.012',NULL,'spark-calc-1234-driver',NULL,NULL,'XYZ'
[2024-07-29, 22:12:39 UTC] {logging_mixin.py:151} INFO - Concatenated string values_str_2:
[2024-07-29, 22:12:39 UTC] {logging_mixin.py:151} INFO - 1234, 5678, 'ABC_123', 'xyz-calc', '2024-01-01', NULL, '9876', NULL, 'example', 42, '2024-07-28T01:23:45.678', '2024-07-28T02:34:56.789', '2024-07-28T03:45:67.890', 'user_test', 'complete', '2024-07-28T04:56:78.901', '2024-07-28T05:67:89.012', NULL, 'spark-calc-1234-driver', NULL, NULL, 'XYZ'
[2024-07-29, 22:12:39 UTC] {logging_mixin.py:151} INFO - Concatenated string values_str_3:
[2024-07-29, 22:12:39 UTC] {logging_mixin.py:151} INFO - 1234,
    5678,
    'ABC_123',
    'xyz-calc',
    '2024-01-01',
    NULL,
    '9876',
    NULL,
    'example',
    42,
    '2024-07-28T01:23:45.678',
    '2024-07-28T02:34:56.789',
    '2024-07-28T03:45:67.890',
    'user_test',
    'complete',
    '2024-07-28T04:56:78.901',
    '2024-07-28T05:67:89.012',
    NULL,
    'spark-calc-1234-driver',
    NULL,
    'XYZ'
[2024-07-29, 22:12:39 UTC] {2044_16_subscription.py:471} INFO - Concatenated string values_str_3:
[2024-07-29, 22:12:39 UTC] {2044_16_subscription.py:472} INFO - 1234,
    5678,
    'ABC_123',
    'xyz-calc',
    '2024-01-01',
    NULL,
    '9876',
    NULL,
    'example',
    42,
    '2024-07-28T01:23:45.678',
    '2024-07-28T02:34:56.789',
    '2024-07-28T03:45:67.890',
    'user_test',
    'complete',
    '2024-07-28T04:56:78.901',
    '2024-07-28T05:67:89.012',
    NULL,
    'spark-calc-1234-driver',
    NULL,
    'XYZ'

```

### Operating System

Linux

### Versions of Apache Airflow Providers

_No response_

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

Interestingly, when the concatenated string is split, the result is correct:

```
print(""Original string values_str:"", values_str.split(','))
print(""Original string values_str_2:"", values_str_2.split(', '))
print(""Original string values_str_3:"", values_str_3.split(',\n    '))
```

Logs:
```

[2024-07-29, 22:12:39 UTC] {logging_mixin.py:151} INFO - Original string values_str: ['1234', '5678', ""'ABC_123'"", ""'xyz-calc'"", ""'2024-01-01'"", 'NULL', ""'9876'"", 'NULL', ""'example'"", '42', ""'2024-07-28T01:23:45.678'"", ""'2024-07-28T02:34:56.789'"", ""'2024-07-28T03:45:67.890'"", ""'user_test'"", ""'complete'"", ""'2024-07-28T04:56:78.901'"", ""'2024-07-28T05:67:89.012'"", 'NULL', ""'spark-calc-1234-driver'"", 'NULL', 'NULL', ""'XYZ'""]
[2024-07-29, 22:12:39 UTC] {logging_mixin.py:151} INFO - Original string values_str_2: ['1234', '5678', ""'ABC_123'"", ""'xyz-calc'"", ""'2024-01-01'"", 'NULL', ""'9876'"", 'NULL', ""'example'"", 42, ""'2024-07-28T01:23:45.678'"", ""'2024-07-28T02:34:56.789'"", ""'2024-07-28T03:45:67.890'"", ""'user_test'"", ""'complete'"", ""'2024-07-28T04:56:78.901'"", ""'2024-07-28T05:67:89.012'"", 'NULL', ""'spark-calc-1234-driver'"", 'NULL', 'NULL', ""'XYZ'""]
[2024-07-29, 22:12:39 UTC] {logging_mixin.py:151} INFO - Original string values_str_3: ['1234', '5678', ""'ABC_123'"", ""'xyz-calc'"", ""'2024-01-01'"", 'NULL', ""'9876'"", 'NULL', ""'example'"", '42', ""'2024-07-28T01:23:45.678'"", ""'2024-07-28T02:34:56.789'"", ""'2024-07-28T03:45:67.890'"", ""'user_test'"", ""'complete'"", ""'2024-07-28T04:56:78.901'"", ""'2024-07-28T05:67:89.012'"", 'NULL', ""'spark-calc-1234-driver'"", 'NULL', 'NULL', ""'XYZ'""]
```









### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",artemSSSS,2024-08-02 17:18:44+00:00,[],2024-08-02 17:31:30+00:00,2024-08-02 17:31:30+00:00,https://github.com/apache/airflow/issues/41224,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2265828876, 'issue_id': 2445442693, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 2, 17, 18, 47, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-02 17:18:47 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2445115977,issue,closed,completed,Filesystem to Gitsync Migration,"### What do you see as an issue?

I am going to migrate airflow to use gitsync, where currently DAGs are baked into a custom container image.

I have a question around this, having spotted in the database that DAGs have a file path defined.

Is this a straightforward switch, is any more steps required after switiching the gitsync and will the existing DAGs be synchronized with those from the gitsync repo going forward?

Thanks

### Solving the problem

_No response_

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",antwacky,2024-08-02 14:42:40+00:00,[],2024-08-02 16:32:41+00:00,2024-08-02 16:32:41+00:00,https://github.com/apache/airflow/issues/41221,"[('kind:bug', 'This is a clearly a bug'), ('kind:documentation', ''), ('area:core', ''), ('needs-triage', ""label for new issues that we didn't triage yet"")]","[{'comment_id': 2265561491, 'issue_id': 2445115977, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 2, 14, 42, 43, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-02 14:42:43 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

"
2444388836,issue,closed,not_planned,Airflow task takes success status if driver was deleted,"### Apache Airflow Provider(s)

cncf-kubernetes

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes 8.3.1

### Apache Airflow version

2.9.2

### Operating System

Debian GNU/Linux 12 (bookworm)

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### What happened

If driver pod was deleted (manually), SparkKubernetesOperator task take '**success**' status, despite driver wasn't complete all the jobs of SparkApplication.

### What you think should happen instead

If driver pod was deleted, but wasn't complete his work, SparkKubernetesOperator task should take '**failed**' status.

### How to reproduce

Start SparkApplication using SparkKubernetesOperator task. Wait until driver pod will be created. Delete driver pod during it does some work. SparkApplication completes immediately (with status=COMPLETED) and task will become success, despite driver wasn't complete the work.

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",andallo,2024-08-02 08:26:17+00:00,[],2024-09-12 00:14:37+00:00,2024-09-12 00:14:37+00:00,https://github.com/apache/airflow/issues/41212,"[('kind:bug', 'This is a clearly a bug'), (""Can't Reproduce"", 'The problem cannot be reproduced'), ('stale', 'Stale PRs per the .github/workflows/stale.yml policy file'), ('area:providers', ''), ('pending-response', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]","[{'comment_id': 2267536951, 'issue_id': 2444388836, 'author': 'bugraoz93', 'body': 'I have started working on a fix. I will publish the PR soon', 'created_at': datetime.datetime(2024, 8, 4, 13, 6, 9, tzinfo=datetime.timezone.utc)}, {'comment_id': 2267665911, 'issue_id': 2444388836, 'author': 'bugraoz93', 'body': 'Have you experienced any similar issues with your KubernetesPodOpertors or any operator from the provider?', 'created_at': datetime.datetime(2024, 8, 4, 20, 55, 40, tzinfo=datetime.timezone.utc)}, {'comment_id': 2268957552, 'issue_id': 2444388836, 'author': 'andallo', 'body': 'I haven\'t been deal with other operators, only with SparkKubernetesOperator.\r\n\r\nLet me give some notes about [PR](https://github.com/apache/airflow/pull/41255). I tested changes of custom_object_launcher.py and result isn\'t what I expect. First launch of the application failed with error: \r\n```\r\n[2024-08-05, 06:33:26 UTC] {taskinstance.py:2905} ERROR - Task failed with exception\r\nTraceback (most recent call last):\r\n  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py"", line 465, in _execute_task\r\n    result = _execute_callable(context=context, **execute_callable_kwargs)\r\n  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py"", line 432, in _execute_callable\r\n    return execute_callable(context=context, **execute_callable_kwargs)\r\n  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py"", line 401, in wrapper\r\n    return func(self, *args, **kwargs)\r\n  File ""/opt/airflow/dags/repo/custom_operators/spark_kubernetes_operator_8_3_1.py"", line 314, in execute\r\n    self.pod = self.get_or_create_spark_crd(self.launcher, context)\r\n  File ""/opt/airflow/dags/repo/custom_operators/spark_kubernetes_operator_8_3_1.py"", line 250, in get_or_create_spark_crd\r\n    driver_pod, spark_obj_spec = launcher.start_spark_job(\r\n  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 330, in wrapped_f\r\n    return self(f, *args, **kw)\r\n  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 467, in __call__\r\n    do = self.iter(retry_state=retry_state)\r\n  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 368, in iter\r\n    result = action(retry_state)\r\n  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 390, in <lambda>\r\n    self._add_action_func(lambda rs: rs.outcome.result())\r\n  File ""/usr/local/lib/python3.8/concurrent/futures/_base.py"", line 437, in result\r\n    return self.__get_result()\r\n  File ""/usr/local/lib/python3.8/concurrent/futures/_base.py"", line 389, in __get_result\r\n    raise self._exception\r\n  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 470, in __call__\r\n    result = fn(*args, **kwargs)\r\n  File ""/opt/airflow/dags/repo/custom_operators/custom_object_launcher.py"", line 313, in start_spark_job\r\n    raise e\r\n  File ""/opt/airflow/dags/repo/custom_operators/custom_object_launcher.py"", line 300, in start_spark_job\r\n    while self.spark_job_not_running(self.spark_obj_spec):\r\n  File ""/opt/airflow/dags/repo/custom_operators/custom_object_launcher.py"", line 369, in spark_job_not_running\r\n    raise AirflowException(\r\nairflow.exceptions.AirflowException: Spark Job Status/Driver State not found. Please check if the Job/Pod is running.\r\n```\r\n\r\nBut this error failed only Airflow task (pod). Driver and spark application were continue to work. Second try started the task, because reattach_on_restart logic connected new task to the driver and CustomLauncher wasn\'t used. Then I tried to remove Driver pod manually and I was expected the task became failed. But again task took success status. \r\n\r\nI think you try to solve the case when Driver are deleted during the launching of application. But I described different case in this issue. I delete Driver after application is launched, when driver is running and application already returned from start_spark_job function of CustomLauncher.', 'created_at': datetime.datetime(2024, 8, 5, 12, 29, 5, tzinfo=datetime.timezone.utc)}, {'comment_id': 2278788971, 'issue_id': 2444388836, 'author': 'bugraoz93', 'body': ""Thanks for sharing more information! \r\nYes, this is the before part indeed. I just saw a case for the deleted pod in the Spark job submit phase and tried to include it in the solution. \r\nFrom here this operator is using KubernetesPodOperator flavours. After it launches the driver pod, the remaining part of the operator is managed by KubernetesPodOperator. That's why I asked if you are using it and experiencing similar results where you are killing the pod but the Airflow is showing your task as a success.\r\nI will check this more during the weekend and hope I can get back to you with a solution"", 'created_at': datetime.datetime(2024, 8, 9, 21, 39, 41, tzinfo=datetime.timezone.utc)}, {'comment_id': 2282913006, 'issue_id': 2444388836, 'author': 'bugraoz93', 'body': ""I haven't had the chance to check this yet, but I plan to do so in the next few days."", 'created_at': datetime.datetime(2024, 8, 11, 22, 43, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2297709572, 'issue_id': 2444388836, 'author': 'bugraoz93', 'body': ""Okay, I checked the same case in the main branch. I couldn't reproduced the same result :( When the task submitted and in running state, I have deleted the driver pod manually. My task went into `up_for_retry`. I have got a proper error message which is stating pod not found `raise ApiException(http_resp=r) kubernetes.client.exceptions.ApiException: (404) Reason: Not Found` \r\nCould you please use later versions? Maybe `2.10.0`, it's recently released and should have updated code."", 'created_at': datetime.datetime(2024, 8, 19, 23, 51, 15, tzinfo=datetime.timezone.utc)}, {'comment_id': 2330357408, 'issue_id': 2444388836, 'author': 'github-actions[bot]', 'body': 'This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.', 'created_at': datetime.datetime(2024, 9, 5, 0, 14, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2345009645, 'issue_id': 2444388836, 'author': 'github-actions[bot]', 'body': 'This issue has been closed because it has not received response from the issue author.', 'created_at': datetime.datetime(2024, 9, 12, 0, 14, 36, tzinfo=datetime.timezone.utc)}]","bugraoz93 on (2024-08-04 13:06:09 UTC): I have started working on a fix. I will publish the PR soon

bugraoz93 on (2024-08-04 20:55:40 UTC): Have you experienced any similar issues with your KubernetesPodOpertors or any operator from the provider?

andallo (Issue Creator) on (2024-08-05 12:29:05 UTC): I haven't been deal with other operators, only with SparkKubernetesOperator.

Let me give some notes about [PR](https://github.com/apache/airflow/pull/41255). I tested changes of custom_object_launcher.py and result isn't what I expect. First launch of the application failed with error: 
```
[2024-08-05, 06:33:26 UTC] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py"", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py"", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File ""/home/airflow/.local/lib/python3.8/site-packages/airflow/models/baseoperator.py"", line 401, in wrapper
    return func(self, *args, **kwargs)
  File ""/opt/airflow/dags/repo/custom_operators/spark_kubernetes_operator_8_3_1.py"", line 314, in execute
    self.pod = self.get_or_create_spark_crd(self.launcher, context)
  File ""/opt/airflow/dags/repo/custom_operators/spark_kubernetes_operator_8_3_1.py"", line 250, in get_or_create_spark_crd
    driver_pod, spark_obj_spec = launcher.start_spark_job(
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 330, in wrapped_f
    return self(f, *args, **kw)
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 467, in __call__
    do = self.iter(retry_state=retry_state)
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 368, in iter
    result = action(retry_state)
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 390, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
  File ""/usr/local/lib/python3.8/concurrent/futures/_base.py"", line 437, in result
    return self.__get_result()
  File ""/usr/local/lib/python3.8/concurrent/futures/_base.py"", line 389, in __get_result
    raise self._exception
  File ""/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py"", line 470, in __call__
    result = fn(*args, **kwargs)
  File ""/opt/airflow/dags/repo/custom_operators/custom_object_launcher.py"", line 313, in start_spark_job
    raise e
  File ""/opt/airflow/dags/repo/custom_operators/custom_object_launcher.py"", line 300, in start_spark_job
    while self.spark_job_not_running(self.spark_obj_spec):
  File ""/opt/airflow/dags/repo/custom_operators/custom_object_launcher.py"", line 369, in spark_job_not_running
    raise AirflowException(
airflow.exceptions.AirflowException: Spark Job Status/Driver State not found. Please check if the Job/Pod is running.
```

But this error failed only Airflow task (pod). Driver and spark application were continue to work. Second try started the task, because reattach_on_restart logic connected new task to the driver and CustomLauncher wasn't used. Then I tried to remove Driver pod manually and I was expected the task became failed. But again task took success status. 

I think you try to solve the case when Driver are deleted during the launching of application. But I described different case in this issue. I delete Driver after application is launched, when driver is running and application already returned from start_spark_job function of CustomLauncher.

bugraoz93 on (2024-08-09 21:39:41 UTC): Thanks for sharing more information! 
Yes, this is the before part indeed. I just saw a case for the deleted pod in the Spark job submit phase and tried to include it in the solution. 
From here this operator is using KubernetesPodOperator flavours. After it launches the driver pod, the remaining part of the operator is managed by KubernetesPodOperator. That's why I asked if you are using it and experiencing similar results where you are killing the pod but the Airflow is showing your task as a success.
I will check this more during the weekend and hope I can get back to you with a solution

bugraoz93 on (2024-08-11 22:43:34 UTC): I haven't had the chance to check this yet, but I plan to do so in the next few days.

bugraoz93 on (2024-08-19 23:51:15 UTC): Okay, I checked the same case in the main branch. I couldn't reproduced the same result :( When the task submitted and in running state, I have deleted the driver pod manually. My task went into `up_for_retry`. I have got a proper error message which is stating pod not found `raise ApiException(http_resp=r) kubernetes.client.exceptions.ApiException: (404) Reason: Not Found` 
Could you please use later versions? Maybe `2.10.0`, it's recently released and should have updated code.

github-actions[bot] on (2024-09-05 00:14:02 UTC): This issue has been automatically marked as stale because it has been open for 14 days with no response from the author. It will be closed in next 7 days if no further activity occurs from the issue author.

github-actions[bot] on (2024-09-12 00:14:36 UTC): This issue has been closed because it has not received response from the issue author.

"
2444196507,issue,open,,SparkKubernetesOperator reattach_on_restart logic doesn't working,"### Apache Airflow Provider(s)

cncf-kubernetes

### Versions of Apache Airflow Providers

apache-airflow-providers-cncf-kubernetes 8.3.1

### Apache Airflow version

2.9.2

### Operating System

Debian GNU/Linux 12 (bookworm)

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### What happened

When reattach_on_restart option is on, SparkKubernetesOperator tries to find already launched driver pod by labels: **dag_id**, **task_id**, **run_id**. But operator doesn't add these labels to the driver, so there is no guarantee it can find that driver in case it exists. Operator can find already launched driver only if mentioned labels were specified for driver in parameters of itself.

### What you think should happen instead

SparkKubernetesOperator should add labels **dag_id**, **task_id**, **run_id** to specification of SparkApplication for driver and executor. Specification come from application_file or template_spec parameters, and then it become template_body parameter. It is easy to add labels to template_body parameter, because operator has a context that keep all values for mentioned labels.

### How to reproduce

Start SparkApplication using SparkKubernetesOperator. Don't specify **dag_id**, **task_id**, **run_id** labels in parameters for driver and executor (for example in application_file parameter). Then task pod submitting SparkApplication will have mentioned labels, but driver and executors pods will not. 

That is a problem for reattach_on_restart logic, because it is searching driver by that labels.

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",andallo,2024-08-02 06:44:38+00:00,[],2024-08-05 22:28:05+00:00,,https://github.com/apache/airflow/issues/41211,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]","[{'comment_id': 2270025890, 'issue_id': 2444196507, 'author': 'bugraoz93', 'body': 'I will check this out', 'created_at': datetime.datetime(2024, 8, 5, 22, 28, 4, tzinfo=datetime.timezone.utc)}]","bugraoz93 on (2024-08-05 22:28:04 UTC): I will check this out

"
2444027760,issue,open,,Manual run with same execution date as scheduled run causes the scheduled run to be skipped.,"### Apache Airflow version

main (development)

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When there is a manual dagrun, it is created with the logical date as data interval end. In case of scheduled runs the logical date is the data interval start. In this case when a future run is scheduled there is a check for existing dagruns in the loop with respect to execution date (logical date) which causes the dagrun to be skipped since it exists from the manual run with a different data interval.

This looks like a known issue in 2.x and can to be changed in 3.x . Related issue https://github.com/apache/airflow/issues/41014

https://github.com/apache/airflow/blob/4d849e87ecce654e100c36dca3d0fbe19bcd8b6b/airflow/jobs/scheduler_job_runner.py#L1314-L1348

### What you think should happen instead?

Scheduled dag run should be executed though a manual run with same execution date exists.

### How to reproduce

1. Place the below dag and parse it.
2. Before unpausing the dag trigger a manual run with logical date as ""2024-07-01T00:00:00+00"".
3. A manual run is triggered with run_id as manual__2024-07-01T00:00:00+00:00 and exeuction_date as 2024-07-01 00:00:00+00:00. Data interval is (2024-06-01, 00:00:00 UTC, 2024-07-01, 00:00:00 UTC)
4. The run with data interval (2024-07-01T00:00:00+00:00, 2024-08-01T00:00:00+00:00) is not scheduled.

```python
from datetime import datetime, timedelta

from airflow import DAG
from airflow.decorators import task


with DAG(
    dag_id=""execution_date_debug"",
    start_date=datetime(2024, 1, 1),
    catchup=True,
    schedule_interval=""0 0 1 * *"",
) as dag:

    @task
    def empty():
        pass

    empty()
```

### Operating System

Ubuntu

### Versions of Apache Airflow Providers

_No response_

### Deployment

Other

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",tirkarthi,2024-08-02 04:41:34+00:00,['Prab-27'],2024-09-06 19:10:05+00:00,,https://github.com/apache/airflow/issues/41208,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('area:core', '')]","[{'comment_id': 2334115958, 'issue_id': 2444027760, 'author': 'Prab-27', 'body': ""hi @shahar1, I'd like to work on this issue .Could you please assign it to me ?"", 'created_at': datetime.datetime(2024, 9, 6, 13, 56, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2334662970, 'issue_id': 2444027760, 'author': 'shahar1', 'body': ""> hi @shahar1, I'd like to work on this issue .Could you please assign it to me ?\r\n\r\nSure, thank you and good luck!"", 'created_at': datetime.datetime(2024, 9, 6, 19, 10, 4, tzinfo=datetime.timezone.utc)}]","Prab-27 (Assginee) on (2024-09-06 13:56:06 UTC): hi @shahar1, I'd like to work on this issue .Could you please assign it to me ?

shahar1 on (2024-09-06 19:10:04 UTC): Sure, thank you and good luck!

"
2443218526,issue,open,,Automatically expand log line groups,"### Description

Is there a way to automatically expand [grouping of log lines](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/logging-tasks.html#grouping-of-log-lines)? When I am debugging a task failure, I often need to look at the `Post task execution logs` to find out why a task failed. Manually expanding that log group every time I'm debugging is tedious.



### Use case/motivation

It would be great to have a feature to either:
- Automatically expand specific log groups
- Disable log grouping for specific log groups

It looks like the post execution group start is printed [on this line of code](https://github.com/apache/airflow/blob/main/airflow/models/taskinstance.py#L340).

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",mjohansenwork,2024-08-01 19:13:40+00:00,['mjohansenwork'],2024-08-08 18:17:57+00:00,,https://github.com/apache/airflow/issues/41198,"[('area:logging', ''), ('kind:feature', 'Feature Requests'), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2263797804, 'issue_id': 2443218526, 'author': 'mjohansenwork', 'body': 'https://stackoverflow.com/questions/78822727/automatically-expand-post-task-execution-logs-log-group-in-airflow', 'created_at': datetime.datetime(2024, 8, 1, 19, 18, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2263947678, 'issue_id': 2443218526, 'author': 'mjohansenwork', 'body': 'https://github.com/apache/airflow/pull/41201', 'created_at': datetime.datetime(2024, 8, 1, 20, 42, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2272104356, 'issue_id': 2443218526, 'author': 'jedcunningham', 'body': ""@mjohansenwork doesn't #40146 solve this problem? That's coming in 2.10."", 'created_at': datetime.datetime(2024, 8, 6, 20, 36, 34, tzinfo=datetime.timezone.utc)}, {'comment_id': 2276403907, 'issue_id': 2443218526, 'author': 'mjohansenwork', 'body': 'I saw https://github.com/apache/airflow/pull/40146, but that only prints the actual _exception_, right? What about additional debug lines that are printed post-execution, for example as part of the `on_failure_callback`?', 'created_at': datetime.datetime(2024, 8, 8, 18, 17, 56, tzinfo=datetime.timezone.utc)}]","mjohansenwork (Issue Creator) on (2024-08-01 19:18:56 UTC): https://stackoverflow.com/questions/78822727/automatically-expand-post-task-execution-logs-log-group-in-airflow

mjohansenwork (Issue Creator) on (2024-08-01 20:42:56 UTC): https://github.com/apache/airflow/pull/41201

jedcunningham on (2024-08-06 20:36:34 UTC): @mjohansenwork doesn't #40146 solve this problem? That's coming in 2.10.

mjohansenwork (Issue Creator) on (2024-08-08 18:17:56 UTC): I saw https://github.com/apache/airflow/pull/40146, but that only prints the actual _exception_, right? What about additional debug lines that are printed post-execution, for example as part of the `on_failure_callback`?

"
2443054444,issue,closed,not_planned,Support for SLAs on Dataset driven DAGs,"### Description

It looks like SLA miss callbacks are only really supported on scheduled DAGs which is problematic moving to Datasets.

### Use case/motivation

It would still be useful to alert on tasks that are taking longer than expected

### Related issues

_No response_

### Are you willing to submit a PR?

- [x] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",samuhepp,2024-08-01 17:56:54+00:00,[],2024-08-02 20:03:07+00:00,2024-08-02 20:03:02+00:00,https://github.com/apache/airflow/issues/41195,"[('kind:feature', 'Feature Requests'), ('duplicate', 'Issue that is duplicated'), ('area:datasets', 'Issues related to the datasets feature'), ('area:SLA', 'AIP-57')]","[{'comment_id': 2263644458, 'issue_id': 2443054444, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 1, 17, 56, 56, tzinfo=datetime.timezone.utc)}, {'comment_id': 2266060876, 'issue_id': 2443054444, 'author': 'shahar1', 'body': 'Duplicate of #38099 - this feature request should be implemented a part of a greater [Airflow purposal](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-57+Refactor+SLA+Feature).', 'created_at': datetime.datetime(2024, 8, 2, 20, 3, 2, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-01 17:56:56 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

shahar1 on (2024-08-02 20:03:02 UTC): Duplicate of #38099 - this feature request should be implemented a part of a greater [Airflow purposal](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-57+Refactor+SLA+Feature).

"
2443024740,issue,closed,completed,Allow uploading a new model version to Google Cloud Vertex AI Model Registry using the `UploadModelOperator`,"### Description

Using Vertex AI, it is possible to upload a model as a new version of an existing model. This is possible by passing the `parent_model` to the `UploadModelRequest`. However, the `ModelServiceHook.upload_model` (under `airflow.providers.google.cloud.hooks.vertex_ai.model_service.py`) does not allow a user to pass the `parent_model`.

Currently, when a new model is uploaded, a new model is created instead of creating a new version on an already existing model.

### Use case/motivation

Adding this feature enables users to use the[ model versioning](https://cloud.google.com/vertex-ai/docs/model-registry/versioning) functionality in Vertex AI.

### Related issues

_No response_

### Are you willing to submit a PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",GLaDAP,2024-08-01 17:45:25+00:00,[],2024-09-09 13:32:05+00:00,2024-09-09 13:32:05+00:00,https://github.com/apache/airflow/issues/41194,"[('provider:google', 'Google (including GCP) related issues'), ('area:providers', ''), ('kind:feature', 'Feature Requests'), ('good first issue', '')]","[{'comment_id': 2266029513, 'issue_id': 2443024740, 'author': 'shahar1', 'body': 'cc: @VladaZakharova , @e-galan', 'created_at': datetime.datetime(2024, 8, 2, 19, 38, 16, tzinfo=datetime.timezone.utc)}, {'comment_id': 2335114636, 'issue_id': 2443024740, 'author': 'jx2lee', 'body': 'Can i take this? 👀 \r\nIf possible, please assign this issue to me. (it will be done until day after tomorrow)', 'created_at': datetime.datetime(2024, 9, 7, 8, 16, 25, tzinfo=datetime.timezone.utc)}, {'comment_id': 2337829130, 'issue_id': 2443024740, 'author': 'VladaZakharova', 'body': '> Can i take this? 👀 If possible, please assign this issue to me. (it will be done until day after tomorrow)\r\n\r\nsure, thank you :)', 'created_at': datetime.datetime(2024, 9, 9, 11, 13, 28, tzinfo=datetime.timezone.utc)}]","shahar1 on (2024-08-02 19:38:16 UTC): cc: @VladaZakharova , @e-galan

jx2lee on (2024-09-07 08:16:25 UTC): Can i take this? 👀 
If possible, please assign this issue to me. (it will be done until day after tomorrow)

VladaZakharova on (2024-09-09 11:13:28 UTC): sure, thank you :)

"
2442583668,issue,closed,completed,SparkKubernetesOperator doesn't respect name from application_file parameter (yaml),"### Apache Airflow Provider(s)

cncf-kubernetes

### Versions of Apache Airflow Providers

8.3.1

### Apache Airflow version

2.9.2

### Operating System

Debian GNU/Linux 12 (bookworm)

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### What happened

SparkKubernetesOperator creates SparkApplication with templated name. The name is always consist from task name and unique 8 symbols string connected by '-'. Operator ignore the name of SparkApplication from yaml, that it consumes by application_file parameter:
```
apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
    name: user-specified-name
    namespace: forge
spec:
...
```

This is inconvenient in case, when there are a lot of DAGs with same name of SparkKubernetesOperator task. As a result all SparkApplications looks similar. It also causes the similar names for drivers, because driver names are consist from SparkApplication name and suffix '-driver'.

### What you think should happen instead

SparkKubernetesOperator should inspect there is a name for SparkApplication in yaml from application_file parameter (path in yaml = metadata.name). If there is a name in the yaml operator should use it for SparkApplication CRD. If no name specified in the yaml, operator should use templated name for it.

### How to reproduce

Start SparkApplication using SparkKubernetesOperator. Pass yaml with specified metadata.name option to application_file parameter. The name of created SparkApplication will not be the same as in yaml.

### Anything else

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",andallo,2024-08-01 14:09:25+00:00,[],2024-09-27 12:55:26+00:00,2024-09-27 12:55:25+00:00,https://github.com/apache/airflow/issues/41188,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]","[{'comment_id': 2263178977, 'issue_id': 2442583668, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 1, 14, 9, 28, tzinfo=datetime.timezone.utc)}, {'comment_id': 2352829795, 'issue_id': 2442583668, 'author': 'gopidesupavan', 'body': 'Am checking on this.', 'created_at': datetime.datetime(2024, 9, 16, 12, 49, 48, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-01 14:09:28 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

gopidesupavan on (2024-09-16 12:49:48 UTC): Am checking on this.

"
2442261853,issue,open,,Scheduler unable to process large number of orphan Datasets,"### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When it starts, the scheduler starts marking Datasets as orphans in `airflow/jobs/scheduler_job_runner.py` in the method `_orphan_unreferenced_datasets`.

A large number of Datasets is queried from the Metadata DB (~200K). It seems the query does not filter Datasets that are already orphans (it queries with HAVING both no DAG nor Task reference; thus selecting also Datasets already flagged as orphans).

As we use Kubernetes, we have a livenessProbe failing for the Scheduler since it helplessly tries to orphan everything in a single run (Note: AIRFLOW__SCHEDULER__PARSING_CLEANUP_INTERVAL is 60s, but changing the value would not change this behaviour).

A lot of `INFO - Orphaning unreferenced dataset` are logged in the Scheduler output.

Kubernetes kills the unhealthy pod. The pod starts anew, and again fails to orphan everything at once. Etc. Etc.

### What you think should happen instead?

Several thing could be done to improve the situation:

1. Limit the number of Datasets selected in the query in `_orphan_unreferenced_datasets`, so that the `_set_orphaned` works by batches of Datasets to orphan (new config `AIRFLOW__SCHEDULER__UNREFERENCED_DATASETS_BATCH_SIZE` for example)
2. Do not select already orphaned Datasets, by improving the query in `_orphan_unreferenced_datasets`
3. Add an optional TTL to Datasets in the Metadata DB so that old Datasets can be ignored by the query and/or cleaned up without the need to orphan them beforehand (if the TTL has passed, then orphan or not is not relevant and the computation can be skipped)

### How to reproduce

Create a very large number of Datasets, that the Scheduler cannot process in one round, faster than its livenessProbe, so that the livenessProbe is not processed and the pod is considered unhealthy.

### Operating System

Kubernetes

### Versions of Apache Airflow Providers

apache-airflow-providers-amazon
apache-airflow-providers-common-sql
apache-airflow-providers-elasticsearch
apache-airflow-providers-hashicorp
apache-airflow-providers-http
apache-airflow-providers-microsoft-winrm
apache-airflow-providers-microsoft-azure
apache-airflow-providers-opsgenie
apache-airflow-providers-postgres
apache-airflow-providers-redis
apache-airflow-providers-sftp
apache-airflow-providers-smtp
apache-airflow-providers-ssh

with versions as of Airflow 2.9.3 constraints.

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

AIRFLOW__SCHEDULER__PARSING_CLEANUP_INTERVAL = 60s (default value)

### Anything else?

FYI we are also working on guidelines for our DAG developers, in order to promote best practices. We do not have one for Datasets at this stage, and I think having a large number of Datasets should be avoided; but I'm not yet sure we can shrink to a point where the requests will work...

And it would be very much appreciated if you know some guidelines. I usually look after the community's and Astronomer's guidelines and best practices, but for Datasets there seem to be few. 

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",NBardelot,2024-08-01 11:55:44+00:00,[],2024-08-19 08:36:55+00:00,,https://github.com/apache/airflow/issues/41185,"[('kind:bug', 'This is a clearly a bug'), ('area:Scheduler', 'including HA (high availability) scheduler'), ('area:core', ''), ('area:datasets', 'Issues related to the datasets feature')]","[{'comment_id': 2263378178, 'issue_id': 2442261853, 'author': 'NBardelot', 'body': ""We've found the root cause of the 200k Datasets.\r\n\r\nWe have a guidline for developers so that they do not put code in the DAG global scope, but here a developer was computing a date in the global scope which was used in the Jinja template of the outlets of a task (so 2 bad practices colliding). Thus, every time the Dag Processor was running, a new Dataset was created. Plus, each new Dataset created this way was rendering the previous Dataset orphan.\r\n\r\nWe're going to check the dataset.orphaned and throw alerts from now on if the number of orphans skyrockets.\r\n\r\nThat being said, the proposals in the issue could still be useful to protect the Scheduler against such development errors in one DAG. Moreover, is it really a good idea that the Dag Processor is the one creating Datasets, while the Scheduler is the one computing the orphans? And shouldn't those processes be more isolated?"", 'created_at': datetime.datetime(2024, 8, 1, 15, 37, 55, tzinfo=datetime.timezone.utc)}, {'comment_id': 2295994492, 'issue_id': 2442261853, 'author': 'NBardelot', 'body': 'See https://github.com/apache/airflow/pull/40806.', 'created_at': datetime.datetime(2024, 8, 19, 8, 36, 53, tzinfo=datetime.timezone.utc)}]","NBardelot (Issue Creator) on (2024-08-01 15:37:55 UTC): We've found the root cause of the 200k Datasets.

We have a guidline for developers so that they do not put code in the DAG global scope, but here a developer was computing a date in the global scope which was used in the Jinja template of the outlets of a task (so 2 bad practices colliding). Thus, every time the Dag Processor was running, a new Dataset was created. Plus, each new Dataset created this way was rendering the previous Dataset orphan.

We're going to check the dataset.orphaned and throw alerts from now on if the number of orphans skyrockets.

That being said, the proposals in the issue could still be useful to protect the Scheduler against such development errors in one DAG. Moreover, is it really a good idea that the Dag Processor is the one creating Datasets, while the Scheduler is the one computing the orphans? And shouldn't those processes be more isolated?

NBardelot (Issue Creator) on (2024-08-19 08:36:53 UTC): See https://github.com/apache/airflow/pull/40806.

"
2441824713,issue,closed,completed,[helm] - Flower ingress incorrect when fullnameOverride applied,"### Official Helm Chart version

1.15.0 (latest released)

### Apache Airflow version

2.8.3

### Kubernetes Version

1.29

### Helm Chart configuration


`values.yaml`
```yaml
airflow:
  fullnameOverride: ""airflow""
```

### Docker Image customizations

_No response_

### What happened

We are sub charting the official airflow chart and for this reason we need to use `fullnameOverride: ""airflow""` settings to achieve the desired k8s resource names. However when we are using it the flower ingress becomes invalid because it want's to connect to non existing service name. Root cause are in the ingress.yaml. The webserver ingress can handle it with no problem.

Incorrect code section

`flower-ingress.yaml` - [code section](https://github.com/apache/airflow/blob/daccc75b064c735636027b20f3edb82da69ffab1/chart/templates/flower/flower-ingress.yaml#L75)
```yaml
spec:
  rules:
    - http:
        paths:
          - backend:
              service:
                name: {{ $.Release.Name }}-flower
                port:
                  name: flower-ui
```

While in the correct code section

`webserver-ingress.yaml` - [code section](https://github.com/apache/airflow/blob/daccc75b064c735636027b20f3edb82da69ffab1/chart/templates/webserver/webserver-ingress.yaml#L83)

```yaml
spec:
  rules:
    - http:
        paths:
          - backend:
              service:
                name: {{ $fullname }}-webserver
                port:
                  name: airflow-ui
```


To overcome this issue we need to run this command on every deployment
```bash
kubectl patch ingress airflow-flower-ingress \
    --namespace ""airflow"" \
    --type='json' \
    -p='[{""op"": ""replace"", ""path"": ""/spec/rules/0/http/paths/0/backend/service/name"", ""value"":""airflow-flower""}]'
```

### What you think should happen instead

The following code should be updated to match how webserver ingress works
`flower-ingress.yaml` - [code section](https://github.com/apache/airflow/blob/daccc75b064c735636027b20f3edb82da69ffab1/chart/templates/flower/flower-ingress.yaml#L75)
```yaml
spec:
  rules:
    - http:
        paths:
          - backend:
              service:
                name: {{ $fullname }}-flower
                port:
                  name: flower-ui
```


### How to reproduce

Sub chart the official chart with this configuration
`values.yaml`
```yaml
airflow:
  fullnameOverride: ""airflow""
```

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",andormarkus,2024-08-01 08:32:27+00:00,['andormarkus'],2024-09-25 10:39:07+00:00,2024-09-25 10:39:07+00:00,https://github.com/apache/airflow/issues/41175,"[('kind:bug', 'This is a clearly a bug'), ('good first issue', ''), ('area:helm-chart', 'Airflow Helm Chart')]","[{'comment_id': 2262417277, 'issue_id': 2441824713, 'author': 'eladkal', 'body': 'feel free to submit a PR', 'created_at': datetime.datetime(2024, 8, 1, 8, 47, 47, tzinfo=datetime.timezone.utc)}, {'comment_id': 2262459023, 'issue_id': 2441824713, 'author': 'andormarkus', 'body': 'Hi @eladkal \r\nHere is the PR #41179', 'created_at': datetime.datetime(2024, 8, 1, 8, 57, 34, tzinfo=datetime.timezone.utc)}]","eladkal on (2024-08-01 08:47:47 UTC): feel free to submit a PR

andormarkus (Issue Creator) on (2024-08-01 08:57:34 UTC): Hi @eladkal 
Here is the PR #41179

"
2441743030,issue,closed,not_planned,Unable to fetch logs from amazon s3,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.2

### What happened?

I have configured the airflow.cfg file and make sure the the Celery worker logs are stored in both the worker instance and the Amazon S3.

The logs in S3 are stored in s3://${BUCKET_NAME}/airflow_logs/dag_id=xxxx/run_id=scheduled__2024-07-30T03:20:00+00:00/task_id=xxxx/，and logs saved in the worker instance with path ${AIRFLOW_HOME}/logs/dag_id=xxxx/run_id=scheduled__2024-07-30T03:20:00+00:00/task_id=xxxx/

When I check the Airflow UI for task retrieval, it cannot retrieve the logs from S3, but it can retrieve the logs from the worker instance. Logs are as below:

```
ip-10-121-129-159.eu-west-1.compute.internal
*** No logs found on s3 for ti=<TaskInstance: logstash_alice_etl.convert_euw1 scheduled__2024-07-30T03:20:00+00:00 [success]>
*** Found logs served from host http://ip-10-121-129-159.eu-west-1.compute.internal:8793/log/dag_id=xxxx/run_id=scheduled__2024-07-30T03:20:00+00:00/task_id=xxxx/attempt=4.log
[2024-08-01, 07:29:55 UTC] {local_task_job_runner.py:120} ▶ Pre task execution logs
[2024-08-01, 07:30:03 UTC] {subprocess.py:63} INFO - Tmp dir root location: /tmp
...
```


### What you think should happen instead?

The logs should be found in S3 instead of the worker instance

### How to reproduce

configure the [logging] parts in airflow.cfg with the below modifications:

base_log_folder = /opt/apache/airflow/logs
remote_logging = True
remote_log_conn_id = aws_airflow_euw1                  # Just an IAM connection to S3 in eu-west-1
remote_base_log_folder = s3://${BUCKET_NAME}/airflow_logs



### Operating System

Amazon Linux 2023

### Versions of Apache Airflow Providers

airflow 2.9.2

### Deployment

Virtualenv installation

### Deployment details

Deploy in Amazon Linux 2023 with default python environment.

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",KanaSukita,2024-08-01 07:50:28+00:00,[],2024-08-07 02:23:06+00:00,2024-08-07 02:23:05+00:00,https://github.com/apache/airflow/issues/41171,"[('kind:bug', 'This is a clearly a bug'), ('provider:amazon', 'AWS/Amazon - related issues'), ('area:providers', ''), ('area:logging', '')]","[{'comment_id': 2262283508, 'issue_id': 2441743030, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 1, 7, 50, 31, tzinfo=datetime.timezone.utc)}, {'comment_id': 2266006802, 'issue_id': 2441743030, 'author': 'burito111', 'body': ""Hi, I can confirm similar issue with the same version 2.9.2 and MWAA on AWS. I get following information when going into task logs.\r\n\r\n*** Unable to read remote logs from Cloudwatch (log_group: airflow-airflow-Task, log_stream: dag_id=pineapple/run_id=manual__2024-08-02T19_16_42.903450+00_00/task_id=training_model_C/attempt=1.log)\r\n*** An error occurred (ResourceNotFoundException) when calling the GetLogEvents operation: The specified log stream does not exist.\r\n\r\n*** Could not read served logs: Request URL is missing an 'http://'/ or 'https://'/ protocol."", 'created_at': datetime.datetime(2024, 8, 2, 19, 21, 6, tzinfo=datetime.timezone.utc)}, {'comment_id': 2269792682, 'issue_id': 2441743030, 'author': 'eladkal', 'body': 'cc @vincbeck @o-nikolas you might want to look into this one', 'created_at': datetime.datetime(2024, 8, 5, 19, 47, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2271532214, 'issue_id': 2441743030, 'author': 'vincbeck', 'body': 'I tested it using the same configuration with breeze (against main and against Airflow 2.9.2) and it works properly. @KanaSukita, @burito111, did it stop working when you upgraded Airflow? Amazon provider package? What triggered the logs to stop working?', 'created_at': datetime.datetime(2024, 8, 6, 15, 10, 32, tzinfo=datetime.timezone.utc)}, {'comment_id': 2271912246, 'issue_id': 2441743030, 'author': 'shubham22', 'body': '@burito111 MWAA does not support S3 for logs, so this is expected behavior.', 'created_at': datetime.datetime(2024, 8, 6, 18, 37, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2272493916, 'issue_id': 2441743030, 'author': 'eladkal', 'body': 'Closing as not Airflow issue.\r\n\r\nIf further assistance required on this please contact MWAA support.', 'created_at': datetime.datetime(2024, 8, 7, 2, 23, 5, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-01 07:50:31 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

burito111 on (2024-08-02 19:21:06 UTC): Hi, I can confirm similar issue with the same version 2.9.2 and MWAA on AWS. I get following information when going into task logs.

*** Unable to read remote logs from Cloudwatch (log_group: airflow-airflow-Task, log_stream: dag_id=pineapple/run_id=manual__2024-08-02T19_16_42.903450+00_00/task_id=training_model_C/attempt=1.log)
*** An error occurred (ResourceNotFoundException) when calling the GetLogEvents operation: The specified log stream does not exist.

*** Could not read served logs: Request URL is missing an 'http://'/ or 'https://'/ protocol.

eladkal on (2024-08-05 19:47:30 UTC): cc @vincbeck @o-nikolas you might want to look into this one

vincbeck on (2024-08-06 15:10:32 UTC): I tested it using the same configuration with breeze (against main and against Airflow 2.9.2) and it works properly. @KanaSukita, @burito111, did it stop working when you upgraded Airflow? Amazon provider package? What triggered the logs to stop working?

shubham22 on (2024-08-06 18:37:33 UTC): @burito111 MWAA does not support S3 for logs, so this is expected behavior.

eladkal on (2024-08-07 02:23:05 UTC): Closing as not Airflow issue.

If further assistance required on this please contact MWAA support.

"
2441707144,issue,open,,"DAG Overview Last Run, Next Run are confusing/wrong, also the next Run ID and Run date on the DAG run details are confusing/wrong","### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When viewing the DAG overview the columns Last Run and Next run are showing the previous run dates (Same as date interval start and Date interval end)

At the details of a DAG it wil show the run date as for example 31-07 and the actual date was 01-08

Also the Next Run ID is confusing as it is showing a past date

### What you think should happen instead?

When viewing the DAG overview the columns Last Run and Next run are showing the previous run dates (Same as date interval start and Date interval end), I would expect to see the actual last run date and actual next scheduled run date.

![image](https://github.com/user-attachments/assets/16753f0f-5881-4ee1-8009-7d85858c30e5)

Furthermore when you look at the details of a DAG it wil show the run date as for example 31-07 and the actual date was 01-08.

![image](https://github.com/user-attachments/assets/e62d53cd-fca4-42e3-bff0-017538800538)

Also the Next Run ID is confusing:

![image](https://github.com/user-attachments/assets/3a8826a7-7cb9-4580-8539-46bffa40ca85)

This should display the actual next scheduled run date.

### How to reproduce

Schedule a DAG and see that the these columns are representing unlogical dates.

### Operating System

Airflow Docker image

### Versions of Apache Airflow Providers

_No response_

### Deployment

Official Apache Airflow Helm Chart

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",steege2,2024-08-01 07:32:10+00:00,[],2024-08-02 19:35:43+00:00,,https://github.com/apache/airflow/issues/41170,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.')]","[{'comment_id': 2262251444, 'issue_id': 2441707144, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 8, 1, 7, 32, 13, tzinfo=datetime.timezone.utc)}, {'comment_id': 2262827902, 'issue_id': 2441707144, 'author': 'tirkarthi', 'body': 'Related : https://github.com/apache/airflow/pull/38365', 'created_at': datetime.datetime(2024, 8, 1, 11, 40, 22, tzinfo=datetime.timezone.utc)}, {'comment_id': 2262831396, 'issue_id': 2441707144, 'author': 'tirkarthi', 'body': 'Next run ID related previous discussion : https://github.com/apache/airflow/issues/31365', 'created_at': datetime.datetime(2024, 8, 1, 11, 42, 29, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-08-01 07:32:13 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

tirkarthi on (2024-08-01 11:40:22 UTC): Related : https://github.com/apache/airflow/pull/38365

tirkarthi on (2024-08-01 11:42:29 UTC): Next run ID related previous discussion : https://github.com/apache/airflow/issues/31365

"
2441147274,issue,closed,completed,Webserver does not fetch server logs when there are remote logs,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.9.3

### What happened?

The webserver is not reaching out to the triggerer log server for the corresponding trigger logs of a deferred task instance. The webserver only reads from the worker / triggerer log server when there are no local logs or remote logs. This behaviour was introduced in #39177.

When the task instance is in a non-terminal state and have remote logs, the [behaviour](https://github.com/apache/airflow/blob/2.9.3/airflow/utils/log/file_task_handler.py#L347-L371) no longer aligns with expectation of live logs as described in the documentation for [Serving logs from workers and triggerer](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/logging-tasks.html#serving-logs-from-workers-and-triggerer). 

A specific use case is deferrable operators which has essentially two executions.
1. First execution is to submit the trigger and put the task into a deferred state 
2. Second execution is to process the trigger event
 
After the first execution, task log is pushed to the remote location. From then on, the task log view see the log in the remote location and fetches it as expected but it also means the webserver will not reach out to the triggerer for logs.

### What you think should happen instead?

If the task instance is deferred and have remote logs, the webserver should still reach out to the triggerer log server.

The behaviour introduced by #39177 so that task instances in a terminal state can continue to fetch logs from the log server if there are no remote logs or local logs. The user stated that their logs are stored in a persistent storage on their worker which is why the user wants to allow server log fetching when there are no remote logging.

I think the log reading code needs to specify a logical path where there are no remote log or local log for deployments without remote logging and logs are not stored on the webserver.

### How to reproduce

Setup a deployment with remote logging and run a deferrable task. 

### Operating System

n/a

### Versions of Apache Airflow Providers

n/a

### Deployment

Astronomer

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",wolfier,2024-07-31 23:56:39+00:00,[],2024-09-05 17:28:23+00:00,2024-09-05 17:28:23+00:00,https://github.com/apache/airflow/issues/41164,"[('kind:bug', 'This is a clearly a bug'), ('area:webserver', 'Webserver related Issues'), ('area:logging', ''), ('area:core', '')]","[{'comment_id': 2261679371, 'issue_id': 2441147274, 'author': 'wolfier', 'body': '@kahlstrm / @RNHTTR \r\n\r\nCan you speak more about the change introduced in #39177 in case my interpretation is insufficient / incorrect.', 'created_at': datetime.datetime(2024, 7, 31, 23, 58, 30, tzinfo=datetime.timezone.utc)}, {'comment_id': 2261742577, 'issue_id': 2441147274, 'author': 'kahlstrm', 'body': '> @kahlstrm / @RNHTTR\r\n> \r\n> Can you speak more about the change introduced in #39177 in case my interpretation is insufficient / incorrect.\r\n\r\n> The webserver only reads from the worker / triggerer log server when there are no local logs or remote logs. This behaviour was introduced in https://github.com/apache/airflow/pull/39177.\r\n\r\nTo clarify on this point, #39177 introduced this particular behaviour as an alternative implementation of #32561, which entirely removed fetching logs from the worker / triggerer log server for past task runs. The way #39177 was implemented is to retain the wanted behaviour of #32561 of not triggering the HTTP request in cases where remote logs were found, but still to support our use case of storing the logs on the worker with a persistent volume.\r\n\r\nThe deferred state logic was kept to be as close as possible to the previous implementations, however it became evident in https://github.com/apache/airflow/pull/39496#issuecomment-2149692239, adding tests for the deferred state caused test flakiness with unexpected results. It might be that this has caused a regression in the deferred state, as that one was untested in our use case, whereas viewing previous task attempts was confirmed to be working as expected again after #39177.\r\n\r\nIf this is deemed problematic/ a regression, going back to behavior prior to #32561 would be fine at least for our use case, as that initially introduced this behavior of not serving the worker / triggerer logs in certain circumstances.', 'created_at': datetime.datetime(2024, 8, 1, 1, 3, 1, tzinfo=datetime.timezone.utc)}, {'comment_id': 2277818453, 'issue_id': 2441147274, 'author': 'kahlstrm', 'body': 'This is probably resolved by #41272 ?', 'created_at': datetime.datetime(2024, 8, 9, 12, 18, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2332275331, 'issue_id': 2441147274, 'author': 'wolfier', 'body': 'Closed by #41272 as the original behaviour is restored.', 'created_at': datetime.datetime(2024, 9, 5, 17, 28, 23, tzinfo=datetime.timezone.utc)}]","wolfier (Issue Creator) on (2024-07-31 23:58:30 UTC): @kahlstrm / @RNHTTR 

Can you speak more about the change introduced in #39177 in case my interpretation is insufficient / incorrect.

kahlstrm on (2024-08-01 01:03:01 UTC): To clarify on this point, #39177 introduced this particular behaviour as an alternative implementation of #32561, which entirely removed fetching logs from the worker / triggerer log server for past task runs. The way #39177 was implemented is to retain the wanted behaviour of #32561 of not triggering the HTTP request in cases where remote logs were found, but still to support our use case of storing the logs on the worker with a persistent volume.

The deferred state logic was kept to be as close as possible to the previous implementations, however it became evident in https://github.com/apache/airflow/pull/39496#issuecomment-2149692239, adding tests for the deferred state caused test flakiness with unexpected results. It might be that this has caused a regression in the deferred state, as that one was untested in our use case, whereas viewing previous task attempts was confirmed to be working as expected again after #39177.

If this is deemed problematic/ a regression, going back to behavior prior to #32561 would be fine at least for our use case, as that initially introduced this behavior of not serving the worker / triggerer logs in certain circumstances.

kahlstrm on (2024-08-09 12:18:26 UTC): This is probably resolved by #41272 ?

wolfier (Issue Creator) on (2024-09-05 17:28:23 UTC): Closed by #41272 as the original behaviour is restored.

"
2441045430,issue,open,,The Celery Worker Pods terminated and lost task logs when scale down happened during HPA autoscaling ,"### Official Helm Chart version

1.15.0 (latest released)

### Apache Airflow version

2.7.1

### Kubernetes Version

1.27.2

### Helm Chart configuration

After official helm charts supports HPA, then here is my HPA settings.
```
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ""airflow-worker""
  namespace: airflow
  labels:
    app: airflow
    component: worker
    release: airflow
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: StatefulSet
    name: ""airflow-worker""
  minReplicas: 2
  maxReplicas:16
  metrics:
    - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 70
    - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80

```

### Docker Image customizations

Not too much special packages, just added openjdk, mongo client and some other python related dependency.

### What happened

When I enabled HPA in the Airflow cluster, then during the scale down some long running tasks which might be over 15hrs got failed and lost the logs. More details.
- The long running task for example is calling python api to spin up a google dataproc cluster which will run the job over 15hrs. When the HPA started scale down the pods, then these tasks failed looks due to the pod terminated after the `terminationGracePeriodSeconds` was hitting. And then it caused the task failed, but actually the tasks still waiting the final response status from dataproc jobs.
- Also after the pod terminated, then the tasks' logs actually was not published to the GCS bucket. I set the remote log to GCS bucket already. But the logs actually are still in persistent disk side, I have to spin up the same pod then can upload the logs to GCS bucket. again.
- The `terminationGracePeriodSeconds` I set is 1hour right now, but due to the long running tasks so i am not sure how the HPA settings can support it.

### What you think should happen instead

It should be happened during HPA scaling down either the celery work pod need to graceful shutdown until all tasks are completed or Airflow should provide some config or something else to support kill the task process before final termination deadline and complete the cleanup and upload logs to remote settings like GCS bucket.

### How to reproduce

- Using official helm chart with HPA enabled with CPU/Memory metrics deploy airflow to Kubernetes.
- Setup remote logging
```
logging:
    delete_local_logs: 'False'
    remote_base_log_folder: ""gs://xxx""
    remote_logging: 'True'
```
- Run a long running task maybe just need 30mins or few hours, but you have to make sure setup `terminationGracePeriodSeconds` maybe to a small number.
- Try to trigger the HPA scale down the pods, then you should see the issue from the UI you should see the task failed without logs.

### Anything else

_No response_

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",andycai-sift,2024-07-31 22:24:45+00:00,['andycai-sift'],2024-08-10 18:28:08+00:00,,https://github.com/apache/airflow/issues/41163,"[('kind:bug', 'This is a clearly a bug'), ('area:providers', ''), ('area:logging', ''), ('area:helm-chart', 'Airflow Helm Chart'), ('provider:cncf-kubernetes', 'Kubernetes provider related issues')]","[{'comment_id': 2261560505, 'issue_id': 2441045430, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 7, 31, 22, 24, 48, tzinfo=datetime.timezone.utc)}, {'comment_id': 2282236848, 'issue_id': 2441045430, 'author': 'dimon222', 'body': 'Regarding first proposed solution - HPA waiting for completion of work:\r\nI\'m afraid this is not fixable by Airflow project, as it\'s the limitation of Kubernetes that during scale down operation request you can\'t choose the ""wait for completion of work"" dynamic amount of time. The timing for graceful shutdown has to be configured at container start time, you can\'t update this when the container of celery worker is already running (the resource spec is immutable, you have to trigger ""deploy"" style operation to bounce container to do any changes).\r\n\r\nI have been waiting for this for years, but K8s development community isn\'t working on it currently.\r\nhttps://github.com/kubernetes/enhancements/issues/2255#issuecomment-2261281830\r\n\r\nNot sure about second proposed solution though. I\'m not certain is Kubernetes sends upfront signal to remind that shutdown signal will come in grace period amount of seconds.', 'created_at': datetime.datetime(2024, 8, 10, 18, 27, 18, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-07-31 22:24:48 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

dimon222 on (2024-08-10 18:27:18 UTC): Regarding first proposed solution - HPA waiting for completion of work:
I'm afraid this is not fixable by Airflow project, as it's the limitation of Kubernetes that during scale down operation request you can't choose the ""wait for completion of work"" dynamic amount of time. The timing for graceful shutdown has to be configured at container start time, you can't update this when the container of celery worker is already running (the resource spec is immutable, you have to trigger ""deploy"" style operation to bounce container to do any changes).

I have been waiting for this for years, but K8s development community isn't working on it currently.
https://github.com/kubernetes/enhancements/issues/2255#issuecomment-2261281830

Not sure about second proposed solution though. I'm not certain is Kubernetes sends upfront signal to remind that shutdown signal will come in grace period amount of seconds.

"
2440673135,issue,open,,Remove the ability to set parallelism to infinity in Airflow V3,"### Description

See this [issue](https://github.com/apache/airflow/issues/41055) and this subsequent [PR](https://github.com/apache/airflow/pull/41107) for context. But in short: There was a undocumented/untested/buggy feature to allow setting parallelism to infinity (by setting it to zero) which was dropped during the implementation of Multiple Executor Config. This raised a discussion of if this feature should really be supported. I propose we drop this feature in Airflow 3 because:

1. It adds unnecessary complexity to the code
2. This behaviour can be achieved by setting parallelism to a sufficiently high number
3. (most importantly) I think it's actually important for the user to have to do 2), it forces them to actually think ""hmm how parallel should I _actually_ run tasks? Is infinity appropriated? Will infinity actually cause degraded performance?"". I think allowing 0 gives an easy way for folks to set and forget without weighing the implications.
4. `scheduler.max_tis_per_query` is a very important config for performance and depends on `core.parallelism` if it is set to 0 (which means to track the value of parallelism) then we may have infinite query sizes which would drastically impact performance. This is an easy trap for users to fall into.

### Related issues

https://github.com/apache/airflow/issues/41055
https://github.com/apache/airflow/pull/41107

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",o-nikolas,2024-07-31 18:28:43+00:00,[],2024-08-15 01:36:15+00:00,,https://github.com/apache/airflow/issues/41162,"[('kind:feature', 'Feature Requests'), ('airflow3.0:candidate', 'Potential candidates for Airflow 3.0'), ('airflow3.0:breaking', 'Candidates for Airflow 3.0 that contain breaking changes')]",[],
2440403648,issue,open,,`id` of sentinel values not preserved by `partial_subset` ,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.7.3

### What happened?

when running a dag created with `partial_subset`, the `id` of sentinel values are not properly maintained for decorated python mapped tasks, specifically `EXPAND_INPUT_EMPTY` that is set as the default value for `expand_input` for any mapped `@task`. this causes all calls to the mapped, decorated tasks to fail with following error: `AssertionError: unexpected expand_input: DictOfListsExpandInput(value={})` raises [here](https://github.com/apache/airflow/blob/main/airflow/decorators/base.py#L558) as the comparison is done using `is not`; i have added a minimally reproducible example below. 

in debugging, adding `EXPAND_INPUT_EMPTY` to deepcopy's `memo` arg fixed the issue within [partial_subset](https://github.com/apache/airflow/blob/main/airflow/models/dag.py#L2581) like
```
memo = {id(self.task_dict): None, id(self._task_group): None, id(EXPAND_INPUT_EMPTY): EXPAND_INPUT_EMPTY}
```
as I imagine the comparison to a sentinel value by `id` instead of by value will want to continue. If that is the desired path forward, I can open a PR to that effect.



### What you think should happen instead?

the mapped task should run without failure.

### How to reproduce

```python
import datetime
import os

from airflow import settings
from airflow.utils.state import DagRunState
from airflow.utils.db import initdb, resetdb
from airflow.decorators import dag
from airflow.decorators import task


@task
def generate_input() -> list[int]:
    return [1, 2, 3, 4, 5, 6, 7]


@task
def mapped(multiplier: int, value: int) -> int:
    return multiplier * value


@task
def validate_output(multiplier: int, results: list[int]) -> None:
    for result in results:
        if result % multiplier != 0:
            raise ValueError(""Oops!"")


@dag(
    dag_id=""reproducible_dag"",
    default_args={
        ""start_date"": datetime.date.today().strftime(""%Y-%m-%d""),
        ""retries"": 0,
    },
    schedule=None,
    catchup=False,
    render_template_as_native_obj=True,
)
def build_reproducible_dag() -> None:
    mapped_input = generate_input()
    mapped_output = mapped.partial(multiplier=3).expand(value=mapped_input)
    validate_output(multiplier=3, results=mapped_output)


reproducible_dag = build_reproducible_dag()


if __name__ == ""__main__"":
    from recidiviz.tools.postgres import local_postgres_helpers

    # set up
    temp_dir = local_postgres_helpers.start_on_disk_postgresql_database()
    os.environ[""AIRFLOW__DATABASE__SQL_ALCHEMY_CONN""] = (
        local_postgres_helpers.on_disk_postgres_db_url().render_as_string()
    )
    settings.initialize()
    initdb(load_connections=False)

    dag_run = reproducible_dag.test()  # this run will succeeed

    assert dag_run.get_state() == DagRunState.SUCCESS

    # reset for test
    resetdb()

    reproducible_subset = reproducible_dag.partial_subset(
        task_ids_or_regex=[
            ""generate_input"",
            ""mapped"",
            ""validate_output"",
        ]
    )
    dag_run = reproducible_subset.test()  # this run will fail

    assert dag_run.get_state() == DagRunState.SUCCESS

    # tear down
    resetdb(skip_init=True)
    local_postgres_helpers.stop_and_clear_on_disk_postgresql_database(temp_dir)

```

for `local_postgres_helpers` code see, [pulse-data](https://github.com/Recidiviz/pulse-data/blob/main/recidiviz/tools/postgres/local_postgres_helpers.py)



task failure trace
```
[2024-07-31 11:42:49,263] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1518, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1647, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2285, in render_templates
    original_task.render_template_fields(context)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 725, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/decorators/base.py"", line 527, in _expand_mapped_kwargs
    raise AssertionError(f""unexpected expand_input: {self.expand_input}"")
AssertionError: unexpected expand_input: DictOfListsExpandInput(value={})
ERROR [airflow.task] Task failed with exception
Traceback (most recent call last):
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1518, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1647, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2285, in render_templates
    original_task.render_template_fields(context)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 725, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/decorators/base.py"", line 527, in _expand_mapped_kwargs
    raise AssertionError(f""unexpected expand_input: {self.expand_input}"")
AssertionError: unexpected expand_input: DictOfListsExpandInput(value={})
```

### Operating System

macOS::14.1

### Versions of Apache Airflow Providers

```
apache-airflow==2.7.3
apache-airflow-providers-apache-beam==5.6.3
apache-airflow-providers-celery==3.6.0
apache-airflow-providers-cncf-kubernetes==8.1.0
apache-airflow-providers-common-sql==1.12.0
apache-airflow-providers-dbt-cloud==3.7.1
apache-airflow-providers-ftp==3.8.0
apache-airflow-providers-google==10.17.0
apache-airflow-providers-hashicorp==3.6.4
apache-airflow-providers-http==4.10.1
apache-airflow-providers-imap==3.5.0
apache-airflow-providers-mysql==5.5.4
apache-airflow-providers-postgres==5.10.2
apache-airflow-providers-sendgrid==3.4.0
apache-airflow-providers-sftp==4.9.1
apache-airflow-providers-sqlite==3.7.1
apache-airflow-providers-ssh==3.10.1
google-cloud-orchestration-airflow==1.12.1
```

### Deployment

Google Cloud Composer

### Deployment details

pipfile for env, drawn from https://cloud.google.com/composer/docs/concepts/versioning/composer-versions for `composer-2.7.1-airflow-2.7.3`
```
[[source]]
url = ""https://pypi.org/simple""
verify_ssl = true
name = ""pypi""

[dev-packages]
pytest = ""*""
pylint = ""*""
more-itertools = ""*""
mypy = ""*""
black = ""*""
freezegun = ""*""

[packages]
# Packages specified in cloud-composer.tf
us = ""==3.1.1""
apache-airflow-providers-sftp = ""==4.9.1""
python-levenshtein = ""==0.25.1""
dateparser = ""==1.2.0""

# We pin all dependencies to make sure they remain on versions supported by our current 
# Cloud Composer environment. We must manually update the dependencies when we update 
# our Cloud Composer version. https://cloud.google.com/composer/docs/concepts/versioning/composer-versions
absl-py = ""==2.1.0""
agate = ""==1.6.3""
aiodebug = ""==2.3.0""
aiofiles = ""==23.2.1""
aiohttp = ""==3.9.4""
aiosignal = ""==1.3.1""
alembic = ""==1.13.1""
amqp = ""==5.2.0""
annotated-types = ""==0.6.0""
anyio = ""==4.3.0""
apache-airflow = ""==2.7.3""
apache-airflow-providers-apache-beam = ""==5.6.3""
apache-airflow-providers-celery = ""==3.6.0""
apache-airflow-providers-cncf-kubernetes = ""==8.1.0""
apache-airflow-providers-common-sql = ""==1.12.0""
apache-airflow-providers-dbt-cloud = ""==3.7.1""
apache-airflow-providers-ftp = ""==3.8.0""
apache-airflow-providers-google = ""==10.17.0""
apache-airflow-providers-hashicorp = ""==3.6.4""
apache-airflow-providers-http = ""==4.10.1""
apache-airflow-providers-imap = ""==3.5.0""
apache-airflow-providers-mysql = ""==5.5.4""
apache-airflow-providers-postgres = ""==5.10.2""
apache-airflow-providers-sendgrid = ""==3.4.0""
apache-airflow-providers-sqlite = ""==3.7.1""
apache-airflow-providers-ssh = ""==3.10.1""
apache-beam = ""==2.55.1""
apispec = ""==6.6.0""
appdirs = ""==1.4.4""
argcomplete = ""==3.3.0""
asgiref = ""==3.8.1""
astunparse = ""==1.6.3""
attrs = ""==23.2.0""
Babel = ""==2.14.0""
backoff = ""==2.2.1""
backports-tarfile = ""==1.0.0""
bcrypt = ""==4.1.2""
billiard = ""==4.2.0""
blinker = ""==1.7.0""
CacheControl = ""==0.14.0""
cachelib = ""==0.9.0""
cachetools = ""==5.3.3""
cattrs = ""==23.2.3""
celery = ""==5.3.6""
certifi = ""==2024.2.2""
cffi = ""==1.16.0""
chardet = ""==5.2.0""
charset-normalizer = ""==3.3.2""
click = ""==8.1.3""
click-didyoumean = ""==0.3.1""
click-plugins = ""==1.1.1""
click-repl = ""==0.3.0""
clickclick = ""==20.10.2""
cloudpickle = ""==2.2.1""
colorama = ""==0.4.6""
colorlog = ""==4.8.0""
ConfigUpdater = ""==3.2""
connexion = ""==2.14.2""
crcmod = ""==1.7""
cron-descriptor = ""==1.4.3""
croniter = ""==2.0.3""
cryptography = ""==41.0.7""
db-dtypes = ""==1.2.0""
dbt-bigquery = ""==1.5.4""
dbt-core = ""==1.5.4""
dbt-extractor = ""==0.4.1""
decorator = ""==5.1.1""
Deprecated = ""==1.2.14""
diff_cover = ""==9.0.0""
dill = ""==0.3.1.1""
distlib = ""==0.3.8""
dnspython = ""==2.6.1""
docopt = ""==0.6.2""
docstring_parser = ""==0.16""
docutils = ""==0.21.1""
email-validator = ""==1.3.1""
fastavro = ""==1.9.4""
fasteners = ""==0.19""
filelock = ""==3.13.3""
firebase-admin = ""==6.5.0""
Flask = ""==2.2.5""
Flask-AppBuilder = ""==4.3.6""
Flask-Babel = ""==2.0.0""
Flask-Bcrypt = ""==1.0.1""
Flask-Caching = ""==2.1.0""
Flask-JWT-Extended = ""==4.6.0""
Flask-Limiter = ""==3.5.1""
Flask-Login = ""==0.6.3""
Flask-Session = ""==0.5.0""
Flask-SQLAlchemy = ""==2.5.1""
Flask-WTF = ""==1.2.1""
flatbuffers = ""==24.3.25""
flower = ""==2.0.1""
frozenlist = ""==1.4.1""
fsspec = ""==2024.3.1""
future = ""==1.0.0""
gast = ""==0.5.4""
gcloud-aio-auth = ""==4.2.3""
gcloud-aio-bigquery = ""==7.1.0""
gcloud-aio-storage = ""==9.2.0""
gcsfs = ""==2024.3.1""
google-ads = ""==23.1.0""
google-analytics-admin = ""==0.22.7""
google-api-core = ""==2.18.0""
google-api-python-client = ""==2.125.0""
google-apitools = ""==0.5.32""
google-auth = ""==2.29.0""
google-auth-httplib2 = ""==0.2.0""
google-auth-oauthlib = ""==1.2.0""
google-cloud-access-context-manager = ""==0.2.0""
google-cloud-aiplatform = ""==1.47.0""
google-cloud-appengine-logging = ""==1.4.3""
google-cloud-asset = ""==3.26.0""
google-cloud-audit-log = ""==0.2.5""
google-cloud-automl = ""==2.13.3""
google-cloud-batch = ""==0.17.17""
google-cloud-bigquery = ""==3.20.1""
google-cloud-bigquery-datatransfer = ""==3.15.1""
google-cloud-bigtable = ""==2.23.0""
google-cloud-build = ""==3.24.0""
google-cloud-common = ""==1.3.3""
google-cloud-compute = ""==1.18.0""
google-cloud-container = ""==2.45.0""
google-cloud-core = ""==2.4.1""
google-cloud-datacatalog = ""==3.19.0""
google-cloud-datacatalog-lineage = ""==0.3.1""
# google-cloud-datacatalog-lineage-producer-client = ""==0.1.0""
google-cloud-dataflow-client = ""==0.8.10""
google-cloud-dataform = ""==0.5.9""
google-cloud-dataplex = ""==1.13.0""
google-cloud-dataproc = ""==5.9.3""
google-cloud-dataproc-metastore = ""==1.15.3""
google-cloud-datastore = ""==2.19.0""
google-cloud-dlp = ""==3.16.0""
google-cloud-documentai = ""==2.25.0""
google-cloud-filestore = ""==1.9.3""
google-cloud-firestore = ""==2.16.0""
google-cloud-kms = ""==2.21.3""
google-cloud-language = ""==2.13.3""
google-cloud-logging = ""==3.10.0""
google-cloud-memcache = ""==1.9.3""
google-cloud-monitoring = ""==2.19.3""
google-cloud-orchestration-airflow = ""==1.12.1""
google-cloud-org-policy = ""==1.11.0""
google-cloud-os-config = ""==1.17.3""
google-cloud-os-login = ""==2.14.3""
google-cloud-pubsub = ""==2.21.1""
google-cloud-pubsublite = ""==0.6.1""
google-cloud-redis = ""==2.15.3""
google-cloud-resource-manager = ""==1.12.3""
google-cloud-run = ""==0.10.5""
google-cloud-secret-manager = ""==2.19.0""
google-cloud-spanner = ""==3.44.0""
google-cloud-speech = ""==2.26.0""
google-cloud-storage = ""==2.16.0""
google-cloud-storage-transfer = ""==1.11.3""
google-cloud-tasks = ""==2.16.3""
google-cloud-texttospeech = ""==2.16.3""
google-cloud-translate = ""==3.15.3""
google-cloud-videointelligence = ""==2.13.3""
google-cloud-vision = ""==3.7.2""
google-cloud-workflows = ""==1.14.3""
google-crc32c = ""==1.5.0""
google-pasta = ""==0.2.0""
google-re2 = ""==1.1""
google-resumable-media = ""==2.7.0""
googleapis-common-protos = ""==1.63.0""
graphviz = ""==0.20.3""
greenlet = ""==3.0.3""
grpc-google-iam-v1 = ""==0.13.0""
grpc-interceptor = ""==0.15.4""
grpcio = ""==1.62.1""
grpcio-gcp = ""==0.2.2""
grpcio-status = ""==1.62.1""
gunicorn = ""==21.2.0""
h11 = ""==0.14.0""
h5py = ""==3.11.0""
hdfs = ""==2.7.3""
hologram = ""==0.0.16""
httpcore = ""==1.0.5""
httplib2 = ""==0.22.0""
httpx = ""==0.27.0""
humanize = ""==4.9.0""
hvac = ""==2.1.0""
idna = ""==3.7""
importlib-metadata = ""==7.0.0""
importlib_resources = ""==6.4.0""
inflection = ""==0.5.1""
iniconfig = ""==2.0.0""
isodate = ""==0.6.1""
itsdangerous = ""==2.1.2""
jaraco-classes = ""==3.4.0""
jaraco-context = ""==5.3.0""
jaraco-functools = ""==4.0.0""
jeepney = ""==0.8.0""
Jinja2 = ""==3.1.2""
Js2Py = ""==0.74""
json-merge-patch = ""==0.2""
jsonpickle = ""==3.0.4""
jsonschema = ""==4.21.1""
jsonschema-specifications = ""==2023.12.1""
keras = ""==3.2.1""
keyring = ""==25.1.0""
keyrings-google-artifactregistry-auth = ""==1.1.2""
kombu = ""==5.3.7""
kubernetes = ""==29.0.0""
kubernetes_asyncio = ""==29.0.0""
lazy-object-proxy = ""==1.10.0""
leather = ""==0.4.0""
libclang = ""==18.1.1""
limits = ""==3.10.1""
linkify-it-py = ""==2.0.3""
lockfile = ""==0.12.2""
Logbook = ""==1.5.3""
looker-sdk = ""==24.4.0""
Mako = ""==1.3.3""
Markdown = ""==3.6""
markdown-it-py = ""==3.0.0""
MarkupSafe = ""==2.1.5""
marshmallow = ""==3.21.1""
marshmallow-oneofschema = ""==3.1.1""
marshmallow-sqlalchemy = ""==0.26.1""
mashumaro = ""==3.6""
mdit-py-plugins = ""==0.4.0""
mdurl = ""==0.1.2""
minimal-snowplow-tracker = ""==0.0.2""
ml-dtypes = ""==0.3.2""
more-itertools = ""==10.2.0""
msgpack = ""==1.0.8""
multidict = ""==6.0.5""
mysql-connector-python = ""==8.3.0""
mysqlclient = ""==2.2.4""
namex = ""==0.0.8""
networkx = ""==2.8.8""
numpy = ""==1.26.4""
oauth2client = ""==4.1.3""
oauthlib = ""==3.2.2""
objsize = ""==0.7.0""
opentelemetry-api = ""==1.24.0""
opentelemetry-exporter-otlp = ""==1.24.0""
opentelemetry-exporter-otlp-proto-common = ""==1.24.0""
opentelemetry-exporter-otlp-proto-grpc = ""==1.24.0""
opentelemetry-exporter-otlp-proto-http = ""==1.24.0""
opentelemetry-proto = ""==1.24.0""
opentelemetry-sdk = ""==1.24.0""
opentelemetry-semantic-conventions = ""==0.45b0""
opt-einsum = ""==3.3.0""
optree = ""==0.11.0""
ordered-set = ""==4.1.0""
orjson = ""==3.10.0""
overrides = ""==6.5.0""
packaging = ""==24.0""
pandas = ""==2.1.4""
pandas-gbq = ""==0.22.0""
paramiko = ""==3.4.0""
parsedatetime = ""==2.4""
pathspec = ""==0.11.2""
pendulum = ""==2.1.2""
pip = ""==23.2.1""
pipdeptree = ""==2.18.1""
platformdirs = ""==4.2.0""
pluggy = ""==1.4.0""
prison = ""==0.2.1""
prometheus_client = ""==0.20.0""
prompt-toolkit = ""==3.0.43""
proto-plus = ""==1.23.0""
protobuf = ""==4.25.3""
psutil = ""==5.9.8""
psycopg2-binary = ""==2.9.9""
pyarrow = ""==14.0.2""
pyarrow-hotfix = ""==0.6""
pyasn1 = ""==0.5.1""
pyasn1-modules = ""==0.3.0""
pycparser = ""==2.22""
pydantic = ""==2.7.0""
pydantic_core = ""==2.18.1""
pydata-google-auth = ""==1.8.2""
pydot = ""==1.4.2""
Pygments = ""==2.17.2""
pyjsparser = ""==2.7.1""
PyJWT = ""==2.8.0""
pymongo = ""==4.6.3""
PyNaCl = ""==1.5.0""
pyOpenSSL = ""==24.1.0""
pyparsing = ""==3.1.2""
pytest = ""==7.4.4""
python-daemon = ""==3.0.1""
python-dateutil = ""==2.8.2""
python-http-client = ""==3.3.7""
python-nvd3 = ""==0.15.0""
python-slugify = ""==8.0.4""
pytimeparse = ""==1.1.8""
pytz = ""==2024.1""
pytzdata = ""==2020.1""
PyYAML = ""==6.0.1""
redis = ""==4.6.0""
referencing = ""==0.34.0""
regex = ""==2023.12.25""
requests = ""==2.31.0""
requests-oauthlib = ""==2.0.0""
requests-toolbelt = ""==1.0.0""
rfc3339-validator = ""==0.1.4""
rich = ""==13.7.1""
rich-argparse = ""==1.4.0""
rpds-py = ""==0.18.0""
rsa = ""==4.9""
SecretStorage = ""==3.3.3""
sendgrid = ""==6.11.0""
setproctitle = ""==1.3.3""
setuptools = ""==66.1.1""
shapely = ""==2.0.3""
six = ""==1.16.0""
sniffio = ""==1.3.1""
SQLAlchemy = ""==1.4.52""
sqlalchemy-bigquery = ""==1.10.0""
SQLAlchemy-JSONField = ""==1.0.2""
sqlalchemy-spanner = ""==1.6.2""
SQLAlchemy-Utils = ""==0.41.2""
sqlfluff = ""==2.3.5""
sqllineage = ""==1.4.9""
sqlparse = ""==0.4.4""
sshtunnel = ""==0.4.0""
starkbank-ecdsa = ""==2.2.0""
statsd = ""==4.0.1""
tabulate = ""==0.9.0""
tblib = ""==3.0.0""
tenacity = ""==8.2.3""
tensorboard = ""==2.16.2""
tensorboard-data-server = ""==0.7.2""
tensorflow = ""==2.16.1""
tensorflow-io-gcs-filesystem = ""==0.36.0""
termcolor = ""==2.4.0""
text-unidecode = ""==1.3""
tornado = ""==6.4""
tqdm = ""==4.66.2""
typing_extensions = ""==4.11.0""
tzdata = ""==2024.1""
tzlocal = ""==5.2""
uc-micro-py = ""==1.0.3""
unicodecsv = ""==0.14.1""
uritemplate = ""==4.1.1""
urllib3 = ""==2.2.1""
vine = ""==5.1.0""
virtualenv = ""==20.25.1""
wcwidth = ""==0.2.13""
websocket-client = ""==1.7.0""
Werkzeug = ""==2.2.3""
wheel = ""==0.43.0""
wrapt = ""==1.16.0""
WTForms = ""==3.0.1""
yarl = ""==1.9.4""
zipp = ""==3.18.1""
zstandard = ""==0.22.0""

[requires]
python_version = ""3.11.8""

```

### Anything else?

here are full logs from failed dag run
```
[2024-07-31T12:01:54.270-0400] {environment.py:213} INFO - Test environment, proceeding.
[2024-07-31T12:01:54.274-0400] {environment.py:213} INFO - Test environment, proceeding.
Created database `recidiviz_test_db0` on postgres instance bound to port 54300
[2024-07-31T12:01:55.179-0400] {environment.py:213} INFO - Test environment, proceeding.
To query data, connect with `psql postgresql://recidiviz_test_usr@localhost:54300/recidiviz_test_db0`
[2024-07-31T12:01:55.180-0400] {environment.py:213} INFO - Test environment, proceeding.
[2024-07-31T12:01:55.309-0400] {migration.py:216} INFO - Context impl PostgresqlImpl.
[2024-07-31T12:01:55.309-0400] {migration.py:219} INFO - Will assume transactional DDL.
INFO  [alembic.runtime.migration] Context impl PostgresqlImpl.
INFO  [alembic.runtime.migration] Will assume transactional DDL.
INFO  [alembic.runtime.migration] Running stamp_revision  -> 405de8318b3a
[2024-07-31 12:01:55,790] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='reproducible_dag' AIRFLOW_CTX_TASK_ID='generate_input' AIRFLOW_CTX_EXECUTION_DATE='2024-07-31T16:01:55.743454+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-07-31T16:01:55.743454+00:00'
INFO  [airflow.task] Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='reproducible_dag' AIRFLOW_CTX_TASK_ID='generate_input' AIRFLOW_CTX_EXECUTION_DATE='2024-07-31T16:01:55.743454+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-07-31T16:01:55.743454+00:00'
[2024-07-31 12:01:55,791] {python.py:194} INFO - Done. Returned value was: [1, 2, 3, 4, 5, 6, 7]
INFO  [airflow.task.operators] Done. Returned value was: [1, 2, 3, 4, 5, 6, 7]
[2024-07-31 12:01:55,795] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=reproducible_dag, task_id=generate_input, execution_date=20240731T160155, start_date=, end_date=20240731T160155
INFO  [airflow.task] Marking task as SUCCESS. dag_id=reproducible_dag, task_id=generate_input, execution_date=20240731T160155, start_date=, end_date=20240731T160155
[2024-07-31 12:01:55,816] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1518, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1647, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2285, in render_templates
    original_task.render_template_fields(context)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 725, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/decorators/base.py"", line 527, in _expand_mapped_kwargs
    raise AssertionError(f""unexpected expand_input: {self.expand_input}"")
AssertionError: unexpected expand_input: DictOfListsExpandInput(value={})
ERROR [airflow.task] Task failed with exception
Traceback (most recent call last):
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1518, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1647, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2285, in render_templates
    original_task.render_template_fields(context)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 725, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/decorators/base.py"", line 527, in _expand_mapped_kwargs
    raise AssertionError(f""unexpected expand_input: {self.expand_input}"")
AssertionError: unexpected expand_input: DictOfListsExpandInput(value={})
[2024-07-31 12:01:55,818] {taskinstance.py:1983} ERROR - Unable to unmap task to determine if we need to send an alert email
ERROR [airflow.task] Unable to unmap task to determine if we need to send an alert email
[2024-07-31 12:01:55,818] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=reproducible_dag, task_id=mapped, map_index=0, execution_date=20240731T160155, start_date=, end_date=20240731T160155
INFO  [airflow.task] Marking task as FAILED. dag_id=reproducible_dag, task_id=mapped, map_index=0, execution_date=20240731T160155, start_date=, end_date=20240731T160155
ERROR [airflow.models.dag.DAG] Task failed; ti=<TaskInstance: reproducible_dag.mapped manual__2024-07-31T16:01:55.743454+00:00 map_index=0 [failed]>
Traceback (most recent call last):
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/dag.py"", line 2776, in test
    ret = _run_task(ti, session=session)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/dag.py"", line 3938, in _run_task
    ret = ti._run_raw_task(session=session)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/utils/session.py"", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1518, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1647, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2285, in render_templates
    original_task.render_template_fields(context)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 725, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/decorators/base.py"", line 527, in _expand_mapped_kwargs
    raise AssertionError(f""unexpected expand_input: {self.expand_input}"")
AssertionError: unexpected expand_input: DictOfListsExpandInput(value={})
[2024-07-31 12:01:55,825] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1518, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1647, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2285, in render_templates
    original_task.render_template_fields(context)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 725, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/decorators/base.py"", line 527, in _expand_mapped_kwargs
    raise AssertionError(f""unexpected expand_input: {self.expand_input}"")
AssertionError: unexpected expand_input: DictOfListsExpandInput(value={})
ERROR [airflow.task] Task failed with exception
Traceback (most recent call last):
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1518, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1647, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2285, in render_templates
    original_task.render_template_fields(context)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 725, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/decorators/base.py"", line 527, in _expand_mapped_kwargs
    raise AssertionError(f""unexpected expand_input: {self.expand_input}"")
AssertionError: unexpected expand_input: DictOfListsExpandInput(value={})
[2024-07-31 12:01:55,825] {taskinstance.py:1983} ERROR - Unable to unmap task to determine if we need to send an alert email
ERROR [airflow.task] Unable to unmap task to determine if we need to send an alert email
[2024-07-31 12:01:55,826] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=reproducible_dag, task_id=mapped, map_index=1, execution_date=20240731T160155, start_date=, end_date=20240731T160155
INFO  [airflow.task] Marking task as FAILED. dag_id=reproducible_dag, task_id=mapped, map_index=1, execution_date=20240731T160155, start_date=, end_date=20240731T160155
ERROR [airflow.models.dag.DAG] Task failed; ti=<TaskInstance: reproducible_dag.mapped manual__2024-07-31T16:01:55.743454+00:00 map_index=1 [failed]>
Traceback (most recent call last):
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/dag.py"", line 2776, in test
    ret = _run_task(ti, session=session)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/dag.py"", line 3938, in _run_task
    ret = ti._run_raw_task(session=session)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/utils/session.py"", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1518, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1647, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2285, in render_templates
    original_task.render_template_fields(context)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 725, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/decorators/base.py"", line 527, in _expand_mapped_kwargs
    raise AssertionError(f""unexpected expand_input: {self.expand_input}"")
AssertionError: unexpected expand_input: DictOfListsExpandInput(value={})
[2024-07-31 12:01:55,831] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1518, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1647, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2285, in render_templates
    original_task.render_template_fields(context)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 725, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/decorators/base.py"", line 527, in _expand_mapped_kwargs
    raise AssertionError(f""unexpected expand_input: {self.expand_input}"")
AssertionError: unexpected expand_input: DictOfListsExpandInput(value={})
ERROR [airflow.task] Task failed with exception
Traceback (most recent call last):
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1518, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1647, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2285, in render_templates
    original_task.render_template_fields(context)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 725, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/decorators/base.py"", line 527, in _expand_mapped_kwargs
    raise AssertionError(f""unexpected expand_input: {self.expand_input}"")
AssertionError: unexpected expand_input: DictOfListsExpandInput(value={})
[2024-07-31 12:01:55,832] {taskinstance.py:1983} ERROR - Unable to unmap task to determine if we need to send an alert email
ERROR [airflow.task] Unable to unmap task to determine if we need to send an alert email
[2024-07-31 12:01:55,832] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=reproducible_dag, task_id=mapped, map_index=2, execution_date=20240731T160155, start_date=, end_date=20240731T160155
INFO  [airflow.task] Marking task as FAILED. dag_id=reproducible_dag, task_id=mapped, map_index=2, execution_date=20240731T160155, start_date=, end_date=20240731T160155
ERROR [airflow.models.dag.DAG] Task failed; ti=<TaskInstance: reproducible_dag.mapped manual__2024-07-31T16:01:55.743454+00:00 map_index=2 [failed]>
Traceback (most recent call last):
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/dag.py"", line 2776, in test
    ret = _run_task(ti, session=session)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/dag.py"", line 3938, in _run_task
    ret = ti._run_raw_task(session=session)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/utils/session.py"", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1518, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1647, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2285, in render_templates
    original_task.render_template_fields(context)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 725, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/decorators/base.py"", line 527, in _expand_mapped_kwargs
    raise AssertionError(f""unexpected expand_input: {self.expand_input}"")
AssertionError: unexpected expand_input: DictOfListsExpandInput(value={})
[2024-07-31 12:01:55,838] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1518, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1647, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2285, in render_templates
    original_task.render_template_fields(context)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 725, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/decorators/base.py"", line 527, in _expand_mapped_kwargs
    raise AssertionError(f""unexpected expand_input: {self.expand_input}"")
AssertionError: unexpected expand_input: DictOfListsExpandInput(value={})
ERROR [airflow.task] Task failed with exception
Traceback (most recent call last):
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1518, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1647, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2285, in render_templates
    original_task.render_template_fields(context)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 725, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/decorators/base.py"", line 527, in _expand_mapped_kwargs
    raise AssertionError(f""unexpected expand_input: {self.expand_input}"")
AssertionError: unexpected expand_input: DictOfListsExpandInput(value={})
[2024-07-31 12:01:55,840] {taskinstance.py:1983} ERROR - Unable to unmap task to determine if we need to send an alert email
ERROR [airflow.task] Unable to unmap task to determine if we need to send an alert email
[2024-07-31 12:01:55,840] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=reproducible_dag, task_id=mapped, map_index=3, execution_date=20240731T160155, start_date=, end_date=20240731T160155
INFO  [airflow.task] Marking task as FAILED. dag_id=reproducible_dag, task_id=mapped, map_index=3, execution_date=20240731T160155, start_date=, end_date=20240731T160155
ERROR [airflow.models.dag.DAG] Task failed; ti=<TaskInstance: reproducible_dag.mapped manual__2024-07-31T16:01:55.743454+00:00 map_index=3 [failed]>
Traceback (most recent call last):
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/dag.py"", line 2776, in test
    ret = _run_task(ti, session=session)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/dag.py"", line 3938, in _run_task
    ret = ti._run_raw_task(session=session)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/utils/session.py"", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1518, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1647, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2285, in render_templates
    original_task.render_template_fields(context)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 725, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/decorators/base.py"", line 527, in _expand_mapped_kwargs
    raise AssertionError(f""unexpected expand_input: {self.expand_input}"")
AssertionError: unexpected expand_input: DictOfListsExpandInput(value={})
[2024-07-31 12:01:55,847] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1518, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1647, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2285, in render_templates
    original_task.render_template_fields(context)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 725, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/decorators/base.py"", line 527, in _expand_mapped_kwargs
    raise AssertionError(f""unexpected expand_input: {self.expand_input}"")
AssertionError: unexpected expand_input: DictOfListsExpandInput(value={})
ERROR [airflow.task] Task failed with exception
Traceback (most recent call last):
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1518, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1647, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2285, in render_templates
    original_task.render_template_fields(context)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 725, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/decorators/base.py"", line 527, in _expand_mapped_kwargs
    raise AssertionError(f""unexpected expand_input: {self.expand_input}"")
AssertionError: unexpected expand_input: DictOfListsExpandInput(value={})
[2024-07-31 12:01:55,848] {taskinstance.py:1983} ERROR - Unable to unmap task to determine if we need to send an alert email
ERROR [airflow.task] Unable to unmap task to determine if we need to send an alert email
[2024-07-31 12:01:55,848] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=reproducible_dag, task_id=mapped, map_index=4, execution_date=20240731T160155, start_date=, end_date=20240731T160155
INFO  [airflow.task] Marking task as FAILED. dag_id=reproducible_dag, task_id=mapped, map_index=4, execution_date=20240731T160155, start_date=, end_date=20240731T160155
ERROR [airflow.models.dag.DAG] Task failed; ti=<TaskInstance: reproducible_dag.mapped manual__2024-07-31T16:01:55.743454+00:00 map_index=4 [failed]>
Traceback (most recent call last):
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/dag.py"", line 2776, in test
    ret = _run_task(ti, session=session)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/dag.py"", line 3938, in _run_task
    ret = ti._run_raw_task(session=session)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/utils/session.py"", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1518, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1647, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2285, in render_templates
    original_task.render_template_fields(context)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 725, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/decorators/base.py"", line 527, in _expand_mapped_kwargs
    raise AssertionError(f""unexpected expand_input: {self.expand_input}"")
AssertionError: unexpected expand_input: DictOfListsExpandInput(value={})
[2024-07-31 12:01:55,853] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1518, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1647, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2285, in render_templates
    original_task.render_template_fields(context)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 725, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/decorators/base.py"", line 527, in _expand_mapped_kwargs
    raise AssertionError(f""unexpected expand_input: {self.expand_input}"")
AssertionError: unexpected expand_input: DictOfListsExpandInput(value={})
ERROR [airflow.task] Task failed with exception
Traceback (most recent call last):
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1518, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1647, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2285, in render_templates
    original_task.render_template_fields(context)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 725, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/decorators/base.py"", line 527, in _expand_mapped_kwargs
    raise AssertionError(f""unexpected expand_input: {self.expand_input}"")
AssertionError: unexpected expand_input: DictOfListsExpandInput(value={})
[2024-07-31 12:01:55,854] {taskinstance.py:1983} ERROR - Unable to unmap task to determine if we need to send an alert email
ERROR [airflow.task] Unable to unmap task to determine if we need to send an alert email
[2024-07-31 12:01:55,854] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=reproducible_dag, task_id=mapped, map_index=5, execution_date=20240731T160155, start_date=, end_date=20240731T160155
INFO  [airflow.task] Marking task as FAILED. dag_id=reproducible_dag, task_id=mapped, map_index=5, execution_date=20240731T160155, start_date=, end_date=20240731T160155
ERROR [airflow.models.dag.DAG] Task failed; ti=<TaskInstance: reproducible_dag.mapped manual__2024-07-31T16:01:55.743454+00:00 map_index=5 [failed]>
Traceback (most recent call last):
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/dag.py"", line 2776, in test
    ret = _run_task(ti, session=session)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/dag.py"", line 3938, in _run_task
    ret = ti._run_raw_task(session=session)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/utils/session.py"", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1518, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1647, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2285, in render_templates
    original_task.render_template_fields(context)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 725, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/decorators/base.py"", line 527, in _expand_mapped_kwargs
    raise AssertionError(f""unexpected expand_input: {self.expand_input}"")
AssertionError: unexpected expand_input: DictOfListsExpandInput(value={})
[2024-07-31 12:01:55,860] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1518, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1647, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2285, in render_templates
    original_task.render_template_fields(context)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 725, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/decorators/base.py"", line 527, in _expand_mapped_kwargs
    raise AssertionError(f""unexpected expand_input: {self.expand_input}"")
AssertionError: unexpected expand_input: DictOfListsExpandInput(value={})
ERROR [airflow.task] Task failed with exception
Traceback (most recent call last):
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1518, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1647, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2285, in render_templates
    original_task.render_template_fields(context)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 725, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/decorators/base.py"", line 527, in _expand_mapped_kwargs
    raise AssertionError(f""unexpected expand_input: {self.expand_input}"")
AssertionError: unexpected expand_input: DictOfListsExpandInput(value={})
[2024-07-31 12:01:55,862] {taskinstance.py:1983} ERROR - Unable to unmap task to determine if we need to send an alert email
ERROR [airflow.task] Unable to unmap task to determine if we need to send an alert email
[2024-07-31 12:01:55,862] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=reproducible_dag, task_id=mapped, map_index=6, execution_date=20240731T160155, start_date=, end_date=20240731T160155
INFO  [airflow.task] Marking task as FAILED. dag_id=reproducible_dag, task_id=mapped, map_index=6, execution_date=20240731T160155, start_date=, end_date=20240731T160155
ERROR [airflow.models.dag.DAG] Task failed; ti=<TaskInstance: reproducible_dag.mapped manual__2024-07-31T16:01:55.743454+00:00 map_index=6 [failed]>
Traceback (most recent call last):
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/dag.py"", line 2776, in test
    ret = _run_task(ti, session=session)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/dag.py"", line 3938, in _run_task
    ret = ti._run_raw_task(session=session)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/utils/session.py"", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1518, in _run_raw_task
    self._execute_task_with_callbacks(context, test_mode, session=session)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 1647, in _execute_task_with_callbacks
    task_orig = self.render_templates(context=context)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/taskinstance.py"", line 2285, in render_templates
    original_task.render_template_fields(context)
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/models/mappedoperator.py"", line 725, in render_template_fields
    mapped_kwargs, seen_oids = self._expand_mapped_kwargs(context, session)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/oro/.local/share/virtualenvs/airflow-JBf5o6AR/lib/python3.11/site-packages/airflow/decorators/base.py"", line 527, in _expand_mapped_kwargs
    raise AssertionError(f""unexpected expand_input: {self.expand_input}"")
AssertionError: unexpected expand_input: DictOfListsExpandInput(value={})
ERROR [airflow.models.dagrun.DagRun] Marking run <DagRun reproducible_dag @ 2024-07-31T16:01:55.743454+00:00: manual__2024-07-31T16:01:55.743454+00:00, state:running, queued_at: None. externally triggered: False> failed
Traceback (most recent call last):
  File ""/Users/oro/Library/Application Support/Code/User/globalStorage/buenon.scratchpads/scratchpads/1a4f2b73c916c9af3926773a674c6453/scratch107.py"", line 74, in <module>
    assert dag_run.get_state() == DagRunState.SUCCESS
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError
```

### Are you willing to submit PR?

- [X] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",ethan-oro,2024-07-31 16:02:30+00:00,['ethan-oro'],2024-08-17 12:10:34+00:00,,https://github.com/apache/airflow/issues/41160,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:dynamic-task-mapping', 'AIP-42')]","[{'comment_id': 2260865113, 'issue_id': 2440403648, 'author': 'boring-cyborg[bot]', 'body': 'Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.', 'created_at': datetime.datetime(2024, 7, 31, 16, 2, 33, tzinfo=datetime.timezone.utc)}, {'comment_id': 2289789116, 'issue_id': 2440403648, 'author': 'harjeevanmaan', 'body': 'I would like to work on it', 'created_at': datetime.datetime(2024, 8, 14, 20, 13, 37, tzinfo=datetime.timezone.utc)}]","boring-cyborg[bot] on (2024-07-31 16:02:33 UTC): Thanks for opening your first issue here! Be sure to follow the issue template! If you are willing to raise PR to address this issue please do so, no need to wait for approval.

harjeevanmaan on (2024-08-14 20:13:37 UTC): I would like to work on it

"
2439582448,issue,open,,Cannot delete DAG with many runs,"### Apache Airflow version

Other Airflow 2 version (please specify below)

### If ""Other Airflow 2 version"" selected, which one?

2.8.1 (AWS MWAA)

### What happened?

I tried to delete a DAG with many runs (14000+). After about 4 minutes I receive ""xxx.yyy.airflow.amazonaws.com didn’t send any data. ERR_EMPTY_RESPONSE"", and the DAG is not deleted.

### What you think should happen instead?

The DAG should have been deleted.

### How to reproduce

Create a DAG with 14000 runs and try to delete it.

### Operating System

AWS MWAA

### Versions of Apache Airflow Providers

_No response_

### Deployment

Amazon (AWS) MWAA

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",GergelyKalmar,2024-07-31 09:31:06+00:00,[],2024-08-06 06:35:17+00:00,,https://github.com/apache/airflow/issues/41148,"[('kind:bug', 'This is a clearly a bug'), ('area:MetaDB', 'Meta Database related issues.'), ('area:core', ''), ('affected_version:2.8', 'Issues Reported for 2.8')]","[{'comment_id': 2260726141, 'issue_id': 2439582448, 'author': 'eladkal', 'body': 'yes this is a bug but just to clarify the delete partially works.\r\nIt deletes some of the records before raising the error, so every time you click on the button it will reduce the amount of records. I assume 2-3 times will delete all the records\r\n\r\n> and the DAG is not deleted.\r\n\r\nDAG can not be deleted from the UI. the delete only removes the records associated with it (DagRun, TaskInstance...)\r\nif you want to remove dag completely then you need also to remove the .py file of the DAG and then delete the records (with the UI or API if the dag is not present)', 'created_at': datetime.datetime(2024, 7, 31, 14, 58, 14, tzinfo=datetime.timezone.utc)}, {'comment_id': 2260729083, 'issue_id': 2439582448, 'author': 'eladkal', 'body': 'tagged with DB label as this is probably related to inefficient query to the DB.', 'created_at': datetime.datetime(2024, 7, 31, 14, 59, 26, tzinfo=datetime.timezone.utc)}, {'comment_id': 2262808513, 'issue_id': 2439582448, 'author': 'GergelyKalmar', 'body': ""> yes this is a bug but just to clarify the delete partially works. It deletes some of the records before raising the error, so every time you click on the button it will reduce the amount of records. I assume 2-3 times will delete all the records\r\n\r\nAre you sure? The DAG run instance numbers seems to be the same after the crash, could it be that the deletion is rolled back and so it does not execute partially? I've tried to re-run the deletion many times, and I saw no change.\r\n\r\n> DAG can not be deleted from the UI. the delete only removes the records associated with it (DagRun, TaskInstance...) if you want to remove dag completely then you need also to remove the .py file of the DAG and then delete the records (with the UI or API if the dag is not present)\r\n\r\nYes, I'm aware of this, I meant the removal of the related records."", 'created_at': datetime.datetime(2024, 8, 1, 11, 29, 2, tzinfo=datetime.timezone.utc)}, {'comment_id': 2270491305, 'issue_id': 2439582448, 'author': 'omkar-foss', 'body': '>I tried to delete a DAG with many runs (14000+). After about 4 minutes I receive ""xxx.yyy.airflow.amazonaws.com didn’t send any data. ERR_EMPTY_RESPONSE"", and the DAG is not deleted.\r\n\r\n@GergelyKalmar the `ERR_EMPTY_RESPONSE` suggests that the connection is closed before the DAG deletion completed, particularly because the 14k+ runs deletion operation [in this function](https://github.com/apache/airflow/blob/main/airflow/api/common/delete_dag.py#L42-L108) is taking too long. This is a bug as @eladkal pointed out, and when this bug gets fixed here in upstream Airflow, it may be a while before it reflects in downstream MWAA which you\'re using.\r\n\r\nMeanwhile, I\'d suggest you try to delete the DAG via the Airflow REST API with an increased request timeout. I\'ve written a small script for this ([Gist here](https://gist.github.com/omkar-foss/51fe2e79a8da628833412652e69f32d1)) by referring to the MWAA docs, please check it out. You can tweak the script as per your need and increase the timeout beyond 10 mins if the deletion takes longer and fails. Hope this helps.', 'created_at': datetime.datetime(2024, 8, 6, 6, 34, 38, tzinfo=datetime.timezone.utc)}]","eladkal on (2024-07-31 14:58:14 UTC): yes this is a bug but just to clarify the delete partially works.
It deletes some of the records before raising the error, so every time you click on the button it will reduce the amount of records. I assume 2-3 times will delete all the records


DAG can not be deleted from the UI. the delete only removes the records associated with it (DagRun, TaskInstance...)
if you want to remove dag completely then you need also to remove the .py file of the DAG and then delete the records (with the UI or API if the dag is not present)

eladkal on (2024-07-31 14:59:26 UTC): tagged with DB label as this is probably related to inefficient query to the DB.

GergelyKalmar (Issue Creator) on (2024-08-01 11:29:02 UTC): Are you sure? The DAG run instance numbers seems to be the same after the crash, could it be that the deletion is rolled back and so it does not execute partially? I've tried to re-run the deletion many times, and I saw no change.


Yes, I'm aware of this, I meant the removal of the related records.

omkar-foss on (2024-08-06 06:34:38 UTC): @GergelyKalmar the `ERR_EMPTY_RESPONSE` suggests that the connection is closed before the DAG deletion completed, particularly because the 14k+ runs deletion operation [in this function](https://github.com/apache/airflow/blob/main/airflow/api/common/delete_dag.py#L42-L108) is taking too long. This is a bug as @eladkal pointed out, and when this bug gets fixed here in upstream Airflow, it may be a while before it reflects in downstream MWAA which you're using.

Meanwhile, I'd suggest you try to delete the DAG via the Airflow REST API with an increased request timeout. I've written a small script for this ([Gist here](https://gist.github.com/omkar-foss/51fe2e79a8da628833412652e69f32d1)) by referring to the MWAA docs, please check it out. You can tweak the script as per your need and increase the timeout beyond 10 mins if the deletion takes longer and fails. Hope this helps.

"
2438887926,issue,closed,completed,Using `MappedOperator` creates `Triggered DAG` incorrectly,"### Apache Airflow version

2.9.3

### If ""Other Airflow 2 version"" selected, which one?

_No response_

### What happened?

When I create a `TriggerDagRunOperator` with `partial` and `expand_kwargs`,
the `Triggered DAG` link button appears to be disabled,
and when I click it, it leads to the wrong link.

incorrect url sample: `https://localhost:8080/www/dags/trigger_test/grid?tab=details&dag_run_id=manual__2024-07-31T00%3A27%3A35.113684%2B00%3A00&task_id=trigger&map_index=0`
expected url sample: `https://localhost:8080/www/dags/trigger_target/graph?dag_run_id=manual__2024-07-31T00%3A44%3A45.582823%2B00%3A00-0`

![스크린샷 2024-07-31 오전 9 45 22](https://github.com/user-attachments/assets/5606ee0a-2332-4a3e-a9cf-ed924e5b0567)
![스크린샷 2024-07-31 오전 9 45 13](https://github.com/user-attachments/assets/24058961-ba3b-4c57-b340-c31d962999dc)


### What you think should happen instead?

_No response_

### How to reproduce

```python
# pyright: reportTypedDictNotRequiredAccess=false
from __future__ import annotations

from typing import Any

from airflow.decorators import dag, task
from airflow.operators.empty import EmptyOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from pendulum import datetime


@dag(start_date=datetime(2024, 1, 1), schedule=None, catchup=False)
def trigger_test() -> None:  # noqa: D103
    @task.python(do_xcom_push=True, multiple_outputs=False)
    def create_conf() -> list[dict[str, Any]]:
        from airflow.operators.python import get_current_context

        context = get_current_context()
        task_instance = context[""task_instance""]
        run_id = task_instance.run_id
        return [
            {""conf"": {""value"": idx}, ""trigger_run_id"": f""{run_id}-{idx}""}
            for idx in range(2)
        ]

    conf = create_conf()
    trigger = TriggerDagRunOperator.partial(
        trigger_dag_id=""trigger_target"",
        task_id=""trigger"",
        deferrable=False,
        wait_for_completion=False,
    ).expand_kwargs(conf)
    other = TriggerDagRunOperator(
        task_id=""other"",
        trigger_dag_id=""trigger_target"",
        trigger_run_id=""{{ ti.run_id }}-other"",
        deferrable=False,
        wait_for_completion=False,
        conf={""value"": 3},
    )

    _ = conf >> [trigger, other]


@dag(start_date=datetime(2024, 1, 1), schedule=None, catchup=False)
def trigger_target() -> None:  # noqa: D103
    EmptyOperator(task_id=""empty_task"")


trigger_test()
trigger_target()

```

### Operating System

PRETTY_NAME=""Debian GNU/Linux 12 (bookworm)"" NAME=""Debian GNU/Linux"" VERSION_ID=""12"" VERSION=""12 (bookworm)"" VERSION_CODENAME=bookworm ID=debian HOME_URL=""https://www.debian.org/"" SUPPORT_URL=""https://www.debian.org/support"" BUG_REPORT_URL=""https://bugs.debian.org/""

### Versions of Apache Airflow Providers

```shell
➜  airflow pip freeze | grep apache-airflow-providers
apache-airflow-providers-celery==3.7.2
apache-airflow-providers-common-io==1.3.2
apache-airflow-providers-common-sql==1.14.2
apache-airflow-providers-docker==3.12.2
apache-airflow-providers-fab==1.1.1
apache-airflow-providers-ftp==3.10.0
apache-airflow-providers-http==4.12.0
apache-airflow-providers-imap==3.6.1
apache-airflow-providers-jdbc==4.3.1
apache-airflow-providers-odbc==4.6.2
apache-airflow-providers-postgres==5.11.2
apache-airflow-providers-redis==3.7.1
apache-airflow-providers-smtp==1.7.1
apache-airflow-providers-sqlite==3.8.1
```

### Deployment

Docker-Compose

### Deployment details

_No response_

### Anything else?

_No response_

### Are you willing to submit PR?

- [ ] Yes I am willing to submit a PR!

### Code of Conduct

- [X] I agree to follow this project's [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)
",phi-friday,2024-07-31 00:39:16+00:00,[],2024-11-18 07:08:46+00:00,2024-11-18 07:08:46+00:00,https://github.com/apache/airflow/issues/41145,"[('kind:bug', 'This is a clearly a bug'), ('area:core', ''), ('area:UI', 'Related to UI/UX. For Frontend Developers.'), ('area:dynamic-task-mapping', 'AIP-42'), ('affected_version:2.9', 'Issues Reported for 2.9')]","[{'comment_id': 2364992555, 'issue_id': 2438887926, 'author': 'fredthomsen', 'body': 'The request to the `/extra_links` view causes a crash with the follow traceback:\r\n\r\n```\r\n[2024-09-19T03:38:49.712+0000] {app.py:1744} ERROR - Exception on /extra_links [GET]\r\nTraceback (most recent call last):\r\n  File ""/usr/local/lib/python3.11/site-packages/flask/app.py"", line 2529, in wsgi_app\r\n    response = self.full_dispatch_request()\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.11/site-packages/flask/app.py"", line 1825, in full_dispatch_request\r\n    rv = self.handle_user_exception(e)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.11/site-packages/flask/app.py"", line 1823, in full_dispatch_request\r\n    rv = self.dispatch_request()\r\n         ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.11/site-packages/flask/app.py"", line 1799, in dispatch_request\r\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/airflow/airflow/www/auth.py"", line 226, in decorated\r\n    return _has_access(\r\n           ^^^^^^^^^^^^\r\n  File ""/opt/airflow/airflow/www/auth.py"", line 139, in _has_access\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/airflow/airflow/utils/session.py"", line 97, in wrapper\r\n    return func(*args, session=session, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/airflow/airflow/www/views.py"", line 3231, in extra_links\r\n    url = task.get_extra_links(ti, link_name)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/airflow/airflow/models/abstractoperator.py"", line 541, in get_extra_links\r\n    return link.get_link(self.unmap(None), ti_key=ti.key)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/airflow/airflow/operators/trigger_dagrun.py"", line 78, in get_link\r\n    return build_airflow_url_with_query(query)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/opt/airflow/airflow/utils/helpers.py"", line 242, in build_airflow_url_with_query\r\n    return flask.url_for(f""Airflow.{view}"", **query)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.11/site-packages/flask/helpers.py"", line 256, in url_for\r\n    return current_app.url_for(\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.11/site-packages/flask/app.py"", line 2034, in url_for\r\n    return self.handle_url_build_error(error, endpoint, values)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.11/site-packages/flask/app.py"", line 2023, in url_for\r\n    rv = url_adapter.build(  # type: ignore[union-attr]\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File ""/usr/local/lib/python3.11/site-packages/werkzeug/routing/map.py"", line 917, in build\r\n    raise BuildError(endpoint, values, method, self)\r\nwerkzeug.routing.exceptions.BuildError: Could not build url for endpoint \'Airflow.grid\' with values [\'dag_run_id\']. Did you forget to specify values [\'dag_id\']\r\n```\r\n\r\nHas some similarity to #32150.', 'created_at': datetime.datetime(2024, 9, 21, 4, 22, 26, tzinfo=datetime.timezone.utc)}]","fredthomsen on (2024-09-21 04:22:26 UTC): The request to the `/extra_links` view causes a crash with the follow traceback:

```
[2024-09-19T03:38:49.712+0000] {app.py:1744} ERROR - Exception on /extra_links [GET]
Traceback (most recent call last):
  File ""/usr/local/lib/python3.11/site-packages/flask/app.py"", line 2529, in wsgi_app
    response = self.full_dispatch_request()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/flask/app.py"", line 1825, in full_dispatch_request
    rv = self.handle_user_exception(e)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/flask/app.py"", line 1823, in full_dispatch_request
    rv = self.dispatch_request()
         ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/flask/app.py"", line 1799, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/airflow/www/auth.py"", line 226, in decorated
    return _has_access(
           ^^^^^^^^^^^^
  File ""/opt/airflow/airflow/www/auth.py"", line 139, in _has_access
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/airflow/utils/session.py"", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/airflow/www/views.py"", line 3231, in extra_links
    url = task.get_extra_links(ti, link_name)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/airflow/models/abstractoperator.py"", line 541, in get_extra_links
    return link.get_link(self.unmap(None), ti_key=ti.key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/airflow/operators/trigger_dagrun.py"", line 78, in get_link
    return build_airflow_url_with_query(query)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/opt/airflow/airflow/utils/helpers.py"", line 242, in build_airflow_url_with_query
    return flask.url_for(f""Airflow.{view}"", **query)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/flask/helpers.py"", line 256, in url_for
    return current_app.url_for(
           ^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/flask/app.py"", line 2034, in url_for
    return self.handle_url_build_error(error, endpoint, values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/flask/app.py"", line 2023, in url_for
    rv = url_adapter.build(  # type: ignore[union-attr]
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/site-packages/werkzeug/routing/map.py"", line 917, in build
    raise BuildError(endpoint, values, method, self)
werkzeug.routing.exceptions.BuildError: Could not build url for endpoint 'Airflow.grid' with values ['dag_run_id']. Did you forget to specify values ['dag_id']
```

Has some similarity to #32150.

"
